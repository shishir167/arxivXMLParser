<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T03:50:16Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|80001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08478</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08478</id><created>2015-06-28</created><authors><author><keyname>Hyder</keyname><forenames>Md Mashud</forenames></author></authors><title>Multiuser Detection for Random Access Bandwidth Request in WiMAX</title><categories>cs.IT math.IT</categories><comments>12 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random access is a multiple access communication protocol where the users
simultaneously communicate with a base station (BS) in an uncoordinated
fashion. In this work, we consider the problem of multiuser detection in a
random access bandwidth request context. We propose an enhanced random access
scheme where the fixed/low-mobility M2M devices pre-equalize their random
access codes using the estimated frequency response of the slowly-varying
wireless channel. Consequently, we have developed two different multiuser
detection algorithms. The first algorithm works in a greedy fashion where it
performs cross-correlation of the received signal with a set of decoder
sequences and detects active users based on the correlation output. We derive
the condition under which the algorithm can detect a given number of active
users with high probability. Subsequently, we demonstrate an efficient decoder
design procedure which enhances the user detection performance. A basis
mismatched sparse recovery technique has been applied in the second algorithm
which exploit an inherent structure of the random access protocol. The
performance of the proposed schemes is demonstrated in a WiMAX network
environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08485</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08485</id><created>2015-06-28</created><authors><author><keyname>Melman</keyname><forenames>Shachaf</forenames></author><author><keyname>Moses</keyname><forenames>Yael</forenames></author><author><keyname>Medioni</keyname><forenames>G&#xe9;rard</forenames></author><author><keyname>Cai</keyname><forenames>Yinghao</forenames></author></authors><title>The Multi-Strand Graph for a PTZ Tracker</title><categories>cs.CV</categories><comments>9 pages, 7 figures, AVSS2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High-resolution images can be used to resolve matching ambiguities between
trajectory fragments (tracklets), which is one of the main challenges in
multiple target tracking. A PTZ camera, which can pan, tilt and zoom, is a
powerful and efficient tool that offers both close-up views and wide area
coverage on demand. The wide-area view makes it possible to track many targets
while the close-up view allows individuals to be identified from
high-resolution images of their faces. A central component of a PTZ tracking
system is a scheduling algorithm that determines which target to zoom in on.
  In this paper we study this scheduling problem from a theoretical
perspective, where the high resolution images are also used for tracklet
matching. We propose a novel data structure, the Multi-Strand Tracking Graph
(MSG), which represents the set of tracklets computed by a tracker and the
possible associations between them. The MSG allows efficient scheduling as well
as resolving -- directly or by elimination -- matching ambiguities between
tracklets. The main feature of the MSG is the auxiliary data saved in each
vertex, which allows efficient computation while avoiding time-consuming graph
traversal. Synthetic data simulations are used to evaluate our scheduling
algorithm and to demonstrate its superiority over a na\&quot;ive one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08487</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08487</id><created>2015-06-28</created><updated>2015-07-04</updated><authors><author><keyname>Peng</keyname><forenames>T.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author></authors><title>Study of Buffer-Aided Space-Time Coding for Multiple-Antenna Cooperative
  Wireless Networks</title><categories>cs.IT math.IT</categories><comments>7 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we propose an adaptive buffer-aided space-time coding scheme for
cooperative wireless networks. A maximum likelihood receiver and adjustable
code vectors are considered subject to a power constraint with an
amplify-and-forward cooperation strategy. Each multiple-antenna relay is
equipped with a buffer and is capable of storing the received symbols before
forwarding them to the destination. We also present an adaptive relay selection
and optimization algorithm, in which the instantaneous signal to noise ratio in
each link is calculated and compared at the destination. An adjustable code
vector obtained by a feedback channel at each relay is employed to form a
space-time coded vector which achieves a higher coding gain than standard
schemes. A stochastic gradient algorithm is developed to compute the parameters
of the adjustable code vector with reduced computational complexity. Simulation
results show that the proposed buffer-aided scheme and algorithm obtain
performance gains over existing schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08499</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08499</id><created>2015-06-28</created><authors><author><keyname>Liu</keyname><forenames>Yipeng</forenames></author><author><keyname>De Vos</keyname><forenames>Maarten</forenames></author><author><keyname>Van Huffel</keyname><forenames>Sabine</forenames></author></authors><title>Compressed Sensing of Multi-Channel EEG Signals: The Simultaneous
  Cosparsity and Low Rank Optimization</title><categories>cs.IT math.IT stat.ML</categories><comments>11 pages, 3 figures; accepted by IEEE Transactions on Biomedical
  Engineering</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Goal: This paper deals with the problems that some EEG signals have no good
sparse representation and single channel processing is not computationally
efficient in compressed sensing of multi-channel EEG signals. Methods: An
optimization model with L0 norm and Schatten-0 norm is proposed to enforce
cosparsity and low rank structures in the reconstructed multi-channel EEG
signals. Both convex relaxation and global consensus optimization with
alternating direction method of multipliers are used to compute the
optimization model. Results: The performance of multi-channel EEG signal
reconstruction is improved in term of both accuracy and computational
complexity. Conclusion: The proposed method is a better candidate than previous
sparse signal recovery methods for compressed sensing of EEG signals.
Significance: The proposed method enables successful compressed sensing of EEG
signals even when the signals have no good sparse representation. Using
compressed sensing would much reduce the power consumption of wireless EEG
system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08501</identifier>
 <datestamp>2015-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08501</id><created>2015-06-29</created><updated>2015-08-05</updated><authors><author><keyname>Fayaz</keyname><forenames>Seyed K.</forenames></author><author><keyname>Tobioka</keyname><forenames>Yoshiaki</forenames></author><author><keyname>Sekar</keyname><forenames>Vyas</forenames></author><author><keyname>Bailey</keyname><forenames>Michael</forenames></author></authors><title>A New Approach to DDoS Defense using SDN and NFV</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Networks today rely on expensive and proprietary hard- ware appliances, which
are deployed at fixed locations, for DDoS defense. This introduces key
limitations with respect to flexibility (e.g., complex routing to get traffic
to these &quot;chokepoints&quot;) and elasticity in handling changing attack patterns. We
observe an opportunity to ad- dress these limitations using new networking
paradigms such as software-defined networking (SDN) and network functions
virtualization (NFV). Based on this observation, we design and implement of
Bohatei, an elastic and flexible DDoS defense system. In designing Bohatei, we
address key challenges of scalability, responsive- ness, and
adversary-resilience. We have implemented defenses for several well-known DDoS
attacks in Bohatei. Our evaluations show that Bohatei is scalable (handling 500
Gbps attacks), responsive (mitigating attacks within one minute), and resilient
to dynamic adversaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08503</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08503</id><created>2015-06-29</created><authors><author><keyname>Kepner</keyname><forenames>Jeremy</forenames></author><author><keyname>Gadepally</keyname><forenames>Vijay</forenames></author><author><keyname>Hancock</keyname><forenames>Braden</forenames></author><author><keyname>Michaleas</keyname><forenames>Peter</forenames></author><author><keyname>Michel</keyname><forenames>Elizabeth</forenames></author><author><keyname>Varia</keyname><forenames>Mayank</forenames></author></authors><title>Parallel Vectorized Algebraic AES in MATLAB for Rapid Prototyping of
  Encrypted Sensor Processing Algorithms and Database Analytics</title><categories>cs.CR cs.DS</categories><comments>6 pages; accepted to IEEE High Performance Extreme Computing
  Conference (HPEC) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing use of networked sensor systems and networked databases has
led to an increased interest in incorporating encryption directly into sensor
algorithms and database analytics. MATLAB is the dominant tool for rapid
prototyping of sensor algorithms and has extensive database analytics
capabilities. The advent of high level and high performance Galois Field
mathematical environments allows encryption algorithms to be expressed
succinctly and efficiently. This work leverages the Galois Field primitives
found the MATLAB Communication Toolbox to implement a mode of the Advanced
Encrypted Standard (AES) based on first principals mathematics. The resulting
implementation requires 100x less code than standard AES implementations and
delivers speed that is effective for many design purposes. The parallel version
achieves speed comparable to native OpenSSL on a single node and is sufficient
for real-time prototyping of many sensor processing algorithms and database
analytics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08505</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08505</id><created>2015-06-29</created><authors><author><keyname>Hubbell</keyname><forenames>Matthew</forenames></author><author><keyname>Moran</keyname><forenames>Andrew</forenames></author><author><keyname>Arcand</keyname><forenames>William</forenames></author><author><keyname>Bestor</keyname><forenames>David</forenames></author><author><keyname>Bergeron</keyname><forenames>Bill</forenames></author><author><keyname>Byun</keyname><forenames>Chansup</forenames></author><author><keyname>Gadepally</keyname><forenames>Vijay</forenames></author><author><keyname>Michaleas</keyname><forenames>Peter</forenames></author><author><keyname>Mullen</keyname><forenames>Julie</forenames></author><author><keyname>Prout</keyname><forenames>Andrew</forenames></author><author><keyname>Reuther</keyname><forenames>Albert</forenames></author><author><keyname>Rosa</keyname><forenames>Antonio</forenames></author><author><keyname>Yee</keyname><forenames>Charles</forenames></author><author><keyname>Kepner</keyname><forenames>Jeremy</forenames></author></authors><title>Big Data Strategies for Data Center Infrastructure Management Using a 3D
  Gaming Platform</title><categories>cs.DC</categories><comments>6 pages; accepted to IEEE High Peformance Extreme Computing (HPEC)
  conference 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High Performance Computing (HPC) is intrinsically linked to effective Data
Center Infrastructure Management (DCIM). Cloud services and HPC have become key
components in Department of Defense and corporate Information Technology
competitive strategies in the global and commercial spaces. As a result, the
reliance on consistent, reliable Data Center space is more critical than ever.
The costs and complexity of providing quality DCIM are constantly being tested
and evaluated by the United States Government and companies such as Google,
Microsoft and Facebook. This paper will demonstrate a system where Big Data
strategies and 3D gaming technology is leveraged to successfully monitor and
analyze multiple HPC systems and a lights-out modular HP EcoPOD 240a Data
Center on a singular platform. Big Data technology and a 3D gaming platform
enables the relative real time monitoring of 5000 environmental sensors, more
than 3500 IT data points and display visual analytics of the overall operating
condition of the Data Center from a command center over 100 miles away. In
addition, the Big Data model allows for in depth analysis of historical trends
and conditions to optimize operations achieving even greater efficiencies and
reliability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08506</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08506</id><created>2015-06-29</created><authors><author><keyname>Prout</keyname><forenames>Andrew</forenames></author><author><keyname>Kepner</keyname><forenames>Jeremy</forenames></author><author><keyname>Michaleas</keyname><forenames>Peter</forenames></author><author><keyname>Arcand</keyname><forenames>William</forenames></author><author><keyname>Bestor</keyname><forenames>David</forenames></author><author><keyname>Bergeron</keyname><forenames>Bill</forenames></author><author><keyname>Byun</keyname><forenames>Chansup</forenames></author><author><keyname>Edwards</keyname><forenames>Lauren</forenames></author><author><keyname>Gadepally</keyname><forenames>Vijay</forenames></author><author><keyname>Hubbell</keyname><forenames>Matthew</forenames></author><author><keyname>Mullen</keyname><forenames>Julie</forenames></author><author><keyname>Rosa</keyname><forenames>Antonio</forenames></author><author><keyname>Yee</keyname><forenames>Charles</forenames></author><author><keyname>Reuther</keyname><forenames>Albert</forenames></author></authors><title>Enabling On-Demand Database Computing with MIT SuperCloud Database
  Management System</title><categories>cs.DB cs.DC</categories><comments>6 pages; accepted to IEEE High Performance Extreme Computing (HPEC)
  conference 2015. arXiv admin note: text overlap with arXiv:1406.4923</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The MIT SuperCloud database management system allows for rapid creation and
flexible execution of a variety of the latest scientific databases, including
Apache Accumulo and SciDB. It is designed to permit these databases to run on a
High Performance Computing Cluster (HPCC) platform as seamlessly as any other
HPCC job. It ensures the seamless migration of the databases to the resources
assigned by the HPCC scheduler and centralized storage of the database files
when not running. It also permits snapshotting of databases to allow
researchers to experiment and push the limits of the technology without
concerns for data or productivity loss if the database becomes unstable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08518</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08518</id><created>2015-06-29</created><updated>2015-12-22</updated><authors><author><keyname>Fici</keyname><forenames>Gabriele</forenames></author><author><keyname>Kociumaka</keyname><forenames>Tomasz</forenames></author><author><keyname>Lecroq</keyname><forenames>Thierry</forenames></author><author><keyname>Lefebvre</keyname><forenames>Arnaud</forenames></author><author><keyname>Prieur-Gaston</keyname><forenames>Elise</forenames></author></authors><title>Fast Computation of Abelian Runs</title><categories>cs.DS</categories><comments>To appear in Theoretical Computer Science</comments><doi>10.1016/j.tcs.2015.12.010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a word $w$ and a Parikh vector $\mathcal{P}$, an abelian run of period
$\mathcal{P}$ in $w$ is a maximal occurrence of a substring of $w$ having
abelian period $\mathcal{P}$. Our main result is an online algorithm that,
given a word $w$ of length $n$ over an alphabet of cardinality $\sigma$ and a
Parikh vector $\mathcal{P}$, returns all the abelian runs of period
$\mathcal{P}$ in $w$ in time $O(n)$ and space $O(\sigma+p)$, where $p$ is the
norm of $\mathcal{P}$, i.e., the sum of its components. We also present an
online algorithm that computes all the abelian runs with periods of norm $p$ in
$w$ in time $O(np)$, for any given norm $p$. Finally, we give an $O(n^2)$-time
offline randomized algorithm for computing all the abelian runs of $w$. Its
deterministic counterpart runs in $O(n^2\log\sigma)$ time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08527</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08527</id><created>2015-06-29</created><updated>2015-08-17</updated><authors><author><keyname>Koutschan</keyname><forenames>Christoph</forenames></author><author><keyname>Ranetbauer</keyname><forenames>Helene</forenames></author><author><keyname>Regensburger</keyname><forenames>Georg</forenames></author><author><keyname>Wolfram</keyname><forenames>Marie-Therese</forenames></author></authors><title>Symbolic Derivation of Mean-Field PDEs from Lattice-Based Models</title><categories>cs.SC cs.CE math.AP nlin.AO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transportation processes, which play a prominent role in the life and social
sciences, are typically described by discrete models on lattices. For studying
their dynamics a continuous formulation of the problem via partial differential
equations (PDE) is employed. In this paper we propose a symbolic computation
approach to derive mean-field PDEs from a lattice-based model. We start with
the microscopic equations, which state the probability to find a particle at a
given lattice site. Then the PDEs are formally derived by Taylor expansions of
the probability densities and by passing to an appropriate limit as the time
steps and the distances between lattice sites tend to zero. We present an
implementation in a computer algebra system that performs this transition for a
general class of models. In order to rewrite the mean-field PDEs in a
conservative formulation, we adapt and implement symbolic integration methods
that can handle unspecified functions in several variables. To illustrate our
approach, we consider an application in crowd motion analysis where the
dynamics of bidirectional flows are studied. However, the presented approach
can be applied to various transportation processes of multiple species with
variable size in any dimension, for example, to confirm several proposed
mean-field models for cell motility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08529</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08529</id><created>2015-06-29</created><authors><author><keyname>Elhoseiny</keyname><forenames>Mohamed</forenames></author><author><keyname>Elgammal</keyname><forenames>Ahmed</forenames></author><author><keyname>Saleh</keyname><forenames>Babak</forenames></author></authors><title>Tell and Predict: Kernel Classifier Prediction for Unseen Visual Classes
  from Unstructured Text Descriptions</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a framework for predicting kernelized classifiers in
the visual domain for categories with no training images where the knowledge
comes from textual description about these categories. Through our optimization
framework, the proposed approach is capable of embedding the class-level
knowledge from the text domain as kernel classifiers in the visual domain. We
also proposed a distributional semantic kernel between text descriptions which
is shown to be effective in our setting. The proposed framework is not
restricted to textual descriptions, and can also be applied to other forms
knowledge representations. Our approach was applied for the challenging task of
zero-shot learning of fine-grained categories from text descriptions of these
categories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08535</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08535</id><created>2015-06-29</created><authors><author><keyname>Vitanov</keyname><forenames>Nikolay K.</forenames></author><author><keyname>Ausloos</keyname><forenames>Marcel</forenames></author></authors><title>Test of two hypotheses explaining the size of populations in a system of
  cities</title><categories>physics.soc-ph cs.SI stat.AP</categories><comments>13 pages; 4 figures, 1 Table; 25 references; prepared for Journal of
  Applied Statistics</comments><journal-ref>J. Appl. Stat. 42 (12) 2686-2693, 2015</journal-ref><doi>10.1080/02664763.2015.1047744</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two classical hypotheses are examined about the population growth in a system
of cities: Hypothesis 1 pertains to Gibrat's and Zipf's theory which states
that the city growth-decay process is size independent; Hypothesis 2 pertains
to the so called Yule process which states that the growth of populations in
cities happens when (i) the distribution of the city population initial size
obeys a log-normal function, (ii) the growth of the settlements follows a
stochastic process. The basis for the test is some official data on Bulgarian
cities at various times. This system was chosen because (i) Bulgaria is a
country for which one does not expect biased theoretical conditions; (ii) the
city populations were determined rather precisely. The present results show
that: (i) the population size growth of the Bulgarian cities is size dependent,
whence Hypothesis 1 is not confirmed for Bulgaria; (ii) the population size
growth of Bulgarian cities can be described by a double Pareto log-normal
distribution, whence Hypothesis 2 is valid for the Bulgarian city system. It is
expected that this fine study brings some information and light on other,
usually considered to be more pertinent, city systems in various countries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08536</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08536</id><created>2015-06-29</created><updated>2016-01-13</updated><authors><author><keyname>Citi</keyname><forenames>Luca</forenames></author></authors><title>A simple yet efficient algorithm for multiple kernel learning under
  elastic-net constraints</title><categories>stat.ML cs.LG</categories><comments>11 pages, no figures, updated version of technical report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report presents an algorithm for the solution of multiple kernel
learning (MKL) problems with elastic-net constraints on the kernel weights.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08538</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08538</id><created>2015-06-29</created><authors><author><keyname>Raha</keyname><forenames>Rajorshee</forenames></author><author><keyname>Dey</keyname><forenames>Soumyajit</forenames></author><author><keyname>Chakrabarti</keyname><forenames>Partha Pratim</forenames></author><author><keyname>Dasgupta</keyname><forenames>Pallab</forenames></author></authors><title>Multi-mode Sampling Period Selection for Embedded Real Time Control</title><categories>cs.SY cs.ET</categories><comments>Work in Progress Poster, 51st Design Automation Conference (DAC), San
  Francisco, CA, 7-11, June 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent studies have shown that adaptively regulating the sampling rate
results in significant reduction in computational resources in embedded
software based control. Selecting a uniform sampling rate for a control loop is
robust, but overtly pessimistic for sharing processors among multiple control
loops. Fine grained regulation of periodicity achieves better resource
utilization, but is hard to implement online in a robust way. In this paper we
propose multi-mode sampling period selection, derived from an offline control
theoretic analysis of the system. We report significant gains in computational
efficiency without trading off control performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08544</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08544</id><created>2015-06-29</created><authors><author><keyname>Peyrard</keyname><forenames>Nathalie</forenames></author><author><keyname>de Givry</keyname><forenames>Simon</forenames></author><author><keyname>Franc</keyname><forenames>Alain</forenames></author><author><keyname>Robin</keyname><forenames>St&#xe9;phane</forenames></author><author><keyname>Sabbadin</keyname><forenames>R&#xe9;gis</forenames></author><author><keyname>Schiex</keyname><forenames>Thomas</forenames></author><author><keyname>Vignes</keyname><forenames>Matthieu</forenames></author></authors><title>Exact and approximate inference in graphical models: variable
  elimination and beyond</title><categories>stat.ML cs.AI cs.LG</categories><comments>2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic graphical models offer a powerful framework to account for the
dependence structure between variables, which can be represented as a graph.
The dependence between variables may render inference tasks such as computing
normalizing constant, marginalization or optimization intractable. The
objective of this paper is to review techniques exploiting the graph structure
for exact inference borrowed from optimization and computer science. They are
not yet standard in the statistician toolkit, and we specify under which
conditions they are efficient in practice. They are built on the principle of
variable elimination whose complexity is dictated in an intricate way by the
order in which variables are eliminated in the graph. The so-called treewidth
of the graph characterizes this algorithmic complexity: low-treewidth graphs
can be processed efficiently. Algorithmic solutions derived from variable
elimination and the notion of treewidth are illustrated on problems of
treewidth computation and inference in challenging benchmarks from optimization
competitions. We also review how efficient techniques for approximate inference
such as loopy belief propagation and variational approaches can be linked to
variable elimination and we illustrate them in the context of
Expectation-Maximisation procedures for parameter estimation in coupled Hidden
Markov Models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08546</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08546</id><created>2015-06-29</created><authors><author><keyname>Miele</keyname><forenames>Andrea</forenames></author></authors><title>Buffer overflow vulnerabilities in CUDA: a preliminary analysis</title><categories>cs.CR</categories><comments>12 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a preliminary study of buffer overflow vulnerabilities in CUDA
software running on GPUs. We show how an attacker can overrun a buffer to
corrupt sensitive data or steer the execution flow by overwriting function
pointers, e.g., manipulating the virtual table of a C++ object. In view of a
potential mass market diffusion of GPU accelerated software this may be a major
concern.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08547</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08547</id><created>2015-06-29</created><updated>2015-11-10</updated><authors><author><keyname>Kolmogorov</keyname><forenames>Vladimir</forenames></author></authors><title>Commutativity in the Algorithmic Lovasz Local Lemma</title><categories>cs.DS</categories><comments>Minor changes in the presentation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the recent formulation of the Algorithmic Lov\'asz Local Lemma
[9,2] for finding objects that avoid &quot;bad features&quot;, or &quot;flaws&quot;. It extends the
Moser-Tardos resampling algorithm [15] to more general discrete spaces. At each
step the method picks a flaw present in the current state and &quot;resamples&quot; it
using a &quot;resampling oracle&quot; provided by the user. However, it is less flexible
than the Moser-Tardos method since [9,2] require a specific flaw selection
rule, whereas [15] allows an arbitrary rule (and thus can potentially be
implemented more efficiently).
  We formulate a new &quot;commutativity&quot; condition, and prove that it is sufficient
for an arbitrary rule to work. It also enables an efficient parallelization
under an additional assumption. We then show that existing resampling oracles
for perfect matchings and permutations do satisfy this condition.
  Finally, we generalize the precondition in [2] (in the case of symmetric
potential causality graphs). This unifies special cases that previously were
treated separately.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08548</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08548</id><created>2015-06-29</created><authors><author><keyname>Zhang</keyname><forenames>Lei</forenames></author><author><keyname>Wu</keyname><forenames>Qianhong</forenames></author><author><keyname>Domingo-Ferrer</keyname><forenames>Josep</forenames></author><author><keyname>Qin</keyname><forenames>Bo</forenames></author><author><keyname>Hu</keyname><forenames>Chuanyan</forenames></author></authors><title>On the Security of MTA-OTIBASs (Multiple-TA One-Time Identity-Based
  Aggregate Signatures)</title><categories>cs.CR</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In [3] the authors proposed a new aggregate signature scheme referred to as
multiple-TA (trusted authority) one-time identity-based aggregate signature
(MTA-OTIBAS). Further, they gave a concrete MTA-OTIBAS scheme. We recall here
the definition of MTA-OTIBAS and the concrete proposed scheme. Then we prove
that our MTA-OTIBAS concrete scheme is existentially unforgeable against
adaptively chosen-message attacks in the random oracle model under the co-CDH
problem assumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08563</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08563</id><created>2015-06-29</created><updated>2015-09-15</updated><authors><author><keyname>Egly</keyname><forenames>Uwe</forenames></author><author><keyname>Lonsing</keyname><forenames>Florian</forenames></author><author><keyname>Oetsch</keyname><forenames>Johannes</forenames></author></authors><title>Automated Benchmarking of Incremental SAT and QBF Solvers</title><categories>cs.LO</categories><comments>camera-ready version (8 pages + 2 pages appendix), to appear in the
  proceedings of the 20th International Conference on Logic for Programming,
  Artificial Intelligence and Reasoning (LPAR), LNCS, Springer, 2015</comments><doi>10.1007/978-3-662-48899-7_13</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Incremental SAT and QBF solving potentially yields improvements when
sequences of related formulas are solved. An incremental application is usually
tailored towards some specific solver and decomposes a problem into incremental
solver calls. This hinders the independent comparison of different solvers,
particularly when the application program is not available. As a remedy, we
present an approach to automated benchmarking of incremental SAT and QBF
solvers. Given a collection of formulas in (Q)DIMACS format generated
incrementally by an application program, our approach automatically translates
the formulas into instructions to import and solve a formula by an incremental
SAT/QBF solver. The result of the translation is a program which replays the
incremental solver calls and thus allows to evaluate incremental solvers
independently from the application program. We illustrate our approach by
different hardware verification problems for SAT and QBF solvers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08565</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08565</id><created>2015-06-29</created><updated>2016-01-20</updated><authors><author><keyname>Vogt</keyname><forenames>Hendrik</forenames></author><author><keyname>Awan</keyname><forenames>Zohaib Hassan</forenames></author><author><keyname>Sezgin</keyname><forenames>Aydin</forenames></author></authors><title>Full-Duplex vs. Half-Duplex Secret-Key Generation</title><categories>cs.IT cs.CR math.IT</categories><comments>Extended version, submitted to IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Full-duplex (FD) communication is regarded as a key technology in future 5G
and Internet of Things (IoT) systems. In addition to high data rate
constraints, the success of these systems depends on the ability to allow for
confidentiality and security. Secret-key agreement from reciprocal wireless
channels can be regarded as a valuable supplement for security at the physical
layer. In this work, we study the role of FD communication in conjunction with
secret-key agreement. We first introduce two complementary key generation
models for FD and half-duplex (HD) settings and compare the performance by
introducing the key-reconciliation function. Furthermore, we study the impact
of the so called probing-reconciliation trade-off, the role of a strong
eavesdropper and analyze the system in the high SNR regime. We show that under
certain conditions, the FD mode enforces a deteriorating impact on the
capabilities of the eavesdropper and offers several advantages in terms of
secret-key rate over the conventional HD setups. Our analysis reveals as an
interesting insight that perfect self-interference cancellation is not
necessary in order to obtain performance gains over the HD mode.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08578</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08578</id><created>2015-06-29</created><authors><author><keyname>Gao</keyname><forenames>Xiaoyan</forenames></author></authors><title>Joint Source-Channel Coding for Real-Time Video Transmission to
  Multi-homed Mobile Terminals</title><categories>cs.NI</categories><comments>5 pages. arXiv admin note: text overlap with arXiv:1406.7054 by other
  authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study focuses on the mobile video delivery from a video server to a
multi-homed client with a network of heterogeneous wireless. Joint
Source-Channel Coding is effectively used to transmit video over
bandwidth-limited, noisy wireless networks. But most existing JSCC methods only
consider single path video transmission of the server and the client network.
The problem will become more complicated when consider multi-path video
transmission, because involving low-bandwidth, high-drop-rate or high-latency
wireless network will only reduce the video quality. To solve this critical
problem, we propose a novel Path Adaption JSCC (PA-JSCC) method that contain
below characters: (1) path adaption, and (2) dynamic rate allocation. We use
Exata to evaluate the performance of PA-JSCC and Experiment show that PA-JSCC
has a good results in terms of PSNR (Peak Signal-to-Noise Ratio).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08581</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08581</id><created>2015-06-29</created><authors><author><keyname>Makantasis</keyname><forenames>Konstantinos</forenames></author><author><keyname>Doulamis</keyname><forenames>Anastasios</forenames></author><author><keyname>Doulamis</keyname><forenames>Nikolaos</forenames></author></authors><title>Variational Inference for Background Subtraction in Infrared Imagery</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a Gaussian mixture model for background subtraction in infrared
imagery. Following a Bayesian approach, our method automatically estimates the
number of Gaussian components as well as their parameters, while simultaneously
it avoids over/under fitting. The equations for estimating model parameters are
analytically derived and thus our method does not require any sampling
algorithm that is computationally and memory inefficient. The pixel density
estimate is followed by an efficient and highly accurate updating mechanism,
which permits our system to be automatically adapted to dynamically changing
operation conditions. Experimental results and comparisons with other methods
show that our method outperforms, in terms of precision and recall, while at
the same time it keeps computational cost suitable for real-time applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08592</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08592</id><created>2015-06-29</created><authors><author><keyname>Boyar</keyname><forenames>Joan</forenames></author><author><keyname>Kudahl</keyname><forenames>Christian</forenames></author></authors><title>Adding Isolated Vertices Makes some Online Algorithms Optimal</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An unexpected difference between online and offline algorithms is observed.
The natural greedy algorithms are shown to be worst case online optimal for
Online Independent Set and Online Vertex Cover on graphs with 'enough' isolated
vertices, Freckle Graphs. For Online Dominating Set, the greedy algorithm is
shown to be worst case online optimal on graphs with at least one isolated
vertex. These algorithms are not online optimal in general. The online
optimality results for these greedy algorithms imply optimality according to
various worst case performance measures, such as the competitive ratio. It is
also shown that, despite this worst case optimality, there are Freckle graphs
where the greedy independent set algorithm is objectively less good than
another algorithm. It is shown that it is NP-hard to determine any of the
following for a given graph: the online independence number, the online vertex
cover number, and the online domination number.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08603</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08603</id><created>2015-06-29</created><authors><author><keyname>Carbone</keyname><forenames>Paris</forenames></author><author><keyname>F&#xf3;ra</keyname><forenames>Gyula</forenames></author><author><keyname>Ewen</keyname><forenames>Stephan</forenames></author><author><keyname>Haridi</keyname><forenames>Seif</forenames></author><author><keyname>Tzoumas</keyname><forenames>Kostas</forenames></author></authors><title>Lightweight Asynchronous Snapshots for Distributed Dataflows</title><categories>cs.DC</categories><comments>8 pages, 7 figures</comments><report-no>ISBN 978-91-7595-651-0</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed stateful stream processing enables the deployment and execution
of large scale continuous computations in the cloud, targeting both low latency
and high throughput. One of the most fundamental challenges of this paradigm is
providing processing guarantees under potential failures. Existing approaches
rely on periodic global state snapshots that can be used for failure recovery.
Those approaches suffer from two main drawbacks. First, they often stall the
overall computation which impacts ingestion. Second, they eagerly persist all
records in transit along with the operation states which results in larger
snapshots than required. In this work we propose Asynchronous Barrier
Snapshotting (ABS), a lightweight algorithm suited for modern dataflow
execution engines that minimises space requirements. ABS persists only operator
states on acyclic execution topologies while keeping a minimal record log on
cyclic dataflows. We implemented ABS on Apache Flink, a distributed analytics
engine that supports stateful stream processing. Our evaluation shows that our
algorithm does not have a heavy impact on the execution, maintaining linear
scalability and performing well with frequent snapshots.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08606</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08606</id><created>2015-06-29</created><updated>2015-07-23</updated><authors><author><keyname>Bhargav</keyname><forenames>Nidhi</forenames></author><author><keyname>Cotton</keyname><forenames>Simon L.</forenames></author><author><keyname>Simmons</keyname><forenames>David E.</forenames></author></authors><title>Secrecy Capacity Analysis over $\kappa-\mu$ Fading Channels: Theory and
  Applications</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the transmission of confidential information over
a $\kappa$-$\mu$ fading channel in the presence of an eavesdropper, who also
observes $\kappa$-$\mu$ fading. In particular, we obtain novel analytical
solutions for the probability of strictly positive secrecy capacity (SPSC) and
the lower bound of secure outage probability (SOP$^L$) for channel coefficients
that are positive, real, independent and non-identically distributed
($i.n.i.d.$). We also provide a closed-form expression for the probability of
SPSC when the $\mu$ parameter is assumed to only take positive integer values.
We then apply the derived results to assess the secrecy performance of the
system in terms of the average signal-to-noise ratio (SNR) as a function of the
$\kappa$ and $\mu$ fading parameters. We observed that for fixed values of the
eavesdropper's average SNR, increases in the average SNR of the main channel
produce a higher probability of SPSC and a lower secure outage probability
(SOP). It was also found that when the main channel experiences a higher
average SNR than the eavesdropper's channel, the probability of SPSC improved
while the SOP was found to decrease with increasing values of $\kappa$ and
$\mu$ for the legitimate channel. The versatility of the $\kappa$-$\mu$ fading
model, means that the results presented in this paper can be used to determine
the probability of SPSC and SOP$^L$ for a large number of other fading
scenarios such as Rayleigh, Rice (Nakagami-$n$), Nakagami-$m$, One-Sided
Gaussian and mixtures of these common fading models. Additionally, due to the
duality of the analysis of secrecy capacity and co-channel interference, the
results presented here will also have immediate applicability in the analysis
of outage probability in wireless systems affected by co-channel interference
and background noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08612</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08612</id><created>2015-06-29</created><authors><author><keyname>Memeti</keyname><forenames>Suejb</forenames></author><author><keyname>Pllana</keyname><forenames>Sabri</forenames></author></authors><title>Accelerating DNA Sequence Analysis using Intel Xeon Phi</title><categories>cs.DC</categories><comments>PBio at ISPA-2015, Helsinki, Finland, 20-22 August, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Genetic information is increasing exponentially, doubling every 18 months.
Analyzing this information within a reasonable amount of time requires parallel
computing resources. While considerable research has addressed DNA analysis
using GPUs, so far not much attention has been paid to the Intel Xeon Phi
coprocessor. In this paper we present an algorithm for large-scale DNA analysis
that exploits thread-level and the SIMD parallelism of the Intel Xeon Phi. We
evaluate our approach for various numbers of cores and thread allocation
affinities in the context of real-world DNA sequences of mouse, cat, dog,
chicken, human and turkey. The experimental results on Intel Xeon Phi show
speed-ups of up to 10x compared to a sequential implementation running on an
Intel Xeon processor E5.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08615</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08615</id><created>2015-06-23</created><authors><author><keyname>Ciak</keyname><forenames>Ren&#xe9;</forenames></author></authors><title>Coercive functions from a topological viewpoint and properties of
  minimizing sets of convex functions appearing in image restoration</title><categories>math.OC cs.CV math.CA math.FA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many tasks in image processing can be tackled by modeling an appropriate data
fidelity term $\Phi: \mathbb{R}^n \rightarrow \mathbb{R} \cup \{+\infty\}$ and
then solve one of the regularized minimization problems \begin{align*}
  &amp;{}(P_{1,\tau}) \qquad \mathop{\rm argmin}_{x \in \mathbb R^n} \big\{ \Phi(x)
\;{\rm s.t.}\; \Psi(x) \leq \tau \big\} \\ &amp;{}(P_{2,\lambda}) \qquad
\mathop{\rm argmin}_{x \in \mathbb R^n} \{ \Phi(x) + \lambda \Psi(x) \}, \;
\lambda &gt; 0 \end{align*} with some function $\Psi: \mathbb{R}^n \rightarrow
\mathbb{R} \cup \{+\infty\}$ and a good choice of the parameter(s). Two tasks
arise naturally here: \begin{align*} {}&amp; \text{1. Study the solver sets ${\rm
SOL}(P_{1,\tau})$ and
  ${\rm SOL}(P_{2,\lambda})$ of the minimization problems.} \\ {}&amp; \text{2.
Ensure that the minimization problems have solutions.} \end{align*} This thesis
provides contributions to both tasks: Regarding the first task for a more
special setting we prove that there are intervals $(0,c)$ and $(0,d)$ such that
the setvalued curves \begin{align*}
  \tau \mapsto {}&amp; {\rm SOL}(P_{1,\tau}), \; \tau \in (0,c) \\ {} \lambda
\mapsto {}&amp; {\rm SOL}(P_{2,\lambda}), \; \lambda \in (0,d) \end{align*} are the
same, besides an order reversing parameter change $g: (0,c) \rightarrow (0,d)$.
Moreover we show that the solver sets are changing all the time while $\tau$
runs from $0$ to $c$ and $\lambda$ runs from $d$ to $0$.
  In the presence of lower semicontinuity the second task is done if we have
additionally coercivity. We regard lower semicontinuity and coercivity from a
topological point of view and develop a new technique for proving lower
semicontinuity plus coercivity.
  Dropping any lower semicontinuity assumption we also prove a theorem on the
coercivity of a sum of functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08620</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08620</id><created>2015-06-29</created><authors><author><keyname>Cannizzo</keyname><forenames>Fabio</forenames></author></authors><title>Fast and Vectorizable Alternatives to Binary Search</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an array X of N+1 strictly ordered floating point numbers and an array
Z of M floating point numbers belonging to the interval [X[0],X[N]), a common
problem in numerical methods algorithms is to find the indices of the largest
numbers in the array X which are smaller or equal than the numbers in the array
Z. The general solution to this problem is the well known &quot;binary search&quot;
algorithm, which has complexity O(M log2 N). This paper describes improvements
to the binary search algorithm, which are faster and vectorizable
(SIMD-friendly). Next it proposes a new vectorizable algorithm based on a
indexing technique, which reduces complexity of search operations to O(M). Some
benchmark test results using SSE2 instructions demonstrate that with N=1025 the
proposed algorithm is about 26 times faster than the classical binary search in
single precision and 15 times faster in double precision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08623</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08623</id><created>2015-06-29</created><authors><author><keyname>Cotton</keyname><forenames>Simon L.</forenames></author></authors><title>Second-Order Statistics of $\kappa-\mu$ Shadowed Fading Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, novel closed-form expressions for the level crossing rate
(LCR) and average fade duration (AFD) of $\kappa-\mu$ shadowed fading channels
are derived. The new equations provide the capability of modeling the
correlation between the time derivative of the shadowed dominant and multipath
components of the $\kappa-\mu$ shadowed fading envelope. Verification of the
new equations is performed by reduction to a number of known special cases. It
is shown that as the shadowing of the resultant dominant component decreases,
the signal crosses lower threshold levels at a reduced rate. Furthermore, the
impact of increasing correlation between the slope of the shadowed dominant and
multipath components similarly acts to reduce crossings at lower signal levels.
The new expressions for the second-order statistics are also compared with
field measurements obtained for cellular device-to-device and body centric
communications channels which are known to be susceptible to shadowed fading.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08637</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08637</id><created>2015-06-29</created><authors><author><keyname>Costa</keyname><forenames>Maice</forenames></author><author><keyname>Codreanu</keyname><forenames>Marian</forenames></author><author><keyname>Ephremides</keyname><forenames>Anthony</forenames></author></authors><title>On The Age Of Information In Status Update Systems With Packet
  Management</title><categories>cs.IT cs.NI math.IT</categories><comments>20 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a communication system in which status updates arrive at a source
node, and should be transmitted through a network to the intended destination
node. The status updates are samples of a random process under observation,
transmitted as packets, which also contain the time stamp to identify when the
sample was generated. The age of the information available to the destination
node is the time elapsed since the last received update was generated. In this
paper, we model the source-destination link using queuing theory, and we assume
that the time it takes to successfully transmit a packet to the destination is
an exponentially distributed service time. We analyze the age of information in
the case that the source node has the capability to manage the arriving
samples, possibly discarding packets in order to avoid wasting network
resources with the transmission of stale information. In addition to
characterizing the average age, we propose a new metric, called peak age, which
provides information about the maximum value of the age, achieved immediately
before receiving an update.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08641</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08641</id><created>2015-06-25</created><authors><author><keyname>Koc</keyname><forenames>Yakup</forenames></author><author><keyname>Raman</keyname><forenames>Abhishek</forenames></author><author><keyname>Warnier</keyname><forenames>Martijn</forenames></author><author><keyname>Kumar</keyname><forenames>Tarun</forenames></author></authors><title>Structural Vulnerability Analysis of Electric Power Distribution Grids</title><categories>cs.SY physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Power grid outages cause huge economical and societal costs. Disruptions in
the power distribution grid are responsible for a significant fraction of
electric power unavailability to customers. The impact of extreme weather
conditions, continuously increasing demand, and the over-ageing of assets in
the grid, deteriorates the safety of electric power delivery in the near
future. It is this dependence on electric power that necessitates further
research in the power distribution grid security assessment. Thus measures to
analyze the robustness characteristics and to identify vulnerabilities as they
exist in the grid are of utmost importance. This research investigates exactly
those concepts- the vulnerability and robustness of power distribution grids
from a topological point of view, and proposes a metric to quantify them with
respect to assets in a distribution grid. Real-world data is used to
demonstrate the applicability of the proposed metric as a tool to assess the
criticality of assets in a distribution grid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08643</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08643</id><created>2015-06-29</created><updated>2015-09-07</updated><authors><author><keyname>Tilles</keyname><forenames>Paulo F. C.</forenames></author><author><keyname>Fontanari</keyname><forenames>Jos&#xe9; F.</forenames></author></authors><title>Diffusion of innovations in Axelrod's model</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><doi>10.1088/1742-5468/2015/11/P11026</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Axelrod's model for the dissemination of culture contains two key factors
required to model the process of diffusion of innovations, namely, social
influence (i.e., individuals become more similar when they interact) and
homophily (i.e., individuals interact preferentially with similar others). The
strength of these social influences are controlled by two parameters: $F$, the
number of features that characterizes the cultures and $q$, the common number
of states each feature can assume. Here we assume that the innovation is a new
state of a cultural feature of a single individual -- the innovator -- and
study how the innovation spreads through the networks among the individuals.
For infinite regular lattices in one (1D) and two dimensions (2D), we find that
initially the successful innovation spreads linearly with the time $t$, but in
the long-time limit it spreads diffusively ($\sim t^{1/2}$) in 1D and
sub-diffusively ($\sim t/\ln t$) in 2D. For finite lattices, the growth curves
for the number of adopters are typically concave functions of $t$. For random
graphs with a finite number of nodes $N$, we argue that the classical S-shaped
growth curves result from a trade-off between the average connectivity $K$ of
the graph and the per feature diversity $q$. A large $q$ is needed to reduce
the pace of the initial spreading of the innovation and thus delimit the
early-adopters stage, whereas a large $K$ is necessary to ensure the onset of
the take-off stage at which the number of adopters grows superlinearly with
$t$. In an infinite random graph we find that the number of adopters of a
successful innovation scales with $t^\gamma$ with $\gamma =1$ for $K&gt; 2$ and
$1/2 &lt; \gamma &lt; 1$ for $K=2$. We suggest that the exponent $\gamma$ may be a
useful index to characterize the process of diffusion of successful innovations
in diverse scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08658</identifier>
 <datestamp>2015-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08658</id><created>2015-06-29</created><updated>2015-11-19</updated><authors><author><keyname>Viaud</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author><author><keyname>Bertet</keyname><forenames>Karell</forenames></author><author><keyname>Demko</keyname><forenames>Christophe</forenames></author><author><keyname>Missaoui</keyname><forenames>Rokia</forenames></author></authors><title>Lattice decompositions through methods using congruence relations</title><categories>cs.DM</categories><comments>This paper has been withdrawn since it is only a draft paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known by analysts that a concept lattice has an exponential size
in the data. Thus, as soon as he works with real data, the size of the concept
lattice is a fundamental problem. In this chapter, we propose to investigate
factor lattices as a tool to get meaningful parts of the whole lattice. These
factor lattices have been widely studied from the early theory of lattices to
more recent work in the FCA field. This chapter is divided into three parts. In
the first part, we present pieces of lattice theory and formal concept
analysis, namely compatible sub-contexts, arrow-closed sub-contexts and
congruence relations, all three notions used for the sub-direct decomposition
and the doubling convex construction used for the second decomposition, also
based on congruence relations. In the second part, the subdirect decomposition
into subdirectly irreducible factor is given, polynomial algorithms to compute
such a decomposition are given and an example is detailled to illustrate the
theory. Then in the third section, a new decomposition named &quot;revese doubling
construction&quot; is given. An example is given to explain this decomposition.
Theoretical results are given and proofs for the new ones also.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08663</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08663</id><created>2015-06-14</created><authors><author><keyname>Piattelli-Palmarini</keyname><forenames>Massimo</forenames></author><author><keyname>Vitiello</keyname><forenames>Giuseppe</forenames></author></authors><title>Linguistics and some aspects of its underlying dynamics</title><categories>cs.CL quant-ph</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, central components of a new approach to linguistics, the
Minimalist Program (MP) have come closer to physics. Features of the Minimalist
Program, such as the unconstrained nature of recursive Merge, the operation of
the Labeling Algorithm that only operates at the interface of Narrow Syntax
with the Conceptual-Intentional and the Sensory-Motor interfaces, the
difference between pronounced and un-pronounced copies of elements in a
sentence and the build-up of the Fibonacci sequence in the syntactic derivation
of sentence structures, are directly accessible to representation in terms of
algebraic formalism. Although in our scheme linguistic structures are classical
ones, we find that an interesting and productive isomorphism can be established
between the MP structure, algebraic structures and many-body field theory
opening new avenues of inquiry on the dynamics underlying some central aspects
of linguistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08669</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08669</id><created>2015-06-29</created><updated>2016-01-07</updated><authors><author><keyname>Huang</keyname><forenames>Tzu-Kuo</forenames></author><author><keyname>Agarwal</keyname><forenames>Alekh</forenames></author><author><keyname>Hsu</keyname><forenames>Daniel J.</forenames></author><author><keyname>Langford</keyname><forenames>John</forenames></author><author><keyname>Schapire</keyname><forenames>Robert E.</forenames></author></authors><title>Efficient and Parsimonious Agnostic Active Learning</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a new active learning algorithm for the streaming setting
satisfying three important properties: 1) It provably works for any classifier
representation and classification problem including those with severe noise. 2)
It is efficiently implementable with an ERM oracle. 3) It is more aggressive
than all previous approaches satisfying 1 and 2. To do this we create an
algorithm based on a newly defined optimization problem and analyze it. We also
conduct the first experimental analysis of all efficient agnostic active
learning algorithms, evaluating their strengths and weaknesses in different
settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08670</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08670</id><created>2015-06-29</created><authors><author><keyname>Isikdogan</keyname><forenames>F.</forenames></author><author><keyname>Bovik</keyname><forenames>A. C.</forenames></author><author><keyname>Passalacqua</keyname><forenames>P.</forenames></author></authors><title>Automatic Channel Network Extraction from Remotely Sensed Images by
  Singularity Analysis</title><categories>cs.CV</categories><comments>IEEE Geosci. Remote Sens. Lett., in review</comments><doi>10.1109/LGRS.2015.2458898</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantitative analysis of channel networks plays an important role in river
studies. To provide a quantitative representation of channel networks, we
propose a new method that extracts channels from remotely sensed images and
estimates their widths. Our fully automated method is based on a recently
proposed Multiscale Singularity Index that responds strongly to curvilinear
structures but weakly to edges. The algorithm produces a channel map, using a
single image where water and non-water pixels have contrast, such as a Landsat
near-infrared band image or a water index defined on multiple bands. The
proposed method provides a robust alternative to the procedures that are used
in remote sensing of fluvial geomorphology and makes classification and
analysis of channel networks easier. The source code of the algorithm is
available at: http://live.ece.utexas.edu/research/cne/.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08682</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08682</id><created>2015-06-29</created><authors><author><keyname>Sengupta</keyname><forenames>Dhriti</forenames></author><author><keyname>Kundu</keyname><forenames>Merina</forenames></author><author><keyname>Dastidar</keyname><forenames>Jayati Ghosh</forenames></author></authors><title>Human Shape Variation - An Efficient Implementation using Skeleton</title><categories>cs.CV</categories><journal-ref>IJACR, Volume 4, Issue 14, March 2014, pp. 145-150</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is at times important to detect human presence automatically in secure
environments. This needs a shape recognition algorithm that is robust, fast and
has low error rates. The algorithm needs to process camera images quickly to
detect any human in the range of vision, and generate alerts, especially if the
object under scrutiny is moving in certain directions. We present here a
simple, efficient and fast algorithm using skeletons of the images, and simple
features like posture and length of the object.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08689</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08689</id><created>2015-06-29</created><authors><author><keyname>Bizeray</keyname><forenames>A. M.</forenames></author><author><keyname>Zhao</keyname><forenames>S.</forenames></author><author><keyname>Duncan</keyname><forenames>S. R.</forenames></author><author><keyname>Howey</keyname><forenames>D. A.</forenames></author></authors><title>Lithium-ion battery thermal-electrochemical model-based state estimation
  using orthogonal collocation and a modified extended Kalman filter</title><categories>cs.SY</categories><comments>Submitted to the Journal of Power Sources</comments><journal-ref>Journal of Power Sources 296 (2015) 400-412</journal-ref><doi>10.1016/j.jpowsour.2015.07.019</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the state estimation of a high-fidelity spatially
resolved thermal- electrochemical lithium-ion battery model commonly referred
to as the pseudo two-dimensional model. The partial-differential algebraic
equations (PDAEs) constituting the model are spatially discretised using
Chebyshev orthogonal collocation enabling fast and accurate simulations up to
high C-rates. This implementation of the pseudo-2D model is then used in
combination with an extended Kalman filter algorithm for differential-algebraic
equations to estimate the states of the model. The state estimation algorithm
is able to rapidly recover the model states from current, voltage and
temperature measurements. Results show that the error on the state estimate
falls below 1 % in less than 200 s despite a 30 % error on battery initial
state-of-charge and additive measurement noise with 10 mV and 0.5 K standard
deviations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08690</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08690</id><created>2015-06-29</created><authors><author><keyname>Nagy</keyname><forenames>Gabor</forenames></author><author><keyname>Barta</keyname><forenames>Gergo</forenames></author><author><keyname>Henk</keyname><forenames>Tamas</forenames></author></authors><title>Portfolio optimization using local linear regression ensembles in
  RapidMiner</title><categories>q-fin.PM cs.LG stat.ML</categories><comments>RCOMM 2012: Rapidminer Community Meeting and Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we implement a Local Linear Regression Ensemble Committee
(LOLREC) to predict 1-day-ahead returns of 453 assets form the S&amp;P500. The
estimates and the historical returns of the committees are used to compute the
weights of the portfolio from the 453 stock. The proposed method outperforms
benchmark portfolio selection strategies that optimize the growth rate of the
capital. We investigate the effect of algorithm parameter m: the number of
selected stocks on achieved average annual yields. Results suggest the
algorithm's practical usefulness in everyday trading.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08691</identifier>
 <datestamp>2015-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08691</id><created>2015-06-29</created><updated>2015-11-19</updated><authors><author><keyname>Wohlbrandt</keyname><forenames>Attila</forenames></author><author><keyname>Hu</keyname><forenames>Nan</forenames></author><author><keyname>Guerin</keyname><forenames>Sebastien</forenames></author><author><keyname>Ewert</keyname><forenames>Roland</forenames></author></authors><title>Analytical reconstruction of isotropic turbulence spectra based on the
  Gaussian transform</title><categories>cs.CE physics.flu-dyn</categories><comments>Preprint, submitted to Computers &amp; Fluids</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Random Particle Mesh (RPM) method used to simulate turbulence-induced
broadband noise in several aeroacoustic applications is extended to realise
isotropic turbulence spectra. With this method turbulent fluctuations are
synthesised by filtering white noise with a Gaussian filter kernel that in turn
gives a Gaussian spectrum. The Gaussian function is smooth and its derivatives
and integrals are again Gaussian functions. The Gaussian filter is efficient
and finds wide-spread applications in stochastic signal processing. However in
many applications Gaussian spectra do not correspond to real turbulence
spectra. Thus in turbo-machines the von K\'arm\'an, Liepmann, and modified von
K\'arm\'an spectra are more realistic model spectra. In this note we
analytically derive weighting functions to realise arbitrary isotropic
solenoidal spectra using a superposition of weighted Gaussian spectra of
different length scales. The analytic weighting functions for the von
K\'arm\'an, the Liepmann, and the modified von K\'arm\'an spectra are derived
subsequently. Finally a method is proposed to discretise the problem using a
limited number of Gaussian spectra. The effectivity of this approach is
demonstrated by realising a von K\'arm\'an velocity spectrum using the RPM
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08694</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08694</id><created>2015-06-26</created><authors><author><keyname>Pereira</keyname><forenames>Jos&#xe9; C.</forenames></author><author><keyname>Lobo</keyname><forenames>Fernando G.</forenames></author></authors><title>A Java Implementation of Parameter-less Evolutionary Algorithms</title><categories>cs.MS cs.NE</categories><comments>12 pages. arXiv admin note: text overlap with arXiv:1506.07980</comments><acm-class>I.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Parameter-less Genetic Algorithm was first presented by Harik and Lobo in
1999 as an alternative to the usual trial-and-error method of finding, for each
given problem, an acceptable set-up of the parameter values of the genetic
algorithm. Since then, the same strategy has been successfully applied to
create parameter-less versions of other population-based search algorithms such
as the Extended Compact Genetic Algorithm and the Hierarchical Bayesian
Optimization Algorithm. This report describes a Java implementation,
Parameter-less Evolutionary Algorithm (P-EAJava), that integrates several
parameter-less evolutionary algorithms into a single platform. Along with a
brief description of P-EAJava, we also provide detailed instructions on how to
use it, how to implement new problems, and how to generate new parameter-less
versions of evolutionary algorithms.
  At present time, P-EAJava already includes parameter-less versions of the
Simple Genetic Algorithm, the Extended Compact Genetic Algorithm, the
Univariate Marginal Distribution Algorithm, and the Hierarchical Bayesian
Optimization Algorithm. The source and binary files of the Java implementation
of P-EAJava are available for free download at
https://github.com/JoseCPereira/2015ParameterlessEvolutionaryAlgorithmsJava.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08700</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08700</id><created>2015-06-29</created><updated>2016-01-07</updated><authors><author><keyname>Bouthillier</keyname><forenames>Xavier</forenames></author><author><keyname>Konda</keyname><forenames>Kishore</forenames></author><author><keyname>Vincent</keyname><forenames>Pascal</forenames></author><author><keyname>Memisevic</keyname><forenames>Roland</forenames></author></authors><title>Dropout as data augmentation</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dropout is typically interpreted as bagging a large number of models sharing
parameters. We show that using dropout in a network can also be interpreted as
a kind of data augmentation in the input space without domain knowledge. We
present an approach to projecting the dropout noise within a network back into
the input space, thereby generating augmented versions of the training data,
and we show that training a deterministic network on the augmented samples
yields similar results. Finally, we propose a new dropout noise scheme based on
our observations and show that it improves dropout results without adding
significant computational cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08704</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08704</id><created>2015-06-29</created><authors><author><keyname>Adhikari</keyname><forenames>Subhajit</forenames></author><author><keyname>Kar</keyname><forenames>Joydeep</forenames></author><author><keyname>Dastidar</keyname><forenames>Jayati Ghosh</forenames></author></authors><title>An automatic and efficient foreground object extraction scheme</title><categories>cs.CV</categories><journal-ref>Subhajit Adhikari, Joydeep Kar, Jayati Ghosh Dastidar, &quot;An
  automatic and efficient foreground object extraction scheme&quot;, International
  Journal of Science and Advanced Information Technology, 3 (2), 2014, pp.
  40-43</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a method to differentiate the foreground objects from the
background of a color image. Firstly a color image of any size is input for
processing. The algorithm converts it to a grayscale image. Next we apply canny
edge detector to find the boundary of the foreground object. We concentrate to
find the maximum distance between each boundary pixel column wise and row wise
and we fill the region that is bound by the edges. Thus we are able to extract
the grayscale values of pixels that are in the bounded region and convert the
grayscale image back to original color image containing only the foreground
object.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08712</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08712</id><created>2015-06-29</created><authors><author><keyname>Saha</keyname><forenames>Biswajit</forenames></author><author><keyname>Ray</keyname><forenames>Utpal Kumar</forenames></author></authors><title>Learning Programming : An Indian Perspective</title><categories>cs.CY</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While teaching introductory programming courses for over a decade in reputed
institutions we have experienced that several factors play significant role in
developing problem solving skills and program development skills in students.
There are certain types of difficulties that are encountered by the beginners.
These difficulties vary in their nature. Beginners find difficulties with the
programming language that they use, the compilers that they use and so on.
These difficulties if not overcome proves detrimental to their career as
professional engineers at a later stage. This paper focuses on the various
types of difficulties that a novice programmer faces while learning programming
and tries to find out ways to overcome them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08725</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08725</id><created>2015-06-29</created><authors><author><keyname>Sivanandan</keyname><forenames>Sandeep</forenames></author></authors><title>Fail Fast - Fail Often: Enhancing Agile Methodology using Dynamic
  Regression, Code Bisector and Code Quality in Continuous Integration (CI)</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Agile practices are receiving considerable attention from industry as an
alternative to traditional software development approaches. However, there are
a number of challenges in combining Agile [2] with Test-driven development
(TDD) [10] practices, cloud deployments, continuous integration (CI), non-stop
performance, load, security and accessibly testing. From these challenges;
Continuous Integration is a relatively an approach widely discussed and
practiced in software testing. This paper describes an approach for improved
Agile Methodology using Code Quality, Code Bisector and Dynamic Regression in
Continuous Integration. The set of tools used for this analysis, design and
development are Jenkins, Robot Framework [4], Perforce and Git.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08726</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08726</id><created>2015-06-29</created><updated>2016-02-22</updated><authors><author><keyname>Jacobs</keyname><forenames>Swen</forenames></author><author><keyname>Bloem</keyname><forenames>Roderick</forenames></author><author><keyname>Brenguier</keyname><forenames>Romain</forenames></author><author><keyname>Ehlers</keyname><forenames>R&#xfc;diger</forenames></author><author><keyname>Hell</keyname><forenames>Timotheus</forenames></author><author><keyname>K&#xf6;nighofer</keyname><forenames>Robert</forenames></author><author><keyname>P&#xe9;rez</keyname><forenames>Guillermo A.</forenames></author><author><keyname>Raskin</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author><author><keyname>Ryzhyk</keyname><forenames>Leonid</forenames></author><author><keyname>Sankur</keyname><forenames>Ocan</forenames></author><author><keyname>Seidl</keyname><forenames>Martina</forenames></author><author><keyname>Tentrup</keyname><forenames>Leander</forenames></author><author><keyname>Walker</keyname><forenames>Adam</forenames></author></authors><title>The First Reactive Synthesis Competition (SYNTCOMP 2014)</title><categories>cs.LO</categories><comments>24 pages, accepted for publication in STTT</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the reactive synthesis competition (SYNTCOMP), a long-term
effort intended to stimulate and guide advances in the design and application
of synthesis procedures for reactive systems. The first iteration of SYNTCOMP
is based on the controller synthesis problem for finite-state systems and
safety specifications. We provide an overview of this problem and existing
approaches to solve it, and report on the design and results of the first
SYNTCOMP. This includes the definition of the benchmark format, the collection
of benchmarks, the rules of the competition, and the five synthesis tools that
participated. We present and analyze the results of the competition and draw
conclusions on the state of the art. Finally, we give an outlook on future
directions of SYNTCOMP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08733</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08733</id><created>2015-06-29</created><authors><author><keyname>Wang</keyname><forenames>Junyuan</forenames></author><author><keyname>Dai</keyname><forenames>Lin</forenames></author></authors><title>Downlink Rate Analysis for Virtual-Cell based Large-Scale Distributed
  Antenna Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite substantial rate gains achieved by coordinated transmission from a
massive amount of geographically distributed antennas, the resulting
computational cost and channel measurement overhead could be unaffordable for a
large-scale distributed antenna system (DAS). A scalable signal processing
framework is therefore highly desirable, which, as recently demonstrated in
\cite{Dai_TWireless}, could be established based on the concept of virtual
cell.
  In a virtual-cell based DAS, each user chooses a few closest base-station
(BS) antennas to form its virtual cell, that is, its own serving BS antenna
set. In this paper, we focus on a downlink DAS with a large number of users and
BS antennas uniformly distributed in a certain area, and aim to study the
effect of the virtual cell size on the average user rate. Specifically, by
assuming that maximum ratio transmission (MRT) is adopted in each user's
virtual cell, the achievable ergodic rate of each user is derived as an
explicit function of the large-scale fading coefficients from all the users to
their virtual cells, and an upper-bound of the average user rate is
established, based on which a rule of thumb is developed for determining the
optimal virtual cell size to maximize the average user rate. The analysis is
further extended to consider multiple users grouped together and jointly served
by their virtual cells using zero-forcing beamforming (ZFBF). In contrast to
the no-grouping case where a small virtual cell size is preferred, it is shown
that by grouping users with overlapped virtual cells, the average user rate can
be significantly improved by increasing the virtual cell size, though at the
cost of a higher signal processing complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08752</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08752</id><created>2015-06-15</created><updated>2016-02-10</updated><authors><author><keyname>Manyam</keyname><forenames>Satyanarayana</forenames></author><author><keyname>Rathinam</keyname><forenames>Sivakumar</forenames></author></authors><title>On Tightly Bounding the Dubins Traveling Salesman's Optimum</title><categories>math.OC cs.DM cs.DS cs.RO</categories><comments>Presented at the International Symposium on Mathematical Programming,
  2015.
  https://informs.emeetingsonline.com/emeetings/formbuilder/clustersessiondtl.asp?csnno=22283&amp;mmnno=264&amp;ppnno=86444</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Dubins Traveling Salesman Problem (DTSP) has generated significant
interest over the last decade due to its occurrence in several civil and
military surveillance applications. Currently, there is no algorithm that can
find an optimal solution to the problem. In addition, relaxing the motion
constraints and solving the resulting Euclidean TSP (ETSP) provides the only
lower bound available for the problem. However, in many problem instances, the
lower bound computed by solving the ETSP is far below the cost of the feasible
solutions obtained by some well-known algorithms for the DTSP. This article
addresses this fundamental issue and presents the first systematic procedure
for developing tight lower bounds for the DTSP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08754</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08754</id><created>2015-06-29</created><updated>2015-10-06</updated><authors><author><keyname>Moran</keyname><forenames>Andrew</forenames></author><author><keyname>Gadepally</keyname><forenames>Vijay</forenames></author><author><keyname>Hubbell</keyname><forenames>Matthew</forenames></author><author><keyname>Kepner</keyname><forenames>Jeremy</forenames></author></authors><title>Improving Big Data Visual Analytics with Interactive Virtual Reality</title><categories>cs.HC cs.CY</categories><comments>6 pages, 8 figures, 2015 IEEE High Performance Extreme Computing
  Conference (HPEC '15); corrected typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For decades, the growth and volume of digital data collection has made it
challenging to digest large volumes of information and extract underlying
structure. Coined 'Big Data', massive amounts of information has quite often
been gathered inconsistently (e.g from many sources, of various forms, at
different rates, etc.). These factors impede the practices of not only
processing data, but also analyzing and displaying it in an efficient manner to
the user. Many efforts have been completed in the data mining and visual
analytics community to create effective ways to further improve analysis and
achieve the knowledge desired for better understanding. Our approach for
improved big data visual analytics is two-fold, focusing on both visualization
and interaction. Given geo-tagged information, we are exploring the benefits of
visualizing datasets in the original geospatial domain by utilizing a virtual
reality platform. After running proven analytics on the data, we intend to
represent the information in a more realistic 3D setting, where analysts can
achieve an enhanced situational awareness and rely on familiar perceptions to
draw in-depth conclusions on the dataset. In addition, developing a
human-computer interface that responds to natural user actions and inputs
creates a more intuitive environment. Tasks can be performed to manipulate the
dataset and allow users to dive deeper upon request, adhering to desired
demands and intentions. Due to the volume and popularity of social media, we
developed a 3D tool visualizing Twitter on MIT's campus for analysis. Utilizing
emerging technologies of today to create a fully immersive tool that promotes
visualization and interaction can help ease the process of understanding and
representing big data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08760</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08760</id><created>2015-06-29</created><authors><author><keyname>Dasarathy</keyname><forenames>Gautam</forenames></author><author><keyname>Nowak</keyname><forenames>Robert</forenames></author><author><keyname>Zhu</keyname><forenames>Xiaojin</forenames></author></authors><title>S2: An Efficient Graph Based Active Learning Algorithm with Application
  to Nonparametric Classification</title><categories>cs.LG stat.ML</categories><comments>A version of this paper appears in the Conference on Learning Theory
  (COLT) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the problem of active learning for binary label
prediction on a graph. We introduce a simple and label-efficient algorithm
called S2 for this task. At each step, S2 selects the vertex to be labeled
based on the structure of the graph and all previously gathered labels.
Specifically, S2 queries for the label of the vertex that bisects the *shortest
shortest* path between any pair of oppositely labeled vertices. We present a
theoretical estimate of the number of queries S2 needs in terms of a novel
parametrization of the complexity of binary functions on graphs. We also
present experimental results demonstrating the performance of S2 on both real
and synthetic data. While other graph-based active learning algorithms have
shown promise in practice, our algorithm is the first with both good
performance and theoretical guarantees. Finally, we demonstrate the
implications of the S2 algorithm to the theory of nonparametric active
learning. In particular, we show that S2 achieves near minimax optimal excess
risk for an important class of nonparametric classification problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08761</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08761</id><created>2015-06-26</created><authors><author><keyname>Lieberoth</keyname><forenames>Andreas</forenames></author><author><keyname>Pedersen</keyname><forenames>Mads Kock</forenames></author><author><keyname>Marin</keyname><forenames>Andreea Catalina</forenames></author><author><keyname>Planke</keyname><forenames>Tilo</forenames></author><author><keyname>Sherson</keyname><forenames>Jacob Friis</forenames></author></authors><title>Getting Humans to do Quantum Optimization - User Acquisition, Engagement
  and Early Results from the Citizen Cyberscience Game Quantum Moves</title><categories>cs.CY physics.ed-ph quant-ph</categories><comments>26 pages, 15 figures</comments><journal-ref>Human Computation 1(2) 219-244 (2014)</journal-ref><doi>10.15346/hc.v1i2.11</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The game Quantum Moves was designed to pit human players against computer
algorithms, combining their solutions into hybrid optimization to control a
scalable quantum computer. In this midstream report, we open our design process
and describe the series of constitutive building stages going into a quantum
physics citizen science game. We present our approach from designing a core
gameplay around quantum simulations, to putting extra game elements in place in
order to frame, structure, and motivate players' difficult path from curious
visitors to competent science contributors. The player base is extremely
diverse - for instance, two top players are a 40 year old female accountant and
a male taxi driver. Among statistical predictors for retention and in-game high
scores, the data from our first year suggest that people recruited based on
real-world physics interest and via real-world events, but only with an
intermediate science education, are more likely to become engaged and skilled
contributors. Interestingly, female players tended to perform better than male
players, even though men played more games per day. To understand this
relationship, we explore the profiles of our top players in more depth. We
discuss in-world and in-game performance factors departing in psychological
theories of intrinsic and extrinsic motivation, and the implications for using
real live humans to do hybrid optimization via initially simple, but ultimately
very cognitively complex games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08762</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08762</id><created>2015-06-29</created><authors><author><keyname>Wang</keyname><forenames>Hanlei</forenames></author></authors><title>Passivity-Based Adaptive Control for Visually Servoed Robotic Systems</title><categories>cs.SY cs.RO</categories><comments>17 pages, 5 figures, submitted to IEEE Transactions on Automatic
  Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the visual servoing problem for robotic systems with
uncertain kinematic, dynamic, and camera parameters. We first present the
passivity properties associated with the overall kinematics of the system, and
then propose two passivity-based adaptive control schemes to resolve the visual
tracking problem. One scheme employs the adaptive inverse-Jacobian-like
feedback, and the other employs the adaptive transpose Jacobian feedback. With
the Lyapunov analysis approach, it is shown that under either of the proposed
control schemes, the image-space tracking errors converge to zero irrespective
of the invertibility of the estimated depth. Numerical simulations are
performed to show the tracking performance of the proposed adaptive
controllers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08765</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08765</id><created>2015-06-29</created><authors><author><keyname>Arrigoni</keyname><forenames>Federica</forenames></author><author><keyname>Fusiello</keyname><forenames>Andrea</forenames></author><author><keyname>Rossi</keyname><forenames>Beatrice</forenames></author></authors><title>Spectral Motion Synchronization in SE(3)</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of motion synchronization (or averaging) and
describes a simple, closed-form solution based on a spectral decomposition,
which does not consider rotation and translation separately but works straight
in SE(3), the manifold of rigid motions. Besides its theoretical interest,
being the first closed form solution in SE(3), experimental results show that
it compares favourably with the state of the art both in terms of precision and
speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08781</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08781</id><created>2015-06-29</created><updated>2016-02-15</updated><authors><author><keyname>Preen</keyname><forenames>Richard J.</forenames></author><author><keyname>Bull</keyname><forenames>Larry</forenames></author></authors><title>On Design Mining: Coevolution and Surrogate Models</title><categories>cs.NE cs.AI cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Design mining is the use of computational intelligence techniques to
iteratively search and model the attribute space of physical objects evaluated
directly through rapid prototyping to meet given objectives. It enables the
exploitation of novel materials and processes without formal models or complex
simulation. In this paper, we focus upon the coevolutionary nature of the
design process when it is decomposed into concurrent sub-design threads due to
the overall complexity of the task. Using an abstract, tuneable model of
coevolution we consider strategies to sample sub-thread designs for whole
system testing, how best to construct and use surrogate models within the
coevolutionary scenario, and the effects of access to multiple whole system
(physical) testing equipment on performance. Drawing on our findings, the paper
then describes the effective design of an array of six heterogeneous
vertical-axis wind turbines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08789</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08789</id><created>2015-06-29</created><authors><author><keyname>Al-Saati</keyname><forenames>Najla</forenames></author><author><keyname>Abdul-Jaleel</keyname><forenames>Raghda</forenames></author></authors><title>Requirement Tracing using Term Extraction</title><categories>cs.SE cs.CL cs.IR</categories><journal-ref>IJCSIS Vol. 13, No. 5, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Requirements traceability is an essential step in ensuring the quality of
software during the early stages of its development life cycle. Requirements
tracing usually consists of document parsing, candidate link generation and
evaluation and traceability analysis. This paper demonstrates the applicability
of Statistical Term Extraction metrics to generate candidate links. It is
applied and validated using two data sets and four types of filters two for
each data set, 0.2 and 0.25 for MODIS, 0 and 0.05 for CM1. This method
generates requirements traceability matrices between textual requirements
artifacts (such as high-level requirements traced to low-level requirements).
The proposed method includes ten word frequency metrics divided into three main
groups for calculating the frequency of terms. The results show that the
proposed method gives better result when compared with the traditional TF-IDF
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08800</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08800</id><created>2015-06-29</created><updated>2015-09-11</updated><authors><author><keyname>Saur</keyname><forenames>Karla</forenames></author><author><keyname>Dumitra&#x15f;</keyname><forenames>Tudor</forenames></author><author><keyname>Hicks</keyname><forenames>Michael</forenames></author></authors><title>Evolving NoSQL Databases Without Downtime</title><categories>cs.DB</categories><comments>11 pages, 7 figures. This is an update to the previous submission.
  The prior submission was a client-side implementation and had significant
  overhead. This paper presents an server-side implementation has resolves the
  overhead problems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  NoSQL databases like Redis, Cassandra, and MongoDB are increasingly popular
because they are flexible, lightweight, and easy to work with. Applications
that use these databases will evolve over time, sometimes necessitating (or
preferring) a change to the format or organization of the data. The problem we
address in this paper is: How can we support the evolution of high-availability
applications and their NoSQL data online, without excessive delays or
interruptions, even in the presence of backward-incompatible data- format
changes?
  We present KVolve, an extension to the popular Redis NoSQL database, as a
solution to this problem. KVolve permits a developer to submit an upgrade
specification that defines how to transform existing data to the newest
version. This transformation is applied lazily as applications interact with
the database, thus avoiding long pause times. We demonstrate performing updates
to the data stored in Redis from programs with backward-incompatible naming and
format changes. We find that KVolve has essentially no overhead in general use
and minimal impact during updates, which is a significant improvement over the
lengthy pause times incurred when transforming the entire database offline.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08801</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08801</id><created>2015-06-29</created><authors><author><keyname>Mezzavilla</keyname><forenames>Marco</forenames></author><author><keyname>Dutta</keyname><forenames>Sourjya</forenames></author><author><keyname>Zhang</keyname><forenames>Menglei</forenames></author><author><keyname>Akdeniz</keyname><forenames>Mustafa Riza</forenames></author><author><keyname>Rangan</keyname><forenames>Sundeep</forenames></author></authors><title>5G mmWave Module for ns-3 Network Simulator</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing demand of data, along with the spectrum scarcity, are
motivating a urgent shift towards exploiting new bands. This is the main reason
behind identifying mmWaves as the key disruptive enabling technology for 5G
cellular networks. Indeed, utilizing new bands means facing new challenges; in
this context, they are mainly related to the radio propagation, which is
shorter in range and more sensitive to obstacles. The resulting key aspects
that need to be taken into account when designing mmWave cellular systems are
directionality and link intermittency. The lack of network level results
motivated this work, which aims at providing the first of a kind open source
mmWave framework, based on the network simulator ns-3. The main focus of this
work is the modeling of customizable channel, physical (PHY) and medium access
control (MAC) layers for mmWave systems. The overall design and architecture of
the model are discussed in details. Finally, the validity of our proposed
framework is corroborated through the simulation of a simple scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08811</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08811</id><created>2015-05-23</created><authors><author><keyname>Kokabifar</keyname><forenames>E.</forenames></author><author><keyname>Loghmani</keyname><forenames>G. B.</forenames></author><author><keyname>Latif</keyname><forenames>A.</forenames></author></authors><title>A new approach for image compression using normal matrices</title><categories>cs.MM math.NA</categories><comments>arXiv admin note: text overlap with arXiv:1506.01952</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present methods for image compression on the basis of
eigenvalue decomposition of normal matrices. The proposed methods are
convenient and self-explanatory, requiring fewer and easier computations as
compared to some existing methods. Through the proposed techniques, the image
is transformed to the space of normal matrices. Then, the properties of
spectral decomposition are dealt with to obtain compressed images. Experimental
results are provided to illustrate the validity of the methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08813</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08813</id><created>2015-06-26</created><updated>2015-07-01</updated><authors><author><keyname>Young</keyname><forenames>Anthony P.</forenames></author><author><keyname>Modgil</keyname><forenames>Sanjay</forenames></author><author><keyname>Rodrigues</keyname><forenames>Odinaldo</forenames></author></authors><title>Argumentation Semantics for Prioritised Default Logic</title><categories>cs.AI</categories><comments>46 pages, 4 figures</comments><acm-class>I.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We endow prioritised default logic (PDL) with argumentation semantics using
the ASPIC+ framework for structured argumentation, and prove that the
conclusions of the justified arguments are exactly the prioritised default
extensions. Argumentation semantics for PDL will allow for the application of
argument game proof theories to the process of inference in PDL, making the
reasons for accepting a conclusion transparent and the inference process more
intuitive. This also opens up the possibility for argumentation-based
distributed reasoning and communication amongst agents with PDL representations
of mental attitudes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08814</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08814</id><created>2015-06-28</created><updated>2015-10-07</updated><authors><author><keyname>Dvijotham</keyname><forenames>Krishnamurthy</forenames></author><author><keyname>Chertkov</keyname><forenames>Michael</forenames></author><author><keyname>Low</keyname><forenames>Steven</forenames></author></authors><title>A differential analysis of the power flow equations</title><categories>cs.SY</categories><comments>arXiv admin note: text overlap with arXiv:1506.08472</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The AC power flow equations are fundamental in all aspects of power systems
planning and operations. They are routinely solved using Newton-Raphson like
methods. However, there is little theoretical understanding of when these
algorithms are guaranteed to find a solution of the power flow equations or how
long they may take to converge. Further, it is known that in general these
equations have multiple solutions and can exhibit chaotic behavior. In this
paper, we show that the power flow equations can be solved efficiently provided
that the solution lies in a certain set. We introduce a family of convex
domains, characterized by Linear Matrix Inequalities, in the space of voltages
such that there is at most one power flow solution in each of these domains.
Further, if a solution exists in one of these domains, it can be found
efficiently, and if one does not exist, a certificate of non-existence can also
be obtained efficiently. The approach is based on the theory of monotone
operators and related algorithms for solving variational inequalities involving
monotone operators. We validate our approach on IEEE test networks and show
that practical power flow solutions lie within an appropriately chosen convex
domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08815</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08815</id><created>2015-06-29</created><authors><author><keyname>Kundu</keyname><forenames>Merina</forenames></author><author><keyname>Sengupta</keyname><forenames>Dhriti</forenames></author><author><keyname>Dastidar</keyname><forenames>Jayati Ghosh</forenames></author></authors><title>Tracking Direction of Human Movement - An Efficient Implementation using
  Skeleton</title><categories>cs.CV</categories><comments>arXiv admin note: text overlap with arXiv:1506.08682</comments><journal-ref>International Journal of Computer Applications 96(13):27-33, June
  2014</journal-ref><doi>10.5120/16855-6722</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sometimes a simple and fast algorithm is required to detect human presence
and movement with a low error rate in a controlled environment for security
purposes. Here a light weight algorithm has been presented that generates alert
on detection of human presence and its movement towards a certain direction.
The algorithm uses fixed angle CCTV camera images taken over time and relies
upon skeleton transformation of successive images and calculation of difference
in their coordinates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08834</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08834</id><created>2015-06-29</created><authors><author><keyname>Harrow</keyname><forenames>Aram W.</forenames></author><author><keyname>Natarajan</keyname><forenames>Anand</forenames></author><author><keyname>Wu</keyname><forenames>Xiaodi</forenames></author></authors><title>An improved semidefinite programming hierarchy for testing entanglement</title><categories>quant-ph cs.DS math.OC</categories><comments>21 pages</comments><report-no>MIT-CTP/4587</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a stronger version of the Doherty-Parrilo-Spedalieri (DPS)
hierarchy of approximations for the set of separable states. Unlike DPS, our
hierarchy converges exactly at a finite number of rounds for any fixed input
dimension. This yields an algorithm for separability testing which is singly
exponential in dimension and polylogarithmic in accuracy. Our analysis makes
use of tools from algebraic geometry, but our algorithm is elementary and
differs from DPS only by one simple additional collection of constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08839</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08839</id><created>2015-06-29</created><authors><author><keyname>McAuley</keyname><forenames>Julian</forenames></author><author><keyname>Pandey</keyname><forenames>Rahul</forenames></author><author><keyname>Leskovec</keyname><forenames>Jure</forenames></author></authors><title>Inferring Networks of Substitutable and Complementary Products</title><categories>cs.SI cs.IR</categories><comments>12 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a modern recommender system, it is important to understand how products
relate to each other. For example, while a user is looking for mobile phones,
it might make sense to recommend other phones, but once they buy a phone, we
might instead want to recommend batteries, cases, or chargers. These two types
of recommendations are referred to as substitutes and complements: substitutes
are products that can be purchased instead of each other, while complements are
products that can be purchased in addition to each other.
  Here we develop a method to infer networks of substitutable and complementary
products. We formulate this as a supervised link prediction task, where we
learn the semantics of substitutes and complements from data associated with
products. The primary source of data we use is the text of product reviews,
though our method also makes use of features such as ratings, specifications,
prices, and brands. Methodologically, we build topic models that are trained to
automatically discover topics from text that are successful at predicting and
explaining such relationships. Experimentally, we evaluate our system on the
Amazon product catalog, a large dataset consisting of 9 million products, 237
million links, and 144 million reviews.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08865</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08865</id><created>2015-06-29</created><updated>2015-10-23</updated><authors><author><keyname>Perera</keyname><forenames>Charith</forenames></author><author><keyname>Ranjan</keyname><forenames>Rajiv</forenames></author><author><keyname>Wang</keyname><forenames>Lizhe</forenames></author></authors><title>End-to-End Privacy for Open Big Data Markets</title><categories>cs.CY cs.CR cs.NI</categories><comments>Accepted to be published in IEEE Cloud Computing Magazine: Special
  Issue Cloud Computing and the Law</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The idea of an open data market envisions the creation of a data trading
model to facilitate exchange of data between different parties in the Internet
of Things (IoT) domain. The data collected by IoT products and solutions are
expected to be traded in these markets. Data owners will collect data using IoT
products and solutions. Data consumers who are interested will negotiate with
the data owners to get access to such data. Data captured by IoT products will
allow data consumers to further understand the preferences and behaviours of
data owners and to generate additional business value using different
techniques ranging from waste reduction to personalized service offerings. In
open data markets, data consumers will be able to give back part of the
additional value generated to the data owners. However, privacy becomes a
significant issue when data that can be used to derive extremely personal
information is being traded. This paper discusses why privacy matters in the
IoT domain in general and especially in open data markets and surveys existing
privacy-preserving strategies and design techniques that can be used to
facilitate end to end privacy for open data markets. We also highlight some of
the major research challenges that need to be address in order to make the
vision of open data markets a reality through ensuring the privacy of
stakeholders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08867</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08867</id><created>2015-06-26</created><authors><author><keyname>Pereira</keyname><forenames>Jos&#xe9; C.</forenames></author><author><keyname>Lobo</keyname><forenames>Fernando G.</forenames></author></authors><title>Java Implementation of a Parameter-less Evolutionary Portfolio</title><categories>cs.MS cs.NE</categories><comments>7 pages. arXiv admin note: substantial text overlap with
  arXiv:1506.08694, arXiv:1506.07980</comments><acm-class>I.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Java implementation of a portfolio of parameter-less evolutionary
algorithms is presented. The Parameter-less Evolutionary Portfolio implements a
heuristic that performs adaptive selection of parameter-less evolutionary
algorithms in accordance with performance criteria that are measured during
running time. At present time, the portfolio includes three parameter-less
evolutionary algorithms: Parameter-less Univariate Marginal Distribution
Algorithm, Parameter-less Extended Compact Genetic Algorithm, and
Parameter-less Hierarchical Bayesian Optimization Algorithm. Initial
experiments showed that the parameter-less portfolio can solve various classes
of problems without the need for any prior parameter setting technique and with
an increase in computational effort that can be considered acceptable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08879</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08879</id><created>2015-06-29</created><authors><author><keyname>Clerckx</keyname><forenames>Bruno</forenames></author><author><keyname>Bayguzina</keyname><forenames>Ekaterina</forenames></author><author><keyname>Yates</keyname><forenames>David</forenames></author><author><keyname>Mitcheson</keyname><forenames>Paul D.</forenames></author></authors><title>Waveform Optimization for Wireless Power Transfer with Nonlinear Energy
  Harvester Modeling</title><categories>cs.IT cs.NI math.IT</categories><comments>paper to be presented at IEEE International Symposium on Wireless
  Communication Systems (ISWCS 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Far-field Wireless Power Transfer (WPT) and Simultaneous Wireless Information
and Power Transfer (SWIPT) have attracted significant attention in the RF and
communication communities. Despite the rapid progress, the problem of waveform
design to enhance the output DC power of wireless energy harvester has received
limited attention so far. In this paper, we bridge communication and RF design
and derive novel multisine waveforms for multi-antenna wireless power transfer.
The waveforms are adaptive to the channel state information and result from a
posynomial maximization problem that originates from the non-linearity of the
energy harvester. They are shown through realistic simulations to provide
significant gains (in terms of harvested DC power) over state-of-the-art
waveforms under a fixed transmit power constraint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08891</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08891</id><created>2015-06-29</created><updated>2015-09-22</updated><authors><author><keyname>Fan</keyname><forenames>Miao</forenames></author><author><keyname>Kim</keyname><forenames>Doo Soon</forenames></author></authors><title>Detecting Table Region in PDF Documents Using Distant Supervision</title><categories>cs.CV cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Superior to state-of-the-art approaches which compete in table recognition
with 67 annotated government reports in PDF format released by {\it ICDAR 2013
Table Competition}, this paper contributes a novel paradigm leveraging
large-scale unlabeled PDF documents to open-domain table detection. We
integrate the paradigm into our latest developed system ({\it PdfExtra}) to
detect the region of tables by means of 9,466 academic articles from the entire
repository of {\it ACL Anthology}, where almost all papers are archived by PDF
format without annotation for tables. The paradigm first designs heuristics to
automatically construct weakly labeled data. It then feeds diverse evidences,
such as layouts of documents and linguistic features, which are extracted by
{\it Apache PDFBox} and processed by {\it Stanford NLP} toolkit, into different
canonical classifiers. We finally use these classifiers, i.e. {\it Naive
Bayes}, {\it Logistic Regression} and {\it Support Vector Machine}, to
collaboratively vote on the region of tables. Experimental results show that
{\it PdfExtra} achieves a great leap forward, compared with the
state-of-the-art approach. Moreover, we discuss the factors of different
features, learning models and even domains of documents that may impact the
performance. Extensive evaluations demonstrate that our paradigm is compatible
enough to leverage various features and learning models for open-domain table
region detection within PDF files.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08895</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08895</id><created>2015-06-29</created><authors><author><keyname>Salman</keyname><forenames>Mohamed</forenames></author><author><keyname>El-Keyi</keyname><forenames>Amr</forenames></author><author><keyname>Nafie</keyname><forenames>Mohammed</forenames></author><author><keyname>Hasna</keyname><forenames>Mazen Omar</forenames></author></authors><title>Sensing/Decision-Based Cooperative Relaying Schemes With Multi-Access
  Transmission: Stability Region And Average Delay Characterization</title><categories>cs.IT math.IT</categories><comments>13 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a cooperative relaying system which consists of a number of
source terminals, one shared relay, and a common destination with multi-packet
reception (MPR) capability. In this paper, we study the stability and delay
analysis for two cooperative relaying schemes; the sensing-based cooperative
(SBC) scheme and the decision-based cooperative (DBC) scheme. In the SBC
scheme, the relay senses the channel at the beginning of each time slot. In the
idle time slots, the relay transmits the packet at the head of its queue, while
in the busy one, the relay decides either to transmit simultaneously with the
source terminal or to listen to the source transmission. The SBC scheme is a
novel paradigm that utilizes the spectrum more efficiently than the other
cooperative schemes because the relay not only exploits the idle time slots,
but also has the capability to mildly interfere with the source terminal. On
the other hand, in the DBC scheme, the relay does not sense the channel and it
decides either to transmit or to listen according to certain probabilities.
Numerical results reveal that the two proposed schemes outperform existing
cooperative schemes that restrict the relay to send only in the idle time
slots. Moreover, we show how the MPR capability at the destination can
compensate for the sensing need at the relay, i.e., the DBC scheme achieves
almost the same stability region as that of the SBC scheme. Furthermore, we
derive the condition under which the two proposed schemes achieve the same
maximum stable throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08898</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08898</id><created>2015-06-29</created><updated>2016-02-18</updated><authors><author><keyname>Hou</keyname><forenames>Junhui</forenames></author><author><keyname>Chau</keyname><forenames>Lap-Pui</forenames></author><author><keyname>Magnenat-Thalmann</keyname><forenames>Nadia</forenames></author><author><keyname>He</keyname><forenames>Ying</forenames></author></authors><title>Low-latency compression of mocap data using learned spatial
  decorrelation transform</title><categories>cs.MM</categories><comments>15 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the growing needs of human motion capture (mocap) in movie, video
games, sports, etc., it is highly desired to compress mocap data for efficient
storage and transmission. This paper presents two efficient frameworks for
compressing human mocap data with low latency. The first framework processes
the data in a frame-by-frame manner so that it is ideal for mocap data
streaming and time critical applications. The second one is clip-based and
provides a flexible tradeoff between latency and compression performance. Since
mocap data exhibits some unique spatial characteristics, we propose a very
effective transform, namely learned orthogonal transform (LOT), for reducing
the spatial redundancy. The LOT problem is formulated as minimizing square
error regularized by orthogonality and sparsity and solved via alternating
iteration. We also adopt a predictive coding and temporal DCT for temporal
decorrelation in the frame- and clip-based frameworks, respectively.
Experimental results show that the proposed frameworks can produce higher
compression performance at lower computational cost and latency than the
state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08903</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08903</id><created>2015-06-29</created><updated>2015-08-14</updated><authors><author><keyname>Otter</keyname><forenames>Nina</forenames></author><author><keyname>Porter</keyname><forenames>Mason A.</forenames></author><author><keyname>Tillmann</keyname><forenames>Ulrike</forenames></author><author><keyname>Grindrod</keyname><forenames>Peter</forenames></author><author><keyname>Harrington</keyname><forenames>Heather A.</forenames></author></authors><title>A roadmap for the computation of persistent homology</title><categories>math.AT cs.CG physics.data-an q-bio.QM</categories><comments>17 pages, 6 figures. Corrected two references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Persistent homology is a method used in topological data analysis to study
qualitative features of data. It is robust to perturbations, independent of
dimensions and scale, and provides a compact representation of the outputs. The
computation of persistent homology, despite recent progress, remains a wide
open area with numerous important and fascinating challenges. We investigate
the challenges of computing persistent homology, and we navigate the various
algorithms that can be used for it. Specifically, we evaluate the (currently
available) open-source implementations of persistent-homology computations on a
range of synthetic and real-world data sets, and we indicate which algorithms
and implementations are best suited to these data. We provide guidelines for
the computation of persistent homology, make our own implementations used in
this study available to the public, and suggest measures to quantify the
challenges of the computation of persistent homology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08905</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08905</id><created>2015-06-29</created><updated>2015-09-14</updated><authors><author><keyname>Liang</keyname><forenames>Jia Hui</forenames></author><author><keyname>Ganesh</keyname><forenames>Vijay</forenames></author><author><keyname>Zulkoski</keyname><forenames>Ed</forenames></author><author><keyname>Zaman</keyname><forenames>Atulan</forenames></author><author><keyname>Czarnecki</keyname><forenames>Krzysztof</forenames></author></authors><title>Understanding VSIDS Branching Heuristics in Conflict-Driven
  Clause-Learning SAT Solvers</title><categories>cs.LO</categories><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conflict-Driven Clause-Learning SAT solvers crucially depend on the Variable
State Independent Decaying Sum (VSIDS) branching heuristic for their
performance. Although VSIDS was proposed nearly fifteen years ago, and many
other branching heuristics for SAT solving have since been proposed, VSIDS
remains one of the most effective branching heuristics.
  In this paper, we advance our understanding of VSIDS by answering the
following key questions. The first question we pose is &quot;what is special about
the class of variables that VSIDS chooses to additively bump?&quot; In answering
this question we showed that VSIDS overwhelmingly picks, bumps, and learns
bridge variables, defined as the variables that connect distinct communities in
the community structure of SAT instances. This is surprising since VSIDS was
invented more than a decade before the link between community structure and SAT
solver performance was discovered. Additionally, we show that VSIDS viewed as a
ranking function correlates strongly with temporal graph centrality measures.
Putting these two findings together, we conclude that VSIDS picks
high-centrality bridge variables. The second question we pose is &quot;what role
does multiplicative decay play in making VSIDS so effective?&quot; We show that the
multiplicative decay behaves like an exponential moving average (EMA) that
favors variables that persistently occur in conflicts (the signal) over
variables that occur intermittently (the noise). The third question we pose is
&quot;whether VSIDS is temporally and spatially focused.&quot; We show that VSIDS
disproportionately picks variables from a few communities unlike, say, the
random branching heuristic. We put these findings together to invent a new
adaptive VSIDS branching heuristic that solves more instances than one of the
best-known VSIDS variants over the SAT Competition 2013 benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08907</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08907</id><created>2015-06-29</created><authors><author><keyname>Kashyap</keyname><forenames>Sidharth N.</forenames></author><author><keyname>Fewings</keyname><forenames>Ade J.</forenames></author><author><keyname>Davies</keyname><forenames>Jay</forenames></author><author><keyname>Morris</keyname><forenames>Ian</forenames></author><author><keyname>Green</keyname><forenames>Andrew Thomas Thomas</forenames></author><author><keyname>Guest</keyname><forenames>Martyn F.</forenames></author></authors><title>Big Data at HPC Wales</title><categories>cs.DC</categories><comments>Accepted for publication at the 'Big Data Analytics Workshop' - 2014
  http://web.ornl.gov/sci/knowledgediscovery/CloudComputing/PDAC-SC14/BDAC-14-Agenda.htm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes an automated approach to handling Big Data workloads on
HPC systems. We describe a solution that dynamically creates a unified cluster
based on YARN in an HPC Environment, without the need to configure and allocate
a dedicated Hadoop cluster. The end user can choose to write the solution in
any combination of supported frameworks, a solution that scales seamlessly from
a few cores to thousands of cores. This coupling of environments creates a
platform for applications to utilize the native HPC solutions along with the
Big Data Frameworks. The user will be provided with HPC Wales APIs in multiple
languages that will let them integrate this flow into their environment,
thereby ensuring that the traditional means of HPC access do not become a
bottleneck. We describe the behavior of the cluster creation and performance
results on Terasort.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08908</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08908</id><created>2015-06-29</created><authors><author><keyname>De</keyname><forenames>Sushovan</forenames></author><author><keyname>Hu</keyname><forenames>Yuheng</forenames></author><author><keyname>Vamsikrishna</keyname><forenames>Meduri Venkata</forenames></author><author><keyname>Chen</keyname><forenames>Yi</forenames></author><author><keyname>Kambhampati</keyname><forenames>Subbarao</forenames></author></authors><title>BayesWipe: A Scalable Probabilistic Framework for Cleaning BigData</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent efforts in data cleaning of structured data have focused exclusively
on problems like data deduplication, record matching, and data standardization;
none of the approaches addressing these problems focus on fixing incorrect
attribute values in tuples. Correcting values in tuples is typically performed
by a minimum cost repair of tuples that violate static constraints like CFDs
(which have to be provided by domain experts, or learned from a clean sample of
the database). In this paper, we provide a method for correcting individual
attribute values in a structured database using a Bayesian generative model and
a statistical error model learned from the noisy database directly. We thus
avoid the necessity for a domain expert or clean master data. We also show how
to efficiently perform consistent query answering using this model over a dirty
database, in case write permissions to the database are unavailable. We
evaluate our methods over both synthetic and real data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08909</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08909</id><created>2015-06-29</created><updated>2016-02-03</updated><authors><author><keyname>Lowe</keyname><forenames>Ryan</forenames></author><author><keyname>Pow</keyname><forenames>Nissan</forenames></author><author><keyname>Serban</keyname><forenames>Iulian</forenames></author><author><keyname>Pineau</keyname><forenames>Joelle</forenames></author></authors><title>The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured
  Multi-Turn Dialogue Systems</title><categories>cs.CL cs.AI cs.LG cs.NE</categories><comments>SIGDIAL 2015. 10 pages, 5 figures. Update includes link to new
  version of the dataset, with some added features and bug fixes. See:
  https://github.com/rkadlec/ubuntu-ranking-dataset-creator</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the Ubuntu Dialogue Corpus, a dataset containing almost
1 million multi-turn dialogues, with a total of over 7 million utterances and
100 million words. This provides a unique resource for research into building
dialogue managers based on neural language models that can make use of large
amounts of unlabeled data. The dataset has both the multi-turn property of
conversations in the Dialog State Tracking Challenge datasets, and the
unstructured nature of interactions from microblog services such as Twitter. We
also describe two neural learning architectures suitable for analyzing this
dataset, and provide benchmark performance on the task of selecting the best
next response.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08910</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08910</id><created>2015-06-29</created><authors><author><keyname>Ganti</keyname><forenames>Ravi</forenames></author><author><keyname>Rao</keyname><forenames>Nikhil</forenames></author><author><keyname>Willett</keyname><forenames>Rebecca M.</forenames></author><author><keyname>Nowak</keyname><forenames>Robert</forenames></author></authors><title>Learning Single Index Models in High Dimensions</title><categories>stat.ML cs.LG stat.ME</categories><comments>16 pages, 2 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Single Index Models (SIMs) are simple yet flexible semi-parametric models for
classification and regression. Response variables are modeled as a nonlinear,
monotonic function of a linear combination of features. Estimation in this
context requires learning both the feature weights, and the nonlinear function.
While methods have been described to learn SIMs in the low dimensional regime,
a method that can efficiently learn SIMs in high dimensions has not been
forthcoming. We propose three variants of a computationally and statistically
efficient algorithm for SIM inference in high dimensions. We establish excess
risk bounds for the proposed algorithms and experimentally validate the
advantages that our SIM learning methods provide relative to Generalized Linear
Model (GLM) and low dimensional SIM based learning methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08915</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08915</id><created>2015-06-29</created><authors><author><keyname>Wang</keyname><forenames>Jue</forenames></author></authors><title>Bayes-Optimal Sequential Multi-Hypothesis Testing in Exponential
  Families</title><categories>cs.IT math.IT stat.ME</categories><comments>19 pages, 4 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian sequential testing of multiple simple hypotheses is a classical
sequential decision problem. But the optimal policy is computationally
intractable in general, because the posterior probability space is
exponentially increasing in the number of hypotheses (i.e, the curse of
dimensionality in state space). We consider a specialized problem in which
observations are drawn from the same exponential family. By reconstructing the
posterior probability vector using the natural sufficient statistic, it is
shown that the intrinsic dimension of the posterior probability space cannot
exceed the number of parameters governing the exponential family, or the number
of hypotheses, whichever is smaller. For univariate exponential families
commonly used in practice, the probability space is of one or two dimension in
most cases. Hence, the optimal policy can be attainable with only moderate
computation. Geometric interpretation and illustrative examples are presented.
Simulation studies suggest that the optimal policy can substantially outperform
the existing method. The results are also extended to the sequential sampling
control problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08916</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08916</id><created>2015-06-29</created><authors><author><keyname>Oselio</keyname><forenames>Brandon</forenames></author><author><keyname>Kulesza</keyname><forenames>Alex</forenames></author><author><keyname>Hero</keyname><forenames>Alfred</forenames></author></authors><title>Socio-Spatial Pareto Frontiers of Twitter Networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social media provides a rich source of networked data. This data is
represented by a set of nodes and a set of relations (edges). It is often
possible to obtain or infer multiple types of relations from the same set of
nodes, such as observed friend connections, inferred links via semantic
comparison, or relations based off of geographic proximity. These edge sets can
be represented by one multi-layer network. In this paper we review a method to
perform community detection of multilayer networks, and illustrate its use as a
visualization tool for analyzing different community partitions. The algorithm
is illustrated on a dataset from Twitter, specifically regarding the National
Football League (NFL).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08919</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08919</id><created>2015-06-29</created><authors><author><keyname>Schwind</keyname><forenames>Nicolas</forenames></author><author><keyname>Inoue</keyname><forenames>Katsumi</forenames></author></authors><title>Characterization of Logic Program Revision as an Extension of
  Propositional Revision</title><categories>cs.AI</categories><comments>42 pages, 5 figures, to appear in Theory and Practice of Logic
  Programming (accepted in June 2015)</comments><journal-ref>Theory and Practice of Logic Programming 16 (2015) 111-138</journal-ref><doi>10.1017/S1471068415000101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of belief revision of logic programs, i.e., how to
incorporate to a logic program P a new logic program Q. Based on the structure
of SE interpretations, Delgrande et al. adapted the well-known AGM framework to
logic program (LP) revision. They identified the rational behavior of LP
revision and introduced some specific operators. In this paper, a constructive
characterization of all rational LP revision operators is given in terms of
orderings over propositional interpretations with some further conditions
specific to SE interpretations. It provides an intuitive, complete procedure
for the construction of all rational LP revision operators and makes easier the
comprehension of their semantic and computational properties. We give a
particular consideration to logic programs of very general form, i.e., the
generalized logic programs (GLPs). We show that every rational GLP revision
operator is derived from a propositional revision operator satisfying the
original AGM postulates. Interestingly, the further conditions specific to GLP
revision are independent from the propositional revision operator on which a
GLP revision operator is based. Taking advantage of our characterization
result, we embed the GLP revision operators into structures of Boolean
lattices, that allow us to bring to light some potential weaknesses in the
adapted AGM postulates. To illustrate our claim, we introduce and characterize
axiomatically two specific classes of (rational) GLP revision operators which
arguably have a drastic behavior. We additionally consider two more restricted
forms of logic programs, i.e., the disjunctive logic programs (DLPs) and the
normal logic programs (NLPs) and adapt our characterization result to DLP and
NLP revision operators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08928</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08928</id><created>2015-06-29</created><authors><author><keyname>Song</keyname><forenames>Changkyu</forenames></author><author><keyname>Yoon</keyname><forenames>Sejong</forenames></author><author><keyname>Pavlovic</keyname><forenames>Vladimir</forenames></author></authors><title>Fast ADMM Algorithm for Distributed Optimization with Adaptive Penalty</title><categories>cs.LG cs.CV math.OC</categories><comments>8 pages manuscript, 2 pages appendix, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose new methods to speed up convergence of the Alternating Direction
Method of Multipliers (ADMM), a common optimization tool in the context of
large scale and distributed learning. The proposed method accelerates the speed
of convergence by automatically deciding the constraint penalty needed for
parameter consensus in each iteration. In addition, we also propose an
extension of the method that adaptively determines the maximum number of
iterations to update the penalty. We show that this approach effectively leads
to an adaptive, dynamic network topology underlying the distributed
optimization. The utility of the new penalty update schemes is demonstrated on
both synthetic and real data, including a computer vision application of
distributed structure from motion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08938</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08938</id><created>2015-06-30</created><authors><author><keyname>Nguyen</keyname><forenames>Duy-Khuong</forenames></author><author><keyname>Ho</keyname><forenames>Tu-Bao</forenames></author></authors><title>Accelerated Parallel and Distributed Algorithm using Limited Internal
  Memory for Nonnegative Matrix Factorization</title><categories>math.OC cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nonnegative matrix factorization (NMF) is a powerful technique for dimension
reduction, extracting latent factors and learning part-based representation.
For large datasets, NMF performance depends on some major issues: fast
algorithms, fully parallel distributed feasibility and limited internal memory.
This research aims to design a fast fully parallel and distributed algorithm
using limited internal memory to reach high NMF performance for large datasets.
In particular, we propose a flexible accelerated algorithm for NMF with all its
$L_1$ $L_2$ regularized variants based on full decomposition, which is a
combination of an anti-lopsided algorithm and a fast block coordinate descent
algorithm. The proposed algorithm takes advantages of both these algorithms to
achieve a linear convergence rate of $\mathcal{O}(1-\frac{1}{||Q||_2})^k$ in
optimizing each factor matrix when fixing the other factor one in the sub-space
of passive variables, where $r$ is the number of latent components; where
$\sqrt{r} \leq ||Q||_2 \leq r$. In addition, the algorithm can exploit the data
sparseness to run on large datasets with limited internal memory of machines.
Furthermore, our experimental results are highly competitive with 7
state-of-the-art methods about three significant aspects of convergence,
optimality and average of the iteration number. Therefore, the proposed
algorithm is superior to fast block coordinate descent methods and accelerated
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08941</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08941</id><created>2015-06-30</created><updated>2015-09-11</updated><authors><author><keyname>Narasimhan</keyname><forenames>Karthik</forenames></author><author><keyname>Kulkarni</keyname><forenames>Tejas</forenames></author><author><keyname>Barzilay</keyname><forenames>Regina</forenames></author></authors><title>Language Understanding for Text-based Games Using Deep Reinforcement
  Learning</title><categories>cs.CL cs.AI</categories><comments>11 pages, Appearing at EMNLP, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the task of learning control policies for
text-based games. In these games, all interactions in the virtual world are
through text and the underlying state is not observed. The resulting language
barrier makes such environments challenging for automatic game players. We
employ a deep reinforcement learning framework to jointly learn state
representations and action policies using game rewards as feedback. This
framework enables us to map text descriptions into vector representations that
capture the semantics of the game states. We evaluate our approach on two game
worlds, comparing against baselines using bag-of-words and bag-of-bigrams for
state representations. Our algorithm outperforms the baselines on both worlds
demonstrating the importance of learning expressive representations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08953</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08953</id><created>2015-06-30</created><authors><author><keyname>Hameed</keyname><forenames>Sufian</forenames></author><author><keyname>Ali</keyname><forenames>Usman</forenames></author></authors><title>On the Efficacy of Live DDoS Detection with Hadoop</title><categories>cs.CR cs.DC cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed Denial of Service flooding attacks are one of the biggest
challenges to the availability of online services today. These DDoS attacks
overwhelm the victim with huge volume of traffic and render it incapable of
performing normal communication or crashes it completely. If there are delays
in detecting the flooding attacks, nothing much can be done except to manually
disconnect the victim and fix the problem. With the rapid increase of DDoS
volume and frequency, the current DDoS detection technologies are challenged to
deal with huge attack volume in reasonable and affordable response time.
  In this paper, we propose HADEC, a Hadoop based Live DDoS Detection framework
to tackle efficient analysis of flooding attacks by harnessing MapReduce and
HDFS. We implemented a counter-based DDoS detection algorithm for four major
flooding attacks (TCP-SYN, HTTP GET, UDP and ICMP) in MapReduce, consisting of
map and reduce functions. We deployed a testbed to evaluate the performance of
HADEC framework for live DDoS detection. Based on the experiments we showed
that HADEC is capable of processing and detecting DDoS attacks in affordable
time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08956</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08956</id><created>2015-06-30</created><updated>2015-07-16</updated><authors><author><keyname>Sun</keyname><forenames>Libin</forenames></author><author><keyname>Guenter</keyname><forenames>Brian</forenames></author><author><keyname>Joshi</keyname><forenames>Neel</forenames></author><author><keyname>Therien</keyname><forenames>Patrick</forenames></author><author><keyname>Hays</keyname><forenames>James</forenames></author></authors><title>Lens Factory: Automatic Lens Generation Using Off-the-shelf Components</title><categories>cs.GR cs.CV</categories><comments>12 pages, 18 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Custom optics is a necessity for many imaging applications. Unfortunately,
custom lens design is costly (thousands to tens of thousands of dollars), time
consuming (10-12 weeks typical lead time), and requires specialized optics
design expertise. By using only inexpensive, off-the-shelf lens components the
Lens Factory automatic design system greatly reduces cost and time. Design,
ordering of parts, delivery, and assembly can be completed in a few days, at a
cost in the low hundreds of dollars. Lens design constraints, such as focal
length and field of view, are specified in terms familiar to the graphics
community so no optics expertise is necessary. Unlike conventional lens design
systems, which only use continuous optimization methods, Lens Factory adds a
discrete optimization stage. This stage searches the combinatorial space of
possible combinations of lens elements to find novel designs, evolving simple
canonical lens designs into more complex, better designs. Intelligent pruning
rules make the combinatorial search feasible. We have designed and built
several high performance optical systems which demonstrate the practicality of
the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08959</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08959</id><created>2015-06-30</created><updated>2015-09-24</updated><authors><author><keyname>Yang</keyname><forenames>Linjie</forenames></author><author><keyname>Luo</keyname><forenames>Ping</forenames></author><author><keyname>Loy</keyname><forenames>Chen Change</forenames></author><author><keyname>Tang</keyname><forenames>Xiaoou</forenames></author></authors><title>A Large-Scale Car Dataset for Fine-Grained Categorization and
  Verification</title><categories>cs.CV cs.AI</categories><comments>An extension to our conference paper in CVPR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Updated on 24/09/2015: This update provides preliminary experiment results
for fine-grained classification on the surveillance data of CompCars. The
train/test splits are provided in the updated dataset. See details in Section
6.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08961</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08961</id><created>2015-06-30</created><authors><author><keyname>Dougherty</keyname><forenames>Steven T.</forenames></author><author><keyname>Rif&#xe0;</keyname><forenames>Josep</forenames></author><author><keyname>Villanueva</keyname><forenames>Merc&#xe8;</forenames></author></authors><title>Ranks and Kernels of Codes from Generalized Hadamard Matrices</title><categories>cs.IT math.IT</categories><comments>13 pages Submitted to IEEE-IT</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ranks and kernels of generalized Hadamard matrices are studied. It is
proven that any generalized Hadamard matrix $H(q,\lambda)$ over $F_q$, $q&gt;3$,
or $q=3$ and $\gcd(3,\lambda)\not =1$, generates a self-orthogonal code. This
result puts a natural upper bound on the rank of the generalized Hadamard
matrices. Lower and upper bounds are given for the dimension of the kernel of
the corresponding generalized Hadamard codes. For specific ranks and dimensions
of the kernel within these bounds, generalized Hadamard codes are constructed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08966</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08966</id><created>2015-06-30</created><authors><author><keyname>Butt</keyname><forenames>Bilal Hayat</forenames></author><author><keyname>Rafi</keyname><forenames>Muhammad</forenames></author><author><keyname>Jamal</keyname><forenames>Arsal</forenames></author><author><keyname>Rehman</keyname><forenames>Raja Sami Ur</forenames></author><author><keyname>Alam</keyname><forenames>Syed Muhammad Zubair</forenames></author><author><keyname>Alam</keyname><forenames>Muhammad Bilal</forenames></author></authors><title>Classification of Research Citations (CRC)</title><categories>cs.IR cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research is a continuous phenomenon. It is recursive in nature. Every
research is based on some earlier research outcome. A general approach in
reviewing the literature for a problem is to categorize earlier work for the
same problem as positive and negative citations. In this paper, we propose a
novel automated technique, which classifies whether an earlier work is cited as
sentiment positive or sentiment negative. Our approach first extracted the
portion of the cited text from citing paper. Using a sentiment lexicon we
classify the citation as positive or negative by picking a window of at most
five (5) sentences around the cited place (corpus). We have used Na\&quot;ive-Bayes
Classifier for sentiment analysis. The algorithm is evaluated on a manually
annotated and class labelled collection of 150 research papers from the domain
of computer science. Our preliminary results show an accuracy of 80%. We assert
that our approach can be generalized to classification of scientific research
papers in different disciplines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08977</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08977</id><created>2015-06-30</created><updated>2015-09-05</updated><authors><author><keyname>Roux</keyname><forenames>Maurice</forenames></author></authors><title>A comparative study of divisive hierarchical clustering algorithms</title><categories>cs.DS q-bio.QM</categories><comments>11 pages, 1 figure</comments><msc-class>62-07</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A general scheme for divisive hierarchical clustering algorithms is proposed.
It is made of three main steps : first a splitting procedure for the
subdivision of clusters into two subclusters, second a local evaluation of the
bipartitions resulting from the tentative splits and, third, a formula for
determining the nodes levels of the resulting dendrogram. A number of such
algorithms is given. These algorithms are compared using the Goodman-Kruskal
correlation coefficient. As a global criterion it is an internal
goodness-of-fit measure based on the set order induced by the hierarchy
compared to the order associated to the given dissimilarities. Applied to a
hundred of random data tables, these comparisons are in favor of two methods
based on unusual ratio-type formulas for the splitting procedures, namely the
Silhouette criterion and Dunn's criterion. These two criteria take into account
both the within cluster and the between cluster mean dissimilarity. In general
the results of these two algorithms are better than the classical Agglomerative
Average Link method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08978</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08978</id><created>2015-06-30</created><updated>2015-12-02</updated><authors><author><keyname>Bar-Sinai</keyname><forenames>Michael</forenames></author></authors><title>Big Data Technology Literature Review</title><categories>cs.DC cs.DB</categories><comments>10 pages, 11 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A short overview of various algorithms and technologies that are helpful for
big data storage and manipulation. Includes pointers to papers for further
reading, and, where applicable, pointers to open source projects implementing a
described storage type.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08982</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08982</id><created>2015-06-30</created><authors><author><keyname>Li</keyname><forenames>Lvzhou</forenames></author><author><keyname>Feng</keyname><forenames>Yuan</forenames></author></authors><title>Quantum Markov chains: description of hybrid systems, decidability of
  equivalence, and model checking linear-time properties</title><categories>quant-ph cs.LO</categories><comments>This paper has been accepted for publication in Information and
  Computation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study a model of quantum Markov chains that is a quantum
analogue of Markov chains and is obtained by replacing probabilities in
transition matrices with quantum operations. We show that this model is very
suited to describe hybrid systems that consist of a quantum component and a
classical one, although it has the same expressive power as another quantum
Markov model proposed in the literature.
  Indeed, hybrid systems are often encountered in quantum information
processing; for example, both quantum programs and quantum protocols can be
regarded as hybrid systems. Thus, we further propose a model called hybrid
quantum automata (HQA) that can be used to describe these hybrid systems that
receive inputs (actions) from the outer world. We show the language equivalence
problem of HQA is decidable in polynomial time. Furthermore, we apply this
result to the trace equivalence problem of quantum Markov chains, and thus it
is also decidable in polynomial time. Finally, we discuss model checking
linear-time properties of quantum Markov chains, and show the quantitative
analysis of regular safety properties can be addressed successfully.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08987</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08987</id><created>2015-06-30</created><authors><author><keyname>Joroughi</keyname><forenames>Vahid</forenames></author><author><keyname>V&#xe1;zquez</keyname><forenames>Miguel &#xc1;ngel</forenames></author><author><keyname>P&#xe9;rez-Neira</keyname><forenames>Ana I.</forenames></author><author><keyname>Devillers</keyname><forenames>Bertrand</forenames></author></authors><title>On-Board Beam Generation for Multibeam Satellite Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims at designing an on-board beam generation process for
multibeam satellite systems with the goal of reducing the traffic at the feeder
link. Full frequency reuse among beams is considered and the beamforming at the
satellite is designed for supporting interference mitigation techniques. In
addition, in order to reduce the payload cost and complexity, this on-board
processing is assumed to be constant and the same for forward and return link
transmissions. To meet all these requirements a novel robust minimum mean
square error (MMSE) optimization is conceived. The benefits of the considered
scheme are evaluated with respect to the current approaches both analytically
and numerically. Indeed, we show that with the DVB-RCS and DVB-S2 standards,
our proposal allows to increase the total throughput within a range between 6\%
and 15\% with respect to other on-board processing techniques in the return and
forward link, respectively. Furthermore, the proposed solution presents an
implicit feeder link bandwidth reduction with respect to the on-ground beam
generation process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08988</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08988</id><created>2015-06-30</created><authors><author><keyname>Catal&#xe1;n</keyname><forenames>Sandra</forenames></author><author><keyname>Igual</keyname><forenames>Francisco D.</forenames></author><author><keyname>Mayo</keyname><forenames>Rafael</forenames></author><author><keyname>Rodr&#xed;guez-S&#xe1;nchez</keyname><forenames>Rafael</forenames></author><author><keyname>Quintana-Ort&#xed;</keyname><forenames>Enrique S.</forenames></author></authors><title>Architecture-Aware Configuration and Scheduling of Matrix Multiplication
  on Asymmetric Multicore Processors</title><categories>cs.PF cs.DC cs.MS cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Asymmetric multicore processors (AMPs) have recently emerged as an appealing
technology for severely energy-constrained environments, especially in mobile
appliances where heterogeneity in applications is mainstream. In addition,
given the growing interest for low-power high performance computing, this type
of architectures is also being investigated as a means to improve the
throughput-per-Watt of complex scientific applications.
  In this paper, we design and embed several architecture-aware optimizations
into a multi-threaded general matrix multiplication (gemm), a key operation of
the BLAS, in order to obtain a high performance implementation for ARM
big.LITTLE AMPs. Our solution is based on the reference implementation of gemm
in the BLIS library, and integrates a cache-aware configuration as well as
asymmetric--static and dynamic scheduling strategies that carefully tune and
distribute the operation's micro-kernels among the big and LITTLE cores of the
target processor. The experimental results on a Samsung Exynos 5422, a
system-on-chip with ARM Cortex-A15 and Cortex-A7 clusters that implements the
big.LITTLE model, expose that our cache-aware versions of gemm with asymmetric
scheduling attain important gains in performance with respect to its
architecture-oblivious counterparts while exploiting all the resources of the
AMP to deliver considerable energy efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08994</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08994</id><created>2015-06-30</created><authors><author><keyname>Wang</keyname><forenames>Dongming</forenames></author></authors><title>On the Connection Between Ritt Characteristic Sets and
  Buchberger-Gr\&quot;obner Bases</title><categories>math.AC cs.SC</categories><comments>15 pages</comments><msc-class>13P10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For any polynomial ideal $I$, let the minimal triangular set contained in the
reduced Buchberger-Gr\&quot;obner basis of $I$ with respect to the purely
lexicographical term order be called the W-characteristic set of $I$. In this
paper, we establish a strong connection between Ritt's characteristic sets and
Buchberger's Gr\&quot;obner bases of polynomial ideals by showing that the
W-characteristic set $C$ of $I$ is a Ritt characteristic set of $I$ whenever
$C$ is an ascending set, and a Ritt characteristic set of $I$ can always be
computed from $C$ with simple pseudo-division when $C$ is regular. We also
prove that under certain variable ordering, either the W-characteristic set of
$I$ is normal, or irregularity occurs for the $j$th, but not the $(j+1)$th,
elimination ideal of $I$ for some $j$. In the latter case, we provide explicit
pseudo-divisibility relations, which lead to nontrivial factorizations of
certain polynomials in the Buchberger-Gr\&quot;obner basis and thus reveal the
structure of such polynomials. The pseudo-divisibility relations may be used to
devise an algorithm to decompose arbitrary polynomial sets into normal
triangular sets based on Buchberger-Gr\&quot;obner bases computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.09000</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.09000</id><created>2015-06-30</created><authors><author><keyname>Heideklang</keyname><forenames>Ren&#xe9;</forenames></author><author><keyname>Shokouhi</keyname><forenames>Parisa</forenames></author></authors><title>Decision-level multi-method fusion of spatially scattered data from
  nondestructive inspection of ferromagnetic parts</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article deals with the fusion of flaw detections from multi-sensor
nondestructive materials testing. Because each testing method makes use of
different physical effects for defect localization, a multi-method approach is
promising to effectively distinguish the many false alarms from actual material
defects. To this end, we propose a new fusion technique for scattered two- or
three-dimensional location data. Using a density-based approach, the proposed
method is able to explicitly address the localization uncertainties such as
registration errors. We provide guidelines on how to set all key parameters and
demonstrate the technique's robustness. Finally, we apply our fusion approach
to experimental data and demonstrate its ability to find small defects by
substantially reducing false alarms under conditions where no single-sensor
method is adequate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.09016</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.09016</id><created>2015-06-30</created><authors><author><keyname>Bouchard</keyname><forenames>Guillaume</forenames></author><author><keyname>Trouillon</keyname><forenames>Th&#xe9;o</forenames></author><author><keyname>Perez</keyname><forenames>Julien</forenames></author><author><keyname>Gaidon</keyname><forenames>Adrien</forenames></author></authors><title>Accelerating Stochastic Gradient Descent via Online Learning to Sample</title><categories>cs.LG cs.CV cs.NA math.OC stat.ML</categories><comments>10+5 pages. Submitted to NIPS 2015. Slightly modified version with
  theoretical analysis moved up from appendices</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic Gradient Descent (SGD) is one of the most widely used techniques
for online optimization in machine learning. In this work, we accelerate SGD by
adaptively learning how to sample the most useful training examples at each
time step. First, we show that SGD can be used to learn the best possible
sampling distribution of an importance sampling estimator. Second, we show that
the sampling distribution of a SGD algorithm can be estimated online by
incrementally minimizing the variance of the gradient. The resulting algorithm
- called Adaptive Weighted SGD (AW-SGD) - maintains a set of parameters to
optimize, as well as a set of parameters to sample learning examples. We show
that AW-SGD yields faster convergence in three different applications: (i)
image classification with deep features, where the sampling of images depends
on their labels, (ii) matrix factorization, where rows and columns are not
sampled uniformly, and (iii) reinforcement learning, where the optimized and
explore policies are estimated at the same time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.09019</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.09019</id><created>2015-06-30</created><authors><author><keyname>Pabico</keyname><forenames>Jaderick P.</forenames></author></authors><title>Artificial Catalytic Reactions in 2D for Combinatorial Optimization</title><categories>cs.ET cs.NE</categories><comments>8 pages, 2 figures, In H.N. Adorna (ed.) Proceedings of the 3rd
  Symposium on Mathematical Aspects of Computer Science (SMACS 2006), Adventist
  University of the Philippines, Silang, Cavite, Philippines, 19-20 October
  2006 (Published by the Computing Society of the Philippines)</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Presented in this paper is a derivation of a 2D catalytic reaction-based
model to solve combinatorial optimization problems (COPs). The simulated
catalytic reactions, a computational metaphor, occurs in an artificial chemical
reactor that finds near-optimal solutions to COPs. The artificial environment
is governed by catalytic reactions that can alter the structure of artificial
molecular elements. Altering the molecular structure means finding new
solutions to the COP. The molecular mass of the elements was considered as a
measure of goodness of fit of the solutions. Several data structures and
matrices were used to record the directions and locations of the molecules.
These provided the model the 2D topology. The Traveling Salesperson Problem
(TSP) was used as a working example. The performance of the model in finding a
solution for the TSP was compared to the performance of a topology-less model.
Experimental results show that the 2D model performs better than the
topology-less one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.09023</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.09023</id><created>2015-06-30</created><authors><author><keyname>Hao</keyname><forenames>Chenxi</forenames></author><author><keyname>Wu</keyname><forenames>Yueping</forenames></author><author><keyname>Clerckx</keyname><forenames>Bruno</forenames></author></authors><title>Rate Analysis of Two-Receiver MISO Broadcast Channel with Finite Rate
  Feedback: A Rate-Splitting Approach</title><categories>cs.IT math.IT</categories><comments>accepted to IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To enhance the multiplexing gain of two-receiver Multiple-Input-Single-Output
Broadcast Channel with imperfect channel state information at the transmitter
(CSIT), a class of Rate-Splitting (RS) approaches has been proposed recently,
which divides one receiver's message into a common and a private part, and
superposes the common message on top of Zero-Forcing precoded private messages.
In this paper, with quantized CSIT, we study the ergodic sum rate of two
schemes, namely RS-S and RS-ST, where the common message(s) are transmitted via
a space and space-time design, respectively. Firstly, we upper-bound the sum
rate loss incurred by each scheme relative to Zero-Forcing Beamforming (ZFBF)
with perfect CSIT. Secondly, we show that, to maintain a constant sum rate
loss, RS-S scheme enables a feedback overhead reduction over ZFBF with
quantized CSIT. Such reduction scales logarithmically with the constant rate
loss at high Signal-to-Noise-Ratio (SNR). We also find that, compared to RS-S
scheme, RS-ST scheme offers a further feedback overhead reduction that scales
with the discrepancy between the feedback overhead employed by the two
receivers when there are alternating receiver-specific feedback qualities.
Finally, simulation results show that both schemes offer a significant SNR gain
over conventional single-user/multiuser mode switching when the feedback
overhead is fixed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.09032</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.09032</id><created>2015-06-30</created><authors><author><keyname>Abbas</keyname><forenames>Hosny Ahmed</forenames></author><author><keyname>Shaheen</keyname><forenames>Samir Ibrahim</forenames></author><author><keyname>Amin</keyname><forenames>Mohammed Hussein</forenames></author></authors><title>Organization of Multi-Agent Systems: An Overview</title><categories>cs.MA</categories><comments>12 pages</comments><journal-ref>Journal of Intelligent Information Systems. Vol. 4, No. 3, 2015,
  pp. 46-57</journal-ref><doi>10.11648/j.ijiis.20150403.11</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In complex, open, and heterogeneous environments, agents must be able to
reorganize towards the most appropriate organizations to adapt unpredictable
environment changes within Multi-Agent Systems (MAS). Types of reorganization
can be seen from two different levels. The individual agents level
(micro-level) in which an agent changes its behaviors and interactions with
other agents to adapt its local environment. And the organizational level
(macro-level) in which the whole system changes it structure by adding or
removing agents. This chapter is dedicated to overview different aspects of
what is called MAS Organization including its motivations, paradigms, models,
and techniques adopted for statically or dynamically organizing agents in MAS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.09039</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.09039</id><created>2015-06-30</created><updated>2016-02-09</updated><authors><author><keyname>Chen</keyname><forenames>Yutian</forenames></author><author><keyname>Ghahramani</keyname><forenames>Zoubin</forenames></author></authors><title>Scalable Discrete Sampling as a Multi-Armed Bandit Problem</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Drawing a sample from a discrete distribution is one of the building
components for Monte Carlo methods. Like other sampling algorithms, discrete
sampling also suffers from high computational burden in large-scale inference
problems. We study the problem of sampling a discrete random variable with a
high degree of dependency that is typical in large-scale Bayesian inference and
graphical models, and propose an efficient approximate solution with a
subsampling approach. We make a novel connection between the discrete sampling
and Multi-Armed Bandits problems with a finite reward population and provide
three algorithms with theoretical guarantees. Empirical evaluations show the
robustness and efficiency of the approximate algorithms in both synthetic and
real-world large-scale problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.09044</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.09044</id><created>2015-06-30</created><authors><author><keyname>Siccardi</keyname><forenames>Stefano</forenames></author><author><keyname>Tuszynski</keyname><forenames>Jack A.</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author></authors><title>Boolean gates on actin filaments</title><categories>cs.ET physics.bio-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Actin is a globular protein which forms long polar filaments in the
eukaryotic cytoskeleton. Actin networks play a key role in cell mechanics and
cell motility. They have also been implicated in information transmission and
processing, memory and learning in neuronal cells. The acting filaments have
been shown to support propagation of voltage pulses. Here we apply a coupled
nonlinear transmission line model of actin filaments to study interactions
between voltage pulses. By assigning a logical {\sc Truth} to the presence of a
voltage pulses in a given location of the actin filament, and {\sc False} to
the pulse's absence we represent digital information transmission along these
filaments. When two pulses, representing Boolean values of input variables,
interact, then they can facilitate or inhibit further propagation of each
other. We explore this phenomenon to construct Boolean logical gates and a
one-bit half-adder with interacting voltage pulses. We discuss implications of
these findings on cellular process and technological applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.09060</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.09060</id><created>2015-06-30</created><authors><author><keyname>Al-Safadi</keyname><forenames>Ebrahim B.</forenames></author><author><keyname>Al-Naffouri</keyname><forenames>Tareq Y.</forenames></author><author><keyname>Masood</keyname><forenames>Mudassir</forenames></author><author><keyname>Ali</keyname><forenames>Anum</forenames></author></authors><title>Nonlinear Distortion Reduction in OFDM from Reliable Perturbations in
  Data Carriers</title><categories>cs.IT math.IT stat.AP</categories><comments>27 pages, 11 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel method for correcting the effect of nonlinear distortion in
orthogonal frequency division multiplexing signals is proposed. The method
depends on adaptively selecting the distortion over a subset of the data
carriers, and then using tools from compressed sensing and sparse Bayesian
recovery to estimate the distortion over the other carriers. Central to this
method is the fact that carriers (or tones) are decoded with different levels
of confidence, depending on a coupled function of the magnitude and phase of
the distortion over each carrier, in addition to the respective channel
strength. Moreover, as no pilots are required by this method, a significant
improvement in terms of achievable rate can be achieved relative to previous
work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.09061</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.09061</id><created>2015-06-30</created><updated>2015-07-02</updated><authors><author><keyname>Bose</keyname><forenames>Prosenjit</forenames></author><author><keyname>Hill</keyname><forenames>Darryl</forenames></author><author><keyname>Smid</keyname><forenames>Michiel</forenames></author></authors><title>Improved Spanning Ratio for Low Degree Plane Spanners</title><categories>cs.CG</categories><comments>39 pages, appendix has been integrated into the main paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe an algorithm that builds a plane spanner with a maximum degree of
8 and a spanning ratio of approximately 4.414 with respect to the complete
graph. This is the best currently known spanning ratio for a plane spanner with
a maximum degree of less than 14.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.09067</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.09067</id><created>2015-06-30</created><authors><author><keyname>Viebke</keyname><forenames>Andre</forenames></author><author><keyname>Pllana</keyname><forenames>Sabri</forenames></author></authors><title>The Potential of the Intel Xeon Phi for Supervised Deep Learning</title><categories>cs.DC</categories><comments>The 17th IEEE International Conference on High Performance Computing
  and Communications (HPCC 2015), Aug. 24 - 26, 2015, New York, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Supervised learning of Convolutional Neural Networks (CNNs), also known as
supervised Deep Learning, is a computationally demanding process. To find the
most suitable parameters of a network for a given application, numerous
training sessions are required. Therefore, reducing the training time per
session is essential to fully utilize CNNs in practice. While numerous research
groups have addressed the training of CNNs using GPUs, so far not much
attention has been paid to the Intel Xeon Phi coprocessor. In this paper we
investigate empirically and theoretically the potential of the Intel Xeon Phi
for supervised learning of CNNs. We design and implement a parallelization
scheme named CHAOS that exploits both the thread- and SIMD-parallelism of the
coprocessor. Our approach is evaluated on the Intel Xeon Phi 7120P using the
MNIST dataset of handwritten digits for various thread counts and CNN
architectures. Results show a 103.5x speed up when training our large network
for 15 epochs using 244 threads, compared to one thread on the coprocessor.
Moreover, we develop a performance model and use it to assess our
implementation and answer what-if questions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.09074</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.09074</id><created>2015-06-30</created><authors><author><keyname>Primault</keyname><forenames>Vincent</forenames></author><author><keyname>Mokhtar</keyname><forenames>Sonia Ben</forenames></author><author><keyname>Brunie</keyname><forenames>Lionel</forenames></author></authors><title>Privacy-preserving Publication of Mobility Data with High Utility</title><categories>cs.CR</categories><comments>2015 35th IEEE International Conference on Distributed Computed
  Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An increasing amount of mobility data is being collected every day by
different means, e.g., by mobile phone operators. This data is sometimes
published after the application of simple anonymization techniques, which might
lead to severe privacy threats. We propose in this paper a new solution whose
novelty is twofold. Firstly, we introduce an algorithm designed to hide places
where a user stops during her journey (namely points of interest), by enforcing
a constant speed along her trajectory. Secondly, we leverage places where users
meet to take a chance to swap their trajectories and therefore confuse an
attacker.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.09075</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.09075</id><created>2015-06-30</created><updated>2016-02-29</updated><authors><author><keyname>Wu</keyname><forenames>Yuanyuan</forenames></author><author><keyname>He</keyname><forenames>Xiaohai</forenames></author><author><keyname>Kang</keyname><forenames>Byeongkeun</forenames></author><author><keyname>Nguyen</keyname><forenames>Truong Q.</forenames></author></authors><title>Long-Range Motion Trajectories Extraction of Articulated Human Using
  Mesh Evolution</title><categories>cs.CV</categories><comments>IEEE Signal Processing Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter presents a novel approach to extract reliable dense and
long-range motion trajectories of articulated human in a video sequence.
Compared with existing approaches that emphasize temporal consistency of each
tracked point, we also consider the spatial structure of tracked points on the
articulated human. We treat points as a set of vertices, and build a triangle
mesh to join them in image space. The problem of extracting long-range motion
trajectories is changed to the issue of consistency of mesh evolution over
time. First, self-occlusion is detected by a novel mesh-based method and an
adaptive motion estimation method is proposed to initialize mesh between
successive frames. Furthermore, we propose an iterative algorithm to
efficiently adjust vertices of mesh for a physically plausible deformation,
which can meet the local rigidity of mesh and silhouette constraints. Finally,
we compare the proposed method with the state-of-the-art methods on a set of
challenging sequences. Evaluations demonstrate that our method achieves
favorable performance in terms of both accuracy and integrity of extracted
trajectories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.09081</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.09081</id><created>2015-06-30</created><updated>2015-12-03</updated><authors><author><keyname>Cerf</keyname><forenames>Rapha&#xeb;l</forenames></author></authors><title>The quasispecies regime for the simple genetic algorithm with
  roulette-wheel selection</title><categories>cs.NE math.PR</categories><comments>The proof of theorem 3.1 is now included. References have been added.
  arXiv admin note: text overlap with arXiv:1403.5427</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new parameter to discuss the behavior of a genetic algorithm.
This parameter is the mean number of exact copies of the best fit chromosomes
from one generation to the next. We argue that the genetic algorithm should
operate efficiently when this parameter is slightly larger than $1$. We
consider the case of the simple genetic algorithm with the roulette--wheel
selection mechanism. We denote by $\ell$ the length of the chromosomes, by $m$
the population size, by $p_C$ the crossover probability and by $p_M$ the
mutation probability. We start the genetic algorithm with an initial population
whose maximal fitness is equal to $f_0^*$ and whose mean fitness is equal to
${\overline{f_0}}$. We show that, in the limit of large populations, the
dynamics of the genetic algorithm depends in a critical way on the parameter
$\pi \,=\,\big({f_0^*}/{\overline{f_0}}\big) (1-p_C)(1-p_M)^\ell\,.$ Our
results suggest that the mutation and crossover probabilities should be tuned
so that, at each generation, $\text{maximal fitness} \times (1-p_C)
(1-p_M)^\ell &gt; \text{mean fitness}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.09084</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.09084</id><created>2015-06-30</created><authors><author><keyname>Faulwasser</keyname><forenames>Timm</forenames></author><author><keyname>Weber</keyname><forenames>Tobias</forenames></author><author><keyname>Zometa</keyname><forenames>Juan Pablo</forenames></author><author><keyname>Findeisen</keyname><forenames>Rolf</forenames></author></authors><title>Implementation of Nonlinear Model Predictive Path-Following Control for
  an Industrial Robot</title><categories>cs.SY math.OC</categories><comments>12 pages, 4 figures</comments><msc-class>93C83, 70q05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many robotic applications, such as milling, gluing, or high precision
measurements, require the exact following of a pre-defined geometric path. In
this paper, we investigate the real-time feasible implementation of model
predictive path-following control for an industrial robot. We consider
constrained output path following with and without reference speed assignment.
We present results from an implementation of the proposed model predictive
path-following controller on a KUKA LWR IV robot.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.09100</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.09100</id><created>2015-06-30</created><authors><author><keyname>Ganian</keyname><forenames>Robert</forenames></author><author><keyname>Kalany</keyname><forenames>Martin</forenames></author><author><keyname>Szeider</keyname><forenames>Stefan</forenames></author><author><keyname>Tr&#xe4;ff</keyname><forenames>Jesper Larsson</forenames></author></authors><title>Polynomial-time Construction of Optimal Tree-structured Communication
  Data Layout Descriptions</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the problem of constructing tree-structured descriptions of data
layouts that are optimal with respect to space or other criteria from given
sequences of displacements, can be solved in polynomial time. The problem is
relevant for efficient compiler and library support for communication of
noncontiguous data, where tree-structured descriptions with low-degree nodes
and small index arrays are beneficial for the communication soft- and hardware.
An important example is the Message-Passing Interface (MPI) which has a
mechanism for describing arbitrary data layouts as trees using a set of
increasingly general constructors. Our algorithm shows that the so-called MPI
datatype reconstruction problem by trees with the full set of MPI constructors
can be solved optimally in polynomial time, refuting previous conjectures that
the problem is NP-hard. Our algorithm can handle further, natural constructors,
currently not found in MPI.
  Our algorithm is based on dynamic programming, and requires the solution of a
series of shortest path problems on an incrementally built, directed, acyclic
graph. The algorithm runs in $O(n^4)$ time steps and requires $O(n^2)$ space
for input displacement sequences of length $n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.09107</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.09107</id><created>2015-06-30</created><updated>2015-08-17</updated><authors><author><keyname>Amancio</keyname><forenames>Diego R.</forenames></author></authors><title>A complex network approach to stylometry</title><categories>cs.CL</categories><comments>PLoS ONE, 2015 (to appear)</comments><journal-ref>PLoS ONE 10(8): e0136076, 2015</journal-ref><doi>10.1371/journal.pone.0136076</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistical methods have been widely employed to study the fundamental
properties of language. In recent years, methods from complex and dynamical
systems proved useful to create several language models. Despite the large
amount of studies devoted to represent texts with physical models, only a
limited number of studies have shown how the properties of the underlying
physical systems can be employed to improve the performance of natural language
processing tasks. In this paper, I address this problem by devising complex
networks methods that are able to improve the performance of current
statistical methods. Using a fuzzy classification strategy, I show that the
topological properties extracted from texts complement the traditional textual
description. In several cases, the performance obtained with hybrid approaches
outperformed the results obtained when only traditional or networked methods
were used. Because the proposed model is generic, the framework devised here
could be straightforwardly used to study similar textual applications where the
topology plays a pivotal role in the description of the interacting agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.09109</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.09109</id><created>2015-06-30</created><authors><author><keyname>Jang</keyname><forenames>Jinyoung</forenames></author><author><keyname>Chung</keyname><forenames>MinKeun</forenames></author><author><keyname>Hwang</keyname><forenames>Hae Gwang</forenames></author><author><keyname>Lim</keyname><forenames>Yeon-Geun</forenames></author><author><keyname>Yoon</keyname><forenames>Hong-jib</forenames></author><author><keyname>Oh</keyname><forenames>TaeckKeun</forenames></author><author><keyname>Min</keyname><forenames>Byung-Wook</forenames></author><author><keyname>Lee</keyname><forenames>Yongshik</forenames></author><author><keyname>Kim</keyname><forenames>Kwang Soon</forenames></author><author><keyname>Chae</keyname><forenames>Chan-Byoung</forenames></author><author><keyname>Kim</keyname><forenames>Dong Ku</forenames></author></authors><title>Smart Small Cell for 5G: Theoretical Feasibility and Prototype Results</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we present a real-time three dimensional (3D) hybrid
beamforming for fifth generation (5G) wireless networks. One of the key
concepts in 5G cellular systems is a small cell network, which settles the high
mobile traffic demand and provides uniform user-experienced data rates. The
overall capacity of the small cell network can be enhanced with the enabling
technology of 3D hybrid beamforming. This study validates the feasibility of
the 3D hybrid beamforming, mostly for link-level performances, through the
implementation of a real-time testbed using a software-defined radio (SDR)
platform and fabricated antenna array. Based on the measured data, we also
investigate system-level performances to verify the gain of the proposed smart
small cell system over long term evolution (LTE) systems by performing
system-level simulations based on a 3D ray-tracing tool.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.09110</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.09110</id><created>2015-06-30</created><authors><author><keyname>Shafiee</keyname><forenames>Mohammad Javad</forenames></author><author><keyname>Wong</keyname><forenames>Alexander</forenames></author><author><keyname>Fieguth</keyname><forenames>Paul</forenames></author></authors><title>Forming A Random Field via Stochastic Cliques: From Random Graphs to
  Fully Connected Random Fields</title><categories>cs.CV</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random fields have remained a topic of great interest over past decades for
the purpose of structured inference, especially for problems such as image
segmentation. The local nodal interactions commonly used in such models often
suffer the short-boundary bias problem, which are tackled primarily through the
incorporation of long-range nodal interactions. However, the issue of
computational tractability becomes a significant issue when incorporating such
long-range nodal interactions, particularly when a large number of long-range
nodal interactions (e.g., fully-connected random fields) are modeled.
  In this work, we introduce a generalized random field framework based around
the concept of stochastic cliques, which addresses the issue of computational
tractability when using fully-connected random fields by stochastically forming
a sparse representation of the random field. The proposed framework allows for
efficient structured inference using fully-connected random fields without any
restrictions on the potential functions that can be utilized. Several
realizations of the proposed framework using graph cuts are presented and
evaluated, and experimental results demonstrate that the proposed framework can
provide competitive performance for the purpose of image segmentation when
compared to existing fully-connected and principled deep random field
frameworks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.09115</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.09115</id><created>2015-06-30</created><authors><author><keyname>Omodei</keyname><forenames>Elisa</forenames></author><author><keyname>De Domenico</keyname><forenames>Manlio</forenames></author><author><keyname>Arenas</keyname><forenames>Alex</forenames></author></authors><title>Characterizing interactions in online social networks during exceptional
  events</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, millions of people interact on a daily basis on online social media
like Facebook and Twitter, where they share and discuss information about a
wide variety of topics. In this paper, we focus on a specific online social
network, Twitter, and we analyze multiple datasets each one consisting of
individuals' online activity before, during and after an exceptional event in
terms of volume of the communications registered. We consider important events
that occurred in different arenas that range from policy to culture or science.
For each dataset, the users' online activities are modeled by a multilayer
network in which each layer conveys a different kind of interaction,
specifically: retweeting, mentioning and replying. This representation allows
us to unveil that these distinct types of interaction produce networks with
different statistical properties, in particular concerning the degree
distribution and the clustering structure. These results suggests that models
of online activity cannot discard the information carried by this multilayer
representation of the system, and should account for the different processes
generated by the different kinds of interactions. Secondly, our analysis
unveils the presence of statistical regularities among the different events,
suggesting that the non-trivial topological patterns that we observe may
represent universal features of the social dynamics on online social networks
during exceptional events.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.09118</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.09118</id><created>2015-06-30</created><authors><author><keyname>Wang</keyname><forenames>Meisong</forenames></author><author><keyname>Perera</keyname><forenames>Charith</forenames></author><author><keyname>Jayaraman</keyname><forenames>Prem Prakash</forenames></author><author><keyname>Zhang</keyname><forenames>Miranda</forenames></author><author><keyname>Strazdins</keyname><forenames>Peter</forenames></author><author><keyname>Ranjan</keyname><forenames>Rajiv</forenames></author></authors><title>City Data Fusion: Sensor Data Fusion in the Internet of Things</title><categories>cs.CY cs.NI</categories><comments>Accepted to be published in International Journal of Distributed
  Systems and Technologies (IJDST), 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internet of Things (IoT) has gained substantial attention recently and play a
significant role in smart city application deployments. A number of such smart
city applications depend on sensor fusion capabilities in the cloud from
diverse data sources. We introduce the concept of IoT and present in detail ten
different parameters that govern our sensor data fusion evaluation framework.
We then evaluate the current state-of-the art in sensor data fusion against our
sensor data fusion framework. Our main goal is to examine and survey different
sensor data fusion research efforts based on our evaluation framework. The
major open research issues related to sensor data fusion are also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.09124</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.09124</id><created>2015-06-30</created><authors><author><keyname>Yi</keyname><forenames>Saehoon</forenames></author><author><keyname>Pavlovic</keyname><forenames>Vladimir</forenames></author></authors><title>Multi-Cue Structure Preserving MRF for Unconstrained Video Segmentation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Video segmentation is a stepping stone to understanding video context. Video
segmentation enables one to represent a video by decomposing it into coherent
regions which comprise whole or parts of objects. However, the challenge
originates from the fact that most of the video segmentation algorithms are
based on unsupervised learning due to expensive cost of pixelwise video
annotation and intra-class variability within similar unconstrained video
classes. We propose a Markov Random Field model for unconstrained video
segmentation that relies on tight integration of multiple cues: vertices are
defined from contour based superpixels, unary potentials from temporal smooth
label likelihood and pairwise potentials from global structure of a video.
Multi-cue structure is a breakthrough to extracting coherent object regions for
unconstrained videos in absence of supervision. Our experiments on VSB100
dataset show that the proposed model significantly outperforms competing
state-of-the-art algorithms. Qualitative analysis illustrates that video
segmentation result of the proposed model is consistent with human perception
of objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.09140</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.09140</id><created>2015-06-30</created><authors><author><keyname>Carayol</keyname><forenames>Arnaud</forenames></author><author><keyname>L&#xf6;ding</keyname><forenames>Christof</forenames></author><author><keyname>Serre</keyname><forenames>Olivier</forenames></author></authors><title>Pure Strategies in Imperfect Information Stochastic Games</title><categories>cs.FL cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider imperfect information stochastic games where we require the
players to use pure (i.e. non randomised) strategies. We consider reachability,
safety, B\&quot;uchi and co-B\&quot;uchi objectives, and investigate the existence of
almost-sure/positively winning strategies for the first player when the second
player is perfectly informed or more informed than the first player. We obtain
decidability results for positive reachability and almost-sure B\&quot;uchi with
optimal algorithms to decide existence of a pure winning strategy and to
compute one if exists. We complete the picture by showing that positive safety
is undecidable when restricting to pure strategies even if the second player is
perfectly informed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.09145</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.09145</id><created>2015-06-30</created><authors><author><keyname>Bannister</keyname><forenames>Michael J.</forenames></author><author><keyname>Devanny</keyname><forenames>William E.</forenames></author><author><keyname>Dujmovi&#x107;</keyname><forenames>Vida</forenames></author><author><keyname>Eppstein</keyname><forenames>David</forenames></author><author><keyname>Wood</keyname><forenames>David R.</forenames></author></authors><title>Track Layouts, Layered Path Decompositions, and Leveled Planarity</title><categories>math.CO cs.DS</categories><comments>19 pages, 8 figures</comments><msc-class>05C10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate two types of graph layouts, track layouts and layered path
decompositions, and the relations between their associated parameters
track-number and layered pathwidth. We use these two types of layouts to
characterize leveled planar graphs, the graphs with planar layered drawings
with no dummy vertices. It follows from the known NP-completeness of leveled
planarity that track-number and layered pathwidth are also NP-complete, even
for the smallest constant parameter values that make these parameters
nontrivial. We prove that the graphs with bounded layered pathwidth include
outerplanar graphs, Halin graphs, and squaregraphs, but that (despite having
bounded track-number) series-parallel graphs do not have bounded layered
pathwidth. Finally, we investigate the parameterized complexity of these
layouts, showing that past methods used for book layouts don't work to
parameterize the problem by treewidth or almost-tree number but that the
problem is (non-uniformly) fixed-parameter tractable for tree-depth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.09153</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.09153</id><created>2015-06-30</created><authors><author><keyname>Widmer</keyname><forenames>Christian</forenames></author><author><keyname>Kloft</keyname><forenames>Marius</forenames></author><author><keyname>Sreedharan</keyname><forenames>Vipin T</forenames></author><author><keyname>R&#xe4;tsch</keyname><forenames>Gunnar</forenames></author></authors><title>Framework for Multi-task Multiple Kernel Learning and Applications in
  Genome Analysis</title><categories>stat.ML cs.CE cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a general regularization-based framework for Multi-task learning
(MTL), in which the similarity between tasks can be learned or refined using
$\ell_p$-norm Multiple Kernel learning (MKL). Based on this very general
formulation (including a general loss function), we derive the corresponding
dual formulation using Fenchel duality applied to Hermitian matrices. We show
that numerous established MTL methods can be derived as special cases from
both, the primal and dual of our formulation. Furthermore, we derive a modern
dual-coordinate descend optimization strategy for the hinge-loss variant of our
formulation and provide convergence bounds for our algorithm. As a special
case, we implement in C++ a fast LibLinear-style solver for $\ell_p$-norm MKL.
In the experimental section, we analyze various aspects of our algorithm such
as predictive performance and ability to reconstruct task relationships on
biologically inspired synthetic data, where we have full control over the
underlying ground truth. We also experiment on a new dataset from the domain of
computational biology that we collected for the purpose of this paper. It
concerns the prediction of transcription start sites (TSS) over nine organisms,
which is a crucial task in gene finding. Our solvers including all discussed
special cases are made available as open-source software as part of the SHOGUN
machine learning toolbox (available at \url{http://shogun.ml}).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.09155</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.09155</id><created>2015-06-30</created><updated>2015-11-16</updated><authors><author><keyname>Guazzini</keyname><forenames>Andrea</forenames></author><author><keyname>Vilone</keyname><forenames>Daniele</forenames></author><author><keyname>Donati</keyname><forenames>Camillo</forenames></author><author><keyname>Nardi</keyname><forenames>Annalisa</forenames></author><author><keyname>Levnajic</keyname><forenames>Zoran</forenames></author></authors><title>Modeling crowdsourcing as collective problem solving</title><categories>physics.soc-ph cs.SI physics.pop-ph</categories><comments>19 pages, 3 figures</comments><journal-ref>Scientific Reports, 5, 16557 (2015)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crowdsourcing is a process of accumulating the ideas, thoughts or information
from many independent participants, with aim to find the best solution for a
given challenge. Modern information technologies allow for massive number of
subjects to be involved in a more or less spontaneous way. Still, the full
potentials of crowdsourcing are yet to be reached. We introduce a modeling
framework through which we study the effectiveness of crowdsourcing in relation
to the level of collectivism in facing the problem. Our findings reveal an
intricate relationship between the number of participants and the difficulty of
the problem, indicating the optimal size of the crowdsourced group. We discuss
our results in the context of modern utilization of crowdsourcing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.09158</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.09158</id><created>2015-06-30</created><authors><author><keyname>Dell'Amico</keyname><forenames>Matteo</forenames></author><author><keyname>Carra</keyname><forenames>Damiano</forenames></author><author><keyname>Michiardi</keyname><forenames>Pietro</forenames></author></authors><title>On Fair Size-Based Scheduling</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By executing jobs serially rather than in parallel, size-based scheduling
policies can shorten time needed to complete jobs; however, major obstacles to
their applicability are fairness guarantees and the fact that job sizes are
rarely known exactly a-priori. Here, we introduce the Pri family of size-based
scheduling policies; Pri simulates any reference scheduler and executes jobs in
the order of their simulated completion: we show that these schedulers give
strong fairness guarantees, since no job completes later in Pri than in the
reference policy. In addition, we introduce PSBS, a practical implementation of
such a scheduler: it works online (i.e., without needing knowledge of jobs
submitted in the future), it has an efficient O(log n) implementation and it
allows setting priorities to jobs. Most importantly, unlike earlier size-based
policies, the performance of PSBS degrades gracefully with errors, leading to
performances that are close to optimal in a variety of realistic use cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.09163</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.09163</id><created>2015-06-30</created><authors><author><keyname>Marti</keyname><forenames>Gautier</forenames></author><author><keyname>Nielsen</keyname><forenames>Frank</forenames></author><author><keyname>Very</keyname><forenames>Philippe</forenames></author><author><keyname>Donnat</keyname><forenames>Philippe</forenames></author></authors><title>Comment partitionner automatiquement des marches al\'eatoires ? Avec
  application \`a la finance quantitative</title><categories>cs.CE stat.ME</categories><comments>in French</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present in this paper a novel non-parametric approach useful for
clustering Markov processes. We introduce a pre-processing step consisting in
mapping multivariate independent and identically distributed samples from
random variables to a generic non-parametric representation which factorizes
dependency and marginal distribution apart without losing any. An associated
metric is defined where the balance between random variables dependency and
distribution information is controlled by a single parameter. This mixing
parameter can be learned or played with by a practitioner, such use is
illustrated on the case of clustering financial time series. Experiments,
implementation and results obtained on public financial time series are online
on a web portal \url{http://www.datagrapple.com}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.09166</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.09166</id><created>2015-06-30</created><authors><author><keyname>Avanaki</keyname><forenames>Ali R. N.</forenames></author><author><keyname>Espig</keyname><forenames>Kathryn S.</forenames></author><author><keyname>Sawhney</keyname><forenames>Sameer</forenames></author><author><keyname>Pantanowitz</keyname><forenames>Liron</forenames></author><author><keyname>Parwani</keyname><forenames>Anil V.</forenames></author><author><keyname>Xthona</keyname><forenames>Albert</forenames></author><author><keyname>Kimpe</keyname><forenames>Tom R. L.</forenames></author></authors><title>Aging display's effect on interpretation of digital pathology slides</title><categories>cs.CV cs.GR</categories><doi>10.1117/12.2082315</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is our conjecture that the variability of colors in a pathology image
effects the interpretation of pathology cases, whether it is diagnostic
accuracy, diagnostic confidence, or workflow efficiency. In this paper, digital
pathology images are analyzed to quantify the perceived difference in color
that occurs due to display aging, in particular a change in the maximum
luminance, white point, and color gamut. The digital pathology images studied
include diagnostically important features, such as the conspicuity of nuclei.
Three different display aging models are applied to images: aging of luminance
&amp; chrominance, aging of chrominance only, and a stabilized luminance &amp;
chrominance (i.e., no aging). These display models and images are then used to
compare conspicuity of nuclei using CIE deltaE2000, a perceptual color
difference metric. The effect of display aging using these display models and
images is further analyzed through a human reader study designed to quantify
the effects from a clinical perspective. Results from our reader study indicate
significant impact of aged displays on workflow as well as diagnosis as follow.
As compared to the originals (no-aging), slides with the effect of aging
simulated were significantly more difficult to read (p-value of 0.0005) and
took longer to score (p-value of 0.02). Moreover, luminance+chrominance aging
significantly reduced inter-session percent agreement of diagnostic scores
(p-value of 0.0418).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.09169</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.09169</id><created>2015-06-30</created><authors><author><keyname>Avanaki</keyname><forenames>Ali R. N.</forenames></author><author><keyname>Espig</keyname><forenames>Kathryn S.</forenames></author><author><keyname>Kimpe</keyname><forenames>Tom R. L.</forenames></author><author><keyname>Maidment</keyname><forenames>Andrew D. A.</forenames></author></authors><title>On anthropomorphic decision making in a model observer</title><categories>cs.CV cs.HC</categories><doi>10.1117/12.2082129</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By analyzing human readers' performance in detecting small round lesions in
simulated digital breast tomosynthesis background in a location known exactly
scenario, we have developed a model observer that is a better predictor of
human performance with different levels of background complexity (i.e.,
anatomical and quantum noise). Our analysis indicates that human observers
perform a lesion detection task by combining a number of sub-decisions, each an
indicator of the presence of a lesion in the image stack. This is in contrast
to a channelized Hotelling observer, where the detection task is conducted
holistically by thresholding a single decision variable, made from an optimally
weighted linear combination of channels. However, it seems that the sub-par
performance of human readers compared to the CHO cannot be fully explained by
their reliance on sub-decisions, or perhaps we do not consider a sufficient
number of sub-decisions. To bridge the gap between the performances of human
readers and the model observer based upon sub-decisions, we use an additive
noise model, the power of which is modulated with the level of background
complexity. The proposed model observer better predicts the fast drop in human
detection performance with background complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.09174</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.09174</id><created>2015-06-30</created><updated>2015-06-30</updated><authors><author><keyname>Kim</keyname><forenames>Jongpil</forenames></author><author><keyname>Pavlovic</keyname><forenames>Vladimir</forenames></author></authors><title>Discovering Characteristic Landmarks on Ancient Coins using
  Convolutional Networks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel method to find characteristic landmarks on
ancient Roman imperial coins using deep convolutional neural network models
(CNNs). We formulate an optimization problem to discover class-specific regions
while guaranteeing specific controlled loss of accuracy. Analysis on
visualization of the discovered region confirms that not only can the proposed
method successfully find a set of characteristic regions per class, but also
the discovered region is consistent with human expert annotations. We also
propose a new framework to recognize the Roman coins which exploits
hierarchical structure of the ancient Roman coins using the state-of-the-art
classification power of the CNNs adopted to a new task of coin classification.
Experimental results show that the proposed framework is able to effectively
recognize the ancient Roman coins. For this research, we have collected a new
Roman coin dataset where all coins are annotated and consist of observe (head)
and reverse (tail) images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.09177</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.09177</id><created>2015-06-30</created><updated>2015-07-27</updated><authors><author><keyname>Gosavi</keyname><forenames>Tanay</forenames></author><author><keyname>Manipatruni</keyname><forenames>Sasikanth</forenames></author><author><keyname>Nikonov</keyname><forenames>Dmitri</forenames></author><author><keyname>Young</keyname><forenames>Ian A.</forenames></author><author><keyname>Bhave</keyname><forenames>Sunil</forenames></author></authors><title>Experimental Demonstration of Efficient Spin-Orbit Torque Switching of
  an MTJ with sub-100 ns Pulses</title><categories>cond-mat.mes-hall cs.ET</categories><comments>This paper has been withdrawn due to issues concerning IP</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper has been withdrawn due to issues concerning IP
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.09179</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.09179</id><created>2015-06-30</created><authors><author><keyname>Madooei</keyname><forenames>Ali</forenames></author><author><keyname>Drew</keyname><forenames>Mark S.</forenames></author><author><keyname>Hajimirsadeghi</keyname><forenames>Hossein</forenames></author></authors><title>Learning to Detect Blue-white Structures in Dermoscopy Images with Weak
  Supervision</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel approach to identify one of the most significant
dermoscopic criteria in the diagnosis of Cutaneous Melanoma: the Blue-whitish
structure. In this paper, we achieve this goal in a Multiple Instance Learning
framework using only image-level labels of whether the feature is present or
not. As the output, we predict the image classification label and as well
localize the feature in the image. Experiments are conducted on a challenging
dataset with results outperforming state-of-the-art. This study provides an
improvement on the scope of modelling for computerized image analysis of skin
lesions, in particular in that it puts forward a framework for identification
of dermoscopic local features from weakly-labelled data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.09191</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.09191</id><created>2015-06-30</created><authors><author><keyname>Huh</keyname><forenames>Jina</forenames></author><author><keyname>Kwon</keyname><forenames>Bum Chul</forenames></author><author><keyname>Choo</keyname><forenames>Jaegul</forenames></author><author><keyname>Kim</keyname><forenames>Sung-Hee</forenames></author><author><keyname>Yi</keyname><forenames>Ji Soo</forenames></author></authors><title>Coddlers, Scientists, Adventurers, and Opportunists: Personas to Inform
  Online Health Community Development</title><categories>cs.HC</categories><comments>10 pages, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As online health communities (OHCs) grow, users find it challenging to
properly search, read, and contribute to the community because of its
overwhelming content. Our goal is to understand OHC users' needs and
requirements for better delivering large-scale OHC content. We interviewed 14
OHC users with interests in diabetes to investigate their attitudes and needs
towards using OHCs and 2 OHC administrators to assess our findings. Four
personas -Coddlers, Scientists, Adventurers, and Opportunists- emerged, which
inform users' interaction behavior and attitudes with OHCs. An individual can
possess the characteristics of multiple personas, which can also change over
time. Our personas uniquely describe users' OHC participation intertwined with
illness contexts compared to existing social types in general online
communities. We discuss broader implications back to the literature and how our
findings apply to other illness contexts in OHCs. We end with requirements for
personalized delivery of large-scale OHC content.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.09208</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.09208</id><created>2015-06-30</created><updated>2015-09-11</updated><authors><author><keyname>Loiseau</keyname><forenames>Patrick</forenames></author><author><keyname>Wu</keyname><forenames>Xiaohu</forenames></author></authors><title>Improved Competitive Analysis of Online Scheduling Deadline-Sensitive
  Jobs</title><categories>cs.DS</categories><comments>further improvement</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the following scheduling problem. There is a single machine and
the jobs will arrive for completion online. Each job j is preemptive and, upon
its arrival, its other characteristics are immediately revealed to the machine:
the deadline requirement, the workload and the value. The objective is to
maximize the aggregate value of jobs completed by their deadlines. Using the
minimum of the ratios of deadline minus arrival time to workload over all jobs
as the slackness s, a non-committed and a committed online scheduling algorithm
is proposed in [Lucier et al., SPAA'13; Azar et al., EC'15], achieving
competitive ratios of 2+f(s), where the big O notation
f(s)=\mathcal{O}(\frac{1}{(\sqrt[3]{s}-1)^{2}}), and (2+f(s*b))/b respectively,
where b=\omega*(1-\omega), \omega is in (0, 1), and s is no less than 1/b. In
this paper, without recourse to the dual fitting technique used in the above
works, we propose a simpler and more intuitive analytical framework for the two
algorithms, improving the competitive ratio of the first algorithm by 1 and
therefore improving the competitive ratio of the second algorithm by 1/b. As
stated in [Lucier et al., SPAA'13; Azar et al. EC'15], it is justifiable in
scenarios like the online batch processing for cloud computing that the
slackness s is large, hence the big O notation in the above competitive ratios
can be ignored. Under the assumption, our analysis brings very significant
improvements to the competitive ratios of the two algorithms: from 2 to 1 and
from 2/b to 1/b respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.09210</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.09210</id><created>2015-06-30</created><updated>2016-01-24</updated><authors><author><keyname>Salhab</keyname><forenames>Rabih</forenames></author><author><keyname>Malham&#xe9;</keyname><forenames>Roland P.</forenames></author><author><keyname>Ny</keyname><forenames>Jerome Le</forenames></author></authors><title>A Dynamic Game Model of Collective Choice in Multi-Agent Systems</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by successful biological collective decision mechanisms such as
honey bees searching for a new colony or the collective navigation of fish
schools, we consider a mean field games (MFG)-like scenario where a large
number of agents have to make a choice among a set of different potential
target destinations. Each individual both influences and is influenced by the
group's decision, as well as the mean trajectory of all the agents. The model
can be interpreted as a stylized version of opinion crystallization in an
election for example. The agents' biases are dictated first by their initial
spatial position and, in a subsequent generalization of the model, by a
combination of initial position and a priori individual preference. The agents
have linear dynamics and are coupled through a modified form of quadratic cost.
Fixed point based finite population equilibrium conditions are identified and
associated existence conditions are established. In general multiple equilibria
may exist and the agents need to know all initial conditions to compute them
precisely. However, as the number of agents increases sufficiently, we show
that 1) the computed fixed point equilibria qualify as epsilon Nash equilibria,
2) agents no longer require all initial conditions to compute the equilibria
but rather can do so based on a representative probability distribution of
these conditions now viewed as random variables. Numerical results are
reported.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.09215</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.09215</id><created>2015-06-30</created><updated>2015-11-30</updated><authors><author><keyname>Alayrac</keyname><forenames>Jean-Baptiste</forenames></author><author><keyname>Bojanowski</keyname><forenames>Piotr</forenames></author><author><keyname>Agrawal</keyname><forenames>Nishant</forenames></author><author><keyname>Sivic</keyname><forenames>Josef</forenames></author><author><keyname>Laptev</keyname><forenames>Ivan</forenames></author><author><keyname>Lacoste-Julien</keyname><forenames>Simon</forenames></author></authors><title>Unsupervised Learning from Narrated Instruction Videos</title><categories>cs.CV cs.LG</categories><comments>improved NLP method and bigger dataset</comments><acm-class>I.5.1; I.5.4; I.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of automatically learning the main steps to complete a
certain task, such as changing a car tire, from a set of narrated instruction
videos. The contributions of this paper are three-fold. First, we develop a new
unsupervised learning approach that takes advantage of the complementary nature
of the input video and the associated narration. The method solves two
clustering problems, one in text and one in video, applied one after each other
and linked by joint constraints to obtain a single coherent sequence of steps
in both modalities. Second, we collect and annotate a new challenging dataset
of real-world instruction videos from the Internet. The dataset contains about
800,000 frames for five different tasks that include complex interactions
between people and objects, and are captured in a variety of indoor and outdoor
settings. Third, we experimentally demonstrate that the proposed method can
automatically discover, in an unsupervised manner, the main steps to achieve
the task and locate the steps in the input videos.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00019</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00019</id><created>2015-06-30</created><updated>2015-08-18</updated><authors><author><keyname>Wang</keyname><forenames>Jingyan</forenames></author><author><keyname>Zhou</keyname><forenames>Yihua</forenames></author><author><keyname>Yin</keyname><forenames>Ming</forenames></author><author><keyname>Chen</keyname><forenames>Shaochang</forenames></author><author><keyname>Edwards</keyname><forenames>Benjamin</forenames></author></authors><title>Representing data by sparse combination of contextual data points for
  classification</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the problem of using contextual da- ta points of a
data point for its classification problem. We propose to represent a data point
as the sparse linear reconstruction of its context, and learn the sparse
context to gather with a linear classifier in a su- pervised way to increase
its discriminative ability. We proposed a novel formulation for context
learning, by modeling the learning of context reconstruction coefficients and
classifier in a unified objective. In this objective, the reconstruction error
is minimized and the coefficient spar- sity is encouraged. Moreover, the hinge
loss of the classifier is minimized and the complexity of the classifier is
reduced. This objective is opti- mized by an alternative strategy in an
iterative algorithm. Experiments on three benchmark data set show its advantage
over state-of-the-art context-based data representation and classification
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00026</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00026</id><created>2015-06-30</created><updated>2015-08-07</updated><authors><author><keyname>Zhang</keyname><forenames>Qin</forenames></author></authors><title>On the Communication Complexity of Distributed Clustering</title><categories>cs.CC</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we give a first set of communication lower bounds for
distributed clustering problems, in particular, for k-center, k-median and
k-means. When the input is distributed across a large number of machines and
the number of clusters k is small, our lower bounds match the current best
upper bounds up to a logarithmic factor. We have designed a new composition
framework in our proofs for multiparty number-in-hand communication complexity
which may be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00029</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00029</id><created>2015-06-30</created><updated>2015-11-09</updated><authors><author><keyname>Zhu</keyname><forenames>Zhihui</forenames></author><author><keyname>Wakin</keyname><forenames>Michael B.</forenames></author></authors><title>Approximating Sampled Sinusoids and Multiband Signals Using Multiband
  Modulated DPSS Dictionaries</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many signal processing problems--such as analysis, compression, denoising,
and reconstruction--can be facilitated by expressing the signal as a linear
combination of atoms from a well-chosen dictionary. In this paper, we study
possible dictionaries for representing the discrete vector one obtains when
collecting a finite set of uniform samples from a multiband analog signal. By
analyzing the spectrum of combined time- and multiband-limiting operations in
the discrete-time domain, we conclude that the information level of the sampled
multiband vectors is essentially equal to the time-frequency area. For
representing these vectors, we consider a dictionary formed by concatenating a
collection of modulated Discrete Prolate Spheroidal Sequences (DPSS's). We
study the angle between the subspaces spanned by this dictionary and an optimal
dictionary, and we conclude that the multiband modulated DPSS dictionary--which
is simple to construct and more flexible than the optimal dictionary in
practical applications--is nearly optimal for representing multiband sample
vectors. We also show that the multiband modulated DPSS dictionary not only
provides a very high degree of approximation accuracy in an MSE sense for
multiband sample vectors (using a number of atoms comparable to the information
level), but also that it can provide high-quality approximations of all sampled
sinusoids within the bands of interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00039</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00039</id><created>2015-06-30</created><authors><author><keyname>Lee</keyname><forenames>Jason D.</forenames></author></authors><title>Selective Inference and Learning Mixed Graphical Models</title><categories>stat.ML cs.LG</categories><comments>Jason D. Lee PhD Dissertation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This thesis studies two problems in modern statistics. First, we study
selective inference, or inference for hypothesis that are chosen after looking
at the data. The motiving application is inference for regression coefficients
selected by the lasso. We present the Condition-on-Selection method that allows
for valid selective inference, and study its application to the lasso, and
several other selection algorithms.
  In the second part, we consider the problem of learning the structure of a
pairwise graphical model over continuous and discrete variables. We present a
new pairwise model for graphical models with both continuous and discrete
variables that is amenable to structure learning. In previous work, authors
have considered structure learning of Gaussian graphical models and structure
learning of discrete models. Our approach is a natural generalization of these
two lines of work to the mixed case. The penalization scheme involves a novel
symmetric use of the group-lasso norm and follows naturally from a particular
parametrization of the model. We provide conditions under which our estimator
is model selection consistent in the high-dimensional regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00043</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00043</id><created>2015-06-30</created><updated>2015-07-07</updated><authors><author><keyname>Nikolakopoulos</keyname><forenames>Athanasios N.</forenames></author><author><keyname>Garofalakis</keyname><forenames>John D.</forenames></author></authors><title>Top-N recommendations in the presence of sparsity: An NCD-based approach</title><categories>cs.IR cs.AI stat.ML</categories><comments>To appear in the Web Intelligence Journal as a regular paper</comments><doi>10.3233/WEB-150324</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Making recommendations in the presence of sparsity is known to present one of
the most challenging problems faced by collaborative filtering methods. In this
work we tackle this problem by exploiting the innately hierarchical structure
of the item space following an approach inspired by the theory of
Decomposability. We view the itemspace as a Nearly Decomposable system and we
define blocks of closely related elements and corresponding indirect proximity
components. We study the theoretical properties of the decomposition and we
derive sufficient conditions that guarantee full item space coverage even in
cold-start recommendation scenarios. A comprehensive set of experiments on the
MovieLens and the Yahoo!R2Music datasets, using several widely applied
performance metrics, support our model's theoretically predicted properties and
verify that NCDREC outperforms several state-of-the-art algorithms, in terms of
recommendation accuracy, diversity and sparseness insensitivity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00056</identifier>
 <datestamp>2015-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00056</id><created>2015-06-30</created><updated>2015-11-24</updated><authors><author><keyname>Sheffet</keyname><forenames>Or</forenames></author></authors><title>Private Approximations of the 2nd-Moment Matrix Using Existing
  Techniques in Linear Regression</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce three differentially-private algorithms that approximates the
2nd-moment matrix of the data. These algorithm, which in contrast to existing
algorithms output positive-definite matrices, correspond to existing techniques
in linear regression literature. Specifically, we discuss the following three
techniques. (i) For Ridge Regression, we propose setting the regularization
coefficient so that by approximating the solution using Johnson-Lindenstrauss
transform we preserve privacy. (ii) We show that adding a small batch of random
samples to our data preserves differential privacy. (iii) We show that sampling
the 2nd-moment matrix from a Bayesian posterior inverse-Wishart distribution is
differentially private provided the prior is set correctly. We also evaluate
our techniques experimentally and compare them to the existing &quot;Analyze Gauss&quot;
algorithm of Dwork et al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00066</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00066</id><created>2015-06-30</created><authors><author><keyname>Joulani</keyname><forenames>Pooria</forenames></author><author><keyname>Gy&#xf6;rgy</keyname><forenames>Andr&#xe1;s</forenames></author><author><keyname>Szepesv&#xe1;ri</keyname><forenames>Csaba</forenames></author></authors><title>Fast Cross-Validation for Incremental Learning</title><categories>stat.ML cs.AI cs.LG</categories><comments>Appearing in the International Joint Conference on Artificial
  Intelligence (IJCAI-2015), Buenos Aires, Argentina, July 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cross-validation (CV) is one of the main tools for performance estimation and
parameter tuning in machine learning. The general recipe for computing CV
estimate is to run a learning algorithm separately for each CV fold, a
computationally expensive process. In this paper, we propose a new approach to
reduce the computational burden of CV-based performance estimation. As opposed
to all previous attempts, which are specific to a particular learning model or
problem domain, we propose a general method applicable to a large class of
incremental learning algorithms, which are uniquely fitted to big data
problems. In particular, our method applies to a wide range of supervised and
unsupervised learning tasks with different performance criteria, as long as the
base learning algorithm is incremental. We show that the running time of the
algorithm scales logarithmically, rather than linearly, in the number of CV
folds. Furthermore, the algorithm has favorable properties for parallel and
distributed implementation. Experiments with state-of-the-art incremental
learning algorithms confirm the practicality of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00067</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00067</id><created>2015-06-30</created><updated>2016-01-29</updated><authors><author><keyname>Cooper</keyname><forenames>Jacob W.</forenames></author><author><keyname>Kaiser</keyname><forenames>Tom&#xe1;&#x161;</forenames></author><author><keyname>Kr&#xe1;&#x13e;</keyname><forenames>Daniel</forenames></author><author><keyname>Noel</keyname><forenames>Jonathan A.</forenames></author></authors><title>Weak regularity and finitely forcible graph limits</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graphons are analytic objects representing limits of convergent sequences of
graphs. Lov\'asz and Szegedy conjectured that every finitely forcible graphon,
i.e. any graphon determined by finitely many subgraph densities, has a simple
structure. In particular, one of their conjectures would imply that every
finitely forcible graphon has a weak $\varepsilon$-regular partition with the
number of parts bounded by a polynomial in $\varepsilon^{-1}$. We construct a
finitely forcible graphon $W$ such that the number of parts in any weak
$\varepsilon$-regular partition of $W$ is at least exponential in
$\varepsilon^{-2}/2^{5\log^*\varepsilon^{-2}}$. This bound almost matches the
known upper bound for graphs and, in a certain sense, is the best possible for
graphons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00071</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00071</id><created>2015-06-30</created><authors><author><keyname>Rakovic</keyname><forenames>Valentin</forenames></author><author><keyname>Denkovski</keyname><forenames>Daniel</forenames></author><author><keyname>Hadzi-Velkov</keyname><forenames>Zoran</forenames></author><author><keyname>Gavrilovska</keyname><forenames>Liljana</forenames></author></authors><title>Optimal time sharing in underlay cognitive radio systems with RF energy
  harvesting</title><categories>cs.IT math.IT</categories><comments>Proceedings of the 2015 IEEE International Conference on
  Communications (IEEE ICC 2015), 8-12 June 2015, London, UK</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the fundamental tradeoffs, achieving spectrum efficiency and energy
efficiency are two contending design challenges for the future wireless
networks. However, applying radio-frequency (RF) energy harvesting (EH) in a
cognitive radio system could potentially circumvent this tradeoff, resulting in
a secondary system with limitless power supply and meaningful achievable
information rates. This paper proposes an online solution for the optimal time
allocation (time sharing) between the EH phase and the information transmission
(IT) phase in an underlay cognitive radio system, which harvests the RF energy
originating from the primary system. The proposed online solution maximizes the
average achievable rate of the cognitive radio system, subject to the
$\varepsilon$-percentile protection criteria for the primary system. The
optimal time sharing achieves significant gains compared to equal time
allocation between the EH and IT phases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00073</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00073</id><created>2015-06-30</created><authors><author><keyname>Castaneda</keyname><forenames>Armando</forenames></author><author><keyname>Raynal</keyname><forenames>Michel</forenames></author><author><keyname>Rajsbaum</keyname><forenames>Sergio</forenames></author></authors><title>Specifying Concurrent Problems: Beyond Linearizability</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tasks and objects are two predominant ways of specifying distributed
problems. A task is specified by an input/output relation, defining for each
set of processes that may run concurrently, and each assignment of inputs to
the processes in the set, the valid outputs of the processes. An object is
specified by an automaton describing the outputs the object may produce when it
is accessed sequentially. Thus, tasks explicitly state what may happen only
when sets of processes run concurrently, while objects only specify what
happens when processes access the object sequentially. Each one requires its
own implementation notion, to tell when an execution satisfies the
specification. For objects linearizability is commonly used, a very elegant and
useful consistency condition. For tasks implementation notions are less
explored.
  The paper introduces the notion of interval-sequential object. The
corresponding implementation notion of interval-linearizability generalizes
linearizability, and allows to associate states along the interval of execution
of an operation. Interval-linearizability allows to specify any task, however,
there are sequential one-shot objects that cannot be expressed as tasks, under
the simplest interpretation of a task. It also shows that a natural extension
of the notion of a task is expressive enough to specify any interval-sequential
object.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00087</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00087</id><created>2015-06-30</created><authors><author><keyname>Oselio</keyname><forenames>Brandon</forenames></author><author><keyname>Kulesza</keyname><forenames>Alex</forenames></author><author><keyname>Hero</keyname><forenames>Alfred</forenames></author></authors><title>Information Extraction from Larger Multi-layer Social Networks</title><categories>cs.SI physics.soc-ph</categories><comments>2015 ICASSP</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social networks often encode community structure using multiple distinct
types of links between nodes. In this paper we introduce a novel method to
extract information from such multi-layer networks, where each type of link
forms its own layer. Using the concept of Pareto optimality, community
detection in this multi-layer setting is formulated as a multiple criterion
optimization problem. We propose an algorithm for finding an approximate Pareto
frontier containing a family of solutions. The power of this approach is
demonstrated on a Twitter dataset, where the nodes are hashtags and the layers
correspond to (1) behavioral edges connecting pairs of hashtags whose temporal
profiles are similar and (2) relational edges connecting pairs of hashtags that
appear in the same tweets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00088</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00088</id><created>2015-06-30</created><authors><author><keyname>Corriveau</keyname><forenames>Guillaume</forenames></author><author><keyname>Guilbault</keyname><forenames>Raynald</forenames></author><author><keyname>Tahan</keyname><forenames>Antoine</forenames></author><author><keyname>Sabourin</keyname><forenames>Robert</forenames></author></authors><title>Evaluation of Genotypic Diversity Measurements Exploited in Real-Coded
  Representation</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerous genotypic diversity measures (GDMs) are available in the literature
to assess the convergence status of an evolutionary algorithm (EA) or describe
its search behavior. In a recent study, the authors of this paper drew
attention to the need for a GDM validation framework. In response, this study
proposes three requirements (monotonicity in individual varieties, twinning,
and monotonicity in distance) that can clearly portray any GDMs. These
diversity requirements are analysed by means of controlled population
arrangements. In this paper four GDMs are evaluated with the proposed
validation framework. The results confirm that properly evaluating population
diversity is a rather difficult task, as none of the analysed GDMs complies
with all the diversity requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00090</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00090</id><created>2015-06-30</created><updated>2015-11-07</updated><authors><author><keyname>Ortigoza</keyname><forenames>Jammily</forenames></author><author><keyname>L&#xf3;pez-Pires</keyname><forenames>Fabio</forenames></author><author><keyname>Bar&#xe1;n</keyname><forenames>Benjam&#xed;n</forenames></author></authors><title>Workload Trace Generation for Dynamic Environments in Cloud Computing</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing datacenters provide millions of virtual machines in actual
cloud markets. In this context, Virtual Machine Placement (VMP) is one of the
most challenging problems in cloud infrastructure management, considering the
large number of possible optimization criteria and different formulations that
could be studied. Considering the on-demand model of cloud computing, the VMP
problem should be optimized dynamically to efficiently attend typical workload
of modern applications. This work proposes several dynamic environments for
solving the VMP from the providers' perspective based on the most relevant
dynamic parameters studied so far in the VMP literature. A complete set of
environments and workload traces examples are presented in this work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00091</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00091</id><created>2015-06-30</created><authors><author><keyname>Natarajan</keyname><forenames>Lakshmi</forenames></author><author><keyname>Hong</keyname><forenames>Yi</forenames></author><author><keyname>Viterbo</keyname><forenames>Emanuele</forenames></author></authors><title>Capacity of Coded Index Modulation</title><categories>cs.IT math.IT</categories><comments>To appear in Proc. IEEE Int. Symp. Inf. Theory (ISIT) 2015, Hong
  Kong, Jun. 2015. 5 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the special case of index coding over the Gaussian broadcast
channel where each receiver has prior knowledge of a subset of messages at the
transmitter and demands all the messages from the source. We propose a
concatenated coding scheme for this problem, using an index code for the
Gaussian channel as an inner code/modulation to exploit side information at the
receivers, and an outer code to attain coding gain against the channel noise.
We derive the capacity region of this scheme by viewing the resulting channel
as a multiple-access channel with many receivers, and relate it to the 'side
information gain' -- which is a measure of the advantage of a code in utilizing
receiver side information -- of the inner index code/modulation. We demonstrate
the utility of the proposed architecture by simulating the performance of an
index code/modulation concatenated with an off-the-shelf convolutional code
through bit-interleaved coded-modulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00092</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00092</id><created>2015-06-30</created><authors><author><keyname>Zawawi</keyname><forenames>Zati Bayani</forenames></author><author><keyname>Park</keyname><forenames>Jaehyun</forenames></author><author><keyname>Clerckx</keyname><forenames>Bruno</forenames></author></authors><title>Simultaneous Wireless Information and Power Transfer in a Two-User OFDM
  Interference Channel</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the Simultaneous Wireless Information and Power
Transfer (SWIPT) in a Single-Input Single-Output (SISO) two-user Orthogonal
Frequency Division Multiplexing (OFDM) Interference Channel (IFC). We assume
that the transmitters are non-cooperative and have perfect knowledge of the
local Channel State Information (CSI). We show that the necessary condition for
the optimal transmission strategy at high SNR is for the energy transmitter to
transmit its signal by allocating its transmit power on a single subcarrier.
Accordingly, we propose a one-subcarrier selection method for the energy
transmitter and identify the achievable rate-energy region. In addition, we
further enlarge the achievable rate-energy region by enabling a basic form of
transmitter cooperation where messages are exchanged to inform the energy
transmitter about the subcarriers unutilized by the information transmitter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00093</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00093</id><created>2015-06-30</created><authors><author><keyname>Prasad</keyname><forenames>H. L.</forenames></author><author><keyname>Bhatnagar</keyname><forenames>Shalabh</forenames></author></authors><title>A Study of Gradient Descent Schemes for General-Sum Stochastic Games</title><categories>cs.LG cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Zero-sum stochastic games are easy to solve as they can be cast as simple
Markov decision processes. This is however not the case with general-sum
stochastic games. A fairly general optimization problem formulation is
available for general-sum stochastic games by Filar and Vrieze [2004]. However,
the optimization problem there has a non-linear objective and non-linear
constraints with special structure. Since gradients of both the objective as
well as constraints of this optimization problem are well defined, gradient
based schemes seem to be a natural choice. We discuss a gradient scheme tuned
for two-player stochastic games. We show in simulations that this scheme indeed
converges to a Nash equilibrium, for a simple terrain exploration problem
modelled as a general-sum stochastic game. However, it turns out that only
global minima of the optimization problem correspond to Nash equilibria of the
underlying general-sum stochastic game, while gradient schemes only guarantee
convergence to local minima. We then provide important necessary conditions for
gradient schemes to converge to Nash equilibria in general-sum stochastic
games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00095</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00095</id><created>2015-06-30</created><authors><author><keyname>Im</keyname><forenames>Sanghun</forenames></author><author><keyname>Jeon</keyname><forenames>Hyoungsuk</forenames></author><author><keyname>Choi</keyname><forenames>Jinho</forenames></author><author><keyname>Ha</keyname><forenames>Jeongseok</forenames></author></authors><title>Secret Key Agreement with Large Antenna Arrays under the Pilot
  Contamination Attack</title><categories>cs.CR cs.IT math.IT</categories><comments>15 pages, 5 figures, and the paper is under minor revision for the
  publication in IEEE transactions on wireless communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a secret key agreement (SKA) protocol for a multi-user
time-division duplex system where a base-station (BS) with a large antenna
array (LAA) shares secret keys with users in the presence of non-colluding
eavesdroppers. In the system, when the BS transmits random sequences to
legitimate users for sharing common randomness, the eavesdroppers can attempt
the pilot contamination attack (PCA) in which each of eavesdroppers transmits
its target user's training sequence in hopes of acquiring possible information
leak by steering beam towards the eavesdropper. We show that there exists a
crucial complementary relation between the received signal strengths at the
eavesdropper and its target user. This relation tells us that the eavesdropper
inevitably leaves a trace that enables us to devise a way of measuring the
amount of information leakage to the eavesdropper even if PCA parameters are
unknown. To this end, we derive an estimator for the channel gain from the BS
to the eavesdropper and propose a rate-adaptation scheme for adjusting the
length of secret key under the PCA. Extensive analysis and evaluations are
carried out under various setups, which show that the proposed scheme
adequately takes advantage of the LAA to establish the secret keys under the
PCA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00101</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00101</id><created>2015-07-01</created><authors><author><keyname>Yang</keyname><forenames>Huei-Fang</forenames></author><author><keyname>Lin</keyname><forenames>Kevin</forenames></author><author><keyname>Chen</keyname><forenames>Chu-Song</forenames></author></authors><title>Supervised Learning of Semantics-Preserving Hashing via Deep Neural
  Networks for Large-Scale Image Search</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a supervised deep hashing approach that constructs binary
hash codes from labeled data for large-scale image search. We assume that
semantic labels are governed by a set of latent attributes in which each
attribute can be on or off, and classification relies on these attributes.
Based on this assumption, our approach, dubbed supervised semantics-preserving
deep hashing (SSDH), constructs hash functions as a latent layer in a deep
network in which binary codes are learned by the optimization of an objective
function defined over classification error and other desirable properties of
hash codes. With this design, SSDH has a nice property that classification and
retrieval are unified in a single learning model, and the learned binary codes
not only preserve the semantic similarity between images but also are efficient
for image search. Moreover, SSDH performs joint learning of image
representations, hash codes, and classification in a pointwised manner and thus
is naturally scalable to large-scale datasets. SSDH is simple and can be easily
realized by a slight modification of an existing deep architecture for
classification; yet it is effective and outperforms other unsupervised and
supervised hashing approaches on several benchmarks and one large dataset
comprising more than 1 million images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00110</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00110</id><created>2015-07-01</created><authors><author><keyname>Liu</keyname><forenames>Fang</forenames></author><author><keyname>Shi</keyname><forenames>Junfei</forenames></author><author><keyname>Jiao</keyname><forenames>Licheng</forenames></author><author><keyname>Liu</keyname><forenames>Hongying</forenames></author><author><keyname>Yang</keyname><forenames>Shuyuan</forenames></author><author><keyname>Wu</keyname><forenames>Jie</forenames></author><author><keyname>Hao</keyname><forenames>Hongxia</forenames></author><author><keyname>Yuan</keyname><forenames>Jialing</forenames></author></authors><title>Polarimetric Hierarchical Semantic Model and Scattering Mechanism Based
  PolSAR Image Classification</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For polarimetric SAR (PolSAR) image classification, it is a challenge to
classify the aggregated terrain types, such as the urban area, into semantic
homogenous regions due to sharp bright-dark variations in intensity. The
aggregated terrain type is formulated by the similar ground objects aggregated
together. In this paper, a polarimetric hierarchical semantic model (PHSM) is
firstly proposed to overcome this disadvantage based on the constructions of a
primal-level and a middle-level semantic. The primal-level semantic is a
polarimetric sketch map which consists of sketch segments as the sparse
representation of a PolSAR image. The middle-level semantic is a region map
which can extract semantic homogenous regions from the sketch map by exploiting
the topological structure of sketch segments. Mapping the region map to the
PolSAR image, a complex PolSAR scene is partitioned into aggregated, structural
and homogenous pixel-level subspaces with the characteristics of relatively
coherent terrain types in each subspace. Then, according to the characteristics
of three subspaces above, three specific methods are adopted, and furthermore
polarimetric information is exploited to improve the segmentation result.
Experimental results on PolSAR data sets with different bands and sensors
demonstrate that the proposed method is superior to the state-of-the-art
methods in region homogeneity and edge preservation for terrain classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00113</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00113</id><created>2015-07-01</created><authors><author><keyname>Akkutlu</keyname><forenames>I. Y.</forenames></author><author><keyname>Efendiev</keyname><forenames>Yalchin</forenames></author><author><keyname>Vasilyeva</keyname><forenames>Maria</forenames></author></authors><title>Multiscale model reduction for shale gas transport in fractured media</title><categories>cs.CE math.NA physics.comp-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we develop a multiscale model reduction technique that
describes shale gas transport in fractured media. Due to the pore-scale
heterogeneities and processes, we use upscaled models to describe the matrix.
We follow our previous work \cite{aes14}, where we derived an upscaled model in
the form of generalized nonlinear diffusion model to describe the effects of
kerogen. To model the interaction between the matrix and the fractures, we use
Generalized Multiscale Finite Element Method. In this approach, the matrix and
the fracture interaction is modeled via local multiscale basis functions. We
developed the GMsFEM and applied for linear flows with horizontal or vertical
fracture orientations on a Cartesian fine grid. In this paper, we consider
arbitrary fracture orientations and use triangular fine grid and developed
GMsFEM for nonlinear flows. Moreover, we develop online basis function
strategies to adaptively improve the convergence. The number of multiscale
basis functions in each coarse region represents the degrees of freedom needed
to achieve a certain error threshold. Our approach is adaptive in a sense that
the multiscale basis functions can be added in the regions of interest.
Numerical results for two-dimensional problem are presented to demonstrate the
efficiency of proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00121</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00121</id><created>2015-07-01</created><authors><author><keyname>Mizutaka</keyname><forenames>Shogo</forenames></author><author><keyname>Yakubo</keyname><forenames>Kousuke</forenames></author></authors><title>Robustness of scale-free networks to cascading failures induced by
  fluctuating loads</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>9 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Taking into account the fact that overload failures in real-world functional
networks are usually caused by extreme values of temporally fluctuating loads
that exceed the allowable range, we study the robustness of scale-free networks
against cascading overload failures induced by fluctuating loads. In our model,
loads are described by random walkers moving on a network and a node fails when
the number of walkers on the node is beyond the node capacity. Our results
obtained by using the generating function method shows that scale-free networks
are more robust against cascading overload failures than Erd\H{o}s-R\'enyi
random graphs with homogeneous degree distributions. This conclusion is
contrary to that predicted by previous works which neglect the effect of
fluctuations of loads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00126</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00126</id><created>2015-07-01</created><updated>2015-07-11</updated><authors><author><keyname>Sobkowicz</keyname><forenames>Pawel</forenames></author></authors><title>Breakdown of metastable political duopoly due to asymmetry of emotions
  in partisan propaganda</title><categories>physics.soc-ph cs.SI</categories><comments>Version 2 contains updated data on the actual party support, covering
  June-July polls, as well as an improved treatment of the political propaganda
  of the parties, treating separately the effects on the support base and
  outside it. A better qualitative agreement of the model with the observations
  is achieved, as well as a discussion of possible future outcomes</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present results of opinion dynamics simulations based on the
emotion/information/opinion (E/I/O) model, applied to a strongly polarized
society. Under certain conditions the model leads to metastable coexistence of
two subcommunities (supporting each of the opinions) of comparable size --
which corresponds to bipartisan split found in many real world communities.
Spurred by the recent breakdown of such system, which existed in Poland for
over 9 years, we extend the model by allowing a third opinion. We show that if
the propaganda messages of the two incumbent parties differ in emotional tone,
the system may be &quot;invaded&quot; by a newcomer third party very quickly -- in
qualitative agreement with the actual political situation in Poland in 2015.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00130</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00130</id><created>2015-07-01</created><authors><author><keyname>Goel</keyname><forenames>Gagan</forenames></author><author><keyname>Hajiaghayi</keyname><forenames>MohammadTaghi</forenames></author><author><keyname>Khani</keyname><forenames>Mohammad Reza</forenames></author></authors><title>Randomized Revenue Monotone Mechanisms for Online Advertising</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online advertising is the main source of revenue for many Internet firms. A
central component of online advertising is the underlying mechanism that
selects and prices the winning ads for a given ad slot. In this paper we study
designing a mechanism for the Combinatorial Auction with Identical Items (CAII)
in which we are interested in selling $k$ identical items to a group of bidders
each demanding a certain number of items between $1$ and $k$. CAII generalizes
important online advertising scenarios such as image-text and video-pod
auctions [GK14]. In image-text auction we want to fill an advertising slot on a
publisher's web page with either $k$ text-ads or a single image-ad and in
video-pod auction we want to fill an advertising break of $k$ seconds with
video-ads of possibly different durations.
  Our goal is to design truthful mechanisms that satisfy Revenue Monotonicity
(RM). RM is a natural constraint which states that the revenue of a mechanism
should not decrease if the number of participants increases or if a participant
increases her bid.
  [GK14] showed that no deterministic RM mechanism can attain PoRM of less than
$\ln(k)$ for CAII, i.e., no deterministic mechanism can attain more than
$\frac{1}{\ln(k)}$ fraction of the maximum social welfare. [GK14] also design a
mechanism with PoRM of $O(\ln^2(k))$ for CAII.
  In this paper, we seek to overcome the impossibility result of [GK14] for
deterministic mechanisms by using the power of randomization. We show that by
using randomization, one can attain a constant PoRM. In particular, we design a
randomized RM mechanism with PoRM of $3$ for CAII.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00131</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00131</id><created>2015-07-01</created><authors><author><keyname>Villacr&#xe9;s</keyname><forenames>Grace</forenames></author><author><keyname>Koch</keyname><forenames>Tobias</forenames></author></authors><title>Wireless networks of bounded capacity</title><categories>cs.IT math.IT</categories><comments>13 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a noncoherent wireless network, where the transmitters and
receivers are cognizant of the statistics of the fading coefficients, but are
ignorant of their realizations. We demonstrate that if the nodes do not
cooperate, if they transmit symbols that all follow the same distribution, and
if the variances of the fading coefficients decay exponentially or more slowly,
then the channel capacity is bounded in the SNR. This confirms a similar result
by Lozano, Heath, and Andrews.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00133</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00133</id><created>2015-07-01</created><authors><author><keyname>Borz&#xec;</keyname><forenames>Valeria</forenames></author><author><keyname>Faro</keyname><forenames>Simone</forenames></author><author><keyname>Pavone</keyname><forenames>Arianna</forenames></author><author><keyname>Sansone</keyname><forenames>Sabrina</forenames></author></authors><title>Prior Polarity Lexical Resources for the Italian Language</title><categories>cs.CL</categories><comments>10 pages, Accepted to NLPCS 2015, the 12th International Workshop on
  Natural Language Processing and Cognitive Science</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present SABRINA (Sentiment Analysis: a Broad Resource for
Italian Natural language Applications) a manually annotated prior polarity
lexical resource for Italian natural language applications in the field of
opinion mining and sentiment induction. The resource consists in two different
sets, an Italian dictionary of more than 277.000 words tagged with their prior
polarity value, and a set of polarity modifiers, containing more than 200
words, which can be used in combination with non neutral terms of the
dictionary in order to induce the sentiment of Italian compound terms. To the
best of our knowledge this is the first prior polarity manually annotated
resource which has been developed for the Italian natural language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00136</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00136</id><created>2015-07-01</created><updated>2015-12-04</updated><authors><author><keyname>Chen</keyname><forenames>Zhouye</forenames></author><author><keyname>Basarab</keyname><forenames>Adrian</forenames></author><author><keyname>Kouam&#xe9;</keyname><forenames>Denis</forenames></author></authors><title>Compressive Deconvolution in Medical Ultrasound Imaging</title><categories>cs.CV</categories><doi>10.1109/TMI.2015.2493241</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The interest of compressive sampling in ultrasound imaging has been recently
extensively evaluated by several research teams. Following the different
application setups, it has been shown that the RF data may be reconstructed
from a small number of measurements and/or using a reduced number of ultrasound
pulse emissions. Nevertheless, RF image spatial resolution, contrast and signal
to noise ratio are affected by the limited bandwidth of the imaging transducer
and the physical phenomenon related to US wave propagation. To overcome these
limitations, several deconvolution-based image processing techniques have been
proposed to enhance the ultrasound images. In this paper, we propose a novel
framework, named compressive deconvolution, that reconstructs enhanced RF
images from compressed measurements. Exploiting an unified formulation of the
direct acquisition model, combining random projections and 2D convolution with
a spatially invariant point spread function, the benefit of our approach is the
joint data volume reduction and image quality improvement. The proposed
optimization method, based on the Alternating Direction Method of Multipliers,
is evaluated on both simulated and in vivo data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00138</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00138</id><created>2015-07-01</created><updated>2015-09-29</updated><authors><author><keyname>Sinha</keyname><forenames>Atul Kumar</forenames></author><author><keyname>Chaturvedi</keyname><forenames>A. K.</forenames></author></authors><title>Low Complexity Opportunistic Interference Alignment in $K$-Transmitter
  MIMO Interference Channels</title><categories>cs.IT math.IT</categories><comments>8 pages, 8 figures, typos corrected, some clarifications added in
  'Performance Comparison'</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose low complexity opportunistic methods for
interference alignment in $K$-transmitter MIMO interference channels by
exploiting multiuser diversity. We do not assume availability of channel state
information (CSI) at the transmitters. Receivers are required to feed back
analog values indicating the extent to which the received interference
subspaces are aligned. The proposed opportunistic interference alignment (OIA)
achieves sum-rate comparable to conventional OIA schemes but with a
significantly reduced computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00142</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00142</id><created>2015-07-01</created><authors><author><keyname>Ge</keyname><forenames>Cunjing</forenames></author><author><keyname>Ma</keyname><forenames>Feifei</forenames></author><author><keyname>Zhang</keyname><forenames>Jian</forenames></author></authors><title>A Tool for Computing and Estimating the Volume of the Solution Space of
  SMT(LA)</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are already quite a few tools for solving the Satisfiability Modulo
Theories (SMT) problems. In this paper, we present \texttt{VolCE}, a tool for
counting the solutions of SMT constraints, or in other words, for computing the
volume of the solution space. Its input is essentially a set of Boolean
combinations of linear constraints, where the numeric variables are either all
integers or all reals, and each variable is bounded. The tool extends SMT
solving with integer solution counting and volume computation/estimation for
convex polytopes. Effective heuristics are adopted, which enable the tool to
deal with high-dimensional problem instances efficiently and accurately.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00151</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00151</id><created>2015-07-01</created><authors><author><keyname>Shi</keyname><forenames>Zuoqiang</forenames></author></authors><title>Convergence of Laplacian spectra from random samples</title><categories>cs.IT math.IT math.NA math.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Eigenvectors and eigenvalues of discrete graph Laplacians are often used for
manifold learning and nonlinear dimensionality reduction. It was previously
proved by Belkin and Niyogi that the eigenvectors and eigenvalues of the graph
Laplacian converge to the eigenfunctions and eigenvalues of the
Laplace-Beltrami operator of the manifold in the limit of infinitely many data
points sampled independently from the uniform distribution over the manifold.
Recently, we introduced Point Integral method (PIM) to solve elliptic equations
and corresponding eigenvalue problem on point clouds. We have established a
unified framework to approximate the elliptic differential operators on point
clouds. In this paper, we prove that the eigenvectors and eigenvalues obtained
by PIM converge in the limit of infinitely many random samples independently
from a distribution (not necessarily to be uniform distribution). Moreover, one
estimate of the rate of the convergence is also given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00154</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00154</id><created>2015-07-01</created><authors><author><keyname>Andersen</keyname><forenames>Jens Peter</forenames></author><author><keyname>Haustein</keyname><forenames>Stefanie</forenames></author></authors><title>Influence of study type on Twitter activity for medical research papers</title><categories>cs.DL</categories><comments>Presented at the 15th International Society on Scientometrics &amp;
  Informetrics (ISSI) Conference, 01 Jul 2015, Istanbul, Turkey</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Twitter has been identified as one of the most popular and promising
altmetrics data sources, as it possibly reflects a broader use of research
articles by the general public. Several factors, such as document age,
scientific discipline, number of authors and document type, have been shown to
affect the number of tweets received by scientific documents. The particular
meaning of tweets mentioning scholarly papers is, however, not entirely
understood and their validity as impact indicators debatable. This study
contributes to the understanding of factors influencing Twitter popularity of
medical papers investigating differences between medical study types. 162,830
documents indexed in Embase to a medical study type have been analysed for the
study type specific tweet frequency. Meta-analyses, systematic reviews and
clinical trials were found to be tweeted substantially more frequently than
other study types, while all basic research received less attention than the
average. The findings correspond well with clinical evidence hierarchies. It is
suggested that interest from laymen and patients may be a factor in the
observed effects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00159</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00159</id><created>2015-07-01</created><authors><author><keyname>Joroughi</keyname><forenames>Vahid</forenames></author><author><keyname>V&#xe1;zquez</keyname><forenames>Miguel &#xc1;ngel</forenames></author><author><keyname>P&#xe9;rez-Neira</keyname><forenames>Ana I.</forenames></author></authors><title>Precoding in Multigateway Multibeam Satellite Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a multigateway multibeam satellite system with multiple
feeds per beam. In these systems, each gateway serves a set of beams (cluster)
so that the overall data traffic is generated at different geographical areas.
Full frequency reuse among beams is considered so that interference mitigation
techniques are mandatory. Precisely, this paper aims at designing the precoding
scheme which, in contrast to single gateway schemes, entails two main
challenges. First, the precoding matrix shall be separated into feed groups
assigned to each gateway. Second, complete channel state information (CSI) is
required at each gateway, leading to a large communication overhead. In order
to solve these problems, a design based on a regularized singular value block
decomposition of the channel matrix is presented so that both inter-cluster
(i.e. beams of different clusters) and intra-cluster (i.e. beams of the same
cluster) interference is minimized. In addition, different gateway cooperative
schemes are analysed in order to keep the inter-gateway communication low.
Furthermore, the impact of the feeder link interference (i.e. interference
between different feeder links) is analysed and it is shown both numerically
and analytically that the system performance is reduced severally whenever this
interference occurs even though precoding reverts this additional interference.
Finally, numerical simulations are shown considering the latest fixed broadband
communication standard DVB-S2X so that the quantized feedback effect is
evaluated. The proposed precoding technique results to achieve a performance
close to the single gateway operation even when the cooperation among gateways
is low.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00163</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00163</id><created>2015-07-01</created><updated>2015-07-15</updated><authors><author><keyname>Cardelli</keyname><forenames>Luca</forenames></author><author><keyname>Tribastone</keyname><forenames>Mirco</forenames></author><author><keyname>Tschaikowski</keyname><forenames>Max</forenames></author><author><keyname>Vandin</keyname><forenames>Andrea</forenames></author></authors><title>Forward and Backward Bisimulations for Chemical Reaction Networks</title><categories>cs.LO</categories><comments>Extended version of the CONCUR 2015 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present two quantitative behavioral equivalences over species of a
chemical reaction network (CRN) with semantics based on ordinary differential
equations. Forward CRN bisimulation identifies a partition where each
equivalence class represents the exact sum of the concentrations of the species
belonging to that class. Backward CRN bisimulation relates species that have
the identical solutions at all time points when starting from the same initial
conditions. Both notions can be checked using only CRN syntactical information,
i.e., by inspection of the set of reactions. We provide a unified algorithm
that computes the coarsest refinement up to our bisimulations in polynomial
time. Further, we give algorithms to compute quotient CRNs induced by a
bisimulation. As an application, we find significant reductions in a number of
models of biological processes from the literature. In two cases we allow the
analysis of benchmark models which would be otherwise intractable due to their
memory requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00169</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00169</id><created>2015-07-01</created><authors><author><keyname>Wider</keyname><forenames>Nicolas</forenames></author><author><keyname>Garas</keyname><forenames>Antonios</forenames></author><author><keyname>Scholtes</keyname><forenames>Ingo</forenames></author><author><keyname>Schweitzer</keyname><forenames>Frank</forenames></author></authors><title>An ensemble perspective on multi-layer networks</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study properties of multi-layered, interconnected networks from an
ensemble perspective, i.e. we analyze ensembles of multi-layer networks that
share similar aggregate characteristics. Using a diffusive process that evolves
on a multi-layer network, we analyze how the speed of diffusion depends on the
aggregate characteristics of both intra- and inter-layer connectivity. Through
a block-matrix model representing the distinct layers, we construct transition
matrices of random walkers on multi-layer networks, and estimate expected
properties of multi-layer networks using a mean-field approach. In addition, we
quantify and explore conditions on the link topology that allow to estimate the
ensemble average by only considering aggregate statistics of the layers. Our
approach can be used when only partial information is available, like it is
usually the case for real-world multi-layer complex systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00176</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00176</id><created>2015-07-01</created><authors><author><keyname>Kajirunga</keyname><forenames>Alfred</forenames></author><author><keyname>Kalegele</keyname><forenames>Khamisi</forenames></author></authors><title>Analysis of Activities and Operations in the Current E-Health Landscape
  in Tanzania: Focus on Interoperability and Collaboration</title><categories>cs.CY</categories><comments>6 pages,2 figures</comments><journal-ref>IJCSIS, 13(6) (2015)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although the basic application of Information and Communication Technologies
(ICT) in the Tanzanian health care systems started years ago, still
fragmentation of Information Systems (IS) and limited interoperability remain
to be big challenges. In this paper, we present an analysis done on the present
health care delivery service, HIS and on some of existing eHealth solutions
focusing on interoperability and collaboration. Through interviews,
questionnaires and analysis on e-health implementations in relation to
interoperability and collaboration we have established that, the lack of
standard procedures to guide the lifecycle of eHealth systems across the health
sector and poor willingness to collaboration among health stakeholders are key
issues which hinders the manifestation of the benefit of ICT use in the health
sector of Tanzania. Based on the findings, we provide some recommendations with
a view to improve interoperability and collaboration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00177</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00177</id><created>2015-07-01</created><authors><author><keyname>Kumar</keyname><forenames>Mrinal</forenames></author><author><keyname>Saptharishi</keyname><forenames>Ramprasad</forenames></author></authors><title>An exponential lower bound for homogeneous depth-5 circuits over finite
  fields</title><categories>cs.CC</categories><acm-class>I.1.1; F.1.3</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we show exponential lower bounds for the class of homogeneous
depth-$5$ circuits over all small finite fields. More formally, we show that
there is an explicit family $\{P_d : d \in \mathbb{N}\}$ of polynomials in
$\mathsf{VNP}$, where $P_d$ is of degree $d$ in $n = d^{O(1)}$ variables, such
that over all finite fields $\mathbb{F}_q$, any homogeneous depth-$5$ circuit
which computes $P_d$ must have size at least $\exp(\Omega_q(\sqrt{d}))$.
  To the best of our knowledge, this is the first super-polynomial lower bound
for this class for any field $\mathbb{F}_q \neq \mathbb{F}_2$.
  Our proof builds up on the ideas developed on the way to proving lower bounds
for homogeneous depth-$4$ circuits [GKKS13, FLMS13, KLSS14, KS14] and for
non-homogeneous depth-$3$ circuits over finite fields [GK98, GR00]. Our key
insight is to look at the space of shifted partial derivatives of a polynomial
as a space of functions from $\mathbb{F}_q^n \rightarrow \mathbb{F}_q$ as
opposed to looking at them as a space of formal polynomials and builds over a
tighter analysis of the lower bound of Kumar and Saraf [KS14].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00182</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00182</id><created>2015-07-01</created><authors><author><keyname>Tamoor-ul-Hassan</keyname><forenames>Syed</forenames></author><author><keyname>Bennis</keyname><forenames>Mehdi</forenames></author><author><keyname>Nardelli</keyname><forenames>Pedro H. J.</forenames></author><author><keyname>Latva-Aho</keyname><forenames>Matti</forenames></author></authors><title>Modeling and Analysis of Content Caching in Wireless Small Cell Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>accepted for publication, IEEE ISWCS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network densification with small cell base stations is a promising solution
to satisfy future data traffic demands. However, increasing small cell base
station density alone does not ensure better users quality-of-experience and
incurs high operational expenditures. Therefore, content caching on different
network elements has been proposed as a mean of offloading he backhaul by
caching strategic contents at the network edge, thereby reducing latency. In
this paper, we investigate cache-enabled small cells in which we model and
characterize the outage probability, defined as the probability of not
satisfying users requests over a given coverage area. We analytically derive a
closed form expression of the outage probability as a function of
signal-to-interference ratio, cache size, small cell base station density and
threshold distance. By assuming the distribution of base stations as a Poisson
point process, we derive the probability of finding a specific content within a
threshold distance and the optimal small cell base station density that
achieves a given target cache hit probability. Furthermore, simulation results
are performed to validate the analytical model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00184</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00184</id><created>2015-07-01</created><authors><author><keyname>Laporte</keyname><forenames>Jonathan</forenames></author><author><keyname>Chaillet</keyname><forenames>Antoine</forenames></author><author><keyname>Chitour</keyname><forenames>Yacine</forenames></author></authors><title>Global stabilization of classes of linear control systems with bounds on
  the feedback and its successive derivatives</title><categories>cs.SY</categories><comments>arXiv admin note: substantial text overlap with arXiv:1503.06364</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the problem of globally stabilizing a linear
time-invariant (LTI) system by means of a static feedback law whose amplitude
and successive time derivatives, up to a prescribed order $p$, are bounded by
arbitrary prescribed values. We solve this problem for two classes of LTI
systems, namely integrator chains and skew-symmetric systems with single input.
For the integrator chains, the solution we propose is based on the nested
saturations introduced by A.R. Teel. We show that this construction fails for
skew-symmetric systems and propose an alternative feedback law. We illustrate
these findings by the stabilization of the third order integrator with
prescribed bounds on the feedback and its first two derivatives, and similarly
for the harmonic oscillator with prescribed bounds on the feedback and its
first derivative.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00201</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00201</id><created>2015-07-01</created><authors><author><keyname>Deleforge</keyname><forenames>Antoine</forenames></author><author><keyname>Gannot</keyname><forenames>Sharon</forenames></author><author><keyname>Kellermann</keyname><forenames>Walter</forenames></author></authors><title>Towards a Generalization of Relative Transfer Functions to More Than One
  Source</title><categories>cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a natural way to generalize relative transfer functions (RTFs) to
more than one source. We first prove that such a generalization is not possible
using a single multichannel spectro-temporal observation, regardless of the
number of microphones. We then introduce a new transform for multichannel
multi-frame spectrograms, i.e., containing several channels and time frames in
each time-frequency bin. This transform allows a natural generalization which
satisfies the three key properties of RTFs, namely, they can be directly
estimated from observed signals, they capture spatial properties of the sources
and they do not depend on emitted signals. Through simulated experiments, we
show how this new method can localize multiple simultaneously active sound
sources using short spectro-temporal windows, without relying on source
separation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00206</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00206</id><created>2015-07-01</created><authors><author><keyname>Charlier</keyname><forenames>Emilie</forenames></author><author><keyname>Leroy</keyname><forenames>Julien</forenames></author><author><keyname>Rigo</keyname><forenames>Michel</forenames></author></authors><title>Asymptotic properties of free monoid morphisms</title><categories>math.CO cs.DM cs.FL</categories><comments>25 pages</comments><msc-class>68R15, 15B36</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by applications in the theory of numeration systems and
recognizable sets of integers, this paper deals with morphic words when erasing
morphisms are taken into account. Cobham showed that if an infinite word $w
=g(f^\omega(a))$ is the image of a fixed point of a morphism $f$ under another
morphism $g$, then there exist a non-erasing morphism $\sigma$ and a coding
$\tau$ such that $w =\tau(\sigma^\omega(b))$.
  Based on the Perron theorem about asymptotic properties of powers of
non-negative matrices, our main contribution is an in-depth study of the growth
type of iterated morphisms when one replaces erasing morphisms with non-erasing
ones. We also explicitly provide an algorithm computing $\sigma$ and $\tau$
from $f$ and $g$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00209</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00209</id><created>2015-07-01</created><authors><author><keyname>Zhuge</keyname><forenames>Hai</forenames></author></authors><title>Dimensionality on Summarization</title><categories>cs.CL cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Summarization is one of the key features of human intelligence. It plays an
important role in understanding and representation. With rapid and continual
expansion of texts, pictures and videos in cyberspace, automatic summarization
becomes more and more desirable. Text summarization has been studied for over
half century, but it is still hard to automatically generate a satisfied
summary. Traditional methods process texts empirically and neglect the
fundamental characteristics and principles of language use and understanding.
This paper summarizes previous text summarization approaches in a
multi-dimensional classification space, introduces a multi-dimensional
methodology for research and development, unveils the basic characteristics and
principles of language use and understanding, investigates some fundamental
mechanisms of summarization, studies the dimensions and forms of
representations, and proposes a multi-dimensional evaluation mechanisms.
Investigation extends to the incorporation of pictures into summary and to the
summarization of videos, graphs and pictures, and then reaches a general
summarization framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00210</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00210</id><created>2015-07-01</created><authors><author><keyname>Desjardins</keyname><forenames>Guillaume</forenames></author><author><keyname>Simonyan</keyname><forenames>Karen</forenames></author><author><keyname>Pascanu</keyname><forenames>Razvan</forenames></author><author><keyname>Kavukcuoglu</keyname><forenames>Koray</forenames></author></authors><title>Natural Neural Networks</title><categories>stat.ML cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce Natural Neural Networks, a novel family of algorithms that speed
up convergence by adapting their internal representation during training to
improve conditioning of the Fisher matrix. In particular, we show a specific
example that employs a simple and efficient reparametrization of the neural
network weights by implicitly whitening the representation obtained at each
layer, while preserving the feed-forward computation of the network. Such
networks can be trained efficiently via the proposed Projected Natural Gradient
Descent algorithm (PRONG), which amortizes the cost of these reparametrizations
over many parameter updates and is closely related to the Mirror Descent online
learning algorithm. We highlight the benefits of our method on both
unsupervised and supervised learning tasks, and showcase its scalability by
training on the large-scale ImageNet Challenge dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00212</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00212</id><created>2015-07-01</created><authors><author><keyname>Paschke</keyname><forenames>Adrian</forenames></author><author><keyname>Schaefermeier</keyname><forenames>Ralph</forenames></author></authors><title>Aspect OntoMaven - Aspect-Oriented Ontology Development and
  Configuration With OntoMaven</title><categories>cs.SE</categories><comments>arXiv admin note: substantial text overlap with arXiv:1309.7341</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In agile ontology-based software engineering projects support for modular
reuse of ontologies from large existing remote repositories, ontology project
life cycle management, and transitive dependency management are important
needs. The contribution of this paper is a new design artifact called OntoMaven
combined with a unified approach to ontology modularization, aspect-oriented
ontology development, which was inspired by aspect-oriented programming.
OntoMaven adopts the Apache Maven-based development methodology and adapts its
concepts to knowledge engineering for Maven-based ontology development and
management of ontology artifacts in distributed ontology repositories. The
combination with aspect-oriented ontology development allows for fine-grained,
declarative configuration of ontology modules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00213</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00213</id><created>2015-07-01</created><updated>2015-10-07</updated><authors><author><keyname>Sikora</keyname><forenames>Jamie</forenames></author><author><keyname>Varvitsiotis</keyname><forenames>Antonios</forenames></author><author><keyname>Wei</keyname><forenames>Zhaohui</forenames></author></authors><title>On the minimum dimension of a Hilbert space needed to generate a quantum
  correlation</title><categories>quant-ph cs.CC cs.IT math.IT</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a two-party correlation that can be generated by performing local
measurements on a bipartite quantum system. A question of fundamental
importance is to understand how many resources, which we quantify by the
dimension of the underlying quantum system, are needed to reproduce this
correlation. In this paper, we identify an easy-to-compute lower bound on the
smallest Hilbert space dimension needed to generate a given two-party quantum
correlation. We show that our bound is tight on the correlations generated by
optimal quantum strategies for the CHSH Game and the Magic Square Game. We also
identify sufficient conditions for showing that a correlation cannot be
generated using finite-dimensional quantum strategies. Using this we give
alternative proofs that a family of PR-boxes and all correlations corresponding
to perfect strategies for the Fortnow-Feige-Lov\'asz Game cannot be realized
using finitely many qubits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00216</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00216</id><created>2015-07-01</created><authors><author><keyname>Veiga</keyname><forenames>Jorge Rodr&#xed;guez</forenames></author><author><keyname>Flores</keyname><forenames>Guido I. Novoa</forenames></author><author><keyname>M&#xe9;ndez</keyname><forenames>Balbina V. Casas</forenames></author></authors><title>Implementing generating functions to obtain power indices with coalition
  configuration</title><categories>math.OC cs.GT</categories><comments>14 pages, 1 Table</comments><msc-class>91A80</msc-class><acm-class>F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the Banzhaf-Coleman and Owen power indices for weighted majority
games modified by a coalition configuration. We present calculation algorithms
of them that make use of the method of generating functions. We programmed the
procedure in the open language R and it is illustrated by a real life example
taken from social sciences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00219</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00219</id><created>2015-07-01</created><authors><author><keyname>Oyaro</keyname><forenames>Denis</forenames></author><author><keyname>Triverio</keyname><forenames>Piero</forenames></author></authors><title>TurboMOR: an Efficient Model Order Reduction Technique for RC Networks
  with Many Ports</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model order reduction (MOR) techniques play a crucial role in the
computer-aided design of modern integrated circuits, where they are used to
reduce the size of parasitic networks. Unfortunately, the efficient reduction
of passive networks with many ports is still an open problem. Existing
techniques do not scale well with the number of ports, and lead to dense
reduced models that burden subsequent simulations. In this paper, we propose
TurboMOR, a novel MOR technique for the efficient reduction of passive RC
networks. TurboMOR is based on moment-matching, achieved through efficient
congruence transformations based on Householder reflections. A novel feature of
TurboMOR is the block-diagonal structure of the reduced models, that makes them
more efficient than the dense models produced by existing techniques. Moreover,
the model structure allows for an insightful interpretation of the reduction
process in terms of system theory. Numerical results show that TurboMOR scales
more favourably than existing techniques in terms of reduction time, simulation
time and memory consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00220</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00220</id><created>2015-07-01</created><authors><author><keyname>Cloninger</keyname><forenames>Alexander</forenames></author><author><keyname>Coifman</keyname><forenames>Ronald R.</forenames></author><author><keyname>Downing</keyname><forenames>Nicholas</forenames></author><author><keyname>Krumholz</keyname><forenames>Harlan M.</forenames></author></authors><title>Bigeometric Organization of Deep Nets</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we build an organization of high-dimensional datasets that
cannot be cleanly embedded into a low-dimensional representation due to missing
entries and a subset of the features being irrelevant to modeling functions of
interest. Our algorithm begins by defining coarse neighborhoods of the points
and defining an expected empirical function value on these neighborhoods. We
then generate new non-linear features with deep net representations tuned to
model the approximate function, and re-organize the geometry of the points with
respect to the new representation. Finally, the points are locally z-scored to
create an intrinsic geometric organization which is independent of the
parameters of the deep net, a geometry designed to assure smoothness with
respect to the empirical function. We examine this approach on data from the
Center for Medicare and Medicaid Services Hospital Quality Initiative, and
generate an intrinsic low-dimensional organization of the hospitals that is
smooth with respect to an expert driven function of quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00235</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00235</id><created>2015-07-01</created><authors><author><keyname>Mart&#xed;</keyname><forenames>Daniel</forenames></author><author><keyname>Rigotti</keyname><forenames>Mattia</forenames></author><author><keyname>Seok</keyname><forenames>Mingoo</forenames></author><author><keyname>Fusi</keyname><forenames>Stefano</forenames></author></authors><title>Energy-efficient neuromorphic classifiers</title><categories>q-bio.NC cs.NE</categories><comments>11 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neuromorphic engineering combines the architectural and computational
principles of systems neuroscience with semiconductor electronics, with the aim
of building efficient and compact devices that mimic the synaptic and neural
machinery of the brain. Neuromorphic engineering promises extremely low energy
consumptions, comparable to those of the nervous system. However, until now the
neuromorphic approach has been restricted to relatively simple circuits and
specialized functions, rendering elusive a direct comparison of their energy
consumption to that used by conventional von Neumann digital machines solving
real-world tasks. Here we show that a recent technology developed by IBM can be
leveraged to realize neuromorphic circuits that operate as classifiers of
complex real-world stimuli. These circuits emulate enough neurons to compete
with state-of-the-art classifiers. We also show that the energy consumption of
the IBM chip is typically 2 or more orders of magnitude lower than that of
conventional digital machines when implementing classifiers with comparable
performance. Moreover, the spike-based dynamics display a trade-off between
integration time and accuracy, which naturally translates into algorithms that
can be flexibly deployed for either fast and approximate classifications, or
more accurate classifications at the mere expense of longer running times and
higher energy costs. This work finally proves that the neuromorphic approach
can be efficiently used in real-world applications and it has significant
advantages over conventional digital devices when energy consumption is
considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00239</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00239</id><created>2015-07-01</created><authors><author><keyname>Chakraborty</keyname><forenames>Kaushik</forenames></author><author><keyname>Chailloux</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Leverrier</keyname><forenames>Anthony</forenames></author></authors><title>Arbitrarily long relativistic bit commitment</title><categories>quant-ph cs.CR</categories><comments>In an independent and concurrent work, Fehr and Fillinger [FF15]
  proved a general composition theorem for two-prover commitments which implies
  a similar bound on the security of the Lunghi et al. protocol than the one
  derived here</comments><journal-ref>Phys. Rev. Lett. 115, 250501 (2015)</journal-ref><doi>10.1103/PhysRevLett.115.250501</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the recent relativistic bit commitment protocol introduced by
Lunghi et al. [Phys. Rev. Lett. 2015] and present a new security analysis
against classical attacks. In particular, while the initial complexity of the
protocol scaled double-exponentially with the commitment time, our analysis
shows that the correct dependence is only linear. This has dramatic
implications in terms of implementation: in particular, the commitment time can
easily be made arbitrarily long, by only requiring both parties to communicate
classically and perform efficient classical computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00245</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00245</id><created>2015-07-01</created><authors><author><keyname>Stokkink</keyname><forenames>Quinten</forenames></author><author><keyname>Treep</keyname><forenames>Harmjan</forenames></author><author><keyname>Pouwelse</keyname><forenames>Johan</forenames></author></authors><title>Performance analysis of a Tor-like onion routing implementation</title><categories>cs.DC cs.CR</categories><comments>6 pages, 6 figures</comments><msc-class>68M14</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The current onion routing implementation of Tribler works as expected but
throttles the overall throughput of the Tribler system. This article discusses
a measuring procedure to reproducibly profile the tunnel implementation so
further optimizations of the tunnel community can be made. Our work has been
integrated into the Tribler eco-system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00248</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00248</id><created>2015-07-01</created><authors><author><keyname>L&#xf3;pez</keyname><forenames>Eduardo</forenames></author><author><keyname>Guerrero</keyname><forenames>Omar</forenames></author><author><keyname>Axtell</keyname><forenames>Robert L.</forenames></author></authors><title>The Network Picture of Labor Flow</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>27 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct a data-driven model of flows in graphs that captures the
essential elements of the movement of workers between jobs in the companies
(firms) of entire economic systems such as countries. The model is based on the
observation that certain job transitions between firms are often repeated over
time, showing persistent behavior, and suggesting the construction of static
graphs to act as the scaffolding for job mobility. Individuals in the job
market (the workforce) are modelled by a discrete-time random walk on graphs,
where each individual at a node can possess two states: employed or unemployed,
and the rates of becoming unemployed and of finding a new job are node
dependent parameters. We calculate the steady state solution of the model and
compare it to extensive micro-datasets for Mexico and Finland, comprised of
hundreds of thousands of firms and individuals. We find that our model
possesses the correct behavior for the numbers of employed and unemployed
individuals in these countries down to the level of individual firms. Our
framework opens the door to a new approach to the analysis of labor mobility at
high resolution, with the tantalizing potential for the development of full
forecasting methods in the future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00255</identifier>
 <datestamp>2015-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00255</id><created>2015-07-01</created><updated>2015-10-22</updated><authors><author><keyname>Ren</keyname><forenames>Jingjing</forenames></author><author><keyname>Rao</keyname><forenames>Ashwin</forenames></author><author><keyname>Lindorfer</keyname><forenames>Martina</forenames></author><author><keyname>Legout</keyname><forenames>Arnaud</forenames></author><author><keyname>Choffnes</keyname><forenames>David</forenames></author></authors><title>ReCon: Revealing and Controlling Privacy Leaks in Mobile Network Traffic</title><categories>cs.CR cs.NI</categories><comments>16 pages, recon.meddle.mobi</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that apps running on mobile devices extensively track and
leak users' personally identifiable information (PII); however, these users
have little visibility into PII leaked through the network traffic generated by
their devices, and have poor control over how, when and where that traffic is
sent and handled by third parties. In this paper, we present the design,
implementation, and evaluation of ReCon: a cross-platform system that reveals
PII leaks and gives users control over them without requiring any special
privileges or custom OSes. ReCon leverages machine learning to reveal potential
PII leaks by inspecting network traffic, and provides a visualization tool to
empower users with the ability to control these leaks via blocking or
substitution of PII. We evaluate ReCon's effectiveness with measurements from
controlled experiments using leaks from the 100 most popular iOS, Android, and
Windows Phone apps, and via an IRB-approved user study with 31 participants. We
show that ReCon is accurate, efficient, and identifies a wider range of PII
than previous approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00257</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00257</id><created>2015-07-01</created><authors><author><keyname>Salimi</keyname><forenames>Babak</forenames></author><author><keyname>Bertossi</keyname><forenames>Leopoldo</forenames></author></authors><title>From Causes for Database Queries to Repairs and Model-Based Diagnosis
  and Back</title><categories>cs.DB cs.AI cs.LO</categories><comments>Journal submission. Extended version of ICDT 2015 paper
  arXiv:1412.4311</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we establish and investigate connections between causes for
query answers in databases, database repairs wrt. denial constraints, and
consistency-based diagnosis. The first two are relatively new research areas in
databases, and the third one is an established subject in knowledge
representation. We show how to obtain database repairs from causes, and the
other way around. Causality problems are formulated as diagnosis problems, and
the diagnoses provide causes and their responsibilities. The vast body of
research on database repairs can be applied to the newer problems of computing
actual causes for query answers and their responsibilities. These connections,
which are interesting per se, allow us, after a transition -inspired by
consistency-based diagnosis- to computational problems on hitting sets and
vertex covers in hypergraphs, to obtain several new algorithmic and complexity
results for database causality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00267</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00267</id><created>2015-07-01</created><updated>2016-02-23</updated><authors><author><keyname>Steinerberger</keyname><forenames>Stefan</forenames></author></authors><title>A Hidden Signal in the Ulam sequence</title><categories>math.CO cs.DM math.NT</categories><comments>slightly extended, additional remarks and examples added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Ulam sequence is defined as $a_1 =1, a_2 = 2$ and $a_n$ being the
smallest integer that can be written as the sum of two distinct earlier
elements in a unique way. This gives $$1, 2, 3, 4, 6, 8, 11, 13, 16, 18, 26,
28, 36, 38, 47, \dots$$ Ulam remarked that understanding the sequence, which
has been described as 'quite erratic', seems difficult and indeed nothing is
known. We report the empirical discovery of a surprising global rigidity
phenomenon: there seems to exist a real $\alpha \sim 2.5714474995\dots$ such
that $$\left\{\alpha a_n: n\in \mathbb{N}\right\} \quad \mbox{mod}~2\pi \quad
\mbox{generates an absolutely continuous \textit{non-uniform} measure}$$
supported on a subset of $\mathbb{T}$. Indeed, for the first $10^7$ elements of
Ulam's sequence, $$ \cos{\left( 2.5714474995~ a_n\right)} &lt; 0 \qquad \mbox{for
all}~a_n \notin \left\{2, 3, 47, 69\right\}.$$ The same phenomenon arises for
some other initial conditions $a_1, a_2$: the distribution functions look very
different from each other and have curious shapes. A similar but more subtle
phenomenon seems to arise in Lagarias' variant of MacMahon's 'primes of
measurement' sequence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00270</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00270</id><created>2015-07-01</created><authors><author><keyname>Barbeau</keyname><forenames>Michel</forenames></author><author><keyname>Cloutier</keyname><forenames>Steve R.</forenames></author><author><keyname>Garcia-Alfaro</keyname><forenames>Joaquin</forenames></author></authors><title>Quantum Computing Assisted Medium Access Control for Multiple Client
  Station Networks</title><categories>cs.ET quant-ph</categories><comments>18 pages, 12 figures, 3 tables; manuscript under review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A medium access control protocol based on quantum entanglement has been
introduced by Berces and Imre (2006) and Van Meter (2012). This protocol
entirely avoids collisions. It is assumed that the network consists of one
access point and two client stations. We extend this scheme to a network with
an arbitrary number of client stations. We propose three approaches, namely,
the qubit distribution, transmit first election and temporal ordering
protocols. The qubit distribution protocol leverages the concepts of Bell-EPR
pair or W state triad. It works for networks of up to four CSs. With up to
three CSs, there is no probability of collision. In a four-CS network, there is
a low probability of collision. The transmit first election protocol and
temporal ordering protocols work for a network with any number of CSs. The
transmit first election builds upon the concept of W state of size
corresponding to the number of client stations. It is fair and collision free.
The temporal ordering protocol employs the concepts of Lehmer code and quantum
oracle. It is collision free, has a normalized throughput of 100% and achieves
quasi-fairness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00272</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00272</id><created>2015-07-01</created><updated>2016-01-11</updated><authors><author><keyname>Noferini</keyname><forenames>Vanni</forenames></author><author><keyname>Townsend</keyname><forenames>Alex</forenames></author></authors><title>Numerical instability of resultant methods for multidimensional
  rootfinding</title><categories>math.NA cs.NA</categories><comments>24 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hidden-variable resultant methods are a class of algorithms for solving
multidimensional polynomial rootfinding problems. In two dimensions, when
significant care is taken, they are competitive practical rootfinders. However,
in higher dimensions they are known to miss zeros, calculate roots to low
precision, and introduce spurious solutions. We show that the hidden variable
resultant method based on the Cayley (Dixon or B\'ezout) matrix is inherently
and spectacularly numerically unstable by a factor that grows exponentially
with the dimension. We also show that the Sylvester matrix for solving
bivariate polynomial systems can square the condition number of the problem. In
other words, two popular hidden variable resultant methods are numerically
unstable, and this mathematically explains the difficulties that are frequently
reported by practitioners. Regardless of how the constructed polynomial
eigenvalue problem is solved, severe numerical difficulties will be present.
Along the way, we prove that the Cayley resultant is a generalization of
Cramer's rule for solving linear systems and generalize Clenshaw's algorithm to
an evaluation scheme for polynomials expressed in a degree-graded polynomial
basis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00276</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00276</id><created>2015-07-01</created><authors><author><keyname>Warriach</keyname><forenames>Ehsan Ullah</forenames></author><author><keyname>Ozcelebi</keyname><forenames>Tanir</forenames></author><author><keyname>Lukkien</keyname><forenames>Johan J.</forenames></author></authors><title>Proactive Dependability Framework for Smart Environment Applications</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Smart environment applications demand novel solutions for managing quality of
services, especially availability and reliability at run-time. The underlying
systems are changing dynamically due to addition and removal of system
components, changing execution environments, and resources depletion.
Therefore, in such dynamic systems, the functionality and the performance of
smart environment applications can be hampered by faults. In this paper, we
follow a proactive approach to anticipate system state at runtime. We present a
proactive dependability framework to prevent faults at runtime based on
predictive analysis to increase availability and reliability of smart
environment applications, and reduce manual user interventions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00280</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00280</id><created>2015-07-01</created><authors><author><keyname>Hallac</keyname><forenames>David</forenames></author><author><keyname>Leskovec</keyname><forenames>Jure</forenames></author><author><keyname>Boyd</keyname><forenames>Stephen</forenames></author></authors><title>Network Lasso: Clustering and Optimization in Large Graphs</title><categories>cs.SI math.OC stat.AP stat.ME</categories><acm-class>H.2.8</acm-class><doi>10.1145/2783258.2783313</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convex optimization is an essential tool for modern data analysis, as it
provides a framework to formulate and solve many problems in machine learning
and data mining. However, general convex optimization solvers do not scale
well, and scalable solvers are often specialized to only work on a narrow class
of problems. Therefore, there is a need for simple, scalable algorithms that
can solve many common optimization problems. In this paper, we introduce the
\emph{network lasso}, a generalization of the group lasso to a network setting
that allows for simultaneous clustering and optimization on graphs. We develop
an algorithm based on the Alternating Direction Method of Multipliers (ADMM) to
solve this problem in a distributed and scalable manner, which allows for
guaranteed global convergence even on large graphs. We also examine a
non-convex extension of this approach. We then demonstrate that many types of
problems can be expressed in our framework. We focus on three in particular -
binary classification, predicting housing prices, and event detection in time
series data - comparing the network lasso to baseline approaches and showing
that it is both a fast and accurate method of solving large optimization
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00287</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00287</id><created>2015-07-01</created><authors><author><keyname>Ghauch</keyname><forenames>Hadi</forenames></author><author><keyname>Kim</keyname><forenames>Taejoon</forenames></author><author><keyname>Bengtsson</keyname><forenames>Mats</forenames></author><author><keyname>Skoglund</keyname><forenames>Mikael</forenames></author></authors><title>Subspace Estimation and Decomposition for Large Millimeter-Wave MIMO
  systems</title><categories>cs.IT math.IT</categories><comments>journal, 13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Channel estimation and precoding in hybrid analog-digital millimeter-wave
(mmWave) MIMO systems is a fundamental problem that has yet to be addressed,
before any of the promised gains can be harnessed. For that matter, we propose
a method (based on the well-known Arnoldi iteration) exploiting channel
reciprocity in TDD systems and the sparsity of the channel's eigenmodes, to
estimate the right (resp. left) singular subspaces of the channel, at the BS
(resp. MS). We first describe the algorithm in the context of conventional MIMO
systems, and derive bounds on the estimation error in the presence of
distortions at both BS and MS. We later identify obstacles that hinder the
application of such an algorithm to the hybrid analog-digital architecture, and
address them individually. In view of fulfilling the constraints imposed by the
hybrid analog-digital architecture, we further propose an iterative algorithm
for subspace decomposition, whereby the above estimated subspaces, are
approximated by a cascade of analog and digital precoder / combiner. Finally,
we evaluate the performance of our scheme against the perfect CSI, fully
digital case (i.e., an equivalent conventional MIMO system), and conclude that
similar performance can be achieved, especially at medium-to-high SNR (where
the performance gap is less than 5%), however, with a drastically lower number
of RF chains (4 to 8 times less).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00300</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00300</id><created>2015-07-01</created><authors><author><keyname>Osband</keyname><forenames>Ian</forenames></author><author><keyname>Van Roy</keyname><forenames>Benjamin</forenames></author></authors><title>Bootstrapped Thompson Sampling and Deep Exploration</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This technical note presents a new approach to carrying out the kind of
exploration achieved by Thompson sampling, but without explicitly maintaining
or sampling from posterior distributions. The approach is based on a bootstrap
technique that uses a combination of observed and artificially generated data.
The latter serves to induce a prior distribution which, as we will demonstrate,
is critical to effective exploration. We explain how the approach can be
applied to multi-armed bandit and reinforcement learning problems and how it
relates to Thompson sampling. The approach is particularly well-suited for
contexts in which exploration is coupled with deep learning, since in these
settings, maintaining or generating samples from a posterior distribution
becomes computationally infeasible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00302</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00302</id><created>2015-07-01</created><authors><author><keyname>Mori</keyname><forenames>Greg</forenames></author><author><keyname>Pantofaru</keyname><forenames>Caroline</forenames></author><author><keyname>Kothari</keyname><forenames>Nisarg</forenames></author><author><keyname>Leung</keyname><forenames>Thomas</forenames></author><author><keyname>Toderici</keyname><forenames>George</forenames></author><author><keyname>Toshev</keyname><forenames>Alexander</forenames></author><author><keyname>Yang</keyname><forenames>Weilong</forenames></author></authors><title>Pose Embeddings: A Deep Architecture for Learning to Match Human Poses</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method for learning an embedding that places images of humans in
similar poses nearby. This embedding can be used as a direct method of
comparing images based on human pose, avoiding potential challenges of
estimating body joint positions. Pose embedding learning is formulated under a
triplet-based distance criterion. A deep architecture is used to allow learning
of a representation capable of making distinctions between different poses.
Experiments on human pose matching and retrieval from video data demonstrate
the potential of the method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00304</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00304</id><created>2015-07-01</created><authors><author><keyname>Dolgov</keyname><forenames>Maxim</forenames></author><author><keyname>Hanebeck</keyname><forenames>Uwe D.</forenames></author></authors><title>Infinite-horizon Linear Optimal Control of Markov Jump Systems without
  Mode Observation via State Feedback</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider stochastic optimal control of Markov Jump Linear
Systems with state feedback but without observation of the jumping parameter.
The proposed control law is assumed to be linear with constant gains that can
be obtained from the necessary optimality conditions using an iterative
algorithm. The proposed approach is demonstrated in a numerical example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00310</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00310</id><created>2015-07-01</created><updated>2015-10-02</updated><authors><author><keyname>Chavalarias</keyname><forenames>David</forenames><affiliation>CAMS</affiliation></author></authors><title>Rencontre improbable entre von Foerster et Snowden</title><categories>cs.CY</categories><comments>in French</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although ICT have created hope for a shared pluralistic world, democratic
principles are far from being respected in the public digital environment, and
require a detailed knowledge of the laws by which they are governed. Von
Foerster's conjecture is one of the early theoretical results that could help
to understand these laws. Although neglected since a long time, the advent of
the overlying layer of recommendation and ranking mechanisms which is
progressively occupying the Web has given empirical evidences of this
conjecture, which predicts the consequences of increasing inter-individual
influences on social dynamics and the susceptibility of these latter to
manipulation. With both von Foerster's conjecture and the Snowden revelations
in the background, we analyse the impact of ICT on human societies and their
governance, in view of the fact that they have a massive impact on the way in
which people influence each other in their tastes and actions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00313</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00313</id><created>2015-07-01</created><authors><author><keyname>Amjad</keyname><forenames>Wasim</forenames></author><author><keyname>Garc&#xed;a</keyname><forenames>Javier</forenames></author><author><keyname>Munir</keyname><forenames>Jawad</forenames></author><author><keyname>Mezghani</keyname><forenames>Amine</forenames></author><author><keyname>Nossek</keyname><forenames>Josef A.</forenames></author></authors><title>Compensation of Amplifier Distortion for OFDM signals based on Iterative
  Hard Thresholding</title><categories>cs.IT math.IT</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The mitigation of nonlinear distortion caused by power amplifiers (PA) in
Orthogonal Frequency Division Multiplexing (OFDM) systems is an essential issue
to enable energy efficient operation. In this work we proposed a new algorithm
for receiver-based clipping estimation in OFDM systems that combines the
Iterative Hard Thresholding (IHT) method with a weighting corresponding to the
estimated probability of clipping. Thereby a more general amplifier
input-output characteristic is considered, which is assumed to be unknown at
the receiver side. Further, we avoid the use of dedicated subcarriers and
formulate the recovery problem solely on reliably detected sub-carriers.
Through simulations, we show that the proposed technique achieves a better
complexity-performance tradeoff compared to existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00315</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00315</id><created>2015-07-01</created><updated>2015-07-20</updated><authors><author><keyname>Assarpour</keyname><forenames>Ali</forenames></author><author><keyname>Barnoy</keyname><forenames>Amotz</forenames></author><author><keyname>Liu</keyname><forenames>Ou</forenames></author></authors><title>Counting the Number of Langford Skolem Pairings</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We compute the exact number, L(n), of solutions to the Langford pairings
problem for any positive integer n&lt;29 and the exact number of solutions to the
Nickerson variant of the problem, N(n), for any positive integer n&lt;26. These
numbers correspond to the sequences A014552, A059106 in Sloane's Online
Encyclopedia of Integer Sequences. The exact value of these numbers were known
for any positive integer n&lt;27 for the A014552 sequence and for any positive
integer n&lt;24 for the A059106 sequence. First we report that the number of
Langford pairings for n=27 is L(27)=111,683,611,098,764,903,232, and for n=28
it is L(28)=1,607,383,260,609,382,393,152. Next we report that the number of
solutions for the Nickerson variant of Langford pairings for n=24 is
N(24)=102,388,058,845,620,672 and for n=25 it is
N(25)=1,317,281,759,888,482,688.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00317</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00317</id><created>2015-07-01</created><updated>2015-11-04</updated><authors><author><keyname>Lu</keyname><forenames>Wei</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Lakshmanan</keyname><forenames>Laks V. S.</forenames></author></authors><title>From Competition to Complementarity: Comparative Influence Diffusion and
  Maximization</title><categories>cs.SI physics.soc-ph</categories><comments>An abridged of this work is to appear in the Proceedings of VLDB
  Endowment (PVDLB), Vol 9, No 2. Also, the paper will be presented in the VLDB
  2016 conference in New Delhi, India. This update contains new theoretical and
  experimental results, and the paper is now in single-column format (44 pages)</comments><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Influence maximization is a well-studied problem that asks for a small set of
influential users from a social network, such that by targeting them as early
adopters, the expected total adoption through influence cascades over the
network is maximized. However, almost all prior work focuses on cascades of a
single propagating entity or purely-competitive entities. In this work, we
propose the Comparative Independent Cascade (Com-IC) model that covers the full
spectrum of entity interactions from competition to complementarity. In Com-IC,
users' adoption decisions depend not only on edge-level information
propagation, but also on a node-level automaton whose behavior is governed by a
set of model parameters, enabling our model to capture not only competition,
but also complementarity, to any possible degree. We study two natural
optimization problems, Self Influence Maximization and Complementary Influence
Maximization, in a novel setting with complementary entities. Both problems are
NP-hard, and we devise efficient and effective approximation algorithms via
non-trivial techniques based on reverse-reachable sets and a novel &quot;sandwich
approximation&quot;. The applicability of both techniques extends beyond our model
and problems. Our experiments show that the proposed algorithms consistently
outperform intuitive baselines in four real-world social networks, often by a
significant margin. In addition, we learn model parameters from real user
action logs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00333</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00333</id><created>2015-06-30</created><updated>2015-10-26</updated><authors><author><keyname>Lu</keyname><forenames>Yuan</forenames></author><author><keyname>Yang</keyname><forenames>Jie</forenames></author></authors><title>Notes on Low-rank Matrix Factorization</title><categories>cs.NA cs.IR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low-rank matrix factorization (MF) is an important technique in data science.
The key idea of MF is that there exists latent structures in the data, by
uncovering which we could obtain a compressed representation of the data. By
factorizing an original matrix to low-rank matrices, MF provides a unified
method for dimension reduction, clustering, and matrix completion. In this
article we review several important variants of MF, including: Basic MF,
Non-negative MF, Orthogonal non-negative MF. As can be told from their names,
non-negative MF and orthogonal non-negative MF are variants of basic MF with
non-negativity and/or orthogonality constraints. Such constraints are useful in
specific senarios. In the first part of this article, we introduce, for each of
these models, the application scenarios, the distinctive properties, and the
optimizing method. By properly adapting MF, we can go beyond the problem of
clustering and matrix completion. In the second part of this article, we will
extend MF to sparse matrix compeletion, enhance matrix compeletion using
various regularization methods, and make use of MF for (semi-)supervised
learning by introducing latent space reinforcement and transformation. We will
see that MF is not only a useful model but also as a flexible framework that is
applicable for various prediction problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00353</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00353</id><created>2015-07-01</created><authors><author><keyname>van Seijen</keyname><forenames>Harm</forenames></author><author><keyname>Mahmood</keyname><forenames>A. Rupam</forenames></author><author><keyname>Pilarski</keyname><forenames>Patrick M.</forenames></author><author><keyname>Sutton</keyname><forenames>Richard S.</forenames></author></authors><title>An Empirical Evaluation of True Online TD({\lambda})</title><categories>cs.AI cs.LG stat.ML</categories><comments>European Workshop on Reinforcement Learning (EWRL) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The true online TD({\lambda}) algorithm has recently been proposed (van
Seijen and Sutton, 2014) as a universal replacement for the popular
TD({\lambda}) algorithm, in temporal-difference learning and reinforcement
learning. True online TD({\lambda}) has better theoretical properties than
conventional TD({\lambda}), and the expectation is that it also results in
faster learning. In this paper, we put this hypothesis to the test.
Specifically, we compare the performance of true online TD({\lambda}) with that
of TD({\lambda}) on challenging examples, random Markov reward processes, and a
real-world myoelectric prosthetic arm. We use linear function approximation
with tabular, binary, and non-binary features. We assess the algorithms along
three dimensions: computational cost, learning speed, and ease of use. Our
results confirm the strength of true online TD({\lambda}): 1) for sparse
feature vectors, the computational overhead with respect to TD({\lambda}) is
minimal; for non-sparse features the computation time is at most twice that of
TD({\lambda}), 2) across all domains/representations the learning speed of true
online TD({\lambda}) is often better, but never worse than that of
TD({\lambda}), and 3) true online TD({\lambda}) is easier to use, because it
does not require choosing between trace types, and it is generally more stable
with respect to the step-size. Overall, our results suggest that true online
TD({\lambda}) should be the first choice when looking for an efficient,
general-purpose TD method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00365</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00365</id><created>2015-07-01</created><authors><author><keyname>Nikulchev</keyname><forenames>E.</forenames></author><author><keyname>Pluzhnik</keyname><forenames>E.</forenames></author><author><keyname>Biryukov</keyname><forenames>D.</forenames></author><author><keyname>Lukyanchikov</keyname><forenames>O.</forenames></author><author><keyname>Payain</keyname><forenames>S.</forenames></author></authors><title>Experimental Study of the Cloud Architecture Selection for Effective Big
  Data Processing</title><categories>cs.DC</categories><journal-ref>IJACSA 6 (2015) 22-26</journal-ref><doi>10.14569/IJACSA.2015.060603</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Big data dictate their requirements to the hardware and software. Simple
migration to the cloud data processing, while solving the problem of increasing
computational capabilities, however creates some issues: the need to ensure the
safety, the need to control the quality during data transmission, the need to
optimize requests. Computational cloud does not simply provide scalable
resources but also requires network infrastructure, unknown routes and the
number of user requests. In addition, during functioning situation can occur,
in which you need to change the architecture of the application - part of the
data needs to be placed in a private cloud, part in a public cloud, part stays
on the client.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00379</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00379</id><created>2015-07-01</created><authors><author><keyname>Jung</keyname><forenames>Sang Yeob</forenames></author><author><keyname>Kim</keyname><forenames>Seong-Lyun</forenames></author></authors><title>Bidding, Pricing, and User Subscription Dynamics in Asymmetric-valued
  Korean LTE Spectrum Auction: A Hierarchical Dynamic Game Approach</title><categories>cs.NI</categories><comments>10 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The tremendous increase in mobile data traffic coupled with fierce
competition in wireless industry brings about spectrum scarcity and bandwidth
fragmentation. This inevitably results in asymmetric-valued LTE spectrum
allocation that stems from different timing for twice improvement in capacity
between competing operators, given spectrum allocations today. This motivates
us to study the economic effects of asymmetric-valued LTE spectrum allocation.
In this paper, we formulate the interactions between operators and users as a
hierarchical dynamic game framework, where two spiteful operators
simultaneously make spectrum acquisition decisions in the upper-level
first-price sealed-bid auction game, and dynamic pricing decisions in the
lower-level differential game, taking into account user subscription dynamics.
Using backward induction, we derive the equilibrium of the entire game under
mild conditions. Through analytical and numerical results, we verify our
studies by comparing the latest result of LTE spectrum auction in South Korea,
which serves as the benchmark of asymmetric-valued LTE spectrum auction
designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00385</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00385</id><created>2015-07-01</created><authors><author><keyname>Vazou</keyname><forenames>Niki</forenames></author><author><keyname>Bakst</keyname><forenames>Alexander</forenames></author><author><keyname>Jhala</keyname><forenames>Ranjit</forenames></author></authors><title>Bounded Refinement Types</title><categories>cs.PL cs.SE</categories><comments>14 pages, International Conference on Functional Programming, ICFP
  2015</comments><acm-class>D.2.4; D.3.3; F.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a notion of bounded quantification for refinement types and show
how it expands the expressiveness of refinement typing by using it to develop
typed combinators for: (1) relational algebra and safe database access, (2)
Floyd-Hoare logic within a state transformer monad equipped with combinators
for branching and looping, and (3) using the above to implement a refined IO
monad that tracks capabilities and resource usage. This leap in expressiveness
comes via a translation to &quot;ghost&quot; functions, which lets us retain the
automated and decidable SMT based checking and inference that makes refinement
typing effective in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00387</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00387</id><created>2015-07-01</created><updated>2016-03-08</updated><authors><author><keyname>Mezzavilla</keyname><forenames>Marco</forenames></author><author><keyname>Goyal</keyname><forenames>Sanjay</forenames></author><author><keyname>Panwar</keyname><forenames>Shivendra</forenames></author><author><keyname>Rangan</keyname><forenames>Sundeep</forenames></author><author><keyname>Zorzi</keyname><forenames>Michele</forenames></author></authors><title>An MDP Model for Optimal Handover Decisions in mmWave Cellular Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The new frontier in cellular networks is harnessing the enormous spectrum
available in millimeter wave (mmWave) frequencies above 28 GHz. The challenging
radio propagation characteristics at these frequencies, and the use of highly
directional beamforming, lead to intermittent links between the base station
(BS) and the user equipment (UE). In this paper, we revisit the problem of cell
selection to maintain an acceptable level of service, despite the underlying
intermittent link connectivity typical of mmWave links. We propose a Markov
Decision Processe (MDP) framework to study the properties and performance of
our proposed cell selection strategy, which jointly considers several factors
such as dynamic channel load and link quality. We use the Value Iteration
Algorithm (VIA) to solve the MDP, and obtain the optimal set of associations.
We address the multi user problem through a distributed iterative approach, in
which each UE characterizes the evolution of the system based on stationary
channel distribution and cell selection statistics of other UEs. Through
simulation results, we show that our proposed technique makes judicious handoff
choices, thereby providing a significant improvement in the overall network
capacity. Further, our technique reduces the total number of handoffs, thus
lowering the signaling overhead, while providing a higher quality of service to
the UEs. We believe that this work takes us a step closer toward harnessing the
enormous spectrum available in mmWave cellular networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00389</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00389</id><created>2015-07-01</created><updated>2015-07-20</updated><authors><author><keyname>Ahmad</keyname><forenames>Nasir</forenames></author><author><keyname>Derrible</keyname><forenames>Sybil</forenames></author><author><keyname>Eason</keyname><forenames>Tarsha</forenames></author><author><keyname>Cabezas</keyname><forenames>Heriberto</forenames></author></authors><title>Using Fisher Information In Big Data</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this era of Big Data, proficient use of data mining is the key to capture
useful information from any dataset. As numerous data mining techniques make
use of information theory concepts, in this paper, we discuss how Fisher
information (FI) can be applied to analyze patterns in Big Data. The main
advantage of FI is its ability to combine multiple variables together to inform
us on the overall trends and stability of a system. It can therefore detect
whether a system is losing dynamic order and stability, which may serve as a
signal of an impending regime shift. In this work, we first provide a brief
overview of Fisher information theory, followed by a simple step-by-step
numerical example on how to compute FI. Finally, as a numerical demonstration,
we calculate the evolution of FI for GDP per capita (current US Dollar) and
total population of the USA from 1960 to 2013.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00391</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00391</id><created>2015-07-01</created><authors><author><keyname>Huberman</keyname><forenames>Bernardo A.</forenames></author><author><keyname>Chua</keyname><forenames>Freddy C.</forenames></author></authors><title>Partitioning Uncertain Workflows</title><categories>cs.DC cs.CY physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is common practice to partition complex workflows into separate channels
in order to speed up their completion times. When this is done within a
distributed environment, unavoidable fluctuations make individual realizations
depart from the expected average gains. We present a method for breaking any
complex workflow into several workloads in such a way that once their outputs
are joined, their full completion takes less time and exhibit smaller variance
than when running in only one channel. We demonstrate the effectiveness of this
method in two different scenarios; the optimization of a convex function and
the transmission of a large computer file over the Internet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00396</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00396</id><created>2015-07-01</created><authors><author><keyname>Zhang</keyname><forenames>June</forenames></author><author><keyname>Moura</keyname><forenames>Jos&#xe9; M. F.</forenames></author></authors><title>Contact Process with Exogenous Infection and the Scaled SIS Process</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Propagation of contagion in networks depends on the graph topology. This
paper is concerned with studying the time-asymptotic behavior of the extended
contact processes on static, undirected, finite-size networks. This is a
contact process with nonzero exogenous infection rate (also known as the
{\epsilon}-SIS, {\epsilon} susceptible-infected-susceptible, model [1]). The
only known analytical characterization of the equilibrium distribution of this
process is for complete networks. For large networks with arbitrary topology,
it is infeasible to numerically solve for the equilibrium distribution since it
requires solving the eigenvalue-eigenvector problem of a matrix that is
exponential in N , the size of the network. We show that, for a certain range
of the network process parameters, the equilibrium distribution of the extended
contact process on arbitrary, finite-size networks is well approximated by the
equilibrium distribution of the scaled SIS process, which we derived in
closed-form in prior work. We confirm this result with numerical simulations
comparing the equilibrium distribution of the extended contact process with
that of a scaled SIS process. We use this approximation to decide, in
polynomial-time, which agents and network substructures are more susceptible to
infection by the extended contact process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00400</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00400</id><created>2015-07-01</created><authors><author><keyname>Abrardo</keyname><forenames>Andrea</forenames></author><author><keyname>Barni</keyname><forenames>Mauro</forenames></author><author><keyname>Kallas</keyname><forenames>Kassem</forenames></author><author><keyname>Tondi</keyname><forenames>Benedetta</forenames></author></authors><title>A Game-Theoretic Framework for Optimum Decision Fusion in the Presence
  of Byzantines</title><categories>cs.SY cs.GT</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Optimum decision fusion in the presence of malicious nodes - often referred
to as Byzantines - is hindered by the necessity of exactly knowing the
statistical behavior of Byzantines. By focusing on a simple, yet widely
studied, set-up in which a Fusion Center (FC) is asked to make a binary
decision about a sequence of system states by relying on the possibly corrupted
decisions provided by local nodes, we propose a game-theoretic framework which
permits to exploit the superior performance provided by optimum decision
fusion, while limiting the amount of a-priori knowledge required. We first
derive the optimum decision strategy by assuming that the statistical behavior
of the Byzantines is known. Then we relax such an assumption by casting the
problem into a game-theoretic framework in which the FC tries to guess the
behavior of the Byzantines, which, in turn, must fix their corruption strategy
without knowing the guess made by the FC. We use numerical simulations to
derive the equilibrium of the game, thus identifying the optimum behavior for
both the FC and the Byzantines, and to evaluate the achievable performance at
the equilibrium. We analyze several different setups, showing that in all cases
the proposed solution permits to improve the accuracy of data fusion. We also
show that, in some instances, it is preferable for the Byzantines to minimize
the mutual information between the status of the observed system and the
reports submitted to the FC, rather than always flipping the decision made by
the local nodes as it is customarily assumed in previous works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00403</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00403</id><created>2015-07-01</created><authors><author><keyname>Wu</keyname><forenames>Jingbo</forenames></author><author><keyname>Ugrinovskii</keyname><forenames>Valery</forenames></author><author><keyname>Allg&#xf6;wer</keyname><forenames>Frank</forenames></author></authors><title>Cooperative H-infinity Estimation for Large-Scale Interconnected Linear
  Systems</title><categories>cs.SY</categories><comments>Short version published in Proc. American Control Conference (ACC),
  pp.2119-2124. Chicago, IL, 2015</comments><journal-ref>In Proc. American Control Conference (ACC), p. 2119-2114, Chicago,
  IL, USA, 2015</journal-ref><doi>10.1109/ACC.2015.7171046</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a synthesis method for distributed estimation is presented,
which is suitable for dealing with large-scale interconnected linear systems
with disturbance. The main feature of the proposed method is that local
estimators only estimate a reduced set of state variables and their complexity
does not increase with the size of the system. Nevertheless, the local
estimators are able to deal with lack of local detectability. Moreover, the
estimators guarantee H-infinity-performance of the estimates with respect to
model and measurement disturbances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00407</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00407</id><created>2015-07-01</created><updated>2015-12-10</updated><authors><author><keyname>Syrgkanis</keyname><forenames>Vasilis</forenames></author><author><keyname>Agarwal</keyname><forenames>Alekh</forenames></author><author><keyname>Luo</keyname><forenames>Haipeng</forenames></author><author><keyname>Schapire</keyname><forenames>Robert E.</forenames></author></authors><title>Fast Convergence of Regularized Learning in Games</title><categories>cs.GT cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that natural classes of regularized learning algorithms with a form
of recency bias achieve faster convergence rates to approximate efficiency and
to coarse correlated equilibria in multiplayer normal form games. When each
player in a game uses an algorithm from our class, their individual regret
decays at $O(T^{-3/4})$, while the sum of utilities converges to an approximate
optimum at $O(T^{-1})$--an improvement upon the worst case $O(T^{-1/2})$ rates.
We show a black-box reduction for any algorithm in the class to achieve
$\tilde{O}(T^{-1/2})$ rates against an adversary, while maintaining the faster
rates against algorithms in the class. Our results extend those of [Rakhlin and
Shridharan 2013] and [Daskalakis et al. 2014], who only analyzed two-player
zero-sum games for specific algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00410</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00410</id><created>2015-07-01</created><updated>2015-09-18</updated><authors><author><keyname>Barron</keyname><forenames>Jonathan T.</forenames></author></authors><title>Convolutional Color Constancy</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Color constancy is the problem of inferring the color of the light that
illuminated a scene, usually so that the illumination color can be removed.
Because this problem is underconstrained, it is often solved by modeling the
statistical regularities of the colors of natural objects and illumination. In
contrast, in this paper we reformulate the problem of color constancy as a 2D
spatial localization task in a log-chrominance space, thereby allowing us to
apply techniques from object detection and structured prediction to the color
constancy problem. By directly learning how to discriminate between correctly
white-balanced images and poorly white-balanced images, our model is able to
improve performance on standard benchmarks by nearly 40%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00418</identifier>
 <datestamp>2015-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00418</id><created>2015-07-01</created><updated>2015-11-19</updated><authors><author><keyname>Hartline</keyname><forenames>Jason</forenames></author><author><keyname>Syrgkanis</keyname><forenames>Vasilis</forenames></author><author><keyname>Tardos</keyname><forenames>Eva</forenames></author></authors><title>No-Regret Learning in Bayesian Games</title><categories>cs.GT cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent price-of-anarchy analyses of games of complete information suggest
that coarse correlated equilibria, which characterize outcomes resulting from
no-regret learning dynamics, have near-optimal welfare. This work provides two
main technical results that lift this conclusion to games of incomplete
information, a.k.a., Bayesian games. First, near-optimal welfare in Bayesian
games follows directly from the smoothness-based proof of near-optimal welfare
in the same game when the private information is public. Second, no-regret
learning dynamics converge to Bayesian coarse correlated equilibrium in these
incomplete information games. These results are enabled by interpretation of a
Bayesian game as a stochastic game of complete information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00421</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00421</id><created>2015-07-01</created><authors><author><keyname>Cao</keyname><forenames>Yang</forenames></author><author><keyname>Xie</keyname><forenames>Yao</forenames></author></authors><title>Categorical Matrix Completion</title><categories>cs.NA cs.LG math.ST stat.ML stat.TH</categories><comments>Submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of completing a matrix with categorical-valued
entries from partial observations. This is achieved by extending the
formulation and theory of one-bit matrix completion. We recover a low-rank
matrix $X$ by maximizing the likelihood ratio with a constraint on the nuclear
norm of $X$, and the observations are mapped from entries of $X$ through
multiple link functions. We establish theoretical upper and lower bounds on the
recovery error, which meet up to a constant factor $\mathcal{O}(K^{3/2})$ where
$K$ is the fixed number of categories. The upper bound in our case depends on
the number of categories implicitly through a maximization of terms that
involve the smoothness of the link functions. In contrast to one-bit matrix
completion, our bounds for categorical matrix completion are optimal up to a
factor on the order of the square root of the number of categories, which is
consistent with an intuition that the problem becomes harder when the number of
categories increases. By comparing the performance of our method with the
conventional matrix completion method on the MovieLens dataset, we demonstrate
the advantage of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00432</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00432</id><created>2015-07-02</created><authors><author><keyname>Ito</keyname><forenames>Tsuyoshi</forenames></author><author><keyname>Jeffery</keyname><forenames>Stacey</forenames></author></authors><title>Approximate Span Programs</title><categories>quant-ph cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Span programs are a model of computation that have been used to design
quantum algorithms, mainly in the query model. For any decision problem, there
exists a span program that leads to an algorithm with optimal quantum query
complexity, but finding such an algorithm is generally challenging.
  We consider new ways of designing quantum algorithms using span programs. We
show how any span program that decides a problem $f$ can also be used to decide
&quot;property testing&quot; versions of $f$, or more generally, approximate the span
program witness size, a property of the input related to $f$. For example,
using our techniques, the span program for OR, which can be used to design an
optimal algorithm for the OR function, can also be used to design optimal
algorithms for: threshold functions, in which we want to decide if the Hamming
weight of a string is above a threshold or far below, given the promise that
one of these is true; and approximate counting, in which we want to estimate
the Hamming weight of the input. We achieve these results by relaxing the
requirement that 1-inputs hit some target exactly in the span program, which
could make design of span programs easier.
  We also give an exposition of span program structure, which increases the
understanding of this important model. One implication is alternative
algorithms for estimating the witness size when the phase gap of a certain
unitary can be lower bounded. We show how to lower bound this phase gap in some
cases.
  As applications, we give the first upper bounds in the adjacency query model
on the quantum time complexity of estimating the effective resistance between
$s$ and $t$, $R_{s,t}(G)$, of $\tilde
O(\frac{1}{\epsilon^{3/2}}n\sqrt{R_{s,t}(G)})$, and, when $\mu$ is a lower
bound on $\lambda_2(G)$, by our phase gap lower bound, we can obtain $\tilde
O(\frac{1}{\epsilon}n\sqrt{R_{s,t}(G)/\mu})$, both using $O(\log n)$ space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00436</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00436</id><created>2015-07-02</created><updated>2015-07-14</updated><authors><author><keyname>Zhan</keyname><forenames>Yusen</forenames></author><author><keyname>Taylor</keyname><forenames>Matthew E.</forenames></author></authors><title>Online Transfer Learning in Reinforcement Learning Domains</title><categories>cs.AI cs.LG</categories><comments>18 pages, 2 figures</comments><acm-class>I.2.11; I.2.6</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This paper proposes an online transfer framework to capture the interaction
among agents and shows that current transfer learning in reinforcement learning
is a special case of online transfer. Furthermore, this paper re-characterizes
existing agents-teaching-agents methods as online transfer and analyze one such
teaching method in three ways. First, the convergence of Q-learning and Sarsa
with tabular representation with a finite budget is proven. Second, the
convergence of Q-learning and Sarsa with linear function approximation is
established. Third, the we show the asymptotic performance cannot be hurt
through teaching. Additionally, all theoretical results are empirically
validated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00438</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00438</id><created>2015-07-02</created><authors><author><keyname>Rakotomamonjy</keyname><forenames>Alain</forenames><affiliation>LITIS</affiliation></author><author><keyname>Flamary</keyname><forenames>Remi</forenames><affiliation>LAGRANGE, OCA</affiliation></author><author><keyname>Gasso</keyname><forenames>Gilles</forenames><affiliation>LITIS</affiliation></author></authors><title>DC Proximal Newton for Non-Convex Optimization Problems</title><categories>cs.LG cs.NA stat.ML</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a novel algorithm for solving learning problems where both the
loss function and the regularizer are non-convex but belong to the class of
difference of convex (DC) functions. Our contribution is a new general purpose
proximal Newton algorithm that is able to deal with such a situation. The
algorithm consists in obtaining a descent direction from an approximation of
the loss function and then in performing a line search to ensure sufficient
descent. A theoretical analysis is provided showing that the iterates of the
proposed algorithm {admit} as limit points stationary points of the DC
objective function. Numerical experiments show that our approach is more
efficient than current state of the art for a problem with a convex loss
functions and non-convex regularizer. We have also illustrated the benefit of
our algorithm in high-dimensional transductive learning problem where both loss
function and regularizers are non-convex.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00443</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00443</id><created>2015-07-02</created><authors><author><keyname>Primault</keyname><forenames>Vincent</forenames><affiliation>DRIM, INSA Lyon</affiliation></author><author><keyname>Mokhtar</keyname><forenames>Sonia Ben</forenames><affiliation>DRIM, INSA Lyon</affiliation></author><author><keyname>Lauradoux</keyname><forenames>C&#xe9;dric</forenames><affiliation>PRIVATICS</affiliation></author><author><keyname>Brunie</keyname><forenames>Lionel</forenames><affiliation>DRIM, INSA Lyon</affiliation></author></authors><title>Time Distortion Anonymization for the Publication of Mobility Data with
  High Utility</title><categories>cs.CR</categories><comments>in 14th IEEE International Conference on Trust, Security and Privacy
  in Computing and Communications, Aug 2015, Helsinki, Finland</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An increasing amount of mobility data is being collected every day by
different means, such as mobile applications or crowd-sensing campaigns. This
data is sometimes published after the application of simple anonymization
techniques (e.g., putting an identifier instead of the users' names), which
might lead to severe threats to the privacy of the participating users.
Literature contains more sophisticated anonymization techniques, often based on
adding noise to the spatial data. However, these techniques either compromise
the privacy if the added noise is too little or the utility of the data if the
added noise is too strong. We investigate in this paper an alternative
solution, which builds on time distortion instead of spatial distortion.
Specifically, our contribution lies in (1) the introduction of the concept of
time distortion to anonymize mobility datasets (2) Promesse, a protection
mechanism implementing this concept (3) a practical study of Promesse compared
to two representative spatial distortion mechanisms, namely Wait For Me, which
enforces k-anonymity, and Geo-Indistinguishability, which enforces differential
privacy. We evaluate our mechanism practically using three real-life datasets.
Our results show that time distortion reduces the number of points of interest
that can be retrieved by an adversary to under 3 %, while the introduced
spatial error is almost null and the distortion introduced on the results of
range queries is kept under 13 % on average.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00447</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00447</id><created>2015-07-02</created><updated>2015-10-02</updated><authors><author><keyname>Levin</keyname><forenames>Asaf</forenames></author><author><keyname>Onn</keyname><forenames>Shmuel</forenames></author></authors><title>Shifted Matroid Optimization</title><categories>math.OC cs.DM cs.DS math.CO</categories><msc-class>05A, 15A, 51M, 52A, 52B, 52C, 62H, 68Q, 68R, 68U, 68W, 90B, 90C</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that finding lexicographically minimal $n$ bases in a matroid can be
done in polynomial time in the oracle model. This follows from a more general
result that the shifted problem over a matroid can be solved in polynomial time
as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00448</identifier>
 <datestamp>2015-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00448</id><created>2015-07-02</created><updated>2015-11-25</updated><authors><author><keyname>Gupta</keyname><forenames>Saurabh</forenames></author><author><keyname>Hoffman</keyname><forenames>Judy</forenames></author><author><keyname>Malik</keyname><forenames>Jitendra</forenames></author></authors><title>Cross Modal Distillation for Supervision Transfer</title><categories>cs.CV</categories><comments>Updated version (v2) contains additional experiments and results</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we propose a technique that transfers supervision between images
from different modalities. We use learned representations from a large labeled
modality as a supervisory signal for training representations for a new
unlabeled paired modality. Our method enables learning of rich representations
for unlabeled modalities and can be used as a pre-training procedure for new
modalities with limited labeled data. We show experimental results where we
transfer supervision from labeled RGB images to unlabeled depth and optical
flow images and demonstrate large improvements for both these cross modal
supervision transfers. Code, data and pre-trained models are available at
https://github.com/s-gupta/fast-rcnn/tree/distillation
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00451</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00451</id><created>2015-07-02</created><authors><author><keyname>Loach</keyname><forenames>Tamar V.</forenames></author><author><keyname>Evans</keyname><forenames>Tim S.</forenames></author></authors><title>Ranking Journals Using Altmetrics</title><categories>cs.DL cs.CY cs.SI</categories><comments>6 pages. To appear in the proceedings of ISSI 2015, the 15th
  International Society of Scientometrics and Informetrics conference held in
  Istanbul on 30th June to 2nd July 2015</comments><report-no>Imperial/TP/15/TSE/2</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rank of a journal based on simple citation information is a popular
measure. The simplicity and availability of rankings such as Impact Factor,
Eigenfactor and SciMago Journal Rank based on trusted commercial sources
ensures their widespread use for many important tasks despite the well-known
limitations of such rankings. In this paper we look at an alternative approach
based on information on papers from social and mainstream media sources. Our
data comes from altmetric.com who identify mentions of individual academic
papers in sources such as Twitter, Facebook, blogs and news outlets. We
consider several different methods to produce a ranking of journals from such
data. We show that most (but not all) schemes produce results, which are
roughly similar, suggesting that there is a basic consistency between social
media based approaches and traditional citation based methods. Most ranking
schemes applied to one data set produce relatively little variation and we
suggest this provides a measure of the uncertainty in any journal rating. The
differences we find between data sources also shows they are capturing
different aspects of journal impact. We conclude a small number of such ratings
will provide the best information on journal impact.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00459</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00459</id><created>2015-07-02</created><updated>2015-12-18</updated><authors><author><keyname>Chen</keyname><forenames>Jingchao</forenames></author></authors><title>Fast Blocked Clause Decomposition with High Quality</title><categories>cs.LO</categories><comments>15 pages</comments><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Any CNF formula can be decomposed two blocked subsets such that both can be
solved by BCE (Blocked Clause Elimination). To make the decomposition more
useful, one hopes to have the decomposition as unbalanced as possible. It is
often time consuming to achieve this goal. So far there have been several
decomposition and post-processing algorithms such as PureDecompose,
QuickDecompose, EagerMover etc. We found that these existing algorithms are
often either inefficient or low-quality decomposition. This paper aims at
improving the decomposition quality, while keeping the runtime of algorithms
under control. To achieve this goal, we improve the existing BCE, and present
two new variants of PureDecompose, a new heuristic decomposition called
LessInterfereDecompose, and a new post-processing algorithm called
RsetGuidedDecompose. Combining these new techniques results in a new algorithm
called MixDecompose. In our experiments, there is no application formula where
the quality of PureDecompose+EagerMover is better than MixDecompose. In terms
of speed, MixDecompose is also very fast. Our average runtime is a little
longer, but the worst-case runtime is shorter. In theory, our two variants of
PureDecompose requires linear time in the number of clauses. By limiting the
size of the touch list used by BCE, we can guarantee always that MixDecompose
runs in linear time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00473</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00473</id><created>2015-07-02</created><updated>2016-02-07</updated><authors><author><keyname>Hanneke</keyname><forenames>Steve</forenames></author></authors><title>The Optimal Sample Complexity of PAC Learning</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work establishes a new upper bound on the number of samples sufficient
for PAC learning in the realizable case. The bound matches known lower bounds
up to numerical constant factors. This solves a long-standing open problem on
the sample complexity of PAC learning. The technique and analysis build on a
recent breakthrough by Hans Simon.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00474</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00474</id><created>2015-07-02</created><authors><author><keyname>Bouzid</keyname><forenames>Zohir</forenames><affiliation>NPA</affiliation></author><author><keyname>Raynal</keyname><forenames>Michel</forenames><affiliation>ASAP</affiliation></author><author><keyname>Sutra</keyname><forenames>Pierre</forenames></author></authors><title>Anonymous Obstruction-free $(n,k)$-Set Agreement with $n-k+1$ Atomic
  Read/Write Registers</title><categories>cs.DC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The $k$-set agreement problem is a generalization of the consensus problem.
Namely, assuming each process proposes a value, each non-faulty process has to
decide a value such that each decided value was proposed, and no more than $k$
different values are decided. This is a hard problem in the sense that it
cannot be solved in asynchronous systems as soon as $k$ or more processes may
crash. One way to circumvent this impossibility consists in weakening its
termination property, requiring that a process terminates (decides) only if it
executes alone during a long enough period. This is the well-known
obstruction-freedom progress condition. Considering a system of $n$ {\it
anonymous asynchronous} processes, which communicate through atomic {\it
read/write registers only}, and where {\it any number of processes may crash},
this paper addresses and solves the challenging open problem of designing an
obstruction-free $k$-set agreement algorithm with $(n-k+1)$ atomic registers
only. From a shared memory cost point of view, this algorithm is the best
algorithm known so far, thereby establishing a new upper bound on the number of
registers needed to solve the problem (its gain is $(n-k)$ with respect to the
previous upper bound). The algorithm is then extended to address the repeated
version of $(n,k)$-set agreement. As it is optimal in the number of atomic
read/write registers, this algorithm closes the gap on previously established
lower/upper bounds for both the anonymous and non-anonymous versions of the
repeated $(n,k)$-set agreement problem. Finally, for $1 \leq x\leq k
\textless{} n$, a generalization suited to $x$-obstruction-freedom is also
described, which requires $(n-k+x)$ atomic registers only.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00477</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00477</id><created>2015-07-02</created><updated>2015-08-01</updated><authors><author><keyname>Holme</keyname><forenames>Petter</forenames></author><author><keyname>Liljeros</keyname><forenames>Fredrik</forenames></author></authors><title>Mechanistic Models in Computational Social Science</title><categories>physics.soc-ph cs.SI</categories><comments>v2, typos corrected, layout polished</comments><report-no>Front. Phys. 3, 78 (2015)</report-no><doi>10.3389/fphy.2015.00078</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantitative social science is not only about regression analysis or, in
general, data inference. Computer simulations of social mechanisms have an over
60 years long history. They have been used for many different purposes -- to
test scenarios, to test the consistency of descriptive theories
(proof-of-concept models), to explore emergent phenomena, for forecasting, etc.
In this essay, we sketch these historical developments, the role of mechanistic
models in the social sciences and the influences from the natural and formal
sciences. We argue that mechanistic computational models form a natural common
ground for social and natural sciences, and look forward to possible future
information flow across the social-natural divide.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00500</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00500</id><created>2015-07-02</created><authors><author><keyname>Laporte</keyname><forenames>L&#xe9;a</forenames><affiliation>IRIT</affiliation></author><author><keyname>Flamary</keyname><forenames>R&#xe9;mi</forenames><affiliation>OCA, LAGRANGE</affiliation></author><author><keyname>Canu</keyname><forenames>Stephane</forenames><affiliation>LITIS</affiliation></author><author><keyname>D&#xe9;jean</keyname><forenames>S&#xe9;bastien</forenames><affiliation>IMT</affiliation></author><author><keyname>Mothe</keyname><forenames>Josiane</forenames><affiliation>IRIT</affiliation></author></authors><title>Non-convex Regularizations for Feature Selection in Ranking With Sparse
  SVM</title><categories>cs.LG</categories><proxy>ccsd</proxy><journal-ref>IEEE Transactions on Neural Networks and Learning Systems, IEEE,
  2013, pp.1,1</journal-ref><doi>10.1109/TNNLS.2013.2286696</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature selection in learning to rank has recently emerged as a crucial
issue. Whereas several preprocessing approaches have been proposed, only a few
works have been focused on integrating the feature selection into the learning
process. In this work, we propose a general framework for feature selection in
learning to rank using SVM with a sparse regularization term. We investigate
both classical convex regularizations such as $\ell\_1$ or weighted $\ell\_1$
and non-convex regularization terms such as log penalty, Minimax Concave
Penalty (MCP) or $\ell\_p$ pseudo norm with $p\textless{}1$. Two algorithms are
proposed, first an accelerated proximal approach for solving the convex
problems, second a reweighted $\ell\_1$ scheme to address the non-convex
regularizations. We conduct intensive experiments on nine datasets from Letor
3.0 and Letor 4.0 corpora. Numerical results show that the use of non-convex
regularizations we propose leads to more sparsity in the resulting models while
prediction performance is preserved. The number of features is decreased by up
to a factor of six compared to the $\ell\_1$ regularization. In addition, the
software is publicly available on the web.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00501</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00501</id><created>2015-07-02</created><authors><author><keyname>Ferrari</keyname><forenames>Andr&#xe9;</forenames><affiliation>LAGRANGE, OCA</affiliation></author><author><keyname>Mary</keyname><forenames>David</forenames><affiliation>LAGRANGE, OCA</affiliation></author><author><keyname>Flamary</keyname><forenames>R&#xe9;mi</forenames><affiliation>LAGRANGE, OCA</affiliation></author><author><keyname>Richard</keyname><forenames>C&#xe9;dric</forenames><affiliation>LAGRANGE, OCA</affiliation></author></authors><title>Distributed image reconstruction for very large arrays in radio
  astronomy</title><categories>astro-ph.IM cs.CV</categories><comments>Sensor Array and Multichannel Signal Processing Workshop (SAM), 2014
  IEEE 8th, Jun 2014, Coruna, Spain. 2014</comments><proxy>ccsd</proxy><doi>10.1109/SAM.2014.6882424</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current and future radio interferometric arrays such as LOFAR and SKA are
characterized by a paradox. Their large number of receptors (up to millions)
allow theoretically unprecedented high imaging resolution. In the same time,
the ultra massive amounts of samples makes the data transfer and computational
loads (correlation and calibration) order of magnitudes too high to allow any
currently existing image reconstruction algorithm to achieve, or even approach,
the theoretical resolution. We investigate here decentralized and distributed
image reconstruction strategies which select, transfer and process only a
fraction of the total data. The loss in MSE incurred by the proposed approach
is evaluated theoretically and numerically on simple test cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00504</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00504</id><created>2015-07-02</created><authors><author><keyname>Courty</keyname><forenames>Nicolas</forenames><affiliation>OBELIX</affiliation></author><author><keyname>Flamary</keyname><forenames>R&#xe9;mi</forenames><affiliation>LAGRANGE, OCA</affiliation></author><author><keyname>Tuia</keyname><forenames>Devis</forenames><affiliation>LASIG</affiliation></author><author><keyname>Rakotomamonjy</keyname><forenames>Alain</forenames><affiliation>LITIS</affiliation></author></authors><title>Optimal Transport for Domain Adaptation</title><categories>cs.LG</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Domain adaptation from one data space (or domain) to another is one of the
most challenging tasks of modern data analytics. If the adaptation is done
correctly, models built on a specific data space become more robust when
confronted to data depicting the same semantic concepts (the classes), but
observed by another observation system with its own specificities. Among the
many strategies proposed to adapt a domain to another, finding a common
representation has shown excellent properties: by finding a common
representation for both domains, a single classifier can be effective in both
and use labelled samples from the source domain to predict the unlabelled
samples of the target domain. In this paper, we propose a regularized
unsupervised optimal transportation model to perform the alignment of the
representations in the source and target domains. We learn a transportation
plan matching both PDFs, which constrains labelled samples in the source domain
to remain close during transport. This way, we exploit at the same time the few
labeled information in the source and the unlabelled distributions observed in
both domains. Experiments in toy and challenging real visual adaptation
examples show the interest of the method, that consistently outperforms state
of the art approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00505</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00505</id><created>2015-07-02</created><authors><author><keyname>Bil&#xf2;</keyname><forenames>Davide</forenames></author><author><keyname>Grandoni</keyname><forenames>Fabrizio</forenames></author><author><keyname>Gual&#xe0;</keyname><forenames>Luciano</forenames></author><author><keyname>Leucci</keyname><forenames>Stefano</forenames></author><author><keyname>Proietti</keyname><forenames>Guido</forenames></author></authors><title>Improved Purely Additive Fault-Tolerant Spanners</title><categories>cs.DS</categories><comments>17 pages, 4 figures, ESA 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G$ be an unweighted $n$-node undirected graph. A \emph{$\beta$-additive
spanner} of $G$ is a spanning subgraph $H$ of $G$ such that distances in $H$
are stretched at most by an additive term $\beta$ w.r.t. the corresponding
distances in $G$. A natural research goal related with spanners is that of
designing \emph{sparse} spanners with \emph{low} stretch.
  In this paper, we focus on \emph{fault-tolerant} additive spanners, namely
additive spanners which are able to preserve their additive stretch even when
one edge fails. We are able to improve all known such spanners, in terms of
either sparsity or stretch. In particular, we consider the sparsest known
spanners with stretch $6$, $28$, and $38$, and reduce the stretch to $4$, $10$,
and $14$, respectively (while keeping the same sparsity).
  Our results are based on two different constructions. On one hand, we show
how to augment (by adding a \emph{small} number of edges) a fault-tolerant
additive \emph{sourcewise spanner} (that approximately preserves distances only
from a given set of source nodes) into one such spanner that preserves all
pairwise distances. On the other hand, we show how to augment some known
fault-tolerant additive spanners, based on clustering techniques. This way we
decrease the additive stretch without any asymptotic increase in their size. We
also obtain improved fault-tolerant additive spanners for the case of one
vertex failure, and for the case of $f$ edge failures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00509</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00509</id><created>2015-07-02</created><authors><author><keyname>Soudjani</keyname><forenames>Sadegh Esmaeil Zadeh</forenames></author><author><keyname>Abate</keyname><forenames>Alessandro</forenames></author><author><keyname>Majumdar</keyname><forenames>Rupak</forenames></author></authors><title>Dynamic Bayesian Networks as Formal Abstractions of Structured
  Stochastic Processes</title><categories>cs.SY</categories><comments>Accepted in 26th Conference on Concurrency Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of finite-horizon probabilistic invariance for
discrete-time Markov processes over general (uncountable) state spaces. We
compute discrete-time, finite-state Markov chains as formal abstractions of
general Markov processes. Our abstraction differs from existing approaches in
two ways. First, we exploit the structure of the underlying Markov process to
compute the abstraction separately for each dimension. Second, we employ
dynamic Bayesian networks (DBN) as compact representations of the abstraction.
In contrast, existing approaches represent and store the (exponentially large)
Markov chain explicitly, which leads to heavy memory requirements limiting the
application to models of dimension less than half, according to our
experiments. We show how to construct a DBN abstraction of a Markov process
satisfying an independence assumption on the driving process noise. We compute
a guaranteed bound on the error in the abstraction w.r.t.\ the probabilistic
invariance property; the dimension-dependent abstraction makes the error bounds
more precise than existing approaches. Additionally, we show how factor graphs
and the sum-product algorithm for DBNs can be used to solve the finite-horizon
probabilistic invariance problem. Together, DBN-based representations and
algorithms can be significantly more efficient than explicit representations of
Markov chains for abstracting and model checking structured Markov processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00522</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00522</id><created>2015-07-02</created><authors><author><keyname>Chun</keyname><forenames>Young Jin</forenames></author><author><keyname>Cotton</keyname><forenames>Simon L.</forenames></author><author><keyname>Hasna</keyname><forenames>Mazen O.</forenames></author><author><keyname>Ghrayeb</keyname><forenames>Ali</forenames></author></authors><title>A Stochastic Geometry Based Approach to Modeling Interference
  Correlation in Cooperative Relay Networks</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Future wireless networks are expected to be a convergence of many diverse
network technologies and architectures, such as cellular networks, wireless
local area networks, sensor networks, and device to device communications.
Through cooperation between dissimilar wireless devices, this new combined
network topology promises to unlock ever larger data rates and provide truly
ubiquitous coverage for end users, as well as enabling higher spectral
efficiency. However, it also increases the risk of co-channel interference and
introduces the possibility of correlation in the aggregated interference that
not only impacts the communication performance, but also makes the associated
mathematical analysis much more complex. To address this problem and evaluate
the communication performance of cooperative relay networks, we adopt a
stochastic geometry based approach by assuming that the interfering nodes are
randomly distributed according to a Poisson point process (PPP). We also use a
random medium access protocol to counteract the effects of interference
correlation. Using this approach, we derive novel closed-form expressions for
the successful transmission probability and local delay of a relay network with
correlated interference. As well as this, we find the optimal transmission
probability $p$ that jointly maximizes the successful transmission probability
and minimizes the local delay. Finally numerical results are provided to
confirm that the proposed joint optimization strategy achieves a significant
performance gain compared to a conventional scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00524</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00524</id><created>2015-07-02</created><authors><author><keyname>Miller</keyname><forenames>Bruce R.</forenames></author></authors><title>Strategies for Parallel Markup</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cross-referenced parallel markup for mathematics allows the combination of
both presentation and content representations while associating the components
of each. Interesting applications are enabled by such an arrangement, such as
interaction with parts of the presentation to manipulate and querying the
corresponding content, and enhanced search indexing. Although the idea of such
markup is hardly new, effective techniques for creating and manipulating it are
more difficult than it appears. Since the structures and tokens in the two
formats often do not correspond one-to-one, decisions and heuristics must be
developed to determine in which way each component refers to and is referred to
by components of the other representation. Conversion between fine and coarse
grained parallel markup complicates ID assignments. In this paper, we will
describe the techniques developed for \LaTeXML, a \TeX/\LaTeX to XML converter,
to create cross-referenced parallel MathML. While we do not yet consider
\LaTeXML's content MathML to be useful, the current effort is a step towards
that continuing goal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00525</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00525</id><created>2015-07-02</created><authors><author><keyname>Saint-Bauzel</keyname><forenames>Ludovic</forenames><affiliation>ISIR</affiliation></author><author><keyname>Pasqui</keyname><forenames>Viviane</forenames><affiliation>ISIR</affiliation></author><author><keyname>Monteil</keyname><forenames>Isabelle</forenames></author></authors><title>A Reactive Robotized Interface for Lower Limb Rehabilitation: Clinical
  Results</title><categories>cs.RO cs.SY</categories><proxy>ccsd</proxy><journal-ref>IEEE Transactions on Robotics, Institute of Electrical and
  Electronics Engineers (IEEE), 2009, pp.583 - 592</journal-ref><doi>10.1109/TRO.2009.2019886</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  -This article presents clinical results from the use of MONIMAD, a reactive
robotized interface for lower limb Rehabilitation of patients suffering from
cerebellar disease. The first problem to be addressed is the postural analysis
of sit-to-stand motion. Experiments with healthy subjects were performed for
this purpose. Analysis of external forces shows that sit-to-stand transfer can
be subdivided into several phases: preaccel-eration, acceleration, start
rising, rising. Observation of Center of Pressure, ground forces and horizontal
components force on handles yields rules to identify the stability of the
patient and to adjust the robotic interface motion to the human voluntary
movement. These rules are used in a fuzzy-based controller implementation. The
controller is validated on experiments with diseased patients in Bellan
Hospital.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00541</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00541</id><created>2015-07-02</created><authors><author><keyname>Vaverka</keyname><forenames>Ondrej</forenames></author><author><keyname>Vychodil</keyname><forenames>Vilem</forenames></author></authors><title>Relational Division in Rank-Aware Databases</title><categories>cs.DB</categories><msc-class>68P15, 03B52</msc-class><acm-class>H.2.3; H.2.4; I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a survey of existing approaches to relational division in
rank-aware databases, discuss issues of the present approaches, and outline
generalizations of several types of classic division-like operations. We work
in a model which generalizes the Codd model of data by considering tuples in
relations annotated by ranks, indicating degrees to which tuples in relations
match queries. The approach utilizes complete residuated lattices as the basic
structures of degrees. We argue that unlike the classic model, relational
divisions are fundamental operations which cannot in general be expressed by
means of other operations. In addition, we compare the existing and proposed
operations and identify those which are faithful counterparts of universally
quantified queries formulated in relational calculi. We introduce Pseudo Tuple
Calculus in the ranked model which is further used to show mutual definability
of the various forms of divisions presented in the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00552</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00552</id><created>2015-07-02</created><authors><author><keyname>Pagh</keyname><forenames>Rasmus</forenames></author><author><keyname>Pham</keyname><forenames>Ninh</forenames></author><author><keyname>Silvestri</keyname><forenames>Francesco</forenames></author><author><keyname>St&#xf6;ckel</keyname><forenames>Morten</forenames></author></authors><title>I/O-Efficient Similarity Join</title><categories>cs.DS</categories><comments>20 pages in Proceedings of the 23rd Annual European Symposium on
  Algorithms 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an I/O-efficient algorithm for computing similarity joins based on
locality-sensitive hashing (LSH). In contrast to the filtering methods commonly
suggested our method has provable sub-quadratic dependency on the data size.
Further, in contrast to straightforward implementations of known LSH-based
algorithms on external memory, our approach is able to take significant
advantage of the available internal memory: Whereas the time complexity of
classical algorithms includes a factor of $N^\rho$, where $\rho$ is a parameter
of the LSH used, the I/O complexity of our algorithm merely includes a factor
$(N/M)^\rho$, where $N$ is the data size and $M$ is the size of internal
memory. Our algorithm is randomized and outputs the correct result with high
probability. It is a simple, recursive, cache-oblivious procedure, and we
believe that it will be useful also in other computational settings such as
parallel computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00553</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00553</id><created>2015-07-02</created><authors><author><keyname>Kimble</keyname><forenames>Chris</forenames></author></authors><title>Business Models for e-Health: Evidence from Ten Case Studies</title><categories>cs.CY</categories><journal-ref>Global Business and Organizational Excellence, 34(4), 2015, pp.
  18-30</journal-ref><doi>10.1002/joe.21611</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An increasingly aging population and spiraling healthcare costs have made the
search for financially viable healthcare models an imperative of this century.
The careful and creative application of information technology can play a
significant role in meeting that challenge. Valuable lessons can be learned
from an analysis of ten innovative telemedicine and e-health initiatives.
Having proven their effectiveness in addressing a variety of medical needs,
they have progressed beyond small-scale implementations to become an
established part of healthcare delivery systems around the world.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00557</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00557</id><created>2015-07-02</created><authors><author><keyname>Le</keyname><forenames>Van Bang</forenames></author><author><keyname>Podelleck</keyname><forenames>Thomas</forenames></author></authors><title>Characterization and recognition of some opposition and coalition graph
  classes</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph is an opposition graph, respectively, a coalition graph, if it admits
an acyclic orientation which puts the two end-edges of every chordless 4-vertex
path in opposition, respectively, in the same direction. Opposition and
coalition graphs have been introduced and investigated in connection to perfect
graphs. Recognizing and characterizing opposition and coalition graphs are
long-standing open problems. This paper gives characterizations for opposition
graphs and coalition graphs on some restricted graph classes. Implicit in our
arguments are polynomial time recognition algorithms for these graphs. We also
give a good characterization for the so-called generalized opposition graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00559</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00559</id><created>2015-07-02</created><authors><author><keyname>Dorbec</keyname><forenames>Paul</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Duch&#xea;ne</keyname><forenames>Eric</forenames><affiliation>GOAL, LIRIS</affiliation></author><author><keyname>Fabbri</keyname><forenames>Andr&#xe9;</forenames><affiliation>LIRIS</affiliation></author><author><keyname>Moncel</keyname><forenames>Julien</forenames><affiliation>GOAL, LIRIS</affiliation></author><author><keyname>Parreau</keyname><forenames>Aline</forenames><affiliation>GOAL, LIRIS</affiliation></author><author><keyname>Sopena</keyname><forenames>Eric</forenames><affiliation>LaBRI</affiliation></author></authors><title>Ice sliding games</title><categories>math.CO cs.DM cs.GT</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with sliding games, which are a variant of the better known
pushpush game. On a given structure (grid, torus...), a robot can move in a
specific set of directions, and stops when it hits a block or boundary of the
structure. The objective is to place the minimum number of blocks such that the
robot can visit all the possible positions of the structure. In particular, we
give the exact value of this number when playing on a rectangular grid and a
torus. Other variants of this game are also considered, by constraining the
robot to stop on each case, or by replacing blocks by walls.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00564</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00564</id><created>2015-07-02</created><authors><author><keyname>Pillonetto</keyname><forenames>Gianluigi</forenames></author><author><keyname>Chen</keyname><forenames>Tianshi</forenames></author><author><keyname>Chiuso</keyname><forenames>Alessandro</forenames></author><author><keyname>De Nicolao</keyname><forenames>Giuseppe</forenames></author><author><keyname>Ljung</keyname><forenames>Lennart</forenames></author></authors><title>Regularized linear system identification using atomic, nuclear and
  kernel-based norms: the role of the stability constraint</title><categories>cs.SY cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by ideas taken from the machine learning literature, new
regularization techniques have been recently introduced in linear system
identification. In particular, all the adopted estimators solve a regularized
least squares problem, differing in the nature of the penalty term assigned to
the impulse response. Popular choices include atomic and nuclear norms (applied
to Hankel matrices) as well as norms induced by the so called stable spline
kernels. In this paper, a comparative study of estimators based on these
different types of regularizers is reported. Our findings reveal that stable
spline kernels outperform approaches based on atomic and nuclear norms since
they suitably embed information on impulse response stability and smoothness.
This point is illustrated using the Bayesian interpretation of regularization.
We also design a new class of regularizers defined by &quot;integral&quot; versions of
stable spline/TC kernels. Under quite realistic experimental conditions, the
new estimators outperform classical prediction error methods also when the
latter are equipped with an oracle for model order selection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00567</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00567</id><created>2015-07-02</created><authors><author><keyname>Jamshidi</keyname><forenames>Pooyan</forenames></author><author><keyname>Sharifloo</keyname><forenames>Amir</forenames></author><author><keyname>Pahl</keyname><forenames>Claus</forenames></author><author><keyname>Metzger</keyname><forenames>Andreas</forenames></author><author><keyname>Estrada</keyname><forenames>Giovani</forenames></author></authors><title>Self-Learning Cloud Controllers: Fuzzy Q-Learning for Knowledge
  Evolution</title><categories>cs.SY cs.AI cs.DC cs.LG cs.SE</categories><acm-class>I.2.6; D.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud controllers aim at responding to application demands by automatically
scaling the compute resources at runtime to meet performance guarantees and
minimize resource costs. Existing cloud controllers often resort to scaling
strategies that are codified as a set of adaptation rules. However, for a cloud
provider, applications running on top of the cloud infrastructure are more or
less black-boxes, making it difficult at design time to define optimal or
pre-emptive adaptation rules. Thus, the burden of taking adaptation decisions
often is delegated to the cloud application. Yet, in most cases, application
developers in turn have limited knowledge of the cloud infrastructure. In this
paper, we propose learning adaptation rules during runtime. To this end, we
introduce FQL4KE, a self-learning fuzzy cloud controller. In particular, FQL4KE
learns and modifies fuzzy rules at runtime. The benefit is that for designing
cloud controllers, we do not have to rely solely on precise design-time
knowledge, which may be difficult to acquire. FQL4KE empowers users to specify
cloud controllers by simply adjusting weights representing priorities in system
goals instead of specifying complex adaptation rules. The applicability of
FQL4KE has been experimentally assessed as part of the cloud application
framework ElasticBench. The experimental results indicate that FQL4KE
outperforms our previously developed fuzzy controller without learning
mechanisms and the native Azure auto-scaling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00576</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00576</id><created>2015-07-02</created><updated>2015-09-14</updated><authors><author><keyname>Pawlick</keyname><forenames>Jeffrey</forenames></author><author><keyname>Farhang</keyname><forenames>Sadegh</forenames></author><author><keyname>Zhu</keyname><forenames>Quanyan</forenames></author></authors><title>Flip the Cloud: Cyber-Physical Signaling Games in the Presence of
  Advanced Persistent Threats</title><categories>cs.CR cs.GT</categories><comments>To be presented at the 2015 Conference on Decision and Game Theory
  for Security (GameSec 2015)</comments><doi>10.13140/RG.2.1.3128.9446</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Access to the cloud has the potential to provide scalable and cost effective
enhancements of physical devices through the use of advanced computational
processes run on apparently limitless cyber infrastructure. On the other hand,
cyber-physical systems and cloud-controlled devices are subject to numerous
design challenges; among them is that of security. In particular, recent
advances in adversary technology pose Advanced Persistent Threats (APTs) which
may stealthily and completely compromise a cyber system. In this paper, we
design a framework for the security of cloud-based systems that specifies when
a device should trust commands from the cloud which may be compromised. This
interaction can be considered as a game between three players: a cloud
defender/administrator, an attacker, and a device. We use traditional signaling
games to model the interaction between the cloud and the device, and we use the
recently proposed FlipIt game to model the struggle between the defender and
attacker for control of the cloud. Because attacks upon the cloud can occur
without knowledge of the defender, we assume that strategies in both games are
picked according to prior commitment. This framework requires a new equilibrium
concept, which we call Gestalt Equilibrium, a fixed-point that expresses the
interdependence of the signaling and FlipIt games. We present the solution to
this fixed-point problem under certain parameter cases, and illustrate an
example application of cloud control of an unmanned vehicle. Our results
contribute to the growing understanding of cloud-controlled systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00592</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00592</id><created>2015-07-02</created><updated>2015-10-20</updated><authors><author><keyname>Rahaman</keyname><forenames>Ramij</forenames></author><author><keyname>Kar</keyname><forenames>Guruprasad</forenames></author></authors><title>GHZ correlation provides secure Anonymous Veto Protocol</title><categories>quant-ph cs.CR</categories><comments>5 pages, In this version we provide detail security proof of our
  quantum protocols</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Anonymous Veto (AV) and Dining cryptographers (DC) are two basic primitives
for the cryptographic problems where the main aim is to hide the identity of
the senders of the messages. These can be achieved by classical methods where
the security is based either on computational hardness or on shared private
keys. In this regard, we present a secure quantum protocol for both DC and AV
by exploiting the GHZ correlations. We first solve a generalized version of the
DC problem with the help of multiparty GHZ state. This allow us to provide a
secure quantum protocol for the AV. Securities for both the protocols rely on
some novel and fundamental features of GHZ correlations related to quantum
nonlocality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00598</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00598</id><created>2015-07-02</created><authors><author><keyname>Zou</keyname><forenames>Yulong</forenames></author><author><keyname>Zhu</keyname><forenames>Jia</forenames></author><author><keyname>Yang</keyname><forenames>Liuqing</forenames></author><author><keyname>Liang</keyname><forenames>Ying-Chang</forenames></author><author><keyname>Yao</keyname><forenames>Yu-Dong</forenames></author></authors><title>Securing Physical-Layer Communications for Cognitive Radio Networks</title><categories>cs.IT math.IT</categories><comments>15 pages in IEEE Communications Magazine, September 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article investigates the physical-layer security of cognitive radio (CR)
networks, which are vulnerable to various newly arising attacks targeting on
the weaknesses of CR communications and networking. We first review a range of
physical-layer attacks in CR networks, including the primary user emulation,
sensing falsification, intelligence compromise, jamming and eavesdropping
attacks. Then we focus on the physical-layer security of CR networks against
eavesdropping and examine the secrecy performance of cognitive communications
in terms of secrecy outage probability. We further consider the use of relays
for improving the CR security against eavesdropping and propose an
opportunistic relaying scheme, where a relay node that makes CR communications
most resistant to eavesdropping is chosen to participate in assisting the
transmission from a cognitive source to its destination. It is illustrated that
the physical-layer secrecy of CR communications relying on the opportunistic
relaying can be significantly improved by increasing the number of relays,
showing the security benefit of exploiting relay nodes. Finally, we present
some open challenges in the field of relays assisted physical-layer security
for CR networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00600</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00600</id><created>2015-07-02</created><authors><author><keyname>Konstantinidis</keyname><forenames>Stavros</forenames></author><author><keyname>Mastnak</keyname><forenames>Mitja</forenames></author></authors><title>Embedding rationally independent languages into maximal ones</title><categories>cs.FL</categories><comments>20 pages, 5 figures</comments><msc-class>68Q45, 94A45, 94B99</msc-class><acm-class>F.4.3; E.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the embedding problem in coding theory: given an independence (a
code-related property) and an independent language $L$, find a maximal
independent language containing $L$. We consider the case where the
code-related property is defined via a rational binary relation that is
decreasing with respect to any fixed total order on the set of words. Our
method works by iterating a max-min operator that has been used before for the
embedding problem for properties defined by length-increasing-and-transitive
binary relations. By going to order-decreasing rational relations, represented
by input-decreasing transducers, we are able to include many known properties
from both the noiseless and noisy domains of coding theory, as well as any
combination of such properties. Moreover, in many cases the desired maximal
embedding is effectively computable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00604</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00604</id><created>2015-07-02</created><updated>2015-07-17</updated><authors><author><keyname>Borges</keyname><forenames>Hudson</forenames></author><author><keyname>Valente</keyname><forenames>Marco Tulio</forenames></author><author><keyname>Hora</keyname><forenames>Andre</forenames></author><author><keyname>Coelho</keyname><forenames>Jailton</forenames></author></authors><title>On the Popularity of GitHub Applications: A Preliminary Note</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  GitHub is the world's largest collection of open source software. Therefore,
it is important both to software developers and users to compare and track the
popularity of GitHub repositories. In this paper, we propose a framework to
assess the popularity of GitHub software, using their number of stars. We also
propose a set of popularity growth patterns, which describe the evolution of
the number of stars of a system over time. We show that stars tend to correlate
with other measures, like forks, and with the effective usage of GitHub
software by third-party programs. Throughout the paper we illustrate the
application of our framework using real data extracted from GitHub.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00610</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00610</id><created>2015-07-02</created><authors><author><keyname>Lachgar</keyname><forenames>A.</forenames></author><author><keyname>Achahbar</keyname><forenames>A.</forenames></author></authors><title>Network growth with preferential attachment and without &quot;rich get
  richer&quot; mechanism</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><doi>10.1142/S0129183116500200</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a simple preferential attachment model of growing network using
the complementary probability of Barab\'asi-Albert (BA) model, i.e., $\Pi(k_i)
\propto 1-\frac{k_i}{\sum_j k_j}$. In this network, new nodes are
preferentially attached to not well connected nodes. Numerical simulations, in
perfect agreement with the master equation solution, give an exponential degree
distribution. This suggests that the power law degree distribution is a
consequence of preferential attachment probability together with &quot;rich get
richer&quot; phenomena. We also calculate the average degree of a target node at
time t $(&lt;k_s(t)&gt;)$ and its fluctuations, to have a better view of the
microscopic evolution of the network, and we also compare the results with BA
model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00623</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00623</id><created>2015-07-02</created><authors><author><keyname>Brenguier</keyname><forenames>Romain</forenames></author><author><keyname>Raskin</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author><author><keyname>Sankur</keyname><forenames>Ocan</forenames></author></authors><title>Assume-Admissible Synthesis</title><categories>cs.LO cs.GT</categories><comments>31 pages</comments><acm-class>D.2.4; F.3.1; I.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a novel rule for synthesis of reactive systems,
applicable to systems made of n components which have each their own
objectives. It is based on the notion of admissible strategies. We compare our
novel rule with previous rules defined in the literature, and we show that
contrary to the previous proposals, our rule defines sets of solutions which
are rectangular. This property leads to solutions which are robust and
resilient. We provide algorithms with optimal complexity and also an
abstraction framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00627</identifier>
 <datestamp>2015-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00627</id><created>2015-07-02</created><authors><author><keyname>Mihai</keyname><forenames>Florin-Constantin</forenames></author><author><keyname>Ursu</keyname><forenames>Adrian</forenames></author><author><keyname>Ichim</keyname><forenames>Pavel</forenames></author><author><keyname>Chelaru</keyname><forenames>Dan-Adrian</forenames></author></authors><title>Determining rural areas vulnerable to illegal dumping using GIS
  techniques. Case study: Neamt county, Romania</title><categories>cs.OH</categories><acm-class>H.2.8; J.2; I.4.8</acm-class><journal-ref>13th International Multidisciplinary Scientific GeoConference on
  ECOLOGY, ECONOMICS, EDUCATION AND LEGISLATION, SGEM 2013 Conference
  Proceedings vol 1 : 275-282</journal-ref><doi>10.5593/SGEM2013/BE5.V1/S20.037</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper aims to mapping the potential vulnerable areas to illegal dumping
of household waste from rural areas in the extra- Carpathian region of Neamt
County. These areas are ordinary in the proximity of built-up areas and buffers
areas of 1 km were delimited for every locality. Based on various map layers in
vector formats (land use, rivers, built-up areas, roads etc) an assessment
method is performed to highlight the potential areas vulnerable to illegal
dumping inside these buffer areas at local scale. The results are correlated to
field observations and current situation of waste management systems. The maps
outline local disparities due to various geographical conditions of county.
This approach is a necessary tool in EIA studies particularly for rural waste
management systems at local and regional scale which are less studied in
current literature than urban areas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00629</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00629</id><created>2015-07-02</created><authors><author><keyname>Elkhalil</keyname><forenames>Khalil</forenames></author><author><keyname>Kammoun</keyname><forenames>Abla</forenames></author><author><keyname>Al-Naffouri</keyname><forenames>Tareq Y</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Analytical Derivation of the Inverse Moments of One-sided Correlated
  Gram Matrices with Applications</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the development of analytical tools for the computation
of the moments of random Gram matrices with one side correlation. Such a
question is mainly driven by applications in signal processing and wireless
communications wherein such matrices naturally arise. In particular, we derive
closed-form expressions for the inverse moments and show that the obtained
results can help approximate several performance metrics such as the average
estimation error corresponding to the Best Linear Unbiased Estimator (BLUE) and
the Linear Minimum Mean Square Error LMMSE or also other loss functions used to
measure the accuracy of covariance matrix estimates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00639</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00639</id><created>2015-07-02</created><authors><author><keyname>Clarke</keyname><forenames>Daoud</forenames></author></authors><title>Simple, Fast Semantic Parsing with a Tensor Kernel</title><categories>cs.CL</categories><comments>in CICLing 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a simple approach to semantic parsing based on a tensor product
kernel. We extract two feature vectors: one for the query and one for each
candidate logical form. We then train a classifier using the tensor product of
the two vectors. Using very simple features for both, our system achieves an
average F1 score of 40.1% on the WebQuestions dataset. This is comparable to
more complex systems but is simpler to implement and runs faster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00640</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00640</id><created>2015-07-02</created><updated>2015-12-03</updated><authors><author><keyname>Fiala</keyname><forenames>Ji&#x159;&#xed;</forenames></author><author><keyname>Gaven&#x10d;iak</keyname><forenames>Tom&#xe1;&#x161;</forenames></author><author><keyname>Knop</keyname><forenames>Du&#x161;an</forenames></author><author><keyname>Kouteck&#xfd;</keyname><forenames>Martin</forenames></author><author><keyname>Kratochv&#xed;l</keyname><forenames>Jan</forenames></author></authors><title>Fixed parameter complexity of distance constrained labeling and uniform
  channel assignment problems</title><categories>cs.DM</categories><comments>14 pages, 4 figers</comments><msc-class>05C78</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study computational complexity of the class of distance-constrained graph
labeling problems from the fixed parameter tractability point of view. The
parameters studied are neighborhood diversity and clique width.
  We rephrase the distance constrained graph labeling problem as a specific
uniform variant of the Channel Assignment problem and show that this problem is
fixed parameter tractable when parameterized by the neighborhood diversity
together with the largest weight. Consequently, every $L(p_1, p_2, \dots,
p_k)$-labeling problem is FPT when parameterized by the neighborhood diversity,
the maximum $p_i$ and $k.$
  Our results yield also FPT algorithms for all $L(p_1, p_2, \dots,
p_k)$-labeling problems when parameterized by the size of a minimum vertex
cover, answering an open question of Fiala et al.: Parameterized complexity of
coloring problems: Treewidth versus vertex cover. The same consequence applies
on Channel Assignment when the maximum weight is additionally included among
the parameters.
  Finally, we show that the uniform variant of the Channel Assignment problem
becomes NP-complete when generalized to graphs of bounded clique width.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00644</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00644</id><created>2015-07-02</created><updated>2015-07-13</updated><authors><author><keyname>Houston</keyname><forenames>Kevin</forenames></author></authors><title>Compressed Manifold Modes: Fast Calculation and Natural Ordering</title><categories>cs.CG cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed manifold modes are locally supported analogues of eigenfunctions
of the Laplace-Beltrami operator of a manifold. In this paper we describe an
algorithm for the calculation of modes for discrete manifolds that, in
experiments, requires on average 47% fewer iterations and 44% less time than
the previous algorithm. We show how to naturally order the modes in an
analogous way to eigenfunctions, that is we define a compressed eigenvalue.
Furthermore, in contrast to the previous algorithm we permit unlumped mass
matrices for the operator and we show, unlike the case of eigenfunctions, that
modes can, in general, be oriented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00646</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00646</id><created>2015-07-02</created><authors><author><keyname>Schulte</keyname><forenames>Oliver</forenames></author><author><keyname>Qian</keyname><forenames>Zhensong</forenames></author></authors><title>SQL for SRL: Structure Learning Inside a Database System</title><categories>cs.LG cs.DB</categories><comments>3 pages, 1 figure, Position Paper of the Fifth International Workshop
  on Statistical Relational AI at UAI 2015</comments><acm-class>H.2.8; H.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The position we advocate in this paper is that relational algebra can provide
a unified language for both representing and computing with
statistical-relational objects, much as linear algebra does for traditional
single-table machine learning. Relational algebra is implemented in the
Structured Query Language (SQL), which is the basis of relational database
management systems. To support our position, we have developed the FACTORBASE
system, which uses SQL as a high-level scripting language for
statistical-relational learning of a graphical model structure. The design
philosophy of FACTORBASE is to manage statistical models as first-class
citizens inside a database. Our implementation shows how our SQL constructs in
FACTORBASE facilitate fast, modular, and reliable program development.
Empirical evidence from six benchmark databases indicates that leveraging
database system capabilities achieves scalable model structure learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00648</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00648</id><created>2015-07-02</created><authors><author><keyname>Hajiaghayi</keyname><forenames>MohammadTaghi</forenames></author><author><keyname>Kortsarz</keyname><forenames>Guy</forenames></author><author><keyname>MacDavid</keyname><forenames>Robert</forenames></author><author><keyname>Purohit</keyname><forenames>Manish</forenames></author><author><keyname>Sarpatwar</keyname><forenames>Kanthi</forenames></author></authors><title>Approximation Algorithms for Connected Maximum Cut and Related Problems</title><categories>cs.DS</categories><comments>17 pages, Conference version to appear in ESA 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An instance of the Connected Maximum Cut problem consists of an undirected
graph G = (V, E) and the goal is to find a subset of vertices S $\subseteq$ V
that maximizes the number of edges in the cut \delta(S) such that the induced
graph G[S] is connected. We present the first non-trivial \Omega(1/log n)
approximation algorithm for the connected maximum cut problem in general graphs
using novel techniques. We then extend our algorithm to an edge weighted case
and obtain a poly-logarithmic approximation algorithm. Interestingly, in stark
contrast to the classical max-cut problem, we show that the connected maximum
cut problem remains NP-hard even on unweighted, planar graphs. On the positive
side, we obtain a polynomial time approximation scheme for the connected
maximum cut problem on planar graphs and more generally on graphs with bounded
genus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00650</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00650</id><created>2015-07-02</created><authors><author><keyname>Shafranovich</keyname><forenames>Yakov</forenames></author></authors><title>Bluetooth Data Exchange Between Android Phones Without Pairing</title><categories>cs.NI</categories><comments>7 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe a novel method of exchanging data between Bluetooth
smartphones on the Android platform without requiring any pairing of the
devices. We discuss our approach of encoding and decoding data inside the UUIDs
used by the Bluetooth Service Discovery Protocol (SDP). Future research remains
to be done on the latency, bandwidth and compatibility of this approach, as
well as the possibility of utilizing other protocols in conjunction with this
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00655</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00655</id><created>2015-07-02</created><authors><author><keyname>Hannula</keyname><forenames>Miika</forenames></author></authors><title>Reasoning about embedded dependencies using inclusion dependencies</title><categories>cs.LO</categories><msc-class>03B60, 03B70</msc-class><acm-class>F.4.1; H.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The implication problem for the class of embedded dependencies is
undecidable. However, this does not imply lackness of a proof procedure as
exemplified by the chase algorithm. In this paper we present a complete
axiomatization of embedded dependencies that is based on the chase and uses
inclusion dependencies and implicit existential quantification in the
intermediate steps of deductions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00662</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00662</id><created>2015-07-02</created><authors><author><keyname>Kenkre</keyname><forenames>Sreyash</forenames></author><author><keyname>Pandit</keyname><forenames>Vinayaka</forenames></author><author><keyname>Purohit</keyname><forenames>Manish</forenames></author><author><keyname>Saket</keyname><forenames>Rishi</forenames></author></authors><title>On the Approximability of Digraph Ordering</title><categories>cs.DS</categories><comments>21 pages, Conference version to appear in ESA 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an n-vertex digraph D = (V, A) the Max-k-Ordering problem is to compute
a labeling $\ell : V \to [k]$ maximizing the number of forward edges, i.e.
edges (u,v) such that $\ell$(u) &lt; $\ell$(v). For different values of k, this
reduces to Maximum Acyclic Subgraph (k=n), and Max-Dicut (k=2). This work
studies the approximability of Max-k-Ordering and its generalizations,
motivated by their applications to job scheduling with soft precedence
constraints. We give an LP rounding based 2-approximation algorithm for
Max-k-Ordering for any k={2,..., n}, improving on the known
2k/(k-1)-approximation obtained via random assignment. The tightness of this
rounding is shown by proving that for any k={2,..., n} and constant
$\varepsilon &gt; 0$, Max-k-Ordering has an LP integrality gap of 2 -
$\varepsilon$ for $n^{\Omega\left(1/\log\log k\right)}$ rounds of the
Sherali-Adams hierarchy.
  A further generalization of Max-k-Ordering is the restricted maximum acyclic
subgraph problem or RMAS, where each vertex v has a finite set of allowable
labels $S_v \subseteq \mathbb{Z}^+$. We prove an LP rounding based
$4\sqrt{2}/(\sqrt{2}+1) \approx 2.344$ approximation for it, improving on the
$2\sqrt{2} \approx 2.828$ approximation recently given by Grandoni et al.
(Information Processing Letters, Vol. 115(2), Pages 182-185, 2015). In fact,
our approximation algorithm also works for a general version where the
objective counts the edges which go forward by at least a positive offset
specific to each edge.
  The minimization formulation of digraph ordering is DAG edge deletion or
DED(k), which requires deleting the minimum number of edges from an n-vertex
directed acyclic graph (DAG) to remove all paths of length k. We show that
both, the LP relaxation and a local ratio approach for DED(k) yield
k-approximation for any $k\in [n]$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00672</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00672</id><created>2015-07-02</created><authors><author><keyname>Ara</keyname><forenames>Pooneh M.</forenames></author><author><keyname>James</keyname><forenames>Ryan G.</forenames></author><author><keyname>Crutchfield</keyname><forenames>James P.</forenames></author></authors><title>The Elusive Present: Hidden Past and Future Dependency and Why We Build
  Models</title><categories>cond-mat.stat-mech cs.IT math.DS math.IT nlin.CD stat.ML</categories><comments>12 pages, 10 figures;
  http://csc.ucdavis.edu/~cmg/compmech/pubs/tep.htm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modeling a temporal process as if it is Markovian assumes the present encodes
all of the process's history. When this occurs, the present captures all of the
dependency between past and future. We recently showed that if one randomly
samples in the space of structured processes, this is almost never the case.
So, how does the Markov failure come about? That is, how do individual
measurements fail to encode the past? And, how many are needed to capture
dependencies between the past and future? Here, we investigate how much
information can be shared between the past and future, but not be reflected in
the present. We quantify this elusive information, give explicit calculational
methods, and draw out the consequences. The most important of which is that
when the present hides past-future dependency we must move beyond
sequence-based statistics and build state-based models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00674</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00674</id><created>2015-07-02</created><authors><author><keyname>Freire</keyname><forenames>Cibele</forenames></author><author><keyname>Gatterbauer</keyname><forenames>Wolfgang</forenames></author><author><keyname>Immerman</keyname><forenames>Neil</forenames></author><author><keyname>Meliou</keyname><forenames>Alexandra</forenames></author></authors><title>A Characterization of the Complexity of Resilience and Responsibility
  for Self-join-free Conjunctive Queries</title><categories>cs.DB cs.CC</categories><comments>36 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several research thrusts in the area of data management have focused on
understanding how changes in the data affect the output of a view or standing
query. Example applications are explaining query results, propagating updates
through views, and anonymizing datasets. These applications usually rely on
understanding how interventions in a database impact the output of a query. An
important aspect of this analysis is the problem of deleting a minimum number
of tuples from the input tables to make a given Boolean query false. We refer
to this problem as &quot;the resilience of a query&quot; and show its connections to the
well-studied problems of deletion propagation and causal responsibility. In
this paper, we study the complexity of resilience for self-join-free
conjunctive queries, and also make several contributions to previous known
results for the problems of deletion propagation with source side-effects and
causal responsibility: (1) We define the notion of resilience and provide a
complete dichotomy for the class of self-join-free conjunctive queries with
arbitrary functional dependencies; this dichotomy also extends and generalizes
previous tractability results on deletion propagation with source side-effects.
(2) We formalize the connection between resilience and causal responsibility,
and show that resilience has a larger class of tractable queries than
responsibility. (3) We identify a mistake in a previous dichotomy for the
problem of causal responsibility and offer a revised characterization based on
new, simpler, and more intuitive notions. (4) Finally, we extend the dichotomy
for causal responsibility in two ways: (a) we treat cases where the input
tables contain functional dependencies, and (b) we compute responsibility for a
set of tuples specified via wildcards.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00677</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00677</id><created>2015-07-02</created><updated>2016-02-29</updated><authors><author><keyname>Miyato</keyname><forenames>Takeru</forenames></author><author><keyname>Maeda</keyname><forenames>Shin-ichi</forenames></author><author><keyname>Koyama</keyname><forenames>Masanori</forenames></author><author><keyname>Nakae</keyname><forenames>Ken</forenames></author><author><keyname>Ishii</keyname><forenames>Shin</forenames></author></authors><title>Distributional Smoothing with Virtual Adversarial Training</title><categories>stat.ML cs.LG</categories><comments>Under review as a conference paper at ICLR 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose local distributional smoothness (LDS), a new notion of smoothness
for statistical model that can be used as a regularization term to promote the
smoothness of the model distribution. We named the LDS based regularization as
virtual adversarial training (VAT). The LDS of a model at an input datapoint is
defined as the KL-divergence based robustness of the model distribution against
local perturbation around the datapoint. VAT resembles adversarial training,
but distinguishes itself in that it determines the adversarial direction from
the model distribution alone without using the label information, making it
applicable to semi-supervised learning. The computational cost for VAT is
relatively low. For neural network, the approximated gradient of the LDS can be
computed with no more than three pairs of forward and back propagations. When
we applied our technique to supervised and semi-supervised learning for the
MNIST dataset, it outperformed all the training methods other than the current
state of the art method, which is based on a highly advanced generative model.
We also applied our method to SVHN and NORB, and confirmed our method's
superior performance over the current state of the art semi-supervised method
applied to these datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00687</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00687</id><created>2015-07-02</created><authors><author><keyname>Ballard</keyname><forenames>Grey</forenames></author><author><keyname>Benson</keyname><forenames>Austin R.</forenames></author><author><keyname>Druinsky</keyname><forenames>Alex</forenames></author><author><keyname>Lipshitz</keyname><forenames>Benjamin</forenames></author><author><keyname>Schwartz</keyname><forenames>Oded</forenames></author></authors><title>Improving the numerical stability of fast matrix multiplication
  algorithms</title><categories>cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fast algorithms for matrix multiplication, or those that perform
asymptotically fewer scalar operations than the classical algorithm, have been
considered primarily of theoretical interest. Aside from Strassen's original
algorithm, few fast algorithms have been efficiently implemented or used in
practical applications. However, there exist many practical alternatives to
Strassen's algorithm with varying performance and numerical properties. While
fast algorithms are known to be numerically stable, their error bounds are
slightly weaker than the classical algorithm.
  We argue in this paper that the numerical sacrifice of fast algorithms,
particularly for the typical use cases of practical algorithms, is not
prohibitive, and we explore ways to improve the accuracy both theoretically and
empirically. The numerical accuracy of fast matrix multiplication depends on
properties of the algorithm and of the input matrices, and we consider both
contributions independently. We generalize and tighten previous error analyses
of fast algorithms, compare the properties among the class of known practical
fast algorithms, and discuss algorithmic techniques for improving the error
guarantees. We also present means for reducing the numerical inaccuracies
generated by anomalous input matrices using diagonal scaling matrices. Finally,
we include empirical results that test the various improvement techniques, in
terms of both their numerical accuracy and their performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00691</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00691</id><created>2015-07-02</created><updated>2016-03-08</updated><authors><author><keyname>Spencer</keyname><forenames>Gwen</forenames></author></authors><title>Sticky Seeding in Discrete-Time Reversible-Threshold Networks</title><categories>cs.SI</categories><comments>19 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When nodes can repeatedly update their behavior (as in agent-based models
from computational social science or repeated-game play settings) the problem
of optimal network seeding becomes very complex. For a popular
spreading-phenomena model of binary-behavior updating based on thresholds of
adoption among neighbors, we consider several planning problems in the design
of \textit{Sticky Interventions}: when adoption decisions are reversible, the
planner aims to find a Seed Set where temporary intervention leads to long-term
behavior change. We prove that completely converting a network at minimum cost
is $\Omega(\ln (OPT) )$-hard to approximate and that maximizing conversion
subject to a budget is $(1-\frac{1}{e})$-hard to approximate. Optimization
heuristics which rely on many objective function evaluations may still be
practical, particularly in relatively-sparse networks: we prove that the
long-term impact of a Seed Set can be evaluated in $O(|E|^2)$ operations. For a
more descriptive model variant in which some neighbors may be more influential
than others, we show that under integer edge weights from $\{0,1,2,...,k\}$
objective function evaluation requires only $O(k|E|^2)$ operations. These
operation bounds are based on improvements we give for bounds on
time-steps-to-convergence under discrete-time reversible-threshold updates in
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00695</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00695</id><created>2015-07-02</created><authors><author><keyname>DeFord</keyname><forenames>Daryl R.</forenames></author><author><keyname>Pauls</keyname><forenames>Scott D.</forenames></author></authors><title>Network models that reflect multiplex dynamics</title><categories>cs.SI cs.MA physics.soc-ph</categories><comments>8 pages, 8 figures</comments><msc-class>90B10, 91D30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many of the systems that are traditionally analyzed as complex networks have
natural interpretations as multiplex structures. While these formulations
retain more information than standard network models, there is not yet a fully
developed theory for computing network metrics and statistics on these objects.
As many of the structural representations associated to these models can
distort the underlying characteristics of dynamical process, we introduce an
algebraic method for modeling arbitrary dynamics on multiplex networks. Since
several network metrics are based on generalized notions of dynamical transfer,
we can use this framework to extend many of the standard network metrics to
multiplex structures in a consistent fashion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00700</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00700</id><created>2015-07-02</created><updated>2015-10-19</updated><authors><author><keyname>Je&#x159;&#xe1;bek</keyname><forenames>Emil</forenames></author></authors><title>A note on the substructural hierarchy</title><categories>cs.LO math.LO</categories><comments>11 pages; to appear in Mathematical Logic Quarterly</comments><msc-class>03B47</msc-class><journal-ref>Mathematical Logic Quarterly 62 (2016), no. 1--2, pp. 102--110</journal-ref><doi>10.1002/malq.201500066</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that all axiomatic extensions of the full Lambek calculus with
exchange can be axiomatized by formulas on the $\mathcal N_3$ level of the
substructural hierarchy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00701</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00701</id><created>2015-07-02</created><authors><author><keyname>Gupta</keyname><forenames>Abhishek K.</forenames></author><author><keyname>Zhang</keyname><forenames>Xinchen</forenames></author><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author></authors><title>SINR and Throughput Scaling in Ultradense Urban Cellular Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a dense urban cellular network where the base stations (BSs) are
stacked vertically as well as extending infinitely in the horizontal plane,
resulting in a greater than two dimensional (2D) deployment. Using a dual-slope
path loss model that is well supported empirically, we extend recent 2D
coverage probability and potential throughput results to 3 dimensions. We prove
that the &quot;critical close-in path loss exponent&quot; $\alpha_0$ where SINR
eventually decays to zero is equal to the dimensionality $d$, i.e. $\alpha_0
\leq 3$ results in an eventual SINR of 0 in a 3D network. We also show that the
potential (i.e. best case) aggregate throughput decays to zero for $\alpha_0 &lt;
d/2$. Both of these scaling results also hold for the more realistic case that
we term ${3\rm{D}^{+}}$, where there are no BSs below the user, as in a dense
urban network with the user on or near the ground.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00702</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00702</id><created>2015-07-02</created><authors><author><keyname>Bertsekas</keyname><forenames>Dimitri P.</forenames></author></authors><title>Centralized and Distributed Newton Methods for Network Optimization and
  Extensions</title><categories>cs.NA math.OC</categories><report-no>Report LIDS - 2866, Lab. for Information and Decision Systems,
  Massachusetts Institute of Technology, Cambridge, MA</report-no><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  We consider Newton methods for common types of single commodity and
multi-commodity network flow problems. Despite the potentially very large
dimension of the problem, they can be implemented using the conjugate gradient
method and low-dimensional network operations, as shown nearly thirty years
ago. We revisit these methods, compare them to more recent proposals, and
describe how they can be implemented in a distributed computing system. We also
discuss generalizations, including the treatment of arc gains, linear side
constraints, and related special structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00710</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00710</id><created>2015-07-02</created><updated>2015-11-11</updated><authors><author><keyname>Kyng</keyname><forenames>Rasmus</forenames></author><author><keyname>Rao</keyname><forenames>Anup</forenames></author><author><keyname>Sachdeva</keyname><forenames>Sushant</forenames></author></authors><title>Fast, Provable Algorithms for Isotonic Regression in all
  $\ell_{p}$-norms</title><categories>cs.LG cs.DS math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a directed acyclic graph $G,$ and a set of values $y$ on the vertices,
the Isotonic Regression of $y$ is a vector $x$ that respects the partial order
described by $G,$ and minimizes $||x-y||,$ for a specified norm. This paper
gives improved algorithms for computing the Isotonic Regression for all
weighted $\ell_{p}$-norms with rigorous performance guarantees. Our algorithms
are quite practical, and their variants can be implemented to run fast in
practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00712</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00712</id><created>2015-07-02</created><authors><author><keyname>Reddy</keyname><forenames>B. Prashanth</forenames></author></authors><title>New Class of Pseudorandom D-sequences to Generate Cryptographic Keys</title><categories>cs.CR</categories><comments>8 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article proposes the use of pseudorandom decimal sequences that have
gone through an additional random mapping for the design of cryptographic keys.
These sequences are generated by starting with inverse prime expansions in base
3 and then replacing 2 in the expansion with either the string 01 or 10 based
on the preceding bit, which represents a general mapping. We show that the
resulting pseudorandom sequences have excellent autocorrelation properties.
Such a method can be extended to inverse prime expansions to any base.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00717</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00717</id><created>2015-07-02</created><authors><author><keyname>Nax</keyname><forenames>Heinrich H.</forenames></author><author><keyname>Perc</keyname><forenames>Matjaz</forenames></author><author><keyname>Szolnoki</keyname><forenames>Attila</forenames></author><author><keyname>Helbing</keyname><forenames>Dirk</forenames></author></authors><title>Stability of cooperation under image scoring in group interactions</title><categories>q-bio.PE cs.GT physics.soc-ph</categories><comments>6 two-column pages, 4 figures; accepted for publication in Scientific
  Reports</comments><journal-ref>Sci. Rep. 5 (2015) 12145</journal-ref><doi>10.1038/srep12145</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image scoring sustains cooperation in the repeated two-player prisoner's
dilemma through indirect reciprocity, even though defection is the uniquely
dominant selfish behaviour in the one-shot game. Many real-world dilemma
situations, however, firstly, take place in groups and, secondly, lack the
necessary transparency to inform subjects reliably of others' individual past
actions. Instead, there is revelation of information regarding groups, which
allows for `group scoring' but not for image scoring. Here, we study how
sensitive the positive results related to image scoring are to information
based on group scoring. We combine analytic results and computer simulations to
specify the conditions for the emergence of cooperation. We show that under
pure group scoring, that is, under the complete absence of image-scoring
information, cooperation is unsustainable. Away from this extreme case,
however, the necessary degree of image scoring relative to group scoring
depends on the population size and is generally very small. We thus conclude
that the positive results based on image scoring apply to a much broader range
of informational settings that are relevant in the real world than previously
assumed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00723</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00723</id><created>2015-07-02</created><updated>2015-12-07</updated><authors><author><keyname>Meyer</keyname><forenames>Bertrand</forenames></author></authors><title>Theory of Programs</title><categories>cs.PL cs.SE</categories><acm-class>D.1.4; D.1.5; D.1.3; D.2.1; D.2.4; D.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A general theory of programs, programming and programming languages built up
from a few concepts of elementary set theory. Derives, as theorems, properties
treated as axioms by classic approaches to programming. Covers sequential and
concurrent computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00739</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00739</id><created>2015-07-02</created><updated>2016-02-16</updated><authors><author><keyname>Harrow</keyname><forenames>Aram W.</forenames></author><author><keyname>Montanaro</keyname><forenames>Ashley</forenames></author></authors><title>Extremal eigenvalues of local Hamiltonians</title><categories>quant-ph cs.CC</categories><comments>5 pages; v2: additional discussion and minor changes to presentation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We apply classical algorithms for approximately solving constraint
satisfaction problems to find bounds on extremal eigenvalues of local
Hamiltonians. We consider spin Hamiltonians for which we have an upper bound on
the number of terms in which each spin participates, and find extensive bounds
for the operator norm and ground-state energy of such Hamiltonians under this
constraint. In each case the bound is achieved by a product state which can be
found efficiently using a classical algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00748</identifier>
 <datestamp>2015-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00748</id><created>2015-07-02</created><authors><author><keyname>Efsandiari</keyname><forenames>Hossein</forenames></author><author><keyname>Hajiaghyi</keyname><forenames>MohammadTaghi</forenames></author><author><keyname>Koenemann</keyname><forenames>Jochen</forenames></author><author><keyname>Mahini</keyname><forenames>Hamid</forenames></author><author><keyname>Malec</keyname><forenames>David</forenames></author><author><keyname>Sanita</keyname><forenames>Laura</forenames></author></authors><title>Approximate Deadline-Scheduling with Precedence Constraints</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the classic problem of scheduling a set of n jobs
non-preemptively on a single machine. Each job j has non-negative processing
time, weight, and deadline, and a feasible schedule needs to be consistent with
chain-like precedence constraints. The goal is to compute a feasible schedule
that minimizes the sum of penalties of late jobs. Lenstra and Rinnoy Kan
[Annals of Disc. Math., 1977] in their seminal work introduced this problem and
showed that it is strongly NP-hard, even when all processing times and weights
are 1. We study the approximability of the problem and our main result is an
O(log k)-approximation algorithm for instances with k distinct job deadlines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00772</identifier>
 <datestamp>2015-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00772</id><created>2015-07-02</created><authors><author><keyname>Afek</keyname><forenames>Yehuda</forenames></author><author><keyname>Kecher</keyname><forenames>Roman</forenames></author><author><keyname>Sulamy</keyname><forenames>Moshe</forenames></author></authors><title>Optimal and Resilient Pheromone Utilization in Ant Foraging</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pheromones are a chemical substance produced and released by ants as means of
communication. In this work we present the minimum amount of pheromones
necessary and sufficient for a colony of ants (identical mobile agents) to
deterministically find a food source (treasure), assuming that each ant has the
computational capabilities of either a Finite State Machine (FSM) or a Turing
Machine (TM). In addition, we provide pheromone-based foraging algorithms
capable of handling fail-stop faults.
  In more detail, we consider the case where $k$ identical ants, initially
located at the center (nest) of an infinite two-dimensional grid and
communicate only through pheromones, perform a collaborative search for an
adversarially hidden treasure placed at an unknown distance $D$. We begin by
proving a tight lower bound of $\Omega(D)$ on the amount of pheromones required
by any number of FSM based ants to complete the search, and continue to reduce
the lower bound to $\Omega(k)$ for the stronger ants modeled as TM. We provide
algorithms which match the aforementioned lower bounds, and still terminate in
optimal $\mathcal{O}(D + D^2 / k)$ time, under both the synchronous and
asynchronous models. Furthermore, we consider a more realistic setting, where
an unknown number $f &lt; k$ of ants may fail-stop at any time; we provide
fault-tolerant FSM algorithms (synchronous and asynchronous), that terminate in
$\mathcal{O}(D + D^2/(k-f) + Df)$ rounds and emit no more than the same
asymptotic minimum number of $\mathcal{O}(D)$ pheromones overall.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00773</identifier>
 <datestamp>2015-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00773</id><created>2015-07-02</created><authors><author><keyname>Azar</keyname><forenames>Yossi</forenames><affiliation>Seffi</affiliation></author><author><keyname>Kalp-Shaltiel</keyname><forenames>Inna</forenames><affiliation>Seffi</affiliation></author><author><keyname>Lucier</keyname><forenames>Brendan</forenames><affiliation>Seffi</affiliation></author><author><keyname>Menache</keyname><forenames>Ishai</forenames><affiliation>Seffi</affiliation></author><author><keyname>Joseph</keyname><affiliation>Seffi</affiliation></author><author><keyname>Naor</keyname></author><author><keyname>Yaniv</keyname><forenames>Jonathan</forenames></author></authors><title>Truthful Online Scheduling with Commitments</title><categories>cs.DS cs.GT</categories><acm-class>F.2.2; K.6.2</acm-class><doi>10.1145/2764468.2764535</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study online mechanisms for preemptive scheduling with deadlines, with the
goal of maximizing the total value of completed jobs. This problem is
fundamental to deadline-aware cloud scheduling, but there are strong lower
bounds even for the algorithmic problem without incentive constraints. However,
these lower bounds can be circumvented under the natural assumption of deadline
slackness, i.e., that there is a guaranteed lower bound $s &gt; 1$ on the ratio
between a job's size and the time window in which it can be executed.
  In this paper, we construct a truthful scheduling mechanism with a constant
competitive ratio, given slackness $s &gt; 1$. Furthermore, we show that if $s$ is
large enough then we can construct a mechanism that also satisfies a commitment
property: it can be determined whether or not a job will finish, and the
requisite payment if so, well in advance of each job's deadline. This is
notable because, in practice, users with strict deadlines may find it
unacceptable to discover only very close to their deadline that their job has
been rejected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00784</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00784</id><created>2015-07-02</created><updated>2015-07-11</updated><authors><author><keyname>Souza</keyname><forenames>Th&#xe1;rsis Tuani Pinto</forenames></author><author><keyname>Kolchyna</keyname><forenames>Olga</forenames></author><author><keyname>Treleaven</keyname><forenames>Philip C.</forenames></author><author><keyname>Aste</keyname><forenames>Tomaso</forenames></author></authors><title>Twitter Sentiment Analysis Applied to Finance: A Case Study in the
  Retail Industry</title><categories>cs.CY cs.SI q-fin.CP</categories><comments>23 pages, 5 figures, 9 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a financial analysis over Twitter sentiment analytics
extracted from listed retail brands. We investigate whether there is
statistically-significant information between the Twitter sentiment and volume,
and stock returns and volatility. Traditional newswires are also considered as
a proxy for the market sentiment for comparative purpose. The results suggest
that social media is indeed a valuable source in the analysis of the financial
dynamics in the retail sector even when compared to mainstream news such as the
Wall Street Journal and Dow Jones Newswires.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00787</identifier>
 <datestamp>2015-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00787</id><created>2015-07-02</created><authors><author><keyname>Chen</keyname><forenames>Mingming</forenames></author><author><keyname>Kuzmin</keyname><forenames>Konstantin</forenames></author><author><keyname>Szymanski</keyname><forenames>Boleslaw K.</forenames></author></authors><title>Community Detection via Maximization of Modularity and Its Variants</title><categories>physics.soc-ph cs.SI</categories><journal-ref>IEEE Transactions on Computational Social Systems 1(1) March 2014,
  pp. 46-65</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we first discuss the definition of modularity (Q) used as a
metric for community quality and then we review the modularity maximization
approaches which were used for community detection in the last decade. Then, we
discuss two opposite yet coexisting problems of modularity optimization: in
some cases, it tends to favor small communities over large ones while in
others, large communities over small ones (so called the resolution limit
problem). Next, we overview several community quality metrics proposed to solve
the resolution limit problem and discuss Modularity Density (Qds) which
simultaneously avoids the two problems of modularity. Finally, we introduce two
novel fine-tuned community detection algorithms that iteratively attempt to
improve the community quality measurements by splitting and merging the given
network community structure. The first of them, referred to as Fine-tuned Q, is
based on modularity (Q) while the second one is based on Modularity Density
(Qds) and denoted as Fine-tuned Qds. Then, we compare the greedy algorithm of
modularity maximization (denoted as Greedy Q), Fine-tuned Q, and Fine-tuned Qds
on four real networks, and also on the classical clique network and the LFR
benchmark networks, each of which is instantiated by a wide range of
parameters. The results indicate that Fine-tuned Qds is the most effective
among the three algorithms discussed. Moreover, we show that Fine-tuned Qds can
be applied to the communities detected by other algorithms to significantly
improve their results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00789</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00789</id><created>2015-07-02</created><updated>2016-02-11</updated><authors><author><keyname>Wu</keyname><forenames>Yongpeng</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author><author><keyname>Ng</keyname><forenames>Derrick Wing Kwan</forenames></author><author><keyname>Xiao</keyname><forenames>Chengshan</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>Secure Massive MIMO Transmission with an Active Eavesdropper</title><categories>cs.IT math.IT</categories><comments>IEEE Transactions on Information Theory, in Revision. arXiv admin
  note: text overlap with arXiv:1505.01231</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate secure and reliable transmission strategies for
multi-cell multi-user massive multiple-input multiple-output (MIMO) systems
with a multi-antenna active eavesdropper. We consider a time-division duplex
system where uplink training is required and an active eavesdropper can attack
the training phase to cause pilot contamination at the transmitter. This forces
the precoder used in the subsequent downlink transmission phase to implicitly
beamform towards the eavesdropper, thus increasing its received signal power.
Assuming matched filter precoding and artificial noise (AN) generation at the
transmitter, we derive an asymptotic achievable secrecy rate when the number of
transmit antennas approaches infinity. For the case of a single-antenna active
eavesdropper, we obtain a closed-form expression for the optimal power
allocation policy for the transmit signal and the AN, and find the minimum
transmit power required to ensure reliable secure communication. Furthermore,
we show that the transmit antenna correlation diversity of the intended users
and the eavesdropper can be exploited in order to improve the secrecy rate. In
fact, under certain orthogonality conditions of the channel covariance
matrices, the secrecy rate loss introduced by the eavesdropper can be
completely mitigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00790</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00790</id><created>2015-07-02</created><updated>2015-08-11</updated><authors><author><keyname>Aleyasen</keyname><forenames>Amirhossein</forenames></author><author><keyname>Starov</keyname><forenames>Oleksii</forenames></author><author><keyname>Au</keyname><forenames>Alyssa Phung</forenames></author><author><keyname>Schiffman</keyname><forenames>Allan</forenames></author><author><keyname>Shrager</keyname><forenames>Jeff</forenames></author></authors><title>On the Privacy Practices of Just Plain Sites</title><categories>cs.CY</categories><comments>10 pages, 7 figures, 6 tables, 5 authors, and a partridge in a pear
  tree</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In addition to visiting high profile sites such as Facebook and Google, web
users often visit more modest sites, such as those operated by bloggers, or by
local organizations such as schools. Such sites, which we call &quot;Just Plain
Sites&quot; (JPSs) are likely to inadvertently represent greater privacy risks than
high profile sites by virtue of being unable to afford privacy expertise. To
assess the prevalence of the privacy risks to which JPSs may inadvertently be
exposing their visitors, we analyzed a number of easily observed privacy
practices of such sites. We found that many JPSs collect a great deal of
information from their visitors, share a great deal of information about their
visitors with third parties, permit a great deal of tracking of their visitors,
and use deprecated or unsafe security practices. Our goal in this work is not
to scold JPS operators, but to raise awareness of these facts among both JPS
operators and visitors, possibly encouraging the operators of such sites to
take greater care in their implementations, and visitors to take greater care
in how, when, and what they share.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00803</identifier>
 <datestamp>2015-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00803</id><created>2015-07-02</created><authors><author><keyname>Basse</keyname><forenames>Guillaume W.</forenames></author><author><keyname>Airoldi</keyname><forenames>Edoardo M.</forenames></author></authors><title>Optimal design of experiments in the presence of network-correlated
  outcomes</title><categories>stat.ME cs.SI physics.soc-ph stat.ML</categories><comments>29 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of how to assign treatment in a randomized
experiment, when the correlation among the outcomes is informed by a network
available pre-intervention. Working within the potential outcome causal
framework, we develop a class of models that posit such a correlation structure
among the outcomes, and a strategy for allocating treatment optimally, for the
goal of minimizing the integrated mean squared error of the estimated average
treatment effect. We provide insights into features of the optimal designs via
an analytical decomposition of the mean squared error used for optimization. We
illustrate how the proposed treatment allocation strategy improves on
allocations that ignore the network structure, with extensive simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00805</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00805</id><created>2015-07-02</created><updated>2015-07-06</updated><authors><author><keyname>Takabatake</keyname><forenames>Yoshimasa</forenames></author><author><keyname>Tabei</keyname><forenames>Yasuo</forenames></author><author><keyname>Sakamoto</keyname><forenames>Hiroshi</forenames></author></authors><title>Online Self-Indexed Grammar Compression</title><categories>cs.DS</categories><comments>To appear in the Proceedings of the 22nd edition of the International
  Symposium on String Processing and Information Retrieval (SPIRE2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although several grammar-based self-indexes have been proposed thus far,
their applicability is limited to offline settings where whole input texts are
prepared, thus requiring to rebuild index structures for given additional
inputs, which is often the case in the big data era. In this paper, we present
the first online self-indexed grammar compression named OESP-index that can
gradually build the index structure by reading input characters one-by-one.
Such a property is another advantage which enables saving a working space for
construction, because we do not need to store input texts in memory. We
experimentally test OESP-index on the ability to build index structures and
search query texts, and we show OESP-index's efficiency, especially
space-efficiency for building index structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00814</identifier>
 <datestamp>2015-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00814</id><created>2015-07-03</created><updated>2015-11-19</updated><authors><author><keyname>Stadie</keyname><forenames>Bradly C.</forenames></author><author><keyname>Levine</keyname><forenames>Sergey</forenames></author><author><keyname>Abbeel</keyname><forenames>Pieter</forenames></author></authors><title>Incentivizing Exploration In Reinforcement Learning With Deep Predictive
  Models</title><categories>cs.AI cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Achieving efficient and scalable exploration in complex domains poses a major
challenge in reinforcement learning. While Bayesian and PAC-MDP approaches to
the exploration problem offer strong formal guarantees, they are often
impractical in higher dimensions due to their reliance on enumerating the
state-action space. Hence, exploration in complex domains is often performed
with simple epsilon-greedy methods. In this paper, we consider the challenging
Atari games domain, which requires processing raw pixel inputs and delayed
rewards. We evaluate several more sophisticated exploration strategies,
including Thompson sampling and Boltzman exploration, and propose a new
exploration method based on assigning exploration bonuses from a concurrently
learned model of the system dynamics. By parameterizing our learned model with
a neural network, we are able to develop a scalable and efficient approach to
exploration bonuses that can be applied to tasks with complex, high-dimensional
state spaces. In the Atari domain, our method provides the most consistent
improvement across a range of games that pose a major challenge for prior
methods. In addition to raw game-scores, we also develop an AUC-100 metric for
the Atari Learning domain to evaluate the impact of exploration on this
benchmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00819</identifier>
 <datestamp>2015-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00819</id><created>2015-07-03</created><authors><author><keyname>Brucato</keyname><forenames>Matteo</forenames></author><author><keyname>Abouzied</keyname><forenames>Azza</forenames></author><author><keyname>Meliou</keyname><forenames>Alexandra</forenames></author></authors><title>Improving package recommendations through query relaxation</title><categories>cs.DB</categories><journal-ref>Matteo Brucato, Azza Abouzied, and Alexandra Meliou. Improving
  Package Recommendations Through Query Relaxation. In Proceedings of the 1st
  International DATA4U Workshop, in conjunction with VLDB, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recommendation systems aim to identify items that are likely to be of
interest to users. In many cases, users are interested in package
recommendations as collections of items. For example, a dietitian may wish to
derive a dietary plan as a collection of recipes that is nutritionally
balanced, and a travel agent may want to produce a vacation package as a
coordinated collection of travel and hotel reservations. Recent work has
explored extending recommendation systems to support packages of items. These
systems need to solve complex combinatorial problems, enforcing various
properties and constraints defined on sets of items. Introducing constraints on
packages makes recommendation queries harder to evaluate, but also harder to
express: Queries that are under-specified produce too many answers, whereas
queries that are over-specified frequently miss interesting solutions.
  In this paper, we study query relaxation techniques that target package
recommendation systems. Our work offers three key insights: First, even when
the original query result is not empty, relaxing constraints can produce
preferable solutions. Second, a solution due to relaxation can only be
preferred if it improves some property specified by the query. Third,
relaxation should not treat all constraints as equals: some constraints are
more important to the users than others. Our contributions are threefold: (a)
we define the problem of deriving package recommendations through query
relaxation, (b) we design and experimentally evaluate heuristics that relax
query constraints to derive interesting packages, and (c) we present a crowd
study that evaluates the sensitivity of real users to different kinds of
constraints and demonstrates that query relaxation is a powerful tool in
diversifying package recommendations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00824</identifier>
 <datestamp>2015-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00824</id><created>2015-07-03</created><authors><author><keyname>Babagholami-Mohamadabadi</keyname><forenames>Behnam</forenames></author><author><keyname>Yoon</keyname><forenames>Sejong</forenames></author><author><keyname>Pavlovic</keyname><forenames>Vladimir</forenames></author></authors><title>D-MFVI: Distributed Mean Field Variational Inference using Bregman ADMM</title><categories>cs.LG stat.ML</categories><comments>19 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian models provide a framework for probabilistic modelling of complex
datasets. However, many of such models are computationally demanding especially
in the presence of large datasets. On the other hand, in sensor network
applications, statistical (Bayesian) parameter estimation usually needs
distributed algorithms, in which both data and computation are distributed
across the nodes of the network. In this paper we propose a general framework
for distributed Bayesian learning using Bregman Alternating Direction Method of
Multipliers (B-ADMM). We demonstrate the utility of our framework, with Mean
Field Variational Bayes (MFVB) as the primitive for distributed Matrix
Factorization (MF) and distributed affine structure from motion (SfM).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00825</identifier>
 <datestamp>2015-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00825</id><created>2015-07-03</created><authors><author><keyname>Shigeto</keyname><forenames>Yutaro</forenames></author><author><keyname>Suzuki</keyname><forenames>Ikumi</forenames></author><author><keyname>Hara</keyname><forenames>Kazuo</forenames></author><author><keyname>Shimbo</keyname><forenames>Masashi</forenames></author><author><keyname>Matsumoto</keyname><forenames>Yuji</forenames></author></authors><title>Ridge Regression, Hubness, and Zero-Shot Learning</title><categories>cs.LG stat.ML</categories><comments>To be presented at ECML/PKDD 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses the effect of hubness in zero-shot learning, when ridge
regression is used to find a mapping between the example space to the label
space. Contrary to the existing approach, which attempts to find a mapping from
the example space to the label space, we show that mapping labels into the
example space is desirable to suppress the emergence of hubs in the subsequent
nearest neighbor search step. Assuming a simple data model, we prove that the
proposed approach indeed reduces hubness. This was verified empirically on the
tasks of bilingual lexicon extraction and image labeling: hubness was reduced
with both of these tasks and the accuracy was improved accordingly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00827</identifier>
 <datestamp>2015-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00827</id><created>2015-07-03</created><authors><author><keyname>Le</keyname><forenames>Can M.</forenames></author><author><keyname>Levina</keyname><forenames>Elizaveta</forenames></author></authors><title>Estimating the number of communities in networks by spectral methods</title><categories>stat.ML cs.SI math.ST stat.TH</categories><msc-class>62H30, 62G99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community detection is a fundamental problem in network analysis with many
methods available to estimate communities. Most of these methods assume that
the number of communities is known, which is often not the case in practice. We
propose a simple and very fast method for estimating the number of communities
based on the spectral properties of certain graph operators, such as the
non-backtracking matrix and the Bethe Hessian matrix. We show that the method
performs well under several models and a wide range of parameters, and is
guaranteed to be consistent under several asymptotic regimes. We compare the
new method to several existing methods for estimating the number of communities
and show that it is both more accurate and more computationally efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00829</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00829</id><created>2015-07-03</created><updated>2015-08-07</updated><authors><author><keyname>Meka</keyname><forenames>Raghu</forenames></author><author><keyname>Nguyen</keyname><forenames>Oanh</forenames></author><author><keyname>Vu</keyname><forenames>Van</forenames></author></authors><title>Anti-concentration for polynomials of independent random variables</title><categories>math.PR cs.CC</categories><comments>Theorem 1.7 on p-biased distribution and Theorem 2.5 on lower bound
  for approximating OR function are modified due to some errors in the previous
  version. Theorem 1.8 on general distributions and an application on graph
  theory are added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove anti-concentration results for polynomials of independent random
variables with arbitrary degree. Our results extend the classical
Littlewood-Offord result for linear polynomials, and improve several earlier
estimates.
  We discuss applications in two different areas. In complexity theory, we
prove near optimal lower bounds for computing the Parity, addressing a
challenge in complexity theory posed by Razborov and Viola, and also address a
problem concerning OR functions. In random graph theory, we derive a general
anti-concentration result on the number of copies of a fixed graph in a random
graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00840</identifier>
 <datestamp>2015-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00840</id><created>2015-07-03</created><authors><author><keyname>Sawa</keyname><forenames>Koji</forenames></author></authors><title>Logic as a complex network</title><categories>cs.SI cs.LO physics.soc-ph</categories><comments>10 pages, 4 figures, original research paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When we represent logical, connective implications by directed edges, the
resulting set of directed edges can be regarded as a complex network. In this
article, we compose a network model that represents a deductive-logic-like
structure composed solely of implications. The proposed network model grows
like the BA model reported by Barabasi and Albert [Science 286, 509 (1999)].
Though the BA model references the whole of the existing network when a node is
added, our model references only part of the existing network. In this view,
our model is more realistic than the BA model. However, it also exhibits power
law characteristics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00843</identifier>
 <datestamp>2015-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00843</id><created>2015-07-03</created><authors><author><keyname>Huber</keyname><forenames>Mark</forenames></author></authors><title>Optimal linear Bernoulli factories for small mean problems</title><categories>math.PR cs.CC cs.DS stat.CO</categories><comments>18 pages</comments><msc-class>65C50, 68Q17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose a coin with unknown probability $p$ of heads can be flipped as often
as desired. A Bernoulli factory for a function $f$ is an algorithm that uses
flips of the coin together with auxiliary randomness to flip a single coin with
probability $f(p)$ of heads. Applications include near perfect sampling from
the stationary distribution of regenerative processes. When $f$ is analytic,
the problem can be reduced to a Bernoulli factory of the form $f(p) = Cp$ for
constant $C$. Presented here is a new algorithm where for small values of $Cp$,
requires roughly only $C$ coin flips to generate a $Cp$ coin. From information
theory considerations, this is also conjectured to be (to first order) the
minimum number of flips needed by any such algorithm.
  For $Cp$ large, the new algorithm can also be used to build a new Bernoulli
factory that uses only 80\% of the expected coin flips of the older method, and
applies to the more general problem of a multivariate Bernoulli factory, where
there are $k$ coins, the $k$th coin has unknown probability $p_k$ of heads, and
the goal is to simulate a coin flip with probability $C_1 p_1 + \cdots + C_k
p_k$ of heads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00862</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00862</id><created>2015-07-03</created><authors><author><keyname>Semenov</keyname><forenames>Alexander</forenames></author><author><keyname>Zaikin</keyname><forenames>Oleg</forenames></author></authors><title>Using Monte Carlo method for searching partitionings of hard variants of
  Boolean satisfiability problem</title><categories>cs.AI</categories><comments>The reduced version of this paper was accepted for publication in
  proceedings of the PaCT 2015 conference (LNCS Vol. 9251). arXiv admin note:
  substantial text overlap with arXiv:1411.5433</comments><journal-ref>LNCS 9251 (2015) 222-230</journal-ref><doi>10.1007/978-3-319-21909-7_21</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose the approach for constructing partitionings of hard
variants of the Boolean satisfiability problem (SAT). Such partitionings can be
used for solving corresponding SAT instances in parallel. For the same SAT
instance one can construct different partitionings, each of them is a set of
simplified versions of the original SAT instance. The effectiveness of an
arbitrary partitioning is determined by the total time of solving of all SAT
instances from it. We suggest the approach, based on the Monte Carlo method,
for estimating time of processing of an arbitrary partitioning. With each
partitioning we associate a point in the special finite search space. The
estimation of effectiveness of the particular partitioning is the value of
predictive function in the corresponding point of this space. The problem of
search for an effective partitioning can be formulated as a problem of
optimization of the predictive function. We use metaheuristic algorithms
(simulated annealing and tabu search) to move from point to point in the search
space. In our computational experiments we found partitionings for SAT
instances encoding problems of inversion of some cryptographic functions.
Several of these SAT instances with realistic predicted solving time were
successfully solved on a computing cluster and in the volunteer computing
project SAT@home. The solving time agrees well with estimations obtained by the
proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00868</identifier>
 <datestamp>2015-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00868</id><created>2015-07-03</created><authors><author><keyname>Bern&#xe1;th</keyname><forenames>Attila</forenames></author><author><keyname>Pap</keyname><forenames>Gyula</forenames></author></authors><title>Blocking unions of arborescences</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a digraph $D=(V,A)$ and a positive integer $k$, a subset $B\subseteq A$
is called a \textbf{$k$-union-arborescence}, if it is the disjoint union of $k$
spanning arborescences. When also arc-costs $c:A\to \mathbb{R}$ are given,
minimizing the cost of a $k$-union-arborescence is well-known to be tractable.
In this paper we take on the following problem: what is the minimum cardinality
of a set of arcs the removal of which destroys every minimum $c$-cost
$k$-union-arborescence. Actually, the more general weighted problem is also
considered, that is, arc weights $w:A\to \mathbb{R}_+$ (unrelated to $c$) are
also given, and the goal is to find a minimum weight set of arcs the removal of
which destroys every minimum $c$-cost $k$-union-arborescence. An equivalent
version of this problem is where the roots of the arborescences are fixed in
advance. In an earlier paper [A. Bern\'ath and Gy. Pap, \emph{Blocking optimal
arborescences}, Integer Programming and Combinatorial Optimization, Springer,
2013] we solved this problem for $k=1$. This work reports on other partial
results on the problem. We solve the case when both $c$ and $w$ are uniform --
that is, find a minimum size set of arcs that covers all
$k$-union-arbosercences. Our algorithm runs in polynomial time for this
problem. The solution uses a result of [M. B\'ar\'asz, J. Becker, and A. Frank,
\emph{An algorithm for source location in directed graphs}, Oper. Res. Lett.
\textbf{33} (2005)] saying that the family of so-called insolid sets (sets with
the property that every proper subset has a larger in-degree) satisfies the
Helly-property, and thus can be (efficiently) represented as a subtree
hypergraph. We also give an algorithm for the case when only $c$ is uniform but
$w$ is not. This algorithm is only polynomial if $k$ is not part of the input.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00887</identifier>
 <datestamp>2015-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00887</id><created>2015-07-03</created><updated>2015-11-18</updated><authors><author><keyname>Li</keyname><forenames>Guoyin</forenames></author><author><keyname>Pong</keyname><forenames>Ting Kei</forenames></author></authors><title>Peaceman-Rachford splitting for a class of nonconvex optimization
  problems</title><categories>math.OC cs.NA math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the applicability of the Peaceman-Rachford (PR) splitting method for
solving nonconvex optimization problems. When applied to minimizing the sum of
a strongly convex Lipschitz differentiable function and a proper closed
function, we show that if the strongly convex function has a large enough
strong convexity modulus and the step-size parameter is chosen below a
threshold that is computable, then any cluster point of the sequence generated,
if exists, will give a stationary point of the optimization problem. We also
give sufficient conditions guaranteeing boundedness of the sequence generated.
We then discuss one way to split the objective so that the proposed method can
be suitably applied to solving optimization problems whose objective is
coercive, and is the sum of a (not necessarily strongly) convex Lipschitz
differentiable function and a proper closed function; this setting covers a
large class of nonconvex feasibility problems and constrained least-squares
problems. The numerical tests show that our proposed method is faster than the
Douglas-Rachford splitting method when applied to finding a sparse solution of
a linear system, but with a slightly compromised solution quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00898</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00898</id><created>2015-07-03</created><authors><author><keyname>Kutzner</keyname><forenames>Carsten</forenames></author><author><keyname>P&#xe1;ll</keyname><forenames>Szil&#xe1;rd</forenames></author><author><keyname>Fechner</keyname><forenames>Martin</forenames></author><author><keyname>Esztermann</keyname><forenames>Ansgar</forenames></author><author><keyname>de Groot</keyname><forenames>Bert L.</forenames></author><author><keyname>Grubm&#xfc;ller</keyname><forenames>Helmut</forenames></author></authors><title>Best bang for your buck: GPU nodes for GROMACS biomolecular simulations</title><categories>cs.DC cs.PF physics.bio-ph physics.comp-ph q-bio.BM</categories><doi>10.1002/jcc.24030</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The molecular dynamics simulation package GROMACS runs efficiently on a wide
variety of hardware from commodity workstations to high performance computing
clusters. Hardware features are well exploited with a combination of SIMD,
multi-threading, and MPI-based SPMD/MPMD parallelism, while GPUs can be used as
accelerators to compute interactions offloaded from the CPU. Here we evaluate
which hardware produces trajectories with GROMACS 4.6 or 5.0 in the most
economical way. We have assembled and benchmarked compute nodes with various
CPU/GPU combinations to identify optimal compositions in terms of raw
trajectory production rate, performance-to-price ratio, energy efficiency, and
several other criteria. Though hardware prices are naturally subject to trends
and fluctuations, general tendencies are clearly visible. Adding any type of
GPU significantly boosts a node's simulation performance. For inexpensive
consumer-class GPUs this improvement equally reflects in the
performance-to-price ratio. Although memory issues in consumer-class GPUs could
pass unnoticed since these cards do not support ECC memory, unreliable GPUs can
be sorted out with memory checking tools. Apart from the obvious determinants
for cost-efficiency like hardware expenses and raw performance, the energy
consumption of a node is a major cost factor. Over the typical hardware
lifetime until replacement of a few years, the costs for electrical power and
cooling can become larger than the costs of the hardware itself. Taking that
into account, nodes with a well-balanced ratio of CPU and consumer-class GPU
resources produce the maximum amount of GROMACS trajectory over their lifetime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00903</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00903</id><created>2015-07-03</created><updated>2015-07-22</updated><authors><author><keyname>Kech</keyname><forenames>Michael</forenames></author><author><keyname>Wolf</keyname><forenames>Michael M.</forenames></author></authors><title>Quantum Tomography of Semi-Algebraic Sets with Constrained Measurements</title><categories>quant-ph cs.IT math-ph math.IT math.MP</categories><comments>Minor changes in introduction</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyse quantum state tomography in scenarios where measurements and
states are both constrained. States are assumed to live in a semi-algebraic
subset of state space and measurements are supposed to be rank-one POVMs,
possibly with additional constraints. Specifically, we consider sets of von
Neumann measurements and sets of local observables. We provide upper bounds on
the minimal number of measurement settings or outcomes that are required for
discriminating all states within the given set. The bounds exploit tools from
real algebraic geometry and lead to generic results that do not only show the
existence of good measurements but guarantee that almost all measurements with
the same dimension characteristic perform equally well.
  In particular, we show that on an $n$-dimensional Hilbert space any two
states of a semi-algebraic subset can be discriminated by $k$ generic von
Neumann measurements if $k(n-1)$ is larger than twice the dimension of the
subset. In case the subset is given by states of rank at most $r$, we show that
$k$ generic von Neumann measurements suffice to discriminate any two states
provided that $k(n-1)&gt;4r(n-r)-2$. Furthermore, we obtain corresponding results
for low-rank matrix recovery of hermitian matrices and the phase retrieval
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00908</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00908</id><created>2015-07-03</created><authors><author><keyname>Kang</keyname><forenames>Zhao</forenames></author><author><keyname>Peng</keyname><forenames>Chong</forenames></author><author><keyname>Cheng</keyname><forenames>Jie</forenames></author><author><keyname>Chen</keyname><forenames>Qiang</forenames></author></authors><title>LogDet Rank Minimization with Application to Subspace Clustering</title><categories>cs.CV cs.LG stat.ML</categories><comments>10 pages, 4 figures</comments><journal-ref>Computational Intelligence and Neuroscience, Volume 2015, Article
  ID 824289</journal-ref><doi>10.1155/2015/824289</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low-rank matrix is desired in many machine learning and computer vision
problems. Most of the recent studies use the nuclear norm as a convex surrogate
of the rank operator. However, all singular values are simply added together by
the nuclear norm, and thus the rank may not be well approximated in practical
problems. In this paper, we propose to use a log-determinant (LogDet) function
as a smooth and closer, though non-convex, approximation to rank for obtaining
a low-rank representation in subspace clustering. Augmented Lagrange
multipliers strategy is applied to iteratively optimize the LogDet-based
non-convex objective function on potentially large-scale data. By making use of
the angular information of principal directions of the resultant low-rank
representation, an affinity graph matrix is constructed for spectral
clustering. Experimental results on motion segmentation and face clustering
data demonstrate that the proposed method often outperforms state-of-the-art
subspace clustering algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00909</identifier>
 <datestamp>2015-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00909</id><created>2015-07-03</created><authors><author><keyname>Fraigniaud</keyname><forenames>Pierre</forenames></author><author><keyname>Hirvonen</keyname><forenames>Juho</forenames></author><author><keyname>Suomela</keyname><forenames>Jukka</forenames></author></authors><title>Node Labels in Local Decision</title><categories>cs.DC</categories><comments>Conference version to appear in the proceedings of SIROCCO 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The role of unique node identifiers in network computing is well understood
as far as symmetry breaking is concerned. However, the unique identifiers also
leak information about the computing environment - in particular, they provide
some nodes with information related to the size of the network. It was recently
proved that in the context of local decision, there are some decision problems
such that (1) they cannot be solved without unique identifiers, and (2) unique
node identifiers leak a sufficient amount of information such that the problem
becomes solvable (PODC 2013).
  In this work we give study what is the minimal amount of information that we
need to leak from the environment to the nodes in order to solve local decision
problems. Our key results are related to scalar oracles $f$ that, for any given
$n$, provide a multiset $f(n)$ of $n$ labels; then the adversary assigns the
labels to the $n$ nodes in the network. This is a direct generalisation of the
usual assumption of unique node identifiers. We give a complete
characterisation of the weakest oracle that leaks at least as much information
as the unique identifiers.
  Our main result is the following dichotomy: we classify scalar oracles as
large and small, depending on their asymptotic behaviour, and show that (1) any
large oracle is at least as powerful as the unique identifiers in the context
of local decision problems, while (2) for any small oracle there are local
decision problems that still benefit from unique identifiers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00913</identifier>
 <datestamp>2015-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00913</id><created>2015-07-03</created><authors><author><keyname>Rodner</keyname><forenames>Erik</forenames></author><author><keyname>Simon</keyname><forenames>Marcel</forenames></author><author><keyname>Brehm</keyname><forenames>Gunnar</forenames></author><author><keyname>Pietsch</keyname><forenames>Stephanie</forenames></author><author><keyname>W&#xe4;gele</keyname><forenames>J. Wolfgang</forenames></author><author><keyname>Denzler</keyname><forenames>Joachim</forenames></author></authors><title>Fine-grained Recognition Datasets for Biodiversity Analysis</title><categories>cs.CV</categories><comments>CVPR FGVC Workshop 2015; dataset available</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the following paper, we present and discuss challenging applications for
fine-grained visual classification (FGVC): biodiversity and species analysis.
We not only give details about two challenging new datasets suitable for
computer vision research with up to 675 highly similar classes, but also
present first results with localized features using convolutional neural
networks (CNN). We conclude with a list of challenging new research directions
in the area of visual classification for biodiversity research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00921</identifier>
 <datestamp>2015-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00921</id><created>2015-06-09</created><authors><author><keyname>Secondini</keyname><forenames>Marco</forenames></author><author><keyname>Rommel</keyname><forenames>Simon</forenames></author><author><keyname>Fresi</keyname><forenames>Francesco</forenames></author><author><keyname>Forestieri</keyname><forenames>Enrico</forenames></author><author><keyname>Meloni</keyname><forenames>Gianluca</forenames></author><author><keyname>Pot&#xec;</keyname><forenames>Luca</forenames></author></authors><title>Coherent 100G Nonlinear Compensation with Single-Step Digital
  Backpropagation</title><categories>cs.OH cs.IT math.IT</categories><comments>This work has been presented at Optical Networks Design &amp; Modeling
  (ONDM) 2015, Pisa, Italy, May 11-14, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Enhanced-SSFM digital backpropagation (DBP) is experimentally demonstrated
and compared to conventional DBP. A 112 Gb/s PM-QPSK signal is transmitted over
a 3200 km dispersion-unmanaged link. The intradyne coherent receiver includes
single-step digital backpropagation based on the enhanced-SSFM algorithm. In
comparison, conventional DBP requires twenty steps to achieve the same
performance. An analysis of the computational complexity and structure of the
two algorithms reveals that the overall complexity and power consumption of DBP
are reduced by a factor of 16 with respect to a conventional implementation,
while the computation time is reduced by a factor of 20. As a result, the
proposed algorithm enables a practical and effective implementation of DBP in
real-time optical receivers, with only a moderate increase of the computational
complexity, power consumption, and latency with respect to a simple
feed-forward equalizer for dispersion compensation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00925</identifier>
 <datestamp>2015-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00925</id><created>2015-06-17</created><authors><author><keyname>Prokhorenkova</keyname><forenames>Liudmila Ostroumova</forenames></author></authors><title>Global clustering coefficient in scale-free weighted and unweighted
  networks</title><categories>cs.SI math.CO math.PR</categories><comments>arXiv admin note: text overlap with arXiv:1410.1997</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a detailed analysis of the global clustering
coefficient in scale-free graphs. Many observed real-world networks of diverse
nature have a power-law degree distribution. Moreover, the observed degree
distribution usually has an infinite variance. Therefore, we are especially
interested in such degree distributions. In addition, we analyze the clustering
coefficient for both weighted and unweighted graphs.
  There are two well-known definitions of the clustering coefficient of a
graph: the global and the average local clustering coefficients. There are
several models proposed in the literature for which the average local
clustering coefficient tends to a positive constant as a graph grows. On the
other hand, there are no models of scale-free networks with an infinite
variance of the degree distribution and with an asymptotically constant global
clustering coefficient. Models with constant global clustering and finite
variance were also proposed. Therefore, in this paper we focus only on the most
interesting case: we analyze the global clustering coefficient for graphs with
an infinite variance of the degree distribution.
  For unweighted graphs, we prove that the global clustering coefficient tends
to zero with high probability and we also estimate the largest possible
clustering coefficient for such graphs. On the contrary, for weighted graphs,
the constant global clustering coefficient can be obtained even for the case of
an infinite variance of the degree distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00931</identifier>
 <datestamp>2015-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00931</id><created>2015-07-03</created><authors><author><keyname>Pinsker</keyname><forenames>Michael</forenames></author></authors><title>Algebraic and model theoretic methods in constraint satisfaction</title><categories>math.LO cs.CC math.RA</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This text is related to the tutorials I gave at the Banff International
Research Station and within a &quot;Doc-course&quot; at Charles University Prague in the
fall of 2014. It describes my current research and some of the most important
open questions related to it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00937</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00937</id><created>2015-06-16</created><updated>2015-12-23</updated><authors><author><keyname>Lou</keyname><forenames>Taishan</forenames></author><author><keyname>Zhao</keyname><forenames>Liangyu</forenames></author></authors><title>Robust Mars Atmospheric Entry Integrated Navigation based on Parameter
  Sensitivity</title><categories>cs.SY cs.CE</categories><comments>11 pages, 7 figures</comments><journal-ref>Acta Astronautica (2016), pp. 60-70</journal-ref><doi>10.1016/j.actaastro.2015.11.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presented a robust integrated navigation algorithm based on a
special robust desensitized extended Kalman filtering with analytical gain
(ADEKF) during the Mars atmospheric entry. The robust ADEKF is designed by
minimizing a new function penalized by a trace weighted norm of the state error
sensitivities and giving a closed-form gain matrix. The uncertainties of the
Mars atmospheric density and the lift-to-drag ratio (LDR) percentage are
modeled. Sensitivity matrices are defined to character the parameter
uncertainties, and corresponding perturbation matrices are proposed to describe
the navigation errors respected to the parameter uncertainties. The numerical
simulation results show that the robust integrated navigation algorithm based
on the robust ADEKF effectively reduces the negative effects of the two
parameter uncertainties and has good consistency during the Mars entry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00939</identifier>
 <datestamp>2015-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00939</id><created>2015-06-29</created><authors><author><keyname>Sharma</keyname><forenames>Sugam</forenames></author></authors><title>Evolution of as-a-Service Era in Cloud</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today, a paradigm shift is being observed in science, where the focus is
gradually shifting toward the cloud environments to obtain appropriate, robust
and affordable services to deal with Big Data challenges (Sharma et al. 2014,
2015a, 2015b). Cloud computing avoids any need to locally maintain the overly
scaled computing infrastructure that include not only dedicated space, but the
expensive hardware and software also. In this paper, we study the evolution of
as-a-Service modalities, stimulated by cloud computing, and explore the most
complete inventory of new members beyond traditional cloud computing stack.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00942</identifier>
 <datestamp>2015-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00942</id><created>2015-07-03</created><authors><author><keyname>Brucato</keyname><forenames>Matteo</forenames></author><author><keyname>Ramakrishna</keyname><forenames>Rahul</forenames></author><author><keyname>Abouzied</keyname><forenames>Azza</forenames></author><author><keyname>Meliou</keyname><forenames>Alexandra</forenames></author></authors><title>PackageBuilder: From Tuples to Packages</title><categories>cs.DB</categories><journal-ref>PVLDB, vol. 7, no. 13, 2014, pp. 1593-1596</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this demo, we present PackageBuilder, a system that extends database
systems to support package queries. A package is a collection of tuples that
individually satisfy base constraints and collectively satisfy global
constraints. The need for package support arises in a variety of scenarios: For
example, in the creation of meal plans, users are not only interested in the
nutritional content of individual meals (base constraints), but also care to
specify daily consumption limits and control the balance of the entire plan
(global constraints). We introduce PaQL, a declarative SQL-based package query
language, and the interface abstractions which allow users to interactively
specify package queries and easily navigate through their results. To
efficiently evaluate queries, the system employs pruning and heuristics, as
well as state-of-the-art constraint optimization solvers. We demonstrate
PackageBuilder by allowing attendees to interact with the system's interface,
to define PaQL queries and to observe how query evaluation is performed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00943</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00943</id><created>2015-07-03</created><updated>2016-01-22</updated><authors><author><keyname>Baniamerian</keyname><forenames>Amir</forenames></author><author><keyname>Meskin</keyname><forenames>Nader</forenames></author><author><keyname>Khorasani</keyname><forenames>Khashayar</forenames></author></authors><title>A Geometric Approach to Fault Detection and Isolation of Two-Dimensional
  (2D) Systems</title><categories>cs.SY</categories><comments>44 pages, 4 figures</comments><report-no>2D_V02</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we develop a novel fault detection and isolation (FDI) scheme
for discrete-time two-dimensional (2D) systems that are represented by the
Fornasini-Marchesini model II (FMII). This is accomplished by generalizing the
geometric approach of one-dimensional (1D) systems to 2D systems. The basic
invariant subspaces including unobservable, conditioned invariant and
unobservability subspaces of 1D systems are extended and generalized to 2D
models. These extensions have been achieved and facilitated by representing a
2D model as an infinite dimensional (Inf-D) system, and by particularly
constructing algorithms that compute these subspaces in a \emph{finite and
known} number of steps. By utilizing the introduced subspaces the FDI problem
is formulated and necessary and sufficient conditions for its solvability are
provided. Sufficient conditions for solvability of the FDI problem for 2D
systems using both deadbeat and LMI filters are also developed. Moreover, the
capabilities and advantages of our proposed approach are demonstrated by
performing an analytical comparison with the currently available 2D geometric
methods in the literature. Finally, numerical simulations corresponding to an
approximation of a hyperbolic partial differential equation (PDE) system of a
heat exchanger, that is mathematically represented as a 2D model, have also
been provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00953</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00953</id><created>2015-07-03</created><updated>2015-11-04</updated><authors><author><keyname>Czegledi</keyname><forenames>Cristian B.</forenames></author><author><keyname>Karlsson</keyname><forenames>Magnus</forenames></author><author><keyname>Agrell</keyname><forenames>Erik</forenames></author><author><keyname>Johannisson</keyname><forenames>Pontus</forenames></author></authors><title>Polarization Drift Channel Model for Coherent Fibre-Optic Systems</title><categories>physics.optics cs.IT math.IT</categories><comments>15 pages, 4 figures</comments><doi>10.1038/srep21217</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A theoretical framework is introduced to model the dynamical changes of the
state of polarization during transmission in coherent fibre-optic systems. The
model generalizes the one-dimensional phase noise random walk to higher
dimensions, accounting for random polarization drifts, emulating a random walk
on the Poincar\'e sphere, which has been successfully verified using
experimental data. The model is described in the Jones, Stokes and real
four-dimensional formalisms, and the mapping between them is derived. Such a
model will be increasingly important in simulating and optimizing future
systems, where polarization-multiplexed transmission and sophisticated digital
signal processing will be natural parts. The proposed polarization drift model
is the first of its kind as prior work either models polarization drift as a
deterministic process or focuses on polarization-mode dispersion in systems
where the state of polarization does not affect the receiver performance. We
expect the model to be useful in a wide-range of photonics applications where
stochastic polarization fluctuation is an issue.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00954</identifier>
 <datestamp>2015-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00954</id><created>2015-07-03</created><authors><author><keyname>Cheng</keyname><forenames>Minquan</forenames></author><author><keyname>Jiang</keyname><forenames>Jing</forenames></author><author><keyname>Li</keyname><forenames>Haiyan</forenames></author><author><keyname>Miao</keyname><forenames>Ying</forenames></author><author><keyname>Tang</keyname><forenames>Xiaohu</forenames></author></authors><title>Bounds and Constructions for $\overline{3}$-Separable Codes with Length
  $3$</title><categories>cs.IT math.IT</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Separable codes were introduced to provide protection against illegal
redistribution of copyrighted multimedia material. Let $\mathcal{C}$ be a code
of length $n$ over an alphabet of $q$ letters. The descendant code ${\sf
desc}(\mathcal{C}_0)$ of $\mathcal{C}_0 = \{{\bf c}_1, {\bf c}_2, \ldots, {\bf
c}_t\} \subseteq {\mathcal{C}}$ is defined to be the set of words ${\bf x} =
(x_1, x_2, \ldots,x_n)^T$ such that $x_i \in \{c_{1,i}, c_{2,i}, \ldots,
c_{t,i}\}$ for all $i=1, \ldots, n$, where ${\bf
c}_j=(c_{j,1},c_{j,2},\ldots,c_{j,n})^T$. $\mathcal{C}$ is a
$\overline{t}$-separable code if for any two distinct $\mathcal{C}_1,
\mathcal{C}_2 \subseteq \mathcal{C}$ with $|\mathcal{C}_1| \le t$,
$|\mathcal{C}_2| \le t$, we always have ${\sf desc}(\mathcal{C}_1) \neq {\sf
desc}(\mathcal{C}_2)$. Let $M(\overline{t},n,q)$ denote the maximal possible
size of such a separable code. In this paper, an upper bound on
$M(\overline{3},3,q)$ is derived by considering an optimization problem related
to a partial Latin square, and then two constructions for
$\overline{3}$-SC$(3,M,q)$s are provided by means of perfect hash families and
Steiner triple systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00955</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00955</id><created>2015-07-03</created><updated>2015-09-18</updated><authors><author><keyname>Kolchyna</keyname><forenames>Olga</forenames></author><author><keyname>Souza</keyname><forenames>Tharsis T. P.</forenames></author><author><keyname>Treleaven</keyname><forenames>Philip</forenames></author><author><keyname>Aste</keyname><forenames>Tomaso</forenames></author></authors><title>Twitter Sentiment Analysis: Lexicon Method, Machine Learning Method and
  Their Combination</title><categories>cs.CL cs.IR cs.LG stat.ME stat.ML</categories><comments>32 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper covers the two approaches for sentiment analysis: i) lexicon based
method; ii) machine learning method. We describe several techniques to
implement these approaches and discuss how they can be adopted for sentiment
classification of Twitter messages. We present a comparative study of different
lexicon combinations and show that enhancing sentiment lexicons with emoticons,
abbreviations and social-media slang expressions increases the accuracy of
lexicon-based classification for Twitter. We discuss the importance of feature
generation and feature selection processes for machine learning sentiment
classification. To quantify the performance of the main sentiment analysis
methods over Twitter we run these algorithms on a benchmark Twitter dataset
from the SemEval-2013 competition, task 2-B. The results show that machine
learning method based on SVM and Naive Bayes classifiers outperforms the
lexicon method. We present a new ensemble method that uses a lexicon based
sentiment score as input feature for the machine learning approach. The
combined method proved to produce more precise classifications. We also show
that employing a cost-sensitive classifier for highly unbalanced datasets
yields an improvement of sentiment classification performance up to 7%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00956</identifier>
 <datestamp>2015-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00956</id><created>2015-07-03</created><authors><author><keyname>Bulitko</keyname><forenames>Vadim</forenames></author><author><keyname>Hong</keyname><forenames>Jessica</forenames></author><author><keyname>Kumaran</keyname><forenames>Kumar</forenames></author><author><keyname>Swedberg</keyname><forenames>Ivan</forenames></author><author><keyname>Thoang</keyname><forenames>William</forenames></author><author><keyname>von Hauff</keyname><forenames>Patrick</forenames></author><author><keyname>Schmolzer</keyname><forenames>Georg</forenames></author></authors><title>RETAIN: a Neonatal Resuscitation Trainer Built in an Undergraduate
  Video-Game Class</title><categories>cs.CY</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Approximately ten percent of newborns require some help with their breathing
at birth. About one percent require extensive assistance at birth which needs
to be administered by trained personnel. Neonatal resuscitation is taught
through a simulation based training program in North America. Such a training
methodology is cost and resource intensive which reduces its availability
thereby adversely impacting skill acquisition and retention. We implement and
present RETAIN (REsuscitation TrAIning for Neonatal residents) -- a video game
to complement the existing neonatal training. Being a video game, RETAIN runs
on ubiquitous off-the-shelf hardware and can be easily accessed by trainees
almost anywhere at their convenience. Thus we expect RETAIN to help trainees
retain and retrain their resuscitation skills. We also report on how RETAIN was
developed by an interdisciplinary team of six undergraduate students as a
three-month term project for a second year university course.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00957</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00957</id><created>2015-07-03</created><updated>2015-07-06</updated><authors><author><keyname>Grazioso</keyname><forenames>Fabio</forenames></author></authors><title>Introduction to quantum information theory and outline of two
  applications to physics: the black hole information paradox and the
  renormalization group information flow</title><categories>quant-ph cs.IT math.IT</categories><comments>16 pages, 9 figures, review article. Some typos fixed, missing part
  of the abstract added</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This review paper is intended for scholars with different backgrounds,
possibly in only one of the subjects covered, and therefore little background
knowledge is assumed. The first part is an introduction to classical and
quantum information theory (CIT, QIT): basic definitions and tools of CIT are
introduced, such as the information content of a random variable, the typical
set, and some principles of data compression. Some concepts and results of QIT
are then introduced, such as the qubit, the pure and mixed states, the Holevo
theorem, the no-cloning theorem, and the quantum complementarity. In the second
part, two applications of QIT to open problems in theoretical physics are
discussed. The black hole (BH) information paradox is related to the phenomenon
of the Hawking radiation (HR). Consid- ering a BH starting in a pure state,
after its complete evaporation only the Hawking radiation will remain, which is
shown to be in a mixed state. This either describes a non-unitary evolution of
an isolated system, contradicting the evolution postulate of quantum mechanics
and violating the no-cloning theorem, or it implies that the initial
information content can escape the BH, therefore contradicting general
relativity. The progress toward the solution of the paradox is discussed. The
renormalization group (RG) aims at the extraction of the macroscopic
description of a physical system from its microscopic description. This passage
from microscopic to macroscopic can be described in terms of several steps from
one scale to another, and is therefore formalized as the action of a group. The
c-theorem proves the existence, under certain conditions, of a function which
is monotonically decreasing along the group transformations. This result
suggests an interpretation of this function as entropy, and its use to study
the information flow along the RG transformations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00980</identifier>
 <datestamp>2015-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00980</id><created>2015-07-03</created><authors><author><keyname>Rodr&#xed;guez</keyname><forenames>C&#xe9;sar</forenames></author><author><keyname>Sousa</keyname><forenames>Marcelo</forenames></author><author><keyname>Sharma</keyname><forenames>Subodh</forenames></author><author><keyname>Kroening</keyname><forenames>Daniel</forenames></author></authors><title>Unfolding-based Partial Order Reduction</title><categories>cs.LO cs.PL</categories><comments>Long version of a paper with the same title appeared on the
  proceedings of CONCUR 2015</comments><acm-class>D.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Partial order reduction (POR) and net unfoldings are two alternative methods
to tackle state-space explosion caused by concurrency. In this paper, we
propose the combination of both approaches in an effort to combine their
strengths. We first define, for an abstract execution model, unfolding
semantics parameterized over an arbitrary independence relation. Based on it,
our main contribution is a novel stateless POR algorithm that explores at most
one execution per Mazurkiewicz trace, and in general, can explore exponentially
fewer, thus achieving a form of super-optimality. Furthermore, our
unfolding-based POR copes with non-terminating executions and incorporates
state-caching. Over benchmarks with busy-waits, among others, our experiments
show a dramatic reduction in the number of executions when compared to a
state-of-the-art DPOR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00988</identifier>
 <datestamp>2015-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00988</id><created>2015-07-03</created><authors><author><keyname>Wan</keyname><forenames>Daqing</forenames></author><author><keyname>Wang</keyname><forenames>Qiang</forenames></author></authors><title>Index bounds for character sums with polynomials over finite fields</title><categories>math.NT cs.IT math.IT</categories><msc-class>11T24</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide an index bound for character sums of polynomials over finite
fields. This improves the Weil bound for high degree polynomials with small
indices, as well as polynomials with large indices that are generated by
cyclotomic mappings of small indices. As an application, we also give some
general bounds for numbers of solutions of some Artin-Schreier equations and
mininum weights of some cyclic codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00990</identifier>
 <datestamp>2015-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00990</id><created>2015-07-03</created><authors><author><keyname>Vu</keyname><forenames>Ky</forenames></author><author><keyname>Poirion</keyname><forenames>Pierre-Louis</forenames></author><author><keyname>Liberti</keyname><forenames>Leo</forenames></author></authors><title>Using the Johnson-Lindenstrauss lemma in linear and integer programming</title><categories>math.OC cs.DS</categories><msc-class>90C05, 90C10, 68W20</msc-class><acm-class>G.1.6; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Johnson-Lindenstrauss lemma allows dimension reduction on real vectors
with low distortion on their pairwise Euclidean distances. This result is often
used in algorithms such as $k$-means or $k$ nearest neighbours since they only
use Euclidean distances, and has sometimes been used in optimization algorithms
involving the minimization of Euclidean distances. In this paper we introduce a
first attempt at using this lemma in the context of feasibility problems in
linear and integer programming, which cannot be expressed only in function of
Euclidean distances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00993</identifier>
 <datestamp>2015-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00993</id><created>2015-07-03</created><authors><author><keyname>Zeinalkhani</keyname><forenames>Zeinab</forenames></author><author><keyname>Banihashemi</keyname><forenames>Amir H.</forenames></author></authors><title>Ultra Low-Complexity Detection of Spectrum Holes in Compressed Wideband
  Spectrum Sensing</title><categories>cs.IT math.IT</categories><comments>7 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wideband spectrum sensing is a significant challenge in cognitive radios
(CRs) due to requiring very high-speed analog- to-digital converters (ADCs),
operating at or above the Nyquist rate. Here, we propose a very low-complexity
zero-block detection scheme that can detect a large fraction of spectrum holes
from the sub-Nyquist samples, even when the undersampling ratio is very small.
The scheme is based on a block sparse sensing matrix, which is implemented
through the design of a novel analog-to- information converter (AIC). The
proposed scheme identifies some measurements as being zero and then verifies
the sub-channels associated with them as being vacant. Analytical and
simulation results are presented that demonstrate the effectiveness of the
proposed method in reliable detection of spectrum holes with complexity much
lower than existing schemes. This work also introduces a new paradigm in
compressed sensing where one is interested in reliable detection of (some of
the) zero blocks rather than the recovery of the whole block sparse signal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.00996</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.00996</id><created>2015-07-03</created><updated>2015-07-09</updated><authors><author><keyname>Wood</keyname><forenames>Frank</forenames></author><author><keyname>van de Meent</keyname><forenames>Jan Willem</forenames></author><author><keyname>Mansinghka</keyname><forenames>Vikash</forenames></author></authors><title>A New Approach to Probabilistic Programming Inference</title><categories>stat.ML cs.AI cs.PL</categories><comments>Updated version of the 2014 AISTATS paper (to reflect changes in new
  language syntax). 10 pages, 3 figures. Proceedings of the Seventeenth
  International Conference on Artificial Intelligence and Statistics, JMLR
  Workshop and Conference Proceedings, Vol 33, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce and demonstrate a new approach to inference in expressive
probabilistic programming languages based on particle Markov chain Monte Carlo.
Our approach is simple to implement and easy to parallelize. It applies to
Turing-complete probabilistic programming languages and supports accurate
inference in models that make use of complex control flow, including stochastic
recursion. It also includes primitives from Bayesian nonparametric statistics.
Our experiments show that this approach can be more efficient than previously
introduced single-site Metropolis-Hastings methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01020</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01020</id><created>2015-07-03</created><authors><author><keyname>Diekert</keyname><forenames>Volker</forenames></author><author><keyname>Muscholl</keyname><forenames>Anca</forenames></author><author><keyname>Walukiewicz</keyname><forenames>Igor</forenames></author></authors><title>A Note on Monitors and B\&quot;uchi automata</title><categories>cs.FL</categories><msc-class>68Q45</msc-class><acm-class>F.4.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When a property needs to be checked against an unknown or very complex
system, classical exploration techniques like model-checking are not applicable
anymore. Sometimes a~monitor can be used, that checks a given property on the
underlying system at runtime. A monitor for a property $L$ is a deterministic
finite automaton $M_L$ that after each finite execution tells whether (1) every
possible extension of the execution is in $L$, or (2) every possible extension
is in the complement of $L$, or neither (1) nor (2) holds. Moreover, $L$ being
monitorable means that it is always possible that in some future the monitor
reaches (1) or (2). Classical examples for monitorable properties are safety
and cosafety properties. On the other hand, deterministic liveness properties
like &quot;infinitely many $a$'s&quot; are not monitorable. We discuss various monitor
constructions with a focus on deterministic omega-regular languages. We locate
a proper subclass of of deterministic omega-regular languages but also strictly
large than the subclass of languages which are deterministic and
codeterministic, and for this subclass there exists a canonical monitor which
also accepts the language itself.
  We also address the problem to decide monitorability in comparison with
deciding liveness. The state of the art is as follows. Given a B\&quot;uchi
automaton, it is PSPACE-complete to decide liveness or monitorability. Given an
LTL formula, deciding liveness becomes EXPSPACE-complete, but the complexity to
decide monitorability remains open.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01026</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01026</id><created>2015-07-03</created><updated>2015-10-01</updated><authors><author><keyname>Bertsekas</keyname><forenames>Dimitri P.</forenames></author></authors><title>Value and Policy Iteration in Optimal Control and Adaptive Dynamic
  Programming</title><categories>cs.SY cs.DS math.NA math.OC</categories><report-no>Report LIDS-P-3174, Laboratory for Information and Decision Systems,
  M.I.T., Cambridge, Mass</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider discrete-time infinite horizon problems of optimal
control to a terminal set of states. These are the problems that are often
taken as the starting point for adaptive dynamic programming. Under very
general assumptions, we establish the uniqueness of solution of Bellman's
equation, and we provide convergence results for value and policy iteration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01029</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01029</id><created>2015-07-03</created><authors><author><keyname>Bertsekas</keyname><forenames>Dimitri P.</forenames></author></authors><title>Lambda-Policy Iteration: A Review and a New Implementation</title><categories>cs.SY cs.DS math.NA math.OC</categories><report-no>Report LIDS - 2874, Laboratory for Information and Decision Systems,
  MIT, Cambridge, Mass., Feb. 2012</report-no><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  In this paper we discuss $\l$-policy iteration, a method for exact and
approximate dynamic programming. It is intermediate between the classical value
iteration (VI) and policy iteration (PI) methods, and it is closely related to
optimistic (also known as modified) PI, whereby each policy evaluation is done
approximately, using a finite number of VI. We review the theory of the method
and associated questions of bias and exploration arising in simulation-based
cost function approximation. We then discuss various implementations, which
offer advantages over well-established PI methods that use LSPE($\l$),
LSTD($\l$), or TD($\l$) for policy evaluation with cost function approximation.
One of these implementations is based on a new simulation scheme, called
geometric sampling, which uses multiple short trajectories rather than a single
infinitely long trajectory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01030</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01030</id><created>2015-07-03</created><authors><author><keyname>Bertsekas</keyname><forenames>Dimitri P.</forenames></author></authors><title>Incremental Gradient, Subgradient, and Proximal Methods for Convex
  Optimization: A Survey</title><categories>cs.SY cs.DS math.NA math.OC</categories><report-no>Report LIDS - 2848, Laboratory for Information and Decision Systems,
  MIT, Cambridge, Mass., Dec., 2010</report-no><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  We survey incremental methods for minimizing a sum $\sum_{i=1}^mf_i(x)$
consisting of a large number of convex component functions $f_i$. Our methods
consist of iterations applied to single components, and have proved very
effective in practice. We introduce a unified algorithmic framework for a
variety of such methods, some involving gradient and subgradient iterations,
which are known, and some involving combinations of subgradient and proximal
methods, which are new and offer greater flexibility in exploiting the special
structure of $f_i$. We provide an analysis of the convergence and rate of
convergence properties of these methods, including the advantages offered by
randomization in the selection of components. We also survey applications in
inference/machine learning, signal processing, and large-scale and distributed
optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01046</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01046</id><created>2015-07-03</created><authors><author><keyname>Fowler</keyname><forenames>Allan</forenames></author></authors><title>Understanding learning within a commercial video game: A case study</title><categories>cs.CY cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been an increasing interest in the debate on the value and
relevance using video games for learning. Some of the interest stems from
frustration with current educational methods. However, some of this interest
also stems from the observations of large numbers of children that play video
games. This paper finds that children can learn basic construction skills from
playing a video game called World of Goo. The study also employed novel
eye-tracking technology to measure endogenous eye blinks and eye gaze
fixations. Measures of both these indicators of cognitive processing further
suggested that children in the study learned to play the two video games, World
of Goo and Bad Piggies. Overall, the results of the study provide further
support of the potential for children to learn by playing commercial video
games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01048</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01048</id><created>2015-07-03</created><updated>2015-09-23</updated><authors><author><keyname>Kapelko</keyname><forenames>Rafa&#x142;</forenames></author></authors><title>On the moment event distance of Poisson processes</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the event distance between two i.i.d. Poisson processes with arrival
rate $\lambda$ and respective arrival times $X_1,X_2,\dots$ and $Y_1,Y_2,\dots$
on a line. We give a closed analytical formula for the moment distance
$\E{|X_{k+r}-Y_k|^a}, $ for any integer $k\ge 1, r\ge 0,$ when $a$ is natural
number. The moment distance we represent as the combination of the Pochhammer
polynomials. Especially, for $r=0,$ the following identity is valid: $$
\E{|X_k-Y_k|^a}=\frac{a!}{\lambda^a}\frac{\Gamma\left(\frac{a}{2}+k\right)}{\Gamma(k)\Gamma\left(\frac{a}{2}+1\right)},
$$ where $\Gamma(z)$ is Gamma function. Hence, we generalize the results of [7]
to any power $a,$ when $a$ is natural number. As an application to sensor
networks we derive that the expected cost to the power $b$ of a minimum weight
matching with edges $\{X_k,Y_k\}$ between two i.i.d Poisson processes with
arrival times $X_1,X_2,\dots X_n$ and $Y_1,Y_2,\dots Y_n$ is in
$\Theta\left(n^{1-\frac{b}{2}}\right),$ when $b\ge 1,$ and in
$O\left(n^{1-\frac{b}{2}}\right),$ when $0 &lt;b&lt; 1.$
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01053</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01053</id><created>2015-07-03</created><authors><author><keyname>Cho</keyname><forenames>Kyunghyun</forenames></author><author><keyname>Courville</keyname><forenames>Aaron</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Describing Multimedia Content using Attention-based Encoder--Decoder
  Networks</title><categories>cs.NE cs.CL cs.CV cs.LG</categories><comments>Submitted to IEEE Transactions on Multimedia Special Issue on Deep
  Learning for Multimedia Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Whereas deep neural networks were first mostly used for classification tasks,
they are rapidly expanding in the realm of structured output problems, where
the observed target is composed of multiple random variables that have a rich
joint distribution, given the input. We focus in this paper on the case where
the input also has a rich structure and the input and output structures are
somehow related. We describe systems that learn to attend to different places
in the input, for each element of the output, for a variety of tasks: machine
translation, image caption generation, video clip description and speech
recognition. All these systems are based on a shared set of building blocks:
gated recurrent neural networks and convolutional neural networks, along with
trained attention mechanisms. We report on experimental results with these
systems, showing impressively good performance and the advantage of the
attention mechanism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01057</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01057</id><created>2015-07-03</created><authors><author><keyname>Zhang</keyname><forenames>Daqing</forenames></author><author><keyname>Wang</keyname><forenames>Hao</forenames></author><author><keyname>Wang</keyname><forenames>Yasha</forenames></author><author><keyname>Ma</keyname><forenames>Junyi</forenames></author></authors><title>Anti-Fall: A Non-intrusive and Real-time Fall Detector Leveraging CSI
  from Commodity WiFi Devices</title><categories>cs.HC</categories><comments>13 pages,8 figures,corrected version, ICOST conference</comments><doi>10.1007/978-3-319-19312-0_15</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fall is one of the major health threats and obstacles to independent living
for elders, timely and reliable fall detection is crucial for mitigating the
effects of falls. In this paper, leveraging the fine-grained Channel State
Information (CSI) and multi-antenna setting in commodity WiFi devices, we
design and implement a real-time, non-intrusive, and low-cost indoor fall
detector, called Anti-Fall. For the first time, the CSI phase difference over
two antennas is identified as the salient feature to reliably segment the fall
and fall-like activities, both phase and amplitude information of CSI is then
exploited to accurately separate the fall from other fall-like activities.
Experimental results in two indoor scenarios demonstrate that Anti-Fall
consistently outperforms the state-of-the-art approach WiFall, with 10% higher
detection rate and 10% less false alarm rate on average.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01060</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01060</id><created>2015-07-03</created><authors><author><keyname>He</keyname><forenames>Peng</forenames></author><author><keyname>Mao</keyname><forenames>Yuming</forenames></author><author><keyname>Liu</keyname><forenames>Qiang</forenames></author><author><keyname>Yang</keyname><forenames>Kun</forenames></author></authors><title>A Diffusion-Neuron Hybrid System for Molecular Communication</title><categories>cs.ET cs.IT math.IT</categories><comments>6 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Diffusion-based and neural communication are two interesting domains in
molecular communication. Both of them have distinct advantages and are
exploited separately in many works. However, in some cases, neural and
diffusion-based ways have to work together for a communication. Therefore, in
this paper, we propose a hybrid communication system, in which the
diffusion-based and neural communication channels are contained. Multiple
connection nano-devices (CND) are used to connect the two channels. We define
the practice function of the CNDs and develop the mechanism of exchanging
information from diffusion-based to neural channel, based on the biological
characteristics of the both channels. In addition, we establish a brief
mathematical model to present the complete communication process of the hybrid
system. The information exchange process at the CNDs is shown in the
simulation. The bit error rate (BER) indicator is used to verify the
reliability of communication. The result reveals that based on the biological
channels, optimizing some parameters of nano-devices could improve the
reliability performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01062</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01062</id><created>2015-07-04</created><authors><author><keyname>Sureka</keyname><forenames>Ashish</forenames></author></authors><title>Intention-Oriented Process Model Discovery from Incident Management
  Event Logs</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intention-oriented process mining is based on the belief that the fundamental
nature of processes is mostly intentional (unlike activity-oriented process)
and aims at discovering strategy and intentional process models from event-logs
recorded during the process enactment. In this paper, we present an application
of intention-oriented process mining for the domain of incident management of
an Information Technology Infrastructure Library (ITIL) process. We apply the
Map Miner Method (MMM) on a large real-world dataset for discovering hidden and
unobservable user behavior, strategies and intentions. We first discover user
strategies from the given activity sequence data by applying Hidden Markov
Model (HMM) based unsupervised learning technique. We then process the emission
and transition matrices of the discovered HMM to generate a coarse-grained Map
Process Model. We present the first application or study of the new and
emerging field of Intention-oriented process mining on an incident management
event-log dataset and discuss its applicability, effectiveness and challenges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01066</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01066</id><created>2015-07-04</created><updated>2015-08-30</updated><authors><author><keyname>Hutchison</keyname><forenames>Dylan</forenames></author><author><keyname>Kepner</keyname><forenames>Jeremy</forenames></author><author><keyname>Gadepally</keyname><forenames>Vijay</forenames></author><author><keyname>Fuchs</keyname><forenames>Adam</forenames></author></authors><title>Graphulo Implementation of Server-Side Sparse Matrix Multiply in the
  Accumulo Database</title><categories>cs.DB cs.DC cs.MS</categories><comments>To be presented at IEEE HPEC 2015: http://www.ieee-hpec.org/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Apache Accumulo database excels at distributed storage and indexing and
is ideally suited for storing graph data. Many big data analytics compute on
graph data and persist their results back to the database. These graph
calculations are often best performed inside the database server. The GraphBLAS
standard provides a compact and efficient basis for a wide range of graph
applications through a small number of sparse matrix operations. In this
article, we implement GraphBLAS sparse matrix multiplication server-side by
leveraging Accumulo's native, high-performance iterators. We compare the
mathematics and performance of inner and outer product implementations, and
show how an outer product implementation achieves optimal performance near
Accumulo's peak write rate. We offer our work as a core component to the
Graphulo library that will deliver matrix math primitives for graph analytics
within Accumulo.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01067</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01067</id><created>2015-07-04</created><authors><author><keyname>de Robles</keyname><forenames>Marie Yvette B.</forenames></author><author><keyname>Arnejo</keyname><forenames>Zenith O.</forenames></author><author><keyname>Pabico</keyname><forenames>Jaderick P.</forenames></author></authors><title>On Web-grid Implementation Using Single System Image</title><categories>cs.DC</categories><comments>10 pages, 6 figures, appeared in H.N. Adorna and A.L. Sioson (eds.)
  Proceedings of the 6th National Symposium on Mathematical Aspects of Computer
  Science (SMACS 2012), La Carmela de Boracay Convention Center, Boracay
  Island, Malay, Aklan, Philippines, 04-08 December 2012, pp. 135-143</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  With the latest innovations and trend towards personalizing users' web
browsing experience, the web has been increasingly dominated by dynamic
contents. However, delivering dynamic content remains a challenge due to the
many dependencies involved in compiling the content, specifically personalized
ones. This paper presents the use of Single System Image (SSI) clustering
systems for a cheap, off-the-shelf, local lightweight distributed web-grid
composed of desktop PCs. The three clustering systems considered in the study
are Kerrighed, OpenSSI and openMosix. Through an online simulation technique,
the performance savings achieved by the clustering systems were measured.
Results showed that Kerrighed has the least number of missed requests while the
response time is comparable with the rest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01073</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01073</id><created>2015-07-04</created><updated>2015-12-23</updated><authors><author><keyname>Yamada</keyname><forenames>Makoto</forenames></author><author><keyname>Lian</keyname><forenames>Wenzhao</forenames></author><author><keyname>Goyal</keyname><forenames>Amit</forenames></author><author><keyname>Chen</keyname><forenames>Jianhui</forenames></author><author><keyname>Khan</keyname><forenames>Suleiman A</forenames></author><author><keyname>Kaski</keyname><forenames>Samuel</forenames></author><author><keyname>Mamitsuka</keyname><forenames>Hiroshi</forenames></author><author><keyname>Chang</keyname><forenames>Yi</forenames></author></authors><title>Convex Factorization Machine for Regression</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose the convex factorization machine (CFM), which is a convex variant
of the widely used Factorization Machines (FMs). Specifically, we employ a
linear+quadratic model and regularize the linear term with the
$\ell_2$-regularizer and the quadratic term with the trace norm regularizer.
Then, we formulate the CFM optimization as a semidefinite programming problem
and propose an efficient optimization procedure with Hazan's algorithm. A key
advantage of CFM over existing FMs is that it can find a globally optimal
solution, while FMs may get a poor locally optimal solution since the objective
function of FMs is non-convex. In addition, the proposed algorithm is simple
yet effective and can be implemented easily. Finally, CFM is a general
factorization method and can also be used for other factorization problems
including multi-view matrix factorization problems. Through synthetic and
movielens datasets, we first show that the proposed CFM achieves results
competitive to FMs. Furthermore, in a toxicogenomics prediction task, we show
that CFM outperforms a state-of-the-art tensor factorization method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01082</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01082</id><created>2015-07-04</created><authors><author><keyname>Arevalo</keyname><forenames>Chezka Camille P.</forenames></author><author><keyname>Pabico</keyname><forenames>Jaderick P.</forenames></author></authors><title>Preferential Attachment in an Internet-mediated Human Network</title><categories>cs.SI physics.soc-ph</categories><comments>5 pages, 3 figures, this version appeared in H.N. Adorna and R.P.
  Salda\~na (eds.) Proceedings (CDROM ISSN 1908-1146) of the 2009 Philippine
  Computing Science Congress, Silliman University, Dumaguete City, 2-3 March
  2009, pp 97-101</comments><journal-ref>Philippine Computing Journal 2009, vol 4, num 1, pp 31-35 (ISSN
  1908-1995)</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In the advent of the Internet, web-mediated social networking has become of
great influence to Filipinos. Networking sites such as Friendster, YouTube,
FaceBook and MySpace are among the most well known sites on the Internet. These
sites provide a wide range of services to users from different parts of the
world, such as connecting and finding people, as well as, sharing and
organizing contents. The popularity and accessibility of these sites enable
information to be available. These allow people to analyze and study the
characteristics of the population of online social networks. In this study, we
developed a computer program to analyze the structural dynamics of a locally
popular social networking site: The Friendster Network. Understanding the
structural dynamics of a virtual community has many implications, such as
finding an improvement on the current networking system, among others. Based on
our analysis, we found out that users of the site exhibit preferential
attachment to users with high number of friends.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01083</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01083</id><created>2015-07-04</created><authors><author><keyname>Dumas</keyname><forenames>Jean-Guillaume</forenames><affiliation>NCSU</affiliation></author><author><keyname>Kaltofen</keyname><forenames>Erich</forenames><affiliation>NCSU</affiliation></author><author><keyname>Thom&#xe9;</keyname><forenames>Emmanuel</forenames><affiliation>CARAMEL</affiliation></author></authors><title>Interactive certificate for the verification of Wiedemann's Krylov
  sequence: application to the certification of the determinant, the minimal
  and the characteristic polynomials of sparse matrices</title><categories>cs.SC cs.CC cs.CR</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Certificates to a linear algebra computation are additional data structures
for each output, which can be used by a-possibly randomized- verification
algorithm that proves the correctness of each output. Wiede-mann's algorithm
projects the Krylov sequence obtained by repeatedly multiplying a vector by a
matrix to obtain a linearly recurrent sequence. The minimal polynomial of this
sequence divides the minimal polynomial of the matrix. For instance, if the
$n\times n$ input matrix is sparse with n 1+o(1) non-zero entries, the
computation of the sequence is quadratic in the dimension of the matrix while
the computation of the minimal polynomial is n 1+o(1), once that projected
Krylov sequence is obtained. In this paper we give algorithms that compute
certificates for the Krylov sequence of sparse or structured $n\times n$
matrices over an abstract field, whose Monte Carlo verification complexity can
be made essentially linear. As an application this gives certificates for the
determinant, the minimal and characteristic polynomials of sparse or structured
matrices at the same cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01088</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01088</id><created>2015-07-04</created><updated>2015-11-12</updated><authors><author><keyname>Bassino</keyname><forenames>Fr&#xe9;d&#xe9;rique</forenames><affiliation>LIPN</affiliation></author><author><keyname>Nicaud</keyname><forenames>Cyril</forenames><affiliation>LIGM</affiliation></author><author><keyname>Weil</keyname><forenames>Pascal</forenames><affiliation>LaBRI</affiliation></author></authors><title>Generic properties of subgroups of free groups and finite presentations</title><categories>math.GR cs.DM math.CO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Asymptotic properties of finitely generated subgroups of free groups, and of
finite group presentations, can be considered in several fashions, depending on
the way these objects are represented and on the distribution assumed on these
representations: here we assume that they are represented by tuples of reduced
words (generators of a subgroup) or of cyclically reduced words (relators).
Classical models consider fixed size tuples of words (e.g. the few-generator
model) or exponential size tuples (e.g. Gromov's density model), and they
usually consider that equal length words are equally likely. We generalize both
the few-generator and the density models with probabilistic schemes that also
allow variability in the size of tuples and non-uniform distributions on words
of a given length.Our first results rely on a relatively mild prefix-heaviness
hypothesis on the distributions, which states essentially that the probability
of a word decreases exponentially fast as its length grows. Under this
hypothesis, we generalize several classical results: exponentially generically
a randomly chosen tuple is a basis of the subgroup it generates, this subgroup
is malnormal and the tuple satisfies a small cancellation property, even for
exponential size tuples. In the special case of the uniform distribution on
words of a given length, we give a phase transition theorem for the central
tree property, a combinatorial property closely linked to the fact that a tuple
freely generates a subgroup. We then further refine our results when the
distribution is specified by a Markovian scheme, and in particular we give a
phase transition theorem which generalizes the classical results on the
densities up to which a tuple of cyclically reduced words chosen uniformly at
random exponentially generically satisfies a small cancellation property, and
beyond which it presents a trivial group.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01089</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01089</id><created>2015-07-04</created><authors><author><keyname>Duchamp</keyname><forenames>G&#xe9;rard H. E.</forenames><affiliation>LIPN</affiliation></author><author><keyname>Minh</keyname><forenames>Vincel Hoang Ngoc</forenames><affiliation>LIPN</affiliation></author><author><keyname>Tollu</keyname><forenames>Christophe</forenames><affiliation>LIPN</affiliation></author><author><keyname>Bui</keyname><forenames>Van Chi&#xea;n</forenames><affiliation>LIPN</affiliation></author><author><keyname>Ng&#xf4;</keyname><forenames>Quoc Hoan</forenames><affiliation>LIPN</affiliation></author></authors><title>(Pure) transcendence bases in $\phi$-deformed shuffle bialgebras</title><categories>cs.SC math-ph math.CO math.GR math.MP</categories><proxy>ccsd</proxy><journal-ref>Seminaire Lotharingien de Combinatoire, Universit\'e Louis
  Pasteur, 2015, pp.1-31</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computations with integro-differential operators are often carried out in an
associative algebra with unit and they are essentially non-commutative
computations. By adjoining a cocommutative co-product, one can have those
operators perform on a bialgebra isomorphic to an enveloping algebra. That
gives an adequate framework for a computer-algebra implementation via monoidal
factorization, (pure) transcendence bases and Poincar\'e-Birkhoff-Witt bases.
In this paper, we systematically study these deformations, obtaining necessary
and sufficient conditions for the operators to exist, and we give the most
general cocommutative deformations of the shuffle co-product and an effective
construction of pairs of bases in duality. The paper ends by the combinatorial
setting of systems of local systems of coordinates on the group of group-like
series. * The present work is part of a series of papers devoted to the study
of the renormalization of divergent polyzetas (at positive and at non-positive
indices) via the factorization of the non commutative generating series of
polylogarithms and of harmonic sums and via the effective construction of pairs
of dual bases in duality in $\phi$-deformed shuffle algebras. It is a sequel to
[3] and its content was presented in several seminars and meetings, including
the 74th S\'eminaire Lotharingien de Combinatoire.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01095</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01095</id><created>2015-07-04</created><authors><author><keyname>Lalitha</keyname><forenames>V.</forenames></author><author><keyname>Lokam</keyname><forenames>Satyanarayana V.</forenames></author></authors><title>Weight Enumerators and Higher Support Weights of Maximally Recoverable
  Codes</title><categories>cs.IT math.IT</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we establish the matroid structures corresponding to
data-local and local maximally recoverable codes (MRC). The matroid structures
of these codes can be used to determine the associated Tutte polynomial. Greene
proved that the weight enumerators of any code can be determined from its
associated Tutte polynomial. We will use this result to derive explicit
expressions for the weight enumerators of data-local and local MRC. Also, Britz
proved that the higher support weights of any code can be determined from its
associated Tutte polynomial. We will use this result to derive expressions for
the higher support weights of data-local and local MRC with two local codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01098</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01098</id><created>2015-07-04</created><updated>2016-02-01</updated><authors><author><keyname>Karpuk</keyname><forenames>David</forenames></author><author><keyname>Chorti</keyname><forenames>Arsenia</forenames></author></authors><title>Perfect Secrecy in Physical Layer Network Coding Systems from Structured
  Interference</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Physical layer network coding (PNC) has been proposed for next generation
networks. In this contribution, we investigate PNC schemes with embedded
perfect secrecy by exploiting structured interference in relay networks with
two users and a single relay. In a practical scenario where both users employ
finite and uniform signal input distributions we propose upper bounds (UBs) on
the achievable perfect secrecy rates and make these explicit when PAM modems
are used. We then describe two simple, explicit encoders that can achieve
perfect secrecy rates close to these UBs with respect to an untrustworthy relay
in the single antenna and single relay setting. Lastly, we generalize our
system to a MIMO relay channel where the relay has more antennas than the users
and optimal precoding matrices which maintain a required secrecy constraint are
studied. Our results establish that the design of PNC transmission schemes with
enhanced throughput and guaranteed data confidentiality is feasible in next
generation systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01101</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01101</id><created>2015-07-04</created><updated>2015-10-22</updated><authors><author><keyname>Lai</keyname><forenames>Pan</forenames></author><author><keyname>Fan</keyname><forenames>Rui</forenames></author><author><keyname>Zhang</keyname><forenames>Wei</forenames></author><author><keyname>Liu</keyname><forenames>Fang</forenames></author></authors><title>Utility Maximizing Thread Assignment and Resource Allocation</title><categories>cs.DC</categories><comments>11 pages, 7 figures</comments><acm-class>C.1.4; D.4.2; F.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Achieving high performance in many distributed systems, ranging from clouds
to multicore processors, requires finding good assignments of threads to
servers as well as effectively allocating each server's resources to its
assigned threads. Both the assignment and allocation component of this problem
have been studied extensively, though separately in the literature. In this
paper, we introduce the \emph{assign and allocate (AA)} problem, which seeks to
simultaneously find an assignment and allocations that maximize the total
utility of the threads. We first show that this problem is NP-hard, even when
there are only two servers. We then present a $2 (\sqrt{2}-1) &gt; 0.828$ factor
approximation algorithm, which runs in $O(mn^2 + n (\log mC)^2)$ time for $n$
threads and $m$ servers with $C$ amount of resources each. We also present a
faster algorithm with the same approximation ratio and $O(n (\log mC)^2)$
running time. We conducted experiments to test the performance of our algorithm
on different types of threads, and found that it achieves over 99\% of the
optimal utility on average. We also compared our algorithm against several
other assignment and allocation algorithms, and found that it achieves up to
5.7 times better total utility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01109</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01109</id><created>2015-07-04</created><authors><author><keyname>Worthington</keyname><forenames>Simon</forenames></author></authors><title>Book to the Future - a manifesto for book liberation</title><categories>cs.DL cs.CY</categories><comments>178 x 111mm, 44 page, spine 2.769mm, 50.801 gm, cover color matt,
  interior color, paper 70 gsm, illustrations 12</comments><doi>10.5281/zenodo.18166</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The Book Liberation Manifesto is an exploration of publishing outside of
current corporate constraints and beyond the confines of book piracy. We
believe that knowledge should be in free circulation to benefit humankind,
which means an equitable and vibrant economy to support publishing, instead of
the prevailing capitalist hand-me-down system of Sisyphean economic
sustainability. Readers and books have been forced into pirate libraries, while
sales channels have been monopolised by the big Internet giants which exact
extortionate fees from publishers. We have three proposals. First, publications
should be free-at-the-point-of-reading under a variety of open intellectual
property regimes. Second, they should become fully digital -- in order to
facilitate ready reuse, distribution, algorithmic and computational use.
Finally, Open Source software for publishing should be treated as public
infrastructure, with sustained research and investment. The result of such
robust infrastructures will mean lower costs for manufacturing and faster
publishing lifecycles, so that publishers and publics will be more readily able
to afford to invent new futures. For more information on the Hybrid Publishing
Consortium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01111</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01111</id><created>2015-07-04</created><authors><author><keyname>Fern&#xe1;ndez-Duque</keyname><forenames>David</forenames></author><author><keyname>Nepomuceno-Fern&#xe1;ndez</keyname><forenames>&#xc1;ngel</forenames></author><author><keyname>Sarri&#xf3;n-Morrillo</keyname><forenames>Enrique</forenames></author><author><keyname>Soler-Toscano</keyname><forenames>Fernando</forenames></author><author><keyname>Vel&#xe1;zquez-Quesada</keyname><forenames>Fernando R.</forenames></author></authors><title>Forgetting complex propositions</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper uses possible-world semantics to model the changes that may occur
in an agent's knowledge as she loses information. This builds on previous work
in which the agent may forget the truth-value of an atomic proposition, to a
more general case where she may forget the truth-value of a propositional
formula. The generalization poses some challenges, since in order to forget
whether a complex proposition $\pi$ is the case, the agent must also lose
information about the propositional atoms that appear in it, and there is no
unambiguous way to go about this.
  We resolve this situation by considering expressions of the form
$[\boldsymbol{\ddagger} \pi]\varphi$, which quantify over all possible (but
minimal) ways of forgetting whether $\pi$. Propositional atoms are modified
non-deterministically, although uniformly, in all possible worlds. We then
represent this within action model logic in order to give a sound and complete
axiomatization for a logic with knowledge and forgetting. Finally, some
variants are discussed, such as when an agent forgets $\pi$ (rather than
forgets whether $\pi$) and when the modification of atomic facts is done
non-uniformly throughout the model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01120</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01120</id><created>2015-07-04</created><authors><author><keyname>Joret</keyname><forenames>Gwena&#xeb;l</forenames></author><author><keyname>Micek</keyname><forenames>Piotr</forenames></author><author><keyname>Wiechert</keyname><forenames>Veit</forenames></author></authors><title>Sparsity and dimension</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that posets of bounded height whose cover graphs belong to a fixed
class with bounded expansion have bounded dimension. Bounded expansion,
introduced by Ne\v{s}et\v{r}il and Ossona de Mendez as a model for sparsity in
graphs, is a property that is naturally satisfied by a wide range of graph
classes, from graph structure theory (graphs excluding a minor or a topological
minor) to graph drawing (e.g. graphs with constant book thickness). Therefore,
our theorem generalizes a number of results including the most recent one for
posets of bounded height with cover graphs excluding a fixed graph as a
topological minor (Walczak, SODA 2015). We also show that the result is in a
sense best possible, as it does not extend to nowhere dense classes; in fact,
it already fails for cover graphs with locally bounded treewidth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01122</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01122</id><created>2015-07-04</created><updated>2015-11-11</updated><authors><author><keyname>Makdah</keyname><forenames>Gabriel</forenames></author></authors><title>Modeling the Mind: A brief review</title><categories>cs.AI cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The brain is a powerful tool used to achieve amazing feats. There have been
several significant advances in neuroscience and artificial brain research in
the past two decades. This article is a review of such advances, ranging from
the concepts of connectionism, to neural network architectures and
high-dimensional representations. There have also been advances in biologically
inspired cognitive architectures of which we will cite a few. We will be
positioning relatively specific models in a much broader perspective, while
comparing and contrasting their advantages and weaknesses. The projects
presented are targeted to model the brain at different levels, utilizing
different methodologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01127</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01127</id><created>2015-07-04</created><authors><author><keyname>Rothe</keyname><forenames>Sascha</forenames></author><author><keyname>Sch&#xfc;tze</keyname><forenames>Hinrich</forenames></author></authors><title>AutoExtend: Extending Word Embeddings to Embeddings for Synsets and
  Lexemes</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present \textit{AutoExtend}, a system to learn embeddings for synsets and
lexemes. It is flexible in that it can take any word embeddings as input and
does not need an additional training corpus. The synset/lexeme embeddings
obtained live in the same vector space as the word embeddings. A sparse tensor
formalization guarantees efficiency and parallelizability. We use WordNet as a
lexical resource, but AutoExtend can be easily applied to other resources like
Freebase. AutoExtend achieves state-of-the-art performance on word similarity
and word sense disambiguation tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01138</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01138</id><created>2015-07-04</created><authors><author><keyname>Groen</keyname><forenames>Derek</forenames><affiliation>University College London</affiliation></author><author><keyname>Zwart</keyname><forenames>Simon Portegies</forenames><affiliation>Sterrewacht Leiden</affiliation></author></authors><title>From Thread to Transcontinental Computer: Disturbing Lessons in
  Distributed Supercomputing</title><categories>astro-ph.IM cs.CY cs.DC</categories><comments>Accepted for publication in IEEE conference on ERRORs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe the political and technical complications encountered during the
astronomical CosmoGrid project. CosmoGrid is a numerical study on the formation
of large scale structure in the universe. The simulations are challenging due
to the enormous dynamic range in spatial and temporal coordinates, as well as
the enormous computer resources required. In CosmoGrid we dealt with the
computational requirements by connecting up to four supercomputers via an
optical network and make them operate as a single machine. This was
challenging, if only for the fact that the supercomputers of our choice are
separated by half the planet, as three of them are located scattered across
Europe and fourth one is in Tokyo. The co-scheduling of multiple computers and
the 'gridification' of the code enabled us to achieve an efficiency of up to
$93\%$ for this distributed intercontinental supercomputer. In this work, we
find that high-performance computing on a grid can be done much more
effectively if the sites involved are willing to be flexible about their user
policies, and that having facilities to provide such flexibility could be key
to strengthening the position of the HPC community in an increasingly
Cloud-dominated computing landscape. Given that smaller computer clusters owned
by research groups or university departments usually have flexible user
policies, we argue that it could be easier to instead realize distributed
supercomputing by combining tens, hundreds or even thousands of these
resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01145</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01145</id><created>2015-07-04</created><authors><author><keyname>Hogg</keyname><forenames>Tad</forenames></author></authors><title>Energy Dissipation by Metamorphic Micro-Robots in Viscous Fluids</title><categories>cs.RO physics.bio-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Microscopic robots could perform tasks with high spatial precision, such as
acting on precisely-targeted cells in biological tissues. Some tasks may
benefit from robots that change shape, such as elongating to improve chemical
gradient sensing or contracting to squeeze through narrow channels. This paper
evaluates the energy dissipation for shape-changing (i.e., metamorphic) robots
whose size is comparable to bacteria. Unlike larger robots, surface forces
dominate the dissipation. Theoretical estimates indicate that the power likely
to be available to the robots, as determined by previous studies, is sufficient
to change shape fairly rapidly even in highly-viscous biological fluids.
Achieving this performance will require significant improvements in
manufacturing and material properties compared to current micromachines.
Furthermore, optimally varying the speed of shape change only slightly reduces
energy use compared to uniform speed, thereby simplifying robot controllers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01146</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01146</id><created>2015-07-04</created><authors><author><keyname>Casta&#xf1;os</keyname><forenames>Fernando</forenames></author><author><keyname>Estrada</keyname><forenames>Edgar</forenames></author><author><keyname>Mondi&#xe9;</keyname><forenames>Sabine</forenames></author><author><keyname>Ram&#xed;rez</keyname><forenames>Adri&#xe1;n</forenames></author></authors><title>Passivity-based PI control of first-order systems with I/O communication
  delays: A complete sigma-stability analysis</title><categories>math.OC cs.SY</categories><comments>21 pages, 11 figures, submitted to International Journal of Control</comments><msc-class>93C80,</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The PI control of first-order linear passive systems through a delayed
communication channel is revisited in light of the relative stability concept
called sigma-stability. Treating the delayed communication channel as a
transport PDE, the passivity of the overall control-loop is guaranteed,
resulting in a closed-loop system of neutral nature. Spectral methods are then
applied to the system to obtain a complete stability map. In particular, we
perform the D-subdivision method to declare the exact sigma-stability regions
in the space of PI parameters. This framework is then utilized to analytically
determine the maximum achievable exponential decay rate of the system while
achieving the PI tuning as an explicit function of the decay rate and the
system parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01147</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01147</id><created>2015-07-04</created><updated>2015-07-10</updated><authors><author><keyname>Wang</keyname><forenames>Yan</forenames></author><author><keyname>Cho</keyname><forenames>Sunghyun</forenames></author><author><keyname>Wang</keyname><forenames>Jue</forenames></author><author><keyname>Chang</keyname><forenames>Shih-Fu</forenames></author></authors><title>PanoSwarm: Collaborative and Synchronized Multi-Device Panoramic
  Photography</title><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Taking a picture has been traditionally a one-persons task. In this paper we
present a novel system that allows multiple mobile devices to work
collaboratively in a synchronized fashion to capture a panorama of a highly
dynamic scene, creating an entirely new photography experience that encourages
social interactions and teamwork. Our system contains two components: a client
app that runs on all participating devices, and a server program that monitors
and communicates with each device. In a capturing session, the server collects
in realtime the viewfinder images of all devices and stitches them on-the-fly
to create a panorama preview, which is then streamed to all devices as visual
guidance. The system also allows one camera to be the host and to send direct
visual instructions to others to guide camera adjustment. When ready, all
devices take pictures at the same time for panorama stitching. Our preliminary
study suggests that the proposed system can help users capture high quality
panoramas with an enjoyable teamwork experience.
  A demo video of the system in action is provided at
http://youtu.be/PwQ6k_ZEQSs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01148</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01148</id><created>2015-07-04</created><updated>2015-07-08</updated><authors><author><keyname>Wang</keyname><forenames>Yan</forenames></author><author><keyname>Wang</keyname><forenames>Jue</forenames></author><author><keyname>Chang</keyname><forenames>Shih-Fu</forenames></author></authors><title>CamSwarm: Instantaneous Smartphone Camera Arrays for Collaborative
  Photography</title><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Camera arrays (CamArrays) are widely used in commercial filming projects for
achieving special visual effects such as bullet time effect, but are very
expensive to set up. We propose CamSwarm, a low-cost and lightweight
alternative to professional CamArrays for consumer applications. It allows the
construction of a collaborative photography platform from multiple mobile
devices anywhere and anytime, enabling new capturing and editing experiences
that a single camera cannot provide. Our system allows easy team formation;
uses real-time visualization and feedback to guide camera positioning; provides
a mechanism for synchronized capturing; and finally allows the user to
efficiently browse and edit the captured imagery. Our user study suggests that
CamSwarm is easy to use; the provided real-time guidance is helpful; and the
full system achieves high quality results promising for non-professional use.
  A demo video is provided at https://www.youtube.com/watch?v=LgkHcvcyTTM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01150</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01150</id><created>2015-07-04</created><authors><author><keyname>Capriotti</keyname><forenames>Paolo</forenames></author><author><keyname>Kraus</keyname><forenames>Nicolai</forenames></author><author><keyname>Vezzosi</keyname><forenames>Andrea</forenames></author></authors><title>Functions out of Higher Truncations</title><categories>cs.LO math.LO</categories><comments>15 pages; to appear at CSL'15</comments><msc-class>03B15, 03B70</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In homotopy type theory, the truncation operator ||-||n (for a number n &gt; -2)
is often useful if one does not care about the higher structure of a type and
wants to avoid coherence problems. However, its elimination principle only
allows to eliminate into n-types, which makes it hard to construct functions
||A||n -&gt; B if B is not an n-type. This makes it desirable to derive more
powerful elimination theorems. We show a first general result: If B is an
(n+1)-type, then functions ||A||n -&gt; B correspond exactly to functions A -&gt; B
which are constant on all (n+1)-st loop spaces. We give one &quot;elementary&quot; proof
and one proof that uses a higher inductive type, both of which require some
effort. As a sample application of our result, we show that we can construct
&quot;set-based&quot; representations of 1-types, as long as they have &quot;braided&quot; loop
spaces. The main result with one of its proofs and the application have been
formalised in Agda.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01151</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01151</id><created>2015-07-04</created><authors><author><keyname>Chamie</keyname><forenames>Mahmoud El</forenames></author><author><keyname>Acikmese</keyname><forenames>Behcet</forenames></author></authors><title>Finite-Horizon Markov Decision Processes with Sequentially-Observed
  Transitions</title><categories>math.OC cs.SY</categories><comments>submitted to IEEE CDC 2015</comments><msc-class>90C40</msc-class><acm-class>I.2.8; G.1.6; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Markov Decision Processes (MDPs) have been used to formulate many
decision-making problems in science and engineering. The objective is to
synthesize the best decision (action selection) policies to maximize expected
rewards (or minimize costs) in a given stochastic dynamical environment. In
this paper, we extend this model by incorporating additional information that
the transitions due to actions can be sequentially observed. The proposed model
benefits from this information and produces policies with better performance
than those of standard MDPs. The paper also presents an efficient offline
linear programming based algorithm to synthesize optimal policies for the
extended model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01155</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01155</id><created>2015-07-04</created><updated>2015-10-15</updated><authors><author><keyname>Esfandiari</keyname><forenames>Hossein</forenames></author><author><keyname>Hajiaghayi</keyname><forenames>MohammadTaghi</forenames></author><author><keyname>Liaghat</keyname><forenames>Vahid</forenames></author><author><keyname>Monemizadeh</keyname><forenames>Morteza</forenames></author></authors><title>Prophet Secretary</title><categories>cs.GT</categories><comments>A preliminary version of this paper is to appear in Proceedings of
  the 23rd European Symposium on Algorithms (ESA 2015), Patras, Greece. arXiv
  admin note: text overlap with arXiv:1201.4764 by other authors</comments><acm-class>F.2.2; F.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimal stopping theory is a powerful tool for analyzing scenarios such as
online auctions in which we generally require optimizing an objective function
over the space of stopping rules for an allocation process under uncertainty.
Perhaps the most classic problems of stopping theory are the prophet inequality
problem and the secretary problem. The classical prophet inequality states that
by choosing the same threshold OPT/2 for every step, one can achieve the tight
competitive ratio of 0.5. On the other hand, for the basic secretary problem,
the optimal strategy achieves the tight competitive ratio of 1/e.
  In this paper, we introduce Prophet Secretary, a natural combination of the
prophet inequality and the secretary problems. An example motivation for our
problem is as follows. Consider a seller that has an item to sell on the market
to a set of arriving customers. The seller knows the types of customers that
may be interested in the item and he has a price distribution for each type:
the price offered by a customer of a type is anticipated to be drawn from the
corresponding distribution. However, the customers arrive in a random order.
Upon the arrival of a customer, the seller makes an irrevocable decision
whether to sell the item at the offered price. We address the question of
finding a strategy for selling the item at a high price.
  We show that by using a uniform threshold one cannot break the 0.5 barrier.
However, we show that i) using n distinct non-adaptive thresholds one can
obtain a competitive ratio that goes to (1-1/e) as n grows; and ii) no online
algorithm can achieve a competitive ratio better than 0.75. Our results improve
the (asymptotic) approximation guarantee of single-item sequential posted
pricing mechanisms from 0.5 to (1-1/e) when the order of agents (customers) is
chosen randomly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01158</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01158</id><created>2015-07-04</created><authors><author><keyname>Baker</keyname><forenames>Troy</forenames></author><author><keyname>Sitharam</keyname><forenames>Meera</forenames></author><author><keyname>Wang</keyname><forenames>Menghan</forenames></author><author><keyname>Willoughby</keyname><forenames>Joel</forenames></author></authors><title>Optimal Decomposition and Recombination of Isostatic Geometric
  Constraint Systems for Designing Layered Materials</title><categories>cs.CG</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Optimal recursive decomposition (or DR-planning) is crucial for analyzing,
designing, solving or finding realizations of geometric constraint sytems.
While the optimal DR-planning problem is NP-hard even for general 2D bar-joint
constraint systems, we describe an O(n^3) algorithm for a broad class of
constraint systems that are isostatic or underconstrained. The algorithm
achieves optimality by using the new notion of a canonical DR-plan that also
meets various desirable, previously studied criteria. In addition, we leverage
recent results on Cayley configuration spaces to show that the indecomposable
systems---that are solved at the nodes of the optimal DR-plan by recombining
solutions to child systems---can be minimally modified to become decomposable
and have a small DR-plan, leading to efficient realization algorithms. We show
formal connections to well-known problems such as completion of
underconstrained systems. Well suited to these methods are classes of
constraint systems that can be used to efficiently model, design and analyze
quasi-uniform (aperiodic) and self-similar, layered material structures. We
formally illustrate by modeling silica bilayers as body-hyperpin systems and
cross-linking microfibrils as pinned line-incidence systems. A software
implementation of our algorithms and videos demonstrating the software are
publicly available online (visit http://cise.ufl.edu/~tbaker/drp/index.html.)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01159</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01159</id><created>2015-07-04</created><authors><author><keyname>Lee</keyname><forenames>Euiwoong</forenames></author></authors><title>APX-Hardness of Maximizing Nash Social Welfare with Indivisible Items</title><categories>cs.CC cs.DS cs.GT</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of allocating a set of indivisible items to agents with
additive utilities to maximize the Nash social welfare. Cole and Gkatzelis
recently proved that this problem admits a constant factor approximation. We
complement their result by showing that this problem is APX-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01160</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01160</id><created>2015-07-04</created><updated>2015-07-07</updated><authors><author><keyname>Srivastava</keyname><forenames>Vaibhav</forenames></author><author><keyname>Reverdy</keyname><forenames>Paul</forenames></author><author><keyname>Leonard</keyname><forenames>Naomi Ehrich</forenames></author></authors><title>Correlated Multiarmed Bandit Problem: Bayesian Algorithms and Regret
  Analysis</title><categories>math.OC cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the correlated multiarmed bandit (MAB) problem in which the
rewards associated with each arm are modeled by a multivariate Gaussian random
variable, and we investigate the influence of the assumptions in the Bayesian
prior on the performance of the upper credible limit (UCL) algorithm and a new
correlated UCL algorithm. We rigorously characterize the influence of accuracy,
confidence, and correlation scale in the prior on the decision-making
performance of the algorithms. Our results show how priors and correlation
structure can be leveraged to improve performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01162</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01162</id><created>2015-07-04</created><authors><author><keyname>Hong</keyname><forenames>Haibo</forenames></author><author><keyname>Wang</keyname><forenames>Licheng</forenames></author><author><keyname>Ahmad</keyname><forenames>Haseeb</forenames></author><author><keyname>Li</keyname><forenames>Jing</forenames></author><author><keyname>Yang</keyname><forenames>Yixian</forenames></author></authors><title>Minimal Logarithmic Signatures for Sporadic Groups</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As a special type of factorization of finite groups, logarithmic signature
(LS) is used as the main component of cryptographic keys for secret key
cryptosystems such as PGM and public key cryptosystems like MST1, MST2 and
MST3. An LS with the shortest length is called a minimal logarithmic signature
(MLS) and is even desirable for cryptographic constructions. The MLS conjecture
states that every finite simple group has an MLS. Until now, the MLS conjecture
has been proved true for some families of simple groups. In this paper, we will
prove the existence of minimal logarithmic signatures for some sporadic groups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01163</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01163</id><created>2015-07-04</created><authors><author><keyname>Hong</keyname><forenames>Haibo</forenames></author><author><keyname>Wang</keyname><forenames>Licheng</forenames></author><author><keyname>Ahmad</keyname><forenames>Haseeb</forenames></author><author><keyname>Yang</keyname><forenames>Yixian</forenames></author></authors><title>Minimal Logarithmic Signatures for one type of Classical Groups</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As a special type of factorization of finite groups, logarithmic signature
(LS) is used as the main component of cryptographic keys for secret key
cryptosystems such as PGM and public key cryptosystems like MST1, MST2 and
MST3. An LS with the shortest length, called a minimal logarithmic signature
(MLS), is even desirable for cryptographic applications. The MLS conjecture
states that every finite simple group has an MLS. Recently, the conjecture has
been shown to be true for general linear groups GLn(q), special linear groups
SLn(q), and symplectic groups Spn(q) with q a power of primes and for
orthogonal groups On(q) with q as a power of 2. In this paper, we present new
constructions of minimal logarithmic signatures for the orthogonal group On(q)
and SOn(q) with q as a power of odd primes. Furthermore, we give constructions
of MLSs for a type of classical groups projective commutator subgroup.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01167</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01167</id><created>2015-07-05</created><authors><author><keyname>Ye</keyname><forenames>Hongxing</forenames></author><author><keyname>Ge</keyname><forenames>Yinyin</forenames></author><author><keyname>Shahidehpour</keyname><forenames>Mohammad</forenames></author><author><keyname>Li</keyname><forenames>Zuyi</forenames></author></authors><title>Market Clearing for Uncertainty, Generation Reserve, and Transmission
  Reserve--Part II:Case Study</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Part II of this two-part paper, we analyze the marginal prices derived in
Part I of this two-part paper within a robust optimization framework. The load
and generation are priced at Locational Marginal Price (LMP) while the
uncertainty and generation reserve are priced at Uncertainty Marginal
Price(UMP). The Financial Transmission Right (FTR) underfunding is demonstrated
when there is transmission reserve. A comparison between traditional reserve
price and UMP is presented. We also discuss the incentives for market
participants within the new market scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01168</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01168</id><created>2015-07-05</created><authors><author><keyname>Sureka</keyname><forenames>Ashish</forenames></author></authors><title>Kernel Based Sequential Data Anomaly Detection in Business Process Event
  Logs</title><categories>cs.SE cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Business Process Management Systems (BPMS) log events and traces of
activities during the execution of a process. Anomalies are defined as
deviation or departure from the normal or common order. Anomaly detection in
business process logs has several applications such as fraud detection and
understanding the causes of process errors. In this paper, we present a novel
approach for anomaly detection in business process logs. We model the event
logs as a sequential data and apply kernel based anomaly detection techniques
to identify outliers and discordant observations. Our technique is unsupervised
(does not require a pre-annotated training dataset), employs kNN (k-nearest
neighbor) kernel based technique and normalized longest common subsequence
(LCS) similarity measure. We conduct experiments on a recent, large and
real-world incident management data of an enterprise and demonstrate that our
approach is effective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01181</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01181</id><created>2015-07-05</created><authors><author><keyname>Censor-Hillel</keyname><forenames>Keren</forenames></author><author><keyname>Toukan</keyname><forenames>Tariq</forenames></author></authors><title>On Fast and Robust Information Spreading in the Vertex-Congest Model</title><categories>cs.DC</categories><comments>Appears in SIROCCO 2015 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper initiates the study of the impact of failures on the fundamental
problem of \emph{information spreading} in the Vertex-Congest model, in which
in every round, each of the $n$ nodes sends the same $O(\log{n})$-bit message
to all of its neighbors.
  Our contribution to coping with failures is twofold. First, we prove that the
randomized algorithm which chooses uniformly at random the next message to
forward is slow, requiring $\Omega(n/\sqrt{k})$ rounds on some graphs, which we
denote by $G_{n,k}$, where $k$ is the vertex-connectivity.
  Second, we design a randomized algorithm that makes dynamic message choices,
with probabilities that change over the execution. We prove that for $G_{n,k}$
it requires only a near-optimal number of $O(n\log^3{n}/k)$ rounds, despite a
rate of $q=O(k/n\log^3{n})$ failures per round. Our technique of choosing
probabilities that change according to the execution is of independent
interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01191</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01191</id><created>2015-07-05</created><authors><author><keyname>Hub&#xe1;&#x10d;ek</keyname><forenames>Pavel</forenames></author><author><keyname>Naor</keyname><forenames>Moni</forenames></author><author><keyname>Ullman</keyname><forenames>Jonathan</forenames></author></authors><title>When Can Limited Randomness Be Used in Repeated Games?</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The central result of classical game theory states that every finite normal
form game has a Nash equilibrium, provided that players are allowed to use
randomized (mixed) strategies. However, in practice, humans are known to be bad
at generating random-like sequences, and true random bits may be unavailable.
Even if the players have access to enough random bits for a single instance of
the game their randomness might be insufficient if the game is played many
times.
  In this work, we ask whether randomness is necessary for equilibria to exist
in finitely repeated games. We show that for a large class of games containing
arbitrary two-player zero-sum games, approximate Nash equilibria of the
$n$-stage repeated version of the game exist if and only if both players have
$\Omega(n)$ random bits. In contrast, we show that there exists a class of
games for which no equilibrium exists in pure strategies, yet the $n$-stage
repeated version of the game has an exact Nash equilibrium in which each player
uses only a constant number of random bits.
  When the players are assumed to be computationally bounded, if cryptographic
pseudorandom generators (or, equivalently, one-way functions) exist, then the
players can base their strategies on &quot;random-like&quot; sequences derived from only
a small number of truly random bits. We show that, in contrast, in repeated
two-player zero-sum games, if pseudorandom generators \emph{do not} exist, then
$\Omega(n)$ random bits remain necessary for equilibria to exist.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01193</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01193</id><created>2015-07-05</created><authors><author><keyname>Mirowski</keyname><forenames>Piotr</forenames></author><author><keyname>Vlachos</keyname><forenames>Andreas</forenames></author></authors><title>Dependency Recurrent Neural Language Models for Sentence Completion</title><categories>cs.CL cs.AI cs.LG</categories><comments>Accepted for publication at ACL 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work on language modelling has shifted focus from count-based models
to neural models. In these works, the words in each sentence are always
considered in a left-to-right order. In this paper we show how we can improve
the performance of the recurrent neural network (RNN) language model by
incorporating the syntactic dependencies of a sentence, which have the effect
of bringing relevant contexts closer to the word being predicted. We evaluate
our approach on the Microsoft Research Sentence Completion Challenge and show
that the dependency RNN proposed improves over the RNN by about 10 points in
accuracy. Furthermore, we achieve results comparable with the state-of-the-art
models on this task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01196</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01196</id><created>2015-07-05</created><authors><author><keyname>Dinitz</keyname><forenames>Michael</forenames></author><author><keyname>Schapira</keyname><forenames>Michael</forenames></author><author><keyname>Valadarsky</keyname><forenames>Asaf</forenames></author></authors><title>Explicit Expanding Expanders</title><categories>cs.DS math.CO</categories><comments>Extended abstract appears in ESA 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deterministic constructions of expander graphs have been an important topic
of research in computer science and mathematics, with many well-studied
constructions of infinite families of expanders. In some applications, though,
an infinite family is not enough: we need expanders which are &quot;close&quot; to each
other. We study the following question: Construct an an infinite sequence of
expanders $G_0,G_1,\dots$, such that for every two consecutive graphs $G_i$ and
$G_{i+1}$, $G_{i+1}$ can be obtained from $G_i$ by adding a single vertex and
inserting/removing a small number of edges, which we call the expansion cost of
transitioning from $G_i$ to $G_{i+1}$. This question is very natural, e.g., in
the context of datacenter networks, where the vertices represent racks of
servers, and the expansion cost captures the amount of rewiring needed when
adding another rack to the network. We present an explicit construction of
$d$-regular expanders with expansion cost at most $5d/2$, for any $d\geq 6$.
Our construction leverages the notion of a &quot;2-lift&quot; of a graph. This operation
was first analyzed by Bilu and Linial, who repeatedly applied 2-lifts to
construct an infinite family of expanders which double in size from one
expander to the next. Our construction can be viewed as a way to &quot;interpolate&quot;
between Bilu-Linial expanders with low expansion cost while preserving good
edge expansion throughout.
  While our main motivation is centralized (datacenter networks), we also get
the best-known distributed expander construction in the &quot;self-healing&quot; model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01206</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01206</id><created>2015-07-05</created><updated>2015-10-30</updated><authors><author><keyname>Micucci</keyname><forenames>Daniela</forenames></author><author><keyname>Mobilio</keyname><forenames>Marco</forenames></author><author><keyname>Napoletano</keyname><forenames>Paolo</forenames></author><author><keyname>Tisato</keyname><forenames>Francesco</forenames></author></authors><title>Falls as anomalies? An experimental evaluation using smartphone
  accelerometer data</title><categories>cs.SY</categories><comments>submitted to the Journal of Ambient Intelligence and Humanized
  Computing (Springer)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Life expectancy keeps growing and, among elderly people, accidental falls
occur frequently. A system able to promptly detect falls would help in reducing
the injuries that a fall could cause. Such a system should meet the needs of
the people to which is designed, so that it is actually used. In particular,
the system should be minimally invasive and inexpensive. Thanks to the fact
that most of the smartphones embed accelerometers and powerful processing unit,
they are good candidates both as data acquisition devices and as platforms to
host fall detection systems. For this reason, in the last years several fall
detection methods have been experimented on smartphone accelerometer data. Most
of them have been tuned with simulated falls because, to date, datasets of
real-world falls are not available. This article evaluates the effectiveness of
methods that detect falls as anomalies. To this end, we compared traditional
approaches with anomaly detectors. In particular, we experienced the kNN and
the SVM methods using both the one-class and two-classes configurations. The
comparison involved three different collections of accelerometer data, and four
different data representations. Empirical results demonstrated that, in most of
the cases, falls are not required to design an effective fall detector.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01208</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01208</id><created>2015-07-05</created><authors><author><keyname>Dokania</keyname><forenames>Puneet K.</forenames></author><author><keyname>Kumar</keyname><forenames>M. Pawan</forenames></author></authors><title>Parsimonious Labeling</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new family of discrete energy minimization problems, which we
call parsimonious labeling. Specifically, our energy functional consists of
unary potentials and high-order clique potentials. While the unary potentials
are arbitrary, the clique potentials are proportional to the {\em diversity} of
set of the unique labels assigned to the clique. Intuitively, our energy
functional encourages the labeling to be parsimonious, that is, use as few
labels as possible. This in turn allows us to capture useful cues for important
computer vision applications such as stereo correspondence and image denoising.
Furthermore, we propose an efficient graph-cuts based algorithm for the
parsimonious labeling problem that provides strong theoretical guarantees on
the quality of the solution. Our algorithm consists of three steps. First, we
approximate a given diversity using a mixture of a novel hierarchical $P^n$
Potts model. Second, we use a divide-and-conquer approach for each mixture
component, where each subproblem is solved using an effficient
$\alpha$-expansion algorithm. This provides us with a small number of putative
labelings, one for each mixture component. Third, we choose the best putative
labeling in terms of the energy value. Using both sythetic and standard real
datasets, we show that our algorithm significantly outperforms other graph-cuts
based approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01209</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01209</id><created>2015-07-05</created><authors><author><keyname>Kannao</keyname><forenames>Raghvendra</forenames></author><author><keyname>Guha</keyname><forenames>Prithwijit</forenames></author></authors><title>TV News Commercials Detection using Success based Locally Weighted
  Kernel Combination</title><categories>cs.CV cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Commercial detection in news broadcast videos involves judicious selection of
meaningful audio-visual feature combinations and efficient classifiers. And,
this problem becomes much simpler if these combinations can be learned from the
data. To this end, we propose an Multiple Kernel Learning based method for
boosting successful kernel functions while ignoring the irrelevant ones. We
adopt a intermediate fusion approach where, a SVM is trained with a weighted
linear combination of different kernel functions instead of single kernel
function. Each kernel function is characterized by a feature set and kernel
type. We identify the feature sub-space locations of the prediction success of
a particular classifier trained only with particular kernel function. We
propose to estimate a weighing function using support vector regression (with
RBF kernel) for each kernel function which has high values (near 1.0) where the
classifier learned on kernel function succeeded and lower values (nearly 0.0)
otherwise. Second contribution of this work is TV News Commercials Dataset of
150 Hours of News videos. Classifier trained with our proposed scheme has
outperformed the baseline methods on 6 of 8 benchmark dataset and our own TV
commercials dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01215</identifier>
 <datestamp>2015-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01215</id><created>2015-07-05</created><updated>2015-07-22</updated><authors><author><keyname>Gao</keyname><forenames>Ziyuan</forenames></author><author><keyname>Stephan</keyname><forenames>Frank</forenames></author><author><keyname>Zilles</keyname><forenames>Sandra</forenames></author></authors><title>Combining Models of Approximation with Partial Learning</title><categories>cs.LG</categories><comments>28 pages</comments><msc-class>68Q32</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Gold's framework of inductive inference, the model of partial learning
requires the learner to output exactly one correct index for the target object
and only the target object infinitely often. Since infinitely many of the
learner's hypotheses may be incorrect, it is not obvious whether a partial
learner can be modifed to &quot;approximate&quot; the target object.
  Fulk and Jain (Approximate inference and scientific method. Information and
Computation 114(2):179--191, 1994) introduced a model of approximate learning
of recursive functions. The present work extends their research and solves an
open problem of Fulk and Jain by showing that there is a learner which
approximates and partially identifies every recursive function by outputting a
sequence of hypotheses which, in addition, are also almost all finite variants
of the target function.
  The subsequent study is dedicated to the question how these findings
generalise to the learning of r.e. languages from positive data. Here three
variants of approximate learning will be introduced and investigated with
respect to the question whether they can be combined with partial learning.
Following the line of Fulk and Jain's research, further investigations provide
conditions under which partial language learners can eventually output only
finite variants of the target language. The combinabilities of other partial
learning criteria will also be briefly studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01231</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01231</id><created>2015-07-05</created><updated>2015-11-22</updated><authors><author><keyname>Kosolobov</keyname><forenames>Dmitry</forenames></author></authors><title>Computing Runs on a General Alphabet</title><categories>cs.DS</categories><comments>4 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a RAM algorithm computing all runs (maximal repetitions) of a
given string of length $n$ over a general ordered alphabet in
$O(n\log^{\frac{2}3} n)$ time and linear space. Our algorithm outperforms all
known solutions working in $\Theta(n\log\sigma)$ time provided $\sigma =
n^{\Omega(1)}$, where $\sigma$ is the alphabet size. We conjecture that there
exists a linear time RAM algorithm finding all runs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01233</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01233</id><created>2015-07-05</created><authors><author><keyname>Madadipouya</keyname><forenames>Kasra</forenames></author></authors><title>A Review On The Strategic Use Of IT Applications In Achieving And
  Sustaining Competitive Advantage</title><categories>cs.CY</categories><comments>10 pages</comments><journal-ref>International Journal of Managing Public Sector Information and
  Communication Technologies, Volume 6, Issue 2, 21-30 (2015)</journal-ref><doi>10.5121/ijmpict.2015.6202</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information Technology applications are growing significantly fast for the
past decade. Many Business organizations with emphasize on IT applications are
trying to gain competitive advantages. Empirical studies also demonstrate that
IT increases competitive advantage when it acts with human resources. In this
paper we analyzed and review how various IT applications can be utilized to to
provide a company with strategic advantages over the global marketplace
competitive forces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01234</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01234</id><created>2015-07-05</created><updated>2015-07-30</updated><authors><author><keyname>Kontoyiannis</keyname><forenames>Ioannis</forenames></author><author><keyname>Skoularidou</keyname><forenames>Maria</forenames></author></authors><title>Estimating the Directed Information and Testing for Causality</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>Minor typos corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of estimating the directed information rate between two discrete
processes $\{X_n\}$ and $\{Y_n\}$ via the plug-in (or maximum-likelihood)
estimator is considered. When the joint process $\{(X_n,Y_n)\}$ is a Markov
chain of a given memory length, the plug-in estimator is shown to be
asymptotically Gaussian and to converge at the optimal rate $O(1/\sqrt{n})$
under appropriate conditions; this is the first estimator that has been shown
to achieve this rate. An important connection is drawn between the problem of
estimating the directed information rate and that of performing a hypothesis
test for the presence of causal influence between the two processes. Under
fairly general conditions, the null hypothesis, which corresponds to the
absence of causal influence, is equivalent to the requirement that the directed
information rate be equal to zero. In that case a finer result is established,
showing that the plug-in converges at the faster rate $O(1/n)$ and that it is
asymptotically $\chi^2$-distributed. This is proved by showing that this
estimator is equal to (a scalar multiple of) the classical likelihood ratio
statistic for the above hypothesis test. Finally it is noted that these results
facilitate the design of an actual likelihood ratio test for the presence or
absence of causal influence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01238</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01238</id><created>2015-07-05</created><updated>2015-08-31</updated><authors><author><keyname>You</keyname><forenames>C.</forenames></author><author><keyname>Vidal</keyname><forenames>R.</forenames></author></authors><title>Sparse Subspace Clustering by Orthogonal Matching Pursuit</title><categories>cs.CV cs.LG stat.ML</categories><comments>13 pages, 1 figure, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Subspace clustering methods based on $\ell_1$, $\ell_2$ or nuclear norm
regularization have become very popular due to their simplicity, theoretical
guarantees and empirical success. However, the choice of the regularizer can
greatly impact both theory and practice. For instance, $\ell_1$ regularization
is guaranteed to give the correct clustering under broad conditions (e.g.,
arbitrary subspaces and corrupted data), but requires solving a large scale
convex optimization problem. On the other hand, $\ell_2$ and nuclear norm
regularization provide efficient closed form solutions, but require very strong
assumptions to guarantee the correct clustering (e.g., independent subspaces
and uncorrupted data). This paper proposes a new subspace clustering method
based on orthogonal matching pursuit that is computationally efficient and
guaranteed to provide the correct clustering for arbitrary subspaces.
Experiments on synthetic data verify our theoretical analysis, and applications
in handwritten digit and face clustering show that our approach achieves the
best trade off between accuracy and efficiency. Moreover, our approach is the
only one that can handle 100,000 points.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01239</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01239</id><created>2015-07-05</created><authors><author><keyname>Su</keyname><forenames>Hang</forenames></author><author><keyname>Chen</keyname><forenames>Haoyu</forenames></author></authors><title>Experiments on Parallel Training of Deep Neural Network using Model
  Averaging</title><categories>cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we apply model averaging to parallel training of deep neural
network (DNN). Parallelization is done in a model averaging manner. Data is
partitioned and distributed to different nodes for local model updates, and
model averaging across nodes is done every few minibatches. We use multiple
GPUs for data parallelization, and Message Passing Interface (MPI) for
communication between nodes, which allows us to perform model averaging
frequently without losing much time on communication. We investigate the
effectiveness of Natural Gradient Stochastic Gradient Descent (NG-SGD) and
Restricted Boltzmann Machine (RBM) pretraining for parallel training in
model-averaging framework, and explore the best setups in term of different
learning rate schedules, averaging frequencies and minibatch sizes. It is shown
that NG-SGD and RBM pretraining benefits parameter-averaging based model
training. On the 300h Switchboard dataset, a 9.3 times speedup is achieved
using 16 GPUs and 17 times speedup using 32 GPUs with limited decoding accuracy
loss.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01248</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01248</id><created>2015-07-05</created><updated>2015-10-28</updated><authors><author><keyname>Tan</keyname><forenames>Jin</forenames></author><author><keyname>Ma</keyname><forenames>Yanting</forenames></author><author><keyname>Rueda</keyname><forenames>Hoover</forenames></author><author><keyname>Baron</keyname><forenames>Dror</forenames></author><author><keyname>Arce</keyname><forenames>Gonzalo</forenames></author></authors><title>Compressive Hyperspectral Imaging via Approximate Message Passing</title><categories>cs.IT math.IT</categories><comments>13 pages, to appear in Journal of Selected Topics in Signal
  Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a compressive hyperspectral imaging reconstruction problem, where
three-dimensional spatio-spectral information about a scene is sensed by a
coded aperture snapshot spectral imager (CASSI). The CASSI imaging process can
be modeled as suppressing three-dimensional coded and shifted voxels and
projecting these onto a two-dimensional plane, such that the number of acquired
measurements is greatly reduced. On the other hand, because the measurements
are highly compressive, the reconstruction process becomes challenging. We
previously proposed a compressive imaging reconstruction algorithm that is
applied to two-dimensional images based on the approximate message passing
(AMP) framework. AMP is an iterative algorithm that can be used in signal and
image reconstruction by performing denoising at each iteration. We employed an
adaptive Wiener filter as the image denoiser, and called our algorithm
&quot;AMP-Wiener.&quot; In this paper, we extend AMP-Wiener to three-dimensional
hyperspectral image reconstruction, and call it &quot;AMP-3D-Wiener.&quot; Applying the
AMP framework to the CASSI system is challenging, because the matrix that
models the CASSI system is highly sparse, and such a matrix is not suitable to
AMP and makes it difficult for AMP to converge. Therefore, we modify the
adaptive Wiener filter and employ a technique called damping to solve for the
divergence issue of AMP. Our approach is applied in nature, and the numerical
experiments show that AMP-3D-Wiener outperforms existing widely-used algorithms
such as gradient projection for sparse reconstruction (GPSR) and two-step
iterative shrinkage/thresholding (TwIST) given a similar amount of runtime.
Moreover, in contrast to GPSR and TwIST, AMP-3D-Wiener need not tune any
parameters, which simplifies the reconstruction process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01251</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01251</id><created>2015-07-05</created><authors><author><keyname>Camlica</keyname><forenames>Zehra</forenames></author><author><keyname>Tizhoosh</keyname><forenames>H. R.</forenames></author><author><keyname>Khalvati</keyname><forenames>Farzad</forenames></author></authors><title>Autoencoding the Retrieval Relevance of Medical Images</title><categories>cs.CV</categories><comments>To appear in proceedings of The 5th International Conference on Image
  Processing Theory, Tools and Applications (IPTA'15), Nov 10-13, 2015,
  Orleans, France</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Content-based image retrieval (CBIR) of medical images is a crucial task that
can contribute to a more reliable diagnosis if applied to big data. Recent
advances in feature extraction and classification have enormously improved CBIR
results for digital images. However, considering the increasing accessibility
of big data in medical imaging, we are still in need of reducing both memory
requirements and computational expenses of image retrieval systems. This work
proposes to exclude the features of image blocks that exhibit a low encoding
error when learned by a $n/p/n$ autoencoder ($p\!&lt;\!n$). We examine the
histogram of autoendcoding errors of image blocks for each image class to
facilitate the decision which image regions, or roughly what percentage of an
image perhaps, shall be declared relevant for the retrieval task. This leads to
reduction of feature dimensionality and speeds up the retrieval process. To
validate the proposed scheme, we employ local binary patterns (LBP) and support
vector machines (SVM) which are both well-established approaches in CBIR
research community. As well, we use IRMA dataset with 14,410 x-ray images as
test data. The results show that the dimensionality of annotated feature
vectors can be reduced by up to 50% resulting in speedups greater than 27% at
expense of less than 1% decrease in the accuracy of retrieval when validating
the precision and recall of the top 20 hits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01255</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01255</id><created>2015-07-05</created><updated>2016-01-24</updated><authors><author><keyname>Merhav</keyname><forenames>Neri</forenames></author></authors><title>Universal Decoding for Source-Channel Coding with Side Information</title><categories>cs.IT math.IT</categories><comments>31 pages; 2 figures. Submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a setting of Slepian--Wolf coding, where the random bin of the
source vector undergoes channel coding, and then decoded at the receiver, based
on additional side information, correlated to the source. For a given
distribution of the randomly selected channel codewords, we propose a universal
decoder that depends on the statistics of neither the correlated sources nor
the channel, assuming first that they are both memoryless. Exact analysis of
the random-binning/random-coding error exponent of this universal decoder shows
that it is the same as the one achieved by the optimal maximum a-posteriori
(MAP) decoder. Previously known results on universal Slepian-Wolf source
decoding, universal channel decoding, and universal source-channel decoding,
are all obtained as special cases of this result. Subsequently, we further
generalize the results in several directions, including: (i) finite-state
sources and finite-state channels, along with a universal decoding metric that
is based on Lempel-Ziv parsing, (ii) arbitrary sources and channels, where the
universal decoding is with respect to a given class of decoding metrics, and
(iii) full (symmetric) Slepian-Wolf coding, where both source streams are
separately fed into random-binning source encoders, followed by random channel
encoders, which are then jointly decoded by a universal decoder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01265</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01265</id><created>2015-07-05</created><authors><author><keyname>Lastovetsky</keyname><forenames>Alexey</forenames></author><author><keyname>Szustak</keyname><forenames>Lukasz</forenames></author><author><keyname>Wyrzykowski</keyname><forenames>Roman</forenames></author></authors><title>Model-based optimization of MPDATA on Intel Xeon Phi through load
  imbalancing</title><categories>cs.DC</categories><comments>10 pages, 9 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Load balancing is a widely accepted technique for performance optimization of
scientific applications on parallel architectures. Indeed, balanced
applications do not waste processor cycles on waiting at points of
synchronization and data exchange, maximizing this way the utilization of
processors. In this paper, we challenge the universality of the load-balancing
approach to optimization of the performance of parallel applications. First, we
formulate conditions that should be satisfied by the performance profile of an
application in order for the application to achieve its best performance via
load balancing. Then we use a real-life scientific application, MPDATA, to
demonstrate that its performance profile on a modern parallel architecture,
Intel Xeon Phi, significantly deviates from these conditions. Based on this
observation, we propose a method of performance optimization of scientific
applications through load imbalancing. We also propose an algorithm that finds
the optimal, possibly imbalanced, configuration of a data parallel application
on a set of homogeneous processors. This algorithm uses functional performance
models of the application to find the partitioning that minimizes its
computation time but not necessarily balances the load of the processors. We
show how to apply this algorithm to optimization of MPDATA on Intel Xeon Phi.
Experimental results demonstrate that the performance of this carefully
optimized load-balanced application can be further improved by 15\% using the
proposed load-imbalancing optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01266</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01266</id><created>2015-07-05</created><updated>2016-02-20</updated><authors><author><keyname>Taylor</keyname><forenames>Dane</forenames></author><author><keyname>Myers</keyname><forenames>Sean A.</forenames></author><author><keyname>Clauset</keyname><forenames>Aaron</forenames></author><author><keyname>Porter</keyname><forenames>Mason A.</forenames></author><author><keyname>Mucha</keyname><forenames>Peter J.</forenames></author></authors><title>Eigenvector-Based Centrality Measures for Temporal Networks</title><categories>physics.soc-ph cs.SI nlin.AO physics.data-an</categories><comments>34 pages, 7 figures, and 4 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerous centrality measures have been developed to quantify the importances
of nodes in static networks, and many of them can be expressed as the leading
eigenvector of some matrix. With the increasing availability of network data
that changes in time, it is important to extend eigenvector-based centrality
measures to time-dependent networks. In this paper, we introduce a principled
generalization that is valid for any eigenvector-based centrality. We consider
a temporal network with $N$ nodes as a sequence of $T$ layers that describe the
network during different time windows, and we couple centrality matrices for
the layers into a supra-centrality matrix of size $NT \times NT$ whose dominant
eigenvector gives the centrality of each node i at each time t. We refer to
this eigenvector and its components as a joint centrality, as it reflects the
importances of both the node $i$ and the time layer $t$. We also introduce the
concepts of marginal and conditional centralities, which facilitate the study
of centrality trajectories over time. We find that the strength of coupling
between layers is important for determining multiscale properties of
centrality, such as localization phenomena and the time scale of centrality
changes. In the regime of strong coupling, we derive expressions for
time-averaged centralities, which are given by the zeroth-order terms of a
singular perturbation expansion. We also study first-order terms to obtain
first-order-mover scores, which concisely describe the magnitude of nodes'
centrality changes. As examples, we apply our method to three empirical
temporal networks: the United States Ph.D. exchange in mathematics, costarring
relationships among top-billed actors during the Golden Age of Hollywood, and
citations of decisions from the United States Supreme Court.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01269</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01269</id><created>2015-07-05</created><authors><author><keyname>Xie</keyname><forenames>Tianpei</forenames></author><author><keyname>Nasrabadi</keyname><forenames>Nasser M.</forenames></author><author><keyname>Hero</keyname><forenames>Alfred O.</forenames><suffix>III</suffix></author></authors><title>Semi-supervised Multi-sensor Classification via Consensus-based
  Multi-View Maximum Entropy Discrimination</title><categories>cs.IT cs.AI cs.LG math.IT</categories><comments>5 pages, 4 figures, Accepted in 40th IEEE International Conference on
  Acoustics, Speech and Signal Processing (ICASSP 15)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider multi-sensor classification when there is a large
number of unlabeled samples. The problem is formulated under the multi-view
learning framework and a Consensus-based Multi-View Maximum Entropy
Discrimination (CMV-MED) algorithm is proposed. By iteratively maximizing the
stochastic agreement between multiple classifiers on the unlabeled dataset, the
algorithm simultaneously learns multiple high accuracy classifiers. We
demonstrate that our proposed method can yield improved performance over
previous multi-view learning approaches by comparing performance on three real
multi-sensor data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01272</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01272</id><created>2015-07-05</created><authors><author><keyname>Kumar</keyname><forenames>Srijan</forenames></author><author><keyname>Spezzano</keyname><forenames>Francesca</forenames></author><author><keyname>Subrahmanian</keyname><forenames>V. S.</forenames></author></authors><title>VEWS: A Wikipedia Vandal Early Warning System</title><categories>cs.SI physics.soc-ph</categories><comments>To appear in Proceedings of the 21st ACM SIGKDD Conference of
  Knowledge Discovery and Data Mining (KDD 2015)</comments><acm-class>H.2.8</acm-class><doi>10.1145/2783258.2783367</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of detecting vandals on Wikipedia before any human or
known vandalism detection system reports flagging potential vandals so that
such users can be presented early to Wikipedia administrators. We leverage
multiple classical ML approaches, but develop 3 novel sets of features. Our
Wikipedia Vandal Behavior (WVB) approach uses a novel set of user editing
patterns as features to classify some users as vandals. Our Wikipedia
Transition Probability Matrix (WTPM) approach uses a set of features derived
from a transition probability matrix and then reduces it via a neural net
auto-encoder to classify some users as vandals. The VEWS approach merges the
previous two approaches. Without using any information (e.g. reverts) provided
by other users, these algorithms each have over 85% classification accuracy.
Moreover, when temporal recency is considered, accuracy goes to almost 90%. We
carry out detailed experiments on a new data set we have created consisting of
about 33K Wikipedia users (including both a black list and a white list of
editors) and containing 770K edits. We describe specific behaviors that
distinguish between vandals and non-vandals. We show that VEWS beats ClueBot NG
and STiki, the best known algorithms today for vandalism detection. Moreover,
VEWS detects far more vandals than ClueBot NG and on average, detects them 2.39
edits before ClueBot NG when both detect the vandal. However, we show that the
combination of VEWS and ClueBot NG can give a fully automated vandal early
warning system with even higher accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01273</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01273</id><created>2015-07-05</created><updated>2015-09-23</updated><authors><author><keyname>Zhang</keyname><forenames>Marvin</forenames></author><author><keyname>McCarthy</keyname><forenames>Zoe</forenames></author><author><keyname>Finn</keyname><forenames>Chelsea</forenames></author><author><keyname>Levine</keyname><forenames>Sergey</forenames></author><author><keyname>Abbeel</keyname><forenames>Pieter</forenames></author></authors><title>Learning Deep Neural Network Policies with Continuous Memory States</title><categories>cs.LG cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Policy learning for partially observed control tasks requires policies that
can remember salient information from past observations. In this paper, we
present a method for learning policies with internal memory for
high-dimensional, continuous systems, such as robotic manipulators. Our
approach consists of augmenting the state and action space of the system with
continuous-valued memory states that the policy can read from and write to.
Learning general-purpose policies with this type of memory representation
directly is difficult, because the policy must automatically figure out the
most salient information to memorize at each time step. We show that, by
decomposing this policy search problem into a trajectory optimization phase and
a supervised learning phase through a method called guided policy search, we
can acquire policies with effective memorization and recall strategies.
Intuitively, the trajectory optimization phase chooses the values of the memory
states that will make it easier for the policy to produce the right action in
future states, while the supervised learning phase encourages the policy to use
memorization actions to produce those memory states. We evaluate our method on
tasks involving continuous control in manipulation and navigation settings, and
show that our method can learn complex policies that successfully complete a
range of tasks that require memory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01279</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01279</id><created>2015-07-05</created><updated>2016-02-19</updated><authors><author><keyname>Li</keyname><forenames>Shuang</forenames></author><author><keyname>Xie</keyname><forenames>Yao</forenames></author><author><keyname>Dai</keyname><forenames>Hanjun</forenames></author><author><keyname>Song</keyname><forenames>Le</forenames></author></authors><title>$M$-Statistic for Kernel Change-Point Detection</title><categories>cs.LG math.ST stat.ML stat.TH</categories><comments>Submitted for journal publication. Partial results appeared in NIPS
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detecting the emergence of an abrupt change-point is a classic problem in
statistics and machine learning. Kernel-based nonparametric statistics have
been proposed for this task which make fewer assumptions on the distributions
than traditional parametric approach. However, none of the existing kernel
statistics has provided a computationally efficient way to characterize the
extremal behavior of the statistic. Such characterization is crucial for
setting the detection threshold, to control the significance level in the
offline case as well as the false alarm rate (captured by the average run
length) in the online case. In this paper we focus on the scenario when the
amount of background data is large, and propose two related computationally
efficient kernel-based statistics for change-point detection, which we call
&quot;$M$-statistics&quot;. A novel theoretical result of the paper is the
characterization of the tail probability of these statistics using a new
technique based on change-of-measure. Such characterization provides us
accurate detection thresholds for both offline and online cases in
computationally efficient manner, without the need to resort to the more
expensive simulations such as bootstrapping. Moreover, our $M$-statistic can be
applied to high-dimensional data by choosing a proper kernel. We show that our
methods perform well in both synthetic and real world data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01282</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01282</id><created>2015-07-05</created><authors><author><keyname>Monroy-Hern&#xe1;ndez</keyname><forenames>Andr&#xe9;s</forenames></author><author><keyname>Resnick</keyname><forenames>Mitchel</forenames></author></authors><title>Empowering Kids to Create and Share Programmable Media</title><categories>cs.CY cs.HC cs.SI</categories><comments>FEATURE: Empowering kids to create and share programmable media.
  interactions. issue 15, volume 2 (March 2008), pages 50-53</comments><journal-ref>Interactions 15, 2 (March 2008), 50-53</journal-ref><doi>10.1145/1340961.1340974</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This article reflects on the first eight months of existence of the Scratch
Online Community by discussing the design rationale and learning theories
underlying Scratch and its website.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01284</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01284</id><created>2015-07-05</created><authors><author><keyname>Hill</keyname><forenames>Benjamin Mako</forenames></author><author><keyname>Monroy-Hern&#xe1;ndez</keyname><forenames>Andr&#xe9;s</forenames></author><author><keyname>Olson</keyname><forenames>Kristina R.</forenames></author></authors><title>Responses to remixing on a social media sharing website</title><categories>cs.HC cs.CY cs.SI</categories><comments>In Proceedings of the Fourth International AAAI Conference on Weblogs
  and Social Media (ICWSM 2010)</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  In this paper we describe the ways participants of the Scratch online
community, primarily young people, engage in remixing of each others' shared
animations, games, and interactive projects. In particular, we try to answer
the following questions: How do users respond to remixing in a social media
environment where remixing is explicitly permitted? What qualities of
originators and their projects correspond to a higher likelihood of plagiarism
accusations? Is there a connection between plagiarism complaints and
similarities between a remix and the work it is based on? Our findings indicate
that users have a very wide range of reactions to remixing and that as many
users react positively as accuse remixers of plagiarism. We test several
hypotheses that might explain the high number of plagiarism accusations related
to original project complexity, cumulative remixing, originators' integration
into remixing practice, and remixee-remixer project similarity, and find
support for the first and last explanations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01285</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01285</id><created>2015-07-05</created><authors><author><keyname>Monroy-Hern&#xe1;ndez</keyname><forenames>Andr&#xe9;s</forenames></author><author><keyname>Hill</keyname><forenames>Benjamin Mako</forenames></author><author><keyname>Gonzalez-Rivero</keyname><forenames>Jazmin</forenames></author><author><keyname>Boyd</keyname><forenames>Danah</forenames></author></authors><title>Computers Can't Give Credit: How Automatic Attribution Falls Short in an
  Online Remixing Community</title><categories>cs.HC cs.CY</categories><comments>Proceedings of the SIGCHI Conference on Human Factors in Computing
  Systems (CHI 2011), Best paper honorable mention, 10 pages</comments><doi>10.1145/1978942.1979452</doi><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  In this paper, we explore the role that attribution plays in shaping user
reactions to content reuse, or remixing, in a large user-generated content
community. We present two studies using data from the Scratch online community
-- a social media platform where hundreds of thousands of young people share
and remix animations and video games. First, we present a quantitative analysis
that examines the effects of a technological design intervention introducing
automated attribution of remixes on users' reactions to being remixed. We
compare this analysis to a parallel examination of &quot;manual&quot; credit-giving.
Second, we present a qualitative analysis of twelve in-depth, semi-structured,
interviews with Scratch participants on the subject of remixing and
attribution. Results from both studies suggest that automatic attribution done
by technological systems (i.e., the listing of names of contributors) plays a
role that is distinct from, and less valuable than, credit which may
superficially involve identical information but takes on new meaning when it is
given by a human remixer. We discuss the implications of these findings for the
designers of online communities and social media platforms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01287</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01287</id><created>2015-07-05</created><authors><author><keyname>De Choudhury</keyname><forenames>Munmun</forenames></author><author><keyname>Monroy-Hern&#xe1;ndez</keyname><forenames>Andr&#xe9;s</forenames></author><author><keyname>Mark</keyname><forenames>Gloria</forenames></author></authors><title>&quot;Narco&quot; Emotions: Affect and Desensitization in Social Media during the
  Mexican Drug War</title><categories>cs.CY cs.HC cs.SI</categories><comments>Best paper award at the 32nd annual ACM conference on Human factors
  in computing systems (CHI '14). ACM, New York, NY, USA, pages 3563-3572</comments><journal-ref>In Proceedings of the 32nd annual ACM conference on Human factors
  in computing systems (CHI 2014). ACM, New York, NY, USA, pages 3563-3572</journal-ref><doi>10.1145/2556288.2557197</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social media platforms have emerged as prominent information sharing
ecosystems in the context of a variety of recent crises, ranging from mass
emergencies, to wars and political conflicts. We study affective responses in
social media and how they might indicate desensitization to violence
experienced in communities embroiled in an armed conflict. Specifically, we
examine three established affect measures: negative affect, activation, and
dominance as observed on Twitter in relation to a number of statistics on
protracted violence in four major cities afflicted by the Mexican Drug War.
During a two year period (Aug 2010-Dec 2012), while violence was on the rise in
these regions, our findings show a decline in negative emotional expression as
well as a rise in emotional arousal and dominance in Twitter posts: aspects
known to be psychological markers of desensitization. We discuss the
implications of our work for behavioral health, facilitating rehabilitation
efforts in communities enmeshed in an acute and persistent urban warfare, and
the impact on civic engagement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01290</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01290</id><created>2015-07-05</created><authors><author><keyname>Monroy-Hern&#xe1;ndez</keyname><forenames>Andr&#xe9;s</forenames></author><author><keyname>Kiciman</keyname><forenames>Emre</forenames></author><author><keyname>Boyd</keyname><forenames>Danah</forenames></author><author><keyname>Counts</keyname><forenames>Scott</forenames></author></authors><title>Narcotweets: Social Media in Wartime</title><categories>cs.CY cs.SI</categories><comments>In Proceedings of the 2012 International AAAI Conference on Weblogs
  and Social Media</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper describes how people living in armed conflict environments use
social media as a participatory news platform, in lieu of damaged state and
media apparatuses. We investigate this by analyzing the microblogging practices
of Mexican citizens whose everyday life is affected by the Drug War. We provide
a descriptive analysis of the phenomenon, combining content and quantitative
Twitter data analyses. We focus on three interrelated phenomena: general
participation patterns of ordinary citizens, the emergence and role of
information curators, and the tension between governmental regulation and drug
cartel intimidation. This study reveals the complex tensions among citizens,
media actors, and the government in light of large scale organized crime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01291</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01291</id><created>2015-07-05</created><authors><author><keyname>Monroy-Hern&#xe1;ndez</keyname><forenames>Andr&#xe9;s</forenames></author><author><keyname>boyd</keyname><forenames>danah</forenames></author><author><keyname>Kiciman</keyname><forenames>Emre</forenames></author><author><keyname>De Choudhury</keyname><forenames>Munmun</forenames></author><author><keyname>Counts</keyname><forenames>Scott</forenames></author></authors><title>The New War Correspondents: the Rise of Civic Media Curation in Urban
  Warfare</title><categories>cs.CY cs.HC cs.SI</categories><comments>In Proceedings of the 2013 conference on Computer supported
  cooperative work (CSCW 2013). ACM, New York, NY, USA, 1443-1452</comments><doi>10.1145/2441776.2441938</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper we examine the information sharing practices of people living
in cities amid armed conflict. We describe the volume and frequency of
microblogging activity on Twitter from four cities afflicted by the Mexican
Drug War, showing how citizens use social media to alert one another and to
comment on the violence that plagues their communities. We then investigate the
emergence of civic media &quot;curators,&quot; individuals who act as &quot;war
correspondents&quot; by aggregating and disseminating information to large numbers
of people on social media. We conclude by outlining the implications of our
observations for the design of civic media systems in wartime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01292</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01292</id><created>2015-07-05</created><authors><author><keyname>Monroy-Hern&#xe1;ndez</keyname><forenames>Andr&#xe9;s</forenames></author></authors><title>ScratchR: Sharing User-generated Programmable Media</title><categories>cs.HC cs.CY cs.SI</categories><comments>In Proceedings of the 6th international conference on Interaction
  Design and Children (IDC 2007). ACM, New York, NY, USA, 167-168</comments><doi>10.1145/1297277.1297315</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, I describe a platform for sharing programmable media on the
web called ScratchR. As the backbone of an on-line community of creative
learners, ScratchR will give members access to an audience and inspirational
ideas from each other. ScratchR seeks to support different states of
participation: from passive consumption to active creation. This platform is
being evaluated with a group of middle-school students and a larger community
of beta testers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01295</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01295</id><created>2015-07-05</created><authors><author><keyname>Hill</keyname><forenames>Benjamin Mako</forenames></author><author><keyname>Monroy-Hern&#xe1;ndez</keyname><forenames>Andr&#xe9;s</forenames></author></authors><title>The Remixing Dilemma: The Trade-off Between Generativity and Originality</title><categories>cs.CY cs.SI</categories><comments>American Behavioral Scientist (2012)</comments><doi>10.1177/0002764212469359</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper we argue that there is a trade-off between generativity and
originality in online communities that support open collaboration. We build on
foundational theoretical work in peer production to formulate and test a series
of hypotheses suggesting that the generativity of creative works is associated
with moderate complexity, prominent authors, and cumulativeness. We also
formulate and test three hypotheses that these qualities are associated with
decreased originality in resulting derivatives. Our analysis uses a rich data
set from the Scratch Online Community --a large web-site where young people
openly share and remix animations and video games. We discuss the implications
of this trade-off for the design of peer production systems that support
amateur creativity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01297</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01297</id><created>2015-07-05</created><authors><author><keyname>Hill</keyname><forenames>Benjamin Mako</forenames></author><author><keyname>Monroy-Hern&#xe1;ndez</keyname><forenames>Andr&#xe9;s</forenames></author></authors><title>The Cost of Collaboration for Code and Art: Evidence from a Remixing
  Community</title><categories>cs.CY cs.HC cs.SI</categories><comments>Best paper award at CSCW, In Proceedings of the 2013 conference on
  Computer supported cooperative work (CSCW 2013). ACM</comments><doi>10.1145/2441776.2441893</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we use evidence from a remixing community to evaluate two
pieces of common wisdom about collaboration. First, we test the theory that
jointly produced works tend to be of higher quality than individually authored
products. Second, we test the theory that collaboration improves the quality of
functional works like code, but that it works less well for artistic works like
images and sounds. We use data from Scratch, a large online community where
hundreds of thousands of young users share and remix millions of animations and
interactive games. Using peer-ratings as a measure of quality, we estimate a
series of fitted regression models and find that collaborative Scratch projects
tend to receive ratings that are lower than individually authored works. We
also find that code-intensive collaborations are rated higher than
media-intensive efforts. We conclude by discussing the limitations and
implications of these findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01299</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01299</id><created>2015-07-05</created><authors><author><keyname>Matias</keyname><forenames>J. Nathan</forenames></author><author><keyname>Monroy-Hern&#xe1;ndez</keyname><forenames>Andr&#xe9;s</forenames></author></authors><title>NewsPad: Designing for Collaborative Storytelling in Neighborhoods</title><categories>cs.HC cs.CY</categories><comments>NewsPad: designing for collaborative storytelling in neighborhoods.
  In Proceedings of the extended abstracts of the 32nd annual ACM conference on
  Human factors in computing systems (CHI EA 2014)</comments><doi>10.1145/2559206.2581354</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper introduces design explorations in neighborhood collaborative
storytelling. We focus on blogs and citizen journalism, which have been
celebrated as a means to meet the reporting needs of small local communities.
These bloggers have limited capacity and social media feeds seldom have the
context or readability of news stories. We present NewsPad, a content editor
that helps communities create structured stories, collaborate in real time,
recruit contributors, and syndicate the editing process. We evaluate NewsPad in
four pilot deployments and find that the design elicits collaborative story
creation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01300</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01300</id><created>2015-07-05</created><authors><author><keyname>Agapie</keyname><forenames>Elena</forenames></author><author><keyname>Monroy-Hern&#xe1;ndez</keyname><forenames>Andr&#xe9;s</forenames></author></authors><title>Eventful: Crowdsourcing Local News Reporting</title><categories>cs.HC cs.CY</categories><comments>Collective Intelligence Conference 2014, Boston, MA</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present Eventful, a system for producing news reports of local events
using remote and locative crowd workers. The system recruits and guides novice
crowd workers as they perform the roles of field reporter, curator, or writer.
Field reporters attend the events in person, and use Eventful's mobile web app
to get a personalized mission, submit content, and receive feedback. Missions
include tasks such as taking a photo, and asking a question to an attendee. In
parallel, remote curators approve, reject, and give real-time feedback on the
content collected by field reporters. Finally, writers put together a report by
mashing up and tweaking the content approved by the curators. We used Eventful
to produce a news report for each of the six local events we decided to cover
as we piloted the system. The process was typically completed under an hour and
costing under $150 USD.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01305</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01305</id><created>2015-07-05</created><authors><author><keyname>Hallacher</keyname><forenames>Sarah</forenames></author><author><keyname>Rodenhouse</keyname><forenames>Jenny</forenames></author><author><keyname>Monroy-Hernandez</keyname><forenames>Andres</forenames></author></authors><title>Mixsourcing: a remix framework as a form of crowdsourcing</title><categories>cs.HC cs.SI</categories><comments>In CHI 2013 Extended Abstracts on Human Factors in Computing Systems
  (CHI EA '13)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we introduce the concept of mixsourcing as a modality of
crowdsourcing focused on using remixing as a framework to get people to perform
creative tasks. We explore this idea through the design of a system that helped
us identify the promises and challenges of this peer-production modality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01307</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01307</id><created>2015-07-05</created><authors><author><keyname>You</keyname><forenames>C.</forenames></author><author><keyname>Vidal</keyname><forenames>R.</forenames></author></authors><title>Subspace-Sparse Representation</title><categories>stat.ML cs.IT math.IT</categories><comments>15 pages, 3 figures, previous version published in ICML 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an overcomplete dictionary $A$ and a signal $b$ that is a linear
combination of a few linearly independent columns of $A$, classical sparse
recovery theory deals with the problem of recovering the unique sparse
representation $x$ such that $b = A x$. It is known that under certain
conditions on $A$, $x$ can be recovered by the Basis Pursuit (BP) and the
Orthogonal Matching Pursuit (OMP) algorithms. In this work, we consider the
more general case where $b$ lies in a low-dimensional subspace spanned by some
columns of $A$, which are possibly linearly dependent. In this case, the
sparsest solution $x$ is generally not unique, and we study the problem that
the representation $x$ identifies the subspace, i.e. the nonzero entries of $x$
correspond to dictionary atoms that are in the subspace. Such a representation
$x$ is called subspace-sparse. We present sufficient conditions for
guaranteeing subspace-sparse recovery, which have clear geometric
interpretations and explain properties of subspace-sparse recovery. We also
show that the sufficient conditions can be satisfied under a randomized model.
Our results are applicable to the traditional sparse recovery problem and we
get conditions for sparse recovery that are less restrictive than the canonical
mutual coherent condition. We also use the results to analyze the sparse
representation based classification (SRC) method, for which we get conditions
to show its correctness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01308</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01308</id><created>2015-07-05</created><updated>2015-12-23</updated><authors><author><keyname>Li</keyname><forenames>Yanjun</forenames></author><author><keyname>Lee</keyname><forenames>Kiryung</forenames></author><author><keyname>Bresler</keyname><forenames>Yoram</forenames></author></authors><title>Identifiability and Stability in Blind Deconvolution under Minimal
  Assumptions</title><categories>cs.IT math.IT</categories><comments>32 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Blind deconvolution (BD) arises in many applications. Without assumptions on
the signal and the filter, BD does not admit a unique solution. In practice,
subspace or sparsity assumptions have shown the ability to reduce the search
space and yield the unique solution. However, existing theoretical analysis on
uniqueness in BD is rather limited. In an earlier paper, we provided the first
algebraic sample complexities for BD that hold for almost all bases or frames.
We showed that for BD of a pair of vectors in $\mathbb{C}^n$, with subspace
constraints of dimensions $m_1$ and $m_2$, respectively, a sample complexity of
$n\geq m_1m_2$ is sufficient. This result is suboptimal, since the number of
degrees of freedom is merely $m_1+m_2-1$. We provided analogus results, with
similar suboptimality, for BD with sparsity or mixed subspace and sparsity
constraints. In this paper, taking advantage of the recent progress on the
information-theoretic limits of unique low-rank matrix recovery, we finally
bridge this gap, and derive an optimal sample complexity result for BD with
generic bases or frames. We show that for BD of an arbitrary pair (resp. all
pairs) of vectors in $\mathbb{C}^n$, with sparsity constraints of sparsity
levels $s_1$ and $s_2$, a sample complexity of $n &gt; s_1+s_2$ (resp. $n &gt;
2(s_1+s_2)$) is sufficient. We also present analogous results for BD with
subspace constraints or mixed constraints, with the subspace dimension
replacing the sparsity level. Last but not least, in all the above scenarios,
if the bases or frames follow a probabilistic distribution specified in the
paper, the recovery is not only unique, but also stable against small
perturbations in the measurements, under the same sample complexities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01309</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01309</id><created>2015-07-05</created><updated>2015-08-29</updated><authors><author><keyname>Cheriyan</keyname><forenames>Joe</forenames></author><author><keyname>Gao</keyname><forenames>Zhihan</forenames></author></authors><title>Approximating (Unweighted) Tree Augmentation via Lift-and-Project, Part
  II</title><categories>cs.DS</categories><comments>36 pages, 13 figures. Minor revisions to first draft of July 5. Added
  reference [10], revised Introduction (paragraphs 3,4), added some content at
  the end (last 2 pages, Section 8) including Corollary 8.10, Theorem 8.11</comments><msc-class>68W25, 90C22, 90C27, 90C35, 05C85, 05C40,</msc-class><acm-class>F.2.2; G.1.6; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Part II, we study the unweighted Tree Augmentation Problem (TAP) via the
Lasserre (Sum~of~Squares) system. We prove that the integrality ratio of an SDP
relaxation (the Lasserre tightening of an LP relaxation) is $\leq
\frac{3}{2}+\epsilon$, where $\epsilon&gt;0$ can be any small constant. We obtain
this result by designing a polynomial-time algorithm for TAP that achieves an
approximation guarantee of ($\frac32+\epsilon$) relative to the SDP relaxation.
The algorithm is combinatorial and does not solve the SDP relaxation, but our
analysis relies on the SDP relaxation.
  We generalize the combinatorial analysis of integral solutions from the
previous literature to fractional solutions by identifying some properties of
fractional solutions of the Lasserre system via the decomposition result of
Karlin, Mathieu and Nguyen (IPCO 2011).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01311</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01311</id><created>2015-07-05</created><authors><author><keyname>Rzeszotarski</keyname><forenames>Jeffrey M.</forenames></author><author><keyname>Spiro</keyname><forenames>Emma S.</forenames></author><author><keyname>Matias</keyname><forenames>Jorge Nathan</forenames></author><author><keyname>Monroy-Hern&#xe1;ndez</keyname><forenames>Andr&#xe9;s</forenames></author><author><keyname>Morris</keyname><forenames>Meredith Ringel</forenames></author></authors><title>Is Anyone Out There? Unpacking Q&amp;A Hashtags on Twitter</title><categories>cs.SI cs.CY</categories><journal-ref>In Proceedings of the SIGCHI Conference on Human Factors in
  Computing Systems (CHI 2014). ACM, New York, NY, USA, 2755-2758</journal-ref><doi>10.1145/2556288.2557175</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In addition to posting news and status updates, many Twitter users post
questions that seek various types of subjective and objective information.
These questions are often labeled with &quot;Q&amp;A&quot; hashtags, such as #lazyweb or
#twoogle. We surveyed Twitter users and found they employ these Q&amp;A hashtags
both as a topical signifier (this tweet needs an answer!) and to reach out to
those beyond their immediate followers (a community of helpful tweeters who
monitor the hashtag). However, our log analysis of thousands of hashtagged Q&amp;A
exchanges reveals that nearly all replies to hashtagged questions come from a
user's immediate follower network, contradicting user's beliefs that they are
tapping into a larger community by tagging their question tweets. This finding
has implications for designing next-generation social search systems that reach
and engage a wide audience of answerers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01313</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01313</id><created>2015-07-05</created><authors><author><keyname>Breitwieser</keyname><forenames>Christian</forenames></author></authors><title>TiD -- Documentation of TOBI Interface D</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This document contains the documentation of TOBI (Tools for BCI) Interface D
(TiD). TiD tries to establish a standardized interface for event transmission
in neuroscience experiments. It is designed in a client-server architecture.
Clients are connecting to a single server and events are delivered in a
&quot;bus-like&quot; manner. TiD events are based on XML messages. So TiD messages can
also get custom extended. A cross platform C++ library is available and
provides all TiD functionality. To avoid jitter effect, hampering event
processing, TiD was optimized towards performance. The documentation provides
further performance tweaks to decrease TiD message processing latency and also
decrease event timing jitter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01314</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01314</id><created>2015-07-05</created><authors><author><keyname>Glassman</keyname><forenames>Elena L.</forenames></author><author><keyname>Kim</keyname><forenames>Juho</forenames></author><author><keyname>Monroy-Hern&#xe1;ndez</keyname><forenames>Andr&#xe9;s</forenames></author><author><keyname>Morris</keyname><forenames>Meredith Ringel</forenames></author></authors><title>Mudslide: A Spatially Anchored Census of Student Confusion for Online
  Lecture Videos</title><categories>cs.CY cs.HC</categories><comments>Best paper honorable mention</comments><journal-ref>In Proceedings of the 33rd Annual ACM Conference on Human Factors
  in Computing Systems (CHI 2015). ACM, New York, NY, USA, 1555-1564</journal-ref><doi>10.1145/2702123.2702304</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Educators have developed an effective technique to get feedback after
in-person lectures, called &quot;muddy card.&quot; Students are given time to reflect and
write the &quot;muddiest&quot; (least clear) point on an index card, to hand in as they
leave class. This practice of assigning end-of-lecture reflection tasks to
generate explicit student feedback is well suited for adaptation to the
challenge of supporting feedback in online video lectures. We describe the
design and evaluation of Mudslide, a prototype system that translates the
practice of muddy cards into the realm of online lecture videos. Based on an
in-lab study of students and teachers, we find that spatially contextualizing
students' muddy point feedback with respect to particular lecture slides is
advantageous to both students and teachers. We also reflect on further
opportunities for enhancing this feedback method based on teachers' and
students' experiences with our prototype.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01316</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01316</id><created>2015-07-05</created><authors><author><keyname>Zhang</keyname><forenames>Tian</forenames></author></authors><title>Delay-aware data transmission of multi-carrier communications in the
  presence of renewable energy</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the paper, we investigate the delay-aware data transmission in renewable
energy aided multi-carrier system. Besides utilizing the local renewables, the
transmitter can also purchase grid power. By scheduling the amount of
transmitted data (The data are stored in a buffer before transmission), the
sub-carrier allocation, and the renewable allocation in each transmission
period, the transmitter aims to minimize the purchasing cost under a buffer
delay constraint. By theoretical analysis of the formulated stochastic
optimization problem, we find that transmit the scheduled data through the
subcarrier with best condition is optimal and greedy renewable energy is
approximately optimal. Furthermore, based on the theoretical derives and
Lyapunov optimization, an on-line algorithm, which does NOT require future
information, is proposed. Numerical results illustrate the delay and cost
performance of the proposed algorithm. In addition, the comparisons with the
delay-optimal policy and cost-optimal policy are carried out.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01318</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01318</id><created>2015-07-05</created><authors><author><keyname>Kim</keyname><forenames>Juho</forenames></author><author><keyname>Glassman</keyname><forenames>Elena L.</forenames></author><author><keyname>Monroy-Hern&#xe1;ndez</keyname><forenames>Andr&#xe9;s</forenames></author><author><keyname>Morris</keyname><forenames>Meredith Ringel</forenames></author></authors><title>RIMES: Embedding Interactive Multimedia Exercises in Lecture Videos</title><categories>cs.CY cs.HC</categories><journal-ref>In Proceedings of the 33rd Annual ACM Conference on Human Factors
  in Computing Systems (CHI 2015). ACM, New York, NY, USA, 1535-1544</journal-ref><doi>10.1145/2702123.2702186</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Teachers in conventional classrooms often ask learners to express themselves
and show their thought processes by speaking out loud, drawing on a whiteboard,
or even using physical objects. Despite the pedagogical value of such
activities, interactive exercises available in most online learning platforms
are constrained to multiple-choice and short answer questions. We introduce
RIMES, a system for easily authoring, recording, and reviewing interactive
multimedia exercises embedded in lecture videos. With RIMES, teachers can
prompt learners to record their responses to an activity using video, audio,
and inking while watching lecture videos. Teachers can then review and interact
with all the learners' responses in an aggregated gallery. We evaluated RIMES
with 19 teachers and 25 students. Teachers created a diverse set of activities
across multiple subjects that tested deep conceptual and procedural knowledge.
Teachers found the exercises useful for capturing students' thought processes,
identifying misconceptions, and engaging students with content.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01321</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01321</id><created>2015-07-05</created><authors><author><keyname>Yusuf</keyname><forenames>Iman I.</forenames></author><author><keyname>Thomas</keyname><forenames>Ian E.</forenames></author><author><keyname>Spichkova</keyname><forenames>Maria</forenames></author><author><keyname>Androulakis</keyname><forenames>Steve</forenames></author><author><keyname>Meyer</keyname><forenames>Grischa R.</forenames></author><author><keyname>Drumm</keyname><forenames>Daniel W.</forenames></author><author><keyname>Opletal</keyname><forenames>George</forenames></author><author><keyname>Russo</keyname><forenames>Salvy P.</forenames></author><author><keyname>Buckle</keyname><forenames>Ashley M.</forenames></author><author><keyname>Schmidt</keyname><forenames>Heinz W.</forenames></author></authors><title>Chiminey: Reliable Computing and Data Management Platform in the Cloud</title><categories>cs.SE cs.DC</categories><comments>Preprint, ICSE 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The enabling of scientific experiments that are embarrassingly parallel, long
running and data-intensive into a cloud-based execution environment is a
desirable, though complex undertaking for many researchers. The management of
such virtual environments is cumbersome and not necessarily within the core
skill set for scientists and engineers. We present here Chiminey, a software
platform that enables researchers to (i) run applications on both traditional
high-performance computing and cloud-based computing infrastructures, (ii)
handle failure during execution, (iii) curate and visualise execution outputs,
(iv) share such data with collaborators or the public, and (v) search for
publicly available data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01328</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01328</id><created>2015-07-06</created><authors><author><keyname>Priem</keyname><forenames>Jason</forenames></author></authors><title>Altmetrics (Chapter from Beyond Bibliometrics: Harnessing
  Multidimensional Indicators of Scholarly Impact)</title><categories>cs.DL</categories><comments>Published in Cronin, B., &amp; Sugimoto, C. R. (2014). Beyond
  Bibliometrics: Harnessing Multidimensional Indicators of Scholarly Impact (1
  edition). Cambridge, Massachusetts: The MIT Press.
  https://mitpress.mit.edu/books/beyond-bibliometrics</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This chapter discusses altmetrics (short for &quot;alternative metrics&quot;), an
approach to uncovering previously-invisible traces of scholarly impact by
observing activity in online tools and systems. I argue that citations, while
useful, miss many important kinds of impacts, and that the increasing scholarly
use of online tools like Mendeley, Twitter, and blogs may allow us to measure
these hidden impacts. Next, I define altmetrics and discuss research on
altmetric sources--both research mapping the growth of these sources, and
scientometric research measuring activity on them. Following a discussion of
the potential uses of altmetrics, I consider the limitations of altmetrics and
recommend areas ripe for future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01330</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01330</id><created>2015-07-06</created><authors><author><keyname>Guo</keyname><forenames>Xiaojie</forenames></author></authors><title>Visual Data Deblocking using Structural Layer Priors</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The blocking artifact frequently appears in compressed real-world images or
video sequences, especially coded at low bit rates, which is visually annoying
and likely hurts the performance of many computer vision algorithms. A
compressed frame can be viewed as the superimposition of an intrinsic layer and
an artifact one. Recovering the two layers from such frames seems to be a
severely ill-posed problem since the number of unknowns to recover is twice as
many as the given measurements. In this paper, we propose a simple and robust
method to separate these two layers, which exploits structural layer priors
including the gradient sparsity of the intrinsic layer, and the independence of
the gradient fields of the two layers. A novel Augmented Lagrangian Multiplier
based algorithm is designed to efficiently and effectively solve the recovery
problem. Extensive experimental results demonstrate the superior performance of
our method over the state of the arts, in terms of visual quality and
simplicity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01338</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01338</id><created>2015-07-06</created><authors><author><keyname>Yan</keyname><forenames>Bowen</forenames></author><author><keyname>Luo</keyname><forenames>Jianxi</forenames></author></authors><title>Filtered Patent Maps for Predicting Diversification Paths of Inventors
  and Organizations</title><categories>cs.SI cs.IR physics.soc-ph</categories><comments>5 tables, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a patent technology network map, almost all pairs of technology classes
are connected, whereas most of the connections are extremely weak. This
observation suggests the need and also the possibility to filter the network
map by removing the negligible and noisy links. But link removal may reduce the
power of the network for predicting the cross-field patent portfolio
diversification of inventors and inventing organizations. This paper proposes a
metric for such predictive power of a patent network, and a method that allows
one to objectively choose a best tradeoff between predictive power and the
removal of weak links. We show the results that identify filtered networks
below the optimal tradeoff, and also remove a degree of arbitrariness compared
with other filtering treatments from the literature. On that basis, we further
demonstrate the use of filtered technology maps to visualize and analyze the
main paths of patent portfolio diversification of a prolific inventor (Leonard
Forbes) and a technology company (Google Inc.).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01345</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01345</id><created>2015-07-06</created><authors><author><keyname>Deng</keyname><forenames>Zhi-Hong</forenames></author></authors><title>DiffNodesets: An Efficient Structure for Fast Mining Frequent Itemsets</title><categories>cs.DS cs.DB</categories><comments>22 pages, 13 figures</comments><journal-ref>Applied Soft Computing. 41 (2016) 214-223</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mining frequent itemsets is an essential problem in data mining and plays an
important role in many data mining applications. In recent years, some itemset
representations based on node sets have been proposed, which have shown to be
very efficient for mining frequent itemsets. In this paper, we propose
DiffNodeset, a novel and more efficient itemset representation, for mining
frequent itemsets. Based on the DiffNodeset structure, we present an efficient
algorithm, named dFIN, to mining frequent itemsets. To achieve high efficiency,
dFIN finds frequent itemsets using a set-enumeration tree with a hybrid search
strategy and directly enumerates frequent itemsets without candidate generation
under some case. For evaluating the performance of dFIN, we have conduct
extensive experiments to compare it against with existing leading algorithms on
a variety of real and synthetic datasets. The experimental results show that
dFIN is significantly faster than these leading algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01350</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01350</id><created>2015-07-06</created><updated>2015-08-02</updated><authors><author><keyname>Roy</keyname><forenames>Shounak</forenames></author><author><keyname>Srinivasa</keyname><forenames>Shayan G.</forenames></author></authors><title>Two-dimensional Burst Error Correcting Code using Finite Field Fourier
  Transforms</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct two-dimensional codes for correcting burst errors using the
finite field Fourier transform. The encoding procedure is performed in the
transformed domain using the conjugacy property of the finite field Fourier
transform. The decoding procedure is also done in the transformed domain. Our
code is capable of correcting multiple non-overlapping occurrence of different
error patterns from a finite set of predefined error patterns. The code
construction is useful for encoding data in two dimensions for application in
data storage and bar codes
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01353</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01353</id><created>2015-07-06</created><updated>2015-09-20</updated><authors><author><keyname>Kim</keyname><forenames>Anthony</forenames></author></authors><title>Welfare Maximization with Deferred Acceptance Auctions in Reallocation
  Problems</title><categories>cs.GT</categories><comments>A short version to appear in the 23rd European Symposium on
  Algorithms (ESA 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We design approximate weakly group strategy-proof mechanisms for resource
reallocation problems using Milgrom and Segal's deferred acceptance auction
framework: the radio spectrum and network bandwidth reallocation problems in
the procurement auction setting and the cost minimization problem with set
cover constraints in the selling auction setting. Our deferred acceptance
auctions are derived from simple greedy algorithms for the underlying
optimization problems and guarantee approximately optimal social welfare (cost)
of the agents retaining their rights (contracts). In the reallocation problems,
we design procurement auctions to purchase agents' broadcast/access rights to
free up some of the resources such that the unpurchased rights can still be
exercised with respect to the remaining resources. In the cost minimization
problem, we design a selling auction to sell early termination rights to agents
with existing contracts such that some minimal constraints are still satisfied
with remaining contracts. In these problems, while the &quot;allocated&quot; agents
transact, exchanging rights and payments, the objective and feasibility
constraints are on the &quot;rejected&quot; agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01368</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01368</id><created>2015-07-06</created><authors><author><keyname>Gupta</keyname><forenames>Rakhi Misuriya</forenames></author></authors><title>Intelligent Data in the context of the internet-of-things</title><categories>cs.CY</categories><comments>9 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advent of the Internet-of-Things will allow us to optimize equipment and
resource usage, enabling increased efficiencies in automation and enabling new
and more cost efficient business model. As tremendous growth opportunities
emerge, so do the challenges such as diverse devices spanning across multiple
networks, the need to manage the exponential growth of sensor generated data
and to make sense of the huge influx of data in meaningful ways. The multitude
of diversity can best be addressed by fundamentally opening up systems,
architecture and applications. To go the next step and truly exploit the value
of the sensor data would further require real-time analytics to gain
intelligence and respond to events as they happen. Historical analysis can be
used to look for trends, analyze collections of sensor data for correlation and
formulate hints and suggestions based on usage and patterns. In this paper, we
present a framework that overcomes diversity through its ability to flexibly
represent sensor data on the internet. Business goals-driven information
processing, information derived intelligence and information control as
elements of the framework can further enable creation of new and innovative
applications that enhance and exploit the value of Internet-of-Things.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01380</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01380</id><created>2015-07-06</created><authors><author><keyname>Min</keyname><forenames>Byungjoon</forenames></author><author><keyname>Liljeros</keyname><forenames>Fredrik</forenames></author><author><keyname>Makse</keyname><forenames>Hern&#xe1;n A.</forenames></author></authors><title>Finding influential spreaders from human activity beyond network
  location</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><doi>10.1371/journal.pone.0136831</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most centralities proposed for identifying influential spreaders on social
networks to either spread a message or to stop an epidemic require the full
topological information of the network on which spreading occurs. In practice,
however, collecting all connections between agents in social networks can be
hardly achieved. As a result, such metrics could be difficult to apply to real
social networks. Consequently, a new approach for identifying influential
people without the explicit network information is demanded in order to provide
an efficient immunization or spreading strategy, in a practical sense. In this
study, we seek a possible way for finding influential spreaders by using the
social mechanisms of how social connections are formed in real networks. We
find that a reliable immunization scheme can be achieved by asking people how
they interact with each other. From these surveys we find that the
probabilistic tendency to connect to a hub has the strongest predictive power
for influential spreaders among tested social mechanisms. Our observation also
suggests that people who connect different communities is more likely to be an
influential spreader when a network has a strong modular structure. Our finding
implies that not only the effect of network location but also the behavior of
individuals is important to design optimal immunization or spreading schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01384</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01384</id><created>2015-07-06</created><authors><author><keyname>Tucker</keyname><forenames>Christopher A.</forenames></author></authors><title>The method of artificial systems</title><categories>cs.AI</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This document consists the basis of my Master's thesis work at the University
of Reading, UK from 2006-2008. It focuses primarily on two main concepts:
context awareness and choice in artificial systems set within a boundary of
available epistemology which serves to describe it. It was hypothesised that
such definitions yield a strong adaptation mechanism for an artificial entity
to manifest as a consequence of the program's runtime. The problem is presented
by discussing different points of view and proposing experiments to demonstrate
the complexities of such problems in artificial life in terms of machine
consciousness while indicating what a suitable answer might be.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01388</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01388</id><created>2015-07-06</created><authors><author><keyname>Clough</keyname><forenames>James R.</forenames></author><author><keyname>Evans</keyname><forenames>Tim S.</forenames></author></authors><title>Time and Citation Networks</title><categories>cs.DL cs.SI physics.soc-ph</categories><comments>6 pages. In &quot;Proceedings of ISSI 2015 Istanbul: 15th International
  Society of Scientometrics and Informetrics Conference, Istanbul, Turkey, 29
  June to 3 July, 2015&quot;, ISBN 978-975-518-381-7; ISSN 2175-1935. Slides of the
  associated talk are available from
  http://dx.doi.org/10.6084/m9.figshare.1464980</comments><report-no>Imperial/TP/15/TSE/1</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Citation networks emerge from a number of different social systems, such as
academia (from published papers), business (through patents) and law (through
legal judgements). A citation represents a transfer of information, and so
studying the structure of the citation network will help us understand how
knowledge is passed on. What distinguishes citation networks from other
networks is time; documents can only cite older documents. We propose that
existing network measures do not take account of the strong constraint imposed
by time. We will illustrate our approach with two types of causally aware
analysis. We apply our methods to the citation networks formed by academic
papers on the arXiv, to US patents and to US Supreme Court judgements. We show
that our tools can reveal that citation networks which appear to have very
similar structure by standard network measures turn out to have significantly
different properties. We interpret our results as indicating that many papers
in a bibliography were not directly relevant to the work and that we can
provide a simple indicator of the important citations. We suggest our methods
may highlight papers which are of more interest for interdisciplinary research.
We also quantify differences in the diversity of research directions of
different fields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01391</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01391</id><created>2015-07-06</created><authors><author><keyname>Afshani</keyname><forenames>Peyman</forenames></author><author><keyname>Sitchinava</keyname><forenames>Nodari</forenames></author></authors><title>Sorting and Permuting without Bank Conflicts on GPUs</title><categories>cs.DS cs.DC</categories><comments>12 pages, 2 figures, 23rd European Symposium on Algorithms (ESA)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we look at the complexity of designing algorithms without any
bank conflicts in the shared memory of Graphical Processing Units (GPUs). Given
input of size $n$, $w$ processors and $w$ memory banks, we study three
fundamental problems: sorting, permuting and $w$-way partitioning (defined as
sorting an input containing exactly $n/w$ copies of every integer in $[w]$).
  We solve sorting in optimal $O(\frac{n}{w} \log n)$ time. When $n \ge w^2$,
we solve the partitioning problem optimally in $O(n/w)$ time. We also present a
general solution for the partitioning problem which takes $O(\frac{n}{w}
\log^3_{n/w} w)$ time. Finally, we solve the permutation problem using a
randomized algorithm in $O(\frac{n}{w} \log\log\log_{n/w} n)$ time. Our results
show evidence that when working with banked memory architectures, there is a
separation between these problems and the permutation and partitioning problems
are not as easy as simple parallel scanning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01422</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01422</id><created>2015-07-06</created><authors><author><keyname>Pan</keyname><forenames>Junting</forenames></author><author><keyname>Gir&#xf3;-i-Nieto</keyname><forenames>Xavier</forenames></author></authors><title>End-to-end Convolutional Network for Saliency Prediction</title><categories>cs.CV cs.LG cs.NE</categories><comments>Winner of the saliency prediction challenge in the Large-scale Scene
  Understanding (LSUN) Challenge in the associated workshop of the IEEE
  Conference on Computer Vision and Pattern Recognition (CVPR) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The prediction of saliency areas in images has been traditionally addressed
with hand crafted features based on neuroscience principles. This paper however
addresses the problem with a completely data-driven approach by training a
convolutional network. The learning process is formulated as a minimization of
a loss function that measures the Euclidean distance of the predicted saliency
map with the provided ground truth. The recent publication of large datasets of
saliency prediction has provided enough data to train a not very deep
architecture which is both fast and accurate. The convolutional network in this
paper, named JuntingNet, won the LSUN 2015 challenge on saliency prediction
with a superior performance in all considered metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01423</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01423</id><created>2015-07-06</created><authors><author><keyname>Ranzato</keyname><forenames>Francesco</forenames></author></authors><title>Abstract Interpretation of Supermodular Games</title><categories>cs.GT cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Supermodular games find significant applications in a variety of models,
especially in operations research and economic applications of noncooperative
game theory, and feature pure strategy Nash equilibria characterized as fixed
points of multivalued functions on complete lattices. Pure strategy Nash
equilibria of supermodular games are here approximated by resorting to the
theory of abstract interpretation, a well established and known framework used
for designing static analyses of programming languages. This is obtained by
extending the theory of abstract interpretation in order to handle
approximations of multivalued functions and by providing some methods for
abstracting supermodular games, in order to obtain approximate Nash equilibria
which are shown to be correct within the abstract interpretation framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01425</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01425</id><created>2015-07-06</created><updated>2016-01-26</updated><authors><author><keyname>Arisaka</keyname><forenames>Ryuta</forenames></author></authors><title>Latent Belief Theory and Belief Dependencies: A Solution to the Recovery
  Problem in the Belief Set Theories</title><categories>cs.AI</categories><comments>Corrected the following: 1. in Definition 1, earlier versions had
  2^Props x 2^Props x N, but clearly it should be 2^{Props x Props x N}. 2. in
  Definition 1, one disjunctive case was missing. The 5th item is newly added
  to complete. 3. On page 3, in the right column, the 2nd axiom for Compactness
  has a typo. It is not P \in X, but should be P \in L(X)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The AGM recovery postulate says: assume a set of propositions X; assume that
it is consistent and that it is closed under logical consequences; remove a
belief P from the set minimally, but make sure that the resultant set is again
some set of propositions X' which is closed under the logical consequences; now
add P again and close the set under the logical consequences; and we should get
a set of propositions that contains all the propositions that were in X. This
postulate has since met objections; many have observed that it could bear
counter-intuitive results. Nevertheless, the attempts that have been made so
far to amend it either recovered the postulate in full, had to relinquish the
assumption of the logical closure altogether, or else had to introduce fresh
controversies of their own. We provide a solution to the recovery paradox in
this work. Our theoretical basis is the recently proposed belief theory with
latent beliefs (simply the latent belief theory for short). Firstly, through
examples, we will illustrate that the vanilla latent belief theory can be made
more expressive. We will identify that a latent belief, when it becomes
visible, may remain visible only while the beliefs that triggered it into the
agent's consciousness are in the agent's belief set. In order that such
situations can be also handled, we will enrich the latent belief theory with
belief dependencies among attributive beliefs, recording the information as to
which belief is supported of its existence by which beliefs. We will show that
the enriched latent belief theory does not possess the recovery property. The
closure by logical consequences is maintained in the theory, however. Hence it
serves as a solution to the open problem in the belief set theories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01428</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01428</id><created>2015-07-06</created><authors><author><keyname>Codish</keyname><forenames>Michael</forenames></author><author><keyname>Cruz-Filipe</keyname><forenames>Lu&#xed;s</forenames></author><author><keyname>Ehlers</keyname><forenames>Thorsten</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Mike</forenames></author><author><keyname>Schneider-Kamp</keyname><forenames>Peter</forenames></author></authors><title>Sorting Networks: to the End and Back Again</title><categories>cs.DS cs.DM</categories><comments>IMADA-preprint-cs. arXiv admin note: text overlap with
  arXiv:1411.6408</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies new properties of the front and back ends of a sorting
network, and illustrates the utility of these in the search for new bounds on
optimal sorting networks. Search focuses first on the &quot;outsides&quot; of the network
and then on the inner part. All previous works focus only on properties of the
front end of networks and on how to apply these to break symmetries in the
search. The new, out-side-in, properties help shed understanding on how sorting
networks sort, and facilitate the computation of new bounds on optimal sorting
networks. We present new parallel sorting networks for 17 to 20 inputs. For 17,
19, and 20 inputs these networks are faster than the previously known best
networks. For 17 inputs, the new sorting network is shown optimal in the sense
that no sorting network using less layers exists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01442</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01442</id><created>2015-07-06</created><authors><author><keyname>Liu</keyname><forenames>Shicong</forenames></author><author><keyname>Lu</keyname><forenames>Hongtao</forenames></author></authors><title>Learning Better Encoding for Approximate Nearest Neighbor Search with
  Dictionary Annealing</title><categories>cs.CV</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a novel dictionary optimization method for high-dimensional
vector quantization employed in approximate nearest neighbor (ANN) search.
Vector quantization methods first seek a series of dictionaries, then
approximate each vector by a sum of elements selected from these dictionaries.
An optimal series of dictionaries should be mutually independent, and each
dictionary should generate a balanced encoding for the target dataset. Existing
methods did not explicitly consider this. To achieve these goals along with
minimizing the quantization error (residue), we propose a novel dictionary
optimization method called \emph{Dictionary Annealing} that alternatively
&quot;heats up&quot; a single dictionary by generating an intermediate dataset with
residual vectors, &quot;cools down&quot; the dictionary by fitting the intermediate
dataset, then extracts the new residual vectors for the next iteration. Better
codes can be learned by DA for the ANN search tasks. DA is easily implemented
on GPU to utilize the latest computing technology, and can easily extended to
an online dictionary learning scheme. We show by experiments that our optimized
dictionaries substantially reduce the overall quantization error. Jointly used
with residual vector quantization, our optimized dictionaries lead to a better
approximate nearest neighbor search performance compared to the
state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01443</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01443</id><created>2015-07-06</created><authors><author><keyname>Ferragut</keyname><forenames>Erik M.</forenames></author><author><keyname>Laska</keyname><forenames>Jason</forenames></author></authors><title>Nonparametric Bayesian Modeling for Automated Database Schema Matching</title><categories>cs.IR cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of merging databases arises in many government and commercial
applications. Schema matching, a common first step, identifies equivalent
fields between databases. We introduce a schema matching framework that builds
nonparametric Bayesian models for each field and compares them by computing the
probability that a single model could have generated both fields. Our
experiments show that our method is more accurate and faster than the existing
instance-based matching algorithms in part because of the use of nonparametric
Bayesian models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01444</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01444</id><created>2015-06-11</created><updated>2016-01-10</updated><authors><author><keyname>Garcia-Morales</keyname><forenames>Vladimir</forenames></author></authors><title>Fractal surfaces from simple arithmetic operations</title><categories>cs.OH</categories><comments>15 pages, 6 figures, minor corrections. Published in Physica A</comments><journal-ref>Physica A 447, 535 (2016)</journal-ref><doi>10.1016/j.physa.2015.12.028</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fractal surfaces ('patchwork quilts') are shown to arise under most general
circumstances involving simple bitwise operations between real numbers. A
theory is presented for all deterministic bitwise operations on a finite
alphabet. It is shown that these models give rise to a roughness exponent $H$
that shapes the resulting spatial patterns, larger values of the exponent
leading to coarser surfaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01450</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01450</id><created>2015-06-12</created><authors><author><keyname>Alam</keyname><forenames>Muhammad Jawaherul</forenames></author><author><keyname>Bl&#xe4;sius</keyname><forenames>Thomas</forenames></author><author><keyname>Rutter</keyname><forenames>Ignaz</forenames></author><author><keyname>Ueckerdt</keyname><forenames>Torsten</forenames></author><author><keyname>Wolff</keyname><forenames>Alexander</forenames></author></authors><title>Pixel and Voxel Representations of Graphs</title><categories>cs.DM math.CO</categories><msc-class>05C10, 05C62, 68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study contact representations for graphs, which we call pixel
representations in 2D and voxel representations in 3D. Our representations are
based on the unit square grid whose cells we call pixels in 2D and voxels in
3D. Two pixels are adjacent if they share an edge, two voxels if they share a
face. We call a connected set of pixels or voxels a blob. Given a graph, we
represent its vertices by disjoint blobs such that two blobs contain adjacent
pixels or voxels if and only if the corresponding vertices are adjacent. We are
interested in the size of a representation, which is the number of pixels or
voxels it consists of.
  We first show that finding minimum-size representations is NP-complete. Then,
we bound representation sizes needed for certain graph classes. In 2D, we show
that, for $k$-outerplanar graphs with $n$ vertices, $\Theta(kn)$ pixels are
always sufficient and sometimes necessary. In particular, outerplanar graphs
can be represented with a linear number of pixels, whereas general planar
graphs sometimes need a quadratic number. In 3D, $\Theta(n^2)$ voxels are
always sufficient and sometimes necessary for any $n$-vertex graph. We improve
this bound to $\Theta(n\cdot \tau)$ for graphs of treewidth $\tau$ and to
$O((g+1)^2n\log^2n)$ for graphs of genus $g$. In particular, planar graphs
admit representations with $O(n\log^2n)$ voxels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01451</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01451</id><created>2015-07-06</created><updated>2015-07-11</updated><authors><author><keyname>Eiter</keyname><forenames>Thomas</forenames></author><author><keyname>Fink</keyname><forenames>Michael</forenames></author><author><keyname>Ianni</keyname><forenames>Giovambattista</forenames></author><author><keyname>Krennwallner</keyname><forenames>Thomas</forenames></author><author><keyname>Redl</keyname><forenames>Christoph</forenames></author><author><keyname>Sch&#xfc;ller</keyname><forenames>Peter</forenames></author></authors><title>A model building framework for Answer Set Programming with external
  computations</title><categories>cs.AI cs.LO cs.PL</categories><comments>57 pages, 9 figures, 3 tables, 6 algorithms, to appear in Theory and
  Practice of Logic Programming (accepted in June 2015)</comments><msc-class>68T27, 68T30</msc-class><acm-class>D.1.6; I.2.4; I.2.3</acm-class><doi>10.1017/S1471068415000113</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As software systems are getting increasingly connected, there is a need for
equipping nonmonotonic logic programs with access to external sources that are
possibly remote and may contain information in heterogeneous formats. To cater
for this need, HEX programs were designed as a generalization of answer set
programs with an API style interface that allows to access arbitrary external
sources, providing great flexibility. Efficient evaluation of such programs
however is challenging, and it requires to interleave external computation and
model building; to decide when to switch between these tasks is difficult, and
existing approaches have limited scalability in many real-world application
scenarios. We present a new approach for the evaluation of logic programs with
external source access, which is based on a configurable framework for dividing
the non-ground program into possibly overlapping smaller parts called
evaluation units. The latter will be processed by interleaving external
evaluation and model building using an evaluation graph and a model graph,
respectively, and by combining intermediate results. Experiments with our
prototype implementation show a significant improvement compared to previous
approaches. While designed for HEX-programs, the new evaluation approach may be
deployed to related rule-based formalisms as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01456</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01456</id><created>2015-07-06</created><updated>2015-08-07</updated><authors><author><keyname>Ionescu</keyname><forenames>Radu Cristian</forenames></author></authors><title>A scalable system for primal-dual optimization</title><categories>cs.DC</categories><comments>This has been withdrawn by the author due since it is not fully
  complete to reach a publication on arxiv.org</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present some of the most widely used architectures for Big Data,
\textit{Hadoop} and \textit{Spark}, and develop several implementations
exploiting, the advantages of each. We implement a simplified version of the
primal-dual optimization algorithm, described briefly in this paper, by
choosing the smoothing functions to be $\Vert \cdot \Vert^2$ with a zero center
point. Under the assumption that data is provided as a sparse matrix, we assess
the scalability of the designed systems empirically by running them on sample
tests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01458</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01458</id><created>2015-07-06</created><authors><author><keyname>Biondo</keyname><forenames>A. E.</forenames></author><author><keyname>Giarlotta</keyname><forenames>A.</forenames></author><author><keyname>Pluchino</keyname><forenames>A.</forenames></author><author><keyname>Rapisarda</keyname><forenames>A.</forenames></author></authors><title>Perfect Information vs Random Investigation: Safety Guidelines for a
  Consumer in the Jungle of Product Differentiation</title><categories>physics.soc-ph cs.SI</categories><comments>27 pages, 12 figures</comments><doi>10.1371/journal.pone.0146389</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a graph-theoretic model of consumer choice, where final decisions
are shown to be influenced by information and knowledge, in the form of
individual awareness, discriminating ability, and perception of market
structure. Building upon the distance-based Hotelling's differentiation idea,
we describe the behavioral experience of several prototypes of consumers, who
walk a hypothetical cognitive path in an attempt to maximize their
satisfaction. Our simulations show that even consumers endowed with a small
amount of information and knowledge may reach a very high level of utility. On
the other hand, complete ignorance negatively affects the whole consumption
process. In addition, rather unexpectedly, a random walk on the graph reveals
to be a winning strategy, below a minimal threshold of information and
knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01461</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01461</id><created>2015-07-06</created><authors><author><keyname>Ionescu</keyname><forenames>Radu Cristian</forenames></author></authors><title>Revisiting Large Scale Distributed Machine Learning</title><categories>cs.DC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, with the widespread of smartphones and other portable gadgets
equipped with a variety of sensors, data is ubiquitous available and the focus
of machine learning has shifted from being able to infer from small training
samples to dealing with large scale high-dimensional data. In domains such as
personal healthcare applications, which motivates this survey, distributed
machine learning is a promising line of research, both for scaling up learning
algorithms, but mostly for dealing with data which is inherently produced at
different locations. This report offers a thorough overview of and
state-of-the-art algorithms for distributed machine learning, for both
supervised and unsupervised learning, ranging from simple linear logistic
regression to graphical models and clustering. We propose future directions for
most categories, specific to the potential personal healthcare applications.
With this in mind, the report focuses on how security and low communication
overhead can be assured in the specific case of a strictly client-server
architectural model. As particular directions we provides an exhaustive
presentation of an empirical clustering algorithm, k-windows, and proposed an
asynchronous distributed machine learning algorithm that would scale well and
also would be computationally cheap and easy to implement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01465</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01465</id><created>2015-07-06</created><updated>2015-08-27</updated><authors><author><keyname>Csat&#xf3;</keyname><forenames>L&#xe1;szl&#xf3;</forenames></author></authors><title>Distance-based accessibility indices</title><categories>cs.SI physics.soc-ph</categories><msc-class>15A06, 91D30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper attempts to develop a suitable accessibility index for networks
where each link has a value such that a smaller number is preferred like
distance, cost, or travel time. A measure called distance sum is characterized
by three independent properties: anonymity, an appropriately chosen
independence axiom, and dominance preservation, which requires that a node not
far to any other is at least as accessible.
  We argue for the need of eliminating the independence property in certain
applications. Therefore generalized distance sum, a family of accessibility
indices, will be suggested. It is linear, considers the accessibility of
vertices besides their distances and depends on a parameter in order to control
its deviation from distance sum. Generalized distance sum is anonymous and
satisfies dominance preservation if its parameter meets a sufficient condition.
Two detailed examples demonstrate its ability to reflect the vulnerability of
accessibility to link disruptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01476</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01476</id><created>2015-07-06</created><authors><author><keyname>He</keyname><forenames>Niao</forenames></author><author><keyname>Harchaoui</keyname><forenames>Zaid</forenames></author></authors><title>Semi-proximal Mirror-Prox for Nonsmooth Composite Minimization</title><categories>math.OC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new first-order optimisation algorithm to solve high-dimensional
non-smooth composite minimisation problems. Typical examples of such problems
have an objective that decomposes into a non-smooth empirical risk part and a
non-smooth regularisation penalty. The proposed algorithm, called Semi-Proximal
Mirror-Prox, leverages the Fenchel-type representation of one part of the
objective while handling the other part of the objective via linear
minimization over the domain. The algorithm stands in contrast with more
classical proximal gradient algorithms with smoothing, which require the
computation of proximal operators at each iteration and can therefore be
impractical for high-dimensional problems. We establish the theoretical
convergence rate of Semi-Proximal Mirror-Prox, which exhibits the optimal
complexity bounds, i.e. $O(1/\epsilon^2)$, for the number of calls to linear
minimization oracle. We present promising experimental results showing the
interest of the approach in comparison to competing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01477</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01477</id><created>2015-07-06</created><updated>2015-08-27</updated><authors><author><keyname>Csat&#xf3;</keyname><forenames>L&#xe1;szl&#xf3;</forenames></author></authors><title>Between plurality and proportionality: an analysis of vote transfer
  systems</title><categories>cs.GT</categories><msc-class>91B12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper considers a general model of electoral systems combining
district-based elections with a compensatory mechanism in order to create any
outcome between strictly majoritarian and purely proportional seat allocation.
It contains vote transfer and allows for the application of three different
correction formulas. Analysis in a two-party system shows that a trade-off
exists for the dominant party between its expected seat share and its chance of
obtaining majority. Vote transfer rules are also investigated by focusing on
the possibility of manipulation.
  The model is applied to the 2014 Hungarian parliamentary election.
Hypothetical results reveal that the vote transfer rule cannot be evaluated in
itself, only together with the share of constituency seats. With an appropriate
choice of the latter, the three mechanisms may be functionally equivalent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01484</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01484</id><created>2015-07-06</created><updated>2015-08-21</updated><authors><author><keyname>Stopczynski</keyname><forenames>Arkadiusz</forenames></author><author><keyname>Sapiezynski</keyname><forenames>Piotr</forenames></author><author><keyname>Pentland</keyname><forenames>Alex 'Sandy'</forenames></author><author><keyname>Lehmann</keyname><forenames>Sune</forenames></author></authors><title>Temporal Fidelity in Dynamic Social Networks</title><categories>physics.soc-ph cs.SI</categories><doi>10.1140/epjb/e2015-60549-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has recently become possible to record detailed social interactions in
large social systems with high resolution. As we study these datasets, human
social interactions display patterns that emerge at multiple time scales, from
minutes to months. On a fundamental level, understanding of the network
dynamics can be used to inform the process of measuring social networks. The
details of measurement are of particular importance when considering dynamic
processes where minute-to-minute details are important, because collection of
physical proximity interactions with high temporal resolution is difficult and
expensive. Here, we consider the dynamic network of proximity-interactions
between approximately 500 individuals participating in the Copenhagen Networks
Study. We show that in order to accurately model spreading processes in the
network, the dynamic processes that occur on the order of minutes are essential
and must be included in the analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01489</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01489</id><created>2015-07-06</created><authors><author><keyname>Pi&#xf1;a-Garc&#xed;a</keyname><forenames>C. A.</forenames></author><author><keyname>Gu</keyname><forenames>Dongbing</forenames></author></authors><title>Towards a Standard Sampling Methodology on Online Social Networks:
  Collecting Global Trends on Twitter</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most significant current challenges in large-scale online social
networks, is to establish a concise and coherent method able to collect and
summarize data. Sampling the content of an Online Social Network (OSN) plays an
important role as a knowledge discovery tool.
  It is becoming increasingly difficult to ignore the fact that current
sampling methods must cope with a lack of a full sampling frame i.e., there is
an imposed condition determined by a limited data access. In addition, another
key aspect to take into account is the huge amount of data generated by users
of social networking services. This type of conditions make especially
difficult to develop sampling methods to collect truly reliable data.
Therefore, we propose a low computational cost method for sampling emerging
global trends on social networking services such as Twitter.
  The main purpose of this study, is to develop a methodology able to carry out
an efficient collecting process via three random generators: Brownian, Illusion
and Reservoir. These random generators will be combined with a
Metropolis-Hastings Random Walk (MHRW) in order to improve the sampling
process. We demonstrate the effectiveness of our approach by correctly
providing a descriptive statistics of the collected data. In addition, we also
sketch the collecting procedure on real-time carried out on Twitter. Finally,
we conclude with a trend concentration graphical description and a formal
convergence analysis to evaluate whether the sample of draws has attained an
equilibrium state to get a rough estimate of the sample quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01490</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01490</id><created>2015-07-06</created><authors><author><keyname>Borassi</keyname><forenames>Michele</forenames></author><author><keyname>Crescenzi</keyname><forenames>Pierluigi</forenames></author><author><keyname>Marino</keyname><forenames>Andrea</forenames></author></authors><title>Fast and Simple Computation of Top-k Closeness Centralities</title><categories>cs.DS</categories><acm-class>G.2.2; H.2.8; I.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Closeness is an important centrality measure widely used in the analysis of
real-world complex networks. In particular, the problem of selecting the k most
central nodes with respect to this measure has been deeply analyzed in the last
decade. However, even for not very large networks, this problem is
computationally intractable in practice: indeed, Abboud et al have recently
shown that its complexity is strictly related to the complexity of the
All-Pairs Shortest Path (in short, APSP) problem, for which no subcubic
&quot;combinatorial&quot; algorithm is known. In this paper, we propose a new algorithm
for selecting the k most closeness central nodes in a graph. In practice, this
algorithm significantly improves over the APSP approach, even though its
worst-case time complexity is the same. For example, the algorithm is able to
compute the top k nodes in few dozens of seconds even when applied to
real-world networks with millions of nodes and edges. We will also
experimentally prove that our algorithm drastically outperforms the most
recently designed algorithm, proposed by Olsen et al. Finally, we apply the new
algorithm to the computation of the most central actors in the IMDB
collaboration network, where two actors are linked if they played together in a
movie.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01491</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01491</id><created>2015-07-06</created><updated>2016-01-26</updated><authors><author><keyname>Pumpluen</keyname><forenames>Susanne</forenames></author></authors><title>Finite nonassociative algebras obtained from skew polynomials and
  possible applications to $(f,\sigma,\delta)$-codes</title><categories>cs.IT math.IT math.RA</categories><comments>Extended version of previous submission: The algebra construction is
  generalized and more connections given with linear $(f,\sigma,\delta)$-codes
  and their companion matrices and pseudo-linear transformations; some mistakes
  are corrected. Title changed to &quot;Finite nonassociative algebras obtained from
  skew polynomials and possible applications to $(f,\sigma,\delta)$-codes.&quot;</comments><msc-class>Primary: 17A60, Secondary: 94B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $S$ be a unital ring, $S[t;\sigma,\delta]$ a skew polynomial ring, and
suppose $f \in S[t;\sigma,\delta]$ has degree $m$ and a unit as leading
coefficient. Using right division by $f$ to define the multiplication, we
obtain unital nonassociative algebras on the set of skew polynomials in
$S[t;\sigma,\delta]$ of degree less than $m$. We study the structure of these
algebras.
  When $S$ is a Galois ring and $f$ base irreducible, these algebras yield
families of finite unital nonassociative rings $A$, whose set of (left or
right) zero divisors has the form $pA$ for some prime $p$.
  For reducible $f$, the algebras can be employed both to design linear
$(f,\sigma,\delta)$-codes over unital rings and to study their behaviour.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01501</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01501</id><created>2015-07-06</created><updated>2015-07-14</updated><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>Pass</keyname><forenames>Rafael</forenames></author><author><keyname>Reichman</keyname><forenames>Daniel</forenames></author></authors><title>On the Non-Existence of Nash Equilibrium in Games with Resource-Bounded
  Players</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider sequences of games $\mathcal{G}=\{G_1,G_2,\ldots\}$ where, for
all $n$, $G_n$ has the same set of players. Such sequences arise in the
analysis of running time of players in games, in electronic money systems such
as Bitcoin and in cryptographic protocols. Assuming that one-way functions
exist, we prove that there is a sequence of 2-player zero-sum Bayesian games
$\mathcal{G}$ such that, for all $n$, the size of every action in $G_n$ is
polynomial in $n$, the utility function is polynomial computable in $n$, and
yet there is no polynomial-time Nash equilibrium, where we use a notion of Nash
equilibrium that is tailored to sequences of games. We also demonstrate that
Nash equilibrium may not exist when considering players that are constrained to
perform at most $T$ computational steps in each of the games
$\{G_i\}_{i=1}^{\infty}$. These examples may shed light on competitive settings
where the availability of more running time or faster algorithms lead to a
&quot;computational arms race&quot;, precluding the existence of equilibrium. They also
point to inherent limitations of concepts such &quot;best response&quot; and Nash
equilibrium in games with resource-bounded players.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01512</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01512</id><created>2015-07-06</created><updated>2015-10-28</updated><authors><author><keyname>Rusu</keyname><forenames>Irena</forenames></author></authors><title>Log-Lists and Their Applications to Sorting by Transpositions, Reversals
  and Block-Interchanges</title><categories>cs.DS</categories><comments>19 pages, 7 figures</comments><msc-class>68P05, 11Y16</msc-class><acm-class>E.1; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Link-cut trees have been introduced by D.D. Sleator and R.E. Tarjan (Journal
of Computer and System Sciences, 1983) with the aim of efficiently maintaining
a forest of vertex-disjoint dynamic rooted trees under cut and link operations.
These operations respectively disconnect a subtree from a tree, and join two
trees by an edge. Additionally, link-cut trees allow to change the root of a
tree and to perform a number of updates and queries on cost values defined on
the arcs of the trees. All these operations are performed in $O(\log\, n)$
amortized or worst-case time, depending on the implementation, where $n$ is the
total size of the forest.
  In this paper, we show that a list of elements implemented using link-cut
trees (we call it a $\log$-list) allows us to obtain a common running time of
$O(\log\, n)$ for the classical operations on lists, but also for some other
essential operations that usually take linear time on lists. Such operations
require to find the minimum/maximum element in a sublist defined by its
endpoints, the position of a given element in the list or the element placed at
a given position in the list; or they require to add a value $a$, or to
multiply by $-1$, all the elements in a sublist.
  Furthermore, we use $\log$-lists to implement several existing algorithms for
sorting permutations by transpositions and/or reversals and/or
block-interchanges, and obtain $O(n\,\log\, n)$ running time for all of them.
In this way, the running time of several algorithms is improved, whereas in
other cases our algorithms perform as well as the best existing
implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01526</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01526</id><created>2015-07-06</created><updated>2016-01-07</updated><authors><author><keyname>Kalchbrenner</keyname><forenames>Nal</forenames></author><author><keyname>Danihelka</keyname><forenames>Ivo</forenames></author><author><keyname>Graves</keyname><forenames>Alex</forenames></author></authors><title>Grid Long Short-Term Memory</title><categories>cs.NE cs.CL cs.LG</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces Grid Long Short-Term Memory, a network of LSTM cells
arranged in a multidimensional grid that can be applied to vectors, sequences
or higher dimensional data such as images. The network differs from existing
deep LSTM architectures in that the cells are connected between network layers
as well as along the spatiotemporal dimensions of the data. The network
provides a unified way of using LSTM for both deep and sequential computation.
We apply the model to algorithmic tasks such as 15-digit integer addition and
sequence memorization, where it is able to significantly outperform the
standard LSTM. We then give results for two empirical tasks. We find that 2D
Grid LSTM achieves 1.47 bits per character on the Wikipedia character
prediction benchmark, which is state-of-the-art among neural approaches. In
addition, we use the Grid LSTM to define a novel two-dimensional translation
model, the Reencoder, and show that it outperforms a phrase-based reference
system on a Chinese-to-English translation task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01529</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01529</id><created>2015-07-06</created><authors><author><keyname>Murtagh</keyname><forenames>Fionn</forenames></author></authors><title>Correspondence Factor Analysis of Big Data Sets: A Case Study of 30
  Million Words; and Contrasting Analytics using Apache Solr and Correspondence
  Analysis in R</title><categories>cs.CL</categories><comments>38 pages, 17 figures</comments><msc-class>62H25, 62.07</msc-class><acm-class>G.3; H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a large number of text data sets. These are cooking recipes. Term
distribution and other distributional properties of the data are investigated.
Our aim is to look at various analytical approaches which allow for mining of
information on both high and low detail scales. Metric space embedding is
fundamental to our interest in the semantic properties of this data. We
consider the projection of all data into analyses of aggregated versions of the
data. We contrast that with projection of aggregated versions of the data into
analyses of all the data. Analogously for the term set, we look at analysis of
selected terms. We also look at inherent term associations such as between
singular and plural. In addition to our use of Correspondence Analysis in R,
for latent semantic space mapping, we also use Apache Solr. Setting up the Solr
server and carrying out querying is described. A further novelty is that
querying is supported in Solr based on the principal factor plane mapping of
all the data. This uses a bounding box query, based on factor projections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01540</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01540</id><created>2015-07-06</created><updated>2015-11-28</updated><authors><author><keyname>Ye</keyname><forenames>Hongxing</forenames></author><author><keyname>Ge</keyname><forenames>Yinyin</forenames></author><author><keyname>Shahidehpour</keyname><forenames>Mohammad</forenames></author><author><keyname>Li</keyname><forenames>Zuyi</forenames></author></authors><title>Market Clearing for Uncertainty, Generation Reserve, and Transmission
  Reserve</title><categories>math.OC cs.SY</categories><comments>add more discussions; combine two parts into one part; correct typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel market mechanism is proposed to charge the uncertainty and credit the
generation reserve in the Day-ahead market within a Robust Security-Constrained
Unit Commitment (RSCUC) optimization framework. The increasing penetration of
renewable energy in recent years has led to more uncertainties in power
systems. These uncertainties have to be accommodated by flexible resources
(i.e. upward and downward generation reserves). Without the market clearing
tool, the RSCUC can only be applied in the Reliability Assessment Commitment
(RAC) process. In this paper,Locational Marginal Price (LMP) and Uncertainty
Marginal Prices (UMPs) in upward and downward directions are derived in robust
optimization framework. Both uncertainties and generation reserves are priced
at UMPs, which leads to partial market equilibrium. With the help of UMP, the
cost of generation reserves for uncertainty accommodation can be clearly
allocated to uncertainty sources. We prove that transmission reserves for
ramping delivery may lead to Financial Transmission Right (FTR) underfunding
issue within the existing market structure. The FTR underfunding issue can be
resolved by introducing transmission reserve credits based on UMPs. Simulations
on a Six-bus system and IEEE 118-bus system are performed to illustrate the new
concepts and the market mechanism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01546</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01546</id><created>2015-07-06</created><updated>2015-12-06</updated><authors><author><keyname>Gill</keyname><forenames>Sukhpal Singh</forenames></author></authors><title>Autonomic Cloud Computing: Research Perspective</title><categories>cs.DC</categories><comments>Author's Viewpoint on Autonomic Cloud Computing and Uploaded on
  Research Gate [https://www.researchgate.net/profile/Sukhpal_Gill]</comments><msc-class>97P70</msc-class><acm-class>J.7</acm-class><doi>10.13140/RG.2.1.4453.4881</doi><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Cloud computing is an evolving utility computing mechanism in which cloud
consumer can detect, choose and utilize the resources (infrastructure, software
and platform) and provide service to user based on pay per use model as
computing utilities. Current computing mechanism is effective, particular for
medium and small cloud based companies, in which it permits easy and reliable
access to cloud services like infrastructure, software and platform. Present
cloud computing is almost similar to the existing models: cluster computing and
grid computing. The important key technical features of cloud computing which
includes autonomic service, rapid elasticity, end-to-end virtualization
support, on-demand resource pooling and transparency in cloud billing. Further,
non-technical features of cloud computing includes environment friendliness,
little maintenance overhead, lower upfront costs, faster time to deployments,
Service Level Agreement (SLA) and pay-as-you-go-model. In distributed computing
environment, unpredictability of service is a fact, so same possible in cloud
also. The success of next-generation Cloud Computing infrastructures will
depend on how capably these infrastructures will discover and dynamically
tolerate computing platforms, which meet randomly varying resource and service
requirements of Cloud costumer applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01555</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01555</id><created>2015-07-06</created><authors><author><keyname>Eppstein</keyname><forenames>David</forenames></author><author><keyname>Har-Peled</keyname><forenames>Sariel</forenames></author><author><keyname>Sidiropoulos</keyname><forenames>Anastasios</forenames></author></authors><title>Approximate Greedy Clustering and Distance Selection for Graph Metrics</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  $\newcommand{\eps}{\varepsilon}$ In this paper, we consider two important
problems defined on finite metric spaces, and provide efficient new algorithms
and approximation schemes for these problems on inputs given as graph shortest
path metrics or high-dimensional Euclidean metrics. The first of these problems
is the greedy permutation (or farthest-first traversal) of a finite metric
space: a permutation of the points of the space in which each point is as far
as possible from all previous points. We describe randomized algorithms to find
$(1+\eps)$-approximate greedy permutations of any graph with $n$ vertices and
$m$ edges in expected time $O(\eps^{-1}(m+n)\log n\log(n/\eps))$, and to find
$(1+\eps)$-approximate greedy permutations of points in high-dimensional
Euclidean spaces in expected time $O(\eps^{-2} n^{1+1/(1+\eps)^2 + o(1)})$.
Additionally we describe a deterministic algorithm to find exact greedy
permutations of any graph with $n$ vertices and treewidth $O(1)$ in worst-case
time $O(n^{3/2}\log^{O(1)} n)$. The second of the two problems we consider is
distance selection: given $k \in [ \binom{n}{2} ]$, we are interested in
computing the $k$th smallest distance in the given metric space. We show that
for planar graph metrics one can approximate this distance, up to a constant
factor, in near linear time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01563</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01563</id><created>2015-07-06</created><authors><author><keyname>Har-Peled</keyname><forenames>Sariel</forenames></author></authors><title>A Simple Algorithm for Maximum Margin Classification, Revisited</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, we revisit the algorithm of Har-Peled et. al. [HRZ07] for
computing a linear maximum margin classifier. Our presentation is self
contained, and the algorithm itself is slightly simpler than the original
algorithm. The algorithm itself is a simple Perceptron like iterative
algorithm. For more details and background, the reader is referred to the
original paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01569</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01569</id><created>2015-07-06</created><authors><author><keyname>Mahmood</keyname><forenames>A. Rupam</forenames></author><author><keyname>Yu</keyname><forenames>Huizhen</forenames></author><author><keyname>White</keyname><forenames>Martha</forenames></author><author><keyname>Sutton</keyname><forenames>Richard S.</forenames></author></authors><title>Emphatic Temporal-Difference Learning</title><categories>cs.LG cs.AI</categories><comments>9 pages, accepted for presentation at European Workshop on
  Reinforcement Learning</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Emphatic algorithms are temporal-difference learning algorithms that change
their effective state distribution by selectively emphasizing and
de-emphasizing their updates on different time steps. Recent works by Sutton,
Mahmood and White (2015), and Yu (2015) show that by varying the emphasis in a
particular way, these algorithms become stable and convergent under off-policy
training with linear function approximation. This paper serves as a unified
summary of the available results from both works. In addition, we demonstrate
the empirical benefits from the flexibility of emphatic algorithms, including
state-dependent discounting, state-dependent bootstrapping, and the
user-specified allocation of function approximation resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01578</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01578</id><created>2015-07-01</created><authors><author><keyname>Tripathi</keyname><forenames>Subarna</forenames></author><author><keyname>Belongie</keyname><forenames>Serge</forenames></author><author><keyname>Nguyen</keyname><forenames>Truong</forenames></author></authors><title>Beyond Semantic Image Segmentation : Exploring Efficient Inference in
  Video</title><categories>cs.CV</categories><comments>CVPR 2015 workshop WiCV : Extended Abstract</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the efficiency of the CRF inference module beyond image level
semantic segmentation. The key idea is to combine the best of two worlds of
semantic co-labeling and exploiting more expressive models. Similar to
[Alvarez14] our formulation enables us perform inference over ten thousand
images within seconds. On the other hand, it can handle higher-order clique
potentials similar to [vineet2014] in terms of region-level label consistency
and context in terms of co-occurrences. We follow the mean-field updates for
higher order potentials similar to [vineet2014] and extend the spatial
smoothness and appearance kernels [DenseCRF13] to address video data inspired
by [Alvarez14]; thus making the system amenable to perform video semantic
segmentation most effectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01581</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01581</id><created>2015-07-06</created><updated>2015-08-12</updated><authors><author><keyname>Caesar</keyname><forenames>Holger</forenames></author><author><keyname>Uijlings</keyname><forenames>Jasper</forenames></author><author><keyname>Ferrari</keyname><forenames>Vittorio</forenames></author></authors><title>Joint Calibration for Semantic Segmentation</title><categories>cs.CV</categories><comments>Includes improved results based on VGG16 CNN</comments><msc-class>68T45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Semantic segmentation is the task of assigning a class-label to each pixel in
an image. We propose a region-based semantic segmentation framework which
handles both full and weak supervision, and addresses three common problems:
(1) Objects occur at multiple scales and therefore we should use regions at
multiple scales. However, these regions are overlapping which creates
conflicting class predictions at the pixel-level. (2) Class frequencies are
highly imbalanced in realistic datasets. (3) Each pixel can only be assigned to
a single class, which creates competition between classes. We address all three
problems with a joint calibration method which optimizes a multi-class loss
defined over the final pixel-level output labeling, as opposed to simply region
classification. Our method outperforms the state-of-the-art on the popular SIFT
Flow [18] dataset in both the fully and weakly supervised setting by a
considerably margin (+6% and +10%, respectively).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01585</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01585</id><created>2015-07-05</created><authors><author><keyname>Chamie</keyname><forenames>Mahmoud El</forenames></author><author><keyname>Acikmese</keyname><forenames>Behcet</forenames></author></authors><title>Finite-Horizon Markov Decision Processes with State Constraints</title><categories>math.OC cs.SY</categories><comments>submitted to IEEE CDC 2015. arXiv admin note: text overlap with
  arXiv:1507.01151</comments><msc-class>90C40</msc-class><acm-class>I.2.8; G.1.6; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Markov Decision Processes (MDPs) have been used to formulate many
decision-making problems in science and engineering. The objective is to
synthesize the best decision (action selection) policies to maximize expected
rewards (minimize costs) in a given stochastic dynamical environment. In many
practical scenarios (multi-agent systems, telecommunication, queuing, etc.),
the decision-making problem can have state constraints that must be satisfied,
which leads to Constrained MDP (CMDP) problems. In the presence of such state
constraints, the optimal policies can be very hard to characterize. This paper
introduces a new approach for finding non-stationary randomized policies for
finite-horizon CMDPs. An efficient algorithm based on Linear Programming (LP)
and duality theory is proposed, which gives the convex set of feasible policies
and ensures that the expected total reward is above a computable lower-bound.
The resulting decision policy is a randomized policy, which is the projection
of the unconstrained deterministic MDP policy on this convex set. To the best
of our knowledge, this is the first result in state constrained MDPs to give an
efficient algorithm for generating finite horizon randomized policies for CMDP
with optimality guarantees. A simulation example of a swarm of autonomous
agents running MDPs is also presented to demonstrate the proposed CMDP solution
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01586</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01586</id><created>2015-07-06</created><authors><author><keyname>Vasudeva</keyname><forenames>Karthik</forenames></author><author><keyname>Simsek</keyname><forenames>Meryem</forenames></author><author><keyname>Lopez-Perez</keyname><forenames>David</forenames></author><author><keyname>Guvenc</keyname><forenames>Ismail</forenames></author></authors><title>Analysis of Handover Failures in Heterogeneous Networks with Fading</title><categories>cs.NI cs.CG</categories><comments>12 page, 16 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The handover process is one of the most critical functions in a cellular
network, and is in charge of maintaining seamless connectivity of user
equipments (UEs) across multiple cells. It is usually based on signal
measurements from the neighboring base stations (BSs), and it is adversely
affected by the time and frequency selectivity of the radio propagation
channel. In this paper, we introduce a new model for analyzing handover
performance in heterogeneous networks (HetNets) as a function of vehicular user
velocity, cell size, and mobility management parameters. In order to
investigate the impact of shadowing and fading on handover performance, we
extract relevant statistics obtained from a 3rd Generation Partnership Project
(3GPP)-compliant HetNet simulator, and subsequently, we integrate these
statistics into our analytical model to analyze handover failure probability
under fluctuating channel conditions. Computer simulations validate the
analytical findings, which show that fading can significantly degrade the
handover performance in HetNets with vehicular users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01611</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01611</id><created>2015-07-06</created><authors><author><keyname>Yazdi</keyname><forenames>S. M. Hossein Tabatabaei</forenames></author><author><keyname>Kiah</keyname><forenames>Han Mao</forenames></author><author><keyname>Garcia</keyname><forenames>Eva Ruiz</forenames></author><author><keyname>Ma</keyname><forenames>Jian</forenames></author><author><keyname>Zhao</keyname><forenames>Huimin</forenames></author><author><keyname>Milenkovic</keyname><forenames>Olgica</forenames></author></authors><title>DNA-Based Storage: Trends and Methods</title><categories>cs.ET cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide an overview of current approaches to DNA-based storage system
design and accompanying synthesis, sequencing and editing methods. We also
introduce and analyze a suite of new constrained coding schemes for both
archival and random access DNA storage channels. The mathematical basis of our
work is the construction and design of sequences over discrete alphabets that
avoid pre-specified address patterns, have balanced base content, and exhibit
other relevant substring constraints. These schemes adapt the stored signals to
the DNA medium and thereby reduce the inherent error-rate of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01613</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01613</id><created>2015-07-06</created><authors><author><keyname>Brimkov</keyname><forenames>Boris</forenames></author></authors><title>A note on the clique number of complete $k$-partite graphs</title><categories>cs.DM math.CO</categories><comments>6 pages</comments><msc-class>05C69, 05C07</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, we show that a complete $k$-partite graph is the only graph
with clique number $k$ among all degree-equivalent simple graphs. This result
gives a lower bound on the clique number, which is sharper than existing bounds
on a large family of graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01625</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01625</id><created>2015-07-06</created><authors><author><keyname>Hallgren</keyname><forenames>Sean</forenames></author><author><keyname>Smith</keyname><forenames>Adam</forenames></author><author><keyname>Song</keyname><forenames>Fang</forenames></author></authors><title>Classical Cryptographic Protocols in a Quantum World</title><categories>quant-ph cs.CR</categories><comments>Full version of an old paper in Crypto'11. Invited to IJQI. This is
  authors' copy with different formatting</comments><journal-ref>International Journal of Quantum Information Vol. 13, No. 4 (2015)
  1550028 (43 pages)</journal-ref><doi>10.1142/S0219749915500288</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cryptographic protocols, such as protocols for secure function evaluation
(SFE), have played a crucial role in the development of modern cryptography.
The extensive theory of these protocols, however, deals almost exclusively with
classical attackers. If we accept that quantum information processing is the
most realistic model of physically feasible computation, then we must ask: what
classical protocols remain secure against quantum attackers?
  Our main contribution is showing the existence of classical two-party
protocols for the secure evaluation of any polynomial-time function under
reasonable computational assumptions (for example, it suffices that the
learning with errors problem be hard for quantum polynomial time). Our result
shows that the basic two-party feasibility picture from classical cryptography
remains unchanged in a quantum world.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01628</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01628</id><created>2015-07-06</created><authors><author><keyname>Kaya</keyname><forenames>Abidin</forenames></author><author><keyname>Yildiz</keyname><forenames>Bahattin</forenames></author><author><keyname>Pa&#x15f;a</keyname><forenames>Abdullah</forenames></author></authors><title>New extremal binary self-dual codes from a modified four circulant
  construction</title><categories>cs.IT math.CO math.IT</categories><comments>7 tables</comments><msc-class>Primary 94B05, Secondary 94B99</msc-class><journal-ref>Discrete Mathematics Vol 338 Issue 3 2016</journal-ref><doi>10.1016/j.disc.2015.09.010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose a modified four circulant construction for self-dual
codes and a bordered version of the construction using the properties of
\lambda-circulant and \lambda-reverse circulant matrices. By using the
constructions on $F_2$, we obtain new binary codes of lengths 64 and 68. We
also apply the constructions to the ring $R_2$ and considering the $F_2$ and
$R_1$-extensions, we obtain new singly-even extremal binary self-dual codes of
lengths 66 and 68. More precisely, we find 3 new codes of length 64, 15 new
codes of length 66 and 22 new codes of length 68. These codes all have weight
enumerators with parameters that were not known to exist in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01631</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01631</id><created>2015-07-06</created><updated>2015-08-04</updated><authors><author><keyname>Pelekis</keyname><forenames>Christos</forenames></author></authors><title>A generalised isodiametric problem</title><categories>cs.CG</categories><comments>15 pages, 1 figure, Minor typos corrected</comments><msc-class>05Dxx, 51Fxx, 52Cxx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fix positive integers $a$ and $b$ such that $a&gt; b\geq 2$ and a positive real
$\delta&gt;0$. Let $S$ be a planar set of diameter $\delta$ having the following
property: for every $a$ points in $S$, at least $b$ of them have pairwise
distances that are all less than or equal to $2$. What is the maximum Lebesgue
measure of $S$? In this paper we investigate this problem. We discuss the,
devious, motivation that leads to its formulation and provide upper bounds on
the Lebesgue measure of $S$. Our main result is based on a generalisation of a
theorem that is due to Heinrich Jung. In certain instances we are able to find
the extremal set but the general case seems elusive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01636</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01636</id><created>2015-07-06</created><authors><author><keyname>Li</keyname><forenames>Jiwei</forenames></author><author><keyname>Hovy</keyname><forenames>Eduard</forenames></author></authors><title>Reflections on Sentiment/Opinion Analysis</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we described possible directions for deeper understanding,
helping bridge the gap between psychology / cognitive science and computational
approaches in sentiment/opinion analysis literature. We focus on the opinion
holder's underlying needs and their resultant goals, which, in a utilitarian
model of sentiment, provides the basis for explaining the reason a sentiment
valence is held. While these thoughts are still immature, scattered,
unstructured, and even imaginary, we believe that these perspectives might
suggest fruitful avenues for various kinds of future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01637</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01637</id><created>2015-07-06</created><authors><author><keyname>Arslan</keyname><forenames>Omur</forenames></author><author><keyname>Guralnik</keyname><forenames>Dan P.</forenames></author><author><keyname>Koditschek</keyname><forenames>Daniel E.</forenames></author></authors><title>Coordinated Robot Navigation via Hierarchical Clustering</title><categories>cs.RO</categories><comments>29 pages, 13 figures, 8 tables, extended version of a paper in
  preparation for submission to a journal</comments><acm-class>I.2.9</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the use of hierarchical clustering for relaxed, deterministic
coordination and control of multiple robots. Traditionally an unsupervised
learning method, hierarchical clustering offers a formalism for identifying and
representing spatially cohesive and segregated robot groups at different
resolutions by relating the continuous space of configurations to the
combinatorial space of trees. We formalize and exploit this relation,
developing computationally effective reactive algorithms for navigating through
the combinatorial space in concert with geometric realizations for a particular
choice of hierarchical clustering method. These constructions yield
computationally effective vector field planners for both hierarchically
invariant as well as transitional navigation in the configuration space. We
apply these methods to the centralized coordination and control of $n$
perfectly sensed and actuated Euclidean spheres in a $d$-dimensional ambient
space (for arbitrary $n$ and $d$). Given a desired configuration supporting a
desired hierarchy, we construct a hybrid controller which is quadratic in $n$
and algebraic in $d$ and prove that its execution brings all but a measure zero
set of initial configurations to the desired goal with the guarantee of no
collisions along the way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01656</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01656</id><created>2015-07-06</created><authors><author><keyname>Versteeg</keyname><forenames>Steve</forenames></author></authors><title>Languages for Mobile Agents</title><categories>cs.PL</categories><comments>Honours Thesis. Department of Computer Science and Sofware
  Engineering, University of Melbourne. 1997</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile agents represent a new model for network computing. Many different
languages have been used to implement mobile agents. The characteristics that
make a language useful for writing mobile agents are: (1) their support of
agent migration, (2) their support for agent-to-agent communication, (3) how
they allow agents to interact with local resources, (4) security mechanisms,
(5) execution efficiency, (6) language implementation across multiple
platforms, and (7) the language's ease of programming of the tasks mobile
agents perform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01663</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01663</id><created>2015-07-06</created><updated>2015-07-14</updated><authors><author><keyname>Wei</keyname><forenames>Hengfeng</forenames></author><author><keyname>Huang</keyname><forenames>Yu</forenames></author><author><keyname>Cao</keyname><forenames>Jiannong</forenames></author><author><keyname>Lu</keyname><forenames>Jian</forenames></author></authors><title>Almost Strong Consistency: &quot;Good Enough&quot; in Distributed Storage Systems</title><categories>cs.DC cs.DB</categories><comments>17 pages, including 5 pages for appendix; 7 figures; 5 tables; to be
  submitted to VLDB'2016</comments><acm-class>C.2.4; C.4; H.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A consistency/latency tradeoff arises as soon as a distributed storage system
replicates data. For low latency, modern storage systems often settle for weak
consistency conditions, which provide little, or even worse, no guarantee for
data consistency. In this paper we propose the notion of almost strong
consistency as a better balance option for the consistency/latency tradeoff. It
provides both deterministically bounded staleness of data versions for each
read and probabilistic quantification on the rate of &quot;reading stale values&quot;,
while achieving low latency. In the context of distributed storage systems, we
investigate almost strong consistency in terms of 2-atomicity. Our 2AM
(2-Atomicity Maintenance) algorithm completes both reads and writes in one
communication round-trip, and guarantees that each read obtains the value of
within the latest 2 versions. To quantify the rate of &quot;reading stale values&quot;,
we decompose the so-called &quot;old-new inversion&quot; phenomenon into concurrency
patterns and read-write patterns, and propose a stochastic queueing model and a
&quot;timed balls-into-bins model&quot; to analyze them, respectively. The theoretical
analysis not only demonstrates that &quot;old-new inversions&quot; rarely occur as
expected, but also reveals that the read-write pattern dominates in
guaranteeing such rare data inconsistencies. These are further confirmed by the
experimental results, showing that 2-atomicity is &quot;good enough&quot; in distributed
storage systems by achieving low latency, bounded staleness, and rare data
inconsistencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01665</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01665</id><created>2015-07-06</created><updated>2015-07-15</updated><authors><author><keyname>Benmammar</keyname><forenames>Badr</forenames><affiliation>LTT</affiliation></author></authors><title>Allocation de ressources dans un r{\'e}seau de radio cognitive en
  utilisant JADE</title><categories>cs.NI</categories><comments>in French</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we are interested in the negotiation between several PU and
several SU in a cognitive radio context by using multi-agent systems. So, we
have implemented PU coalitions (CPU) and SU coalitions (CSU). We also used the
multi-criteria decision (TOPSIS algorithm) to best meet the needs of
SU.Management by coalitions and the aggregation technique that we have used in
this context significantly reduce the number of messages exchanged in the
network. Processing negotiations simultaneously by several CSU minimizes
response time side SU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01673</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01673</id><created>2015-07-07</created><updated>2016-02-19</updated><authors><author><keyname>Hou</keyname><forenames>Junhui</forenames></author><author><keyname>Chau</keyname><forenames>Lap-Pui</forenames></author><author><keyname>Magnenat-Thalmann</keyname><forenames>Nadia</forenames></author><author><keyname>He</keyname><forenames>Ying</forenames></author></authors><title>SLRMA: Sparse Low-Rank Matrix Approximation for Data Compression</title><categories>cs.MM</categories><comments>11 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low-rank matrix approximation (LRMA) is a powerful technique for signal
processing and pattern analysis. However, its potential for data compression
has not yet been fully investigated in the literature. In this paper, we
propose sparse low-rank matrix approximation (SLRMA), an effective
computational tool for data compression. SLRMA extends the conventional LRMA by
exploring both the intra- and inter-coherence of data samples simultaneously.
With the aid of prescribed orthogonal transforms (e.g., discrete cosine/wavelet
transform and graph transform), SLRMA decomposes a matrix into a product of two
smaller matrices, where one matrix is made of extremely sparse and orthogonal
column vectors, and the other consists of the transform coefficients.
Technically, we formulate SLRMA as a constrained optimization problem, i.e.,
minimizing the approximation error in the least-squares sense regularized by
$\ell_0$-norm and orthogonality, and solve it using the inexact augmented
Lagrangian multiplier method. Through extensive tests on real-world data, such
as 2D image sets and 3D dynamic meshes, we observe that (i) SLRMA empirically
converges well; (ii) SLRMA can produce approximation error comparable to LRMA
but in a much sparse form; (iii) SLRMA-based compression schemes significantly
outperform the state-of-the-art in terms of rate-distortion performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01677</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01677</id><created>2015-07-07</created><updated>2015-11-27</updated><authors><author><keyname>Chauhan</keyname><forenames>Jagmohan</forenames></author><author><keyname>Kaafar</keyname><forenames>Mohamed Ali</forenames></author><author><keyname>Mahanti</keyname><forenames>Anirban</forenames></author></authors><title>The Web for Under-Powered Mobile Devices: Lessons learned from Google
  Glass</title><categories>cs.HC cs.CY</categories><acm-class>C.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines some of the potential challenges associated with enabling
a seamless web experience on underpowered mobile devices such as Google Glass
from the perspective of web content providers, device, and the network. We
conducted experiments to study the impact of webpage complexity, individual web
components and different application layer protocols while accessing webpages
on the performance of Glass browser, by measuring webpage load time,
temperature variation and power consumption and compare it to a smartphone. Our
findings suggest that (a) performance of Glass compared to a smartphone in
terms of power consumption and webpage load time deteriorates with increasing
webpage complexity (b) execution time for popular JavaScript benchmarks is
about 3-8 times higher on Glass compared to a smartphone, (c) WebP is more
energy efficient image format than JPEG and PNG, and (d) seven out of 50
websites studied are optimized for content delivery to Glass.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01685</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01685</id><created>2015-07-07</created><authors><author><keyname>N.</keyname><forenames>Archana</forenames></author><author><keyname>Pawar</keyname><forenames>S. S.</forenames></author></authors><title>Periodicity Detection of Outlier Sequences Using Constraint Based
  Pattern Tree with MAD</title><categories>cs.DB</categories><comments>7 pages, 6 figures</comments><acm-class>H.2.8</acm-class><journal-ref>International Journal of Advanced Studies in Computer Science and
  Engineering (IJASCSE), Volume 4, Issue 6;June 2015; PageNumber: 34-40</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Patterns that appear rarely or unusually in the data can be defined as
outlier patterns. The basic idea behind detecting outlier patterns is
comparison of their relative frequencies with frequent patterns. Their
frequencies of appearance are less and thus have lesser support in the data.
Detecting outlier patterns is an important data mining task which will reveal
some interesting facts. The search for periodicity of patterns gives the
behavior of these patterns across time as to when they repeat likely. This in
turn helps in prediction of events. These patterns are found in Time
series-data, social networks etc. In this paper, an algorithm for periodic
outlier pattern detection is proposed with the usage of a Constraint Based FP
(Frequent Pattern)-tree as the underlying data structure for time series data.
The growth of the tree is limited by using level and monotonic constraints. The
protein sequence of bacteria named E.Coli is collected and periodic outlier
patterns in the sequence are identified. Further the enhancement of results is
obtained by finding the Median Absolute Deviation (MAD) in defining candidate
outlier patterns. The comparative results between STNR-out (Suffix Tree Noise
Resilient for Outlier Detection) and proposed algorithm are illustrated. The
results show the effectiveness and applicability of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01687</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01687</id><created>2015-07-07</created><authors><author><keyname>Dabhi</keyname><forenames>Vipul K.</forenames></author><author><keyname>Chaudhary</keyname><forenames>Sanjay</forenames></author></authors><title>Developing Postfix-GP Framework for Symbolic Regression Problems</title><categories>cs.NE</categories><comments>8 pages, 6 figures</comments><doi>10.1109/ACCT.2015.114</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes Postfix-GP system, postfix notation based Genetic
Programming (GP), for solving symbolic regression problems. It presents an
object-oriented architecture of Postfix-GP framework. It assists the user in
understanding of the implementation details of various components of
Postfix-GP. Postfix-GP provides graphical user interface which allows user to
configure the experiment, to visualize evolved solutions, to analyze GP run,
and to perform out-of-sample predictions. The use of Postfix-GP is demonstrated
by solving the benchmark symbolic regression problem. Finally, features of
Postfix-GP framework are compared with that of other GP systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01688</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01688</id><created>2015-07-07</created><authors><author><keyname>Cohen-Addad</keyname><forenames>Vincent</forenames></author><author><keyname>de Mesmay</keyname><forenames>Arnaud</forenames></author></authors><title>A Fixed Parameter Tractable Approximation Scheme for the Optimal Cut
  Graph of a Surface</title><categories>cs.DS cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a graph $G$ cellularly embedded on a surface $\Sigma$ of genus $g$, a
cut graph is a subgraph of $G$ such that cutting $\Sigma$ along $G$ yields a
topological disk. We provide a fixed parameter tractable approximation scheme
for the problem of computing the shortest cut graph, that is, for any
$\varepsilon &gt;0$, we show how to compute a $(1+ \varepsilon)$ approximation of
the shortest cut graph in time $f(\varepsilon, g)n^3$.
  Our techniques first rely on the computation of a spanner for the problem
using the technique of brick decompositions, to reduce the problem to the case
of bounded tree-width. Then, to solve the bounded tree-width case, we introduce
a variant of the surface-cut decomposition of Ru\'e, Sau and Thilikos, which
may be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01694</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01694</id><created>2015-07-07</created><updated>2016-01-13</updated><authors><author><keyname>You</keyname><forenames>Keyou</forenames></author><author><keyname>Tempo</keyname><forenames>Roberto</forenames></author><author><keyname>Qiu</keyname><forenames>Li</forenames></author></authors><title>Distributed Algorithms for Computation of Centrality Measures in Complex
  Networks</title><categories>cs.SY cs.SI math.OC physics.soc-ph</categories><comments>15 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with distributed computation of several commonly used
centrality measures in complex networks. In particular, we propose
deterministic algorithms, which converge in finite time, for the distributed
computation of the degree, closeness and betweenness centrality measures in
directed graphs. Regarding eigenvector centrality, we consider the PageRank
problem as its typical variant, and design distributed randomized algorithms to
compute PageRank for both fixed and time-varying graphs. A key feature of the
proposed algorithms is that they do not require to know the network size, which
can be simultaneously estimated at every node, and that they are clock-free. To
address the PageRank problem of time-varying graphs, we introduce the novel
concept of persistent graph, which eliminates the effect of spamming nodes.
Moreover, we prove that these algorithms converge almost surely and in the
sense of $L^p$. Finally, the effectiveness of the proposed algorithms is
illustrated via extensive simulations using a classical benchmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01695</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01695</id><created>2015-07-07</created><authors><author><keyname>D'Andrea</keyname><forenames>Annalisa</forenames></author><author><keyname>D'Emidio</keyname><forenames>Mattia</forenames></author><author><keyname>Frigioni</keyname><forenames>Daniele</forenames></author><author><keyname>Leucci</keyname><forenames>Stefano</forenames></author><author><keyname>Proietti</keyname><forenames>Guido</forenames></author></authors><title>Path-Fault-Tolerant Approximate Shortest-Path Trees</title><categories>cs.DS</categories><comments>21 pages, 3 figures, SIROCCO 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G=(V,E)$ be an $n$-nodes non-negatively real-weighted undirected graph.
In this paper we show how to enrich a {\em single-source shortest-path tree}
(SPT) of $G$ with a \emph{sparse} set of \emph{auxiliary} edges selected from
$E$, in order to create a structure which tolerates effectively a \emph{path
failure} in the SPT. This consists of a simultaneous fault of a set $F$ of at
most $f$ adjacent edges along a shortest path emanating from the source, and it
is recognized as one of the most frequent disruption in an SPT. We show that,
for any integer parameter $k \geq 1$, it is possible to provide a very sparse
(i.e., of size $O(kn\cdot f^{1+1/k})$) auxiliary structure that carefully
approximates (i.e., within a stretch factor of $(2k-1)(2|F|+1)$) the true
shortest paths from the source during the lifetime of the failure. Moreover, we
show that our construction can be further refined to get a stretch factor of
$3$ and a size of $O(n \log n)$ for the special case $f=2$, and that it can be
converted into a very efficient \emph{approximate-distance sensitivity oracle},
that allows to quickly (even in optimal time, if $k=1$) reconstruct the
shortest paths (w.r.t. our structure) from the source after a path failure,
thus permitting to perform promptly the needed rerouting operations. Our
structure compares favorably with previous known solutions, as we discuss in
the paper, and moreover it is also very effective in practice, as we assess
through a large set of experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01697</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01697</id><created>2015-07-07</created><authors><author><keyname>Kuhn</keyname><forenames>Tobias</forenames></author><author><keyname>Dumontier</keyname><forenames>Michel</forenames></author></authors><title>Making Digital Artifacts on the Web Verifiable and Reliable</title><categories>cs.CR cs.DL</categories><comments>Extended version of conference paper: arXiv:1401.5775</comments><acm-class>H.3.4; H.3.5</acm-class><doi>10.1109/TKDE.2015.2419657</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The current Web has no general mechanisms to make digital artifacts --- such
as datasets, code, texts, and images --- verifiable and permanent. For digital
artifacts that are supposed to be immutable, there is moreover no commonly
accepted method to enforce this immutability. These shortcomings have a serious
negative impact on the ability to reproduce the results of processes that rely
on Web resources, which in turn heavily impacts areas such as science where
reproducibility is important. To solve this problem, we propose trusty URIs
containing cryptographic hash values. We show how trusty URIs can be used for
the verification of digital artifacts, in a manner that is independent of the
serialization format in the case of structured data files such as
nanopublications. We demonstrate how the contents of these files become
immutable, including dependencies to external digital artifacts and thereby
extending the range of verifiability to the entire reference tree. Our approach
sticks to the core principles of the Web, namely openness and decentralized
architecture, and is fully compatible with existing standards and protocols.
Evaluation of our reference implementations shows that these design goals are
indeed accomplished by our approach, and that it remains practical even for
very large files.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01698</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01698</id><created>2015-07-07</created><authors><author><keyname>Nath</keyname><forenames>Aniruddh</forenames></author><author><keyname>Domingos</keyname><forenames>Pedro</forenames></author></authors><title>Learning Tractable Probabilistic Models for Fault Localization</title><categories>cs.SE cs.LG</categories><comments>Fifth International Workshop on Statistical Relational AI (StaR-AI
  2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, several probabilistic techniques have been applied to
various debugging problems. However, most existing probabilistic debugging
systems use relatively simple statistical models, and fail to generalize across
multiple programs. In this work, we propose Tractable Fault Localization Models
(TFLMs) that can be learned from data, and probabilistically infer the location
of the bug. While most previous statistical debugging methods generalize over
many executions of a single program, TFLMs are trained on a corpus of
previously seen buggy programs, and learn to identify recurring patterns of
bugs. Widely-used fault localization techniques such as TARANTULA evaluate the
suspiciousness of each line in isolation; in contrast, a TFLM defines a joint
probability distribution over buggy indicator variables for each line. Joint
distributions with rich dependency structure are often computationally
intractable; TFLMs avoid this by exploiting recent developments in tractable
probabilistic models (specifically, Relational SPNs). Further, TFLMs can
incorporate additional sources of information, including coverage-based
features such as TARANTULA. We evaluate the fault localization performance of
TFLMs that include TARANTULA scores as features in the probabilistic model. Our
study shows that the learned TFLMs isolate bugs more effectively than previous
statistical methods or using TARANTULA directly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01699</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01699</id><created>2015-07-07</created><authors><author><keyname>Zeng</keyname><forenames>Yong</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Millimeter Wave MIMO with Lens Antenna Array: A New Path Division
  Multiplexing Paradigm</title><categories>cs.IT math.IT</categories><comments>submitted for possible journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Millimeter wave (mmWave) communication is a promising technology for 5G
cellular systems. To compensate for the severe path loss in mmWave systems,
large antenna arrays are generally used to achieve significant beamforming
gains. However, due to the high hardware and power consumption cost associated
with radio frequency (RF) chains, it is desirable to achieve the large-antenna
gains, but with only limited number of RF chains for mmWave communications. To
this end, we study in this paper a new lens antenna array enabled mmWave MIMO
communication system. We first show that the array response of the proposed
lens antenna array at the receiver/transmitter follows a &quot;sinc&quot; function, where
the antenna with the peak response is determined by the angle of arrival
(AoA)/departure (AoD) of the received/transmitted signal. By exploiting this
unique property of lens antenna arrays along with the multi-path sparsity of
mmWave channels, we propose a novel low-cost and capacity-achieving MIMO
transmission scheme, termed \emph{orthogonal path division multiplexing
(OPDM)}. For channels with insufficiently separated AoAs and/or AoDs, we also
propose a simple \emph{path grouping} technique with group-based small-scale
MIMO processing to mitigate the inter-path interference. Numerical results are
provided to compare the performance of the proposed lens antenna arrays for
mmWave MIMO system against that of conventional arrays, under different
practical setups. It is shown that the proposed system achieves significant
throughput gain as well as complexity and hardware cost reduction, both making
it an appealing new paradigm for mmWave MIMO communications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01701</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01701</id><created>2015-07-07</created><authors><author><keyname>Kuhn</keyname><forenames>Tobias</forenames></author></authors><title>A Survey and Classification of Controlled Natural Languages</title><categories>cs.CL</categories><journal-ref>Computational Linguistics, March 2014, Vol. 40, No. 1, pages
  121-170</journal-ref><doi>10.1162/COLI_a_00168</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  What is here called controlled natural language (CNL) has traditionally been
given many different names. Especially during the last four decades, a wide
variety of such languages have been designed. They are applied to improve
communication among humans, to improve translation, or to provide natural and
intuitive representations for formal notations. Despite the apparent
differences, it seems sensible to put all these languages under the same
umbrella. To bring order to the variety of languages, a general classification
scheme is presented here. A comprehensive survey of existing English-based CNLs
is given, listing and describing 100 languages from 1930 until today.
Classification of these languages reveals that they form a single scattered
cloud filling the conceptual space between natural languages such as English on
the one end and formal languages such as propositional logic on the other. The
goal of this article is to provide a common terminology and a common model for
CNL, to contribute to the understanding of their general nature, to provide a
starting point for researchers interested in the area, and to help developers
to make design decisions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01703</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01703</id><created>2015-07-07</created><updated>2015-10-29</updated><authors><author><keyname>Ablinger</keyname><forenames>Jakob</forenames></author></authors><title>Discovering and Proving Infinite Binomial Sums Identities</title><categories>math.NT cs.SC math.CO</categories><comments>25 pages</comments><msc-class>05A10, 68W30, 11M32, 33F05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider binomial and inverse binomial sums at infinity and rewrite them
in terms of a small set of constants, such as powers of $\pi$ or $\log(2)$. In
order to perform these simplifications, we view the series as specializations
of generating series. For these generating series, we derive integral
representations in terms of root-valued iterated integrals. Using
substitutions, we express the interated integrals as cyclotomic harmonic
polylogarithms. Finally, by applying known relations among the cyclotomic
harmonic polylogarithms, we derive expressions in terms of several constants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01706</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01706</id><created>2015-07-07</created><authors><author><keyname>Falk</keyname><forenames>Rainer</forenames></author><author><keyname>Fries</keyname><forenames>Steffen</forenames></author><author><keyname>Hof</keyname><forenames>Hans-Joachim</forenames></author></authors><title>ASIA: An Access Control, Session Invocation and Authorization
  Architecture for Home Energy Appliances in Smart Energy Grid Environments</title><categories>cs.CR</categories><journal-ref>ENERGY 2011: The First International Conference on Smart Grids,
  Green Communications and IT Energy-aware Technologies, pp. 19-26, ISBN:
  978-1-61208-136-6, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the advent of the smart energy grid - an energy transportation and
distribution network being combined with an IT network for its monitoring and
control - information security has gained tremendous importance for energy
distribution and energy automa- tion systems. Integrated security functionality
is crucial to ensure a reliable and continuous operation of the smart energy
grid. Further security related challenges arise from the integration of
millions of smart homes into the smart grid. This paper gives an overview of
the smart energy grid environment and its challenges. Many future use cases are
centered around the smart home, using an ICT gateway. Approaches to protect the
access and data exchange are described, preventing manipulation of ICT gateway
operation. The paper presents ASIA - an Authentication, Session Invocation, and
Authorization component to be used in the smart energy grid, to protect ICT
gateways and to cope with problems like ICT gateway discovery and ICT gateway
addressing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01708</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01708</id><created>2015-07-07</created><authors><author><keyname>Colazzo</keyname><forenames>Dario</forenames></author><author><keyname>Sartiani</keyname><forenames>Carlo</forenames></author></authors><title>Typing Regular Path Query Languages for Data Graphs</title><categories>cs.DB cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regular path query languages for data graphs are essentially \emph{untyped}.
The lack of type information greatly limits the optimization opportunities for
query engines and makes application development more complex. In this paper we
discuss a simple, yet expressive, schema language for edge-labelled data
graphs. This schema language is, then, used to define a query type inference
approach with good precision properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01715</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01715</id><created>2015-07-07</created><updated>2015-07-08</updated><authors><author><keyname>Katz</keyname><forenames>Daniel S.</forenames></author><author><keyname>Choi</keyname><forenames>Sou-Cheng T.</forenames></author><author><keyname>Wilkins-Diehr</keyname><forenames>Nancy</forenames></author><author><keyname>Hong</keyname><forenames>Neil Chue</forenames></author><author><keyname>Venters</keyname><forenames>Colin C.</forenames></author><author><keyname>Howison</keyname><forenames>James</forenames></author><author><keyname>Seinstra</keyname><forenames>Frank</forenames></author><author><keyname>Jones</keyname><forenames>Matthew</forenames></author><author><keyname>Cranston</keyname><forenames>Karen</forenames></author><author><keyname>Clune</keyname><forenames>Thomas L.</forenames></author><author><keyname>de Val-Borro</keyname><forenames>Miguel</forenames></author><author><keyname>Littauer</keyname><forenames>Richard</forenames></author></authors><title>Report on the Second Workshop on Sustainable Software for Science:
  Practice and Experiences (WSSSPE2)</title><categories>cs.SE</categories><doi>10.5334/jors.85</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This technical report records and discusses the Second Workshop on
Sustainable Software for Science: Practice and Experiences (WSSSPE2). The
report includes a description of the alternative, experimental submission and
review process, two workshop keynote presentations, a series of lightning
talks, a discussion on sustainability, and five discussions from the topic
areas of exploring sustainability; software development experiences; credit &amp;
incentives; reproducibility &amp; reuse &amp; sharing; and code testing &amp; code review.
For each topic, the report includes a list of tangible actions that were
proposed and that would lead to potential change. The workshop recognized that
reliance on scientific software is pervasive in all areas of world-leading
research today. The workshop participants then proceeded to explore different
perspectives on the concept of sustainability. Key enablers and barriers of
sustainable scientific software were identified from their experiences. In
addition, recommendations with new requirements such as software credit files
and software prize frameworks were outlined for improving practices in
sustainable software engineering. There was also broad consensus that formal
training in software development or engineering was rare among the
practitioners. Significant strides need to be made in building a sense of
community via training in software and technical practices, on increasing their
size and scope, and on better integrating them directly into graduate education
programs. Finally, journals can define and publish policies to improve
reproducibility, whereas reviewers can insist that authors provide sufficient
information and access to data and software to allow them reproduce the results
in the paper. Hence a list of criteria is compiled for journals to provide to
reviewers so as to make it easier to review software submitted for publication
as a &quot;Software Paper.&quot;
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01716</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01716</id><created>2015-07-07</created><authors><author><keyname>Knight</keyname><forenames>Georgie</forenames></author><author><keyname>Cristadoro</keyname><forenames>Giampaolo</forenames></author><author><keyname>Altmann</keyname><forenames>Eduardo G.</forenames></author></authors><title>Temporal-varying failures of nodes in networks</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>7 pages, 3 figures</comments><journal-ref>Phys. Rev. E 92, 022810 (2015)</journal-ref><doi>10.1103/PhysRevE.92.022810</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider networks in which random walkers are removed because of the
failure of specific nodes. We interpret the rate of loss as a measure of the
importance of nodes, a notion we denote as failure-centrality. We show that the
degree of the node is not sufficient to determine this measure and that, in a
first approximation, the shortest loops through the node have to be taken into
account. We propose approximations of the failure-centrality which are valid
for temporal-varying failures and we dwell on the possibility of externally
changing the relative importance of nodes in a given network, by exploiting the
interference between the loops of a node and the cycles of the temporal pattern
of failures. In the limit of long failure cycles we show analytically that the
escape in a node is larger than the one estimated from a stochastic failure
with the same failure probability. We test our general formalism in two
real-world networks (air-transportation and e-mail users) and show how
communities lead to deviations from predictions for failures in hubs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01719</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01719</id><created>2015-07-07</created><updated>2016-03-01</updated><authors><author><keyname>Bukh</keyname><forenames>Boris</forenames></author><author><keyname>Guruswami</keyname><forenames>Venkatesan</forenames></author><author><keyname>H&#xe5;stad</keyname><forenames>Johan</forenames></author></authors><title>An improved bound on the fraction of correctable deletions</title><categories>cs.IT cs.DM math.CO math.IT</categories><comments>19 pages, this version presents a code with better rate, improves
  exposition, and adds a new author</comments><msc-class>68R15, 94B60, 05D99</msc-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We consider codes over fixed alphabets against worst-case symbol deletions.
For any fixed $k \ge 2$, we construct a family of codes over alphabet of size
$k$ with positive rate, which allow efficient recovery from a worst-case
deletion fraction approaching $1-\frac{2}{k+\sqrt k}$. In particular, for
binary codes, we are able to recover a fraction of deletions approaching
$1/(\sqrt 2 +1)=\sqrt 2-1 \approx 0.414$. Previously, even non-constructively
the largest deletion fraction known to be correctable with positive rate was
$1-\Theta(1/\sqrt{k})$, and around $0.17$ for the binary case.
  Our result pins down the largest fraction of correctable deletions for
$k$-ary codes as $1-\Theta(1/k)$, since $1-1/k$ is an upper bound even for the
simpler model of erasures where the locations of the missing symbols are known.
  Closing the gap between $(\sqrt 2 -1)$ and $1/2$ for the limit of worst-case
deletions correctable by binary codes remains a tantalizing open question.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01728</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01728</id><created>2015-07-07</created><authors><author><keyname>Gorla</keyname><forenames>Elisa</forenames></author><author><keyname>Ravagnani</keyname><forenames>Alberto</forenames></author></authors><title>Equidistant subspace codes</title><categories>cs.IT math.IT</categories><msc-class>11T71, 14G50, 94B60, 51E23, 15A21</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study equidistant subspace codes, i.e. subspace codes with
the property that each two distinct codewords have the same distance. We
provide an almost complete classification of such codes under the assumption
that the cardinality of the ground field is large enough. More precisely, we
prove that for most values of the parameters, an equidistant code of maximum
cardinality is either a sunflower or the orthogonal of a sunflower. We also
study equidistant codes with extremal parameters, and establish general
properties of equidistant codes that are not sunflowers. Finally, we propose a
systematic construction of equidistant codes based on our previous construction
of partial spread codes, and provide an efficient decoding algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01731</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01731</id><created>2015-07-07</created><authors><author><keyname>Ray</keyname><forenames>Kumar Sankar</forenames></author><author><keyname>Mondal</keyname><forenames>Mandrita</forenames></author></authors><title>Prediction of Radiation Fog by DNA Computing</title><categories>q-bio.BM cs.AI cs.ET</categories><comments>36 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a wet lab algorithm for prediction of radiation fog
by DNA computing. The concept of DNA computing is essentially exploited for
generating the classifier algorithm in the wet lab. The classifier is based on
a new concept of similarity based fuzzy reasoning suitable for wet lab
implementation. This new concept of similarity based fuzzy reasoning is
different from conventional approach to fuzzy reasoning based on similarity
measure and also replaces the logical aspect of classical fuzzy reasoning by
DNA chemistry. Thus, we add a new dimension to existing forms of fuzzy
reasoning by bringing it down to nanoscale. We exploit the concept of massive
parallelism of DNA computing by designing this new classifier in the wet lab.
This newly designed classifier is very much generalized in nature and apart
from prediction of radiation fog this methodology can be applied to other types
of data also. To achieve our goal we first fuzzify the given observed
parameters in a form of synthetic DNA sequence which is called fuzzy DNA and
which handles the vague concept of human reasoning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01732</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01732</id><created>2015-07-07</created><updated>2015-07-08</updated><authors><author><keyname>Fiat</keyname><forenames>Amos</forenames></author><author><keyname>Gorelik</keyname><forenames>Ilia</forenames></author><author><keyname>Kaplan</keyname><forenames>Haim</forenames></author><author><keyname>Novgorodov</keyname><forenames>Slava</forenames></author></authors><title>The Temp Secretary Problem</title><categories>cs.DS</categories><msc-class>68Q25</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a generalization of the secretary problem where contracts are
temporary, and for a fixed duration. This models online hiring of temporary
employees, or online auctions for re-usable resources. The problem is related
to the question of Finding a large independent set in a random unit interval
graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01757</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01757</id><created>2015-07-07</created><authors><author><keyname>Galiotto</keyname><forenames>Carlo</forenames></author><author><keyname>Pratas</keyname><forenames>Nuno K.</forenames></author><author><keyname>Doyle</keyname><forenames>Linda</forenames></author><author><keyname>Marchetti</keyname><forenames>Nicola</forenames></author></authors><title>Effect of LOS/NLOS Propagation on Ultra-Dense Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims at investigating the achievable performance and the issues
that arise in ultra-dense networks (UDNs), when the signal propagation includes
both the Line-of-Sight (LOS) and Non-Line-Of-Sight (NLOS) components. Backed by
an analytical stochastic geometry-based model, we study the coverage, the Area
Spectral Efficiency (ASE) and the energy efficiency of UDNs with LOS/NLOS
propagation. We show that when LOS/NLOS propagation components are accounted
for, the network suffers from low coverage and the ASE gain is lower than
linear at high base station densities. However, this performance drop can
partially be attenuated by means of frequency reuse, which is shown to improve
the ASE vs coverage trade-off of cell densification, provided that we have a
degree of freedom on the density of cells. In addition, from an energy
efficiency standpoint, cell densification is shown to be inefficient when both
LOS and NLOS components are taken into account. Overall, based on the findings
of our work that assumes a more advanced system model compared to the current
state-of-the-art, we claim that highly crowded environments of users represent
the worst case scenario for ultra-dense networks. Namely, these are likely to
face serious issues in terms of limited coverage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01759</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01759</id><created>2015-07-07</created><authors><author><keyname>Radkar</keyname><forenames>Anjali N.</forenames></author><author><keyname>Pawar</keyname><forenames>S. S.</forenames></author></authors><title>Mining high on-shelf utility itemsets with negative values from dynamic
  updated database</title><categories>cs.DB</categories><comments>7 pages, 4 figures</comments><acm-class>H.2.8</acm-class><journal-ref>International Journal of advanced studies in Computer Science and
  Engineering IJASCSE, Volume 4, Issue 6, pp.27-33,2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Utility mining emerged to overcome the limitations of frequent itemset mining
by considering the utility of an item. Utility of an item is based on user's
interest or preference. Recently, temporal data mining has become a core
technical data processing technique to deal with changing data. On-shelf
utility mining considers on-shelf time period of item and gets the accurate
utility values of itemsets in temporal database. In traditional on-shelf
utility mining, profits of all items in databases are considered as positive
values. However, in real applications, some items may have negative profit.
Most of the traditional algorithms are used to handle static database. In
practical situations, temporal databases are continually appended or updated.
High on-shelf utility itemsets needs to be updated. Re-running the temporal
mining algorithm every time is ineffective since it neglects previously
discovered itemsets. It repeats the work done previously. In this paper, an
effective algorithm is proposed to find high on-shelf utility itemsets with
negative values from the dynamic updated temporal database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01762</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01762</id><created>2015-07-07</created><updated>2016-02-07</updated><authors><author><keyname>Chau</keyname><forenames>Chi-Kin</forenames></author><author><keyname>Elbassioni</keyname><forenames>Khaled</forenames></author><author><keyname>Khonji</keyname><forenames>Majid</forenames></author></authors><title>Truthful Mechanisms for Combinatorial Allocation of Electric Power in
  Alternating Current Electric Systems for Smart Grid</title><categories>cs.GT</categories><comments>Extended version of AAMAS 14' paper arXiv:1403.3907</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional studies of combinatorial auctions often only consider linear
constraints. The rise of smart grid presents a new class of auctions,
characterized by quadratic constraints. This paper studies the complex-demand
knapsack problem, in which the demands are complex-valued and the capacity of
supplies is described by the magnitude of total complex-valued demand. This
naturally captures the power constraints in alternating current (AC) electric
systems. In this paper, we provide a more complete study and generalize the
problem to the multi-minded version, beyond the previously known
$\frac{1}{2}$-approximation algorithm for only a subclass of the problem. More
precisely, we give a truthful PTAS for the case
$\phi\in[0,\frac{\pi}{2}-\delta]$, and a truthful FPTAS, which fully optimizes
the objective function but violates the capacity constraint by at most
$(1+\epsilon)$, for the case $\phi\in(\frac{\pi}{2},\pi-\delta]$, where $\phi$
is the maximum angle between any two complex-valued demands and
$\epsilon,\delta&gt;0$ are arbitrarily small constants. We complement these
results by showing that, unless P=NP, neither a PTAS can exist for the case
$\phi\in(\frac{\pi}{2},\pi-\delta]$ nor any bi-criteria approximation algorithm
with polynomial guarantees for the case when $\phi$ is arbitrarily close to
$\pi$ (that is, when $\delta$ is arbitrarily close to $0$).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01763</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01763</id><created>2015-07-07</created><updated>2015-10-10</updated><authors><author><keyname>Brimkulov</keyname><forenames>Ulan N.</forenames></author></authors><title>Matrices Whose Inversions are Tridiagonal, Band or Block-Tridiagonal and
  Their Relationship with the Covariance Matrices of a Random Markov Processes
  (Fields)</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article discusses the matrices of the three forms whose inversions are:
tridiagonal matrix, banded matrix or block-tridiagonal matrix and their
relationships with the covariance matrices of measurements of ordinary (simple)
Markov Random Processes (MRP), multiconnected MRP and vector MRP respectively.
Such covariance matrices are frequently occurring in the problems of optimal
filtering, extrapolation and interpolation of MRP and Markov Random Fields
(MRF). It is shown, that the structure of these three forms of matrices has the
same form, but the matrix elements in the first case are scalar quantities; in
the second case matrix elements representing a product of vectors of dimension
m; and in the third case, the off-diagonal elements are the product of matrices
and vectors of dimension m. The properties of such matrices were investigated
and a simple formulas of their inversion was founded. Also computational
efficiency in the storage and inverse of such matrices have been considered. To
illustrate the acquired results an example of the covariance matrix inversions
of two-dimensional MRP is given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01767</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01767</id><created>2015-07-07</created><authors><author><keyname>Elmasry</keyname><forenames>Amr</forenames></author><author><keyname>Kammer</keyname><forenames>Frank</forenames></author></authors><title>Space-Efficient Plane-Sweep Algorithms</title><categories>cs.DS</categories><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce space-efficient plane-sweep algorithms for basic planar
geometric problems. It is assumed that the input is in a read-only array of n
items and that the available workspace is $\Theta(s)$ bits, where $\lg n \leq s
\leq n \cdot \lg n$. In particular, we give an almost-optimal algorithm for
finding the closest pair among a set of $n$ points that runs in $O(n^2/s + n
\cdot \lg s)$ time. We give a simple algorithm to enumerate the intersections
of $n$ line segments whose running time is $O((n^2/s) \cdot \lg^2 s + k)$,
where $k$ is the number of reported intersections. When the segments are
axis-parallel, we give an $O(n^2/s + n \cdot \lg s)$-time algorithm for
counting the intersections, and an algorithm for enumerating the intersections
whose running time is $O((n^2/s) \cdot \lg s \cdot \lg \lg s + n \cdot \lg s +
k)$. We also present space-efficient algorithms to calculate the measure of $n$
axis-parallel rectangles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01768</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01768</id><created>2015-07-07</created><updated>2015-10-13</updated><authors><author><keyname>Haviv</keyname><forenames>Ishay</forenames></author><author><keyname>Regev</keyname><forenames>Oded</forenames></author></authors><title>The Restricted Isometry Property of Subsampled Fourier Matrices</title><categories>cs.DS cs.IT math.IT math.PR</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A matrix $A \in \mathbb{C}^{q \times N}$ satisfies the restricted isometry
property of order $k$ with constant $\varepsilon$ if it preserves the $\ell_2$
norm of all $k$-sparse vectors up to a factor of $1\pm \varepsilon$. We prove
that a matrix $A$ obtained by randomly sampling $q = O(k \cdot \log^2 k \cdot
\log N)$ rows from an $N \times N$ Fourier matrix satisfies the restricted
isometry property of order $k$ with a fixed $\varepsilon$ with high
probability. This improves on Rudelson and Vershynin (Comm. Pure Appl. Math.,
2008), its subsequent improvements, and Bourgain (GAFA Seminar Notes, 2014).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01771</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01771</id><created>2015-07-07</created><authors><author><keyname>Kwon</keyname><forenames>Keehang</forenames></author></authors><title>A New Execution Model for the logic of hereditary Harrop formulas</title><categories>cs.LO</categories><comments>6 pages. arXiv admin note: substantial text overlap with
  arXiv:1211.6535</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The class of first-order Hereditary Harrop formulas ($fohh$) is a
well-established extension of first-order Horn clauses. Its operational
semantics is based on intuitionistic provability.
  We propose another operational semantics for $fohh$ which is based on game
semantics. This new semantics has several interesting aspects: in particular,
it gives a logical status to the $read$ predicate in Prolog.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01773</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01773</id><created>2015-07-07</created><authors><author><keyname>Zhou</keyname><forenames>Huan</forenames></author><author><keyname>Mhedheb</keyname><forenames>Yousri</forenames></author><author><keyname>Idrees</keyname><forenames>Kamran</forenames></author><author><keyname>Glass</keyname><forenames>Colin W.</forenames></author><author><keyname>Gracia</keyname><forenames>Jos&#xe9;</forenames></author><author><keyname>F&#xfc;rlinger</keyname><forenames>Karl</forenames></author><author><keyname>Tao</keyname><forenames>Jie</forenames></author></authors><title>DART-MPI: An MPI-based Implementation of a PGAS Runtime System</title><categories>cs.DC</categories><comments>11 pages, International Conference on Partitioned Global Address
  Space Programming Models (PGAS14)</comments><doi>10.1145/2676870.2676875</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Partitioned Global Address Space (PGAS) approach treats a distributed
system as if the memory were shared on a global level. Given such a global view
on memory, the user may program applications very much like shared memory
systems. This greatly simplifies the tasks of developing parallel applications,
because no explicit communication has to be specified in the program for data
exchange between different computing nodes. In this paper we present DART, a
runtime environment, which implements the PGAS paradigm on large-scale
high-performance computing clusters. A specific feature of our implementation
is the use of one-sided communication of the Message Passing Interface (MPI)
version 3 (i.e. MPI-3) as the underlying communication substrate. We evaluated
the performance of the implementation with several low-level kernels in order
to determine overheads and limitations in comparison to the underlying MPI-3.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01774</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01774</id><created>2015-07-07</created><authors><author><keyname>Zhao</keyname><forenames>Mingxiong</forenames></author><author><keyname>Wang</keyname><forenames>Xiangfeng</forenames></author><author><keyname>Feng</keyname><forenames>Suili</forenames></author></authors><title>Joint Power Splitting and Secure Beamforming Design in the Multiple
  Non-regenerative Wireless-powered Relay Networks</title><categories>cs.IT math.IT</categories><comments>5 pages, 4 figures, accepted by IEEE Communications Letters</comments><doi>10.1109/LCOMM.2015.2453161</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The physical-layer security issue in the multiple non-regenerative
wireless-powered relay (WPR) networks is investigated in this letter, where the
idle relay is treated as a potential eavesdropper. To guarantee secure
communication, the destination-based artificial noise is sent to degrade the
receptions of eavesdroppers, and it also becomes a new source of energy
powering relays to forward the information with power splitting (PS) technique.
We propose an efficient algorithm ground on block-wise penalty function method
to jointly optimize PS ratio and beamforming to maximize the secrecy rate.
Despite the nonconvexity of the considered problem, the proposed algorithm is
numerically efficient and is proved to converge to the local optimal solution.
Simulation results demonstrate that the proposed algorithm outperforms the
benchmark method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01776</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01776</id><created>2015-07-07</created><authors><author><keyname>Powell</keyname><forenames>Robert</forenames></author><author><keyname>Krokhin</keyname><forenames>Andrei</forenames></author></authors><title>A Reduction from Valued CSP to Min Cost Homomorphism Problem for
  Digraphs</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a valued constraint satisfaction problem (VCSP), the goal is to find an
assignment of labels to variables that minimizes a given sum of functions. Each
function in the sum depends on a subset of variables, takes values which are
rational numbers or infinity, and is chosen from a fixed finite set of
functions called a constraint language. The case when all functions take only
values 0 and infinity is known as the constraint satisfaction problem (CSP). It
is known that any CSP with fixed constraint language is polynomial-time
equivalent to one where the constraint language contains a single binary
relation (i.e. a digraph). A recent proof of this by Bulin et al. gives such a
reduction that preserves most of the algebraic properties of the constraint
language that are known to characterize the complexity of the corresponding
CSP. We adapt this proof to the more general setting of VCSP to show that each
VCSP with a fixed finite (valued) constraint language is equivalent to one
where the constraint language consists of one $\{0,\infty\}$-valued binary
function (i.e. a digraph) and one finite-valued unary function, the latter
problem known as the (extended) Minimum Cost Homomorphism Problem for digraphs.
We also show that our reduction preserves some important algebraic properties
of the (valued) constraint language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01777</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01777</id><created>2015-07-07</created><authors><author><keyname>Mandal</keyname><forenames>Swagata</forenames></author><author><keyname>Sau</keyname><forenames>Suman</forenames></author><author><keyname>Chakrabarti</keyname><forenames>Amlan</forenames></author><author><keyname>Saini</keyname><forenames>Jogendra</forenames></author><author><keyname>Pal</keyname><forenames>Sushanta Kumar</forenames></author><author><keyname>Chattopadhyay</keyname><forenames>Subhasish</forenames></author></authors><title>FPGA based Novel High Speed DAQ System Design with Error Correction</title><categories>cs.AR</categories><comments>ISVLSI 2015. arXiv admin note: substantial text overlap with
  arXiv:1505.04569, arXiv:1503.08819</comments><report-no>01A</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Present state of the art applications in the area of high energy physics
experiments (HEP), radar communication, satellite communication and bio medical
instrumentation require fault resilient data acquisition (DAQ) system with the
data rate in the order of Gbps. In order to keep the high speed DAQ system
functional in such radiation environment where direct intervention of human is
not possible, a robust and error free communication system is necessary. In
this work we present an efficient DAQ design and its implementation on field
programmable gate array (FPGA). The proposed DAQ system supports high speed
data communication (~4.8 Gbps) and achieves multi-bit error correction
capabilities. BCH code (named after Raj Bose and D. K. RayChaudhuri) has been
used for multi-bit error correction. The design has been implemented on Xilinx
Kintex-7 board and is tested for board to board communication as well as for
board to PC using PCIe (Peripheral Component Interconnect express) interface.
To the best of our knowledge, the proposed FPGA based high speed DAQ system
utilizing optical link and multi-bit error resiliency can be considered first
of its kind. Performance estimation of the implemented DAQ system is done based
on resource utilization, critical path delay, efficiency and bit error rate
(BER).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01780</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01780</id><created>2015-07-07</created><authors><author><keyname>Yao</keyname><forenames>Chuting</forenames></author><author><keyname>Yang</keyname><forenames>Chenyang</forenames></author><author><keyname>Xiong</keyname><forenames>Zixiang</forenames></author></authors><title>Energy-saving Resource Allocation by Exploiting the Context Information</title><categories>cs.IT cs.NI math.IT</categories><comments>To be presented at IEEE PIMRC 2015, Hong Kong. This work was
  supported by National Natural Science Foundation of China under Grant
  61120106002 and National Basic Research Program of China under Grant
  2012CB316003</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Improving energy efficiency of wireless systems by exploiting the context
information has received attention recently as the smart phone market keeps
expanding. In this paper, we devise energy-saving resource allocation policy
for multiple base stations serving non-real-time traffic by exploiting three
levels of context information, where the background traffic is assumed to
occupy partial resources. Based on the solution from a total energy
minimization problem with perfect future information,a context-aware BS
sleeping, scheduling and power allocation policy is proposed by estimating the
required future information with three levels of context information.
Simulation results show that our policy provides significant gains over those
without exploiting any context information. Moreover, it is seen that different
levels of context information play different roles in saving energy and
reducing outage in transmission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01784</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01784</id><created>2015-07-07</created><updated>2015-11-05</updated><authors><author><keyname>Podosinnikova</keyname><forenames>Anastasia</forenames></author><author><keyname>Bach</keyname><forenames>Francis</forenames></author><author><keyname>Lacoste-Julien</keyname><forenames>Simon</forenames></author></authors><title>Rethinking LDA: moment matching for discrete ICA</title><categories>stat.ML cs.LG</categories><comments>30 pages; added plate diagrams and clarifications, changed style,
  corrected typos, updated figures. in Proceedings of the 29-th Conference on
  Neural Information Processing Systems (NIPS), 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider moment matching techniques for estimation in Latent Dirichlet
Allocation (LDA). By drawing explicit links between LDA and discrete versions
of independent component analysis (ICA), we first derive a new set of
cumulant-based tensors, with an improved sample complexity. Moreover, we reuse
standard ICA techniques such as joint diagonalization of tensors to improve
over existing methods based on the tensor power method. In an extensive set of
experiments on both synthetic and real datasets, we show that our new
combination of tensors and orthogonal joint diagonalization techniques
outperforms existing moment matching methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01796</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01796</id><created>2015-07-07</created><updated>2015-07-11</updated><authors><author><keyname>Wang</keyname><forenames>Jingying</forenames></author><author><keyname>Liu</keyname><forenames>Tianli</forenames></author><author><keyname>Zhu</keyname><forenames>Tingshao</forenames></author><author><keyname>Zhang</keyname><forenames>Lei</forenames></author><author><keyname>Hao</keyname><forenames>Bibo</forenames></author><author><keyname>Chen</keyname><forenames>Zhenxiang</forenames></author></authors><title>Classify Sina Weibo users into High or Low happiness Groups Using
  Linguistic and Behavior Features</title><categories>cs.SI</categories><comments>12 pages, 5 tables, 1 figures, typo fixed, the indexs in table 2
  revised, four authors added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It's of great importance to measure happiness of social network users, but
the existing method based on questionnaires suffers from high costs and low
efficiency. This paper aims at identifying social network users' happiness
level based on their Web behavior. We recruited 548 participants to fill in the
Oxford Happiness Inventory (OHI) and divided them into two groups with high/low
OHI score. We downloaded each Weibo user's data by calling API, and extracted
103 linguistic and behavior features. 24 features are identified with
significant difference between high and low happiness groups. We trained a
Decision Tree on these 24 features to make the prediction of high/low happiness
group. The decision tree can be used to identify happiness level of any new
social network user based on linguistic and behavior features. The Decision
Tree can achieve 67.7% on precision. Although the capability of our Decision
Tree is not ideal, classifying happiness via linguistic and behavior features
on the Internet is proved to be feasible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01818</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01818</id><created>2015-07-07</created><authors><author><keyname>Mehta</keyname><forenames>Punit</forenames></author><author><keyname>Muthu</keyname><forenames>Rahul</forenames></author><author><keyname>Patel</keyname><forenames>Gaurav</forenames></author><author><keyname>Thakkar</keyname><forenames>Om</forenames></author><author><keyname>Vyas</keyname><forenames>Devanshi</forenames></author></authors><title>Improved Upper Bounds on $a'(G\Box H)$</title><categories>math.CO cs.DM</categories><comments>10 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The acyclic edge colouring problem is extensively studied in graph theory.
The corner-stone of this field is a conjecture of Alon et.
al.\cite{alonacyclic} that $a'(G)\le \Delta(G)+2$. In that and subsequent work,
$a'(G)$ is typically bounded in terms of $\Delta(G)$. Motivated by this we
introduce a term $gap(G)$ defined as $gap(G)=a'(G)-\Delta(G)$. Alon's
conjecture can be rephrased as $gap(G)\le2$ for all graphs $G$. In
\cite{manusccartprod} it was shown that $a'(G\Box H)\le a'(G)+a'(H)$, under
some assumptions. Based on Alon's conjecture, we conjecture that $a'(G\Box
H)\le a'(G)+\Delta(H)$ under the same assumptions, resulting in a
strengthening. The results of \cite{alonacyclic} validate our conjecture for
the class of graphs it considers. We prove our conjecture for a significant
subclass of sub-cubic graphs and state some generic conditions under which our
conjecture can be proved. We suggest how our technique can be potentially
applied by future researchers to expand the class of graphs for which our
conjecture holds. Our results improve the understanding of the relationship
between Cartesian Product and acyclic chromatic index.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01826</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01826</id><created>2015-07-07</created><updated>2015-10-09</updated><authors><author><keyname>Stanley</keyname><forenames>Natalie</forenames></author><author><keyname>Shai</keyname><forenames>Saray</forenames></author><author><keyname>Taylor</keyname><forenames>Dane</forenames></author><author><keyname>Mucha</keyname><forenames>Peter J.</forenames></author></authors><title>Clustering Network Layers With the Strata Multilayer Stochastic Block
  Model</title><categories>cs.SI physics.soc-ph stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multilayer networks are a useful data structure for simultaneously capturing
multiple types of relationships between a set of nodes. In such networks, each
relational definition gives rise to a layer. While each layer provides its own
set of information, community structure across layers can be collectively
utilized to discover and quantify underlying relational patterns between nodes.
To concisely extract information from a multilayer network, we propose to
identify and combine sets of layers with meaningful similarities in community
structure. In this paper, we describe the &quot;strata multilayer stochastic block
model'' (sMLSBM), a probabilistic model for multilayer community structure. The
central extension of the model is that there exist groups of layers, called
&quot;strata'', which are defined such that all layers in a given stratum have
community structure described by a common stochastic block model (SBM). That
is, layers in a stratum exhibit similar node-to-community assignments and SBM
probability parameters. Fitting the sMLSBM to a multilayer network provides a
joint clustering that yields node-to-community and layer-to-stratum
assignments, which cooperatively aid one another during inference. We describe
an algorithm for separating layers into their appropriate strata and an
inference technique for estimating the SBM parameters for each stratum. We
demonstrate our method using synthetic networks and a multilayer network
inferred from data collected in the Human Microbiome Project.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01837</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01837</id><created>2015-07-07</created><authors><author><keyname>Zuzek</keyname><forenames>Lucila G. Alvarez</forenames></author><author><keyname>Buono</keyname><forenames>Camila</forenames></author><author><keyname>Braunstein</keyname><forenames>Lidia A.</forenames></author></authors><title>Epidemic spreading and immunization strategy in multiplex networks</title><categories>physics.soc-ph cs.SI</categories><comments>9 pages, 2 figures, accepted in Journal of Physics: Conference Series
  (JPCS)</comments><doi>10.1088/1742-6596/640/1/012007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A more connected world has brought major consequences such as facilitate the
spread of diseases all over the world to quickly become epidemics, reason why
researchers are concentrated in modeling the propagation of epidemics and
outbreaks in Multilayer Networks. In this networks all nodes interact in
different layers with different type of links. However, in many scenarios such
as in the society, a Multiplex Network framework is not completely suitable
since not all individuals participate in all layers. In this paper, we use a
partially overlapped Multiplex Network where only a fraction of the individuals
are shared by the layers. We develop a mitigation strategy for stopping a
disease propagation, considering the Susceptible-Infected-Recover model, in a
system consisted by two layers. We consider a random immunization in one of the
layers and study the effect of the overlapping fraction in both, the
propagation of the disease and the immunization strategy. Using branching
theory, we study this scenario theoretically and via simulations and find a
lower epidemic threshold than in the case without strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01839</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01839</id><created>2015-07-07</created><updated>2015-08-03</updated><authors><author><keyname>Ma</keyname><forenames>Mingbo</forenames></author><author><keyname>Huang</keyname><forenames>Liang</forenames></author><author><keyname>Xiang</keyname><forenames>Bing</forenames></author><author><keyname>Zhou</keyname><forenames>Bowen</forenames></author></authors><title>Dependency-based Convolutional Neural Networks for Sentence Embedding</title><categories>cs.CL cs.AI cs.LG</categories><comments>this paper has been accepted by ACL 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In sentence modeling and classification, convolutional neural network
approaches have recently achieved state-of-the-art results, but all such
efforts process word vectors sequentially and neglect long-distance
dependencies. To exploit both deep learning and linguistic structures, we
propose a tree-based convolutional neural network model which exploit various
long-distance relationships between words. Our model improves the sequential
baselines on all three sentiment and question classification tasks, and
achieves the highest published accuracy on TREC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01845</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01845</id><created>2015-07-07</created><authors><author><keyname>Su</keyname><forenames>Lili</forenames></author><author><keyname>Vaidya</keyname><forenames>Nitin</forenames></author></authors><title>Byzantine Multi-Agent Optimization: Part II</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Part I of this report, we introduced a Byzantine fault-tolerant
distributed optimization problem whose goal is to optimize a sum of convex
(cost) functions with real-valued scalar input/ouput. In this second part, we
introduce a condition-based variant of the original problem over arbitrary
directed graphs. Specifically, for a given collection of $k$ input functions
$h_1(x), \ldots, h_k(x)$, we consider the scenario when the local cost function
stored at agent $j$, denoted by $g_j(x)$, is formed as a convex combination of
the $k$ input functions $h_1(x), \ldots, h_k(x)$. The goal of this
condition-based problem is to generate an output that is an optimum of
$\frac{1}{k}\sum_{i=1}^k h_i(x)$. Depending on the availability of side
information at each agent, two slightly different variants are considered. We
show that for a given graph, the problem can indeed be solved despite the
presence of faulty agents. In particular, even in the absence of side
information at each agent, when adequate redundancy is available in the optima
of input functions, a distributed algorithm is proposed in which each agent
carries minimal state across iterations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01859</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01859</id><created>2015-07-07</created><authors><author><keyname>Gaujal</keyname><forenames>Bruno</forenames></author><author><keyname>Mertikopoulos</keyname><forenames>Panayotis</forenames></author></authors><title>A stochastic approximation algorithm for stochastic semidefinite
  programming</title><categories>math.OC cs.GT cs.IT math.IT</categories><comments>25 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by applications to multi-antenna wireless networks, we propose a
distributed and asynchronous algorithm for stochastic semidefinite programming.
This algorithm is a stochastic approximation of a continous- time matrix
exponential scheme regularized by the addition of an entropy-like term to the
problem's objective function. We show that the resulting algorithm converges
almost surely to an $\varepsilon$-approximation of the optimal solution
requiring only an unbiased estimate of the gradient of the problem's stochastic
objective. When applied to throughput maximization in wireless multiple-input
and multiple-output (MIMO) systems, the proposed algorithm retains its
convergence properties under a wide array of mobility impediments such as user
update asynchronicities, random delays and/or ergodically changing channels.
Our theoretical analysis is complemented by extensive numerical simulations
which illustrate the robustness and scalability of the proposed method in
realistic network conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01882</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01882</id><created>2015-07-07</created><authors><author><keyname>Shrager</keyname><forenames>Jeff</forenames></author></authors><title>Demandance</title><categories>cs.HC</categories><comments>10 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A demandance is a psychological &quot;pull&quot; exerted by a stimulus. It is closely
related to the theory of &quot;affordance&quot;. I introduce the theory of demandance,
offer some motivating examples, briefly explore its psychological basis, and
examine some implications of the theory. I exemplify some of the positive and
negative implications of demandances for design, with special attention to
young children and the design of educational products and practices. I suggest
that demandance offers an approach to one of the persistent mysteries of the
theory of affordance, specifically: Given that there may be many affordances in
any particular setting, how do we choose which to actually act upon?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01888</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01888</id><created>2015-07-05</created><authors><author><keyname>Harrison</keyname><forenames>Robert J.</forenames></author><author><keyname>Beylkin</keyname><forenames>Gregory</forenames></author><author><keyname>Bischoff</keyname><forenames>Florian A.</forenames></author><author><keyname>Calvin</keyname><forenames>Justus A.</forenames></author><author><keyname>Fann</keyname><forenames>George I.</forenames></author><author><keyname>Fosso-Tande</keyname><forenames>Jacob</forenames></author><author><keyname>Galindo</keyname><forenames>Diego</forenames></author><author><keyname>Hammond</keyname><forenames>Jeff R.</forenames></author><author><keyname>Hartman-Baker</keyname><forenames>Rebecca</forenames></author><author><keyname>Hill</keyname><forenames>Judith C.</forenames></author><author><keyname>Jia</keyname><forenames>Jun</forenames></author><author><keyname>Kottmann</keyname><forenames>Jakob S.</forenames></author><author><keyname>Ou</keyname><forenames>M-J. Yvonne</forenames></author><author><keyname>Ratcliff</keyname><forenames>Laura E.</forenames></author><author><keyname>Reuter</keyname><forenames>Matthew G.</forenames></author><author><keyname>Richie-Halford</keyname><forenames>Adam C.</forenames></author><author><keyname>Romero</keyname><forenames>Nichols A.</forenames></author><author><keyname>Sekino</keyname><forenames>Hideo</forenames></author><author><keyname>Shelton</keyname><forenames>William A.</forenames></author><author><keyname>Sundahl</keyname><forenames>Bryan E.</forenames></author><author><keyname>Thornton</keyname><forenames>W. Scott</forenames></author><author><keyname>Valeev</keyname><forenames>Edward F.</forenames></author><author><keyname>V&#xe1;zquez-Mayagoitia</keyname><forenames>&#xc1;lvaro</forenames></author><author><keyname>Vence</keyname><forenames>Nicholas</forenames></author><author><keyname>Yokoi</keyname><forenames>Yukina</forenames></author></authors><title>MADNESS: A Multiresolution, Adaptive Numerical Environment for
  Scientific Simulation</title><categories>cs.MS cs.CE math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MADNESS (multiresolution adaptive numerical environment for scientific
simulation) is a high-level software environment for solving integral and
differential equations in many dimensions that uses adaptive and fast harmonic
analysis methods with guaranteed precision based on multiresolution analysis
and separated representations. Underpinning the numerical capabilities is a
powerful petascale parallel programming environment that aims to increase both
programmer productivity and code scalability. This paper describes the features
and capabilities of MADNESS and briefly discusses some current applications in
chemistry and several areas of physics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01889</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01889</id><created>2015-06-19</created><authors><author><keyname>Lellouch</keyname><forenames>Gabriel</forenames></author><author><keyname>Mishra</keyname><forenames>Amit Kumar</forenames></author><author><keyname>Inggs</keyname><forenames>Michael</forenames></author></authors><title>Design of OFDM radar pulses using genetic algorithm based techniques</title><categories>cs.NE</categories><comments>IN PRESS with TAES</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The merit of evolutionary algorithms (EA) to solve convex optimization
problems is widely acknowledged. In this paper, a genetic algorithm (GA)
optimization based waveform design framework is used to improve the features of
radar pulses relying on the orthogonal frequency division multiplexing (OFDM)
structure. Our optimization techniques focus on finding optimal phase code
sequences for the OFDM signal. Several optimality criteria are used since we
consider two different radar processing solutions which call either for single
or multiple-objective optimizations. When minimization of the so-called
peak-to-mean envelope power ratio (PMEPR) single-objective is tackled, we
compare our findings with existing methods and emphasize on the merit of our
approach. In the scope of the two-objective optimization, we first address
PMEPR and peak-to-sidelobe level ratio (PSLR) and show that our approach based
on the non-dominated sorting genetic algorithm-II (NSGA-II) provides design
solutions with noticeable improvements as opposed to random sets of phase
codes. We then look at another case of interest where the objective functions
are two measures of the sidelobe level, namely PSLR and the integrated-sidelobe
level ratio (ISLR) and propose to modify the NSGA-II to include a constrain on
the PMEPR instead. In the last part, we illustrate via a case study how our
encoding solution makes it possible to minimize the single objective PMEPR
while enabling a target detection enhancement strategy, when the SNR metric
would be chosen for the detection framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01890</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01890</id><created>2015-07-06</created><updated>2015-07-20</updated><authors><author><keyname>Kuncheva</keyname><forenames>Zhana</forenames></author><author><keyname>Montana</keyname><forenames>Giovanni</forenames></author></authors><title>Community detection in multiplex networks using locally adaptive random
  walks</title><categories>cs.SI physics.soc-ph stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiplex networks, a special type of multilayer networks, are increasingly
applied in many domains ranging from social media analytics to biology. A
common task in these applications concerns the detection of community
structures. Many existing algorithms for community detection in multiplexes
attempt to detect communities which are shared by all layers. In this article
we propose a community detection algorithm, LART (Locally Adaptive Random
Transitions), for the detection of communities that are shared by either some
or all the layers in the multiplex. The algorithm is based on a random walk on
the multiplex, and the transition probabilities defining the random walk are
allowed to depend on the local topological similarity between layers at any
given node so as to facilitate the exploration of communities across layers.
Based on this random walk, a node dissimilarity measure is derived and nodes
are clustered based on this distance in a hierarchical fashion. We present
experimental results using networks simulated under various scenarios to
showcase the performance of LART in comparison to related community detection
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01892</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01892</id><created>2015-07-06</created><authors><author><keyname>Montalto</keyname><forenames>Alessandro</forenames></author><author><keyname>Tessitore</keyname><forenames>Giovanni</forenames></author><author><keyname>Prevete</keyname><forenames>Roberto</forenames></author></authors><title>A linear approach for sparse coding by a two-layer neural network</title><categories>cs.LG physics.data-an</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many approaches to transform classification problems from non-linear to
linear by feature transformation have been recently presented in the
literature. These notably include sparse coding methods and deep neural
networks. However, many of these approaches require the repeated application of
a learning process upon the presentation of unseen data input vectors, or else
involve the use of large numbers of parameters and hyper-parameters, which must
be chosen through cross-validation, thus increasing running time dramatically.
In this paper, we propose and experimentally investigate a new approach for the
purpose of overcoming limitations of both kinds. The proposed approach makes
use of a linear auto-associative network (called SCNN) with just one hidden
layer. The combination of this architecture with a specific error function to
be minimized enables one to learn a linear encoder computing a sparse code
which turns out to be as similar as possible to the sparse coding that one
obtains by re-training the neural network. Importantly, the linearity of SCNN
and the choice of the error function allow one to achieve reduced running time
in the learning phase. The proposed architecture is evaluated on the basis of
two standard machine learning tasks. Its performances are compared with those
of recently proposed non-linear auto-associative neural networks. The overall
results suggest that linear encoders can be profitably used to obtain sparse
data representations in the context of machine learning problems, provided that
an appropriate error function is used during the learning phase.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01894</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01894</id><created>2015-07-07</created><updated>2015-07-08</updated><authors><author><keyname>Iliev</keyname><forenames>Oleg</forenames></author><author><keyname>Lakdawala</keyname><forenames>Zahra</forenames></author><author><keyname>Leonard</keyname><forenames>Katherine</forenames></author><author><keyname>Vutov</keyname><forenames>Yavor</forenames></author></authors><title>On pore-scale modeling and simulation of reactive transport in 3D
  geometries</title><categories>cs.CE</categories><comments>15 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pore-scale modeling and simulation of reactive flow in porous media has a
range of diverse applications, and poses a number of research challenges. It is
known that the morphology of a porous medium has significant influence on the
local flow rate, which can have a substantial impact on the rate of chemical
reactions. While there are a large number of papers and software tools
dedicated to simulating either fluid flow in 3D computerized tomography (CT)
images or reactive flow using pore-network models, little attention to date has
been focused on the pore-scale simulation of sorptive transport in 3D CT
images, which is the specific focus of this paper. Here we first present an
algorithm for the simulation of such reactive flows directly on images, which
is implemented in a sophisticated software package. We then use this software
to present numerical results in two resolved geometries, illustrating the
importance of pore-scale simulation and the flexibility of our software
package.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01902</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01902</id><created>2015-07-07</created><authors><author><keyname>JavadiAbhari</keyname><forenames>Ali</forenames></author><author><keyname>Patil</keyname><forenames>Shruti</forenames></author><author><keyname>Kudrow</keyname><forenames>Daniel</forenames></author><author><keyname>Heckey</keyname><forenames>Jeff</forenames></author><author><keyname>Lvov</keyname><forenames>Alexey</forenames></author><author><keyname>Chong</keyname><forenames>Frederic T.</forenames></author><author><keyname>Martonosi</keyname><forenames>Margaret</forenames></author></authors><title>ScaffCC: Scalable Compilation and Analysis of Quantum Programs</title><categories>quant-ph cs.PL</categories><comments>Journal of Parallel Computing (PARCO)</comments><journal-ref>Parallel Comput. 45, C (June 2015), 2-17</journal-ref><doi>10.1016/j.parco.2014.12.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present ScaffCC, a scalable compilation and analysis framework based on
LLVM, which can be used for compiling quantum computing applications at the
logical level. Drawing upon mature compiler technologies, we discuss
similarities and differences between compilation of classical and quantum
programs, and adapt our methods to optimizing the compilation time and output
for the quantum case. Our work also integrates a reversible-logic synthesis
tool in the compiler to facilitate coding of quantum circuits. Lastly, we
present some useful quantum program analysis scenarios and discuss their
implications, specifically with an elaborate discussion of timing analysis for
critical path estimation. Our work focuses on bridging the gap between
high-level quantum algorithm specifi- cations and low-level physical
implementations, while providing good scalability to larger and more
interesting problems
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01906</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01906</id><created>2015-07-07</created><authors><author><keyname>Bazzi</keyname><forenames>Abbas</forenames></author><author><keyname>Norouzi-Fard</keyname><forenames>Ashkan</forenames></author></authors><title>Towards Tight Lower Bounds for Scheduling Problems</title><categories>cs.CC</categories><comments>25 pages, 3 figures, To appear in the Proceedings of the 23rd Annual
  European Symposium on Algorithms 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show a close connection between structural hardness for $k$-partite graphs
and tight inapproximability results for scheduling problems with precedence
constraints. Assuming a natural but nontrivial generalisation of the bipartite
structural hardness result of Bansal and Khot, we obtain a hardness of
$2-\epsilon$ for the problem of minimising the makespan for scheduling
precedence-constrained jobs with preemption on identical parallel machines.
This matches the best approximation guarantee for this problem. Assuming the
same hypothesis, we also obtain a super constant inapproximability result for
the problem of scheduling precedence-constrained jobs on related parallel
machines, making progress towards settling an open question in both lists of
ten open questions by Williamson and Shmoys, and by Schuurman and Woeginger.
  The study of structural hardness of $k$-partite graphs is of independent
interest, as it captures the intrinsic hardness for a large family of
scheduling problems. Other than the ones already mentioned, this generalisation
also implies tight inapproximability to the problem of minimising the weighted
completion time for precedence-constrained jobs on a single machine, and the
problem of minimising the makespan of precedence-constrained jobs on identical
parallel machine, and hence unifying the results of Bansal and Khot, and
Svensson, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01917</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01917</id><created>2015-07-07</created><updated>2015-07-08</updated><authors><author><keyname>Grochow</keyname><forenames>Joshua A.</forenames></author><author><keyname>Qiao</keyname><forenames>Youming</forenames></author></authors><title>Polynomial-time isomorphism test of groups that are tame extensions</title><categories>cs.DS cs.CC math.GR math.RT</categories><comments>23 pages</comments><msc-class>20C40, 20C20, 20J06, 68Q25</msc-class><acm-class>I.1.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give new polynomial-time algorithms for testing isomorphism of a class of
groups given by multiplication tables (GpI). Two results (Cannon &amp; Holt, J.
Symb. Comput. 2003; Babai, Codenotti &amp; Qiao, ICALP 2012) imply that GpI reduces
to the following: given groups G, H with characteristic subgroups of the same
type and isomorphic to $\mathbb{Z}_p^d$, and given the coset of isomorphisms
$Iso(G/\mathbb{Z}_p^d, H/\mathbb{Z}_p^d)$, compute Iso(G, H) in time poly(|G|).
Babai &amp; Qiao (STACS 2012) solved this problem when a Sylow p-subgroup of
$G/\mathbb{Z}_p^d$ is trivial. In this paper, we solve the preceding problem in
the so-called &quot;tame&quot; case, i.e., when a Sylow p-subgroup of $G/\mathbb{Z}_p^d$
is cyclic, dihedral, semi-dihedral, or generalized quaternion. These cases
correspond exactly to the group algebra
$\overline{\mathbb{F}}_p[G/\mathbb{Z}_p^d]$ being of tame type, as in the
celebrated tame-wild dichotomy in representation theory. We then solve new
cases of GpI in polynomial time.
  Our result relies crucially on the divide-and-conquer strategy proposed
earlier by the authors (CCC 2014), which splits GpI into two problems, one on
group actions (representations), and one on group cohomology. Based on this
strategy, we combine permutation group and representation algorithms with new
mathematical results, including bounds on the number of indecomposable
representations of groups in the tame case, and on the size of their cohomology
groups.
  Finally, we note that when a group extension is not tame, the preceding
bounds do not hold. This suggests a precise sense in which the tame-wild
dichotomy from representation theory may also be a dividing line between the
(currently) easy and hard instances of GpI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01922</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01922</id><created>2015-07-07</created><authors><author><keyname>Nunes</keyname><forenames>Eric</forenames></author><author><keyname>Kulkarni</keyname><forenames>Nimish</forenames></author><author><keyname>Shakarian</keyname><forenames>Paulo</forenames></author><author><keyname>Ruef</keyname><forenames>Andrew</forenames></author><author><keyname>Little</keyname><forenames>Jay</forenames></author></authors><title>Cyber-Deception and Attribution in Capture-the-Flag Exercises</title><categories>cs.CR</categories><comments>4 pages Short name accepted to FOSINT-SI 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Attributing the culprit of a cyber-attack is widely considered one of the
major technical and policy challenges of cyber-security. The lack of ground
truth for an individual responsible for a given attack has limited previous
studies. Here, we overcome this limitation by leveraging DEFCON
capture-the-flag (CTF) exercise data where the actual ground-truth is known. In
this work, we use various classification techniques to identify the culprit in
a cyberattack and find that deceptive activities account for the majority of
misclassified samples. We also explore several heuristics to alleviate some of
the misclassification caused by deception.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01926</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01926</id><created>2015-07-07</created><updated>2015-07-23</updated><authors><author><keyname>Baumstark</keyname><forenames>Niklas</forenames></author><author><keyname>Blelloch</keyname><forenames>Guy</forenames></author><author><keyname>Shun</keyname><forenames>Julian</forenames></author></authors><title>Efficient Implementation of a Synchronous Parallel Push-Relabel
  Algorithm</title><categories>cs.DS cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the observation that FIFO-based push-relabel algorithms are able
to outperform highest label-based variants on modern, large maximum flow
problem instances, we introduce an efficient implementation of the algorithm
that uses coarse-grained parallelism to avoid the problems of existing parallel
approaches. We demonstrate good relative and absolute speedups of our algorithm
on a set of large graph instances taken from real-world applications. On a
modern 40-core machine, our parallel implementation outperforms existing
sequential implementations by up to a factor of 12 and other parallel
implementations by factors of up to 3.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01929</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01929</id><created>2015-07-07</created><updated>2015-11-22</updated><authors><author><keyname>da Silva</keyname><forenames>Thiago Ferreira</forenames></author><author><keyname>Amaral</keyname><forenames>Gustavo C.</forenames></author><author><keyname>Tempor&#xe3;o</keyname><forenames>Guilherme P.</forenames></author><author><keyname>von der Weid</keyname><forenames>Jean Pierre</forenames></author></authors><title>Linear-Optic Heralded Photon Source</title><categories>quant-ph cs.CR</categories><comments>9 pages, 7 figures</comments><journal-ref>Phys. Rev. A 92, 033855 (2015)</journal-ref><doi>10.1103/PhysRevA.92.033855</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a Heralded Photon Source based only on linear optics and weak
coherent states. By time-tuning a Hong-Ou-Mandel interferometer fed with
frequency-displaced coherent states, the output photons can be synchronously
heralded following sub-Poisson statistics, which is indicated by the
second-order correlation function ($g^2\left(0\right)=0.556$). The absence of
phase-matching restrictions makes the source widely tunable, with 100-nm
spectral tunability on the telecom bands. The technique presents yield
comparable to state-of-the-art spontaneous parametric down-conversion-based
sources, with high coherence and fiber-optic quantum communication
compatibility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01930</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01930</id><created>2015-07-07</created><authors><author><keyname>Nunes</keyname><forenames>Eric</forenames></author><author><keyname>Buto</keyname><forenames>Casey</forenames></author><author><keyname>Shakarian</keyname><forenames>Paulo</forenames></author><author><keyname>Lebiere</keyname><forenames>Christian</forenames></author><author><keyname>Bennati</keyname><forenames>Stefano</forenames></author><author><keyname>Thomson</keyname><forenames>Robert</forenames></author><author><keyname>Jaenisch</keyname><forenames>Holger</forenames></author></authors><title>Malware Task Identification: A Data Driven Approach</title><categories>cs.CR</categories><comments>8 pages full paper, accepted FOSINT-SI (2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identifying the tasks a given piece of malware was designed to perform (e.g.
logging keystrokes, recording video, establishing remote access, etc.) is a
difficult and time-consuming operation that is largely human-driven in
practice. In this paper, we present an automated method to identify malware
tasks. Using two different malware collections, we explore various
circumstances for each - including cases where the training data differs
significantly from test; where the malware being evaluated employs packing to
thwart analytical techniques; and conditions with sparse training data. We find
that this approach consistently out-performs the current state-of-the art
software for malware task identification as well as standard machine learning
approaches - often achieving an unbiased F1 score of over 0.9. In the near
future, we look to deploy our approach for use by analysts in an operational
cyber-security environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01931</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01931</id><created>2015-07-07</created><updated>2015-07-18</updated><authors><author><keyname>Foroutan</keyname><forenames>Vahid</forenames></author><author><keyname>Majumdar</keyname><forenames>Ratul</forenames></author></authors><title>Scalability of Controlling Heterogenous Stress-Engineered MEMS
  Microrobots (MicroStressBots) through Common Control Signal using
  Electrostatic Hysteresis</title><categories>cs.RO</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in equation 1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present control strategies for implementing reconfigurable
planar microassmbly using multiple stress-engineered MEMS microrobots
(MicroStressBots). A MicroStressBot is an electrostatic microrobot that
consists of an untethered scratch drive actuator (USDA) that provides forward
motion, and a steering-arm actuator that determines whether the robot moves in
straight line or turns. The steering-arm is actuated through electrostatic
pull-down to the substrate initiated by the applied global power delivery and
control signal. Control of multiple MicroStressBots is achieved by varying the
geometry of the steering-arm, and hence affecting its electrostatic pull-down
and/or release voltages. Independent control of many MicroStressBots is
achieved by fabricating the arms of the individual microrobots in such a way
that the robots move differently from one another during portions of the global
control signal. In this paper we analyze the scalability of control in an
obstacle free configuration space. Based on robust control strategies, we
derive the control signals that command some of the robots to make progress
toward the goal, while the others stay in small orbits, for several classes of
steering-arm geometries. We also present a comprehensive analysis and
comparison between the numbers of required independent pull-down and release
voltages, demonstrating significant improvement in terms of the efficiency as
well as the size of the control signal presented in past work. Our analysis
presents an important step for developing multi-microrobots control of
MicroStressBots.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01932</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01932</id><created>2015-07-07</created><authors><author><keyname>Maia</keyname><forenames>Marco M.</forenames></author><author><keyname>Soni</keyname><forenames>Parth</forenames></author><author><keyname>Diez</keyname><forenames>Francisco J.</forenames></author></authors><title>Demonstration of an Aerial and Submersible Vehicle Capable of Flight and
  Underwater Navigation with Seamless Air-Water Transition</title><categories>cs.RO</categories><comments>9 pages, 11 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bio-inspired vehicles are currently leading the way in the quest to produce a
vehicle capable of flight and underwater navigation. However, a fully
functional vehicle has not yet been realized. We present the first fully
functional vehicle platform operating in air and underwater with seamless
transition between both mediums. These unique capabilities combined with the
hovering, high maneuverability and reliability of multirotor vehicles, results
in a disruptive technology for both civil and military application including
air/water search and rescue, inspection, repairs and survey missions among
others. The invention was built on a bio-inspired locomotion force analysis
that combines flight and swimming. Three main advances in the present work has
allowed this invention. The first is the discovery of a seamless transition
method between air and underwater. The second is the design of a multi-medium
propulsion system capable of efficient operation in air and underwater. The
third combines the requirements for lift and thrust for flight (for a given
weight) and the requirements for thrust and neutral buoyancy (in water) for
swimming. The result is a careful balance between lift, thrust, weight, and
neutral buoyancy implemented in the vehicle design. A fully operational
prototype demonstrated the flight, and underwater navigation capabilities as
well as the rapid air/water and water/air transition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01934</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01934</id><created>2015-07-07</created><authors><author><keyname>Kitsunai</keyname><forenames>Kenta</forenames></author><author><keyname>Kobayashi</keyname><forenames>Yasuaki</forenames></author><author><keyname>Tamaki</keyname><forenames>Hisao</forenames></author></authors><title>On the pathwidth of almost semicomplete digraphs</title><categories>cs.DS</categories><comments>33pages, a shorter version to appear in ESA 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We call a digraph {\em $h$-semicomplete} if each vertex of the digraph has at
most $h$ non-neighbors, where a non-neighbor of a vertex $v$ is a vertex $u
\neq v$ such that there is no edge between $u$ and $v$ in either direction.
This notion generalizes that of semicomplete digraphs which are
$0$-semicomplete and tournaments which are semicomplete and have no
anti-parallel pairs of edges. Our results in this paper are as follows. (1) We
give an algorithm which, given an $h$-semicomplete digraph $G$ on $n$ vertices
and a positive integer $k$, in $(h + 2k + 1)^{2k} n^{O(1)}$ time either
constructs a path-decomposition of $G$ of width at most $k$ or concludes
correctly that the pathwidth of $G$ is larger than $k$. (2) We show that there
is a function $f(k, h)$ such that every $h$-semicomplete digraph of pathwidth
at least $f(k, h)$ has a semicomplete subgraph of pathwidth at least $k$.
  One consequence of these results is that the problem of deciding if a fixed
digraph $H$ is topologically contained in a given $h$-semicomplete digraph $G$
admits a polynomial-time algorithm for fixed $h$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01967</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01967</id><created>2015-07-07</created><authors><author><keyname>Friedrich</keyname><forenames>Natalie</forenames></author><author><keyname>Bowman</keyname><forenames>Timothy D.</forenames></author><author><keyname>Stock</keyname><forenames>Wolfgang G.</forenames></author><author><keyname>Haustein</keyname><forenames>Stefanie</forenames></author></authors><title>Adapting sentiment analysis for tweets linking to scientific papers</title><categories>cs.DL</categories><comments>15th International Society of Scientometrics and Informetrics
  Conference (ISSI 2015), Istanbul, Turkey</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  In the context of altmetrics, tweets have been discussed as potential
indicators of immediate and broader societal impact of scientific documents.
However, it is not yet clear to what extent Twitter captures actual research
impact. A small case study (Thelwall et al., 2013b) suggests that tweets to
journal articles neither comment on nor express any sentiments towards the
publication, which suggests that tweets merely disseminate bibliographic
information, often even automatically. This study analyses the sentiments of
tweets for a large representative set of scientific papers by specifically
adapting different methods to academic articles distributed on Twitter. Results
will help to improve the understanding of Twitter's role in scholarly
communication and the meaning of tweets as impact metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01970</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01970</id><created>2015-07-07</created><updated>2015-12-15</updated><authors><author><keyname>Cammerer</keyname><forenames>Sebastian</forenames></author><author><keyname>Aref</keyname><forenames>Vahid</forenames></author><author><keyname>Schmalen</keyname><forenames>Laurent</forenames></author><author><keyname>Brink</keyname><forenames>Stephan ten</forenames></author></authors><title>Triggering Wave-Like Convergence of Tail-biting Spatially Coupled LDPC
  Codes</title><categories>cs.IT math.IT</categories><comments>Submitted to 2016 Annual Conference on Information Science and
  Systems (CISS)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spatially coupled low-density parity-check (SC-LDPC) codes can achieve the
channel capacity under low-complexity belief propagation (BP) decoding,
however, there is a non-negligible rate-loss because of termination effects for
practical finite coupling lengths. In this paper, we study how we can approach
the performance of terminated SC-LDPC codes by random shortening of tail-biting
SC-LDPC codes. We find the minimum required rate-loss in order to achieve the
same performance than terminated codes. We additionally study the use of
tail-biting SC-LDPC codes for transmission over parallel channels (e.g.,
bit-interleaved-coded-modulation (BICM)) and investigate how the distribution
of the coded bits between two parallel channels can change the performance of
the code. We show that a tail-biting SC-LDPC code can be used with BP decoding
almost anywhere within the achievable region of MAP decoding. The optimization
comes with a mandatory buffer at the encoder side. We evaluate different
distributions of coded bits in order to reduce this buffer length.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01971</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01971</id><created>2015-07-07</created><updated>2015-08-24</updated><authors><author><keyname>Rost</keyname><forenames>Peter</forenames></author><author><keyname>Maeder</keyname><forenames>Andreas</forenames></author><author><keyname>Valenti</keyname><forenames>Matthew C.</forenames></author><author><keyname>Talarico</keyname><forenames>Salvatore</forenames></author></authors><title>Computationally Aware Sum-Rate Optimal Scheduling for Centralized Radio
  Access Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>6 pages, 5 figures, 1 table, IEEE Global Telecommunication Conference
  (GLOBECOM), 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a centralized or cloud radio access network, certain portions of the
digital baseband processing of a group of several radio access points are
executed at a central data center. Centralizing the processing improves the
flexibility, scalability, and utilization of computational assets. However, the
performance depends critically on how the limited data processing resources are
allocated to serve the needs of the different wireless devices. As the
processing load imposed by each device depends on its allocated transmission
rate and channel quality, the rate-allocation aspect of the scheduling should
take into account the available computing. In this paper, two computationally
aware schedulers are proposed that have the objective of maximizing the
sum-rate of the system while satisfying a constraint on the offered
computational load. The first scheduler optimally allocates resources and is
implemented according to a water-filling algorithm. The second scheduler is
suboptimal, but uses a simpler and intuitive complexity-cut-off approach. The
performance of both schedulers is evaluated using an LTE-compliant system level
simulator. It is found that both schedulers avoid outages that are caused by an
overflow of computational load (i.e., computational outages) at the cost of a
slight loss of sum-rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01972</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01972</id><created>2015-07-07</created><authors><author><keyname>Montavon</keyname><forenames>Gr&#xe9;goire</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Klaus-Robert</forenames></author><author><keyname>Cuturi</keyname><forenames>Marco</forenames></author></authors><title>Wasserstein Training of Boltzmann Machines</title><categories>stat.ML cs.LG</categories><comments>9 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Boltzmann machine provides a useful framework to learn highly complex,
multimodal and multiscale data distributions that occur in the real world. The
default method to learn its parameters consists of minimizing the
Kullback-Leibler (KL) divergence from training samples to the Boltzmann model.
We propose in this work a novel approach for Boltzmann training which assumes
that a meaningful metric between observations is given. This metric can be
represented by the Wasserstein distance between distributions, for which we
derive a gradient with respect to the model parameters. Minimization of this
new Wasserstein objective leads to generative models that are better when
considering the metric and that have a cluster-like structure. We demonstrate
the practical potential of these models for data completion and denoising, for
which the metric between observations plays a crucial role.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01978</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01978</id><created>2015-07-07</created><authors><author><keyname>Gregorova</keyname><forenames>Magda</forenames></author><author><keyname>Kalousis</keyname><forenames>Alexandros</forenames></author><author><keyname>Marchand-Maillet</keyname><forenames>St&#xe9;phane</forenames></author><author><keyname>Wang</keyname><forenames>Jun</forenames></author></authors><title>Learning vector autoregressive models with focalised Granger-causality
  graphs</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While the importance of Granger-causal (G-causal) relationships for learning
vector autoregressive models (VARs) is widely acknowledged, the
state-of-the-art VAR methods do not address the problem of discovering the
underlying G-causality structure in a principled manner. VAR models can be
restricted if such restrictions are supported by a strong domain theory (e.g.
economics), but without such strong domain-driven constraints the existing VAR
methods typically learn fully connected models where each series is G-caused by
all the others. We develop new VAR methods that address the problem of
discovering structure in the G-causal relationships explicitly. Our methods
learn sparse G-causality graphs with small sets of \emph{focal} series that
govern the dynamical relationships within the time-series system. While
maintaining competitive forecasting accuracy, the sparsity in the G-causality
graphs that our methods achieve is far from reach of any of the
state-of-the-art VAR methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01981</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01981</id><created>2015-07-07</created><updated>2016-02-15</updated><authors><author><keyname>Lim</keyname><forenames>Wei Quan</forenames></author><author><keyname>Gilbert</keyname><forenames>Seth</forenames></author><author><keyname>Lim</keyname><forenames>Wei Zhong</forenames></author></authors><title>Dynamic Reallocation Problems in Scheduling</title><categories>cs.DS</categories><comments>29 pages; updated references and other minor changes</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we look at the problem of scheduling tasks on a
single-processor system, where each task requires unit time and must be
scheduled within a certain time window, and each task can be added to or
removed from the system at any time. On each operation, the system is allowed
to reschedule any tasks, but the goal is to minimize the number of rescheduled
tasks. Our main result is an allocator that maintains a valid schedule for all
tasks in the system if their time windows have constant size and reschedules
O(1/{\epsilon}*log(1/{\epsilon})) tasks on each insertion as {\epsilon}-&gt;0,
where {\epsilon} is a certain measure of the schedule flexibility of the
system. We also show that it is optimal for any allocator that works on
arbitrary instances. We also briefly mention a few variants of the problem,
such as if the tasks have time windows of difference sizes, for which we have
an allocator that we conjecture reschedules only 1 task on each insertion if
the schedule flexibility remains above a certain threshold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01982</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01982</id><created>2015-07-07</created><authors><author><keyname>Li</keyname><forenames>Bo</forenames></author><author><keyname>Petropulu</keyname><forenames>Athina P.</forenames></author><author><keyname>Trappe</keyname><forenames>Wade</forenames></author></authors><title>Optimum Design for Coexistence Between Matrix Completion Based MIMO
  Radars and a MIMO Communication System</title><categories>cs.IT math.IT stat.AP</categories><comments>31 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently proposed multiple input multiple output radars based on matrix
completion (MIMO-MC) employ sparse sampling to reduce the amount of data that
need to be forwarded to the radar fusion center, and as such enable savings in
communication power and bandwidth. This paper proposes designs that optimize
the sharing of spectrum between a MIMO-MC radar and a communication system, so
that the latter interferes minimally with the former. First, the communication
system transmit covariance matrix is designed to minimize the effective
interference power (EIP) to the radar receiver, while maintaining certain
average capacity and transmit power for the communication system. Two
approaches are proposed, namely a noncooperative and a cooperative approach,
with the latter being applicable when the radar sampling scheme is known at the
communication system. Second, a joint design of the communication transmit
covariance matrix and the MIMO-MC radar sampling scheme is proposed, which
achieves even further EIP reduction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01986</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01986</id><created>2015-07-07</created><authors><author><keyname>Soares</keyname><forenames>Nate</forenames></author><author><keyname>Fallenstein</keyname><forenames>Benja</forenames></author></authors><title>Toward Idealized Decision Theory</title><categories>cs.AI</categories><comments>This is an extended version of a paper accepted to AGI-2015</comments><report-no>2014-7</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper motivates the study of decision theory as necessary for aligning
smarter-than-human artificial systems with human interests. We discuss the
shortcomings of two standard formulations of decision theory, and demonstrate
that they cannot be used to describe an idealized decision procedure suitable
for approximation by artificial systems. We then explore the notions of policy
selection and logical counterfactuals, two recent insights into decision theory
that point the way toward promising paths for future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01988</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01988</id><created>2015-07-07</created><authors><author><keyname>Ambainis</keyname><forenames>Andris</forenames></author><author><keyname>Yakaryilmaz</keyname><forenames>Abuzer</forenames></author></authors><title>Automata and Quantum Computing</title><categories>cs.FL cs.CC quant-ph</categories><comments>32 pages. To appear in Automata: From Mathematics to Applications
  edited by Jean-\'Eric Pin</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum computing is a new model of computation, based on quantum physics.
Quantum computers can be exponentially faster than conventional computers for
problems such as factoring. Besides full-scale quantum computers, more
restricted models such as quantum versions of finite automata have been
studied. In this paper, we survey various models of quantum finite automata and
their properties. We also provide some open questions and new directions for
researchers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.01994</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.01994</id><created>2015-07-07</created><updated>2015-09-28</updated><authors><author><keyname>Huang</keyname><forenames>Weiyu</forenames></author><author><keyname>Ribeiro</keyname><forenames>Alejandro</forenames></author></authors><title>Metrics in the Space of High Order Networks</title><categories>cs.SI</categories><doi>10.1109/TSP.2015.2481864</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents methods to compare high order networks, defined as
weighted complete hypergraphs collecting relationship functions between
elements of tuples. They can be considered as generalizations of conventional
networks where only relationship functions between pairs are defined. Important
properties between relationships of tuples of different lengths are
established, particularly when relationships encode dissimilarities or
proximities between nodes. Two families of distances are then introduced in the
space of high order networks. The distances measure differences between
networks. We prove that they are valid metrics in the spaces of high order
dissimilarity and proximity networks modulo permutation isomorphisms. Practical
implications are explored by comparing the coauthorship networks of two popular
signal processing researchers. The metrics succeed in identifying their
respective collaboration patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02000</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02000</id><created>2015-07-07</created><updated>2015-10-18</updated><authors><author><keyname>Lan</keyname><forenames>Guanghui</forenames></author><author><keyname>Zhou</keyname><forenames>Yi</forenames></author></authors><title>An optimal randomized incremental gradient method</title><categories>math.OC cs.CC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a class of finite-sum convex optimization problems
whose objective function is given by the summation of $m$ ($\ge 1$) smooth
components together with some other relatively simple terms. We first introduce
a deterministic primal-dual gradient (PDG) method that can achieve the optimal
black-box iteration complexity for solving these composite optimization
problems using a primal-dual termination criterion. Our major contribution is
to develop a randomized primal-dual gradient (RPDG) method, which needs to
compute the gradient of only one randomly selected smooth component at each
iteration, but can possibly achieve better complexity than PDG in terms of the
total number of gradient evaluations. More specifically, we show that the total
number of gradient evaluations performed by RPDG can be ${\cal O} (\sqrt{m})$
times smaller, both in expectation and with high probability, than those
performed by deterministic optimal first-order methods under favorable
situations. We also show that the complexity of the RPDG method is not
improvable by developing a new lower complexity bound for a general class of
randomized methods for solving large-scale finite-sum convex optimization
problems. Moreover, through the development of PDG and RPDG, we introduce a
novel game-theoretic interpretation for these optimal methods for convex
optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02002</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02002</id><created>2015-07-07</created><authors><author><keyname>Crocetti</keyname><forenames>Giancarlo</forenames></author></authors><title>Topical Discovery of Web Content</title><categories>cs.IR</categories><comments>83 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work describes the theory and the implementation of a new software tool,
the &quot;Web Topical Discovery System&quot; (WTDS), which provides an approach to the
automatic discovery and selection of new web pages relevant to specific
analytical needs. We will see how it is possible to specify the research
context with search keywords related to the area of interest and consider the
important problem of removing extraneous data from a web page containing an
article in order to reduce, to a minimum, false positives represented by a
match on a keyword that is showing up on the latest news box of the same page.
The removal of duplicates, the analysis of richness of information contained in
the article and lexical diversity are all taken into consideration in order to
provide the optimum set of recommendations to the end user or system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02011</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02011</id><created>2015-07-07</created><authors><author><keyname>Bai</keyname><forenames>Qinxun</forenames></author><author><keyname>Lam</keyname><forenames>Henry</forenames></author><author><keyname>Sclaroff</keyname><forenames>Stan</forenames></author></authors><title>A Bayesian Approach for Online Classifier Ensemble</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a Bayesian approach for recursively estimating the classifier
weights in online learning of a classifier ensemble. In contrast with past
methods, such as stochastic gradient descent or online boosting, our approach
estimates the weights by recursively updating its posterior distribution. For a
specified class of loss functions, we show that it is possible to formulate a
suitably defined likelihood function and hence use the posterior distribution
as an approximation to the global empirical loss minimizer. If the stream of
training data is sampled from a stationary process, we can also show that our
approach admits a superior rate of convergence to the expected loss minimizer
than is possible with standard stochastic gradient descent. In experiments with
real-world datasets, our formulation often performs better than
state-of-the-art stochastic gradient descent and online boosting algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02012</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02012</id><created>2015-07-07</created><authors><author><keyname>Gehlot</keyname><forenames>Akanksha</forenames></author><author><keyname>Sharma</keyname><forenames>Vaishali</forenames></author><author><keyname>Singh</keyname><forenames>Shashi Pal</forenames></author><author><keyname>Kumar</keyname><forenames>Ajai</forenames></author></authors><title>Hindi to English Transfer Based Machine Translation System</title><categories>cs.CL</categories><comments>8 pages in International Journal of Advanced Computer ResearchISSN
  (Print): 2249-7277 ISSN (Online): 2277-7970 Volume-5 Issue-19 (June-2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In large societies like India there is a huge demand to convert one human
language into another. Lots of work has been done in this area. Many transfer
based MTS have developed for English to other languages, as MANTRA CDAC Pune,
MATRA CDAC Pune, SHAKTI IISc Bangalore and IIIT Hyderabad. Still there is a
little work done for Hindi to other languages. Currently we are working on it.
In this paper we focus on designing a system, that translate the document from
Hindi to English by using transfer based approach. This system takes an input
text check its structure through parsing. Reordering rules are used to generate
the text in target language. It is better than Corpus Based MTS because Corpus
Based MTS require large amount of word aligned data for translation that is not
available for many languages while Transfer Based MTS requires only knowledge
of both the languages(source language and target language) to make transfer
rules. We get correct translation for simple assertive sentences and almost
correct for complex and compound sentences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02015</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02015</id><created>2015-07-08</created><authors><author><keyname>Garcia-Marco</keyname><forenames>Ignacio</forenames><affiliation>LIP</affiliation></author><author><keyname>Koiran</keyname><forenames>Pascal</forenames><affiliation>LIP</affiliation></author></authors><title>Lower Bounds by Birkhoff Interpolation</title><categories>cs.CC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we give lower bounds for the representation of real univariate
polynomials as sums of powers of degree 1 polynomials. We present two families
of polynomials of degree d such that the number of powers that are required in
such a representation must be at least of order d. This is clearly optimal up
to a constant factor. Previous lower bounds for this problem were only of order
$\Omega$($\sqrt$ d), and were obtained from arguments based on Wronskian
determinants and &quot;shifted derivatives.&quot; We obtain this improvement thanks to a
new lower bound method based on Birkhoff interpolation (also known as &quot;lacunary
polynomial interpolation&quot;).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02020</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02020</id><created>2015-07-08</created><authors><author><keyname>Poibeau</keyname><forenames>Thierry</forenames><affiliation>LaTTICe</affiliation></author><author><keyname>Ruiz</keyname><forenames>Pablo</forenames><affiliation>LaTTICe</affiliation></author></authors><title>Generating Navigable Semantic Maps from Social Sciences Corpora</title><categories>cs.CL cs.AI cs.IR</categories><comments>in Digital Humanities 2015, Jun 2015, Sydney, Australia. Actes de la
  Conf{\'e}rence Digital Humanities 2015. arXiv admin note: text overlap with
  arXiv:1406.4211</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is now commonplace to observe that we are facing a deluge of online
information. Researchers have of course long acknowledged the potential value
of this information since digital traces make it possible to directly observe,
describe and analyze social facts, and above all the co-evolution of ideas and
communities over time. However, most online information is expressed through
text, which means it is not directly usable by machines, since computers
require structured, organized and typed information in order to be able to
manipulate it. Our goal is thus twofold: 1. Provide new natural language
processing techniques aiming at automatically extracting relevant information
from texts, especially in the context of social sciences, and connect these
pieces of information so as to obtain relevant socio-semantic networks; 2.
Provide new ways of exploring these socio-semantic networks, thanks to tools
allowing one to dynamically navigate these networks, de-construct and
re-construct them interactively, from different points of view following the
needs expressed by domain experts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02021</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02021</id><created>2015-07-08</created><authors><author><keyname>M&#xe9;lanie-Becquet</keyname><forenames>Fr&#xe9;d&#xe9;rique</forenames><affiliation>LaTTICe</affiliation></author><author><keyname>Ferguth</keyname><forenames>Johan</forenames><affiliation>LaTTICe</affiliation></author><author><keyname>Gruel</keyname><forenames>Katherine</forenames><affiliation>LaTTICe</affiliation></author><author><keyname>Poibeau</keyname><forenames>Thierry</forenames><affiliation>LaTTICe</affiliation></author></authors><title>Archaeology in the Digital Age: From Paper to Databases</title><categories>cs.DL cs.AI cs.CY cs.IR</categories><comments>Digital Humanities 2015, Jun 2015, Sydney, Australia. 2015,
  Proceedings of the conference &quot;Digital Humanities 2015&quot;</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research units in archaeology often manage large and precious archives
containing various documents, including reports on fieldwork, scholarly studies
and reference books. These archives are of course invaluable, recording decades
of work, but are generally hard to consult and access. In this context,
digitizing full text documents is not enough: information must be formalized,
structured and easy to access thanks to friendly user interfaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02030</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02030</id><created>2015-07-08</created><updated>2015-10-28</updated><authors><author><keyname>Hazan</keyname><forenames>Elad</forenames></author><author><keyname>Levy</keyname><forenames>Kfir Y.</forenames></author><author><keyname>Shalev-Shwartz</keyname><forenames>Shai</forenames></author></authors><title>Beyond Convexity: Stochastic Quasi-Convex Optimization</title><categories>cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic convex optimization is a basic and well studied primitive in
machine learning. It is well known that convex and Lipschitz functions can be
minimized efficiently using Stochastic Gradient Descent (SGD). The Normalized
Gradient Descent (NGD) algorithm, is an adaptation of Gradient Descent, which
updates according to the direction of the gradients, rather than the gradients
themselves. In this paper we analyze a stochastic version of NGD and prove its
convergence to a global minimum for a wider class of functions: we require the
functions to be quasi-convex and locally-Lipschitz. Quasi-convexity broadens
the con- cept of unimodality to multidimensions and allows for certain types of
saddle points, which are a known hurdle for first-order optimization methods
such as gradient descent. Locally-Lipschitz functions are only required to be
Lipschitz in a small region around the optimum. This assumption circumvents
gradient explosion, which is another known hurdle for gradient descent
variants. Interestingly, unlike the vanilla SGD algorithm, the stochastic
normalized gradient descent algorithm provably requires a minimal minibatch
size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02032</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02032</id><created>2015-07-08</created><authors><author><keyname>Nasir</keyname><forenames>A. A.</forenames></author><author><keyname>Durrani</keyname><forenames>S.</forenames></author><author><keyname>Mehrpouyan</keyname><forenames>H.</forenames></author><author><keyname>Blostein</keyname><forenames>S. D.</forenames></author><author><keyname>Kennedy</keyname><forenames>R. A.</forenames></author></authors><title>Timing and Carrier Synchronization in Wireless Communication Systems: A
  Survey and Classification of Research in the Last Five Years</title><categories>cs.IT math.IT</categories><comments>submitted for journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Timing and carrier synchronization is a fundamental requirement for any
wireless communication system to work properly. Timing synchronization is the
process by which a receiver node determines the correct instants of time at
which to sample the incoming signal. Carrier synchronization is the process by
which a receiver adapts the frequency and phase of its local carrier oscillator
with those of the received signal. In this paper, we survey the literature over
the last five years (2010-2014) and present a comprehensive literature review
and classification of the recent research progress in achieving timing and
carrier synchronization in single-input-single-output (SISO),
multiple-input-multiple-output (MIMO), cooperative relaying, and
multiuser/multicell interference networks. Considering both single-carrier and
multi-carrier communication systems, we survey and categorise the timing and
carrier synchronization techniques proposed for the different communication
systems focusing on the system model assumptions for synchronization, the
synchronization challenges, and the state-of-the-art synchronization solutions
and their limitations. Finally, we envision some future research directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02037</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02037</id><created>2015-07-08</created><authors><author><keyname>Hou</keyname><forenames>Thomas Y.</forenames></author><author><keyname>Shi</keyname><forenames>Zuoqiang</forenames></author></authors><title>Sparse Time-Frequency decomposition for multiple signals with same
  frequencies</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider multiple signals sharing same instantaneous
frequencies. This kind of data is very common in scientific and engineering
problems. To take advantage of this special structure, we modify our
data-driven time-frequency analysis by updating the instantaneous frequencies
simultaneously. Moreover, based on the simultaneously sparsity approximation
and fast Fourier transform, some efficient algorithms is developed. Since the
information of multiple signals is used, this method is very robust to the
perturbation of noise. And it is applicable to the general nonperiodic signals
even with missing samples or outliers. Several synthetic and real signals are
used to test this method. The performances of this method are very promising.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02043</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02043</id><created>2015-07-08</created><authors><author><keyname>Zwickl</keyname><forenames>Patrick</forenames></author><author><keyname>Gojmerac</keyname><forenames>Ivan</forenames></author><author><keyname>Fuxjaeger</keyname><forenames>Paul</forenames></author><author><keyname>Reichl</keyname><forenames>Peter</forenames></author><author><keyname>Holland</keyname><forenames>Oliver</forenames></author></authors><title>The Society Spectrum: Self-Regulation of Cellular Network Markets</title><categories>cs.NI cs.CY</categories><comments>6 pages, 3 figures, conference format</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today's cellular telecommunications markets require continuous monitoring and
intervention by regulators in order to balance the interests of various
stakeholders. In order to reduce the extent of regulatory involvements in the
day-to-day business of cellular operators, the present paper proposes a
&quot;self-regulating&quot; spectrum market regime named &quot;society spectrum&quot;. This regime
provides a market-inherent and automatic self-balancing of stakeholder powers,
which at the same time provides a series of coordination and fairness assurance
functions that clearly distinguish it from &quot;spectrum as a commons&quot; solutions.
The present paper will introduce the fundamental regulatory design and will
elaborate on mechanisms to assure fairness among stakeholders and individuals.
This work further puts the society spectrum into the context of contemporary
radio access technologies and cognitive radio approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02045</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02045</id><created>2015-07-08</created><updated>2015-08-16</updated><authors><author><keyname>Jaech</keyname><forenames>Aaron</forenames></author><author><keyname>Ostendorf</keyname><forenames>Mari</forenames></author></authors><title>What Your Username Says About You</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Usernames are ubiquitous on the Internet, and they are often suggestive of
user demographics. This work looks at the degree to which gender and language
can be inferred from a username alone by making use of unsupervised morphology
induction to decompose usernames into sub-units. Experimental results on the
two tasks demonstrate the effectiveness of the proposed morphological features
compared to a character n-gram baseline.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02048</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02048</id><created>2015-07-08</created><updated>2015-07-08</updated><authors><author><keyname>Ma</keyname><forenames>Chaofan</forenames></author><author><keyname>Liang</keyname><forenames>Wei</forenames></author><author><keyname>Zheng</keyname><forenames>Meng</forenames></author></authors><title>A Connectivity-Aware Approximation Algorithm for Relay Node Placement in
  Wireless Sensor Networks</title><categories>cs.NI</categories><comments>14 pages, 24 figures</comments><acm-class>C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In two-tiered Wireless Sensor Networks (WSNs) relay node placement is one of
the key factors impacting the network energy consumption and the system
overhead. In this paper, a novel connectivity-aware approximation algorithm for
relay node placement in WSNs is proposed to offer a major step forward in
saving system overhead. Specifically, a unique Local Search Approximation
Algorithm (LSAA) is introduced to solve the Relay Node Single Cover (RNSC)
problem. In this proposed LSAA approach, the sensor nodes are allocated into
groups and then a local Set Cover (SC) for each group is achieved by a local
search algorithm. The union set of all local SCs constitutes a SC of the RNSC
problem. The approximation ratio and the time complexity of the LSAA are
analyzed by rigorous proof. Additionally, the LSAA approach has been extended
to solve the relay node double cover problem. Then, a Relay Location Selection
Algorithm (RLSA) is proposed to utilize the resulting SC from LSAA in combining
RLSA with the minimum spanning tree heuristic to build the high-tier
connectivity. As the RLSA searches for a nearest location to the sink node for
each relay node, the high-tier network built by RLSA becomes denser than that
by existing works. As a result, the number of added relay nodes for building
the connectivity of the high-tier WSN can be significantly saved. Simulation
results clearly demonstrate that the proposed LSAA outperforms the approaches
reported in literature and the RLSA-based algorithm can noticeably save relay
nodes newly deployed for the high-tier connectivity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02049</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02049</id><created>2015-07-08</created><updated>2015-09-29</updated><authors><author><keyname>Ng</keyname><forenames>Cong Jie</forenames></author><author><keyname>Teoh</keyname><forenames>Andrew Beng Jin</forenames></author></authors><title>DCTNet : A Simple Learning-free Approach for Face Recognition</title><categories>cs.CV</categories><comments>APSIPA ASC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  PCANet was proposed as a lightweight deep learning network that mainly
leverages Principal Component Analysis (PCA) to learn multistage filter banks
followed by binarization and block-wise histograming. PCANet was shown worked
surprisingly well in various image classification tasks. However, PCANet is
data-dependence hence inflexible. In this paper, we proposed a
data-independence network, dubbed DCTNet for face recognition in which we adopt
Discrete Cosine Transform (DCT) as filter banks in place of PCA. This is
motivated by the fact that 2D DCT basis is indeed a good approximation for high
ranked eigenvectors of PCA. Both 2D DCT and PCA resemble a kind of modulated
sine-wave patterns, which can be perceived as a bandpass filter bank. DCTNet is
free from learning as 2D DCT bases can be computed in advance. Besides that, we
also proposed an effective method to regulate the block-wise histogram feature
vector of DCTNet for robustness. It is shown to provide surprising performance
boost when the probe image is considerably different in appearance from the
gallery image. We evaluate the performance of DCTNet extensively on a number of
benchmark face databases and being able to achieve on par with or often better
accuracy performance than PCANet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02062</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02062</id><created>2015-07-08</created><authors><author><keyname>Wan</keyname><forenames>Xiaojun</forenames></author><author><keyname>Cao</keyname><forenames>Ziqiang</forenames></author><author><keyname>Wei</keyname><forenames>Furu</forenames></author><author><keyname>Li</keyname><forenames>Sujian</forenames></author><author><keyname>Zhou</keyname><forenames>Ming</forenames></author></authors><title>Multi-Document Summarization via Discriminative Summary Reranking</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing multi-document summarization systems usually rely on a specific
summarization model (i.e., a summarization method with a specific parameter
setting) to extract summaries for different document sets with different
topics. However, according to our quantitative analysis, none of the existing
summarization models can always produce high-quality summaries for different
document sets, and even a summarization model with good overall performance may
produce low-quality summaries for some document sets. On the contrary, a
baseline summarization model may produce high-quality summaries for some
document sets. Based on the above observations, we treat the summaries produced
by different summarization models as candidate summaries, and then explore
discriminative reranking techniques to identify high-quality summaries from the
candidates for difference document sets. We propose to extract a set of
candidate summaries for each document set based on an ILP framework, and then
leverage Ranking SVM for summary reranking. Various useful features have been
developed for the reranking process, including word-level features,
sentence-level features and summary-level features. Evaluation results on the
benchmark DUC datasets validate the efficacy and robustness of our proposed
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02066</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02066</id><created>2015-07-08</created><authors><author><keyname>Berdan</keyname><forenames>Radu</forenames></author><author><keyname>Vasilaki</keyname><forenames>Eleni</forenames></author><author><keyname>Khiat</keyname><forenames>Ali</forenames></author><author><keyname>Indiveri</keyname><forenames>Giacomo</forenames></author><author><keyname>Serb</keyname><forenames>Alexandru</forenames></author><author><keyname>Prodromakis</keyname><forenames>Themistoklis</forenames></author></authors><title>Emulating short-term synaptic dynamics with memristive devices</title><categories>cs.ET</categories><comments>17 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neuromorphic architectures offer great promise for achieving computation
capacities beyond conventional Von Neumann machines. The essential elements for
achieving this vision are highly scalable synaptic mimics that do not undermine
biological fidelity. Here we demonstrate that single solid-state TiO2
memristors can exhibit non-associative plasticity phenomena observed in
biological synapses, supported by their metastable memory state transition
properties. We show that, contrary to conventional uses of solid-state memory,
the existence of rate-limiting volatility is a key feature for capturing
short-term synaptic dynamics. We also show how the temporal dynamics of our
prototypes can be exploited to implement spatio-temporal computation,
demonstrating the memristors full potential for building biophysically
realistic neural processing systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02067</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02067</id><created>2015-07-08</created><authors><author><keyname>Aistleitner</keyname><forenames>Christoph</forenames></author><author><keyname>Hinrichs</keyname><forenames>Aicke</forenames></author><author><keyname>Rudolf</keyname><forenames>Daniel</forenames></author></authors><title>On the size of the largest empty box amidst a point set</title><categories>cs.CG cs.CC math.NA</categories><comments>6 pages</comments><msc-class>52B55, 68Q25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of finding the largest empty axis-parallel box amidst a point
configuration is a classical problem in computational complexity theory. It is
known that the volume of the largest empty box is of asymptotic order $1/n$ for
$n \to \infty$ and fixed dimension $d$. However, it is natural to assume that
the volume of the largest empty box increases as $d$ gets larger. In the
present paper we prove that this actually is the case: for every set of $n$
points in $[0,1]^d$ there exists an empty box of volume at least $c_d n^{-1}$,
where $c_d \to \infty$ as $d \to \infty$. More precisely, $c_d$ is at least of
order roughly $\log d$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02069</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02069</id><created>2015-07-08</created><authors><author><keyname>Chan</keyname><forenames>Siu On</forenames></author><author><keyname>Kwok</keyname><forenames>Tsz Chiu</forenames></author><author><keyname>Lau</keyname><forenames>Lap Chi</forenames></author></authors><title>Random Walks and Evolving Sets: Faster Convergences and Limitations</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analyzing the mixing time of random walks is a well-studied problem with
applications in random sampling and more recently in graph partitioning. In
this work, we present new analysis of random walks and evolving sets using more
combinatorial graph structures, and show some implications in approximating
small-set expansion. On the other hand, we provide examples showing the
limitations of using random walks and evolving sets in disproving the small-set
expansion hypothesis.
  - We define a combinatorial analog of the spectral gap, and use it to prove
the convergence of non-lazy random walks. A corollary is a tight lower bound on
the small-set expansion of graph powers for any graph.
  - We prove that random walks converge faster when the robust vertex expansion
of the graph is larger. This provides an improved analysis of the local graph
partitioning algorithm using the evolving set process.
  - We give an example showing that the evolving set process fails to disprove
the small-set expansion hypothesis. This refutes a conjecture of Oveis Gharan
and shows the limitations of local graph partitioning algorithms in
approximating small-set expansion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02075</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02075</id><created>2015-07-08</created><updated>2015-10-30</updated><authors><author><keyname>Sahnoun</keyname><forenames>Souleymen</forenames></author><author><keyname>Djermoune</keyname><forenames>El-Hadi</forenames></author><author><keyname>Brie</keyname><forenames>David</forenames></author><author><keyname>Comon</keyname><forenames>Pierre</forenames></author></authors><title>A Simultaneous Sparse Approximation Method for Multidimensional Harmonic
  Retrieval</title><categories>cs.IT math.IT</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a sparse-based method for the estimation of the parameters of
multidimensional ($R$-D) modal (harmonic or damped) complex signals in noise is
presented. The problem is formulated as $R$ simultaneous sparse approximations
of multiple 1-D signals. To get a method able to handle large size signals
while maintaining a sufficient resolution, a multigrid dictionary refinement
technique is associated with the simultaneous sparse approximation problem. The
refinement procedure is proved to converge in the single $R$-D mode case. Then,
for the general multiple modes $R$-D case, the signal tensor model is
decomposed in order to handle each mode separately in an iterative scheme. The
proposed method does not require an association step since the estimated modes
are automatically &quot;paired&quot;. We also derive the Cram\'er-Rao lower bounds of the
parameters of modal $R$-D signals. The expressions are given in compact form in
the single $R$-D mode case. Finally, numerical simulations are conducted to
demonstrate the effectiveness of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02077</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02077</id><created>2015-07-08</created><authors><author><keyname>Gao</keyname><forenames>Yansong</forenames></author><author><keyname>Kavehei</keyname><forenames>Omid</forenames></author><author><keyname>Ranasinghe</keyname><forenames>Damith C.</forenames></author><author><keyname>Al-Sarawi</keyname><forenames>Said F.</forenames></author><author><keyname>Abbott</keyname><forenames>Derek</forenames></author></authors><title>Future Large-Scale Memristive Device Crossbar Arrays: Limits Imposed by
  Sneak-Path Currents on Read Operations</title><categories>cs.ET</categories><comments>8 pages. 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Passive crossbar arrays based upon memristive devices, at crosspoints, hold
great promise for the future high-density and non-volatile memories. The most
significant challenge facing memristive device based crossbars today is the
problem of sneak-path currents. In this paper, we investigate a memristive
device with intrinsic rectification behavior to suppress the sneak-path
currents in crossbar arrays. The device model is implemented in Verilog-A
language and is simulated to match device characteristics readily available in
the literature. Then, we systematically evaluate the read operation performance
of large-scale crossbar arrays utilizing our proposed model in terms of read
margin and power consumption while considering different crossbar sizes,
interconnect resistance values, HRS/LRS (High Resistance State/Low Resistance
State) values, rectification ratios and different read-schemes. The outcomes of
this study are understanding the trade-offs among read margin, power
consumption, read-schemes and most importantly providing a guideline for
circuit designers to improve the performance of a memory based crossbar
structure. In addition, read operation performance comparison of the intrinsic
rectifying memristive device model with other memristive device models are
studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02081</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02081</id><created>2015-07-08</created><authors><author><keyname>Neunert</keyname><forenames>Michael</forenames></author><author><keyname>Bl&#xf6;sch</keyname><forenames>Michael</forenames></author><author><keyname>Buchli</keyname><forenames>Jonas</forenames></author></authors><title>An Open Source, Fiducial Based, Visual-Inertial State Estimation System</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many robotic tasks rely on the estimation of the location of moving bodies
with respect to the robotic workspace. This information about the robots pose
and velocities is usually either directly used for localization and control or
utilized for verification. Often motion capture systems are used to obtain such
a state estimation. However, these systems are very costly and limited in terms
of workspace size and outdoor usage. Therefore, we propose a lightweight and
easy to use, visual inertial Simultaneous Localization and Mapping approach
that leverages paper printable artificial landmarks, so called fiducials.
Results show that by fusing visual and inertial data, the system provides
accurate estimates and is robust against fast motions. Continuous estimation of
the fiducials within the workspace ensures accuracy and avoids additional
calibration. By providing an open source implementation and various datasets
including ground truth information, we enable other community members to run,
test, modify and extend the system using datasets or their own robotic setups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02084</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02084</id><created>2015-07-08</created><authors><author><keyname>Landesa-V&#xe1;zquez</keyname><forenames>Iago</forenames></author><author><keyname>Alba-Castro</keyname><forenames>Jos&#xe9; Luis</forenames></author></authors><title>Shedding Light on the Asymmetric Learning Capability of AdaBoost</title><categories>cs.LG cs.AI cs.CV</categories><journal-ref>Pattern Recognition Letters 33 (2012) 247-255</journal-ref><doi>10.1016/j.patrec.2011.10.022</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a different insight to analyze AdaBoost. This
analysis reveals that, beyond some preconceptions, AdaBoost can be directly
used as an asymmetric learning algorithm, preserving all its theoretical
properties. A novel class-conditional description of AdaBoost, which models the
actual asymmetric behavior of the algorithm, is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02086</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02086</id><created>2015-07-08</created><authors><author><keyname>Ramakrishna</keyname><forenames>Shashishekar</forenames></author><author><keyname>Gorski</keyname><forenames>Lukasz</forenames></author><author><keyname>Paschke</keyname><forenames>Adrian</forenames></author></authors><title>The Role of Pragmatics in Legal Norm Representation</title><categories>cs.CL cs.AI</categories><comments>International Workshop On Legal Domain And Semantic Web Applications
  (LeDA-SWAn 2015), held during the 12th Extended Semantic Web Conference (ESWC
  2015), June 1, 2015, Portoroz, Slovenia. in CEUR Workshop Proceedings 2015</comments><msc-class>68T30</msc-class><acm-class>J.1; I.2.1</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Despite the 'apparent clarity' of a given legal provision, its application
may result in an outcome that does not exactly conform to the semantic level of
a statute. The vagueness within a legal text is induced intentionally to
accommodate all possible scenarios under which such norms should be applied,
thus making the role of pragmatics an important aspect also in the
representation of a legal norm and reasoning on top of it. The notion of
pragmatics considered in this paper does not focus on the aspects associated
with judicial decision making. The paper aims to shed light on the aspects of
pragmatics in legal linguistics, mainly focusing on the domain of patent law,
only from a knowledge representation perspective. The philosophical discussions
presented in this paper are grounded based on the legal theories from Grice and
Marmor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02089</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02089</id><created>2015-07-08</created><updated>2015-09-15</updated><authors><author><keyname>Regts</keyname><forenames>Guus</forenames></author></authors><title>Zero-free regions of partition functions with applications to algorithms
  and graph limits</title><categories>math.CO cs.DS</categories><comments>Some minor changes haven been made. 20 pages</comments><msc-class>05C85 (Primary), 05C31, 68W25, 05C99 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on a technique of Barvinok and Barvinok and Sober\'on we identify a
class of edge-coloring models whose partition functions do not evaluate to zero
on bounded degree graphs. Subsequently we give a quasi-polynomial time
approximation scheme for computing these partition functions. As another
application we show that the normalised partition functions of these models are
continuous with respect the Benjamini-Schramm topology on bounded degree
graphs. We moreover give quasi-polynomial time approximation schemes for
evaluating a large class of graph polynomials, including the Tutte polynomial,
on bounded degree graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02093</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02093</id><created>2015-07-08</created><authors><author><keyname>Zahedi</keyname><forenames>Zohreh</forenames></author><author><keyname>Costas</keyname><forenames>Rodrigo</forenames></author><author><keyname>Wouters</keyname><forenames>Paul</forenames></author></authors><title>Do Mendeley readership counts help to filter highly cited WoS
  publications better than average citation impact of journals (JCS)?</title><categories>cs.DL</categories><comments>This paper presented at the 15th International Conference on
  Scientometrics and Informetrics (ISSI), 29 Jun-4 July, 2015, Bogazici
  University, Istanbul (Turkey)</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  In this study, the academic status of users of scientific publications in
Mendeley is explored in order to analyse the usage pattern of Mendeley users in
terms of subject fields, citation and readership impact. The main focus of this
study is on studying the filtering capacity of Mendeley readership counts
compared to journal citation scores in detecting highly cited WoS publications.
Main finding suggests a faster reception of Mendeley readerships as compared to
citations across 5 major field of science. The higher correlations of
scientific users with citations indicate the similarity between reading and
citation behaviour among these users. It is confirmed that Mendeley readership
counts filter highly cited publications (PPtop 10%) better than journal
citation scores in all subject fields and by most of user types. This result
reinforces the potential role that Mendeley readerships could play for
informing scientific and alternative impacts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02095</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02095</id><created>2015-07-08</created><authors><author><keyname>Zahedi</keyname><forenames>Zohreh</forenames></author><author><keyname>Costas</keyname><forenames>Rodrigo</forenames></author><author><keyname>Wouters</keyname><forenames>Paul</forenames></author></authors><title>How well developed are Altmetrics? Cross-disciplinary analysis of the
  presence of alternative metrics in scientific publications?</title><categories>cs.DL</categories><comments>This paper presented at the 14th International Conference on
  Scientometrics and Informetrics (ISSI), 16-19 July 2013, University of
  Vienna, Austria</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper an analysis of the presence and possibilities of altmetrics for
bibliometric and performance analysis is carried out. Using the web based tool
Impact Story, we have collected metrics for 20,000 random publications from the
Web of Science. We studied the presence and frequency of altmetrics in the set
of publications, across fields, document types and also through the years. The
main result of the study is that less than 50% of the publications have some
kind of altmetrics. The source that provides most metrics is Mendeley, with
metrics on readerships for around 37% of all the publications studied. Other
sources only provide marginal information. Possibilities and limitations of
these indicators are discussed and future research lines are outlined. We also
assessed the accuracy of the data retrieved through Impact Story by focusing on
the analysis of the accuracy of data from Mendeley; in a follow up study, the
accuracy and validity of other data sources not included here will be assessed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02099</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02099</id><created>2015-07-08</created><updated>2016-02-25</updated><authors><author><keyname>Waltman</keyname><forenames>Ludo</forenames></author></authors><title>A review of the literature on citation impact indicators</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Citation impact indicators nowadays play an important role in research
evaluation, and consequently these indicators have received a lot of attention
in the bibliometric and scientometric literature. This paper provides an
in-depth review of the literature on citation impact indicators. First, an
overview is given of the literature on bibliographic databases that can be used
to calculate citation impact indicators (Web of Science, Scopus, and Google
Scholar). Next, selected topics in the literature on citation impact indicators
are reviewed in detail. The first topic is the selection of publications and
citations to be included in the calculation of citation impact indicators. The
second topic is the normalization of citation impact indicators, in particular
normalization for field differences. Counting methods for dealing with
co-authored publications are the third topic, and citation impact indicators
for journals are the last topic. The paper concludes by offering some
recommendations for future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02103</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02103</id><created>2015-07-08</created><updated>2015-12-16</updated><authors><author><keyname>Csat&#xf3;</keyname><forenames>L&#xe1;szl&#xf3;</forenames></author></authors><title>Measuring centrality by a generalization of degree</title><categories>cs.SI physics.soc-ph</categories><comments>20 pages, 8 figures</comments><msc-class>15A06, 91D30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network analysis has emerged as a key technique in communication studies,
economics, geography, history and sociology, among others. A fundamental issue
is how to identify key nodes, for which purpose a number of centrality measures
have been developed. This paper proposes a new parametric family of centrality
measures called generalized degree. It is based on the idea that a relationship
to a more interconnected node contributes to centrality in a greater extent
than a connection to a less central one. Generalized degree improves on degree
by redistributing its sum over the network with the consideration of the global
structure. Application of the measure is supported by a set of basic
properties. A sufficient condition is given for generalized degree to be rank
monotonic, excluding counter-intuitive changes in the centrality ranking after
certain modifications of the network. The measure has a graph interpretation
and can be calculated iteratively. Generalized degree is recommended to apply
besides degree since it preserves most favourable attributes of degree, but
better reflects the role of the nodes in the network and has an increased
ability to distinguish among their importance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02119</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02119</id><created>2015-07-08</created><updated>2015-11-10</updated><authors><author><keyname>Gunawan</keyname><forenames>Andreas D. M.</forenames></author><author><keyname>DasGupta</keyname><forenames>Bhaskar</forenames></author><author><keyname>Zhang</keyname><forenames>Louxin</forenames></author></authors><title>Locating a Tree in a Reticulation-Visible Network in Cubic Time</title><categories>q-bio.PE cs.DS</categories><comments>25 pages, 3 figures</comments><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we answer an open problem in the study of phylogenetic
networks. Phylogenetic trees are rooted binary trees in which all edges are
directed away from the root, whereas phylogenetic networks are rooted acyclic
digraphs. For the purpose of evolutionary model validation, biologists often
want to know whether or not a phylogenetic tree is contained in a phylogenetic
network. The tree containment problem is NP-complete even for very restricted
classes of networks such as tree-sibling phylogenetic networks. We prove that
this problem is solvable in cubic time for stable phylogenetic networks. A
linear time algorithm is also presented for the cluster containment problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02130</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02130</id><created>2015-07-08</created><authors><author><keyname>De Carufel</keyname><forenames>Jean-Lou</forenames></author><author><keyname>Katz</keyname><forenames>Matya</forenames></author><author><keyname>Korman</keyname><forenames>Matias</forenames></author><author><keyname>van Renssen</keyname><forenames>Andre</forenames></author><author><keyname>Roeloffzen</keyname><forenames>Marcel</forenames></author><author><keyname>Smorodinsky</keyname><forenames>Shakhar</forenames></author></authors><title>On Kinetic Range Spaces and their Applications</title><categories>cs.CG cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study geometric hypergrahs in a kinetic setting. That is, the set of
vertices of the hypergraph is a set of moving points in $\Re^d$ with
coordinates that are polynomials in time. The hyperedges are all subsets that
can be realized by intersecting the set of points at some fixed time with some
&quot;simple&quot; geometric shape, such as, say, a halfspace. We show that for many of
the static cases where the \VC-dimension of the hypergraph is bounded, the
kinetic counterpart also has bounded \VC-dimension. This allows us to prove our
main result: for any set of $n$ moving points in $\Re^d$ and any parameter $1 &lt;
k &lt; n$, one can select a non-empty subset of the points of size $O(k \log k)$
such that the Voronoi diagram of this subset is &quot;balanced&quot; at any given time.
By that, we mean that at any time, each Voronoi cell contains at most $O(n/k)$
of the points. We also show that the bound $O(k \log k)$ is near optimal
already for the one dimensional case (i.e., $d =1$) and points moving linearly
(i.e., with constant speed). As an application, we show that we can assign a
communication radius to a collection of $n$ moving sensors so that at any given
time, their interference is $O(\sqrt{n\log n})$. This is optimal up to an
$O(\sqrt{\log n})$ factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02132</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02132</id><created>2015-07-07</created><authors><author><keyname>Chau</keyname><forenames>Chi-Kin</forenames></author><author><keyname>Wang</keyname><forenames>Qian</forenames></author><author><keyname>Chiu</keyname><forenames>Dah-Ming</forenames></author></authors><title>Economic Viability of Paris Metro Pricing for Digital Services</title><categories>cs.GT cs.NI</categories><comments>This paper appears in ACM Transactions on Internet Technology (ToIT),
  Special Issue on Pricing and Incentives in Networks and Systems, Vol. 14, No.
  12, Issue 2-3, pp12:1-12:21, Oct 2014. A preliminary version has been
  presented at IEEE INFOCOM 2010. in C-K Chau (2014)</comments><doi>10.1145/2663492</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays digital services, such as cloud computing and network access
services, allow dynamic resource allocation and virtual resource isolation.
This trend can create a new paradigm of flexible pricing schemes. A simple
pricing scheme is to allocate multiple isolated service classes with
differentiated prices, namely Paris Metro Pricing (PMP). The benefits of PMP
are its simplicity and applicability to a wide variety of general digital
services, without considering specific performance guarantees for different
service classes. The central issue of our study is whether PMP is economically
viable, namely whether it will produce more profit for the service provider and
whether it will achieve more social welfare. Prior studies had only considered
specific models and arrived at conflicting conclusions. In this article, we
identify unifying principles in a general setting and derive general sufficient
conditions that can guarantee the viability of PMP. We further apply the
results to analyze various examples of digital services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02139</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02139</id><created>2015-07-08</created><updated>2015-10-30</updated><authors><author><keyname>Carbone</keyname><forenames>Giuseppe</forenames></author><author><keyname>Giannoccaro</keyname><forenames>Ilaria</forenames></author></authors><title>Model of human collective decision-making in complex environments</title><categories>cs.MA cs.AI nlin.AO physics.soc-ph</categories><comments>12 pages, 8 figues in European Physical Journal B, 2015</comments><journal-ref>European Physical Journal B, 88 (12), 339, 2015</journal-ref><doi>10.1140/epjb/e2015-60609-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A continuous-time Markov process is proposed to analyze how a group of humans
solves a complex task, consisting in the search of the optimal set of decisions
on a fitness landscape. Individuals change their opinions driven by two
different forces: (i) the self-interest, which pushes them to increase their
own fitness values, and (ii) the social interactions, which push individuals to
reduce the diversity of their opinions in order to reach consensus. Results
show that the performance of the group is strongly affected by the strength of
social interactions and by the level of knowledge of the individuals.
Increasing the strength of social interactions improves the performance of the
team. However, too strong social interactions slow down the search of the
optimal solution and worsen the performance of the group. In particular, we
find that the threshold value of the social interaction strength, which leads
to the emergence of a superior intelligence of the group, is just the critical
threshold at which the consensus among the members sets in. We also prove that
a moderate level of knowledge is already enough to guarantee high performance
of the group in making decisions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02140</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02140</id><created>2015-07-08</created><authors><author><keyname>Hu</keyname><forenames>Yue</forenames></author><author><keyname>Wan</keyname><forenames>Xiaojun</forenames></author></authors><title>Mining and Analyzing the Future Works in Scientific Articles</title><categories>cs.DL cs.CL cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Future works in scientific articles are valuable for researchers and they can
guide researchers to new research directions or ideas. In this paper, we mine
the future works in scientific articles in order to 1) provide an insight for
future work analysis and 2) facilitate researchers to search and browse future
works in a research area. First, we study the problem of future work extraction
and propose a regular expression based method to address the problem. Second,
we define four different categories for the future works by observing the data
and investigate the multi-class future work classification problem. Third, we
apply the extraction method and the classification model to a paper dataset in
the computer science field and conduct a further analysis of the future works.
Finally, we design a prototype system to search and demonstrate the future
works mined from the scientific papers. Our evaluation results show that our
extraction method can get high precision and recall values and our
classification model can also get good results and it outperforms several
baseline models. Further analysis of the future work sentences also indicates
interesting results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02144</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02144</id><created>2015-07-08</created><authors><author><keyname>Azizpour</keyname><forenames>Hossein</forenames></author><author><keyname>Arefiyan</keyname><forenames>Mostafa</forenames></author><author><keyname>Parizi</keyname><forenames>Sobhan Naderi</forenames></author><author><keyname>Carlsson</keyname><forenames>Stefan</forenames></author></authors><title>Spotlight the Negatives: A Generalized Discriminative Latent Model</title><categories>cs.CV</categories><comments>Published in proceedings of BMVC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discriminative latent variable models (LVM) are frequently applied to various
visual recognition tasks. In these systems the latent (hidden) variables
provide a formalism for modeling structured variation of visual features.
Conventionally, latent variables are de- fined on the variation of the
foreground (positive) class. In this work we augment LVMs to include negative
latent variables corresponding to the background class. We formalize the
scoring function of such a generalized LVM (GLVM). Then we discuss a framework
for learning a model based on the GLVM scoring function. We theoretically
showcase how some of the current visual recognition methods can benefit from
this generalization. Finally, we experiment on a generalized form of Deformable
Part Models with negative latent variables and show significant improvements on
two different detection tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02145</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02145</id><created>2015-07-08</created><updated>2015-07-13</updated><authors><author><keyname>Huang</keyname><forenames>Xiaojiang</forenames></author><author><keyname>Wan</keyname><forenames>Xiaojun</forenames></author><author><keyname>Xiao</keyname><forenames>Jianguo</forenames></author></authors><title>Learning to Mine Chinese Coordinate Terms Using the Web</title><categories>cs.CL</categories><comments>This paper was written several years ago</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coordinate relation refers to the relation between instances of a concept and
the relation between the directly hyponyms of a concept. In this paper, we
focus on the task of extracting terms which are coordinate with a user given
seed term in Chinese, and grouping the terms which belong to different concepts
if the seed term has several meanings. We propose a semi-supervised method that
integrates manually defined linguistic patterns and automatically learned
semi-structural patterns to extract coordinate terms in Chinese from web search
results. In addition, terms are grouped into different concepts based on their
co-occurring terms and contexts. We further calculate the saliency scores of
extracted terms and rank them accordingly. Experimental results demonstrate
that our proposed method generates results with high quality and wide coverage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02150</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02150</id><created>2015-07-08</created><authors><author><keyname>Mao</keyname><forenames>Xinhua</forenames></author></authors><title>SAR Imaging of Moving Target based on Knowledge-aided Two-dimensional
  Autofocus</title><categories>cs.IT cs.CV math.IT</categories><comments>11 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to uncertainty on target's motion, the range cell migration (RCM) and
azimuth phase error (APE) of moving targets can't be completely compensated in
synthetic aperture radar (SAR) processing. Therefore, moving targets often
appear two-dimensional (2-D) defocused in SAR images. In this paper, a 2-D
autofocus method for refocusing defocused moving targets in SAR images is
presented. The new method only requires a direct estimate of APE, while the
residual 2-D phase error ( or RCM) is computed from the estimated APE by
exploiting the analytical relationship between the 2-D phase error ( or RCM)
and APE. Because the parameter estimation is performed in the reduced-dimension
space by exploiting prior knowledge on phase error structure, the proposed
approach offers clear advantages in both computational efficiency and
estimation accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02154</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02154</id><created>2015-07-08</created><authors><author><keyname>Landesa-V&#xe1;zquez</keyname><forenames>Iago</forenames></author><author><keyname>Alba-Castro</keyname><forenames>Jos&#xe9; Luis</forenames></author></authors><title>Double-Base Asymmetric AdaBoost</title><categories>cs.CV cs.AI cs.LG</categories><journal-ref>Neurocomputing 118 (2013) 101-114</journal-ref><doi>10.1016/j.neucom.2013.02.019</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on the use of different exponential bases to define class-dependent
error bounds, a new and highly efficient asymmetric boosting scheme, coined as
AdaBoostDB (Double-Base), is proposed. Supported by a fully theoretical
derivation procedure, unlike most of the other approaches in the literature,
our algorithm preserves all the formal guarantees and properties of original
(cost-insensitive) AdaBoost, similarly to the state-of-the-art Cost-Sensitive
AdaBoost algorithm. However, the key advantage of AdaBoostDB is that our novel
derivation scheme enables an extremely efficient conditional search procedure,
dramatically improving and simplifying the training phase of the algorithm.
Experiments, both over synthetic and real datasets, reveal that AdaBoostDB is
able to save over 99% training time with regard to Cost-Sensitive AdaBoost,
providing the same cost-sensitive results. This computational advantage of
AdaBoostDB can make a difference in problems managing huge pools of weak
classifiers in which boosting techniques are commonly used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02158</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02158</id><created>2015-07-08</created><authors><author><keyname>Martino</keyname><forenames>Giovanni Da San</forenames></author><author><keyname>Navarin</keyname><forenames>Nicol&#xf2;</forenames></author><author><keyname>Sperduti</keyname><forenames>Alessandro</forenames></author></authors><title>An Empirical Study on Budget-Aware Online Kernel Algorithms for Streams
  of Graphs</title><categories>cs.LG</categories><comments>Preprint submitted to Neurocomputing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kernel methods are considered an effective technique for on-line learning.
Many approaches have been developed for compactly representing the dual
solution of a kernel method when the problem imposes memory constraints.
However, in literature no work is specifically tailored to streams of graphs.
Motivated by the fact that the size of the feature space representation of many
state-of-the-art graph kernels is relatively small and thus it is explicitly
computable, we study whether executing kernel algorithms in the feature space
can be more effective than the classical dual approach. We propose three
different algorithms and various strategies for managing the budget. Efficiency
and efficacy of the proposed approaches are experimentally assessed on
relatively large graph streams exhibiting concept drift. It turns out that,
when strict memory budget constraints have to be enforced, working in feature
space, given the current state of the art on graph kernels, is more than a
viable alternative to dual approaches, both in terms of speed and
classification performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02159</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02159</id><created>2015-07-08</created><authors><author><keyname>Wang</keyname><forenames>Limin</forenames></author><author><keyname>Xiong</keyname><forenames>Yuanjun</forenames></author><author><keyname>Wang</keyname><forenames>Zhe</forenames></author><author><keyname>Qiao</keyname><forenames>Yu</forenames></author></authors><title>Towards Good Practices for Very Deep Two-Stream ConvNets</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep convolutional networks have achieved great success for object
recognition in still images. However, for action recognition in videos, the
improvement of deep convolutional networks is not so evident. We argue that
there are two reasons that could probably explain this result. First the
current network architectures (e.g. Two-stream ConvNets) are relatively shallow
compared with those very deep models in image domain (e.g. VGGNet, GoogLeNet),
and therefore their modeling capacity is constrained by their depth. Second,
probably more importantly, the training dataset of action recognition is
extremely small compared with the ImageNet dataset, and thus it will be easy to
over-fit on the training dataset.
  To address these issues, this report presents very deep two-stream ConvNets
for action recognition, by adapting recent very deep architectures into video
domain. However, this extension is not easy as the size of action recognition
is quite small. We design several good practices for the training of very deep
two-stream ConvNets, namely (i) pre-training for both spatial and temporal
nets, (ii) smaller learning rates, (iii) more data augmentation techniques,
(iv) high drop out ratio. Meanwhile, we extend the Caffe toolbox into Multi-GPU
implementation with high computational efficiency and low memory consumption.
We verify the performance of very deep two-stream ConvNets on the dataset of
UCF101 and it achieves the recognition accuracy of $91.4\%$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02163</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02163</id><created>2015-07-08</created><updated>2015-09-01</updated><authors><author><keyname>Lokshtanov</keyname><forenames>Daniel</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Marcin</forenames></author><author><keyname>van Leeuwen</keyname><forenames>Erik Jan</forenames></author></authors><title>Independence and Efficient Domination on $P_6$-free Graphs</title><categories>cs.DS</categories><comments>v2: added reference to independent work arXiv:1508.07733</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Independent set problem, the input is a graph $G$, every vertex has a
non-negative integer weight, and the task is to find a set $S$ of pairwise
non-adjacent vertices, maximizing the total weight of the vertices in $S$. We
give an $n^{O (\log^2 n)}$ time algorithm for this problem on graphs excluding
the path $P_6$ on $6$ vertices as an induced subgraph. Currently, there is no
constant $k$ known for which Independent Set on $P_{k}$-free graphs becomes
NP-complete, and our result implies that if such a $k$ exists, then $k &gt; 6$
unless all problems in NP can be decided in (quasi)polynomial time.
  Using the combinatorial tools that we develop for the above algorithm, we
also give a polynomial-time algorithm for Efficient Dominating Set on
$P_6$-free graphs. In this problem, the input is a graph $G$, every vertex has
an integer weight, and the objective is to find a set $S$ of maximum weight
such that every vertex in $G$ has exactly one vertex in $S$ in its closed
neighborhood, or to determine that no such set exists. Prior to our work, the
class of $P_6$-free graphs was the only class of graphs defined by a single
forbidden induced subgraph on which the computational complexity of Efficient
Dominating Set was unknown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02168</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02168</id><created>2015-07-08</created><updated>2015-09-20</updated><authors><author><keyname>Pilipczuk</keyname><forenames>Marcin</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Micha&#x142;</forenames></author><author><keyname>Wrochna</keyname><forenames>Marcin</forenames></author></authors><title>Edge Bipartization faster than $2^k$</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Edge Bipartization problem one is given an undirected graph $G$ and an
integer $k$, and the question is whether $k$ edges can be deleted from $G$ so
that it becomes bipartite. In 2006, Guo et al. [J. Comput. Syst. Sci.,
72(8):1386-1396, 2006] proposed an algorithm solving this problem in time
$O(2^k m^2)$; today, this algorithm is a textbook example of an application of
the iterative compression technique. Despite extensive progress in the
understanding of the parameterized complexity of graph separation problems in
the recent years, no significant improvement upon this result has been yet
reported.
  We present an algorithm for Edge Bipartization that works in time $O(1.977^k
nm)$, which is the first algorithm with the running time dependence on the
parameter better than $2^k$. To this end, we combine the general iterative
compression strategy of Guo et al. [J. Comput. Syst. Sci., 72(8):1386-1396,
2006], the technique proposed by Wahlstrom [SODA 2014, 1762-1781] of using a
polynomial-time solvable relaxation in the form of a Valued Constraint
Satisfaction Problem to guide a bounded-depth branching algorithm, and an
involved Measure &amp; Conquer analysis of the recursion tree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02177</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02177</id><created>2015-07-08</created><authors><author><keyname>Minaee</keyname><forenames>Shervin</forenames></author><author><keyname>Abdolrashidi</keyname><forenames>AmirAli</forenames></author><author><keyname>Wang</keyname><forenames>Yao</forenames></author></authors><title>Iris Recognition Using Scattering Transform and Textural Features</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Iris recognition has drawn a lot of attention since the mid-twentieth
century. Among all biometric features, iris is known to possess a rich set of
features. Different features have been used to perform iris recognition in the
past. In this paper, two powerful sets of features are introduced to be used
for iris recognition: scattering transform-based features and textural
features. PCA is also applied on the extracted features to reduce the
dimensionality of the feature vector while preserving most of the information
of its initial value. Minimum distance classifier is used to perform template
matching for each new test sample. The proposed scheme is tested on a
well-known iris database, and showed promising results with the best accuracy
rate of 99.2%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02178</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02178</id><created>2015-07-08</created><authors><author><keyname>Pilipczuk</keyname><forenames>Marcin</forenames></author><author><keyname>Wahlstr&#xf6;m</keyname><forenames>Magnus</forenames></author></authors><title>Directed multicut is W[1]-hard, even for four terminal pairs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that Multicut in directed graphs, parameterized by the size of the
cutset, is W[1]-hard and hence unlikely to be fixed-parameter tractable even if
restricted to instances with only four terminal pairs. This negative result
almost completely resolves one of the central open problems in the area of
parameterized complexity of graph separation problems, posted originally by
Marx and Razgon [SIAM J. Comput. 43(2):355-388 (2014)], leaving only the case
of three terminal pairs open.
  Our gadget methodology allows us also to prove W[1]-hardness of the Steiner
Orientation problem parameterized by the number of terminal pairs, resolving an
open problem of Cygan, Kortsarz, and Nutov [SIAM J. Discrete Math.
27(3):1503-1513 (2013)].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02180</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02180</id><created>2015-07-08</created><updated>2015-09-28</updated><authors><author><keyname>Sobottka</keyname><forenames>Marcelo</forenames></author><author><keyname>Gon&#xe7;alves</keyname><forenames>Daniel</forenames></author></authors><title>A note on the definition of sliding block codes and the
  Curtis-Hedlund-Lyndon Theorem</title><categories>math.DS cs.IT math.HO math.IT</categories><comments>5 pages</comments><msc-class>37B10, 37B15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note we propose an alternative definition for sliding block codes
between shift spaces. This definition coincides with the usual definition in
the case that the shift space is defined on a finite alphabet, but it encompass
a larger class of maps when the alphabet is infinite. In any case, the proposed
definition keeps the idea that a sliding block code is a map with a local rule.
Using this new definition we prove that the Curtis-Hedlund-Lyndon Theorem
always holds for shift spaces over countable alphabets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02184</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02184</id><created>2015-07-08</created><updated>2016-01-24</updated><authors><author><keyname>Jeong</keyname><forenames>Jisu</forenames></author><author><keyname>Kim</keyname><forenames>Eun Jung</forenames></author><author><keyname>Oum</keyname><forenames>Sang-il</forenames></author></authors><title>The &quot;art of trellis decoding&quot; is fixed-parameter tractable</title><categories>cs.DS cs.CC cs.DM math.CO</categories><comments>45 pages, Changed the title, Expanded two sections. Accepted to SODA
  2016 (&quot;constructive algorithms for path-width of matroids&quot;)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given n subspaces of a finite-dimensional vector space over a fixed finite
field $\mathbb F$, we wish to find a linear layout $V_1,V_2,\ldots,V_n$ of the
subspaces such that $\dim((V_1+V_2+\cdots+V_i) \cap (V_{i+1}+\cdots+V_n))\le k$
for all i, such a linear layout is said to have width at most k. When
restricted to 1-dimensional subspaces, this problem is equivalent to computing
the trellis-width (or minimum trellis state-complexity) of a linear code in
coding theory and computing the path-width of an $\mathbb F$-represented
matroid in matroid theory.
  We present a fixed-parameter tractable algorithm to construct a linear layout
of width at most k, if it exists, for input subspaces of a finite-dimensional
vector space over $\mathbb F$. As corollaries, we obtain a fixed-parameter
tractable algorithm to produce a path-decomposition of width at most k for an
input $\mathbb F$-represented matroid of path-width at most k, and a
fixed-parameter tractable algorithm to find a linear rank-decomposition of
width at most k for an input graph of linear rank-width at most k. In both
corollaries, no such algorithms were known previously.
  It was previously known that a fixed-parameter tractable algorithm exists for
the decision version of the problem for matroid path-width, a theorem by
Geelen, Gerards, and Whittle~(2002) implies that for each fixed finite field
$\mathbb F$, there are finitely many forbidden $\mathbb F$-representable minors
for the class of matroids of path-width at most k. An algorithm by
Hlin\v{e}n\'y (2006) can detect a minor in an input $\mathbb F$-represented
matroid of bounded branch-width. However, this indirect approach would not
produce an actual path-decomposition. Our algorithm is the first one to
construct such a path-decomposition and does not depend on the finiteness of
forbidden minors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02186</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02186</id><created>2015-07-08</created><updated>2015-09-03</updated><authors><author><keyname>Navarin</keyname><forenames>Nicol&#xf2;</forenames></author><author><keyname>Sperduti</keyname><forenames>Alessandro</forenames></author><author><keyname>Tesselli</keyname><forenames>Riccardo</forenames></author></authors><title>Extending local features with contextual information in graph kernels</title><categories>cs.LG</categories><comments>To appear in ICONIP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph kernels are usually defined in terms of simpler kernels over local
substructures of the original graphs. Different kernels consider different
types of substructures. However, in some cases they have similar predictive
performances, probably because the substructures can be interpreted as
approximations of the subgraphs they induce. In this paper, we propose to
associate to each feature a piece of information about the context in which the
feature appears in the graph. A substructure appearing in two different graphs
will match only if it appears with the same context in both graphs. We propose
a kernel based on this idea that considers trees as substructures, and where
the contexts are features too. The kernel is inspired from the framework in
[6], even if it is not part of it. We give an efficient algorithm for computing
the kernel and show promising results on real-world graph classification
datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02188</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02188</id><created>2015-07-08</created><authors><author><keyname>Thakur</keyname><forenames>Abhishek</forenames></author><author><keyname>Krohn-Grimberghe</keyname><forenames>Artus</forenames></author></authors><title>AutoCompete: A Framework for Machine Learning Competition</title><categories>stat.ML cs.LG</categories><comments>Paper at AutoML workshop in ICML, 2015</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In this paper, we propose AutoCompete, a highly automated machine learning
framework for tackling machine learning competitions. This framework has been
learned by us, validated and improved over a period of more than two years by
participating in online machine learning competitions. It aims at minimizing
human interference required to build a first useful predictive model and to
assess the practical difficulty of a given machine learning challenge. The
proposed system helps in identifying data types, choosing a machine learn- ing
model, tuning hyper-parameters, avoiding over-fitting and optimization for a
provided evaluation metric. We also observe that the proposed system produces
better (or comparable) results with less runtime as compared to other
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02189</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02189</id><created>2015-07-08</created><authors><author><keyname>Ge</keyname><forenames>Rong</forenames></author><author><keyname>Zou</keyname><forenames>James</forenames></author></authors><title>Intersecting Faces: Non-negative Matrix Factorization With New
  Guarantees</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-negative matrix factorization (NMF) is a natural model of admixture and
is widely used in science and engineering. A plethora of algorithms have been
developed to tackle NMF, but due to the non-convex nature of the problem, there
is little guarantee on how well these methods work. Recently a surge of
research have focused on a very restricted class of NMFs, called separable NMF,
where provably correct algorithms have been developed. In this paper, we
propose the notion of subset-separable NMF, which substantially generalizes the
property of separability. We show that subset-separability is a natural
necessary condition for the factorization to be unique or to have minimum
volume. We developed the Face-Intersect algorithm which provably and
efficiently solves subset-separable NMF under natural conditions, and we prove
that our algorithm is robust to small noise. We explored the performance of
Face-Intersect on simulations and discuss settings where it empirically
outperformed the state-of-art methods. Our work is a step towards finding
provably correct algorithms that solve large classes of NMF problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02196</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02196</id><created>2015-07-08</created><authors><author><keyname>Bokslag</keyname><forenames>Wouter</forenames></author></authors><title>Reverse Engineering of RFID devices</title><categories>cs.CR</categories><comments>14 pages, 7 figures</comments><acm-class>B.1.3; B.4.m; B.8.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses the relevance and potential impact of both RFID and
reverse engineering of RFID technology, followed by a discussion of common
protocols and internals of RFID technology. The focus of the paper is on
providing an overview of the different approaches to reverse engineering RFID
technology and possible countermeasures that could limit the potential of such
reverse engineering attempts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02199</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02199</id><created>2015-07-08</created><updated>2015-07-26</updated><authors><author><keyname>Grebla</keyname><forenames>Guy</forenames></author><author><keyname>Birand</keyname><forenames>Berk</forenames></author><author><keyname>van de Ven</keyname><forenames>Peter</forenames></author><author><keyname>Zussman</keyname><forenames>Gil</forenames></author></authors><title>Joint Transmission in Cellular Networks with CoMP - Stability and
  Scheduling Algorithms</title><categories>cs.NI cs.IT math.IT</categories><comments>31 pages, 11 figures, and 2 appendixes</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the current trend towards smaller cells, an increasing number of users
of cellular networks reside at the edge between two cells; these users
typically receive poor service as a result of the relatively weak signal and
strong interference. Coordinated Multi-Point (CoMP) with Joint Transmission
(JT) is a cellular networking technique allowing multiple Base Stations (BSs)
to jointly transmit to a single user. This improves the users' reception
quality and facilitates better service to cell-edge users. We consider a
CoMP-enabled network, comprised of multiple BSs interconnected via a backhaul
network. We formulate the OFDMA Joint Scheduling (OJS) problem of determining a
subframe schedule and deciding if and how to use JT in order to maximize some
utility function. We show that the OJS problem is NP-hard. We develop optimal
and approximation algorithms for specific and general topologies, respectively.
We consider a time dimension and study a queueing model with packet arrivals in
which the service rates for each subframe are obtained by solving the OJS
problem. We prove that when the problem is formulated with a specific utility
function and solved optimally in each subframe, the resulting scheduling policy
is throughput-optimal. Via extensive simulations we show that the bulk of the
gains from CoMP with JT can be achieved with low capacity backhaul. Moreover,
our algorithms distribute the network resources evenly, increasing the
inter-cell users' throughput at only a slight cost to the intra-cell users.
This is the first step towards a rigorous, network-level understanding of the
impact of cross-layer scheduling algorithms on CoMP networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02205</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02205</id><created>2015-07-08</created><updated>2015-08-16</updated><authors><author><keyname>Jaech</keyname><forenames>Aaron</forenames></author><author><keyname>Zayats</keyname><forenames>Victoria</forenames></author><author><keyname>Fang</keyname><forenames>Hao</forenames></author><author><keyname>Ostendorf</keyname><forenames>Mari</forenames></author><author><keyname>Hajishirzi</keyname><forenames>Hannaneh</forenames></author></authors><title>Talking to the crowd: What do people react to in online discussions?</title><categories>cs.CL cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the question of how language use affects community
reaction to comments in online discussion forums, and the relative importance
of the message vs. the messenger. A new comment ranking task is proposed based
on community annotated karma in Reddit discussions, which controls for topic
and timing of comments. Experimental work with discussion threads from six
subreddits shows that the importance of different types of language features
varies with the community of interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02206</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02206</id><created>2015-07-08</created><updated>2015-08-05</updated><authors><author><keyname>Shin</keyname><forenames>Won-Yong</forenames></author><author><keyname>Singh</keyname><forenames>Bikash C.</forenames></author><author><keyname>Cho</keyname><forenames>Jaehee</forenames></author><author><keyname>Everett</keyname><forenames>Andr&#xe9; M.</forenames></author></authors><title>A New Understanding of Friendships in Space: Complex Networks Meet
  Twitter</title><categories>cs.SI</categories><comments>17 pages, 5 figures, 6 tables, To appear in Journal of Information
  Science (Special Issue on Recent Advances on Big Social Data)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Studies on friendships in online social networks involving geographic
distance have so far relied on the city location provided in users' profiles.
Consequently, most of the research on friendships have provided accuracy at the
city level, at best, to designate a user's location. This study analyzes a
Twitter dataset because it provides the exact geographic distance between
corresponding users. We start by introducing a strong definition of &quot;friend&quot; on
Twitter (i.e., a definition of bidirectional friendship), requiring
bidirectional communication. Next, we utilize geo-tagged mentions delivered by
users to determine their locations, where &quot;@username&quot; is contained anywhere in
the body of tweets. To provide analysis results, we first introduce a friend
counting algorithm. From the fact that Twitter users are likely to post
consecutive tweets in the static mode, we also introduce a two-stage distance
estimation algorithm. As the first of our main contributions, we verify that
the number of friends of a particular Twitter user follows a well-known
power-law distribution (i.e., a Zipf's distribution or a Pareto distribution).
Our study also provides the following newly-discovered friendship degree
related to the issue of space: The number of friends according to distance
follows a double power-law (i.e., a double Pareto law) distribution, indicating
that the probability of befriending a particular Twitter user is significantly
reduced beyond a certain geographic distance between users, termed the
separation point. Our analysis provides concrete evidence that Twitter can be a
useful platform for assigning a more accurate scalar value to the degree of
friendship between two users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02216</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02216</id><created>2015-07-08</created><authors><author><keyname>Chenot</keyname><forenames>Cecile</forenames></author><author><keyname>Bobin</keyname><forenames>Jerome</forenames></author><author><keyname>Rapin</keyname><forenames>Jeremy</forenames></author></authors><title>Robust Sparse Blind Source Separation</title><categories>stat.AP cs.LG stat.ML</categories><comments>Submitted to IEEE Signal Processing Letters</comments><doi>10.1109/LSP.2015.2463232</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Blind Source Separation is a widely used technique to analyze multichannel
data. In many real-world applications, its results can be significantly
hampered by the presence of unknown outliers. In this paper, a novel algorithm
coined rGMCA (robust Generalized Morphological Component Analysis) is
introduced to retrieve sparse sources in the presence of outliers. It
explicitly estimates the sources, the mixing matrix, and the outliers. It also
takes advantage of the estimation of the outliers to further implement a
weighting scheme, which provides a highly robust separation procedure.
Numerical experiments demonstrate the efficiency of rGMCA to estimate the
mixing matrix in comparison with standard BSS techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02221</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02221</id><created>2015-07-08</created><authors><author><keyname>Sordoni</keyname><forenames>Alessandro</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author><author><keyname>Vahabi</keyname><forenames>Hossein</forenames></author><author><keyname>Lioma</keyname><forenames>Christina</forenames></author><author><keyname>Simonsen</keyname><forenames>Jakob G.</forenames></author><author><keyname>Nie</keyname><forenames>Jian-Yun</forenames></author></authors><title>A Hierarchical Recurrent Encoder-Decoder For Generative Context-Aware
  Query Suggestion</title><categories>cs.NE cs.IR</categories><comments>To appear in Conference of Information Knowledge and Management
  (CIKM) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Users may strive to formulate an adequate textual query for their information
need. Search engines assist the users by presenting query suggestions. To
preserve the original search intent, suggestions should be context-aware and
account for the previous queries issued by the user. Achieving context
awareness is challenging due to data sparsity. We present a probabilistic
suggestion model that is able to account for sequences of previous queries of
arbitrary lengths. Our novel hierarchical recurrent encoder-decoder
architecture allows the model to be sensitive to the order of queries in the
context while avoiding data sparsity. Additionally, our model can suggest for
rare, or long-tail, queries. The produced suggestions are synthetic and are
sampled one word at a time, using computationally cheap decoding techniques.
This is in contrast to current synthetic suggestion models relying upon machine
learning pipelines and hand-engineered feature sets. Results show that it
outperforms existing context-aware approaches in a next query prediction
setting. In addition to query suggestion, our model is general enough to be
used in a variety of other applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02222</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02222</id><created>2015-07-08</created><updated>2015-12-09</updated><authors><author><keyname>Bandyapadhyay</keyname><forenames>Sayan</forenames></author><author><keyname>Varadarajan</keyname><forenames>Kasturi</forenames></author></authors><title>Approximate Clustering via Metric Partitioning</title><categories>cs.CG cs.DS math.PR</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the following metric clustering problem. We are given two point
sets $X$ (clients) and $Y$ (servers), and a metric on $Z = X \cup Y$. We would
like to cover the clients by balls centered at the servers. The objective
function to minimize is the sum of the $\alpha$-th power of the radii of the
balls. Here $\alpha \geq 1$ is a parameter of the problem (but not of a problem
instance).
  For any $\eps &gt; 0$, we describe a quasi-polynomial time algorithm that
returns a $(1 + \eps)$ approximation for the problem. Prior to our work, a
$3^{\alpha}$ approximation was achieved by a polynomial-time algorithm. In
contrast, for the variant of the problem where $\alpha$ is part of the input,
we show under standard assumptions that no polynomial time algorithm can
achieve an approximation factor better than $O(\log |X|)$ for $\alpha \geq \log
|X|$.
  In order to achieve the QPTAS, we address the following problem on metric
partitioning: we want to probabilistically partition $Z$ into blocks of at most
half the diameter so that for any ball, the expected number of blocks of the
partition that intersect the ball is appropriately small. We note that this
problem can be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02226</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02226</id><created>2015-07-08</created><authors><author><keyname>Stout</keyname><forenames>Quentin F.</forenames></author></authors><title>L infinity Isotonic Regression for Linear, Multidimensional, and Tree
  Orders</title><categories>cs.DS stat.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Algorithms are given for determining $L_\infty$ isotonic regression of
weighted data where the independent set is n vertices in multidimensional space
or in a rooted tree. For a linear order, or, more generally, a grid in
multidimensional space, an optimal algorithm is given, taking $\Theta(n)$ time.
For vertices at arbitrary locations in d-dimensional space a $\Theta(n
\log^{d-1} n)$ algorithm employs iterative sorting to yield the functionality
of a multidimensional structure while using only $\Theta(n)$ space. A
$\Theta(n)$ time algorithm is also given for rooted trees. These improve upon
previous algorithms by $\Omega(\log n)$. The algorithms utilize a new
non-constructive feasibility test on a rendezvous graph, with bounded error
envelopes at each vertex.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02250</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02250</id><created>2015-07-08</created><authors><author><keyname>Ny</keyname><forenames>Jerome Le</forenames></author></authors><title>Privacy-Preserving Nonlinear Observer Design Using Contraction Analysis</title><categories>cs.SY cs.IT cs.SI math.IT</categories><comments>8 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real-time signal processing applications are increasingly focused on
analyzing privacy-sensitive data obtained from individuals, and this data might
need to be processed through model-based estimators to produce accurate
statistics. Moreover, the models used in population dynamics studies, e.g., in
epidemiology or sociology, are often necessarily nonlinear. This paper presents
a design approach for nonlinear privacy-preserving model-based observers,
relying on contraction analysis to give differential privacy guarantees to the
individuals providing the input data. The approach is illustrated in two
applications: estimation of edge formation probabilities in a dynamic social
network, and syndromic surveillance relying on an epidemiological model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02259</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02259</id><created>2015-07-08</created><updated>2016-01-11</updated><authors><author><keyname>Allen-Zhu</keyname><forenames>Zeyuan</forenames></author><author><keyname>Lee</keyname><forenames>Yin Tat</forenames></author><author><keyname>Orecchia</keyname><forenames>Lorenzo</forenames></author></authors><title>Using Optimization to Obtain a Width-Independent, Parallel, Simpler, and
  Faster Positive SDP Solver</title><categories>cs.DS cs.DC math.NA math.OC math.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the design of polylogarithmic depth algorithms for approximately
solving packing and covering semidefinite programs (or positive SDPs for
short). This is a natural SDP generalization of the well-studied positive LP
problem.
  Although positive LPs can be solved in polylogarithmic depth while using only
$\tilde{O}(\log^{2} n/\varepsilon^2)$ parallelizable iterations, the best known
positive SDP solvers due to Jain and Yao require $O(\log^{14} n
/\varepsilon^{13})$ parallelizable iterations. Several alternative solvers have
been proposed to reduce the exponents in the number of iterations. However, the
correctness of the convergence analyses in these works has been called into
question, as they both rely on algebraic monotonicity properties that do not
generalize to matrix algebra.
  In this paper, we propose a very simple algorithm based on the optimization
framework proposed for LP solvers. Our algorithm only needs $\tilde{O}(\log^2 n
/ \varepsilon^2)$ iterations, matching that of the best LP solver. To surmount
the obstacles encountered by previous approaches, our analysis requires a new
matrix inequality that extends Lieb-Thirring's inequality, and a
sign-consistent, randomized variant of the gradient truncation technique
proposed in.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02266</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02266</id><created>2015-07-08</created><authors><author><keyname>Xie</keyname><forenames>Jianwei</forenames></author><author><keyname>Ulukus</keyname><forenames>Sennur</forenames></author></authors><title>Secure Degrees of Freedom of Multi-user Networks: One-Time-Pads in the
  Air via Alignment</title><categories>cs.IT cs.CR math.IT</categories><comments>To appear in Proceedings of the IEEE, special issue on Physical Layer
  Security and its Applications. arXiv admin note: text overlap with
  arXiv:1404.7478, arXiv:1209.5370</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the recent secure degrees of freedom (s.d.o.f.) results for
one-hop multi-user wireless networks by considering three fundamental wireless
network structures: Gaussian wiretap channel with helpers, Gaussian multiple
access wiretap channel, and Gaussian interference channel with secrecy
constraints. We present main enabling tools and resulting communication schemes
in an expository manner, along with key insights and design principles emerging
from them. The main achievable schemes are based on real interference
alignment, channel prefixing via cooperative jamming, and structured
signalling. Real interference alignment enables aligning the cooperative
jamming signals together with the message carrying signals at the eavesdroppers
to protect them akin to one-time-pad protecting messages in wired systems. Real
interference alignment also enables decodability at the legitimate receivers by
rendering message carrying and cooperative jamming signals separable, and
simultaneously aligning the cooperative jamming signals in the smallest
possible sub-space. The main converse techniques are based on two key lemmas
which quantify the secrecy penalty by showing that the net effect of an
eavesdropper on the system is that it eliminates one of the independent channel
inputs; and the role of a helper by developing a direct relationship between
the cooperative jamming signal of a helper and the message rate. These two
lemmas when applied according to the unique structure of individual networks
provide tight converses. Finally, we present a blind cooperative jamming scheme
for the helper network with no eavesdropper channel state information at the
transmitters that achieves the same optimal s.d.o.f. as in the case of full
eavesdropper channel state information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02268</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02268</id><created>2015-07-08</created><updated>2016-03-02</updated><authors><author><keyname>Cohen</keyname><forenames>Michael B.</forenames></author><author><keyname>Nelson</keyname><forenames>Jelani</forenames></author><author><keyname>Woodruff</keyname><forenames>David P.</forenames></author></authors><title>Optimal approximate matrix product in terms of stable rank</title><categories>cs.DS cs.LG stat.ML</categories><comments>v3: minor edits; v2: fixed one step in proof of Theorem 9 which was
  wrong by a constant factor (see the new Lemma 5 and its use; final theorem
  unaffected)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove, using the subspace embedding guarantee in a black box way, that one
can achieve the spectral norm guarantee for approximate matrix multiplication
with a dimensionality-reducing map having $m = O(\tilde{r}/\varepsilon^2)$
rows. Here $\tilde{r}$ is the maximum stable rank, i.e. squared ratio of
Frobenius and operator norms, of the two matrices being multiplied. This is a
quantitative improvement over previous work of [MZ11, KVZ14], and is also
optimal for any oblivious dimensionality-reducing map. Furthermore, due to the
black box reliance on the subspace embedding property in our proofs, our
theorem can be applied to a much more general class of sketching matrices than
what was known before, in addition to achieving better bounds. For example, one
can apply our theorem to efficient subspace embeddings such as the Subsampled
Randomized Hadamard Transform or sparse subspace embeddings, or even with
subspace embedding constructions that may be developed in the future.
  Our main theorem, via connections with spectral error matrix multiplication
shown in prior work, implies quantitative improvements for approximate least
squares regression and low rank approximation. Our main result has also already
been applied to improve dimensionality reduction guarantees for $k$-means
clustering [CEMMP14], and implies new results for nonparametric regression
[YPW15].
  We also separately point out that the proof of the &quot;BSS&quot; deterministic
row-sampling result of [BSS12] can be modified to show that for any matrices
$A, B$ of stable rank at most $\tilde{r}$, one can achieve the spectral norm
guarantee for approximate matrix multiplication of $A^T B$ by deterministically
sampling $O(\tilde{r}/\varepsilon^2)$ rows that can be found in polynomial
time. The original result of [BSS12] was for rank instead of stable rank. Our
observation leads to a stronger version of a main theorem of [KMST10].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02272</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02272</id><created>2015-07-08</created><updated>2015-09-27</updated><authors><author><keyname>Chlebus</keyname><forenames>Bogdan S.</forenames></author><author><keyname>De Marco</keyname><forenames>Gianluca</forenames></author><author><keyname>Talo</keyname><forenames>Muhammed</forenames></author></authors><title>Anonymous Processors with Synchronous Shared Memory</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate anonymous processors computing in a synchronous manner and
communicating via read-write shared memory. This system is known as a parallel
random access machine (PRAM). It is parameterized by a number of processors~$n$
and a number of shared memory cells. We consider the problem of assigning
unique integer names from the interval $[1,n]$ to all $n$ processors of a PRAM.
We develop algorithms for each of the eight specific cases determined by which
of the following independent properties hold: (1) concurrently attempting to
write distinct values into the same memory cell either is allowed or not, (2)
the number of shared variables either is unlimited or it is a constant
independent of $n$, and (3) the number of processors~$n$ either is known or it
is unknown. Our algorithms terminate almost surely, they are Las Vegas when $n$
is known, they are Monte Carlo when $n$ is unknown, and they always use the
$O(n\log n)$ expected number of random bits. We show lower bounds on time,
depending on whether the amounts of shared memory are constant or unlimited. In
view of these lower bounds, all the Las Vegas algorithms we develop are
asymptotically optimal with respect to their expected time, as determined by
the available shared memory. Our Monte Carlo algorithms are correct with
probabilities that are $1-n^{-\Omega(1)}$, which is best possible when
terminating almost surely and using $O(n\log n)$ random bits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02284</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02284</id><created>2015-07-08</created><authors><author><keyname>Steeg</keyname><forenames>Greg Ver</forenames></author><author><keyname>Galstyan</keyname><forenames>Aram</forenames></author></authors><title>The Information Sieve</title><categories>stat.ML cs.IT cs.LG math.IT</categories><comments>12 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new framework for unsupervised learning of deep
representations based on a novel hierarchical decomposition of information.
Intuitively, data is passed through a series of progressively fine-grained
sieves. Each layer of the sieve recovers a single latent factor that is
maximally informative about multivariate dependence in the data. The data is
transformed after each pass so that the remaining unexplained information
trickles down to the next layer. Ultimately, we are left with a set of latent
factors explaining all the dependence in the original data and remainder
information consisting of independent noise. We present a practical
implementation of this framework for discrete variables and apply it to a
variety of tasks including independent component analysis, lossy and lossless
compression, and predicting missing values in data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02293</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02293</id><created>2015-07-08</created><authors><author><keyname>Farajtabar</keyname><forenames>Mehrdad</forenames></author><author><keyname>Wang</keyname><forenames>Yichen</forenames></author><author><keyname>Rodriguez</keyname><forenames>Manuel Gomez</forenames></author><author><keyname>Li</keyname><forenames>Shuang</forenames></author><author><keyname>Zha</keyname><forenames>Hongyuan</forenames></author><author><keyname>Song</keyname><forenames>Le</forenames></author></authors><title>COEVOLVE: A Joint Point Process Model for Information Diffusion and
  Network Co-evolution</title><categories>cs.SI cs.LG physics.soc-ph stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information diffusion in online social networks is affected by the underlying
network topology, but it also has the power to change it. Online users are
constantly creating new links when exposed to new information sources, and in
turn these links are alternating the way information spreads. However, these
two highly intertwined stochastic processes, information diffusion and network
evolution, have been predominantly studied separately, ignoring their
co-evolutionary dynamics.
  We propose a temporal point process model, COEVOLVE, for such joint dynamics,
allowing the intensity of one process to be modulated by that of the other.
This model allows us to efficiently simulate interleaved diffusion and network
events, and generate traces obeying common diffusion and network patterns
observed in real-world networks. Furthermore, we also develop a convex
optimization framework to learn the parameters of the model from historical
diffusion and network evolution traces. We experimented with both synthetic
data and data gathered from Twitter, and show that our model provides a good
fit to the data as well as more accurate predictions than alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02297</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02297</id><created>2015-07-08</created><updated>2015-08-11</updated><authors><author><keyname>Goldberg</keyname><forenames>Eugene</forenames></author></authors><title>Equivalence Checking and Simulation By Computing Range Reduction</title><categories>cs.LO</categories><comments>The difference of this version from the previous one (i.e. version
  number 2) is twofold. First, I improved the readability of the paper. Second,
  I removed the claim that equivalence checking by computing range reduction is
  noise-insensitive. The final result is indeed noise-insensitive but the
  presence of noise may drastically slow down an algorithm computing this
  result</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce new methods of equivalence checking and simulation based on
Computing Range Reduction (CRR). Given a combinational circuit $N$, the CRR
problem is to compute the set of outputs that disappear from the range of $N$
if a set of inputs of $N$ is excluded from consideration. Importantly, in many
cases, range reduction can be efficiently found even if computing the entire
range of $N$ is infeasible.
  Solving equivalence checking by CRR facilitates generation of proofs of
equivalence that mimic a &quot;cut propagation&quot; approach. A limited version of such
an approach has been successfully used by commercial tools. Functional
verification of a circuit $N$ by simulation can be viewed as a way to reduce
the complexity of computing the range of $N$. Instead of finding the entire
range of $N$ and checking if it contains a bad output, such a range is computed
only for one input. Simulation by CRR offers an alternative way of coping with
the complexity of range computation. The idea is to exclude a subset of inputs
of $N$ and compute the range reduction caused by such an exclusion. If the set
of disappeared outputs contains a bad one, then $N$ is buggy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02301</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02301</id><created>2015-07-08</created><authors><author><keyname>Fotakis</keyname><forenames>Dimitris</forenames></author><author><keyname>Tzamos</keyname><forenames>Christos</forenames></author><author><keyname>Zampetakis</keyname><forenames>Emmanouil</forenames></author></authors><title>Who to Trust for Truthfully Maximizing Welfare?</title><categories>cs.GT</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  We introduce a general approach based on \emph{selective verification} and
obtain approximate mechanisms without money for maximizing the social welfare
in the general domain of utilitarian voting. Having a good allocation in mind,
a mechanism with verification selects few critical agents and detects, using a
verification oracle, whether they have reported truthfully. If yes, the
mechanism produces the desired allocation. Otherwise, the mechanism ignores any
misreports and proceeds with the remaining agents. We obtain randomized
truthful (or almost truthful) mechanisms without money that verify only $O(\ln
m / \epsilon)$ agents, where $m$ is the number of outcomes, independently of
the total number of agents, and are $(1-\epsilon)$-approximate for the social
welfare. We also show that any truthful mechanism with a constant approximation
ratio needs to verify $\Omega(\log m)$ agents. A remarkable property of our
mechanisms is \emph{robustness}, namely that their outcome depends only on the
reports of the truthful agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02313</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02313</id><created>2015-07-08</created><authors><author><keyname>Athiwaratkun</keyname><forenames>Ben</forenames></author><author><keyname>Kang</keyname><forenames>Keegan</forenames></author></authors><title>Feature Representation in Convolutional Neural Networks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional Neural Networks (CNNs) are powerful models that achieve
impressive results for image classification. In addition, pre-trained CNNs are
also useful for other computer vision tasks as generic feature extractors. This
paper aims to gain insight into the feature aspect of CNN and demonstrate other
uses of CNN features. Our results show that CNN feature maps can be used with
Random Forests and SVM to yield classification results that outperforms the
original CNN. A CNN that is less than optimal (e.g. not fully trained or
overfitting) can also extract features for Random Forest/SVM that yield
competitive classification accuracy. In contrast to the literature which uses
the top-layer activations as feature representation of images for other tasks,
using lower-layer features can yield better results for classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02314</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02314</id><created>2015-07-08</created><authors><author><keyname>Kiefer</keyname><forenames>Stefan</forenames></author><author><keyname>Sistla</keyname><forenames>A. Prasad</forenames></author></authors><title>Distinguishing Hidden Markov Chains</title><categories>cs.DS cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hidden Markov Chains (HMCs) are commonly used mathematical models of
probabilistic systems. They are employed in various fields such as speech
recognition, signal processing, and biological sequence analysis. We consider
the problem of distinguishing two given HMCs based on an observation sequence
that one of the HMCs generates. More precisely, given two HMCs and an
observation sequence, a distinguishing algorithm is expected to identify the
HMC that generates the observation sequence. Two HMCs are called
distinguishable if for every $\varepsilon &gt; 0$ there is a distinguishing
algorithm whose error probability is less than $\varepsilon$. We show that one
can decide in polynomial time whether two HMCs are distinguishable. Further, we
present and analyze two distinguishing algorithms for distinguishable HMCs. The
first algorithm makes a decision after processing a fixed number of
observations, and it exhibits two-sided error. The second algorithm processes
an unbounded number of observations, but the algorithm has only one-sided
error. The error probability, for both algorithms, decays exponentially with
the number of processed observations. We also provide an algorithm for
distinguishing multiple HMCs. Finally, we discuss an application in stochastic
runtime verification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02317</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02317</id><created>2015-07-08</created><authors><author><keyname>You</keyname><forenames>Seungil</forenames></author><author><keyname>Matni</keyname><forenames>Nikolai</forenames></author></authors><title>A Convex Approach to Sparse H infinity Analysis &amp; Synthesis</title><categories>cs.SY math.OC</categories><comments>9 pages, Submitted to 54th CDC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new robust analysis tool motivated by large-scale
systems. The H infinity norm of a system measures its robustness by quantifying
the worst-case behavior of a system perturbed by a unit-energy disturbance.
However, the disturbance that induces such worst-case behavior requires perfect
coordination among all disturbance channels. Given that many systems of
interest, such as the power grid, the internet and automated vehicle platoons,
are large-scale and spatially distributed, such coordination may not be
possible, and hence the H infinity norm, used as a measure of robustness, may
be too conservative. We therefore propose a cardinality constrained variant of
the H infinity norm in which an adversarial disturbance can use only a limited
number of channels. As this problem is inherently combinatorial, we present a
semidefinite programming (SDP) relaxation based on the l1 norm that yields an
upper bound on the cardinality constrained robustness problem. We further
propose a simple rounding heuristic based on the optimal solution of SDP
relaxation which provides a lower bound. Motivated by privacy in large-scale
systems, we also extend these relaxations to computing the minimum gain of a
system subject to a limited number of inputs. Finally, we also present a SDP
based optimal controller synthesis method for minimizing the SDP relaxation of
our novel robustness measure. The effectiveness of our semidefinite relaxation
is demonstrated through numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02318</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02318</id><created>2015-07-08</created><authors><author><keyname>Koiliaris</keyname><forenames>Konstantinos</forenames></author><author><keyname>Xu</keyname><forenames>Chao</forenames></author></authors><title>A Faster Pseudopolynomial Time Algorithm for Subset Sum</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a faster pseudopolynomial time algorithm for the SubsetSum
problem: deciding if there exists a subset of a given set $S$ whose elements
sum to a target number $t$, in $\tilde{O}\left(\sqrt{n}\:t\right)$ time, where
$n$ is the size of $S$. In fact, we answer a more general question than that,
we compute this for all target numbers $t\leq u$ in
$\tilde{O}\left(\sqrt{n}\:u\right)$. Our algorithm improves on the textbook
$O(nu)$ dynamic programming algorithm, and as far as we know, is the fastest
general algorithm for the problem. Our approach is based on a fast Minkowski
sum calculation that exploits the structure of subset sums of small intervals.
Finally, we attempt to shed some light on the number of applications this
problem and its variations have.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02319</identifier>
 <datestamp>2015-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02319</id><created>2015-07-08</created><updated>2015-07-10</updated><authors><author><keyname>Alshamary</keyname><forenames>Haider Ali Jasim</forenames></author><author><keyname>Anjum</keyname><forenames>Md Fahim</forenames></author><author><keyname>Al-Naffouri</keyname><forenames>Tareq</forenames></author><author><keyname>Zaib</keyname><forenames>Alam</forenames></author><author><keyname>Xu</keyname><forenames>Weiyu</forenames></author></authors><title>Optimal Non-coherent Data Detection for Massive SIMO Wireless Systems
  with General Constellations: A Polynomial Complexity Solution</title><categories>cs.IT math.IT</categories><comments>Journal version. Conference version accepted to IEEE Signal
  Processing Workshop. arXiv admin note: substantial text overlap with
  arXiv:1411.6739</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive MIMO systems can greatly increase spectral and energy efficiency over
traditional MIMO systems by exploiting large antenna arrays. However,
increasing the number of antennas at the base station (BS) makes the uplink
non-coherent data detection very challenging in massive MIMO systems. In this
paper we consider the joint maximum likelihood (ML) channel estimation and data
detection problem for massive SIMO (single input multiple output) wireless
systems, which is a special case of wireless systems with large antenna arrays.
We propose exact ML non-coherent data detection algorithms for both
constant-modulus and nonconstant-modulus constellations, with a low expected
complexity. Despite the large number of unknown channel coefficients for
massive SIMO systems, we show that the expected computational complexity of
these algorithms is linear in the number of receive antennas and polynomial in
channel coherence time. Simulation results show the performance gains (up to 5
dB improvement) of the optimal non-coherent data detection with a low
computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02321</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02321</id><created>2015-07-08</created><authors><author><keyname>Cur&#xe9;</keyname><forenames>Olivier</forenames></author><author><keyname>Naacke</keyname><forenames>Hubert</forenames></author><author><keyname>Baazizi</keyname><forenames>Mohamed-Amine</forenames></author><author><keyname>Amann</keyname><forenames>Bernd</forenames></author></authors><title>On the Evaluation of RDF Distribution Algorithms Implemented over Apache
  Spark</title><categories>cs.DB</categories><comments>16 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Querying very large RDF data sets in an efficient manner requires a
sophisticated distribution strategy. Several innovative solutions have recently
been proposed for optimizing data distribution with predefined query workloads.
This paper presents an in-depth analysis and experimental comparison of five
representative and complementary distribution approaches. For achieving fair
experimental results, we are using Apache Spark as a common parallel computing
framework by rewriting the concerned algorithms using the Spark API. Spark
provides guarantees in terms of fault tolerance, high availability and
scalability which are essential in such systems. Our different implementations
aim to highlight the fundamental implementation-independent characteristics of
each approach in terms of data preparation, load balancing, data replication
and to some extent to query answering cost and performance. The presented
measures are obtained by testing each system on one synthetic and one
real-world data set over query workloads with differing characteristics and
different partitioning constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02323</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02323</id><created>2015-07-08</created><authors><author><keyname>Agarwal</keyname><forenames>Naman</forenames></author><author><keyname>Bandeira</keyname><forenames>Afonso S.</forenames></author><author><keyname>Koiliaris</keyname><forenames>Konstantinos</forenames></author><author><keyname>Kolla</keyname><forenames>Alexandra</forenames></author></authors><title>Multisection in the Stochastic Block Model using Semidefinite
  Programming</title><categories>cs.DS math.PR stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of identifying underlying community-like structures
in graphs. Towards this end we study the Stochastic Block Model (SBM) on
$k$-clusters: a random model on $n=km$ vertices, partitioned in $k$ equal sized
clusters, with edges sampled independently across clusters with probability $q$
and within clusters with probability $p$, $p&gt;q$. The goal is to recover the
initial &quot;hidden&quot; partition of $[n]$. We study semidefinite programming (SDP)
based algorithms in this context. In the regime $p = \frac{\alpha \log(m)}{m}$
and $q = \frac{\beta \log(m)}{m}$ we show that a certain natural SDP based
algorithm solves the problem of {\em exact recovery} in the $k$-community SBM,
with high probability, whenever $\sqrt{\alpha} - \sqrt{\beta} &gt; \sqrt{1}$, as
long as $k=o(\log n)$. This threshold is known to be the information
theoretically optimal. We also study the case when $k=\theta(\log(n))$. In this
case however we achieve recovery guarantees that no longer match the optimal
condition $\sqrt{\alpha} - \sqrt{\beta} &gt; \sqrt{1}$, thus leaving achieving
optimality for this range an open question.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02331</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02331</id><created>2015-07-08</created><updated>2015-12-11</updated><authors><author><keyname>Bibak</keyname><forenames>Khodakhast</forenames></author><author><keyname>Kapron</keyname><forenames>Bruce M.</forenames></author><author><keyname>Srinivasan</keyname><forenames>Venkatesh</forenames></author><author><keyname>T&#xf3;th</keyname><forenames>L&#xe1;szl&#xf3;</forenames></author></authors><title>On an almost-universal hash function family with applications to
  authentication and secrecy codes</title><categories>cs.CR cs.DS math.CO math.NT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Universal hashing, discovered by Carter and Wegman in 1979, has many
important applications in computer science. The following family, called
MMH$^*$ by Halevi and Krawczyk in 1997, is well known: Let $p$ be a prime and
$k$ be a positive integer. Define \begin{align*} \text{MMH}^*:=\lbrace
g_{\mathbf{x}} \; : \; \mathbb{Z}_p^k \rightarrow \mathbb{Z}_p \; | \;
\mathbf{x}\in \mathbb{Z}_p^k \rbrace, \end{align*} where \begin{align*}
g_{\mathbf{x}}(\mathbf{m}) := \mathbf{m} \cdot \mathbf{x} \pmod{p} =
\sum_{i=1}^k m_ix_i \pmod{p}, \end{align*} for any $\mathbf{x}=\langle x_1,
\ldots, x_k \rangle \in \mathbb{Z}_p^k$, and any $\mathbf{m}=\langle m_1,
\ldots, m_k \rangle \in \mathbb{Z}_p^k$. In this paper, we first give a new
proof for the $\triangle$-universality of MMH$^*$, shown by Halevi and Krawczyk
in 1997, via a novel approach, namely, connecting the universal hashing problem
to the number of solutions of (restricted) linear congruences. We then
introduce a new hash function family --- a variant of MMH$^*$ --- that we call
GRDH, where we use an arbitrary integer $n&gt;1$ instead of prime $p$ and let the
keys $\mathbf{x}=\langle x_1, \ldots, x_k \rangle \in \mathbb{Z}_n^k$ satisfy
the conditions $\gcd(x_i,n)=t_i$ ($1\leq i\leq k$), where $t_1,\ldots,t_k$ are
given positive divisors of $n$. Applying our aforementioned approach, we prove
that the family GRDH is an $\varepsilon$-almost-$\triangle$-universal family of
hash functions for some $\varepsilon&lt;1$ if and only if $n$ is odd and
$\gcd(x_i,n)=t_i=1$ $(1\leq i\leq k)$. Furthermore, if these conditions are
satisfied then GRDH is $\frac{1}{p-1}$-almost-$\triangle$-universal, where $p$
is the smallest prime divisor of $n$. Finally, as an application of our
results, we give a generalization of the authentication code with secrecy
studied by Alomair, Clark, and Poovendran.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02342</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02342</id><created>2015-07-08</created><updated>2015-07-26</updated><authors><author><keyname>Issa</keyname><forenames>Ibrahim</forenames></author><author><keyname>Wagner</keyname><forenames>Aaron B.</forenames></author></authors><title>Measuring Secrecy by the Probability of a Successful Guess</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The secrecy of a communication system in which both the legitimate receiver
and an eavesdropper are allowed some distortion is investigated. The secrecy
metric considered is the exponent of the probability that the eavesdropper
estimates the source sequence successfully within an acceptable distortion
level. When the transmitter and the legitimate receiver do not share any key
and the transmitter is not subject to a rate constraint, a single-letter
characterization of the highest achievable exponent is provided. Moreover,
asymptotically-optimal universal strategies for both the primary user and the
eavesdropper are demonstrated, where universality means independence of the
source statistics. When the transmitter and the legitimate receiver share a
secret key and the transmitter is subject to a rate constraint, upper and lower
bounds are derived on the exponent by analyzing the performance of suggested
strategies for the primary user and the eavesdropper. The bounds admit a
single-letter characterization and they match under certain conditions, which
include the case in which the eavesdropper must reconstruct the source exactly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02346</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02346</id><created>2015-07-08</created><authors><author><keyname>Pabico</keyname><forenames>Jaderick P.</forenames></author><author><keyname>De Grano</keyname><forenames>Alona V.</forenames></author><author><keyname>Zarsuela</keyname><forenames>Alan L.</forenames></author></authors><title>Neural Network Classifiers for Natural Food Products</title><categories>cs.CV</categories><comments>8 pages, 5 figures, appeared in H.N. Adorna, R.E.O. Roxas, and A.L.
  Sioson (eds.) Proceedings of the 12th Philippine Computing Science Congress
  (PCSC 2012), De La Salle Canlubang, Bi\~nan, Laguna, Philippines, 01-03 March
  2012</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Two cheap, off-the-shelf machine vision systems (MVS), each using an
artificial neural network (ANN) as classifier, were developed, improved and
evaluated to automate the classification of tomato ripeness and acceptability
of eggs, respectively. Six thousand color images of human-graded tomatoes and
750 images of human-graded eggs were used to train, test, and validate several
multi-layered ANNs. The ANNs output the corresponding grade of the produce by
accepting as inputs the spectral patterns of the background-less image. In both
MVS, the ANN with the highest validation rate was automatically chosen by a
heuristic and its performance compared to that of the human graders'. Using the
validation set, the MVS correctly graded 97.00\% and 86.00\% of the tomato and
egg data, respectively. The human grader's, however, were measured to perform
at a daily average of 92.65\% and 72.67\% for tomato and egg grading,
respectively. This results show that an ANN-based MVS is a potential
alternative to manual grading.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02347</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02347</id><created>2015-07-08</created><authors><author><keyname>Hwang</keyname><forenames>Jungsik</forenames></author><author><keyname>Jung</keyname><forenames>Minju</forenames></author><author><keyname>Madapana</keyname><forenames>Naveen</forenames></author><author><keyname>Kim</keyname><forenames>Jinhyung</forenames></author><author><keyname>Choi</keyname><forenames>Minkyu</forenames></author><author><keyname>Tani</keyname><forenames>Jun</forenames></author></authors><title>Achieving Synergy in Cognitive Behavior of Humanoids via Deep Learning
  of Dynamic Visuo-Motor-Attentional Coordination</title><categories>cs.AI cs.LG cs.RO</categories><comments>submitted to 2015 IEEE-RAS International Conference on Humanoid
  Robots</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The current study examines how adequate coordination among different
cognitive processes including visual recognition, attention switching, action
preparation and generation can be developed via learning of robots by
introducing a novel model, the Visuo-Motor Deep Dynamic Neural Network (VMDNN).
The proposed model is built on coupling of a dynamic vision network, a motor
generation network, and a higher level network allocated on top of these two.
The simulation experiments using the iCub simulator were conducted for
cognitive tasks including visual object manipulation responding to human
gestures. The results showed that synergetic coordination can be developed via
iterative learning through the whole network when spatio-temporal hierarchy and
temporal one can be self-organized in the visual pathway and in the motor
pathway, respectively, such that the higher level can manipulate them with
abstraction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02349</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02349</id><created>2015-07-08</created><updated>2015-11-11</updated><authors><author><keyname>Boxer</keyname><forenames>Laurence</forenames></author><author><keyname>Ege</keyname><forenames>Ozgur</forenames></author><author><keyname>Karaca</keyname><forenames>Ismet</forenames></author><author><keyname>Lopez</keyname><forenames>Jonathan</forenames></author><author><keyname>Louwsma</keyname><forenames>Joel</forenames></author></authors><title>Digital Fixed Points, Approximate Fixed Points, and Universal Functions</title><categories>math.CO cs.DM</categories><msc-class>Primary 55M20, Secondary 55N35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A. Rosenfeld introduced the notion of a digitally continuous function between
digital images, and showed that although digital images need not have fixed
point properties analogous to those of the Euclidean spaces modeled by the
images, there often are approximate fixed point properties of such images. In
the current paper, we obtain additional results concerning fixed points and
approximate fixed points of digitally continuous functions. Among these are
several results concerning the relationship between universal functions and the
approximate fixed point property (AFPP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02350</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02350</id><created>2015-07-08</created><authors><author><keyname>van Bevern</keyname><forenames>Ren&#xe9;</forenames></author><author><keyname>Kanj</keyname><forenames>Iyad</forenames></author><author><keyname>Komusiewicz</keyname><forenames>Christian</forenames></author><author><keyname>Niedermeier</keyname><forenames>Rolf</forenames></author><author><keyname>Sorge</keyname><forenames>Manuel</forenames></author></authors><title>Well-Formed Separator Sequences, with an Application to Hypergraph
  Drawing</title><categories>cs.DM math.CO</categories><comments>30 pages, 5 figures</comments><msc-class>68R10</msc-class><acm-class>G.2.2; F.2.2; G.2.1; I.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a hypergraph $H$, the Planar Support problem asks whether there is a
planar graph $G$ on the same vertex set as $H$ such that each hyperedge induces
a connected subgraph of $G$. Planar Support is motivated by applications in
graph drawing and data visualization. We show that Planar Support is
fixed-parameter tractable when parameterized by the number of hyperedges in the
input hypergraph and the outerplanarity number of the sought planar graph. To
this end, we develop novel structural results for $r$-outerplanar triangulated
disks, showing that they admit sequences of separators with structural
properties enabling data reduction. This allows us to obtain a problem kernel
for Planar Support, thus showing its fixed-parameter tractability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02351</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02351</id><created>2015-07-08</created><authors><author><keyname>Badanidiyuru</keyname><forenames>Ashwinkumar</forenames></author><author><keyname>Papadimitriou</keyname><forenames>Christos</forenames></author><author><keyname>Rubinstein</keyname><forenames>Aviad</forenames></author><author><keyname>Seeman</keyname><forenames>Lior</forenames></author><author><keyname>Singer</keyname><forenames>Yaron</forenames></author></authors><title>Locally Adaptive Optimization: Adaptive Seeding for Monotone Submodular
  Functions</title><categories>cs.SI cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Adaptive Seeding problem is an algorithmic challenge motivated by
influence maximization in social networks: One seeks to select among certain
accessible nodes in a network, and then select, adaptively, among neighbors of
those nodes as they become accessible in order to maximize a global objective
function. More generally, adaptive seeding is a stochastic optimization
framework where the choices in the first stage affect the realizations in the
second stage, over which we aim to optimize.
  Our main result is a $(1-1/e)^2$-approximation for the adaptive seeding
problem for any monotone submodular function. While adaptive policies are often
approximated via non-adaptive policies, our algorithm is based on a novel
method we call \emph{locally-adaptive} policies. These policies combine a
non-adaptive global structure, with local adaptive optimizations. This method
enables the $(1-1/e)^2$-approximation for general monotone submodular functions
and circumvents some of the impossibilities associated with non-adaptive
policies.
  We also introduce a fundamental problem in submodular optimization that may
be of independent interest: given a ground set of elements where every element
appears with some small probability, find a set of expected size at most $k$
that has the highest expected value over the realization of the elements. We
show a surprising result: there are classes of monotone submodular functions
(including coverage) that can be approximated almost optimally as the
probability vanishes. For general monotone submodular functions we show via a
reduction from \textsc{Planted-Clique} that approximations for this problem are
not likely to be obtainable. This optimization problem is an important tool for
adaptive seeding via non-adaptive policies, and its hardness motivates the
introduction of \emph{locally-adaptive} policies we use in the main result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02355</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02355</id><created>2015-07-08</created><authors><author><keyname>Bose</keyname><forenames>Prosenjit</forenames></author><author><keyname>De Carufel</keyname><forenames>Jean-Lou</forenames></author><author><keyname>Dobbins</keyname><forenames>Michael G.</forenames></author><author><keyname>Kim</keyname><forenames>Heuna</forenames></author><author><keyname>Viglietta</keyname><forenames>Giovanni</forenames></author></authors><title>The Shadows of a Cycle Cannot All Be Paths</title><categories>cs.CG cs.CV math.MG</categories><comments>6 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A &quot;shadow&quot; of a subset $S$ of Euclidean space is an orthogonal projection of
$S$ into one of the coordinate hyperplanes. In this paper we show that it is
not possible for all three shadows of a cycle (i.e., a simple closed curve) in
$\mathbb R^3$ to be paths (i.e., simple open curves).
  We also show two contrasting results: the three shadows of a path in $\mathbb
R^3$ can all be cycles (although not all convex) and, for every $d\geq 1$,
there exists a $d$-sphere embedded in $\mathbb R^{d+2}$ whose $d+2$ shadows
have no holes (i.e., they deformation-retract onto a point).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02356</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02356</id><created>2015-07-08</created><authors><author><keyname>Dalal</keyname><forenames>Chintan A.</forenames></author><author><keyname>Pavlovic</keyname><forenames>Vladimir</forenames></author><author><keyname>Kopp</keyname><forenames>Robert E.</forenames></author></authors><title>Intrinsic Non-stationary Covariance Function for Climate Modeling</title><categories>stat.ML cs.LG</categories><comments>9 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing a covariance function that represents the underlying correlation is
a crucial step in modeling complex natural systems, such as climate models.
Geospatial datasets at a global scale usually suffer from non-stationarity and
non-uniformly smooth spatial boundaries. A Gaussian process regression using a
non-stationary covariance function has shown promise for this task, as this
covariance function adapts to the variable correlation structure of the
underlying distribution. In this paper, we generalize the non-stationary
covariance function to address the aforementioned global scale geospatial
issues. We define this generalized covariance function as an intrinsic
non-stationary covariance function, because it uses intrinsic statistics of the
symmetric positive definite matrices to represent the characteristic length
scale and, thereby, models the local stochastic process. Experiments on a
synthetic and real dataset of relative sea level changes across the world
demonstrate improvements in the error metrics for the regression estimates
using our newly proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02357</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02357</id><created>2015-07-08</created><authors><author><keyname>Kepner</keyname><forenames>Jeremy</forenames></author><author><keyname>Arcand</keyname><forenames>William</forenames></author><author><keyname>Bestor</keyname><forenames>David</forenames></author><author><keyname>Bergeron</keyname><forenames>Bill</forenames></author><author><keyname>Byun</keyname><forenames>Chansup</forenames></author><author><keyname>Edwards</keyname><forenames>Lauren</forenames></author><author><keyname>Gadepally</keyname><forenames>Vijay</forenames></author><author><keyname>Hubbell</keyname><forenames>Matthew</forenames></author><author><keyname>Michaleas</keyname><forenames>Peter</forenames></author><author><keyname>Mullen</keyname><forenames>Julie</forenames></author><author><keyname>Prout</keyname><forenames>Andrew</forenames></author><author><keyname>Rosa</keyname><forenames>Antonio</forenames></author><author><keyname>Yee</keyname><forenames>Charles</forenames></author><author><keyname>Reuther</keyname><forenames>Albert</forenames></author></authors><title>Lustre, Hadoop, Accumulo</title><categories>cs.DC cs.DB</categories><comments>6 pages; accepted to IEEE High Performance Extreme Computing
  conference, Waltham, MA, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data processing systems impose multiple views on data as it is processed by
the system. These views include spreadsheets, databases, matrices, and graphs.
There are a wide variety of technologies that can be used to store and process
data through these different steps. The Lustre parallel file system, the Hadoop
distributed file system, and the Accumulo database are all designed to address
the largest and the most challenging data storage problems. There have been
many ad-hoc comparisons of these technologies. This paper describes the
foundational principles of each technology, provides simple models for
assessing their capabilities, and compares the various technologies on a
hypothetical common cluster. These comparisons indicate that Lustre provides 2x
more storage capacity, is less likely to loose data during 3 simultaneous drive
failures, and provides higher bandwidth on general purpose workloads. Hadoop
can provide 4x greater read bandwidth on special purpose workloads. Accumulo
provides 10,000x lower latency on random lookups than either Lustre or Hadoop
but Accumulo's bulk bandwidth is 10x less. Significant recent work has been
done to enable mix-and-match solutions that allow Lustre, Hadoop, and Accumulo
to be combined in different ways.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02372</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02372</id><created>2015-07-09</created><authors><author><keyname>Yoon</keyname><forenames>Min Sang</forenames></author><author><keyname>Kamal</keyname><forenames>Ahmed E.</forenames></author><author><keyname>Zhu</keyname><forenames>Zhengyuan</forenames></author></authors><title>Request Prediction in Cloud with a Cyclic Window Learning Algorithm</title><categories>cs.DC</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic resource scaling is one advantage of Cloud systems. Cloud systems
are able to scale the number of physical machines depending on user requests.
Therefore, accurate request prediction brings a great improvement in Cloud
systems' performance. If we can make accurate requests prediction, the
appropriate number of physical machines that can accommodate predicted amount
of requests can be activated and Cloud systems will save more energy by
preventing excessive activation of physical machines. Also, Cloud systems can
implement advanced load distribution with accurate requests prediction. We
propose an algorithm that predicts a probability distribution parameters of
requests for each time interval. Maximum Likelihood Estimation (MLE) and Local
Linear Regression (LLR) are used to implement this algorithm. An evaluation of
the proposed algorithm is performed with the Google cluster-trace data. The
prediction is implemented about the number of task arrivals, CPU requests, and
memory requests. Then the accuracy of prediction is measured with Mean Absolute
Percentage Error (MAPE).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02373</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02373</id><created>2015-07-09</created><authors><author><keyname>Wang</keyname><forenames>Jennifer</forenames></author><author><keyname>Schluntz</keyname><forenames>Erik</forenames></author><author><keyname>Otis</keyname><forenames>Brian</forenames></author><author><keyname>Deyle</keyname><forenames>Travis</forenames></author></authors><title>A New Vision for Smart Objects and the Internet of Things: Mobile Robots
  and Long-Range UHF RFID Sensor Tags</title><categories>cs.RO</categories><comments>8 pages</comments><acm-class>I.2.9</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new vision for smart objects and the Internet of Things wherein
mobile robots interact with wirelessly-powered, long-range, ultra-high
frequency radio frequency identification (UHF RFID) tags outfitted with sensing
capabilities. We explore the technology innovations driving this vision by
examining recently-commercialized sensor tags that could be affixed-to or
embedded-in objects or the environment to yield true embodied intelligence.
Using a pair of autonomous mobile robots outfitted with UHF RFID readers, we
explore several potential applications where mobile robots interact with sensor
tags to perform tasks such as: soil moisture sensing, remote crop monitoring,
infrastructure monitoring, water quality monitoring, and remote sensor
deployment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02378</identifier>
 <datestamp>2015-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02378</id><created>2015-07-09</created><updated>2015-11-17</updated><authors><author><keyname>Bienkowski</keyname><forenames>Marcin</forenames></author><author><keyname>B&#xf6;hm</keyname><forenames>Martin</forenames></author><author><keyname>Byrka</keyname><forenames>Jaroslaw</forenames></author><author><keyname>Chrobak</keyname><forenames>Marek</forenames></author><author><keyname>D&#xfc;rr</keyname><forenames>Christoph</forenames></author><author><keyname>Folwarczn&#xfd;</keyname><forenames>Luk&#xe1;&#x161;</forenames></author><author><keyname>Je&#x17c;</keyname><forenames>&#x141;ukasz</forenames></author><author><keyname>Sgall</keyname><forenames>Ji&#x159;&#xed;</forenames></author><author><keyname>Thang</keyname><forenames>Nguyen Kim</forenames></author><author><keyname>Vesel&#xfd;</keyname><forenames>Pavel</forenames></author></authors><title>Online Algorithms for Multi-Level Aggregation</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Multi-Level Aggregation Problem (MLAP), requests arrive at the nodes
of an edge-weighted tree T, and have to be served eventually. A service is
defined as a subtree X of T that contains its root. This subtree X serves all
requests that are pending in the nodes of X, and the cost of this service is
equal to the total weight of X. Each request also incurs waiting cost between
its arrival and service times. The objective is to minimize the total waiting
cost of all requests plus the total cost of all service subtrees. MLAP is a
generalization of some well-studied optimization problems; for example, for
trees of depth 1, MLAP is equivalent to the TCP Acknowledgment Problem, while
for trees of depth 2, it is equivalent to the Joint Replenishment Problem.
Aggregation problem for trees of arbitrary depth arise in multicasting, sensor
networks, communication in organization hierarchies, and in supply-chain
management.
  The instances of MLAP associated with these applications are naturally
online, in the sense that aggregation decisions need to be made without
information about future requests. Constant-competitive online algorithms are
known for MLAP with one or two levels. However, it has been open whether there
exist constant competitive online algorithms for trees of depth more than 2.
Addressing this open problem, we give the first constant competitive online
algorithm for networks of arbitrary (fixed) number of levels. The competitive
ratio is O(D^4 2^D), where D is the depth of T. The algorithm works for
arbitrary waiting cost functions, including the variant with deadlines. We also
show lower and upper bound results for some special cases of MLAP, including
the Single Phase variant and the case when the tree is a path.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02379</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02379</id><created>2015-07-09</created><updated>2015-07-21</updated><authors><author><keyname>Wei</keyname><forenames>Donglai</forenames></author><author><keyname>Zhou</keyname><forenames>Bolei</forenames></author><author><keyname>Torrabla</keyname><forenames>Antonio</forenames></author><author><keyname>Freeman</keyname><forenames>William</forenames></author></authors><title>Understanding Intra-Class Knowledge Inside CNN</title><categories>cs.CV</categories><comments>tech report for: http://vision03.csail.mit.edu/cnn_art/index.html</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional Neural Network (CNN) has been successful in image recognition
tasks, and recent works shed lights on how CNN separates different classes with
the learned inter-class knowledge through visualization. In this work, we
instead visualize the intra-class knowledge inside CNN to better understand how
an object class is represented in the fully-connected layers.
  To invert the intra-class knowledge into more interpretable images, we
propose a non-parametric patch prior upon previous CNN visualization models.
With it, we show how different &quot;styles&quot; of templates for an object class are
organized by CNN in terms of location and content, and represented in a
hierarchical and ensemble way. Moreover, such intra-class knowledge can be used
in many interesting applications, e.g. style-based image retrieval and
style-based object completion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02380</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02380</id><created>2015-07-09</created><authors><author><keyname>He</keyname><forenames>Ran</forenames></author><author><keyname>Tan</keyname><forenames>Tieniu</forenames></author><author><keyname>Davis</keyname><forenames>Larry</forenames></author><author><keyname>Sun</keyname><forenames>Zhenan</forenames></author></authors><title>Learning Structured Ordinal Measures for Video based Face Recognition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a structured ordinal measure method for video-based face
recognition that simultaneously learns ordinal filters and structured ordinal
features. The problem is posed as a non-convex integer program problem that
includes two parts. The first part learns stable ordinal filters to project
video data into a large-margin ordinal space. The second seeks self-correcting
and discrete codes by balancing the projected data and a rank-one ordinal
matrix in a structured low-rank way. Unsupervised and supervised structures are
considered for the ordinal matrix. In addition, as a complement to hierarchical
structures, deep feature representations are integrated into our method to
enhance coding stability. An alternating minimization method is employed to
handle the discrete and low-rank constraints, yielding high-quality codes that
capture prior structures well. Experimental results on three commonly used face
video databases show that our method with a simple voting classifier can
achieve state-of-the-art recognition rates using fewer features and samples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02384</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02384</id><created>2015-07-09</created><authors><author><keyname>Jeong</keyname><forenames>Jisu</forenames></author><author><keyname>S&#xe6;ther</keyname><forenames>Sigve Hortemo</forenames></author><author><keyname>Telle</keyname><forenames>Jan Arne</forenames></author></authors><title>Maximum matching width: new characterizations and a fast algorithm for
  dominating set</title><categories>cs.DS cs.DM math.CO</categories><msc-class>05C85, 68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give alternative definitions for maximum matching width, e.g. a graph $G$
has $\operatorname{mmw}(G) \leq k$ if and only if it is a subgraph of a chordal
graph $H$ and for every maximal clique $X$ of $H$ there exists $A,B,C \subseteq
X$ with $A \cup B \cup C=X$ and $|A|,|B|,|C| \leq k$ such that any subset of
$X$ that is a minimal separator of $H$ is a subset of either $A, B$ or $C$.
Treewidth and branchwidth have alternative definitions through intersections of
subtrees, where treewidth focuses on nodes and branchwidth focuses on edges. We
show that mm-width combines both aspects, focusing on nodes and on edges. Based
on this we prove that given a graph $G$ and a branch decomposition of mm-width
$k$ we can solve Dominating Set in time $O^*({8^k})$, thereby beating
$O^*(3^{\operatorname{tw}(G)})$ whenever $\operatorname{tw}(G) &gt; \log_3{8}
\times k \approx 1.893 k$. Note that $\operatorname{mmw}(G) \leq
\operatorname{tw}(G)+1 \leq 3 \operatorname{mmw}(G)$ and these inequalities are
tight. Given only the graph $G$ and using the best known algorithms to find
decompositions, maximum matching width will be better for solving Dominating
Set whenever $\operatorname{tw}(G) &gt; 1.549 \times \operatorname{mmw}(G)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02385</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02385</id><created>2015-07-09</created><updated>2015-07-20</updated><authors><author><keyname>Wang</keyname><forenames>Qilong</forenames></author><author><keyname>Li</keyname><forenames>Peihua</forenames></author><author><keyname>Zhang</keyname><forenames>Lei</forenames></author><author><keyname>Zuo</keyname><forenames>Wangmeng</forenames></author></authors><title>Towards Effective Codebookless Model for Image Classification</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The bag-of-features (BoF) model for image classification has been thoroughly
studied over the last decade. Different from the widely used BoF methods which
modeled images with a pre-trained codebook, the alternative codebook free image
modeling method, which we call Codebookless Model (CLM), attracted little
attention. In this paper, we present an effective CLM that represents an image
with a single Gaussian for classification. By embedding Gaussian manifold into
a vector space, we show that the simple incorporation of our CLM into a linear
classifier achieves very competitive accuracy compared with state-of-the-art
BoF methods (e.g., Fisher Vector). Since our CLM lies in a high dimensional
Riemannian manifold, we further propose a joint learning method of low-rank
transformation with support vector machine (SVM) classifier on the Gaussian
manifold, in order to reduce computational and storage cost. To study and
alleviate the side effect of background clutter on our CLM, we also present a
simple yet effective partial background removal method based on saliency
detection. Experiments are extensively conducted on eight widely used databases
to demonstrate the effectiveness and efficiency of our CLM method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02387</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02387</id><created>2015-07-09</created><updated>2015-12-18</updated><authors><author><keyname>Khanna</keyname><forenames>Saurabh</forenames></author><author><keyname>Murthy</keyname><forenames>Chandra R.</forenames></author></authors><title>Decentralized Joint-Sparse Signal Recovery: A Sparse Bayesian Learning
  Approach</title><categories>cs.LG cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Signal and Information Processing
  over Networks, 15 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work proposes a decentralized, iterative, Bayesian algorithm called
CB-DSBL for in-network estimation of multiple jointly sparse vectors by a
network of nodes, using noisy and underdetermined linear measurements. The
proposed algorithm exploits the network wide joint sparsity of the un- known
sparse vectors to recover them from significantly fewer number of local
measurements compared to standalone sparse signal recovery schemes. To reduce
the amount of inter-node communication and the associated overheads, the nodes
exchange messages with only a small subset of their single hop neighbors. Under
this communication scheme, we separately analyze the convergence of the
underlying Alternating Directions Method of Multipliers (ADMM) iterations used
in our proposed algorithm and establish its linear convergence rate. The
findings from the convergence analysis of decentralized ADMM are used to
accelerate the convergence of the proposed CB-DSBL algorithm. Using Monte Carlo
simulations, we demonstrate the superior signal reconstruction as well as
support recovery performance of our proposed algorithm compared to existing
decentralized algorithms: DRL-1, DCOMP and DCSP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02407</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02407</id><created>2015-07-09</created><updated>2015-09-09</updated><authors><author><keyname>Yarkony</keyname><forenames>Julian</forenames></author><author><keyname>Fowlkes</keyname><forenames>Charless C.</forenames></author></authors><title>Planar Ultrametric Rounding for Image Segmentation</title><categories>cs.DS cs.CG cs.CV</categories><msc-class>68T45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of hierarchical clustering on planar graphs. We
formulate this in terms of an LP relaxation of ultrametric rounding. To solve
this LP efficiently we introduce a dual cutting plane scheme that uses minimum
cost perfect matching as a subroutine in order to efficiently explore the space
of planar partitions. We apply our algorithm to the problem of hierarchical
image segmentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02414</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02414</id><created>2015-07-09</created><updated>2016-02-08</updated><authors><author><keyname>Fanelli</keyname><forenames>Angelo</forenames></author><author><keyname>Greco</keyname><forenames>Gianluigi</forenames></author></authors><title>Ride Sharing with a Vehicle of Unlimited Capacity</title><categories>cs.DM cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A ride sharing problem is considered where we are given a graph, whose edges
are equipped with a travel cost, plus a set of objects, each associated with a
transportation request given by a pair of origin and destination nodes. A
vehicle travels through the graph, carrying each object from its origin to its
destination without any bound on the number of objects that can be
simultaneously transported. The vehicle starts and terminates its ride at given
nodes, and the goal is to compute a minimum-cost ride satisfying all requests.
This ride sharing problem is shown to be tractable on paths by designing a $O(h
\log h+n)$ algorithm, with $h$ being the number of distinct requests and with
$n$ being the number of nodes in the path. The algorithm is then used as a
subroutine to efficiently solve instances defined over cycles, hence covering
all graphs with maximum degree $2$. This traces the frontier of tractability,
since $\bf NP$-hard instances are exhibited over trees whose maximum degree is
$3$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02426</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02426</id><created>2015-07-09</created><authors><author><keyname>Brach</keyname><forenames>Pawe&#x142;</forenames></author><author><keyname>Cygan</keyname><forenames>Marek</forenames></author><author><keyname>&#x141;&#x105;cki</keyname><forenames>Jakub</forenames></author><author><keyname>Sankowski</keyname><forenames>Piotr</forenames></author></authors><title>Algorithmic Complexity of Power Law Networks</title><categories>cs.DS</categories><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It was experimentally observed that the majority of real-world networks
follow power law degree distribution. The aim of this paper is to study the
algorithmic complexity of such &quot;typical&quot; networks. The contribution of this
work is twofold.
  First, we define a deterministic condition for checking whether a graph has a
power law degree distribution and experimentally validate it on real-world
networks. This definition allows us to derive interesting properties of power
law networks. We observe that for exponents of the degree distribution in the
range $[1,2]$ such networks exhibit double power law phenomenon that was
observed for several real-world networks. Our observation indicates that this
phenomenon could be explained by just pure graph theoretical properties.
  The second aim of our work is to give a novel theoretical explanation why
many algorithms run faster on real-world data than what is predicted by
algorithmic worst-case analysis. We show how to exploit the power law degree
distribution to design faster algorithms for a number of classical P-time
problems including transitive closure, maximum matching, determinant, PageRank
and matrix inverse. Moreover, we deal with the problems of counting triangles
and finding maximum clique. Previously, it has been only shown that these
problems can be solved very efficiently on power law graphs when these graphs
are random, e.g., drawn at random from some distribution. However, it is
unclear how to relate such a theoretical analysis to real-world graphs, which
are fixed. Instead of that, we show that the randomness assumption can be
replaced with a simple condition on the degrees of adjacent vertices, which can
be used to obtain similar results. As a result, in some range of power law
exponents, we are able to solve the maximum clique problem in polynomial time,
although in general power law networks the problem is NP-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02428</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02428</id><created>2015-07-09</created><authors><author><keyname>S&#xfc;nderhauf</keyname><forenames>Niko</forenames></author><author><keyname>Dayoub</keyname><forenames>Feras</forenames></author><author><keyname>McMahon</keyname><forenames>Sean</forenames></author><author><keyname>Talbot</keyname><forenames>Ben</forenames></author><author><keyname>Schulz</keyname><forenames>Ruth</forenames></author><author><keyname>Corke</keyname><forenames>Peter</forenames></author><author><keyname>Wyeth</keyname><forenames>Gordon</forenames></author><author><keyname>Upcroft</keyname><forenames>Ben</forenames></author><author><keyname>Milford</keyname><forenames>Michael</forenames></author></authors><title>Place Categorization and Semantic Mapping on a Mobile Robot</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we focus on the challenging problem of place categorization and
semantic mapping on a robot without environment-specific training. Motivated by
their ongoing success in various visual recognition tasks, we build our system
upon a state-of-the-art convolutional network. We overcome its closed-set
limitations by complementing the network with a series of one-vs-all
classifiers that can learn to recognize new semantic classes online. Prior
domain knowledge is incorporated by embedding the classification system into a
Bayesian filter framework that also ensures temporal coherence. We evaluate the
classification accuracy of the system on a robot that maps a variety of places
on our campus in real-time. We show how semantic information can boost robotic
object detection performance and how the semantic map can be used to modulate
the robot's behaviour during navigation tasks. The system is made available to
the community as a ROS module.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02431</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02431</id><created>2015-07-09</created><authors><author><keyname>Atzarakis</keyname><forenames>Nikolaos</forenames></author></authors><title>Research Approaches on Energy-aware Cognitive Radio Networks and Cloud
  based Infrastructures</title><categories>cs.NI</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As years passes wireless networks were rapidly improved, introducing new
applications and services, as well as important challenges for mobility
support. This research field is new with many researchers and scientists making
their proposals to optimize the provision of multiple services to the mobile
users. In this context, this survey paper studies research approaches from the
following topics: Cognitive Radio Networks, Interactive Broadcasting, Energy
Efficient Networks, Cloud Computing and Resource Management.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02437</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02437</id><created>2015-07-09</created><authors><author><keyname>Chevalier-Boisvert</keyname><forenames>Maxime</forenames></author><author><keyname>Feeley</keyname><forenames>Marc</forenames></author></authors><title>Extending Basic Block Versioning with Typed Object Shapes</title><categories>cs.PL</categories><acm-class>D.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Typical JavaScript (JS) programs feature a large number of object property
accesses. Hence, fast property reads and writes are crucial for good
performance. Unfortunately, many (often redundant) dynamic checks are implied
in each property access and the semantic complexity of JS makes it difficult to
optimize away these tests through program analysis. We introduce two techniques
to effectively eliminate a large proportion of dynamic checks related to object
property accesses.
  Typed shapes enable code specialization based on object property types
without potentially complex and expensive analyses. Shape propagation allows
the elimination of redundant shape checks in inline caches. These two
techniques combine particularly well with Basic Block Versioning (BBV), but
should be easily adaptable to tracing Just-In-Time (JIT) compilers and method
JITs with type feedback.
  To assess the effectiveness of the techniques presented, we have implemented
them in Higgs, a type-specializing JIT compiler for JS. The techniques are
compared to a baseline using polymorphic Inline Caches (PICs), as well as
commercial JS implementations. Empirical results show that across the 26
benchmarks tested, these techniques eliminate on average 48% of type tests,
reduce code size by 17% and reduce execution time by 25%. On several
benchmarks, Higgs performs better than current production JS virtual machines
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02438</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02438</id><created>2015-07-09</created><authors><author><keyname>Kim</keyname><forenames>Tae Hyun</forenames></author><author><keyname>Lee</keyname><forenames>Kyoung Mu</forenames></author></authors><title>Generalized Video Deblurring for Dynamic Scenes</title><categories>cs.CV</categories><comments>CVPR 2015 oral</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several state-of-the-art video deblurring methods are based on a strong
assumption that the captured scenes are static. These methods fail to deblur
blurry videos in dynamic scenes. We propose a video deblurring method to deal
with general blurs inherent in dynamic scenes, contrary to other methods. To
handle locally varying and general blurs caused by various sources, such as
camera shake, moving objects, and depth variation in a scene, we approximate
pixel-wise kernel with bidirectional optical flows. Therefore, we propose a
single energy model that simultaneously estimates optical flows and latent
frames to solve our deblurring problem. We also provide a framework and
efficient solvers to optimize the energy model. By minimizing the proposed
energy function, we achieve significant improvements in removing blurs and
estimating accurate optical flows in blurry frames. Extensive experimental
results demonstrate the superiority of the proposed method in real and
challenging videos that state-of-the-art methods fail in either deblurring or
optical flow estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02439</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02439</id><created>2015-07-09</created><authors><author><keyname>Olugbara</keyname><forenames>Oludayo O.</forenames></author><author><keyname>Joshi</keyname><forenames>Manish</forenames></author><author><keyname>Modiba</keyname><forenames>Michael M.</forenames></author><author><keyname>Bhavsar</keyname><forenames>Virendrakumar C.</forenames></author></authors><title>Automated Matchmaking to Improve Accuracy of Applicant Selection for
  University Education System</title><categories>cs.AI cs.CY</categories><comments>14 pages, 5 Text boxes, 2 tables and a figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The accurate applicant selection for university education is imperative to
ensure fairness and optimal use of institutional resources. Although various
approaches are operational in tertiary educational institutions for selecting
applicants, a novel method of automated matchmaking is explored in the current
study. The method functions by matching a prospective students skills profile
to a programmes requisites profile.
  Empirical comparisons of the results, calculated by automated matchmaking and
two other selection methods, show matchmaking to be a viable alternative for
accurate selection of applicants. Matchmaking offers a unique advantage that it
neither requires data from other applicants nor compares applicants with each
other. Instead, it emphasises norms that define admissibility to a programme.
  We have proposed the use of technology to minimize the gap between students
aspirations, skill sets and course requirements. It is a solution to minimize
the number of students who get frustrated because of mismatched course
selection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02444</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02444</id><created>2015-07-09</created><updated>2015-11-15</updated><authors><author><keyname>Fong</keyname><forenames>Silas L.</forenames></author><author><keyname>Tan</keyname><forenames>Vincent Y. F.</forenames></author><author><keyname>Yang</keyname><forenames>Jing</forenames></author></authors><title>Non-Asymptotic Achievable Rates for Energy-Harvesting Channels using
  Save-and-Transmit</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Journal on Selected Areas in Communications, 1st
  revision completed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the information-theoretic limits of energy-harvesting
(EH) channels in the finite blocklength regime. The EH process is characterized
by a sequence of i.i.d.$~$random variables with finite variances. We use the
save-and-transmit strategy proposed by Ozel and Ulukus (2012) together with
Shannon's non-asymptotic achievability bound to obtain lower bounds on the
achievable rates for both additive white Gaussian noise channels and discrete
memoryless channels under EH constraints. The first-order terms of the lower
bounds of the achievable rates are equal to $C$ and the second-order (backoff
from capacity) terms are proportional to $-\sqrt{ \frac{\log n}{n} }$, where
$n$ denotes the blocklength and $C$ denotes the capacity of the EH channel,
which is the same as the capacity without the EH constraints. The constant of
proportionality of the backoff term is found and qualitative interpretations
are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02447</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02447</id><created>2015-07-09</created><authors><author><keyname>Tirunagari</keyname><forenames>Santosh</forenames></author></authors><title>Data Mining of Causal Relations from Text: Analysing Maritime Accident
  Investigation Reports</title><categories>cs.IR cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Text mining is a process of extracting information of interest from text.
Such a method includes techniques from various areas such as Information
Retrieval (IR), Natural Language Processing (NLP), and Information Extraction
(IE). In this study, text mining methods are applied to extract causal
relations from maritime accident investigation reports collected from the
Marine Accident Investigation Branch (MAIB). These causal relations provide
information on various mechanisms behind accidents, including human and
organizational factors relating to the accident. The objective of this study is
to facilitate the analysis of the maritime accident investigation reports, by
means of extracting contributory causes with more feasibility. A careful
investigation of contributory causes from the reports provide opportunity to
improve safety in future.
  Two methods have been employed in this study to extract the causal relations.
They are 1) Pattern classification method and 2) Connectives method. The
earlier one uses naive Bayes and Support Vector Machines (SVM) as classifiers.
The latter simply searches for the words connecting cause and effect in
sentences.
  The causal patterns extracted using these two methods are compared to the
manual (human expert) extraction. The pattern classification method showed a
fair and sensible performance with F-measure(average) = 65% when compared to
connectives method with F-measure(average) = 58%. This study is an evidence,
that text mining methods could be employed in extracting causal relations from
marine accident investigation reports.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02449</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02449</id><created>2015-07-09</created><authors><author><keyname>Jayin</keyname><forenames>Abigail Mae C.</forenames></author><author><keyname>Batac</keyname><forenames>Rene C.</forenames></author></authors><title>Finding trends and statistical patterns in name mentions in news</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extract the individual names of persons mentioned in news reports from a
Philippine-based daily in the English language from 2010-2012. Names are
extracted using a learning algorithm that filters adjacent capitalized words
and runs it through a database of non-names grown through training. The number
of mentions of individual names shows strong temporal fluctuations, indicative
of the nature of &quot;hot&quot; trends and issues in society. Despite these strong
variations, however, we observe stable rank-frequency distributions across
different years in the form of power-laws with scaling exponents \alpha = 0.7,
reminiscent of the Zipf's law observed in lexical (i.e. non-name) words.
Additionally, we observe that the adjusted frequency for each rank, or the
frequency divided by the number of unique names having the same rank, shows a
distribution with dual scaling behavior, with the higher-ranked names
preserving the \alpha exponent and the lower-ranked ones showing a power-law
exponent \alpha' = 2.9. We reproduced the results using a model wherein the
names are taken from a Barabasi-Albert network representing the social
structure of the system. These results suggest that names, which represent
individuals in the society, are archived differently from regular words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02454</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02454</id><created>2015-07-09</created><authors><author><keyname>Rusu</keyname><forenames>Cristian</forenames></author><author><keyname>Gonz&#xe1;lez-Prelcic</keyname><forenames>Nuria</forenames></author></authors><title>Optimized Compressed Sensing via Incoherent Frames Designed by Convex
  Optimization</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The construction of highly incoherent frames, sequences of vectors placed on
the unit hyper sphere of a finite dimensional Hilbert space with low
correlation between them, has proven very difficult. Algorithms proposed in the
past have focused in minimizing the absolute value off-diagonal entries of the
Gram matrix of these structures. Recently, a method based on convex
optimization that operates directly on the vectors of the frame has been shown
to produce promising results. This paper gives a detailed analysis of the
optimization problem at the heart of this approach and, based on these
insights, proposes a new method that substantially outperforms the initial
approach and all current methods in the literature for all types of frames,
with low and high redundancy. We give extensive experimental results that show
the effectiveness of the proposed method and its application to optimized
compressed sensing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02455</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02455</id><created>2015-07-09</created><authors><author><keyname>Razavi</keyname><forenames>Alireza</forenames></author><author><keyname>Valkama</keyname><forenames>Mikko</forenames></author><author><keyname>Cabric</keyname><forenames>Danijela</forenames></author></authors><title>Compressive Identification of Active OFDM Subcarriers in Presence of
  Timing Offset</title><categories>cs.IT math.IT</categories><comments>To appear in the proceedings of the IEEE Global Communications
  Conference (GLOBECOM) 2015</comments><doi>10.1109/GLOCOM.2014.7417565</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the problem of identifying active subcarriers in an
OFDM signal from compressive measurements sampled at sub-Nyquist rate. The
problem is of importance in Cognitive Radio systems when secondary users (SUs)
are looking for available spectrum opportunities to communicate over them while
sensing at Nyquist rate sampling can be costly or even impractical in case of
very wide bandwidth. We first study the effect of timing offset and derive the
necessary and sufficient conditions for signal recovery in the oracle-assisted
case when the true active sub-carriers are assumed known. Then we propose an
Orthogonal Matching Pursuit (OMP)-based joint sparse recovery method for
identifying active subcarriers when the timing offset is known. Finally we
extend the problem to the case of unknown timing offset and develop a joint
dictionary learning and sparse approximation algorithm, where in the dictionary
learning phase the timing offset is estimated and in the sparse approximation
phase active subcarriers are identified. The obtained results demonstrate that
active subcarrier identification can be carried out reliably, by using the
developed framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02456</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02456</id><created>2015-07-09</created><updated>2015-07-15</updated><authors><author><keyname>Chekol</keyname><forenames>Melisachew Wudage</forenames></author><author><keyname>Huber</keyname><forenames>Jakob</forenames></author><author><keyname>Stuckenschmidt</keyname><forenames>Heiner</forenames></author></authors><title>Towards Log-Linear Logics with Concrete Domains</title><categories>cs.AI</categories><comments>StarAI2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present $\mathcal{MEL}^{++}$ (M denotes Markov logic networks) an
extension of the log-linear description logics $\mathcal{EL}^{++}$-LL with
concrete domains, nominals, and instances. We use Markov logic networks (MLNs)
in order to find the most probable, classified and coherent $\mathcal{EL}^{++}$
ontology from an $\mathcal{MEL}^{++}$ knowledge base. In particular, we develop
a novel way to deal with concrete domains (also known as datatypes) by
extending MLN's cutting plane inference (CPI) algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02458</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02458</id><created>2015-07-09</created><authors><author><keyname>Mataracioglu</keyname><forenames>Tolga</forenames></author><author><keyname>Ozkan</keyname><forenames>Sevgi</forenames></author><author><keyname>Hackney</keyname><forenames>Ray</forenames></author></authors><title>Towards a Security Lifecycle Model against Social Engineering Attacks:
  SLM-SEA</title><categories>cs.CR cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This research considers the impact of social engineering security attacks
which are noted as taking opportunities for critically exploiting user
awareness and behavior. The research proposes in this respect a managerial
method in an attempt to enhance or even ensure protection. The aim of this
study is to construct a security lifecycle model against these eventualities
and to analyze the test results that have been carried out within the context
of the Turkish public sector. The main objective of the study is to determine
why employees shared sensitive information by stating fallacies and related
amendments through interviews and thus to understand user actions when they are
face to face with a real social engineering attack. The research findings
demonstrate that employees in Turkish public organizations are not sufficiently
aware of information security and they generally ignore critically important
security procedures. This represents an important illustration of the
increasing need for further generalized user awareness and responsibilities
where individuals and not simply software form a critical element of the
security protection portfolio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02463</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02463</id><created>2015-07-09</created><authors><author><keyname>Mishra</keyname><forenames>Tapas Kumar</forenames></author><author><keyname>Pal</keyname><forenames>Sudebkumar Prasant</forenames></author></authors><title>An extremal problem in proper $(r,p)$-coloring of hypergraphs</title><categories>cs.DM</categories><msc-class>05D05, 05C65</msc-class><acm-class>G.2.1; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G(V,E)$ be a $k$-uniform hypergraph. A hyperedge $e \in E$ is said to be
properly $(r,p)$ colored by an $r$-coloring of vertices in $V$ if $e$ contains
vertices of at least $p$ distinct colors in the $r$-coloring. An $r$-coloring
of vertices in $V$ is called a {\it strong $(r,p)$ coloring} if every hyperedge
$e \in E$ is properly $(r,p)$ colored by the $r$-coloring. We study the maximum
number of hyperedges that can be properly $(r,p)$ colored by a single
$r$-coloring and the structures that maximizes number of properly $(r,p)$
colored hyperedges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02479</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02479</id><created>2015-07-09</created><updated>2015-07-20</updated><authors><author><keyname>Ganian</keyname><forenames>Robert</forenames></author><author><keyname>Ramanujan</keyname><forenames>M. S.</forenames></author><author><keyname>Szeider</keyname><forenames>Stefan</forenames></author></authors><title>Discovering Archipelagos of Tractability for Constraint Satisfaction and
  Counting</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Constraint Satisfaction Problem (CSP) is a central and generic
computational problem which provides a common framework for many theoretical
and practical applications. A central line of research is concerned with the
identification of classes of instances for which CSP can be solved in
polynomial time; such classes are often called &quot;islands of tractability.&quot; A
prominent way of defining islands of tractability for CSP is to restrict the
relations that may occur in the constraints to a fixed set, called a constraint
language, whereas a constraint language is conservative if it contains all
unary relations. This paper addresses the general limit of the mentioned
tractability results for CSP and #CSP, that they only apply to instances where
all constraints belong to a single tractable language (in general, the union of
two tractable languages isn't tractable). We show that we can overcome this
limitation as long as we keep some control of how constraints over the various
considered tractable languages interact with each other. For this purpose we
utilize the notion of a \emph{strong backdoor} of a CSP instance, as introduced
by Williams et al. (IJCAI 2003), which is a set of variables that when
instantiated moves the instance to an island of tractability, i.e., to a
tractable class of instances. In this paper, we consider strong backdoors into
\emph{scattered classes}, consisting of CSP instances where each connected
component belongs entirely to some class from a list of tractable classes. Our
main result is an algorithm that, given a CSP instance with $n$ variables,
finds in time $f(k)n^{O(1)}$ a strong backdoor into a scattered class
(associated with a list of finite conservative constraint languages) of size
$k$ or correctly decides that there isn't such a backdoor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02482</identifier>
 <datestamp>2015-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02482</id><created>2015-07-09</created><updated>2015-11-24</updated><authors><author><keyname>Sheffet</keyname><forenames>Or</forenames></author></authors><title>Differentially Private Ordinary Least Squares: $t$-Values, Confidence
  Intervals and Rejecting Null-Hypotheses</title><categories>cs.DS cs.CR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear regression is one of the most prevalent techniques in data analysis.
Given a large collection of samples composed of features $\vec x$ and a label
$y$, linear regression is used to find the best prediction of the label as a
linear combination of the features. However, it is also common to use linear
regression for its \emph{explanatory} capabilities rather than label
prediction. Ordinary Least Squares (OLS) is often used in statistics to
establish a correlation between an attribute (e.g. gender) and a label (e.g.
income) in the presence of other features. Under the assumption of a certain
random generative model for the data, OLS derives \emph{$t$-values} ---
representing the likelihood of each real value to be the true correlation in
the underlying distribution. Using $t$-values, OLS can release a
\emph{confidence interval} that is likely to contain the true correlation. When
this interval does not intersect the origin, we can \emph{reject the null
hypothesis} as it is likely that $x_j$ indeed has a non-zero correlation with
$y$.
  Our work aims at achieving similar guarantees on data under differentially
private estimators. We use the Gaussian Johnson-Lindenstrauss transform, which
has been shown to satisfy differential privacy if the given data has large
singular values. We analyze the result of projecting the data using JLT under
the OLS model and derive approximated $t$-values, confidence intervals and
bound the number of samples needed to reject the null hypothesis when the data
is drawn i.i.d from a multivariate Gaussian. When not all singular values of
the data are sufficiently large, we increase the singular values, thus our
projected data yields an approximation for the Ridge Regression problem. We
derive, under certain conditions, confidence intervals in this case as well. We
also derive confidence intervals for the &quot;Analyze Gauss&quot; algorithm of Dwork et
al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02491</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02491</id><created>2015-07-09</created><authors><author><keyname>Yu</keyname><forenames>James J. Q.</forenames></author><author><keyname>Li</keyname><forenames>Victor O. K.</forenames></author></authors><title>Parameter Sensitivity Analysis of Social Spider Algorithm</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social Spider Algorithm (SSA) is a recently proposed general-purpose
real-parameter metaheuristic designed to solve global numerical optimization
problems. This work systematically benchmarks SSA on a suite of 11 functions
with different control parameters. We conduct parameter sensitivity analysis of
SSA using advanced non-parametric statistical tests to generate statistically
significant conclusion on the best performing parameter settings. The
conclusion can be adopted in future work to reduce the effort in parameter
tuning. In addition, we perform a success rate test to reveal the impact of the
control parameters on the convergence speed of the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02492</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02492</id><created>2015-07-09</created><authors><author><keyname>Yu</keyname><forenames>James J. Q.</forenames></author><author><keyname>Lam</keyname><forenames>Albert Y. S.</forenames></author><author><keyname>Li</keyname><forenames>Victor O. K.</forenames></author></authors><title>Adaptive Chemical Reaction Optimization for Global Numerical
  Optimization</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A newly proposed chemical-reaction-inspired metaheurisic, Chemical Reaction
Optimization (CRO), has been applied to many optimization problems in both
discrete and continuous domains. To alleviate the effort in tuning parameters,
this paper reduces the number of optimization parameters in canonical CRO and
develops an adaptive scheme to evolve them. Our proposed Adaptive CRO (ACRO)
adapts better to different optimization problems. We perform simulations with
ACRO on a widely-used benchmark of continuous problems. The simulation results
show that ACRO has superior performance over canonical CRO.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02504</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02504</id><created>2015-07-09</created><authors><author><keyname>Moran</keyname><forenames>Shay</forenames></author><author><keyname>Pinchasi</keyname><forenames>Rom</forenames></author></authors><title>Matchings vs hitting sets among half-spaces in low dimensional euclidean
  spaces</title><categories>math.CO cs.CG cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\mathcal{F}$ be any collection of linearly separable sets of a set $P$
of $n$ points either in $\mathbb{R}^2$, or in $\mathbb{R}^3$. We show that for
every natural number $k$ either one can find $k$ pairwise disjoint sets in
$\mathcal{F}$, or there are $O(k)$ points in $P$ that together hit all sets in
$\mathcal{F}$. The proof is based on showing a similar result for families
$\mathcal{F}$ of sets separable by pseudo-discs in $\mathbb{R}^2$. We
complement these statements by showing that analogous result fails to hold for
collections of linearly separable sets in $\mathbb{R}^4$ and higher dimensional
euclidean spaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02506</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02506</id><created>2015-07-09</created><authors><author><keyname>Cardall</keyname><forenames>Christian Y.</forenames></author><author><keyname>Budiardja</keyname><forenames>Reuben D.</forenames></author></authors><title>GenASiS Basics: Object-oriented utilitarian functionality for
  large-scale physics simulations</title><categories>astro-ph.IM cs.MS</categories><comments>Computer Physics Communications in press</comments><doi>10.1016/j.cpc.2015.06.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Aside from numerical algorithms and problem setup, large-scale physics
simulations on distributed-memory supercomputers require more basic utilitarian
functionality, such as physical units and constants; display to the screen or
standard output device; message passing; I/O to disk; and runtime parameter
management and usage statistics. Here we describe and make available Fortran
2003 classes furnishing extensible object-oriented implementations of this sort
of rudimentary functionality, along with individual `unit test' programs and
larger example problems demonstrating their use. These classes compose the
Basics division of our developing astrophysics simulation code GenASiS (General
Astrophysical Simulation System), but their fundamental nature makes them
useful for physics simulations in many fields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02519</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02519</id><created>2015-07-09</created><updated>2015-12-04</updated><authors><author><keyname>Li</keyname><forenames>Jianwen</forenames></author><author><keyname>Zhu</keyname><forenames>Shufang</forenames></author><author><keyname>Pu</keyname><forenames>Geguang</forenames></author><author><keyname>Vardi</keyname><forenames>Moshe</forenames></author></authors><title>SAT-based Explicit LTL Reasoning</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present here a new explicit reasoning framework for linear temporal logic
(LTL), which is built on top of propositional satisfiability (SAT) solving. As
a proof-of-concept of this framework, we describe a new LTL satisfiability
tool, Aalta\_v2.0, which is built on top of the MiniSAT SAT solver. We test the
effectiveness of this approach by demonnstrating that Aalta\_v2.0 significantly
outperforms all existing LTL satisfiability solvers. Furthermore, we show that
the framework can be extended from propositional LTL to assertional LTL (where
we allow theory atoms), by replacing MiniSAT with the Z3 SMT solver, and
demonstrating that this can yield an exponential improvement in performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02525</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02525</id><created>2015-07-09</created><authors><author><keyname>Andreatto</keyname><forenames>Bartosz</forenames></author><author><keyname>Cariow</keyname><forenames>Aleksandr</forenames></author></authors><title>An algorithm for fast computation of the multiresolution discrete
  Fourier transform</title><categories>cs.DS</categories><comments>8 pages, 2 figures</comments><msc-class>15A23, 15A04, 65Y20</msc-class><acm-class>F.2.1; G.1.0; I.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article presents a computationally effective algorithm for calculating
the multiresolution discrete Fourier transform (MrDFT). The algorithm is based
on the idea of reducing the computational complexity which was introduced by
Wen and Sandler [10] and utilizes the vectorization of calculating process at
each stage of the considered transformation. This allows for the use of a
computational process parallelization and results in a reduction of computation
time. In the description of the computational procedure, which describes the
algorithm, we use the matrix notation. This notation enables to represent
adequately the space-time structures of the implemented computational process
and directly map these structures into the constructions of a high-level
programming language or into a hardware realization space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02528</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02528</id><created>2015-07-09</created><updated>2015-11-05</updated><authors><author><keyname>Abernethy</keyname><forenames>Jacob</forenames></author><author><keyname>Hazan</keyname><forenames>Elad</forenames></author></authors><title>Faster Convex Optimization: Simulated Annealing with an Efficient
  Universal Barrier</title><categories>math.OC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores a surprising equivalence between two seemingly-distinct
convex optimization methods. We show that simulated annealing, a well-studied
random walk algorithms, is directly equivalent, in a certain sense, to the
central path interior point algorithm for the the entropic universal barrier
function. This connection exhibits several benefits. First, we are able improve
the state of the art time complexity for convex optimization under the
membership oracle model. We improve the analysis of the randomized algorithm of
Kalai and Vempala by utilizing tools developed by Nesterov and Nemirovskii that
underly the central path following interior point algorithm. We are able to
tighten the temperature schedule for simulated annealing which gives an
improved running time, reducing by square root of the dimension in certain
instances. Second, we get an efficient randomized interior point method with an
efficiently computable universal barrier for any convex set described by a
membership oracle. Previously, efficiently computable barriers were known only
for particular convex sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02531</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02531</id><created>2015-07-09</created><authors><author><keyname>Bloem</keyname><forenames>Roderick</forenames></author><author><keyname>Ehlers</keyname><forenames>Ruediger</forenames></author><author><keyname>Koenighofer</keyname><forenames>Robert</forenames></author></authors><title>Cooperative Reactive Synthesis</title><categories>cs.LO</categories><comments>18 pages, 3 figures. This is an extended version of [7], featuring an
  additional appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A modern approach to engineering correct-by-construction systems is to
synthesize them automatically from formal specifications. Oftentimes, a system
can only satisfy its guarantees if certain environment assumptions hold, which
motivates their inclusion in the system specification. Experience with modern
synthesis approaches shows that synthesized systems tend to satisfy their
specifications by actively working towards the violation of the assumptions
rather than satisfying assumptions and guarantees together. Such uncooperative
behavior is undesirable because it violates the aim of synthesis: the system
should try to satisfy its guarantees and use the assumptions only when needed.
Also, the assumptions often describe the valid behavior of other components in
a bigger system, which should not be obstructed unnecessarily.
  In this paper, we present a hierarchy of cooperation levels between system
and environment. Each level describes how well the system enforces both the
assumptions and guarantees. We show how to synthesize systems that achieve the
highest possible cooperation level for a given specification in Linear Temporal
Logic (LTL). The synthesized systems can also exploit cooperative environment
behavior during operation to reach a higher cooperation level that is not
enforceable by the system initially. The worst-case time complexity of our
synthesis procedure is doubly-exponential, which matches the complexity of
standard LTL synthesis.
  This is an extended version of [7] that features an additional appendix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02545</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02545</id><created>2015-07-09</created><updated>2015-08-31</updated><authors><author><keyname>Saha</keyname><forenames>Gourav</forenames></author><author><keyname>Pasumarthy</keyname><forenames>Ramkrishna</forenames></author></authors><title>Maximizing Profit of Cloud Brokers under Quantized Billing Cycles: a
  Dynamic Pricing Strategy based on Ski-Rental Problem</title><categories>cs.DC</categories><comments>11 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In cloud computing, users scale their resources (computational) based on
their need. There is massive literature dealing with such resource scaling
algorithms. These works ignore a fundamental constrain imposed by all Cloud
Service Providers (CSP), i.e. one has to pay for a fixed minimum duration
irrespective of their usage. Such quantization in billing cycles poses problem
for users with sporadic workload. In recent literature, Cloud Broker (CB) has
been introduced for the benefit of such users. A CB rents resources from CSP
and in turn provides service to users to generate profit. Contract between CB
and user is that of pay-what-you-use/pay-per-use. However CB faces the
challenge of Quantized Billing Cycles as it negotiates with CSP. We design two
algorithms, one fully online and the other partially online, which maximizes
the profit of the CB. The key idea is to regulate users demand using dynamic
pricing. Our algorithm is inspired by the Ski-Rental problem. We derive
competitive ratio of these algorithms and also conduct simulations using real
world traces to prove the efficiency of our algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02558</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02558</id><created>2015-07-09</created><authors><author><keyname>Gori</keyname><forenames>Ilaria</forenames></author><author><keyname>Aggarwal</keyname><forenames>J. K.</forenames></author><author><keyname>Ryoo</keyname><forenames>Michael</forenames></author></authors><title>Building Unified Human Descriptors For Multi-Type Activity Recognition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Activity recognition is an important as well as a difficult task in computer
vision. In the past years many types of activities -- single actions, two
persons interactions or ego-centric activities to name a few -- have been
analyzed. Nevertheless, researchers have always treated such types of
activities separately. In this paper, we propose a new problem: labeling a
complex scene where activities of different types happen in sequence or
concurrently. We first present a new unified descriptor, called Relation
History Image (RHI), which can be extracted from all the activity types we are
interested in. We then propose a new method to recognize the activities and at
the same time associate them to the humans who are performing them. Next, we
evaluate our approach on a newly recorded dataset which is representative of
the problem we are considering. Finally, we show the efficacy of the RHI
descriptor on publicly available datasets performing extensive evaluations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02563</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02563</id><created>2015-07-09</created><authors><author><keyname>Shen</keyname><forenames>Wen</forenames></author><author><keyname>Lopes</keyname><forenames>Cristina</forenames></author></authors><title>Managing Autonomous Mobility on Demand Systems for Better Passenger
  Experience</title><categories>cs.AI cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Autonomous mobility on demand systems, though still in their infancy, have
very promising prospects in providing urban population with sustainable and
safe personal mobility in the near future. While much research has been
conducted on both autonomous vehicles and mobility on demand systems, to the
best of our knowledge, this is the first work that shows how to manage
autonomous mobility on demand systems for better passenger experience. We
introduce the Expand and Target algorithm which can be easily integrated with
three different scheduling strategies for dispatching autonomous vehicles. We
implement an agent-based simulation platform and empirically evaluate the
proposed approaches with the New York City taxi data. Experimental results
demonstrate that the algorithm significantly improve passengers' experience by
reducing the average passenger waiting time by up to 29.82% and increasing the
trip success rate by up to 7.65%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02564</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02564</id><created>2015-07-09</created><authors><author><keyname>Bubeck</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Eldan</keyname><forenames>Ronen</forenames></author><author><keyname>Lehec</keyname><forenames>Joseph</forenames></author></authors><title>Sampling from a log-concave distribution with Projected Langevin Monte
  Carlo</title><categories>math.PR cs.DS cs.LG</categories><comments>Preliminary version; 23 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend the Langevin Monte Carlo (LMC) algorithm to compactly supported
measures via a projection step, akin to projected Stochastic Gradient Descent
(SGD). We show that (projected) LMC allows to sample in polynomial time from a
log-concave distribution with smooth potential. This gives a new Markov chain
to sample from a log-concave distribution. Our main result shows in particular
that when the target distribution is uniform, LMC mixes in $\tilde{O}(n^7)$
steps (where $n$ is the dimension). We also provide preliminary experimental
evidence that LMC performs at least as well as hit-and-run, for which a better
mixing time of $\tilde{O}(n^4)$ was proved by Lov{\'a}sz and Vempala.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02566</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02566</id><created>2015-07-09</created><authors><author><keyname>Kretz</keyname><forenames>Tobias</forenames></author></authors><title>On Oscillations in the Social Force Model</title><categories>physics.soc-ph cs.CE cs.MA</categories><comments>accepted for publication in Physica A</comments><doi>10.1016/j.physa.2015.07.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Social Force Model is one of the most prominent models of pedestrian
dynamics. As such naturally much discussion and criticism has spawned around
it, some of which concerns the existence of oscillations in the movement of
pedestrians. This contribution is investigating under which circumstances,
parameter choices, and model variants oscillations do occur and how this can be
prevented. It is shown that oscillations can be excluded if the model
parameters fulfill certain relations. The fact that with some parameter choices
oscillations occur and with some not is exploited to verify a specific computer
implementation of the model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02574</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02574</id><created>2015-07-09</created><authors><author><keyname>Blum</keyname><forenames>Avrim</forenames></author><author><keyname>Har-Peled</keyname><forenames>Sariel</forenames></author><author><keyname>Raichel</keyname><forenames>Benjamin</forenames></author></authors><title>Sparse Approximation via Generating Point Sets</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  $ \newcommand{\kalg}{{k_{\mathrm{alg}}}}
  \newcommand{\kopt}{{k_{\mathrm{opt}}}}
  \newcommand{\algset}{{T}} \renewcommand{\Re}{\mathbb{R}}
  \newcommand{\eps}{\varepsilon} \newcommand{\pth}[2][\!]{#1\left({#2}\right)}
\newcommand{\npoints}{n} \newcommand{\ballD}{\mathsf{b}}
\newcommand{\dataset}{{P}} $ For a set $\dataset$ of $\npoints$ points in the
unit ball $\ballD \subseteq \Re^d$, consider the problem of finding a small
subset $\algset \subseteq \dataset$ such that its convex-hull
$\eps$-approximates the convex-hull of the original set. We present an
efficient algorithm to compute such a $\eps'$-approximation of size $\kalg$,
where $\eps'$ is function of $\eps$, and $\kalg$ is a function of the minimum
size $\kopt$ of such an $\eps$-approximation. Surprisingly, there is no
dependency on the dimension $d$ in both bounds. Furthermore, every point of
$\dataset$ can be $\eps$-approximated by a convex-combination of points of
$\algset$ that is $O(1/\eps^2)$-sparse.
  Our result can be viewed as a method for sparse, convex autoencoding:
approximately representing the data in a compact way using sparse combinations
of a small subset $\algset$ of the original data. The new algorithm can be
kernelized, and it preserves sparsity in the original input.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02581</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02581</id><created>2015-07-09</created><authors><author><keyname>Rao</keyname><forenames>Micha&#xeb;l</forenames></author><author><keyname>Rosenfeld</keyname><forenames>Matthieu</forenames></author></authors><title>Avoidability of long $k$-abelian repetitions</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the avoidability of long $k$-abelian-squares and $k$-abelian-cubes
on binary and ternary alphabets. For $k=1$, these are M\&quot;akel\&quot;a's questions.
We show that one cannot avoid abelian-cubes of abelian period at least $2$ in
infinite binary words, and therefore answering negatively one question from
M\&quot;akel\&quot;a. Then we show that one can avoid $3$-abelian-squares of period at
least $3$ in infinite binary words and $2$-abelian-squares of period at least 2
in infinite ternary words. Finally we study the minimum number of distinct
$k$-abelian-squares that must appear in an infinite binary word.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02585</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02585</id><created>2015-07-08</created><authors><author><keyname>Charmandaris</keyname><forenames>V.</forenames><affiliation>Univ. of Crete &amp; Nat. Obs. of Athens, Greece</affiliation></author></authors><title>Greek Astronomy PhDs: The last 200 years</title><categories>physics.hist-ph astro-ph.IM cs.DL</categories><comments>8 pages, 7 figures, (original file also available at
  http://www.helas.gr/reports/Charmandaris_2015.pdf )</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have recently compiled a database with all doctoral dissertations (PhDs)
completed in modern Greece (1837-2014), in the general area of astronomy and
astrophysics, as well as in space and ionospheric physics. A preliminary
statistical analysis of the data is presented, along with a discussion of the
general trends observed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02592</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02592</id><created>2015-07-09</created><updated>2015-09-01</updated><authors><author><keyname>van Erven</keyname><forenames>Tim</forenames></author><author><keyname>Gr&#xfc;nwald</keyname><forenames>Peter D.</forenames></author><author><keyname>Mehta</keyname><forenames>Nishant A.</forenames></author><author><keyname>Reid</keyname><forenames>Mark D.</forenames></author><author><keyname>Williamson</keyname><forenames>Robert C.</forenames></author></authors><title>Fast rates in statistical and online learning</title><categories>cs.LG stat.ML</categories><comments>69 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The speed with which a learning algorithm converges as it is presented with
more data is a central problem in machine learning --- a fast rate of
convergence means less data is needed for the same level of performance. The
pursuit of fast rates in online and statistical learning has led to the
discovery of many conditions in learning theory under which fast learning is
possible. We show that most of these conditions are special cases of a single,
unifying condition, that comes in two forms: the central condition for 'proper'
learning algorithms that always output a hypothesis in the given model, and
stochastic mixability for online algorithms that may make predictions outside
of the model. We show that under surprisingly weak assumptions both conditions
are, in a certain sense, equivalent. The central condition has a
re-interpretation in terms of convexity of a set of pseudoprobabilities,
linking it to density estimation under misspecification. For bounded losses, we
show how the central condition enables a direct proof of fast rates and we
prove its equivalence to the Bernstein condition, itself a generalization of
the Tsybakov margin condition, both of which have played a central role in
obtaining fast rates in statistical learning. Yet, while the Bernstein
condition is two-sided, the central condition is one-sided, making it more
suitable to deal with unbounded losses. In its stochastic mixability form, our
condition generalizes both a stochastic exp-concavity condition identified by
Juditsky, Rigollet and Tsybakov and Vovk's notion of mixability. Our unifying
conditions thus provide a substantial step towards a characterization of fast
rates in statistical learning, similar to how classical mixability
characterizes constant regret in the sequential prediction with expert advice
setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02598</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02598</id><created>2015-07-09</created><authors><author><keyname>Angjelichinoski</keyname><forenames>Marko</forenames></author><author><keyname>Stefanovic</keyname><forenames>Cedomir</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author><author><keyname>Blaabjerg</keyname><forenames>Frede</forenames></author></authors><title>Power Talk in DC Micro Grids: Constellation Design and Error Probability
  Performance</title><categories>cs.IT cs.SY math.IT</categories><comments>IEEE SmartGridComm 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Power talk is a novel concept for communication among units in a Micro Grid
(MG), where information is sent by using power electronics as modems and the
common bus of the MG as a communication medium. The technique is implemented by
modifying the droop control parameters from the primary control level. In this
paper, we consider power talk in a DC MG and introduce a channel model based on
Thevenin equivalent. The result is a channel whose state that can be estimated
by both the transmitter and the receiver. Using this model, we present design
of symbol constellations of arbitrary order and analyze the error probability
performance. Finally, we also show how to design adaptive modulation in the
proposed communication framework, which leads to significant performance
benefits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02615</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02615</id><created>2015-07-09</created><authors><author><keyname>Alaei</keyname><forenames>Saeed</forenames></author><author><keyname>Hartline</keyname><forenames>Jason</forenames></author><author><keyname>Niazadeh</keyname><forenames>Rad</forenames></author><author><keyname>Pountourakis</keyname><forenames>Emmanouil</forenames></author><author><keyname>Yuan</keyname><forenames>Yang</forenames></author></authors><title>Optimal Auctions vs. Anonymous Pricing</title><categories>cs.GT cs.DS</categories><comments>19 pages, 6 figures, To appear in 56th Annual IEEE Symposium on
  Foundations of Computer Science (FOCS 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For selling a single item to agents with independent but non-identically
distributed values, the revenue optimal auction is complex. With respect to it,
Hartline and Roughgarden (2009) showed that the approximation factor of the
second-price auction with an anonymous reserve is between two and four. We
consider the more demanding problem of approximating the revenue of the ex ante
relaxation of the auction problem by posting an anonymous price (while supplies
last) and prove that their worst-case ratio is e. As a corollary, the
upper-bound of anonymous pricing or anonymous reserves versus the optimal
auction improves from four to $e$. We conclude that, up to an $e$ factor,
discrimination and simultaneity are unimportant for driving revenue in
single-item auctions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02618</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02618</id><created>2015-07-09</created><authors><author><keyname>Alstrup</keyname><forenames>Stephen</forenames></author><author><keyname>Dahlgaard</keyname><forenames>S&#xf8;ren</forenames></author><author><keyname>Knudsen</keyname><forenames>Mathias B&#xe6;k Tejs</forenames></author><author><keyname>Porat</keyname><forenames>Ely</forenames></author></authors><title>Sublinear distance labeling for sparse graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A distance labeling scheme labels the $n$ nodes of a graph with binary
strings such that, given the labels of any two nodes, one can determine the
distance in the graph between the two nodes by looking only at the labels. A
$D$-preserving distance labeling scheme only returns precise distances between
pairs of nodes that are at distance at least $D$ from each other. In this paper
we consider distance labeling schemes for the classical case of unweighted and
undirected graphs.
  We present the first distance labeling scheme of size $o(n)$ for sparse
graphs (and hence bounded degree graphs). This addresses an open problem by
Gavoille et. al. [J. Algo. 2004], hereby separating the complexity from general
graphs which require $\Omega(n)$ size Moon [Proc. of Glasgow Math. Association
1965]. As an intermediate result we give a $O(\frac{n}{D}\log^2 D)$
$D$-preserving distance labeling scheme, improving the previous bound by
Bollob\'as et. al. [SIAM J. Discrete Math. 2005]. For this problem, we also
give an almost matching lower bound $\Omega(\frac{n}{D})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02620</identifier>
 <datestamp>2015-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02620</id><created>2015-07-09</created><updated>2015-11-18</updated><authors><author><keyname>Cimpoi</keyname><forenames>Mircea</forenames></author><author><keyname>Maji</keyname><forenames>Subhransu</forenames></author><author><keyname>Kokkinos</keyname><forenames>Iasonas</forenames></author><author><keyname>Vedaldi</keyname><forenames>Andrea</forenames></author></authors><title>Deep filter banks for texture recognition, description, and segmentation</title><categories>cs.CV</categories><comments>29 pages; 13 figures; 8 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visual textures have played a key role in image understanding because they
convey important semantics of images, and because texture representations that
pool local image descriptors in an orderless manner have had a tremendous
impact in diverse applications. In this paper we make several contributions to
texture understanding. First, instead of focusing on texture instance and
material category recognition, we propose a human-interpretable vocabulary of
texture attributes to describe common texture patterns, complemented by a new
describable texture dataset for benchmarking. Second, we look at the problem of
recognizing materials and texture attributes in realistic imaging conditions,
including when textures appear in clutter, developing corresponding benchmarks
on top of the recently proposed OpenSurfaces dataset. Third, we revisit classic
texture representations, including bag-of-visual-words and the Fisher vectors,
in the context of deep learning and show that these have excellent efficiency
and generalization properties if the convolutional layers of a deep model are
used as filter banks. We obtain in this manner state-of-the-art performance in
numerous datasets well beyond textures, an efficient method to apply deep
features to image regions, as well as benefit in transferring features from one
domain to another.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02628</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02628</id><created>2015-07-09</created><authors><author><keyname>Wang</keyname><forenames>Zhiguo</forenames></author><author><keyname>Ittycheriah</keyname><forenames>Abraham</forenames></author></authors><title>FAQ-based Question Answering via Word Alignment</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel word-alignment-based method to solve the
FAQ-based question answering task. First, we employ a neural network model to
calculate question similarity, where the word alignment between two questions
is used for extracting features. Second, we design a bootstrap-based feature
extraction method to extract a small set of effective lexical features. Third,
we propose a learning-to-rank algorithm to train parameters more suitable for
the ranking tasks. Experimental results, conducted on three languages (English,
Spanish and Japanese), demonstrate that the question similarity model is more
effective than baseline systems, the sparse features bring 5% improvements on
top-1 accuracy, and the learning-to-rank algorithm works significantly better
than the traditional method. We further evaluate our method on the answer
sentence selection task. Our method outperforms all the previous systems on the
standard TREC data set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02636</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02636</id><created>2015-07-09</created><authors><author><keyname>Addis</keyname><forenames>Bernardetta</forenames></author><author><keyname>Capone</keyname><forenames>Antonio</forenames></author><author><keyname>Carello</keyname><forenames>Giuliana</forenames></author><author><keyname>Gianoli</keyname><forenames>Luca G.</forenames></author><author><keyname>Sans&#xf2;</keyname><forenames>Brunilde</forenames></author></authors><title>Energy management in communication networks: a journey through modelling
  and optimization glasses</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The widespread proliferation of Internet and wireless applications has
produced a significant increase of ICT energy footprint. As a response, in the
last five years, significant efforts have been undertaken to include
energy-awareness into network management. Several green networking frameworks
have been proposed by carefully managing the network routing and the power
state of network devices.
  Even though approaches proposed differ based on network technologies and
sleep modes of nodes and interfaces, they all aim at tailoring the active
network resources to the varying traffic needs in order to minimize energy
consumption. From a modeling point of view, this has several commonalities with
classical network design and routing problems, even if with different
objectives and in a dynamic context.
  With most researchers focused on addressing the complex and crucial
technological aspects of green networking schemes, there has been so far little
attention on understanding the modeling similarities and differences of
proposed solutions. This paper fills the gap surveying the literature with
optimization modeling glasses, following a tutorial approach that guides
through the different components of the models with a unified symbolism. A
detailed classification of the previous work based on the modeling issues
included is also proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02642</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02642</id><created>2015-07-09</created><authors><author><keyname>Wiebe</keyname><forenames>Nathan</forenames></author><author><keyname>Kapoor</keyname><forenames>Ashish</forenames></author><author><keyname>Granade</keyname><forenames>Christopher</forenames></author><author><keyname>Svore</keyname><forenames>Krysta M</forenames></author></authors><title>Quantum Inspired Training for Boltzmann Machines</title><categories>cs.LG quant-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an efficient classical algorithm for training deep Boltzmann
machines (DBMs) that uses rejection sampling in concert with variational
approximations to estimate the gradients of the training objective function.
Our algorithm is inspired by a recent quantum algorithm for training DBMs. We
obtain rigorous bounds on the errors in the approximate gradients; in turn, we
find that choosing the instrumental distribution to minimize the alpha=2
divergence with the Gibbs state minimizes the asymptotic algorithmic
complexity. Our rejection sampling approach can yield more accurate gradients
than low-order contrastive divergence training and the costs incurred in
finding increasingly accurate gradients can be easily parallelized. Finally our
algorithm can train full Boltzmann machines and scales more favorably with the
number of layers in a DBM than greedy contrastive divergence training.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02672</identifier>
 <datestamp>2015-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02672</id><created>2015-07-09</created><updated>2015-11-24</updated><authors><author><keyname>Rasmus</keyname><forenames>Antti</forenames></author><author><keyname>Valpola</keyname><forenames>Harri</forenames></author><author><keyname>Honkala</keyname><forenames>Mikko</forenames></author><author><keyname>Berglund</keyname><forenames>Mathias</forenames></author><author><keyname>Raiko</keyname><forenames>Tapani</forenames></author></authors><title>Semi-Supervised Learning with Ladder Networks</title><categories>cs.NE cs.LG stat.ML</categories><comments>Revised denoising function, updated results, fixed typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We combine supervised learning with unsupervised learning in deep neural
networks. The proposed model is trained to simultaneously minimize the sum of
supervised and unsupervised cost functions by backpropagation, avoiding the
need for layer-wise pre-training. Our work builds on the Ladder network
proposed by Valpola (2015), which we extend by combining the model with
supervision. We show that the resulting model reaches state-of-the-art
performance in semi-supervised MNIST and CIFAR-10 classification, in addition
to permutation-invariant MNIST classification with all labels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02674</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02674</id><created>2015-07-09</created><authors><author><keyname>Harris</keyname><forenames>David G.</forenames></author><author><keyname>Srinivasan</keyname><forenames>Aravind</forenames></author></authors><title>Algorithmic and enumerative aspects of the Moser-Tardos distribution</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Moser &amp; Tardos have developed a powerful algorithmic approach (henceforth
&quot;MT&quot;) to the Lovasz Local Lemma (LLL); the basic operation done in MT and its
variants is a search for &quot;bad&quot; events in a current configuration. In the
initial stage of MT, the variables are set independently. We examine the
distributions on these variables which arise during intermediate stages of MT.
We show that these configurations have a more or less &quot;random&quot; form, building
further on the &quot;MT-distribution&quot; concept of Haeupler et al. in understanding
the (intermediate and) output distribution of MT. This has a variety of
algorithmic applications; the most important is that bad events can be found
relatively quickly, improving upon MT across the complexity spectrum: it makes
some polynomial-time algorithms sub-linear (e.g., for Latin transversals, which
are of basic combinatorial interest), gives lower-degree polynomial run-times
in some settings, transforms certain super-polynomial-time algorithms into
polynomial-time ones, and leads to Las Vegas algorithms for some coloring
problems for which only Monte Carlo algorithms were known.
  We show that in certain conditions when the LLL condition is violated, a
variant of the MT algorithm can still produce a distribution which avoids most
of the bad events. We show in some cases this MT variant can run faster than
the original MT algorithm itself, and develop the first-known criterion for the
case of the asymmetric LLL. This can be used to find partial Latin transversals
-- improving upon earlier bounds of Stein (1975) -- among other applications.
We furthermore give applications in enumeration, showing that most applications
(where we aim for all or most of the bad events to be avoided) have many more
solutions than known before by proving that the MT-distribution has &quot;large&quot;
min-entropy and hence that its support-size is large.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02703</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02703</id><created>2015-07-09</created><authors><author><keyname>Song</keyname><forenames>Shuran</forenames></author><author><keyname>Zhang</keyname><forenames>Linguang</forenames></author><author><keyname>Xiao</keyname><forenames>Jianxiong</forenames></author></authors><title>Robot In a Room: Toward Perfect Object Recognition in Closed
  Environments</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While general object recognition is still far from being solved, this paper
proposes a way for a robot to recognize every object at an almost human-level
accuracy. Our key observation is that many robots will stay in a relatively
closed environment (e.g. a house or an office). By constraining a robot to stay
in a limited territory, we can ensure that the robot has seen most objects
before and the speed of introducing a new object is slow. Furthermore, we can
build a 3D map of the environment to reliably subtract the background to make
recognition easier. We propose extremely robust algorithms to obtain a 3D map
and enable humans to collectively annotate objects. During testing time, our
algorithm can recognize all objects very reliably, and query humans from crowd
sourcing platform if confidence is low or new objects are identified. This
paper explains design decisions in building such a system, and constructs a
benchmark for extensive evaluation. Experiments suggest that making robot
vision appear to be working from an end user's perspective is a reachable goal
today, as long as the robot stays in a closed environment. By formulating this
task, we hope to lay the foundation of a new direction in vision for robotics.
Code and data will be available upon acceptance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02718</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02718</id><created>2015-07-09</created><authors><author><keyname>Beyhaghi</keyname><forenames>Hedyeh</forenames></author><author><keyname>Dikkala</keyname><forenames>Nishanth</forenames></author><author><keyname>Tardos</keyname><forenames>Eva</forenames></author></authors><title>Effect of Strategic Grading and Early Offers in Matching Markets</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Strategic suppression of grades, as well as early offers and contracts, are
well-known phenomena in the matching process where graduating students apply to
jobs or further education. In this paper, we consider a game theoretic model of
these phenomena introduced by Ostrovsky and Schwarz, and study the loss in
social welfare resulting from strategic behavior of the schools, employers, and
students. We model grading of students as a game where schools suppress grades
in order to improve their students' placements. We also consider the quality
loss due to unraveling of the matching market, the strategic behavior of
students and employers in offering early contracts with the goal to improve the
quality. Our goal is to evaluate if strategic grading or unraveling of the
market (or a combination of the two) can cause significant welfare loss
compared to the optimal assignment of students to jobs. To measure welfare of
the assignment, we assume that welfare resulting from a job -- student pair is
a separable and monotone function of student ability and the quality of the
jobs. Assuming uniform student quality distribution, we show that the quality
loss from the above strategic manipulation is bounded by at most a factor of 2,
and give improved bounds for some special cases of welfare functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02721</identifier>
 <datestamp>2015-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02721</id><created>2015-07-09</created><updated>2015-07-23</updated><authors><author><keyname>M&#xe9;tivier</keyname><forenames>Y.</forenames></author><author><keyname>Robson</keyname><forenames>J. M.</forenames></author><author><keyname>Zemmari</keyname><forenames>A.</forenames></author></authors><title>On Distributed Computing with Beeps</title><categories>cs.DC</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider networks of processes which interact with beeps. Various beeping
models are used. The basic one, defined by Cornejo and Kuhn [CK10], assumes
that a process can choose either to beep or to listen; if it listens it can
distinguish between silence or the presence of at least one beep. The aim of
this paper is the study of the resolution of paradigms such as collision
detection, computation of the degree of a vertex, colouring, or 2-hop-colouring
in the framework of beeping models. For each of these problems we present Las
Vegas or Monte Carlo algorithms and we analyse their complexities expressed in
terms of the number of slots. We present also efficient randomised emulations
of more powerful beeping models on the basic one. We illustrate emulation
procedures with an efficient degree computation algorithm in the basic beeping
model; this algorithm was given initially in a more powerful model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02722</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02722</id><created>2015-07-09</created><authors><author><keyname>Aghazadeh</keyname><forenames>Zahra</forenames></author><author><keyname>Woelfel</keyname><forenames>Philipp</forenames></author></authors><title>On the Time and Space Complexity of ABA Prevention and Detection</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the time and space complexity of detecting and preventing ABAs
in shared memory algorithms for systems with n processes and bounded base
objects. To that end, we define ABA-detecting registers, which are similar to
normal read/write registers, except that they allow a process q to detect with
a read operation, whether some process wrote the register since q's last read.
ABA-detecting registers can be implemented trivially from a single unbounded
register, but we show that they have a high complexity if base objects are
bounded: An obstruction-free implementation of an ABA-detecting single bit
register cannot be implemented from fewer than n-1 bounded registers. Moreover,
bounded CAS objects (or more generally, conditional read-modify-write
primitives) offer little help to implement ABA-detecting single bit registers:
We prove a linear time-space tradeoff for such implementations. We show that
the same time-space tradeoff holds for implementations of single bit LL/SC
primitives from bounded writable CAS objects. This proves that the
implementations of LL/SC/VL by Anderson and Moir (1995) as well as Jayanti and
Petrovic (2003) are optimal. We complement our lower bounds with tight upper
bounds: We give an implementation of ABA-detecting registers from n+1 bounded
registers, which has step complexity O(1). We also show that (bounded) LL/SC/VL
can be implemented from a single bounded CAS object and with O(n) step
complexity. Both upper bounds are asymptotically optimal with respect to their
time-space product. These results give formal evidence that the ABA problem is
inherently difficult, that even writable CAS objects do not provide significant
benefits over registers for dealing with the ABA problem itself, and that there
is no hope of finding a more efficient implementation of LL/SC/VL from bounded
CAS objects and registers than the ones mentioned.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02723</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02723</id><created>2015-07-09</created><authors><author><keyname>Petersen</keyname><forenames>Holger</forenames></author></authors><title>An NL-Complete Puzzle</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the complexity of a puzzle that turns out to be NL-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02726</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02726</id><created>2015-07-09</created><authors><author><keyname>Cuiti&#xf1;o</keyname><forenames>Luis Felipe Tapia</forenames></author><author><keyname>Tironi</keyname><forenames>Andrea Luigi</forenames></author></authors><title>Some properties of skew codes over finite fields</title><categories>cs.IT math.IT math.RA</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  After recalling the definition of codes as modules over skew polynomial
rings, whose multiplication is defined by using an automorphism and a
derivation, and some basic facts about them, in the first part of this paper we
study some of their main algebraic and geometric properties. Finally, for
module skew codes constructed only with an automorphism, we give some BCH type
lower bounds for their minimum distance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02731</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02731</id><created>2015-07-09</created><authors><author><keyname>Mahmoudi</keyname><forenames>Monirehalsadat</forenames></author><author><keyname>Zhou</keyname><forenames>Xuesong</forenames></author></authors><title>Finding Optimal Solutions for Vehicle Routing Problem with Pickup and
  Delivery Services with Time Windows: A Dynamic Programming Approach Based on
  State-space-time Network Representations</title><categories>math.OC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimization of on-demand transportation systems and ride-sharing services
involves solving a class of complex vehicle routing problems with pickup and
delivery with time windows (VRPPDTW). This paper first proposes a new
time-discretized multi-commodity network flow model for the VRPPDTW based on
the integration of vehicles carrying states within space-time transportation
networks, so as to allow a joint optimization of passenger-to-vehicle
assignment and turn-by-turn routing in congested transportation networks. Our
three-dimensional state-space-time network construct is able to comprehensively
enumerate possible transportation states at any given time along vehicle
space-time paths, and further allow a forward dynamic programming solution
algorithm to solve the single vehicle VRPPDTW problem. By utilizing a
Lagrangian relaxation approach, the primal multi-vehicle routing problem is
decomposed to a sequence of single vehicle routing sub-problems, with
Lagrangian multipliers for individual passengers requests being updated by
sub-gradient-based algorithms. We further discuss a number of search space
reduction strategies and test our algorithms, implemented through a specialized
program in C++, on medium-scale and large-scale transportation networks, namely
the Chicago sketch and Phoenix regional networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02743</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02743</id><created>2015-07-09</created><authors><author><keyname>Bhatia</keyname><forenames>Kush</forenames></author><author><keyname>Jain</keyname><forenames>Himanshu</forenames></author><author><keyname>Kar</keyname><forenames>Purushottam</forenames></author><author><keyname>Jain</keyname><forenames>Prateek</forenames></author><author><keyname>Varma</keyname><forenames>Manik</forenames></author></authors><title>Locally Non-linear Embeddings for Extreme Multi-label Learning</title><categories>cs.LG cs.IR math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The objective in extreme multi-label learning is to train a classifier that
can automatically tag a novel data point with the most relevant subset of
labels from an extremely large label set. Embedding based approaches make
training and prediction tractable by assuming that the training label matrix is
low-rank and hence the effective number of labels can be reduced by projecting
the high dimensional label vectors onto a low dimensional linear subspace.
Still, leading embedding approaches have been unable to deliver high prediction
accuracies or scale to large problems as the low rank assumption is violated in
most real world applications.
  This paper develops the X-One classifier to address both limitations. The
main technical contribution in X-One is a formulation for learning a small
ensemble of local distance preserving embeddings which can accurately predict
infrequently occurring (tail) labels. This allows X-One to break free of the
traditional low-rank assumption and boost classification accuracy by learning
embeddings which preserve pairwise distances between only the nearest label
vectors.
  We conducted extensive experiments on several real-world as well as benchmark
data sets and compared our method against state-of-the-art methods for extreme
multi-label classification. Experiments reveal that X-One can make
significantly more accurate predictions then the state-of-the-art methods
including both embeddings (by as much as 35%) as well as trees (by as much as
6%). X-One can also scale efficiently to data sets with a million labels which
are beyond the pale of leading embedding methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02744</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02744</id><created>2015-07-09</created><authors><author><keyname>Ponce-de-Le&#xf3;n</keyname><forenames>Hern&#xe1;n</forenames></author><author><keyname>Rodr&#xed;guez</keyname><forenames>C&#xe9;sar</forenames></author><author><keyname>Carmona</keyname><forenames>Josep</forenames></author><author><keyname>Heljanko</keyname><forenames>Keijo</forenames></author><author><keyname>Haar</keyname><forenames>Stefan</forenames></author></authors><title>Unfolding-Based Process Discovery</title><categories>cs.LO</categories><comments>This is the unabridged version of a paper with the same title
  appearead at the proceedings of ATVA 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel technique for process discovery. In contrast to
the current trend, which only considers an event log for discovering a process
model, we assume two additional inputs: an independence relation on the set of
logged activities, and a collection of negative traces. After deriving an
intermediate net unfolding from them, we perform a controlled folding giving
rise to a Petri net which contains both the input log and all
independence-equivalent traces arising from it. Remarkably, the derived Petri
net cannot execute any trace from the negative collection. The entire chain of
transformations is fully automated. A tool has been developed and experimental
results are provided that witness the significance of the contribution of this
paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02746</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02746</id><created>2015-07-09</created><updated>2015-07-20</updated><authors><author><keyname>Esfandiari</keyname><forenames>Hossein</forenames></author><author><keyname>Kortsarz</keyname><forenames>Guy</forenames></author></authors><title>Low-Risk Mechanisms for the Kidney Exchange Game</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the pairwise kidney exchange game. This game
naturally appears in situations that some service providers benefit from
pairwise allocations on a network, such as the kidney exchanges between
hospitals.
  Ashlagi et al. present a $2$-approximation randomized truthful mechanism for
this problem. This is the best known result in this setting with multiple
players. However, we note that the variance of the utility of an agent in this
mechanism may be as large as $\Omega(n^2)$, which is not desirable in a real
application. In this paper we resolve this issue by providing a
$2$-approximation randomized truthful mechanism in which the variance of the
utility of each agent is at most $2+\epsilon$.
  Interestingly, we could apply our technique to design a deterministic
mechanism such that, if an agent deviates from the mechanism, she does not gain
more than $2\lceil \log_2 m\rceil$. We call such a mechanism an almost truthful
mechanism. Indeed, in a practical scenario, an almost truthful mechanism is
likely to imply a truthful mechanism. We believe that our approach can be used
to design low risk or almost truthful mechanisms for other problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02750</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02750</id><created>2015-07-09</created><updated>2015-09-25</updated><authors><author><keyname>Gajane</keyname><forenames>Pratik</forenames></author><author><keyname>Urvoy</keyname><forenames>Tanguy</forenames></author></authors><title>Utility-based Dueling Bandits as a Partial Monitoring Game</title><categories>cs.LG</categories><comments>Accepted at the 12th European Workshop on Reinforcement Learning
  (EWRL 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Partial monitoring is a generic framework for sequential decision-making with
incomplete feedback. It encompasses a wide class of problems such as dueling
bandits, learning with expect advice, dynamic pricing, dark pools, and label
efficient prediction. We study the utility-based dueling bandit problem as an
instance of partial monitoring problem and prove that it fits the time-regret
partial monitoring hierarchy as an easy - i.e. Theta (sqrt{T})- instance. We
survey some partial monitoring algorithms and see how they could be used to
solve dueling bandits efficiently. Keywords: Online learning, Dueling Bandits,
Partial Monitoring, Partial Feedback, Multiarmed Bandits
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02761</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02761</id><created>2015-07-09</created><authors><author><keyname>Shirvanimoghaddam</keyname><forenames>Mahyar</forenames></author><author><keyname>Li</keyname><forenames>Yonghui</forenames></author><author><keyname>Dohler</keyname><forenames>Mischa</forenames></author><author><keyname>Vucetic</keyname><forenames>Branka</forenames></author></authors><title>Probabilistic Rateless Multiple Access for Machine-to-Machine
  Communication</title><categories>cs.IT math.IT</categories><comments>Accepted to Publish in IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Future machine to machine (M2M) communications need to support a massive
number of devices communicating with each other with little or no human
intervention. Random access techniques were originally proposed to enable M2M
multiple access, but suffer from severe congestion and access delay in an M2M
system with a large number of devices. In this paper, we propose a novel
multiple access scheme for M2M communications based on the capacity-approaching
analog fountain code to efficiently minimize the access delay and satisfy the
delay requirement for each device. This is achieved by allowing M2M devices to
transmit at the same time on the same channel in an optimal probabilistic
manner based on their individual delay requirements. Simulation results show
that the proposed scheme achieves a near optimal rate performance and at the
same time guarantees the delay requirements of the devices. We further propose
a simple random access strategy and characterized the required overhead.
Simulation results show the proposed approach significantly outperforms the
existing random access schemes currently used in long term evolution advanced
(LTE-A) standard in terms of the access delay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02762</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02762</id><created>2015-07-09</created><authors><author><keyname>Langerman</keyname><forenames>Stefan</forenames></author><author><keyname>Winslow</keyname><forenames>Andrew</forenames></author></authors><title>A Quasilinear-Time Algorithm for Tiling the Plane Isohedrally with a
  Polyomino</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A plane tiling consisting of congruent copies of a shape is
$\textit{isohedral}$ provided that for any pair of copies, there exists a
symmetry of the tiling mapping one copy to the other. We give a
$O(n\log^2{n})$-time algorithm for deciding if a polyomino with $n$ edges can
tile the plane isohedrally. This improves on the $O(n^{18})$-time algorithm of
Keating and Vince and generalizes recent work by Brlek, Proven\c{c}al,
F\'{e}dou, and the second author.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02764</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02764</id><created>2015-07-09</created><authors><author><keyname>Kang</keyname><forenames>Jaewook</forenames></author><author><keyname>Jung</keyname><forenames>Hyoyoung</forenames></author><author><keyname>Kim</keyname><forenames>Kiseon</forenames></author></authors><title>Fast Signal Separation of 2D Sparse Mixture via Approximate
  Message-Passing</title><categories>cs.IT math.IT</categories><comments>five figures</comments><journal-ref>Published in IEEE Signal Processing Letters, vol. 22, issue 11,
  pp. 2024-2028, Nov. 2015</journal-ref><doi>10.1109/LSP.2015.2454003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Approximate message-passing (AMP) method is a simple and efficient framework
for the linear inverse problems. In this letter, we propose a faster AMP to
solve the \emph{$L_1$-Split-Analysis} for the 2D sparsity separation, which is
referred to as \emph{MixAMP}. We develop the MixAMP based on the factor
graphical modeling and the min-sum message-passing. Then, we examine MixAMP for
two types of the sparsity separation: separation of the direct-and-group
sparsity, and that of the direct-and-finite-difference sparsity. This case
study shows that the MixAMP method offers computational advantages over the
conventional first-order method, TFOCS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02766</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02766</id><created>2015-07-09</created><authors><author><keyname>Monserrat</keyname><forenames>Toni-Jan Keith P.</forenames></author><author><keyname>Pabico</keyname><forenames>Jaderick P.</forenames></author><author><keyname>Albacea</keyname><forenames>Eliezer A.</forenames></author></authors><title>A Hybrid Graph-drawing Algorithm for Large, Naturally-clustered,
  Disconnected Graphs</title><categories>cs.GR</categories><comments>13 pages, 2 figures, originally appeared in H.N. Adorna and J.M.
  Samaniego (eds.) Proceedings of the 5th National Symposium on Mathematical
  Aspects of Computer Science (SMACS 2010), University of the Philippines Los
  Ba\~nos, College, Laguna, 3-4 December 2010, pp. 32-38</comments><journal-ref>Asia Pacific Journal of Multidisciplinary Research 2(4):119-126</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In this paper, we present a hybrid graph-drawing algorithm (GDA) for
layouting large, naturally-clustered, disconnected graphs. We called it a
hybrid algorithm because it is an implementation of a series of already known
graph-drawing and graph-theoretic procedures. We remedy in this hybrid the
problematic nature of the current force-based GDA which has the inability to
scale to large, naturally-clustered, and disconnected graphs. These kinds of
graph usually model the complex inter-relationships among entities in social,
biological, natural, and artificial networks. Obviously, the hybrid runs longer
than the current GDAs. By using two extreme cases of graphs as inputs, we
present in this paper the derivation of the time complexity of the hybrid which
we found to be $O(|\V|^3)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02770</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02770</id><created>2015-07-09</created><authors><author><keyname>Pabico</keyname><forenames>Jaderick P.</forenames></author><author><keyname>Micor</keyname><forenames>Jose Rene L.</forenames></author></authors><title>Ang Social Network sa Facebook ng mga Taga-Batangas at ng mga
  Taga-Laguna: Isang Paghahambing</title><categories>cs.SI</categories><comments>16 pages, 8 figures; The text is written in Filipino language with
  Abstract translated to English. The title in English is &quot;The Social Network
  of Batangas and Laguna Facebook Users: A Comparison.&quot; Presented in First
  Sotero H. Laurel Social Science Conference, Lyceum of the Philippines
  University-Batangas, Batangas City, Philippines, 24 September 2013</comments><journal-ref>Asia Pacific Journal of Multidisciplinary Research 1(1):138-150</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Online social networking (OSN) has become of great influence to Filipinos,
where Facebook, Twitter, LinkedIn, Google+, and Instagram are among the popular
ones. Their popularity, coupled with their intuitive and interactive use, allow
one's personal information such as gender, age, address, relationship status,
and list of friends to become publicly available. The accessibility of
information from these sites allow, with the aid of computers, for the study of
a wide population's characteristics even in a provincial scale. Aside from
being neighbouring locales, the respective residents of Laguna and Batangas
both derive their livelihoods from two lakes, Laguna de Bay and Taal Lake. Both
residents experience similar problems, such as that, among many others, of fish
kill. The goal of this research is to find out similarities in their respective
online populations, particularly that of Facebook's. With the use of
computational dynamic social network analysis (CDSNA), we found out that the
two communities are similar, among others, as follows:
  o Both populations are dominated by single young female
  o Homophily was observed when choosing a friend in terms of age (i.e.,
friendships were created more often between people whose ages do not differ by
at most five years); and
  o Heterophily was observed when choosing friends in terms of gender (i.e.,
more friendships were created between a male and a female than between both
people of the same gender).
  This paper also presents the differences in the structure of the two social
networks, such as degrees of separation and preferential attachment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02772</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02772</id><created>2015-07-09</created><updated>2015-12-16</updated><authors><author><keyname>Cherian</keyname><forenames>Anoop</forenames></author><author><keyname>Sra</keyname><forenames>Suvrit</forenames></author></authors><title>Riemannian Dictionary Learning and Sparse Coding for Positive Definite
  Matrices</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data encoded as symmetric positive definite (SPD) matrices frequently arise
in many areas of computer vision and machine learning. While these matrices
form an open subset of the Euclidean space of symmetric matrices, viewing them
through the lens of non-Euclidean Riemannian geometry often turns out to be
better suited in capturing several desirable data properties. However,
formulating classical machine learning algorithms within such a geometry is
often non-trivial and computationally expensive. Inspired by the great success
of dictionary learning and sparse coding for vector-valued data, our goal in
this paper is to represent data in the form of SPD matrices as sparse conic
combinations of SPD atoms from a learned dictionary via a Riemannian geometric
approach. To that end, we formulate a novel Riemannian optimization objective
for dictionary learning and sparse coding in which the representation loss is
characterized via the affine invariant Riemannian metric. We also present a
computationally simple algorithm for optimizing our model. Experiments on
several computer vision datasets demonstrate superior classification and
retrieval performance using our approach when compared to sparse coding via
alternative non-Riemannian formulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02779</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02779</id><created>2015-07-10</created><authors><author><keyname>Pham</keyname><forenames>Hai X.</forenames></author><author><keyname>Chen</keyname><forenames>Chongyu</forenames></author><author><keyname>Dao</keyname><forenames>Luc N.</forenames></author><author><keyname>Pavlovic</keyname><forenames>Vladimir</forenames></author><author><keyname>Cai</keyname><forenames>Jianfei</forenames></author><author><keyname>Cham</keyname><forenames>Tat-jen</forenames></author></authors><title>Robust Performance-driven 3D Face Tracking in Long Range Depth Scenes</title><categories>cs.CV</categories><comments>10 pages, 8 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a novel robust hybrid 3D face tracking framework from RGBD video
streams, which is capable of tracking head pose and facial actions without
pre-calibration or intervention from a user. In particular, we emphasize on
improving the tracking performance in instances where the tracked subject is at
a large distance from the cameras, and the quality of point cloud deteriorates
severely. This is accomplished by the combination of a flexible 3D shape
regressor and the joint 2D+3D optimization on shape parameters. Our approach
fits facial blendshapes to the point cloud of the human head, while being
driven by an efficient and rapid 3D shape regressor trained on generic RGB
datasets. As an on-line tracking system, the identity of the unknown user is
adapted on-the-fly resulting in improved 3D model reconstruction and
consequently better tracking performance. The result is a robust RGBD face
tracker, capable of handling a wide range of target scene depths, beyond those
that can be afforded by traditional depth or RGB face trackers. Lastly, since
the blendshape is not able to accurately recover the real facial shape, we use
the tracked 3D face model as a prior in a novel filtering process to further
refine the depth map for use in other tasks, such as 3D reconstruction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02786</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02786</id><created>2015-07-10</created><authors><author><keyname>Chen</keyname><forenames>Ping</forenames></author><author><keyname>Xu</keyname><forenames>Jun</forenames></author><author><keyname>Wang</keyname><forenames>Jun</forenames></author><author><keyname>Liu</keyname><forenames>Peng</forenames></author></authors><title>Instantly Obsoleting the Address-code Associations: A New Principle for
  Defending Advanced Code Reuse Attack</title><categories>cs.CR</categories><comments>23 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fine-grained Address Space Randomization has been considered as an effective
protection against code reuse attacks such as ROP/JOP. However, it only employs
a one-time randomization, and such a limitation has been exploited by recent
just-in-time ROP and side channel ROP, which collect gadgets on-the-fly and
dynamically compile them for malicious purposes. To defeat these advanced code
reuse attacks, we propose a new defense principle: instantly obsoleting the
address-code associations. We have initialized this principle with a novel
technique called virtual space page table remapping and implemented the
technique in a system CHAMELEON. CHAMELEON periodically re-randomizes the
locations of code pages on-the-fly. A set of techniques are proposed to achieve
our goal, including iterative instrumentation that instruments a
to-be-protected binary program to generate a re-randomization compatible
binary, runtime virtual page shuffling, and function reordering and instruction
rearranging optimizations. We have tested CHAMELEON with over a hundred binary
programs. Our experiments show that CHAMELEON can defeat all of our tested
exploits by both preventing the exploit from gathering sufficient gadgets, and
blocking the gadgets execution. Regarding the interval of our re-randomization,
it is a parameter and can be set as short as 100ms, 10ms or 1ms. The experiment
results show that CHAMELEON introduces on average 11.1%, 12.1% and 12.9%
performance overhead for these parameters, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02796</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02796</id><created>2015-07-10</created><updated>2015-07-28</updated><authors><author><keyname>Song</keyname><forenames>Wentu</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author></authors><title>Locally Repairable Codes with Functional Repair and Multiple Erasure
  Tolerance</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of designing [n; k] linear codes for distributed
storage systems (DSS) that satisfy the (r, t)-Local Repair Property, where any
t'(&lt;=t) simultaneously failed nodes can be locally repaired, each with locality
r. The parameters n, k, r, t are positive integers such that r&lt;k&lt;n and t &lt;=
n-k. We consider the functional repair model and the sequential approach for
repairing multiple failed nodes. By functional repair, we mean that the packet
stored in each newcomer is not necessarily an exact copy of the lost data but a
symbol that keep the (r, t)-local repair property. By the sequential approach,
we mean that the t' newcomers are ordered in a proper sequence such that each
newcomer can be repaired from the live nodes and the newcomers that are ordered
before it. Such codes, which we refer to as (n, k, r, t)-functional locally
repairable codes (FLRC), are the most general class of LRCs and contain several
subclasses of LRCs reported in the literature.
  In this paper, we aim to optimize the storage overhead (equivalently, the
code rate) of FLRCs. We derive a lower bound on the code length n given t
belongs to {2,3} and any possible k, r. For t=2, our bound generalizes the rate
bound proved in [14]. For t=3, our bound improves the rate bound proved in
[10]. We also give some onstructions of exact LRCs for t belongs to {2,3} whose
length n achieves the bound of (n, k, r, t)-FLRC, which proves the tightness of
our bounds and also implies that there is no gap between the optimal code
length of functional LRCs and exact LRCs for certain sets of parameters.
Moreover, our constructions are over the binary field, hence are of interest in
practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02798</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02798</id><created>2015-07-10</created><authors><author><keyname>Maatouki</keyname><forenames>Ahmad</forenames></author><author><keyname>Szuba</keyname><forenames>Marek</forenames></author><author><keyname>Meyer</keyname><forenames>J&#xf6;rg</forenames></author><author><keyname>Streit</keyname><forenames>Achim</forenames></author></authors><title>A horizontally-scalable multiprocessing platform based on Node.js</title><categories>cs.DC</categories><comments>8 pages, 7 figures. Accepted for publication as a conference paper
  for the 13th IEEE International Symposium on Parallel and Distributed
  Processing with Applications (IEEE ISPA-15)</comments><journal-ref>CoRR abs/1507.02798 (2015). ISBN: 978-1-4673-7952-6/15</journal-ref><doi>10.1109/Trustcom-BigDataSe-ISPA.2015.618</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a scalable web-based platform called Node Scala which
allows to split and handle requests on a parallel distributed system according
to pre-defined use cases. We applied this platform to a client application that
visualizes climate data stored in a NoSQL database MongoDB. The design of Node
Scala leads to efficient usage of available computing resources in addition to
allowing the system to scale simply by adding new workers. Performance
evaluation of Node Scala demonstrated a gain of up to 74 % compared to the
state-of-the-art techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02799</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02799</id><created>2015-07-10</created><authors><author><keyname>Kortsarz</keyname><forenames>Guy</forenames></author><author><keyname>Nutov</keyname><forenames>Zeev</forenames></author></authors><title>A simplified 1.5-approximation algorithm for augmenting
  edge-connectivity of a graph from 1 to 2</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Tree Augmentation Problem (TAP) is: given a connected graph $G=(V,{\cal
E})$ and an edge set $E$ on $V$ find a minimum size subset of edges $F
\subseteq E$ such that $(V,{\cal E} \cup F)$ is $2$-edge-connected. In the
conference version \cite{EFKN-APPROX} was sketched a $1.5$-approximation
algorithm for the problem. Since a full proof was very complex and long, the
journal version was cut into two parts. In the first part \cite{EFKN-TALG} was
only proved ratio $1.8$. An attempt to simplify the second part produced an
error in \cite{EKN-IPL}. Here we give a correct, different, and self contained
proof of the ratio $1.5$, that is also substantially simpler and shorter than
the previous proofs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02800</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02800</id><created>2015-07-10</created><authors><author><keyname>Xian</keyname><forenames>Chuhua</forenames></author><author><keyname>Jin</keyname><forenames>Shuo</forenames></author><author><keyname>Wang</keyname><forenames>Charlie C. L.</forenames></author></authors><title>Meshfree C^2-Weighting for Shape Deformation</title><categories>cs.GR</categories><comments>10 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Handle-driven deformation based on linear blending is widely used in many
applications because of its merits in intuitiveness, efficiency and easiness of
implementation. We provide a meshfree method to compute the smooth weights of
linear blending for shape deformation. The C2-continuity of weighting is
guaranteed by the carefully formulated basis functions, with which the
computation of weights is in a closed-form. Criteria to ensure the quality of
deformation are preserved by the basis functions after decomposing the shape
domain according to the Voronoi diagram of handles. The cost of inserting a new
handle is only the time to evaluate the distances from the new handle to all
sample points in the space of deformation. Moreover, a virtual handle insertion
algorithm has been developed to allow users freely placing handles while
preserving the criteria on weights. Experimental examples for real-time 2D/3D
deformations are shown to demonstrate the effectiveness of this method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02801</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02801</id><created>2015-07-10</created><updated>2015-10-22</updated><authors><author><keyname>Kaya</keyname><forenames>Heysem</forenames></author><author><keyname>Salah</keyname><forenames>Albert Ali</forenames></author></authors><title>Adaptive Mixtures of Factor Analyzers</title><categories>stat.ML cs.IT cs.LG math.IT</categories><comments>Pre-print has 30 pages including the appendix and references. A
  MATLAB tool of the proposed method is available (see the conclusions section)</comments><acm-class>G.3; I.5.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A mixture of factor analyzers is a semi-parametric density estimator that
generalizes the well-known mixtures of Gaussians model by allowing each
Gaussian in the mixture to be represented in a different lower-dimensional
manifold. This paper presents a robust and parsimonious model selection
algorithm for training a mixture of factor analyzers, carrying out simultaneous
clustering and locally linear, globally nonlinear dimensionality reduction.
Permitting different number of factors per mixture component, the algorithm
adapts the model complexity to the data complexity. We compare the proposed
algorithm with related automatic model selection algorithms on a number of
benchmarks. The results indicate the effectiveness of this fast and robust
approach in clustering, manifold learning and class-conditional modeling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02805</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02805</id><created>2015-07-10</created><authors><author><keyname>M&#xfc;hlenthaler</keyname><forenames>Moritz</forenames></author><author><keyname>Wanka</keyname><forenames>Rolf</forenames></author></authors><title>On the Connectedness of Clash-free Timetables</title><categories>cs.DM cs.DS</categories><msc-class>68T20 (primary), 68R10, 68W05 (secondary)</msc-class><acm-class>I.2.8; F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the connectedness of clash-free timetables with respect to the
Kempe-exchange operation. This investigation is related to the connectedness of
the search space of timetabling problem instances, which is a desirable
property, for example for two-step algorithms using the Kempe-exchange during
the optimization step. The theoretical framework for our investigations is
based on the study of reconfiguration graphs, which model the search space of
timetabling problems. We contribute to this framework by including timeslot
availability requirements in the analysis and we derive improved conditions for
the connectedness of clash-free timetables in this setting. We apply the
theoretical insights to establish the connectedness of clash-free timetables
for a number of benchmark instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02808</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02808</id><created>2015-07-10</created><authors><author><keyname>Angel</keyname><forenames>Eric</forenames></author><author><keyname>Bampis</keyname><forenames>Evripidis</forenames></author><author><keyname>Chau</keyname><forenames>Vincent</forenames></author><author><keyname>Zissimopoulos</keyname><forenames>Vassilis</forenames></author></authors><title>Minimizing Total Calibration Cost</title><categories>cs.DS</categories><comments>10 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bender et al. (SPAA 2013) have proposed a theoretical framework for testing
in contexts where safety mistakes must be avoided. Testing in such a context is
made by machines that need to be often calibrated. Given that calibration
costs, it is important to study policies minimizing the calibration cost while
performing all the necessary tests. We focus on the single-machine setting and
we extend the model proposed by Bender et al. by considering that the jobs have
arbitrary processing times and that the preemption of jobs is allowed. For this
case, we propose an optimal polynomial time algorithm. Then, we study the case
where there are several types of calibrations with different lengths and costs.
We first prove that the problem becomes NP-hard for arbitrary processing times
even when the preemption of the jobs is allowed. Finally, we focus on the case
of unit-time jobs and we show that a more general problem, where the
recalibration of the machine is not instantaneous but takes time, can be solved
in polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02815</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02815</id><created>2015-07-10</created><authors><author><keyname>Axenovich</keyname><forenames>Maria</forenames></author><author><keyname>Ueckerdt</keyname><forenames>Torsten</forenames></author><author><keyname>Weiner</keyname><forenames>Pascal</forenames></author></authors><title>Splitting Planar Graphs of Girth 6 into Two Linear Forests with Short
  Paths</title><categories>math.CO cs.DM</categories><msc-class>05C10, 05C15, 05C38, 05C70</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, Borodin, Kostochka, and Yancey (On $1$-improper $2$-coloring of
sparse graphs. Discrete Mathematics, 313(22), 2013) showed that the vertices of
each planar graph of girth at least $7$ can be $2$-colored so that each color
class induces a subgraph of a matching. We prove that any planar graph of girth
at least $6$ admits a vertex coloring in $2$ colors such that each
monochromatic component is a path of length at most $14$. Moreover, we show a
list version of this result. On the other hand, for each positive integer
$t\geq 3$, we construct a planar graph of girth $4$ such that in any coloring
of vertices in $2$ colors there is a monochromatic path of length at least $t$.
It remains open whether each planar graph of girth $5$ admits a $2$-coloring
with no long monochromatic paths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02821</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02821</id><created>2015-07-10</created><authors><author><keyname>Studer</keyname><forenames>Christoph</forenames></author></authors><title>Recovery of Signals with Low Density</title><categories>cs.IT math.IT</categories><comments>Submitted for possible publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse signals (i.e., vectors with a small number of non-zero entries) build
the foundation of most kernel (or nullspace) results, uncertainty relations,
and recovery guarantees in the sparse signal processing and compressive sensing
literature. In this paper, we introduce a novel signal-density measure that
extends the common notion of sparsity to non-sparse signals whose entries'
magnitudes decay rapidly. By taking into account such magnitude information, we
derive a more general and less restrictive kernel result and uncertainty
relation. Furthermore, we demonstrate the effectiveness of the proposed
signal-density measure by showing that orthogonal matching pursuit (OMP)
provably recovers low-density signals with up to 2$\times$ more non-zero
coefficients compared to that guaranteed by standard results for sparse
signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02825</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02825</id><created>2015-07-10</created><updated>2015-07-16</updated><authors><author><keyname>Maglaras</keyname><forenames>Leandros A.</forenames></author><author><keyname>Jiang</keyname><forenames>Jianmin</forenames></author><author><keyname>Cruz</keyname><forenames>Tiago J.</forenames></author></authors><title>Combining ensemble methods and social network metrics for improving
  accuracy of OCSVM on intrusion detection in SCADA systems</title><categories>cs.CR</categories><comments>25 pages, 15 figures</comments><msc-class>68T05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern Supervisory Control and Data Acquisition SCADA systems used by the
electric utility industry to monitor and control electric power generation,
transmission and distribution are recognized today as critical components of
the electric power delivery infrastructure. SCADA systems are large, complex
and incorporate increasing numbers of widely distributed components. The
presence of a real time intrusion detection mechanism, which can cope with
di?erent types of attacks, is of great importance, in order to defend a system
against cyber attacks This defense mechanism must be distributed, cheap and
above all accurate, since false positive alarms, or mistakes regarding the
origin of the intrusion mean severe costs for the system. Recently an
integrated detection mechanism, namely IT-OCSVM was proposed, which is
distributed in a SCADA network as a part of a distributed intrusion detection
system (IDS), providing accurate data about the origin and the time of an
intrusion. In this paper we also analyze the architecture of the integrated
detection mechanism and we perform extensive simulations based on real cyber
attacks in a small SCADA testbed in order to evaluate the performance of the
proposed mechanism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02826</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02826</id><created>2015-07-10</created><authors><author><keyname>Karahanoglu</keyname><forenames>Nazim Burak</forenames></author><author><keyname>Erdogan</keyname><forenames>Hakan</forenames></author></authors><title>Comments On &quot;Multipath Matching Pursuit&quot; by Kwon, Wang and Shim</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Straightforward combination of tree search with matching pursuits, which was
suggested in 2001 by Cotter and Rao, and then later developed by some other
authors, has been revisited recently as multipath matching pursuit (MMP). In
this comment, we would like to point out some major issues regarding this
publication. First, the idea behind MMP is not novel, and the related
literature has not been properly referenced. MMP has not been compared to
closely related algorithms such as A* orthogonal matching pursuit (A*OMP). The
theoretical analyses do ignore the pruning strategies applied by the authors in
practice. All these issues have the potential to mislead the reader and lead to
misinterpretation of the results. With this short paper, we intend to clarify
the relation of MMP to existing literature in the area and compare its
performance with A*OMP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02835</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02835</id><created>2015-07-10</created><authors><author><keyname>Thakur</keyname><forenames>Chetan Singh</forenames></author><author><keyname>Wang</keyname><forenames>Runchun</forenames></author><author><keyname>Hamilton</keyname><forenames>Tara Julia</forenames></author><author><keyname>Tapson</keyname><forenames>Jonathan</forenames></author><author><keyname>van Schaik</keyname><forenames>Andre</forenames></author></authors><title>A Trainable Neuromorphic Integrated Circuit that Exploits Device
  Mismatch</title><categories>cs.NE</categories><comments>Submitted to TCAS-I</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Random device mismatch that arises as a result of scaling of the CMOS
(complementary metal-oxide semi-conductor) technology into the deep submicron
regime degrades the accuracy of analogue circuits. Methods to combat this
increase the complexity of design. We have developed a novel neuromorphic
system called a Trainable Analogue Block (TAB), which exploits device mismatch
as a means for random projections of the input to a higher dimensional space.
The TAB framework is inspired by the principles of neural population coding
operating in the biological nervous system. Three neuronal layers, namely
input, hidden, and output, constitute the TAB framework, with the number of
hidden layer neurons far exceeding the input layer neurons. Here, we present
measurement results of the first prototype TAB chip built using a 65nm process
technology and show its learning capability for various regression tasks. Our
TAB chip exploits inherent randomness and variability arising due to the
fabrication process to perform various learning tasks. Additionally, we
characterise each neuron and discuss the statistical variability of its tuning
curve that arises due to random device mismatch, a desirable property for the
learning capability of the TAB. We also discuss the effect of the number of
hidden neurons and the resolution of output weights on the accuracy of the
learning capability of the TAB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02839</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02839</id><created>2015-07-10</created><updated>2015-09-01</updated><authors><author><keyname>Bolotin</keyname><forenames>Yu. L.</forenames></author><author><keyname>Cherkaskiy</keyname><forenames>V. A.</forenames></author></authors><title>Principle of maximum force and holographic principle: two principles or
  one?</title><categories>gr-qc cs.IT math.IT</categories><comments>3 pages, minor corrections, text slightly extended, results and
  conclusions unchanged</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how the maximum force principle can be derived from the holographic
principle and vice versa, thus demonstrating equivalence of the two principles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02851</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02851</id><created>2015-07-10</created><authors><author><keyname>Zygmunt</keyname><forenames>Anna</forenames></author></authors><title>Role Identification of Social Networkers</title><categories>cs.SI physics.soc-ph</categories><journal-ref>Encyclopedia of Social Network Analysis and Mining, 1598--1606,
  2014</journal-ref><doi>10.1007/978-1-4614-6170-8_247</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A social network consists of a set of actors and a set of relationships
between them which describe certain patterns of communication. Most current
networks are huge and difficult to analyze and visualize. One of the methods
frequently used is to extract the most important features, namely to create a
certain abstraction, that is the transformation of a large network to a much
smaller one, so the latter is a useful summary of the original one, still
keeping the most important characteristics. In the case of a social network it
can be achieved in two ways. One is to find groups of actors and present only
them and relationships between them. The other is to find actors who play
similar roles and to construct a smaller network in which the connection
between the actors would be replaced with connections between the roles.
Classifying actors by the roles they are playing in the network can help to
understand 'who is who' in a social network. This classification can be very
useful, because it gives us a comprehensive view of the network and helps to
understand how the network is organized, and to predict how it could behave in
the case of certain events (internal or external).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02853</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02853</id><created>2015-07-10</created><updated>2016-01-28</updated><authors><author><keyname>Bille</keyname><forenames>Philip</forenames></author><author><keyname>Christiansen</keyname><forenames>Anders Roy</forenames></author><author><keyname>Cording</keyname><forenames>Patrick Hagge</forenames></author><author><keyname>G&#xf8;rtz</keyname><forenames>Inge Li</forenames></author></authors><title>Finger Search in Grammar-Compressed Strings</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Grammar-based compression, where one replaces a long string by a small
context-free grammar that generates the string, is a simple and powerful
paradigm that captures many popular compression schemes. Given a grammar, the
random access problem is to compactly represent the grammar while supporting
random access, that is, given a position in the original uncompressed string
report the character at that position. In this paper we study the random access
problem with the finger search property, that is, the time for a random access
query should depend on the distance between a specified index $f$, called the
\emph{finger}, and the query index $i$. We consider both a static variant,
where we first place a finger and subsequently access indices near the finger
efficiently, and a dynamic variant where also moving the finger such that the
time depends on the distance moved is supported.
  Let $n$ be the size the grammar, and let $N$ be the size of the string. For
the static variant we give a linear space representation that supports placing
the finger in $O(\log N)$ time and subsequently accessing in $O(\log D)$ time,
where $D$ is the distance between the finger and the accessed index. For the
dynamic variant we give a linear space representation that supports placing the
finger in $O(\log N)$ time and accessing and moving the finger in $O(\log D +
\log \log N)$ time. Compared to the best linear space solution to random
access, we improve a $O(\log N)$ query bound to $O(\log D)$ for the static
variant and to $O(\log D + \log \log N)$ for the dynamic variant, while
maintaining linear space. As an application of our results we obtain an
improved solution to the longest common extension problem in grammar compressed
strings. To obtain our results, we introduce several new techniques of
independent interest, including a novel van Emde Boas style decomposition of
grammars.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02860</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02860</id><created>2015-07-10</created><authors><author><keyname>Liu</keyname><forenames>Shengjun</forenames></author><author><keyname>Wang</keyname><forenames>Charlie C. L.</forenames></author><author><keyname>Brunnett</keyname><forenames>Guido</forenames></author><author><keyname>Wang</keyname><forenames>Jun</forenames></author></authors><title>A Closed-Form Formulation of HRBF-Based Surface Reconstruction</title><categories>cs.GR</categories><comments>13 pages with 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Hermite radial basis functions (HRBFs) implicits have been used to
reconstruct surfaces from scattered Hermite data points. In this work, we
propose a closed-form formulation to construct HRBF-based implicits by a
quasi-solution approximating the exact solution. A scheme is developed to
automatically adjust the support sizes of basis functions to hold the error
bound of a quasi-solution. Our method can generate an implicit function from
positions and normals of scattered points without taking any global operation.
Working together with an adaptive sampling algorithm, the HRBF-based implicits
can also reconstruct surfaces from point clouds with non-uniformity and noises.
Robust and efficient reconstruction has been observed in our experimental tests
on real data captured from a variety of scenes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02866</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02866</id><created>2015-07-10</created><updated>2015-12-04</updated><authors><author><keyname>Cechl&#xe1;rov&#xe1;</keyname><forenames>Katar&#xed;na</forenames></author><author><keyname>Eirinakis</keyname><forenames>Pavlos</forenames></author><author><keyname>Fleiner</keyname><forenames>Tam&#xe1;s</forenames></author><author><keyname>Magos</keyname><forenames>Dimitrios</forenames></author><author><keyname>Manlove</keyname><forenames>David</forenames></author><author><keyname>Mourtos</keyname><forenames>Ioannis</forenames></author><author><keyname>Ocel&#xe1;kov&#xe1;</keyname><forenames>Eva</forenames></author><author><keyname>Rastegari</keyname><forenames>Baharak</forenames></author></authors><title>Pareto Optimal Matchings in Many-to-Many Markets with Ties</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider Pareto-optimal matchings (POMs) in a many-to-many market of
applicants and courses where applicants have preferences, which may include
ties, over individual courses and lexicographic preferences over sets of
courses. Since this is the most general setting examined so far in the
literature, our work unifies and generalizes several known results.
Specifically, we characterize POMs and introduce the \emph{Generalized Serial
Dictatorship Mechanism with Ties (GSDT)} that effectively handles ties via
properties of network flows. We show that GSDT can generate all POMs using
different priority orderings over the applicants, but it satisfies truthfulness
only for certain such orderings. This shortcoming is not specific to our
mechanism; we show that any mechanism generating all POMs in our setting is
prone to strategic manipulation. This is in contrast to the one-to-one case
(with or without ties), for which truthful mechanisms generating all POMs do
exist.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02870</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02870</id><created>2015-07-10</created><authors><author><keyname>Raza</keyname><forenames>Khalid</forenames></author></authors><title>Analysis of Microarray Data using Artificial Intelligence Based
  Techniques</title><categories>cs.AI cs.CE q-bio.GN</categories><comments>32 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Microarray is one of the essential technologies used by the biologist to
measure genome-wide expression levels of genes in a particular organism under
some particular conditions or stimuli. As microarrays technologies have become
more prevalent, the challenges of analyzing these data for getting better
insight about biological processes have essentially increased. Due to
availability of artificial intelligence based sophisticated computational
techniques, such as artificial neural networks, fuzzy logic, genetic
algorithms, and many other nature-inspired algorithms, it is possible to
analyse microarray gene expression data in more better way. Here, we reviewed
artificial intelligence based techniques for the analysis of microarray gene
expression data. Further, challenges in the field and future work direction
have also been suggested.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02873</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02873</id><created>2015-07-10</created><authors><author><keyname>Renkens</keyname><forenames>Joris</forenames></author><author><keyname>Kimmig</keyname><forenames>Angelika</forenames></author><author><keyname>De Raedt</keyname><forenames>Luc</forenames></author></authors><title>Lazy Explanation-Based Approximation for Probabilistic Logic Programming</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a lazy approach to the explanation-based approximation of
probabilistic logic programs. It uses only the most significant part of the
program when searching for explanations. The result is a fast and anytime
approximate inference algorithm which returns hard lower and upper bounds on
the exact probability. We experimentally show that this method outperforms
state-of-the-art approximate inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02874</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02874</id><created>2015-07-10</created><authors><author><keyname>Mukherjee</keyname><forenames>Manuj</forenames></author><author><keyname>Kashyap</keyname><forenames>Navin</forenames></author><author><keyname>Sankarasubramaniam</keyname><forenames>Yogesh</forenames></author></authors><title>On the Public Communication Needed to Achieve SK Capacity in the
  Multiterminal Source Model</title><categories>cs.IT math.IT</categories><comments>Submitted to the IEEE Transactions on Information Theory. arXiv admin
  note: text overlap with arXiv:1504.00629</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The focus of this paper is on the public communication required for
generating a maximal-rate secret key (SK) within the multiterminal source model
of Csisz{\'a}r and Narayan. Building on the prior work of Tyagi for the
two-terminal scenario, we derive a lower bound on the communication complexity,
$R_{\text{SK}}$, defined to be the minimum rate of public communication needed
to generate a maximal-rate SK. It is well known that the minimum rate of
communication for omniscience, denoted by $R_{\text{CO}}$, is an upper bound on
$R_{\text{SK}}$. For the class of pairwise independent network (PIN) models
defined on uniform hypergraphs, we show that a certain &quot;Type $\mathcal{S}$&quot;
condition, which is verifiable in polynomial time, guarantees that our lower
bound on $R_{\text{SK}}$ meets the $R_{\text{CO}}$ upper bound. Thus, PIN
models satisfying our condition are $R_{\text{SK}}$-maximal, meaning that the
upper bound $R_{\text{SK}} \le R_{\text{CO}}$ holds with equality. This allows
us to explicitly evaluate $R_{\text{SK}}$ for such PIN models. We also give
several examples of PIN models that satisfy our Type $\mathcal S$ condition.
Finally, we prove that for an arbitrary multiterminal source model, a stricter
version of our Type $\mathcal S$ condition implies that communication from
\emph{all} terminals (&quot;omnivocality&quot;) is needed for establishing a SK of
maximum rate. For three-terminal source models, the converse is also true:
omnivocality is needed for generating a maximal-rate SK only if the strict Type
$\mathcal S$ condition is satisfied. Counterexamples exist that show that the
converse is not true in general for source models with four or more terminals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02876</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02876</id><created>2015-07-10</created><updated>2015-07-31</updated><authors><author><keyname>Butkova</keyname><forenames>Yuliya</forenames></author><author><keyname>Hatefi</keyname><forenames>Hassan</forenames></author><author><keyname>Hermanns</keyname><forenames>Holger</forenames></author><author><keyname>Krcal</keyname><forenames>Jan</forenames></author></authors><title>Optimal Continuous Time Markov Decisions</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of Markov decision processes running in continuous time, one
of the most intriguing challenges is the efficient approximation of finite
horizon reachability objectives. A multitude of sophisticated model checking
algorithms have been proposed for this. However, no proper benchmarking has
been performed thus far.
  This paper presents a novel and yet simple solution: an algorithm originally
developed for a restricted subclass of models and a subclass of schedulers can
be twisted so as to become competitive with the more sophisticated algorithms
in full generality. As the second main contribution, we perform a comparative
evaluation of the core algorithmic concepts on an extensive set of benchmarks
varying over all key parameters: model size, amount of non-determinism, time
horizon, and precision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02878</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02878</id><created>2015-07-10</created><updated>2015-11-04</updated><authors><author><keyname>Hempel</keyname><forenames>Andreas B.</forenames></author><author><keyname>Goulart</keyname><forenames>Paul</forenames></author><author><keyname>Lygeros</keyname><forenames>John</forenames></author></authors><title>Strong Stationarity Conditions for Optimal Control of Hybrid Systems</title><categories>math.OC cs.SY</categories><comments>28 pages, 3 figures; extensive rewrite from previous version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present necessary and sufficient optimality conditions for finite time
optimal control problems for a class of hybrid systems described by linear
complementarity models. Although these optimal control problems are difficult
in general due to the presence of complementarity constraints, we provide a set
of structural assumptions ensuring that the tangent cone of the constraints
possesses geometric regularity properties. These imply that the classical
Karush-Kuhn-Tucker conditions of nonlinear programming theory are both
necessary and sufficient for local optimality, which is not the case for
general mathematical programs with complementarity constraints. We also present
sufficient conditions for global optimality.
  We proceed to show that every continuous piecewise affine system can be
written as an optimizing process which results in a linear complementarity
model satisfying our structural assumptions. Hence, our stationarity results
apply to a large class of hybrid systems with piecewise affine dynamics. We
present simulation results illustrating the potentially substantial benefits
possible from using a nonlinear programming approach to the optimal control
problem with complementarity constraints instead of a more traditional
mixed-integer formulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02879</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02879</id><created>2015-07-10</created><authors><author><keyname>Sarfraz</keyname><forenames>M. Saquib</forenames></author><author><keyname>Stiefelhagen</keyname><forenames>Rainer</forenames></author></authors><title>Deep Perceptual Mapping for Thermal to Visible Face Recognition</title><categories>cs.CV</categories><comments>BMVC 2015 (oral)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cross modal face matching between the thermal and visible spectrum is a much
de- sired capability for night-time surveillance and security applications. Due
to a very large modality gap, thermal-to-visible face recognition is one of the
most challenging face matching problem. In this paper, we present an approach
to bridge this modality gap by a significant margin. Our approach captures the
highly non-linear relationship be- tween the two modalities by using a deep
neural network. Our model attempts to learn a non-linear mapping from visible
to thermal spectrum while preserving the identity in- formation. We show
substantive performance improvement on a difficult thermal-visible face
dataset. The presented approach improves the state-of-the-art by more than 10%
in terms of Rank-1 identification and bridge the drop in performance due to the
modality gap by more than 40%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02890</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02890</id><created>2015-07-10</created><updated>2015-10-14</updated><authors><author><keyname>Nicolas</keyname><forenames>Bedon</forenames><affiliation>University of Rouen</affiliation></author></authors><title>Logic and Branching Automata</title><categories>cs.FL cs.LO</categories><proxy>LMCS</proxy><journal-ref>LMCS 11 (4:2) 2015</journal-ref><doi>10.2168/LMCS-11(4:2)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the logical aspects of branching automata, as defined
by Lodaya and Weil. We first prove that the class of languages of finite N-free
posets recognized by branching automata is closed under complementation. Then
we define a logic, named P-MSO as it is a extension of monadic second-order
logic with Presburger arithmetic, and show that it is precisely as expressive
as branching automata. As a consequence of the effectiveness of the
construction of one formalism from the other, the P-MSO theory of the class of
all finite N-free posets is decidable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02907</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02907</id><created>2015-07-10</created><authors><author><keyname>Marujo</keyname><forenames>Lu&#xed;s</forenames></author><author><keyname>Ribeiro</keyname><forenames>Ricardo</forenames></author><author><keyname>de Matos</keyname><forenames>David Martins</forenames></author><author><keyname>Neto</keyname><forenames>Jo&#xe3;o P.</forenames></author><author><keyname>Gershman</keyname><forenames>Anatole</forenames></author><author><keyname>Carbonell</keyname><forenames>Jaime</forenames></author></authors><title>Extending a Single-Document Summarizer to Multi-Document: a Hierarchical
  Approach</title><categories>cs.IR cs.CL</categories><comments>6 pages, Please cite: Proceedings of *SEM: the 4th Joint Conference
  on Lexical and Computational Semantics (bibtex:
  http://aclweb.org/anthology/S/S15/S15-1020.bib)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing amount of online content motivated the development of
multi-document summarization methods. In this work, we explore straightforward
approaches to extend single-document summarization methods to multi-document
summarization. The proposed methods are based on the hierarchical combination
of single-document summaries, and achieves state of the art results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02908</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02908</id><created>2015-07-10</created><authors><author><keyname>Drees</keyname><forenames>Maximilian</forenames></author><author><keyname>Feldotto</keyname><forenames>Matthias</forenames></author><author><keyname>Riechers</keyname><forenames>S&#xf6;ren</forenames></author><author><keyname>Skopalik</keyname><forenames>Alexander</forenames></author></authors><title>On Existence and Properties of Approximate Pure Nash Equilibria in
  Bandwidth Allocation Games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In \emph{bandwidth allocation games} (BAGs), the strategy of a player
consists of various demands on different resources. The player's utility is at
most the sum of these demands, provided they are fully satisfied. Every
resource has a limited capacity and if it is exceeded by the total demand, it
has to be split between the players. Since these games generally do not have
pure Nash equilibria, we consider approximate pure Nash equilibria, in which no
player can improve her utility by more than some fixed factor $\alpha$ through
unilateral strategy changes. There is a threshold $\alpha_\delta$ (where
$\delta$ is a parameter that limits the demand of each player on a specific
resource) such that $\alpha$-approximate pure Nash equilibria always exist for
$\alpha \geq \alpha_\delta$, but not for $\alpha &lt; \alpha_\delta$. We give both
upper and lower bounds on this threshold $\alpha_\delta$ and show that the
corresponding decision problem is ${\sf NP}$-hard. We also show that the
$\alpha$-approximate price of anarchy for BAGs is $\alpha+1$. For a restricted
version of the game, where demands of players only differ slightly from each
other (e.g. symmetric games), we show that approximate Nash equilibria can be
reached (and thus also be computed) in polynomial time using the best-response
dynamic. Finally, we show that a broader class of utility-maximization games
(which includes BAGs) converges quickly towards states whose social welfare is
close to the optimum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02912</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02912</id><created>2015-07-10</created><updated>2015-07-13</updated><authors><author><keyname>Cussens</keyname><forenames>James</forenames></author></authors><title>First-order integer programming for MAP problems</title><categories>cs.AI</categories><comments>corrected typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding the most probable (MAP) model in SRL frameworks such as Markov logic
and Problog can, in principle, be solved by encoding the problem as a
`grounded-out' mixed integer program (MIP). However, useful first-order
structure disappears in this process motivating the development of first-order
MIP approaches. Here we present mfoilp, one such approach. Since the syntax and
semantics of mfoilp is essentially the same as existing approaches we focus
here mainly on implementation and algorithmic issues. We start with the
(conceptually) simple problem of using a logic program to generate a MIP
instance before considering more ambitious exploitation of first-order
representations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02921</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02921</id><created>2015-07-10</created><authors><author><keyname>Das</keyname><forenames>Rajib Lochan</forenames></author><author><keyname>Chackraborty</keyname><forenames>Mrityunjoy</forenames></author></authors><title>Zero Attracting PNLMS Algorithm and Its Convergence in Mean</title><categories>cs.IT math.IT</categories><comments>11 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The proportionate normalized least mean square (PNLMS) algorithm and its
variants are by far the most popular adaptive filters that are used to identify
sparse systems. The convergence speed of the PNLMS algorithm, though very high
initially, however, slows down at a later stage, even becoming worse than
sparsity agnostic adaptive filters like the NLMS. In this paper, we address
this problem by introducing a carefully constructed l1 norm (of the
coefficients) penalty in the PNLMS cost function which favors sparsity. This
results in certain zero attractor terms in the PNLMS weight update equation
which help in the shrinkage of the coefficients, especially the inactive taps,
thereby arresting the slowing down of convergence and also producing lesser
steady state excess mean square error (EMSE). We also carry out the convergence
analysis (in mean) of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02929</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02929</id><created>2015-07-10</created><authors><author><keyname>Birch</keyname><forenames>Jenna</forenames></author><author><keyname>Pantelous</keyname><forenames>Athanasios A.</forenames></author><author><keyname>Zuev</keyname><forenames>Konstantin</forenames></author></authors><title>The Maximum Number of 3- and 4-Cliques within a Planar Maximally
  Filtered Graph</title><categories>math-ph cs.DS math.MP</categories><comments>12 pages, 11 figures</comments><journal-ref>Physica A 417 (2015) 221-229</journal-ref><doi>10.1016/j.physa.2014.09.011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Planar Maximally Filtered Graphs (PMFG) are an important tool for filtering
the most relevant information from correlation based networks such as stock
market networks. One of the main characteristics of a PMFG is the number of its
3- and 4-cliques. Recently in a few high impact papers it was stated that,
based on heuristic evidence, the maximum number of 3- and 4-cliques that can
exist in a PMFG with n vertices is 3n - 8 and n - 4 respectively. In this
paper, we prove that this is indeed the case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02931</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02931</id><created>2015-07-10</created><updated>2015-07-19</updated><authors><author><keyname>Goswami</keyname><forenames>Mayank</forenames></author><author><keyname>Li</keyname><forenames>Siming</forenames></author><author><keyname>Zhang</keyname><forenames>Junwei</forenames></author><author><keyname>Saucan</keyname><forenames>Emil</forenames></author><author><keyname>Gu</keyname><forenames>David Xianfeng</forenames></author><author><keyname>Gao</keyname><forenames>Jie</forenames></author></authors><title>Space Filling Curves for 3D Sensor Networks with Complex Topology</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several aspects of managing a sensor network (e.g., motion planning for data
mules, serial data fusion and inference) benefit once the network is linearized
to a path. The linearization is often achieved by constructing a space filling
curve in the domain. However, existing methods cannot handle networks
distributed on surfaces of complex topology. This paper presents a novel method
for generating space filling curves for 3D sensor networks that are distributed
densely on some two-dimensional geometric surface. Our algorithm is completely
distributed and constructs a path which gets uniformly, progressively denser as
it becomes longer. We analyze the algorithm mathematically and prove that the
curve we obtain is dense. Our method is based on the Hodge decomposition
theorem and uses holomorphic differentials on Riemann surfaces. The underlying
high genus surface is conformally mapped to a union of flat tori and then a
proportionally-dense space filling curve on this union is constructed. The
pullback of this curve to the original network gives us the desired curve.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02951</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02951</id><created>2015-07-10</created><authors><author><keyname>S&#xe1;nchez</keyname><forenames>Jos&#xe9;</forenames></author><author><keyname>Yahia</keyname><forenames>Imen Grida Ben</forenames></author><author><keyname>Crespi</keyname><forenames>No&#xeb;l</forenames></author><author><keyname>Rasheed</keyname><forenames>Tinku</forenames></author><author><keyname>Siracusa</keyname><forenames>Domenico</forenames></author></authors><title>Softwarized 5G Networks Resiliency with Selfhealing</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The meaning of 5G is still a subject of discussion in the industry. However,
the softwarization of networks is expected to shape the design, operation and
management of 5G networks. The opportunity is then crucial for Telcos, vendors
and IT players to consider the management of 5G networks during its design time
and avoid the build it first, manage it later paradigm. However, network
softwarization comes with its own set of challenges, including robustness,
scalability and resilience. In this paper, we focus on the importance of
resiliency and propose a Self-Healing based framework for 5G networks to ensure
services and resources availability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02952</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02952</id><created>2015-07-10</created><authors><author><keyname>S&#xe1;nchez</keyname><forenames>Jos&#xe9;</forenames></author><author><keyname>Yahia</keyname><forenames>Imen Grida Ben</forenames></author><author><keyname>Crespi</keyname><forenames>No&#xeb;l</forenames></author></authors><title>POSTER: Self-Healing Mechanisms for Software-Defined Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Operators perceive programmable networks brought by Software Defined Networks
(SDN) as cornerstone to decrease the time to deploy new services, to augment
the flexibility and to adapt network resources to customer needs at runtime.
However, despite the vulnerabilities identified due that the intelligence is
centralized on SDN, its research is more centered on forwarding traffic and
reconfiguration issues, not considering to a great extent the fault management
aspects of the control plane. This paper provides SDN with fault management
capabilities by using autonomic principles like self-healing mechanisms. We
propose a generic self-healing approach that relies on a Bayesian Networks for
the diagnosis block and we applied this algorithm to a centralized SDN
infrastructure to prove its functioning in the presence of faults.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02954</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02954</id><created>2015-07-10</created><authors><author><keyname>Hansen</keyname><forenames>Thomas L.</forenames></author><author><keyname>J&#xf8;rgensen</keyname><forenames>Peter B.</forenames></author><author><keyname>Badiu</keyname><forenames>Mihai-Alin</forenames></author><author><keyname>Fleury</keyname><forenames>Bernard H.</forenames></author></authors><title>Joint Sparse Channel Estimation and Decoding: Continuous and Discrete
  Domain Sparsity</title><categories>cs.IT math.IT stat.AP</categories><comments>13 pages, submitted to IEEE Trans. Signal Process</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent progress in wireless receiver design has been towards iterative
processing, where channel estimation and decoding is considered a joint
optimization problem. Sparse channel estimation is another recent advancement,
which exploits the inherent structure of those wireless channels that are
composed of a small number of multipath components. In this work we design
iterative receivers which incorporate sparse channel estimation.
State-of-the-art sparse channel estimators simplify the estimation problem to
be a finite basis selection problem by restricting the multipath delays to the
discrete domain (i.e. to a grid). Our main contribution is a receiver without
such a restriction; the delays are estimated directly as continuous values. As
a result, our receiver does not suffer from the leakage effect which destroys
sparsity when the delays are restricted to the discrete domain. We discuss
certain connections between continuous and discrete domain sparse estimation
methods. Our receivers outperform state-of-the-art sparse channel estimation
iterative receivers in terms of bit error rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02955</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02955</id><created>2015-07-10</created><authors><author><keyname>Ikenmeyer</keyname><forenames>Christian</forenames></author><author><keyname>Mulmuley</keyname><forenames>Ketan D.</forenames></author><author><keyname>Walter</keyname><forenames>Michael</forenames></author></authors><title>On vanishing of Kronecker coefficients</title><categories>cs.CC math.RT</categories><comments>23 pages</comments><msc-class>20G05</msc-class><acm-class>F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is shown that:
  (1) The problem of deciding positivity of Kronecker coefficients is NP-hard.
  (2) There exists a positive ($\# P$)-formula for a subclass of Kronecker
coefficients whose positivity is NP-hard to decide.
  (3) For any $0 &lt; \epsilon \le 1$, there exists $0&lt;a&lt;1$ such that, for all
$m$, there exist $\Omega(2^{m^a})$ partition triples $(\lambda,\mu,\mu)$ in the
Kronecker cone such that: (a) the Kronecker coefficient $k^\lambda_{\mu,\mu}$
is zero, (b) the height of $\mu$ is $m$, (c) the height of $\lambda$ is $\le
m^\epsilon$, and (d) $|\lambda|= |\mu| \le m^3$.
  The last result takes a step towards proving the existence of
occurrence-based representation-theoretic obstructions in the context of the
GCT approach to the permanent vs. determinant problem. Its proof also
illustrates the effectiveness of the explicit proof strategy of GCT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02964</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02964</id><created>2015-07-09</created><authors><author><keyname>Hassan</keyname><forenames>S. Sarif</forenames></author></authors><title>Dynamics of Delay Logistic Difference Equation in the Complex Plane</title><categories>math.DS cs.DM</categories><comments>Primary Draft</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  The dynamics of the delay logistic equation with complex parameters and
arbitrary complex initial conditions is investigated. The analysis of the local
stability of this difference equation has been carried out. We further exhibit
several interesting characteristics of the solutions of this equation, using
computations, which does not arise when we consider the same equation with
positive real parameters and initial conditions. Some of the interesting
observations led us to pose some open problems and conjectures regarding
chaotic and higher order periodic solutions and global asymptotic convergence
of the delay logistic equation. It is our hope that these observations of this
complex difference equation would certainly be an interesting addition to the
present art of research in rational difference equations in understanding the
behaviour in the complex domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02973</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02973</id><created>2015-07-10</created><authors><author><keyname>Beykikhoshk</keyname><forenames>Adham</forenames></author><author><keyname>Arandjelovic</keyname><forenames>Ognjen</forenames></author><author><keyname>Phung</keyname><forenames>Dinh</forenames></author><author><keyname>Venkatesh</keyname><forenames>Svetha</forenames></author></authors><title>Overcoming data scarcity of Twitter: using tweets as bootstrap with
  application to autism-related topic content analysis</title><categories>cs.SI</categories><comments>IEEE/ACM International Conference on Advances in Social Networks
  Analysis and Mining, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Notwithstanding recent work which has demonstrated the potential of using
Twitter messages for content-specific data mining and analysis, the depth of
such analysis is inherently limited by the scarcity of data imposed by the 140
character tweet limit. In this paper we describe a novel approach for targeted
knowledge exploration which uses tweet content analysis as a preliminary step.
This step is used to bootstrap more sophisticated data collection from directly
related but much richer content sources. In particular we demonstrate that
valuable information can be collected by following URLs included in tweets. We
automatically extract content from the corresponding web pages and treating
each web page as a document linked to the original tweet show how a temporal
topic model based on a hierarchical Dirichlet process can be used to track the
evolution of a complex topic structure of a Twitter community. Using
autism-related tweets we demonstrate that our method is capable of capturing a
much more meaningful picture of information exchange than user-chosen hashtags.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02980</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02980</id><created>2015-07-07</created><authors><author><keyname>Di Costanzo</keyname><forenames>Ezio</forenames></author><author><keyname>Natalini</keyname><forenames>Roberto</forenames></author></authors><title>A hybrid mathematical model of collective motion under alignment and
  chemotaxis</title><categories>math.CA cs.SY math.DS math.OC q-bio.CB</categories><msc-class>82C22, 34D05, 92C17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose and study a hybrid discrete in continuous
mathematical model of collective motion under alignment and chemotaxis effect.
Starting from the paper by Di Costanzo et al (2015), in which the Cucker-Smale
model (Cucker and Smale, 2007) was coupled with other cell mechanisms, to
describe the cell migration and self-organization in the zebrafish lateral line
primordium, we introduce a simplified model in which the coupling between an
alignment and chemotaxis mechanism acts on a system of interacting particles.
In particular we rely on a hybrid description in which the agents are discrete
entities, while the chemoattractant is considered as a continuous signal. The
proposed model is then studied both from an analytical and a numerical point of
view. From the analytic point of view we prove, globally in time, existence and
uniqueness of the solution. Then, the asymptotic behaviour of the system is
investigated. Through a suitable Lyapunov functional we show that, for the
linearized version of our system, for $t\rightarrow +\infty$, the migrating
aggregate exponentially converge to a state in which all the particles have a
same position with zero velocity. Finally, from the numerical point of view,
some meaningful dynamical tests are proposed to simulate the behaviour of the
system, also in comparison with the analytical findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02987</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02987</id><created>2015-07-10</created><authors><author><keyname>Albrecht</keyname><forenames>Felipe</forenames></author></authors><title>Genoogle: an indexed and parallelized search engine for similar DNA
  sequences</title><categories>cs.DC cs.CE cs.IR q-bio.GN</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The search for similar genetic sequences is one of the main bioinformatics
tasks. The genetic sequences data banks are growing exponentially and the
searching techniques that use linear time are not capable to do the search in
the required time anymore. Another problem is that the clock speed of the
modern processors are not growing as it did before, instead, the processing
capacity is growing with the addiction of more processing cores and the
techniques which does not use parallel computing does not have benefits from
these extra cores. This work aims to use data indexing techniques to reduce the
searching process computation cost united with the parallelization of the
searching techniques to use the computational capacity of the multi core
processors. To verify the viability of using these two techniques
simultaneously, a software which uses parallelization techniques with inverted
indexes was developed.
  Experiments were executed to analyze the performance gain when parallelism is
utilized, the search time gain, and also the quality of the results when it
compared with others searching tools. The results of these experiments were
promising, the parallelism gain overcame the expected speedup, the searching
time was 20 times faster than the parallelized NCBI BLAST, and the searching
results showed a good quality when compared with this tool.
  The software source code is available at
https://github.com/felipealbrecht/Genoogle .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02988</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02988</id><created>2015-07-10</created><updated>2015-11-20</updated><authors><author><keyname>Chugh</keyname><forenames>Ravi</forenames></author><author><keyname>Hempel</keyname><forenames>Brian</forenames></author><author><keyname>Spradlin</keyname><forenames>Mitchell</forenames></author><author><keyname>Albers</keyname><forenames>Jacob</forenames></author></authors><title>Programmatic and Direct Manipulation, Together at Last</title><categories>cs.PL</categories><acm-class>D.3.3; F.3.2; H.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the Sketch-n-Sketch editor for Scalable Vector Graphics (SVG) that
integrates programmatic and direct manipulation, two modes of interaction with
complementary strengths. In Sketch-n-Sketch, the user writes a program to
generate an output SVG canvas. Then the user may directly manipulate the canvas
while the system infers real-time updates to the program in order to match the
changes to the output. To achieve this, we propose (i) a technique called
trace-based program synthesis that takes program execution history into account
in order to constrain the search space and (ii) heuristics for dealing with
ambiguities. Based on our experience writing more than 40 examples and from the
results of a study with 25 participants, we conclude that Sketch-n-Sketch
provides a novel and effective workflow between the boundaries of existing
programmatic and direct manipulation systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02989</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02989</id><created>2015-07-10</created><authors><author><keyname>Grabowski</keyname><forenames>Szymon</forenames></author><author><keyname>Susik</keyname><forenames>Robert</forenames></author><author><keyname>Raniszewski</keyname><forenames>Marcin</forenames></author></authors><title>A Bloom filter based semi-index on $q$-grams</title><categories>cs.DS</categories><msc-class>68W32</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a simple $q$-gram based semi-index, which allows to look for a
pattern typically only in a small fraction of text blocks. Several space-time
tradeoffs are presented. Experiments on Pizza &amp; Chili datasets show that our
solution is up to three orders of magnitude faster than the Claude et al.
\cite{CNPSTjda10} semi-index at a comparable space usage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02992</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02992</id><created>2015-07-10</created><authors><author><keyname>Pohl</keyname><forenames>Christoph</forenames></author><author><keyname>Hof</keyname><forenames>Hans-Joachim</forenames></author></authors><title>Secure Scrum: Development of Secure Software with Scrum</title><categories>cs.CR cs.SE</categories><comments>The Ninth International Conference on Emerging Security Information,
  Systems and Technologies - SECURWARE 2015, Venice, Italy, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, the use of agile software development methods like Scrum is common
in industry and academia. Considering the current attacking landscape, it is
clear that developing secure software should be a main concern in all software
development projects. In traditional software projects, security issues require
detailed planning in an initial planning phase, typically resulting in a
detailed security analysis (e.g., threat and risk analysis), a security
architecture, and instructions for security implementation (e.g., specification
of key sizes and cryptographic algorithms to use). Agile software development
methods like Scrum are known for reducing the initial planning phases (e.g.,
sprint 0 in Scrum) and for focusing more on producing running code. Scrum is
also known for allowing fast adaption of the emerging software to changes of
customer wishes. For security, this means that it is likely that there are no
detailed security architecture or security implementation instructions from the
start of the project. It also means that a lot of design decisions will be made
during the runtime of the project. Hence, to address security in Scrum, it is
necessary to consider security issues throughout the whole software development
process. Secure Scrum is a variation of the Scrum framework with special focus
on the development of secure software throughout the whole software development
process. It puts emphasis on implementation of security related issues without
the need of changing the underlying Scrum process or influencing team dynamics.
Secure Scrum allows even non- security experts to spot security issues, to
implement security features, and to verify implementations. A field test of
Secure Scrum shows that the security level of software developed using Secure
Scrum is higher then the security level of software developed using standard
Scrum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02994</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02994</id><created>2015-07-10</created><authors><author><keyname>Quan-Haase</keyname><forenames>Anabel</forenames></author><author><keyname>Martin</keyname><forenames>Kim</forenames></author><author><keyname>McCay-Peet</keyname><forenames>Lori</forenames></author></authors><title>Networks of digital humanities scholars: The informational and social
  uses and gratifications of Twitter</title><categories>cs.CY cs.SI physics.soc-ph</categories><comments>Quan-Haase, A., Martin, K., &amp; McCay-Peet, L. (2015). Networks of
  digital humanities scholars: The informational and social uses and
  gratifications of Twitter. Big Data &amp; Society, January-June, pp. 1-12</comments><doi>10.1177/2053951715589417</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Big data research is currently split on whether and to what extent Twitter
can be characterised as an informational or social network. We contribute to
this line of inquiry through an investigation of digital humanities scholars'
uses and gratifications of Twitter. We conducted a thematic analysis of 25
semistructured interview transcripts to learn about these scholars'
professional use of Twitter. Our findings show that Twitter is considered a
critical tool for informal communication within DH invisible colleges,
functioning at varying levels as both an informational network (learning to
'Twitter' and maintaining awareness) and a social network (imagining audiences
and engaging other digital humanists). We find that Twitter follow
relationships reflect common academic interests and are closely tied to
scholars' preexisting social ties and conference or event co-attendance. The
concept of the invisible college continues to be relevant but requires
revisiting. The invisible college formed on Twitter is messy, consisting of
overlapping social contexts (professional, personal, and public), scholars with
different habits of engagement, and both formal and informal ties. Our research
illustrates the value of using multiple methods to explore the complex
questions arising from big data studies and points toward future research that
could implement big data techniques on a small scale, focusing on sub-topics or
emerging fields, to expose the nature of scholars' invisible colleges made
visible on Twitter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.02999</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.02999</id><created>2015-07-10</created><updated>2015-12-30</updated><authors><author><keyname>Razavi</keyname><forenames>Alireza</forenames></author><author><keyname>Valkama</keyname><forenames>Mikko</forenames></author><author><keyname>Cabric</keyname><forenames>Danijela</forenames></author></authors><title>Compressive Detection of Random Subspace Signals</title><categories>cs.IT math.IT</categories><comments>33 pages, 11 figures, Revised version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of compressive detection of random subspace signals is studied.
We consider signals modeled as $\mathbf{s} = \mathbf{H} \mathbf{x}$ where
$\mathbf{H}$ is an $N \times K$ matrix with $K \le N$ and $\mathbf{x} \sim
\mathcal{N}(\mathbf{0}_{K,1},\sigma_x^2 \mathbf{I}_K)$. We say that signal
$\mathbf{s}$ lies in or leans toward a subspace if the largest eigenvalue of
$\mathbf{H} \mathbf{H}^T$ is strictly greater than its smallest eigenvalue. We
first design a measurement matrix
$\mathbf{\Phi}=[\mathbf{\Phi}_s^T,\mathbf{\Phi}_o^T]^T$ comprising of two
sub-matrices $\mathbf{\Phi}_s$ and $\mathbf{\Phi}_o$ where $\mathbf{\Phi}_s$
projects the signals to the strongest left-singular vectors, i.e., the
left-singular vectors corresponding to the largest singular values, of subspace
matrix $\mathbf{H}$ and $\mathbf{\Phi}_o$ projects it to the weakest
left-singular vectors. We then propose two detectors which work based on the
difference in energies of the samples measured by two sub-matrices
$\mathbf{\Phi}_s$ and $\mathbf{\Phi}_o$ and prove their optimality. Simplified
versions of the proposed detectors for the case when the variance of noise is
known are also provided. Furthermore, we study the performance of the detector
when measurements are imprecise and show how imprecision can be compensated by
employing more measurement devices. The problem is then re-formulated for the
case when the signal lies in the union of a finite number of linear subspaces
instead of a single linear subspace. Finally, we study the performance of the
proposed methods by simulation examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03000</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03000</id><created>2015-07-10</created><updated>2015-07-22</updated><authors><author><keyname>Zhang</keyname><forenames>Zijian</forenames></author><author><keyname>Qin</keyname><forenames>Zhan</forenames></author><author><keyname>Zhu</keyname><forenames>Liehuang</forenames></author><author><keyname>Jiang</keyname><forenames>Wei</forenames></author><author><keyname>Xu</keyname><forenames>Chen</forenames></author><author><keyname>Ren</keyname><forenames>and Kui</forenames></author></authors><title>Toward Practical Differential Privacy in Smart Grid with
  Capacity-Limited Rechargeable Batteries</title><categories>cs.CR</categories><comments>This paper has been withdrawn by the author due to a crucial error in
  definition 3 and theorem 2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The technology of differential privacy, adding a noise drawn from the Laplace
distribution, successfully overcomes a difficulty of keeping both the privacy
of individual data and the utility of the statistical result simultaneously.
Therefore, it is prevalent to use a rechargeable battery as the noise for
achieving differential privacy in the application of smart grid. Unfortunately,
to the best of our knowledge, we observe that the existing privacy protection
mechanisms cannot satisfy differential privacy, when considering physical
restrictions of the battery in practice. In this paper, we first classify two
types of challenges caused by two physical restrictions, the maximum
charging/discharging rate and the capacity of battery. We then propose a
stateless privacy protection scheme by exploring a boundary-changeable
distribution for noise, and prove this scheme satisfies differential privacy,
with regard to the first type of challenge. We further explain the difficulty
to achieve differential privacy under the second type of challenge, and
formalize the definition of a relaxed differential privacy. Finally, we present
a stateful privacy protection scheme that satisfies the relaxed differential
privacy. Experimental analysis shows that the maximum privacy leakage of our
privacy protection schemes at each time point stably outperforms that of the
existing work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03009</identifier>
 <datestamp>2015-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03009</id><created>2015-07-10</created><authors><author><keyname>Kortsarz</keyname><forenames>Guy</forenames></author><author><keyname>Nutov</keyname><forenames>Zeev</forenames></author></authors><title>A $1.75$ LP approximation for the Tree Augmentation Problem</title><categories>cs.DS</categories><comments>arXiv admin note: substantial text overlap with arXiv:1507.02799</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Tree Augmentation Problem (TAP) the goal is to augment a tree $T$ by a
minimum size edge set $F$ from a given edge set $E$ such that $T \cup F$ is
$2$-edge-connected. The best approximation ratio known for TAP is $1.5$. In the
more general Weighted TAP problem, $F$ should be of minimum weight. Weighted
TAP admits several $2$-approximation algorithms w.r.t. to the standard cut
LP-relaxation, but for all of them the performance ratio of $2$ is tight even
for TAP. The problem is equivalent to the problem of covering a laminar set
family. Laminar set families play an important role in the design of
approximation algorithms for connectivity network design problems. In fact,
Weighted TAP is the simplest connectivity network design problem for which a
ratio better than $2$ is not known. Improving this &quot;natural&quot; ratio is a major
open problem, which may have implications on many other network design
problems. It seems that achieving this goal requires finding an LP-relaxation
with integrality gap better than $2$, which is a long time open problem even
for TAP. In this paper we introduce such an LP-relaxation and give an algorithm
that computes a feasible solution for TAP of size at most $1.75$ times the
optimal LP value. This gives some hope to break the ratio $2$ for the weighted
case. Our algorithm computes some initial edge set by solving a partial system
of constraints that form the integral edge-cover polytope, and then applies
local search on $3$-leaf subtrees to exchange some of the edges and to add
additional edges. Thus we do not need to solve the LP, and the algorithm runs
roughly in time required to find a minimum weight edge-cover in a general
graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03030</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03030</id><created>2015-07-10</created><updated>2015-12-02</updated><authors><author><keyname>Sayama</keyname><forenames>Hiroki</forenames></author></authors><title>Estimation of Laplacian spectra of direct and strong product graphs</title><categories>cs.DM cs.SI math.CO</categories><comments>14 pages, 7 figures; to be published in Discrete Applied Mathematics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Calculating a product of multiple graphs has been studied in mathematics,
engineering, computer science, and more recently in network science,
particularly in the context of multilayer networks. One of the important
questions to be addressed in this area is how to characterize spectral
properties of a product graph using those of its factor graphs. While several
such characterizations have already been obtained analytically (mostly for
adjacency spectra), characterization of Laplacian spectra of direct product and
strong product graphs has remained an open problem. Here we develop practical
methods to estimate Laplacian spectra of direct and strong product graphs from
spectral properties of their factor graphs using a few heuristic assumptions.
Numerical experiments showed that the proposed methods produced reasonable
estimation with percentage errors confined within a +/-10% range for most
eigenvalues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03032</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03032</id><created>2015-07-10</created><updated>2015-12-13</updated><authors><author><keyname>Abernethy</keyname><forenames>Jacob</forenames></author><author><keyname>Lee</keyname><forenames>Chansoo</forenames></author><author><keyname>Tewari</keyname><forenames>Ambuj</forenames></author></authors><title>Spectral Smoothing via Random Matrix Perturbations</title><categories>cs.LG</categories><comments>This paper has been withdrawn by the author due to a crucial error in
  Theorem 6.2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider stochastic smoothing of spectral functions of matrices using
perturbations commonly studied in random matrix theory. We show that a spectral
function remains spectral when smoothed using a unitarily invariant
perturbation distribution. We then derive state-of-the-art smoothing bounds for
the maximum eigenvalue function using the Gaussian Orthogonal Ensemble (GOE).
Smoothing the maximum eigenvalue function is important for applications in
semidefinite optimization and online learning. As a direct consequence of our
GOE smoothing results, we obtain an $O((N \log N)^{1/4} \sqrt{T})$ expected
regret bound for the online variance minimization problem using an algorithm
that performs only a single maximum eigenvector computation per time step. Here
$T$ is the number of rounds and $N$ is the matrix dimension. Our algorithm and
its analysis also extend to the more general online PCA problem where the
learner has to output a rank $k$ subspace. The algorithm just requires
computing $k$ maximum eigenvectors per step and enjoys an $O(k (N \log N)^{1/4}
\sqrt{T})$ expected regret bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03037</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03037</id><created>2015-07-10</created><authors><author><keyname>Sarma</keyname><forenames>S.</forenames></author></authors><title>SenseDroid: A Context-Aware Information Exchange Framework for Mobile
  Sensor Networks Using Android Phones</title><categories>cs.NI</categories><comments>7 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile phones and smartphones have evolved to be very powerful devices that
have the potential to be utilized in many application areas apart from generic
communication. With each passing year, we see increasingly powerful smartphones
being manufactured, which have a plethora of powerful embedded sensors like
microphone, camera, digital compass, GPS, accelerometer, temperature sensors
and many more. Moreover, the ability to easily program today's smartphones,
enables us to exploit these sensors, in a wide variety of application such as
personal safety, emergency and calamity response, situation awareness, remote
activity monitoring, transportation and environment monitoring. In this paper,
we survey the existing mobile phone sensing methodologies and application
areas. We also formulate the architectural framework of our project,
SenseDroid, its utility, limitations and possible future applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03040</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03040</id><created>2015-07-10</created><updated>2015-07-16</updated><authors><author><keyname>Maximov</keyname><forenames>Yury</forenames></author><author><keyname>Reshetova</keyname><forenames>Daria</forenames></author></authors><title>Tight Risk Bounds for Multi-Class Margin Classifiers</title><categories>stat.ML cs.LG</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a problem of risk estimation for large-margin multi-class
classifiers. We propose a novel risk bound for the multi-class classification
problem. The bound involves the marginal distribution of the classifier and the
Rademacher complexity of the hypothesis class. We prove that our bound is tight
in the number of classes. Finally, we compare our bound with the related ones
and provide a simplified version of the bound for the multi-class
classification with kernel based hypotheses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03044</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03044</id><created>2015-07-10</created><authors><author><keyname>Huang</keyname><forenames>Weiyu</forenames></author><author><keyname>Ribeiro</keyname><forenames>Alejandro</forenames></author></authors><title>Persistent Homology Lower Bounds on High Order Network Distances</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents methods to compare high order networks using persistent
homology. High order networks are weighted complete hypergraphs collecting
relationship functions between elements of tuples. They can be considered as
generalizations of conventional networks where only relationship functions
between pairs are defined. Valid metric distances between high order networks
have been defined but they are inherently difficult to compute when the number
of nodes is large. We relate high order networks to the filtrations of
simplicial complexes and show that the difference between networks can be lower
bounded by the difference between the homological features of their respective
filtrations. Practical implications are explored by comparing the coauthorship
networks of engineering and mathematics academic journals. The lower bounds
succeed in discriminating engineering communities from mathematics communities
and in differentiating engineering communities with different research
interests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03045</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03045</id><created>2015-07-10</created><authors><author><keyname>Khot</keyname><forenames>Tushar</forenames></author><author><keyname>Balasubramanian</keyname><forenames>Niranjan</forenames></author><author><keyname>Gribkoff</keyname><forenames>Eric</forenames></author><author><keyname>Sabharwal</keyname><forenames>Ashish</forenames></author><author><keyname>Clark</keyname><forenames>Peter</forenames></author><author><keyname>Etzioni</keyname><forenames>Oren</forenames></author></authors><title>Markov Logic Networks for Natural Language Question Answering</title><categories>cs.AI cs.CL</categories><comments>7 pages, 1 figure, StarAI workshop at UAI'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our goal is to answer elementary-level science questions using knowledge
extracted automatically from science textbooks, expressed in a subset of
first-order logic. Given the incomplete and noisy nature of these automatically
extracted rules, Markov Logic Networks (MLNs) seem a natural model to use, but
the exact way of leveraging MLNs is by no means obvious. We investigate three
ways of applying MLNs to our task. In the first, we simply use the extracted
science rules directly as MLN clauses. Unlike typical MLN applications, our
domain has long and complex rules, leading to an unmanageable number of
groundings. We exploit the structure present in hard constraints to improve
tractability, but the formulation remains ineffective. In the second approach,
we instead interpret science rules as describing prototypical entities, thus
mapping rules directly to grounded MLN assertions, whose constants are then
clustered using existing entity resolution methods. This drastically simplifies
the network, but still suffers from brittleness. Finally, our third approach,
called Praline, uses MLNs to align the lexical elements as well as define and
control how inference should be performed in this task. Our experiments,
demonstrating a 15\% accuracy boost and a 10x reduction in runtime, suggest
that the flexibility and different inference semantics of Praline are a better
fit for the natural language question answering task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03046</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03046</id><created>2015-07-10</created><authors><author><keyname>Cifuentes</keyname><forenames>Diego</forenames></author><author><keyname>Parrilo</keyname><forenames>Pablo A.</forenames></author></authors><title>An efficient tree decomposition method for permanents and mixed
  discriminants</title><categories>cs.DM cs.CC cs.DS</categories><comments>32 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an efficient algorithm to compute permanents, mixed discriminants
and hyperdeterminants of structured matrices and multidimensional arrays
(tensors). We describe the sparsity structure of an array in terms of a graph,
and we assume that its treewidth, denoted as $\omega$, is small. Our algorithm
requires $O(n 2^\omega)$ arithmetic operations to compute permanents, and
$O(n^2 + n 3^\omega)$ for mixed discriminants and hyperdeterminants. We finally
show that mixed volume computation continues to be hard under bounded treewidth
assumptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03049</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03049</id><created>2015-07-10</created><updated>2015-07-21</updated><authors><author><keyname>Liu</keyname><forenames>Feilong</forenames><affiliation>The Ohio State University</affiliation></author><author><keyname>Blanas</keyname><forenames>Spyros</forenames><affiliation>The Ohio State University</affiliation></author></authors><title>Forecasting the cost of processing multi-join queries via hashing for
  main-memory databases (Extended version)</title><categories>cs.DB</categories><comments>15 pages, 8 figures, extended version of the paper to appear in
  SoCC'15</comments><doi>10.1145/2806777.2806944</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Database management systems (DBMSs) carefully optimize complex multi-join
queries to avoid expensive disk I/O. As servers today feature tens or hundreds
of gigabytes of RAM, a significant fraction of many analytic databases becomes
memory-resident. Even after careful tuning for an in-memory environment, a
linear disk I/O model such as the one implemented in PostgreSQL may make query
response time predictions that are up to 2X slower than the optimal multi-join
query plan over memory-resident data. This paper introduces a memory I/O cost
model to identify good evaluation strategies for complex query plans with
multiple hash-based equi-joins over memory-resident data. The proposed cost
model is carefully validated for accuracy using three different systems,
including an Amazon EC2 instance, to control for hardware-specific differences.
Prior work in parallel query evaluation has advocated right-deep and bushy
trees for multi-join queries due to their greater parallelization and
pipelining potential. A surprising finding is that the conventional wisdom from
shared-nothing disk-based systems does not directly apply to the modern
shared-everything memory hierarchy. As corroborated by our model, the
performance gap between the optimal left-deep and right-deep query plan can
grow to about 10X as the number of joins in the query increases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03058</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03058</id><created>2015-07-10</created><authors><author><keyname>Tan</keyname><forenames>Bo</forenames></author><author><keyname>Wen</keyname><forenames>Zhi-Xiong</forenames></author><author><keyname>Zhang</keyname><forenames>Yiping</forenames></author></authors><title>Complexity of Substitutive Sequences - Calculation of the Complexities
  of Substitutive Sequences Over a Binary Alphabet</title><categories>math.CO cs.FL</categories><comments>16 pages</comments><msc-class>20M05, 68R15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the complexities of substitutive sequences over a binary
alphabet. By studying various types of special words, we show that, knowing
some initial values, its complexity can be completely formulated via a
recurrence formula determined by the characteristic polynomial.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03059</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03059</id><created>2015-07-10</created><authors><author><keyname>Raymond</keyname><forenames>Annie</forenames></author><author><keyname>Singh</keyname><forenames>Mohit</forenames></author><author><keyname>Thomas</keyname><forenames>Rekha R.</forenames></author></authors><title>Symmetry in Tur\'an Sums of Squares Polynomials from Flag Algebras</title><categories>math.CO cs.DM math.OC</categories><comments>21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tur\'an problems in extremal combinatorics concern asymptotic bounds on the
edge densities of graphs and hypergraphs that avoid specified subgraphs. The
theory of flag algebras proposed by Razborov provides powerful semidefinite
programming based methods to find sums of squares that establish edge density
inequalities in Tur\'an problems. Working with polynomial analogs of the flag
algebra entities, we prove that the sums of squares created by flag algebras
for Tur\'an problems can be retrieved from a restricted version of the
symmetry-adapted semidefinite program proposed by Gatermann and Parrilo. This
involves using the representation theory of the symmetric group for finding
succinct sums of squares expressions for invariant polynomials. This connection
reveals several combinatorial properties of flag algebra sums of squares and
offers new tools for Tur\'an problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03060</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03060</id><created>2015-07-10</created><updated>2015-11-21</updated><authors><author><keyname>Yu</keyname><forenames>Hongkai</forenames></author><author><keyname>Zhou</keyname><forenames>Youjie</forenames></author><author><keyname>Qian</keyname><forenames>Hui</forenames></author><author><keyname>Xian</keyname><forenames>Min</forenames></author><author><keyname>Lin</keyname><forenames>Yuewei</forenames></author><author><keyname>Guo</keyname><forenames>Dazhou</forenames></author><author><keyname>Zheng</keyname><forenames>Kang</forenames></author><author><keyname>Abdelfatah</keyname><forenames>Kareem</forenames></author><author><keyname>Wang</keyname><forenames>Song</forenames></author></authors><title>LooseCut: Interactive Image Segmentation with Loosely Bounded Boxes</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One popular approach to interactively segment the foreground object of
interest from an image is to annotate a bounding box that covers the foreground
object. Then, a binary labeling is performed to achieve a refined segmentation.
One major issue of the existing algorithms for such interactive image
segmentation is their preference of an input bounding box that tightly encloses
the foreground object. This increases the annotation burden, and prevents these
algorithms from utilizing automatically detected bounding boxes. In this paper,
we develop a new LooseCut algorithm that can handle cases where the input
bounding box only loosely covers the foreground object. We propose a new Markov
Random Fields (MRF) model for segmentation with loosely bounded boxes,
including a global similarity constraint to better distinguish the foreground
and background, and an additional energy term to encourage consistent labeling
of similar-appearance pixels. This MRF model is then solved by an iterated
max-flow algorithm. In the experiments, we evaluate LooseCut in three
publicly-available image datasets, and compare its performance against several
state-of-the-art interactive image segmentation algorithms. We also show that
LooseCut can be used for enhancing the performance of unsupervised video
segmentation and image saliency detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03066</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03066</id><created>2015-07-11</created><authors><author><keyname>Singh</keyname><forenames>Abhay Kumar</forenames></author><author><keyname>Kumar</keyname><forenames>Narendra</forenames></author></authors><title>On cyclic self-orthogonal codes over Zpm</title><categories>cs.IT math.IT</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this paper is to study the cyclic self orthogonal codes over
$\mathbb{Z}_{p^m}$. After providing the generator polynomial of cyclic self
orthogonal codes over $\mathbb{Z}_{p^m}$, we give the necessary and sufficient
condition for the existence of non-trivial self orthogonal codes over
$\mathbb{Z}_{p^m}$ . We have also provided the number of such codes of length
$n$ over $\mathbb{Z}_{p^m}$ for any $ (p,n) = 1 $.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03067</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03067</id><created>2015-07-11</created><authors><author><keyname>Uno</keyname><forenames>Takeaki</forenames></author><author><keyname>Maegawa</keyname><forenames>Hiroki</forenames></author><author><keyname>Nakahara</keyname><forenames>Takanobu</forenames></author><author><keyname>Hamuro</keyname><forenames>Yukinobu</forenames></author><author><keyname>Yoshinaka</keyname><forenames>Ryo</forenames></author><author><keyname>Tatsuta</keyname><forenames>Makoto</forenames></author></authors><title>Micro-Clustering: Finding Small Clusters in Large Diversity</title><categories>cs.DS cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of un-supervised soft-clustering called
micro-clustering. The aim of the problem is to enumerate all groups composed of
records strongly related to each other, while standard clustering methods
separate records at sparse parts. The problem formulation of micro-clustering
is non-trivial. Clique mining in a similarity graph is a typical approach, but
it results in a huge number of cliques that are of many similar cliques. We
propose a new concept data polishing. The cause of huge solutions can be
considered that the groups are not clear in the data, that is, the boundaries
of the groups are not clear, because of noise, uncertainty, lie, lack, etc.
Data polishing clarifies the groups by perturbating the data. Specifically,
dense subgraphs that would correspond to clusters are replaced by cliques. The
clusters are clarified as maximal cliques, thus the number of maximal cliques
will be drastically reduced. We also propose an efficient algorithm applicable
even for large scale data. Computational experiments showed the efficiency of
our algorithm, i.e., the number of solutions is small, (e.g., 1,000), the
members of each group are deeply related, and the computation time is short.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03077</identifier>
 <datestamp>2015-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03077</id><created>2015-07-11</created><updated>2015-11-24</updated><authors><author><keyname>Rahimi</keyname><forenames>Adel</forenames></author></authors><title>A new hybrid stemming algorithm for Persian</title><categories>cs.CL cs.IR</categories><comments>8 pages, 5 tables, 1 figure</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Stemming has been an influential part in Information retrieval and search
engines. There have been tremendous endeavours in making stemmer that are both
efficient and accurate. Stemmers can have three method in stemming, Dictionary
based stemmer, statistical-based stemmers, and rule-based stemmers. This paper
aims at building a hybrid stemmer that uses both Dictionary based method and
rule-based method for stemming. This ultimately helps the efficacy and
accurateness of the stemmer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03084</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03084</id><created>2015-07-11</created><updated>2015-07-16</updated><authors><author><keyname>Kabor&#xe9;</keyname><forenames>Jo&#xeb;l</forenames></author><author><keyname>Charkani</keyname><forenames>Mohammed E.</forenames></author></authors><title>Constacyclic codes over F_q + u F_q + v F_q + u v F_q</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let q be a prime power and F_q be a finite field. In this paper, we study
constacyclic codes over the ring F_q+ u F_q +v F_q+ u v F_q, where u^2=u, v^2=v
and uv=vu. We characterized the generator polynomials of constacyclic codes and
their duals using some decomposition of this ring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03097</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03097</id><created>2015-07-11</created><authors><author><keyname>Jiang</keyname><forenames>Shangpu</forenames></author><author><keyname>Lowd</keyname><forenames>Daniel</forenames></author><author><keyname>Dou</keyname><forenames>Dejing</forenames></author></authors><title>Ontology Matching with Knowledge Rules</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ontology matching is the process of automatically determining the semantic
equivalences between the concepts of two ontologies. Most ontology matching
algorithms are based on two types of strategies: terminology-based strategies,
which align concepts based on their names or descriptions, and structure-based
strategies, which exploit concept hierarchies to find the alignment. In many
domains, there is additional information about the relationships of concepts
represented in various ways, such as Bayesian networks, decision trees, and
association rules. We propose to use the similarities between these
relationships to find more accurate alignments. We accomplish this by defining
soft constraints that prefer alignments where corresponding concepts have the
same local relationships encoded as knowledge rules. We use a probabilistic
framework to integrate this new knowledge-based strategy with standard
terminology-based and structure-based strategies. Furthermore, our method is
particularly effective in identifying correspondences between complex concepts.
Our method achieves substantially better F-score than the previous
state-of-the-art on three ontology matching domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03109</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03109</id><created>2015-07-11</created><authors><author><keyname>Ma</keyname><forenames>Jing</forenames></author><author><keyname>Zhang</keyname><forenames>Wenhui</forenames></author></authors><title>Enhancing the Security of Protocols against Actor Key Compromise
  Problems</title><categories>cs.CR</categories><comments>14 pages, 4 figures</comments><msc-class>68N30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Security of complex systems is an important issue in software engineering.
For complex computer systems involving many actors, security protocols are
often used for the communication of sensitive data. Actor key compromise (AKC)
denotes a situation where the long-term secret key of an actor may be known to
an adversary for some reasons. Many protocols are not secure enough for
ensuring security in such a situation. In this paper, we further study this
problem by looking at potential types of attacks, defining their formal
properties and providing solutions to enhance the level of security. As case
studies, we analyze the vulnerabilities (with respect to potential AKC attacks)
of practical protocols, including PKMv2RSA and Kerberos, and provide solutions
to enhance the level of security of such protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03113</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03113</id><created>2015-07-11</created><authors><author><keyname>Murtagh</keyname><forenames>Jack</forenames></author><author><keyname>Vadhan</keyname><forenames>Salil</forenames></author></authors><title>The Complexity of Computing the Optimal Composition of Differential
  Privacy</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the study of differential privacy, composition theorems (starting with the
original paper of Dwork, McSherry, Nissim, and Smith (TCC'06)) bound the
degradation of privacy when composing several differentially private
algorithms. Kairouz, Oh, and Viswanath (ICML'15) showed how to compute the
optimal bound for composing $k$ arbitrary $(\epsilon,\delta)$-differentially
private algorithms. We characterize the optimal composition for the more
general case of $k$ arbitrary
$(\epsilon_{1},\delta_{1}),\ldots,(\epsilon_{k},\delta_{k})$-differentially
private algorithms where the privacy parameters may differ for each algorithm
in the composition. We show that computing the optimal composition in general
is $\#$P-complete. Since computing optimal composition exactly is infeasible
(unless FP=$\#$P), we give an approximation algorithm that computes the
composition to arbitrary accuracy in polynomial time. The algorithm is a
modification of Dyer's dynamic programming approach to approximately counting
solutions to knapsack problems (STOC'03).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03114</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03114</id><created>2015-07-11</created><authors><author><keyname>Varadarajan</keyname><forenames>Venkatanathan</forenames></author><author><keyname>Zhang</keyname><forenames>Yinqian</forenames></author><author><keyname>Ristenpart</keyname><forenames>Thomas</forenames></author><author><keyname>Swift</keyname><forenames>Michael</forenames></author></authors><title>A Placement Vulnerability Study in Multi-tenant Public Clouds</title><categories>cs.CR</categories><comments>This paper is the full version of the paper that earlier appeared in
  USENIX Security 2015 as V. Varadarajan, Y. Zhang, T. Ristenpart, and M.
  Swift. A placement vulnerability study in multi-tenant public clouds. In 24th
  USENIX Security Symposium. USENIX Association, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Public infrastructure-as-a-service clouds, such as Amazon EC2, Google Compute
Engine (GCE) and Microsoft Azure allow clients to run virtual machines (VMs) on
shared physical infrastructure. This practice of multi-tenancy brings economies
of scale, but also introduces the risk of sharing a physical server with an
arbitrary and potentially malicious VM. Past works have demonstrated how to
place a VM alongside a target victim (co-location) in early-generation clouds
and how to extract secret information via side- channels. Although there have
been numerous works on side-channel attacks, there have been no studies on
placement vulnerabilities in public clouds since the adoption of stronger
isolation technologies such as Virtual Private Clouds (VPCs).
  We investigate this problem of placement vulnerabilities and quantitatively
evaluate three popular public clouds for their susceptibility to co-location
attacks. We find that adoption of new technologies (e.g., VPC) makes many prior
attacks, such as cloud cartography, ineffective. We find new ways to reliably
test for co-location across Amazon EC2, Google GCE, and Microsoft Azure. We
also found ways to detect co-location with victim web servers in a multi-tiered
cloud application located behind a load balancer.
  We use our new co-residence tests and multiple customer accounts to launch VM
instances under different strategies that seek to maximize the likelihood of
co-residency. We find that it is much easier (10x higher success rate) and
cheaper (up to $114 less) to achieve co-location in these three clouds when
compared to a secure reference placement policy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03117</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03117</id><created>2015-07-11</created><authors><author><keyname>Pohl</keyname><forenames>Christoph</forenames></author><author><keyname>Meier</keyname><forenames>Michael</forenames></author><author><keyname>Hof</keyname><forenames>Hans-Joachim</forenames></author></authors><title>Apate - A Linux Kernel Module for High Interaction Honeypots</title><categories>cs.CR cs.NI</categories><comments>The Ninth International Conference on Emerging Security Information,
  Systems and Technologies - SECURWARE 2015, Venice, Italy, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Honeypots are used in IT Security to detect and gather information about
ongoing intrusions, e.g., by documenting the approach of an attacker. Honeypots
do so by presenting an interactive system that seems just like a valid
application to an attacker. One of the main design goals of honeypots is to
stay unnoticed by attackers as long as possible. The longer the intruder
interacts with the honeypot, the more valuable information about the attack can
be collected. Of course, another main goal of honeypots is to not open new
vulnerabilities that attackers can exploit. Thus, it is necessary to harden the
honeypot and the surrounding environment. This paper presents Apate, a Linux
Kernel Module (LKM) that is able to log, block and manipulate system calls
based on preconfigurable conditions like Process ID (PID), User Id (UID), and
many more. Apate can be used to build and harden High Interaction Honeypots.
Apate can be configured using an integrated high level language. Thus, Apate is
an important and easy to use building block for upcoming High Interaction
Honeypots.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03125</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03125</id><created>2015-07-11</created><authors><author><keyname>Wang</keyname><forenames>Nan</forenames></author></authors><title>A new boosting algorithm based on dual averaging scheme</title><categories>cs.LG</categories><comments>8 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fields of machine learning and mathematical optimization increasingly
intertwined. The special topic on supervised learning and convex optimization
examines this interplay. The training part of most supervised learning
algorithms can usually be reduced to an optimization problem that minimizes a
loss between model predictions and training data. While most optimization
techniques focus on accuracy and speed of convergence, the qualities of good
optimization algorithm from the machine learning perspective can be quite
different since machine learning is more than fitting the data. Better
optimization algorithms that minimize the training loss can possibly give very
poor generalization performance. In this paper, we examine a particular kind of
machine learning algorithm, boosting, whose training process can be viewed as
functional coordinate descent on the exponential loss. We study the relation
between optimization techniques and machine learning by implementing a new
boosting algorithm. DABoost, based on dual-averaging scheme and study its
generalization performance. We show that DABoost, although slower in reducing
the training error, in general enjoys a better generalization error than
AdaBoost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03126</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03126</id><created>2015-07-11</created><authors><author><keyname>Ambainis</keyname><forenames>Andris</forenames></author><author><keyname>Belovs</keyname><forenames>Aleksandrs</forenames></author><author><keyname>Regev</keyname><forenames>Oded</forenames></author><author><keyname>de Wolf</keyname><forenames>Ronald</forenames></author></authors><title>Efficient Quantum Algorithms for (Gapped) Group Testing and Junta
  Testing</title><categories>cs.CC quant-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the $k$-junta testing problem, a tester has to efficiently decide whether
a given function $f:\{0,1\}^n\rightarrow \{0,1\}$ is a $k$-junta (i.e., depends
on at most $k$ of its input bits) or is $\epsilon$-far from any $k$-junta. Our
main result is a quantum algorithm for this problem with query complexity
$\tilde O(\sqrt{k/\epsilon})$ and time complexity $\tilde
O(n\sqrt{k/\epsilon})$. This quadratically improves over the query complexity
of the previous best quantum junta tester, due to At\i c\i\ and Servedio. Our
tester is based on a new quantum algorithm for a gapped version of the
combinatorial group testing problem, with an up to quartic improvement over the
query complexity of the best classical algorithm. For our upper bound on the
time complexity we give a near-linear time implementation of a shallow variant
of the quantum Fourier transform over the symmetric group, similar to the
Schur-Weyl transform. We also prove a lower bound of $\Omega(k^{1/3})$ queries
for junta-testing (for constant $\epsilon$).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03137</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03137</id><created>2015-07-11</created><authors><author><keyname>Gilray</keyname><forenames>Thomas</forenames></author><author><keyname>Lyde</keyname><forenames>Steven</forenames></author><author><keyname>Adams</keyname><forenames>Michael D.</forenames></author><author><keyname>Might</keyname><forenames>Matthew</forenames></author><author><keyname>Van Horn</keyname><forenames>David</forenames></author></authors><title>Pushdown Control-Flow Analysis for Free</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional control-flow analysis (CFA) for higher-order languages, whether
implemented by constraint-solving or abstract interpretation, introduces
spurious connections between callers and callees. Two distinct invocations of a
function will necessarily pollute one another's return-flow. Recently, three
distinct approaches have been published which provide perfect call-stack
precision in a computable manner: CFA2, PDCFA, and AAC. Unfortunately, CFA2 and
PDCFA are difficult to implement and require significant engineering effort.
Furthermore, all three are computationally expensive; for a monovariant
analysis, CFA2 is in $O(2^n)$, PDCFA is in $O(n^6)$, and AAC is in $O(n^9 log
n)$.
  In this paper, we describe a new technique that builds on these but is both
straightforward to implement and computationally inexpensive. The crucial
insight is an unusual state-dependent allocation strategy for the addresses of
continuation. Our technique imposes only a constant-factor overhead on the
underlying analysis and, with monovariance, costs only O(n3) in the worst case.
  This paper presents the intuitions behind this development, a proof of the
precision of this analysis, and benchmarks demonstrating its efficacy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03145</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03145</id><created>2015-07-11</created><updated>2015-12-02</updated><authors><author><keyname>Li</keyname><forenames>Hang</forenames></author><author><keyname>Huang</keyname><forenames>Chuan</forenames></author><author><keyname>Alsaadi</keyname><forenames>Fuad</forenames></author><author><keyname>Cui</keyname><forenames>Shuguang</forenames></author></authors><title>Performance Analysis for Energy Harvesting Communication Systems: From
  Throughput to Energy Diversity</title><categories>cs.IT math.IT</categories><comments>16 pages, accepted by GLOBECOM 2015. The definition of energy
  diversity gain may need to modify in the journal version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy harvesting (EH) based communication has raised great research
interests due to its wide application and the feasibility of commercialization.
In this paper, we consider wireless communications with EH constraints at the
transmitter. First, for delay-tolerant traffic, we investigate the long-term
average throughput maximization problem and analytically compare the throughput
performance against that of a system supported by conventional power supplies.
Second, for delay-sensitive traffic, we analyze the outage probability by
studying its asymptotic behavior in the high energy arrival rate regime, where
the new concept of energy diversity is formally introduced. Moreover, we show
that the speed of outage probability approaching zero, termed energy diversity
gain, varies under different power supply models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03148</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03148</id><created>2015-07-11</created><updated>2015-07-18</updated><authors><author><keyname>Yang</keyname><forenames>Heng</forenames></author><author><keyname>Mou</keyname><forenames>Wenxuan</forenames></author><author><keyname>Zhang</keyname><forenames>Yichi</forenames></author><author><keyname>Patras</keyname><forenames>Ioannis</forenames></author><author><keyname>Gunes</keyname><forenames>Hatice</forenames></author><author><keyname>Robinson</keyname><forenames>Peter</forenames></author></authors><title>Face Alignment Assisted by Head Pose Estimation</title><categories>cs.CV</categories><comments>Accepted by BMVC2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a supervised initialization scheme for cascaded face
alignment based on explicit head pose estimation. We first investigate the
failure cases of most state of the art face alignment approaches and observe
that these failures often share one common global property, i.e. the head pose
variation is usually large. Inspired by this, we propose a deep convolutional
network model for reliable and accurate head pose estimation. Instead of using
a mean face shape, or randomly selected shapes for cascaded face alignment
initialisation, we propose two schemes for generating initialisation: the first
one relies on projecting a mean 3D face shape (represented by 3D facial
landmarks) onto 2D image under the estimated head pose; the second one searches
nearest neighbour shapes from the training set according to head pose distance.
By doing so, the initialisation gets closer to the actual shape, which enhances
the possibility of convergence and in turn improves the face alignment
performance. We demonstrate the proposed method on the benchmark 300W dataset
and show very competitive performance in both head pose estimation and face
alignment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03160</identifier>
 <datestamp>2015-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03160</id><created>2015-07-11</created><authors><author><keyname>Mishra</keyname><forenames>Tapas Kumar</forenames></author><author><keyname>Pal</keyname><forenames>Sudebkumar Prasant</forenames></author></authors><title>Strong $(r,p)$ Cover for Hypergraphs</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the notion of the { \it strong $(r,p)$ cover} number
$\chi^c(G,k,r,p)$ for $k$-uniform hypergraphs $G(V,E)$, where $\chi^c(G,k,r,p)$
denotes the minimum number of $r$-colorings of vertices in $V$ such that each
hyperedge in $E$ contains at least $min(p,k)$ vertices of distinct colors in at
least one of the $\chi^c(G,k,r,p)$ $r$-colorings. We derive the exact values of
$\chi^c(K_n^k,k,r,p)$ for small values of $n$, $k$, $r$ and $p$, where $K_n^k$
denotes the complete $k$-uniform hypergraph of $n$ vertices. We study the
variation of $\chi^c(G,k,r,p)$ with respect to changes in $k$, $r$, $p$ and
$n$; we show that $\chi^c(G,k,r,p)$ is at least (i) $\chi^c(G,k,r-1,p-1)$, and,
(ii) $\chi^c(G',k-1,r,p-1)$, where $G'$ is any $(n-1)$-vertex induced
sub-hypergraph of $G$. We establish a general upper bound for
$\chi^c(K_n^k,k,r,p)$ for complete $k$-uniform hypergraphs using a
divide-and-conquer strategy for arbitrary values of $k$, $r$ and $p$. We also
relate $\chi^c(G,k,r,p)$ to the number $|E|$ of hyperedges, and the maximum
{\it hyperedge degree (dependency)} $d(G)$, as follows. We show that
$\chi^c(G,k,r,p)\leq x$ for integer $x&gt;0$, if $|E|\leq
\frac{1}{2}({\frac{r^k}{(t-1)^k \binom{r}{t-1}}})^x $, for any $k$-uniform
hypergraph. We prove that a { \it strong $(r,p)$ cover} of size $x$ can be
computed in randomized polynomial time if $d(G)\leq
\frac{1}{e}({\frac{r^k}{(p-1)^k \binom{r}{p-1}}})^x-1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03162</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03162</id><created>2015-07-11</created><updated>2015-10-01</updated><authors><author><keyname>McKenzie</keyname><forenames>Marlon</forenames></author><author><keyname>Fan</keyname><forenames>Hua</forenames></author><author><keyname>Golab</keyname><forenames>Wojciech</forenames></author></authors><title>Continuous Partial Quorums for Consistency-Latency Tuning in Distributed
  NoSQL Storage Systems</title><categories>cs.DC</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  NoSQL storage systems are used extensively by web applications and provide an
attractive alternative to conventional databases when the need for scalability
outweighs the need for transactions. Several of these systems provide
quorum-based replication and present the application developer with a choice of
multiple client-side &quot;consistency levels&quot; that determine the number of replicas
accessed by reads and writes, which in turn affects both latency and the
consistency observed by the client application. Since using a fixed combination
of read and write consistency levels for a given application provides only a
limited number of discrete options, we investigate techniques that allow more
fine-grained tuning of the consistency-latency trade-off, as may be required to
support consistency-based service level agreements (SLAs). We propose a novel
technique called \emph{continuous partial quorums} (CPQ) that assigns the
consistency level on a per-operation basis by choosing randomly between two
options, such as eventual and strong consistency, with a tunable probability.
We evaluate our technique experimentally using Apache Cassandra and demonstrate
that it outperforms an alternative tuning technique that delays operations
artificially.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03165</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03165</id><created>2015-07-11</created><authors><author><keyname>Orhan</keyname><forenames>Oner</forenames></author><author><keyname>Erkip</keyname><forenames>Elza</forenames></author></authors><title>Energy Harvesting Two-Hop Communication Networks</title><categories>cs.IT math.IT</categories><comments>29 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy harvesting multi-hop networks allow for perpetual operation of low
cost, limited range wireless devices. Compared with their battery operated
counterparts, the coupling of energy and data causality constraints with half
duplex relay operation makes it challenging to operate such networks. In this
paper, a throughput maximization problem for energy harvesting two-hop networks
with decode-and-forward half-duplex relays is investigated. For a system with
two parallel relays, various combinations of the following four transmission
modes are considered: Broadcast from the source, multi-access from the relays,
and successive relaying phases I and II. Optimal transmission policies for one
and two parallel relays are studied under the assumption of non-causal
knowledge of energy arrivals and finite size relay data buffers. The problem is
formulated using a convex optimization framework, which allows for efficient
numerical solutions and helps identify important properties of optimal
policies. Numerical results are presented to provide throughput comparisons and
to investigate the impact of multiple relays, size of relay data buffers,
transmission modes, and energy harvesting on the throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03166</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03166</id><created>2015-07-11</created><authors><author><keyname>Ouaknine</keyname><forenames>Jo&#xeb;l</forenames></author><author><keyname>Pinto</keyname><forenames>Jo&#xe3;o Sousa</forenames></author><author><keyname>Worrell</keyname><forenames>James</forenames></author></authors><title>The Polyhedral Escape Problem is Decidable</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Polyhedral Escape Problem for continuous linear dynamical systems
consists of deciding, given an affine function $f: \mathbb{R}^{d} \rightarrow
\mathbb{R}^{d}$ and a convex polyhedron $\mathcal{P} \subseteq \mathbb{R}^{d}$,
whether, for some initial point $\boldsymbol{x}_{0}$ in $\mathcal{P}$, the
trajectory of the unique solution to the differential equation
$\dot{\boldsymbol{x}}(t)=f(\boldsymbol{x}(t))$,
$\boldsymbol{x}(0)=\boldsymbol{x}_{0}$, is entirely contained in $\mathcal{P}$.
We show that this problem is decidable, by reducing it in polynomial time to
the decision version of linear programming with real algebraic coefficients,
thus placing it in $\exists \mathbb{R}$, which lies between NP and PSPACE. Our
algorithm makes use of spectral techniques and relies among others on tools
from Diophantine approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03168</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03168</id><created>2015-07-11</created><authors><author><keyname>Robles-Granda</keyname><forenames>Pablo</forenames></author><author><keyname>Moreno</keyname><forenames>Sebastian</forenames></author><author><keyname>Neville</keyname><forenames>Jennifer</forenames></author></authors><title>Using Bayesian Network Representations for Effective Sampling from
  Generative Network Models</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian networks (BNs) are used for inference and sampling by exploiting
conditional independence among random variables. Context specific independence
(CSI) is a property of graphical models where additional independence relations
arise in the context of particular values of random variables (RVs).
Identifying and exploiting CSI properties can simplify inference. Some
generative network models (models that generate social/information network
samples from a network distribution P(G)), with complex interactions among a
set of RVs, can be represented with probabilistic graphical models, in
particular with BNs. In the present work we show one such a case. We discuss
how a mixed Kronecker Product Graph Model can be represented as a BN, and study
its BN properties that can be used for efficient sampling. Specifically, we
show that instead of exhibiting CSI properties, the model has deterministic
context-specific dependence (DCSD). Exploiting this property focuses the
sampling method on a subset of the sampling space that improves efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03173</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03173</id><created>2015-07-11</created><authors><author><keyname>Zeng</keyname><forenames>Jinshan</forenames></author><author><keyname>Peng</keyname><forenames>Zhimin</forenames></author><author><keyname>Lin</keyname><forenames>Shaobo</forenames></author></authors><title>A Gauss-Seidel Iterative Thresholding Algorithm for lq Regularized Least
  Squares Regression</title><categories>cs.NA</categories><comments>35 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent studies on sparse modeling, $l_q$ ($0&lt;q&lt;1$) regularized least
squares regression ($l_q$LS) has received considerable attention due to its
superiorities on sparsity-inducing and bias-reduction over the convex
counterparts. In this paper, we propose a Gauss-Seidel iterative thresholding
algorithm (called GAITA) for solution to this problem. Different from the
classical iterative thresholding algorithms using the Jacobi updating rule,
GAITA takes advantage of the Gauss-Seidel rule to update the coordinate
coefficients. Under a mild condition, we can justify that the support set and
sign of an arbitrary sequence generated by GAITA will converge within finite
iterations. This convergence property together with the Kurdyka-{\L}ojasiewicz
property of ($l_q$LS) naturally yields the strong convergence of GAITA under
the same condition as above, which is generally weaker than the condition for
the convergence of the classical iterative thresholding algorithms.
Furthermore, we demonstrate that GAITA converges to a local minimizer under
certain additional conditions. A set of numerical experiments are provided to
show the effectiveness, particularly, much faster convergence of GAITA as
compared with the classical iterative thresholding algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03181</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03181</id><created>2015-07-11</created><authors><author><keyname>Jiang</keyname><forenames>Shangpu</forenames></author><author><keyname>Lowd</keyname><forenames>Daniel</forenames></author><author><keyname>Dou</keyname><forenames>Dejing</forenames></author></authors><title>A Probabilistic Approach to Knowledge Translation</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we focus on a novel knowledge reuse scenario where the
knowledge in the source schema needs to be translated to a semantically
heterogeneous target schema. We refer to this task as &quot;knowledge translation&quot;
(KT). Unlike data translation and transfer learning, KT does not require any
data from the source or target schema. We adopt a probabilistic approach to KT
by representing the knowledge in the source schema, the mapping between the
source and target schemas, and the resulting knowledge in the target schema all
as probability distributions, specially using Markov random fields and Markov
logic networks. Given the source knowledge and mappings, we use standard
learning and inference algorithms for probabilistic graphical models to find an
explicit probability distribution in the target schema that minimizes the
Kullback-Leibler divergence from the implicit distribution. This gives us a
compact probabilistic model that represents knowledge from the source schema as
well as possible, respecting the uncertainty in both the source knowledge and
the mapping. In experiments on both propositional and relational domains, we
find that the knowledge obtained by KT is comparable to other approaches that
require data, demonstrating that knowledge can be reused without data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03183</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03183</id><created>2015-07-12</created><authors><author><keyname>Sharma</keyname><forenames>Ankit</forenames></author><author><keyname>Feng</keyname><forenames>Xiaodong</forenames></author><author><keyname>Singhal</keyname><forenames>Kartik</forenames></author><author><keyname>Kuang</keyname><forenames>Rui</forenames></author><author><keyname>Srivastava</keyname><forenames>Jaideep</forenames></author></authors><title>Predicting Small Group Accretion in Social Networks: A topology based
  incremental approach</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Small Group evolution has been of central importance in social sciences and
also in the industry for understanding dynamics of team formation. While most
of research works studying groups deal at a macro level with evolution of
arbitrary size communities, in this paper we restrict ourselves to studying
evolution of small group (size $\leq20$) which is governed by contrasting
sociological phenomenon. Given a previous history of group collaboration
between a set of actors, we address the problem of predicting likely future
group collaborations. Unfortunately, predicting groups requires choosing from
$n \choose r$ possibilities (where $r$ is group size and $n$ is total number of
actors), which becomes computationally intractable as group size increases.
However, our statistical analysis of a real world dataset has shown that two
processes: an external actor joining an existing group (incremental accretion
(IA)) or collaborating with a subset of actors of an exiting group (subgroup
accretion (SA)), are largely responsible for future group formation. This helps
to drastically reduce the $n\choose r$ possibilities. We therefore, model the
attachment of a group for different actors outside this group. In this paper,
we have built three topology based prediction models to study these phenomena.
The performance of these models is evaluated using extensive experiments over
DBLP dataset. Our prediction results shows that the proposed models are
significantly useful for future group predictions both for IA and SA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03190</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03190</id><created>2015-07-12</created><updated>2016-02-22</updated><authors><author><keyname>Zhou</keyname><forenames>Lin</forenames></author><author><keyname>Tan</keyname><forenames>Vincent Yan Fu</forenames></author><author><keyname>Motani</keyname><forenames>Mehul</forenames></author></authors><title>On Error Exponents and Moderate Deviations for Lossless Streaming
  Compression of Correlated Sources</title><categories>cs.IT math.IT</categories><comments>The paper has been withdrawn due to an error in the achievability
  proof</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive upper and lower bounds for the error exponents of lossless
streaming compression of two correlated sources under the blockwise and
symbolwise settings. We consider the linear scaling regime in which the delay
is a scalar multiple of the number of symbol pairs of interest. We show that
for rate pairs satisfying certain constraints, the upper and lower bounds for
the error exponent of blockwise codes coincide. For symbolwise codes, the
bounds coincide for rate pairs satisfying the aforementioned constraints and a
certain condition on the symbol pairs we wish to decode---namely, that their
indices are asymptotically comparable to the blocklength. We also derive
moderate deviations constants for blockwise and symbolwise codes, leveraging
the error exponent results, and using appropriate Taylor series expansions. In
particular, for blockwise codes, we derive an information spectrum-type strong
converse, giving the complete characterization of the moderate deviations
constants. For symbolwise codes, under an additional requirement on the backoff
from the first-order fundamental limit, we can show that the moderate
deviations constants are the same as the blockwise setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03194</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03194</id><created>2015-07-12</created><updated>2015-08-28</updated><authors><author><keyname>T&#xfc;rkmen</keyname><forenames>Ali Caner</forenames></author></authors><title>A Review of Nonnegative Matrix Factorization Methods for Clustering</title><categories>stat.ML cs.LG cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nonnegative Matrix Factorization (NMF) was first introduced as a low-rank
matrix approximation technique, and has enjoyed a wide area of applications.
Although NMF does not seem related to the clustering problem at first, it was
shown that they are closely linked. In this report, we provide a gentle
introduction to clustering and NMF before reviewing the theoretical
relationship between them. We then explore several NMF variants, namely Sparse
NMF, Projective NMF, Nonnegative Spectral Clustering and Cluster-NMF, along
with their clustering interpretations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03195</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03195</id><created>2015-07-12</created><authors><author><keyname>Charpentier</keyname><forenames>Cl&#xe9;ment</forenames><affiliation>IF</affiliation></author></authors><title>The Coloring Game on Planar Graphs with Large Girth, by a result on
  Sparse Cactuses</title><categories>cs.DM math.CO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We denote by $\chi$ g (G) the game chromatic number of a graph G, which is
the smallest number of colors Alice needs to win the coloring game on G. We
know from Montassier et al. [M. Montassier, P. Ossona de Mendez, A. Raspaud and
X. Zhu, Decomposing a graph into forests, J. Graph Theory Ser. B, 102(1):38-52,
2012] and, independantly, from Wang and Zhang, [Y. Wang and Q. Zhang.
Decomposing a planar graph with girth at least 8 into a forest and a matching,
Discrete Maths, 311:844-849, 2011] that planar graphs with girth at least 8
have game chromatic number at most 5. One can ask if this bound of 5 can be
improved for a sufficiently large girth. In this paper, we prove that it
cannot. More than that, we prove that there are cactuses CT (i.e. graphs whose
edges only belong to at most one cycle each) having $\chi$ g (CT) = 5 despite
having arbitrary large girth, and even arbitrary large distance between its
cycles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03196</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03196</id><created>2015-07-12</created><authors><author><keyname>Wang</keyname><forenames>Zhangyang</forenames></author><author><keyname>Yang</keyname><forenames>Jianchao</forenames></author><author><keyname>Jin</keyname><forenames>Hailin</forenames></author><author><keyname>Shechtman</keyname><forenames>Eli</forenames></author><author><keyname>Agarwala</keyname><forenames>Aseem</forenames></author><author><keyname>Brandt</keyname><forenames>Jonathan</forenames></author><author><keyname>Huang</keyname><forenames>Thomas S.</forenames></author></authors><title>DeepFont: Identify Your Font from An Image</title><categories>cs.CV</categories><comments>To Appear in ACM Multimedia as a full paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As font is one of the core design concepts, automatic font identification and
similar font suggestion from an image or photo has been on the wish list of
many designers. We study the Visual Font Recognition (VFR) problem, and advance
the state-of-the-art remarkably by developing the DeepFont system. First of
all, we build up the first available large-scale VFR dataset, named AdobeVFR,
consisting of both labeled synthetic data and partially labeled real-world
data. Next, to combat the domain mismatch between available training and
testing data, we introduce a Convolutional Neural Network (CNN) decomposition
approach, using a domain adaptation technique based on a Stacked Convolutional
Auto-Encoder (SCAE) that exploits a large corpus of unlabeled real-world text
images combined with synthetic data preprocessed in a specific way. Moreover,
we study a novel learning-based model compression approach, in order to reduce
the DeepFont model size without sacrificing its performance. The DeepFont
system achieves an accuracy of higher than 80% (top-5) on our collected
dataset, and also produces a good font similarity measure for font selection
and suggestion. We also achieve around 6 times compression of the model without
any visible loss of recognition accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03206</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03206</id><created>2015-07-12</created><updated>2015-09-29</updated><authors><author><keyname>Dirafzoon</keyname><forenames>Alireza</forenames></author><author><keyname>Bozkurt</keyname><forenames>Alper</forenames></author><author><keyname>Lobaton</keyname><forenames>Edgar</forenames></author></authors><title>Dynamic Topological Mapping with Biobotic Swarms</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an approach for dynamic exploration and mapping of
unknown environments using a swarm of biobotic sensing agents, with a
stochastic natural motion model and a leading agent (e.g., an unmanned aerial
vehicle). The proposed robust mapping technique constructs a topological map of
the environment using only encounter information from the swarm. A sliding
window strategy is adopted in conjunction with a topological mapping strategy
based on local interactions among the swarm in a coordinate-free fashion to
obtain local maps of the environment. These maps are then merged into a global
topological map which can be visualized using a graphical representation that
integrates geometric as well as topological feature of the environment.
Localized robust topological features are extracted using tools from
topological data analysis. Simulation results have been presented to illustrate
and verify the correctness of our dynamic mapping algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03215</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03215</id><created>2015-07-12</created><authors><author><keyname>Diekert</keyname><forenames>Volker</forenames></author></authors><title>More Than 1700 Years of Word Equations</title><categories>cs.LO</categories><comments>The paper will appear as an invited address in the LNCS proceedings
  of CAI 2015, Stuttgart, Germany, September 1 - 4, 2015</comments><acm-class>F.4; F.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Geometry and Diophantine equations have been ever-present in mathematics.
Diophantus of Alexandria was born in the 3rd century (as far as we know), but a
systematic mathematical study of word equations began only in the 20th century.
So, the title of the present article does not seem to be justified at all.
However, a linear Diophantine equation can be viewed as a special case of a
system of word equations over a unary alphabet, and, more importantly, a word
equation can be viewed as a special case of a Diophantine equation. Hence, the
problem WordEquations: &quot;Is a given word equation solvable?&quot; is intimately
related to Hilbert's 10th problem on the solvability of Diophantine equations.
This became clear to the Russian school of mathematics at the latest in the mid
1960s, after which a systematic study of that relation began.
  Here, we review some recent developments which led to an amazingly simple
decision procedure for WordEquations, and to the description of the set of all
solutions as an EDT0L language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03217</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03217</id><created>2015-07-12</created><authors><author><keyname>Kim</keyname><forenames>Yong-Jin</forenames></author><author><keyname>Paek</keyname><forenames>Hyon-Song</forenames></author><author><keyname>Kim</keyname><forenames>Nam-Chol</forenames></author><author><keyname>Byon</keyname><forenames>Chong-Il</forenames></author></authors><title>An algorithm for computing Grobner basis and the complexity evaluation</title><categories>cs.SC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we suggest a new efficient algorithm in order to compute
S-polynomial reduction rapidly in the known algorithm for computing Grobner
bases, and compare the complexity with others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03219</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03219</id><created>2015-07-12</created><authors><author><keyname>Neves</keyname><forenames>Renato</forenames></author><author><keyname>Barbosa</keyname><forenames>Lu&#xed;s S.</forenames></author><author><keyname>Hofmann</keyname><forenames>Dirk</forenames></author><author><keyname>Martins</keyname><forenames>Manuel A.</forenames></author></authors><title>Continuity as a computational effect</title><categories>cs.LO cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The original purpose of component-based development was to provide techniques
to master complex software, through composition, reuse and parametrisation.
However, such systems are rapidly moving towards a level in which software
becomes prevalently intertwined with (continuous) physical processes. A
possible way to accommodate the latter in component calculi relies on a
suitable encoding of continuous behaviour as (yet another) computational
effect. This paper introduces such an encoding through a monad which, in the
compositional development of hybrid systems, may play a role similar to the one
played by the 1+, powerset, and distribution monads in the characterisation of
partial, non deterministic and probabilistic components, respectively. This
monad and its Kleisli category provide a setting in which the effects of
continuity over (different forms of) composition can be suitably studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03223</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03223</id><created>2015-07-12</created><authors><author><keyname>Tyagi</keyname><forenames>Shruti</forenames></author><author><keyname>Chopra</keyname><forenames>Deepti</forenames></author><author><keyname>Mathur</keyname><forenames>Iti</forenames></author><author><keyname>Joshi</keyname><forenames>Nisheeth</forenames></author></authors><title>Classifier-Based Text Simplification for Improved Machine Translation</title><categories>cs.CL</categories><comments>In Proceedings of International Conference on Advances in Computer
  Engineering and Applications 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine Translation is one of the research fields of Computational
Linguistics. The objective of many MT Researchers is to develop an MT System
that produce good quality and high accuracy output translations and which also
covers maximum language pairs. As internet and Globalization is increasing day
by day, we need a way that improves the quality of translation. For this
reason, we have developed a Classifier based Text Simplification Model for
English-Hindi Machine Translation Systems. We have used support vector machines
and Na\&quot;ive Bayes Classifier to develop this model. We have also evaluated the
performance of these classifiers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03225</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03225</id><created>2015-07-12</created><updated>2016-01-07</updated><authors><author><keyname>Pagh</keyname><forenames>Rasmus</forenames></author></authors><title>CoveringLSH: Locality-sensitive Hashing without False Negatives</title><categories>cs.DS</categories><comments>Short version appears in Proceedings of SODA 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a new construction of locality-sensitive hash functions for
Hamming space that is \emph{covering} in the sense that is it guaranteed to
produce a collision for every pair of vectors within a given radius $r$. The
construction is \emph{efficient} in the sense that the expected number of hash
collisions between vectors at distance~$cr$, for a given $c&gt;1$, comes close to
that of the best possible data independent LSH without the covering guarantee,
namely, the seminal LSH construction of Indyk and Motwani (STOC '98). The
efficiency of the new construction essentially \emph{matches} their bound when
the search radius is not too large --- e.g., when $cr = o(\log(n)/\log\log n)$,
where $n$ is the number of points in the data set, and when $cr = \log(n)/k$
where $k$ is an integer constant. In general, it differs by at most a factor
$\ln(4)$ in the exponent of the time bounds. As a consequence, LSH-based
similarity search in Hamming space can avoid the problem of false negatives at
little or no cost in efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03229</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03229</id><created>2015-07-12</created><authors><author><keyname>Suzumura</keyname><forenames>Shinya</forenames></author><author><keyname>Ogawa</keyname><forenames>Kohei</forenames></author><author><keyname>Sugiyama</keyname><forenames>Masashi</forenames></author><author><keyname>Karasuyama</keyname><forenames>Masayuki</forenames></author><author><keyname>Takeuchi</keyname><forenames>Ichiro</forenames></author></authors><title>Homotopy Continuation Approaches for Robust SV Classification and
  Regression</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In support vector machine (SVM) applications with unreliable data that
contains a portion of outliers, non-robustness of SVMs often causes
considerable performance deterioration. Although many approaches for improving
the robustness of SVMs have been studied, two major challenges remain in robust
SVM learning. First, robust learning algorithms are essentially formulated as
non-convex optimization problems. It is thus important to develop a non-convex
optimization method for robust SVM that can find a good local optimal solution.
The second practical issue is how one can tune the hyperparameter that controls
the balance between robustness and efficiency. Unfortunately, due to the
non-convexity, robust SVM solutions with slightly different hyper-parameter
values can be significantly different, which makes model selection highly
unstable. In this paper, we address these two issues simultaneously by
introducing a novel homotopy approach to non-convex robust SVM learning. Our
basic idea is to introduce parametrized formulations of robust SVM which bridge
the standard SVM and fully robust SVM via the parameter that represents the
influence of outliers. We characterize the necessary and sufficient conditions
of the local optimal solutions of robust SVM, and develop an algorithm that can
trace a path of local optimal solutions when the influence of outliers is
gradually decreased. An advantage of our homotopy approach is that it can be
interpreted as simulated annealing, a common approach for finding a good local
optimal solution in non-convex optimization problems. In addition, our homotopy
method allows stable and efficient model selection based on the path of local
optimal solutions. Empirical performances of the proposed approach are
demonstrated through intensive numerical experiments both on robust
classification and regression problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03243</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03243</id><created>2015-07-12</created><updated>2015-08-04</updated><authors><author><keyname>Nazari</keyname><forenames>Sam</forenames></author><author><keyname>Shafai</keyname><forenames>Bahram</forenames></author><author><keyname>Oghbaee</keyname><forenames>Amirreza</forenames></author></authors><title>Positive Unknown Input Observer For Positive Linear Systems</title><categories>cs.SY</categories><comments>This paper has been withdrawn by the authors due to errors in
  equation 25 and 33</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Positive systems are important class of dynamic systems with impressive
properties. The response of such systems to positive initial conditions and
positive inputs remain in the nonnegative orthant of the state space. Although
positive observers have been designed for positive systems, they are unable to
estimate the states when unknown inputs or disturbances are present in the
systems. This paper is a first attempt to design positive unknown input
observers (PUIO) for positive linear systems. The structural constraints on
observer parameters make the design task cumbersome. However, with the aid of a
positive stabilization scheme via LMI and by imposing conditions on positivity
of the generalized inverse associated with a certain design matrix, we provide
a reliable procedure for the design of PUIOs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03255</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03255</id><created>2015-07-12</created><authors><author><keyname>Shmuel</keyname><forenames>Ori</forenames></author><author><keyname>Cohen</keyname><forenames>Asaf</forenames></author><author><keyname>Gurewitz</keyname><forenames>Omer</forenames></author></authors><title>Capacity and performance analysis for multi-user system under
  distributed opportunistic scheduling in a time dependent channel</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the problem of a multi-user multiple access channel. While several
multi-user coding techniques exist, in practical scenarios, not all users can
be scheduled simultaneously. Thus, a key problem is which users to schedule in
a given time slot. Under realistic approach for time dependency of the channel,
we adopt a distributed scheduling algorithm in which each user, in the
beginning of each slot, estimates his channel gain and compares it to a
threshold, and if exceeding it the user can transmit. In this work we are
interested in the expected capacity of the system and the delay and quality of
service of the data accumulated at the users under this scheduling scheme.
First we derive the expected capacity under scheduling (distributed and
centralized) for this time dependent environment and show that its scaling law
is $O(\sigma_g\sqrt{2\log K}+\mu_g)$, were $\sigma_g, \mu_g$ are the good
channel parameters (assuming Gaussian capacity approximation, e.g., under MIMO)
and $K$ is the number of users. Then we turn to the performance analysis of
such system while assuming the users are not necessarily fully backlogged, and
focus specifically on the queueing problem and the strong dependence between
the queues which leave no alternative but to turn to approximate models for
this system. We adopt the celebrated model of Ephremides and Zhu to give new
results on the convergence of the probability of collision to its average value
(as the number of users grows), and hence for the ensuing system performance
metrics, such as throughput and delay. We further utilize this finding to
suggest a much simpler approximate model, which accurately describes the system
behavior when the number of queues is large. The system performance as
predicted by the approximate models shows excellent agreement with simulation
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03257</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03257</id><created>2015-07-12</created><updated>2015-10-12</updated><authors><author><keyname>Voskoglou</keyname><forenames>Michael</forenames></author></authors><title>Use of the Triangular Fuzzy Numbers for Student Assessment</title><categories>cs.AI</categories><comments>9 pages, 2 figures</comments><msc-class>03E72</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In an earlier work we have used the Triangular Fuzzy Numbers (TFNs)as an
assessment tool of student skills.This approach led to an approximate
linguistic characterization of the students' overall performance, but it was
not proved to be sufficient in all cases for comparing the performance of two
different student groups, since tywo TFNs are not always comparable. In the
present paper we complete the above fuzzy assessment approach by presenting a
defuzzification method of TFNS based on the Center of Gravity (COG) technique,
which enables the required comparison. In addition we extend our results by
using the Trapezoidal Fuzzy Numbers (TpFNs) too, which are a generalization of
the TFNs, for student assessment and we present suitable examples illustrating
our new results in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03262</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03262</id><created>2015-07-12</created><authors><author><keyname>Kerjean</keyname><forenames>Marie</forenames><affiliation>PPS</affiliation></author><author><keyname>Tasson</keyname><forenames>Christine</forenames><affiliation>PPS</affiliation></author></authors><title>Mackey-complete spaces and power series -- A topological model of
  Differential Linear Logic</title><categories>cs.LO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we have described a denotational model of Intuitionist Linear
Logic which is also a differential category. Formulas are interpreted as
Mackey-complete topological vector space and linear proofs are interpreted by
bounded linear functions. So as to interpret non-linear proofs of Linear Logic,
we have used a notion of power series between Mackey-complete spaces,
generalizing the notion of entire functions in C. Finally, we have obtained a
quantitative model of Intuitionist Differential Linear Logic, where the
syntactic differentiation correspond to the usual one and where the
interpretations of proofs satisfy a Taylor expansion decomposition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03269</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03269</id><created>2015-07-12</created><authors><author><keyname>Hopkins</keyname><forenames>Samuel B.</forenames></author><author><keyname>Shi</keyname><forenames>Jonathan</forenames></author><author><keyname>Steurer</keyname><forenames>David</forenames></author></authors><title>Tensor principal component analysis via sum-of-squares proofs</title><categories>cs.LG cs.CC cs.DS stat.ML</categories><comments>published in Conference on Learning Theory (COLT) 2015 (submitted
  February 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a statistical model for the tensor principal component analysis
problem introduced by Montanari and Richard: Given a order-$3$ tensor $T$ of
the form $T = \tau \cdot v_0^{\otimes 3} + A$, where $\tau \geq 0$ is a
signal-to-noise ratio, $v_0$ is a unit vector, and $A$ is a random noise
tensor, the goal is to recover the planted vector $v_0$. For the case that $A$
has iid standard Gaussian entries, we give an efficient algorithm to recover
$v_0$ whenever $\tau \geq \omega(n^{3/4} \log(n)^{1/4})$, and certify that the
recovered vector is close to a maximum likelihood estimator, all with high
probability over the random choice of $A$. The previous best algorithms with
provable guarantees required $\tau \geq \Omega(n)$.
  In the regime $\tau \leq o(n)$, natural tensor-unfolding-based spectral
relaxations for the underlying optimization problem break down (in the sense
that their integrality gap is large). To go beyond this barrier, we use convex
relaxations based on the sum-of-squares method. Our recovery algorithm proceeds
by rounding a degree-$4$ sum-of-squares relaxations of the
maximum-likelihood-estimation problem for the statistical model. To complement
our algorithmic results, we show that degree-$4$ sum-of-squares relaxations
break down for $\tau \leq O(n^{3/4}/\log(n)^{1/4})$, which demonstrates that
improving our current guarantees (by more than logarithmic factors) would
require new techniques or might even be intractable.
  Finally, we show how to exploit additional problem structure in order to
solve our sum-of-squares relaxations, up to some approximation, very
efficiently. Our fastest algorithm runs in nearly-linear time using shifted
(matrix) power iteration and has similar guarantees as above. The analysis of
this algorithm also confirms a variant of a conjecture of Montanari and Richard
about singular vectors of tensor unfoldings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03274</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03274</id><created>2015-07-12</created><updated>2015-07-20</updated><authors><author><keyname>Chung</keyname><forenames>Yeounoh</forenames></author><author><keyname>Zamanian</keyname><forenames>Erfan</forenames></author></authors><title>Using RDMA for Lock Management</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we aim to evaluate different Distributed Lock Management
service designs with Remote Direct Memory Access (RDMA). In specific, we
implement and evaluate the centralized and the RDMA-enabled lock manager
designs for fast network settings. Experimental results confirms a couple of
hypotheses. First, in the traditional centralized lock manager design, CPU is
the bottleneck and bypassing CPU on client-to-server communication using RDMA
results in better lock service perofrmance. Second, different lock manager
designs with RDMA in consideration result in even better performance; we need
to re-design lock management system for RDMA and fast networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03278</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03278</id><created>2015-07-12</created><authors><author><keyname>Kandiah</keyname><forenames>V.</forenames></author><author><keyname>Escaith</keyname><forenames>H.</forenames></author><author><keyname>Shepelyansky</keyname><forenames>D. L.</forenames></author></authors><title>Contagion effects in the world network of economic activities</title><categories>q-fin.ST cs.SI physics.soc-ph</categories><comments>this work is linked with arXiv:1504.06773 [q-fin.ST]</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using the new data from the OECD-WTO world network of economic activities we
construct the Google matrix $G$ of this directed network and perform its
detailed analysis. The network contains 58 countries and 37 activity sectors
for years 1995, 2000, 2005, 2008, 2009. The construction of $G$, based on
Markov chain transitions, treats all countries on equal democratic grounds
while the contribution of activity sectors is proportional to their exchange
monetary volume. The Google matrix analysis allows to obtain reliable ranking
of countries and activity sectors and to determine the sensitivity of
CheiRank-PageRank commercial balance of countries in respect to price
variations and labor cost in various countries. We demonstrate that the
developed approach takes into account multiplicity of network links with
economy interactions between countries and activity sectors thus being more
efficient compared to the usual export-import analysis. Our results highlight
the striking increase of the influence of German economic activity on other
countries during the period 1995 to 2009 while the influence of Eurozone
decreases during the same period. We compare our results with the similar
analysis of the world trade network from the UN COMTRADE database. We argue
that the knowledge of network structure allows to analyze the effects of
economic influence and contagion propagation over the world economy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03286</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03286</id><created>2015-07-12</created><authors><author><keyname>Garrousian</keyname><forenames>Mehdi</forenames></author><author><keyname>Tohaneanu</keyname><forenames>Stefan</forenames></author></authors><title>Minimum distance of linear codes and the $\alpha$-invariant</title><categories>math.AC cs.IT math.IT</categories><comments>12 pages</comments><msc-class>68W30 (Primary) 16W70, 52C35, 11T71 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The simple interpretation of the minimum distance of a linear code obtained
by De Boer and Pellikaan, and later refined by the second author, is further
developed through the study of various finitely generated graded modules. We
use the methods of commutative/homological algebra to find connections between
the minimum distance and the $\alpha$-invariant of such modules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03289</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03289</id><created>2015-07-12</created><authors><author><keyname>Yu</keyname><forenames>Jingjin</forenames></author><author><keyname>LaValle</keyname><forenames>Steven M.</forenames></author></authors><title>Optimal Multi-Robot Path Planning on Graphs: Structure and Computational
  Complexity</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of optimal multi-robot path planning on graphs (MPP)
over four distinct minimization objectives: the total arrival time, the
makespan (last arrival time), the total distance, and the maximum (single-robot
traveled) distance. On the structure side, we show that each pair of these four
objectives induces a Pareto front and cannot always be optimized
simultaneously. Then, through reductions from 3-SAT, we further establish that
computation over each objective is an NP-hard task, providing evidence that
solving MPP optimally is generally intractable. Nevertheless, in a related
paper, we design complete algorithms and efficient heuristics for optimizing
all four objectives, capable of solving MPP optimally or near-optimally for
hundreds of robots in challenging setups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03290</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03290</id><created>2015-07-12</created><authors><author><keyname>Yu</keyname><forenames>Jingjin</forenames></author><author><keyname>LaValle</keyname><forenames>Steven M.</forenames></author></authors><title>Optimal Multi-Robot Path Planning on Graphs: Complete Algorithms and
  Effective Heuristics</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of optimal multi-robot path planning on graphs MPP over
four distinct minimization objectives: the makespan (last arrival time), the
maximum (single-robot traveled) distance, the total arrival time, and the total
distance. In a related paper, we show that these objectives are distinct and
NP-hard to optimize. In this work, we focus on efficiently algorithmic
solutions for solving these optimal MPP problems. Toward this goal, we first
establish a one-to-one solution mapping between MPP and network-flow. Based on
this equivalence and integer linear programming (ILP), we design novel and
complete algorithms for optimizing over each of the four objectives. In
particular, our exact algorithm for computing optimal makespan solutions is a
first such that is capable of solving extremely challenging problems with
robot-vertex ratio as high as 100%. Then, we further improve the computational
performance of these exact algorithms through the introduction of principled
heuristics, at the expense of some optimality loss. The combination of ILP
model based algorithms and the heuristics proves to be highly effective,
allowing the computation of 1.x-optimal solutions for problems containing
hundreds of robots, densely populated in the environment, often in just
seconds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03292</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03292</id><created>2015-07-12</created><updated>2016-01-21</updated><authors><author><keyname>Jeong</keyname><forenames>Jaeseong</forenames></author><author><keyname>Leconte</keyname><forenames>Mathieu</forenames></author><author><keyname>Proutiere</keyname><forenames>Alexandre</forenames></author></authors><title>Cluster-Aided Mobility Predictions</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Predicting the future location of users in wireless net- works has numerous
applications, and can help service providers to improve the quality of service
perceived by their clients. The location predictors proposed so far estimate
the next location of a specific user by inspecting the past individual
trajectories of this user. As a consequence, when the training data collected
for a given user is limited, the resulting prediction is inaccurate. In this
paper, we develop cluster-aided predictors that exploit past trajectories
collected from all users to predict the next location of a given user. These
predictors rely on clustering techniques and extract from the training data
similarities among the mobility patterns of the various users to improve the
prediction accuracy. Specifically, we present CAMP (Cluster-Aided Mobility
Predictor), a cluster-aided predictor whose design is based on recent
non-parametric bayesian statistical tools. CAMP is robust and adaptive in the
sense that it exploits similarities in users' mobility only if such
similarities are really present in the training data. We analytically prove the
consistency of the predictions provided by CAMP, and investigate its
performance using two large-scale datasets. CAMP significantly outperforms
existing predictors, and in particular those that only exploit individual past
trajectories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03303</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03303</id><created>2015-07-12</created><authors><author><keyname>Li</keyname><forenames>Yang</forenames><affiliation>Carnegie Mellon University</affiliation></author><author><keyname>Choi</keyname><forenames>Jongmoo</forenames><affiliation>Dankook University</affiliation></author><author><keyname>Sun</keyname><forenames>Jin</forenames><affiliation>Carnegie Mellon University</affiliation></author><author><keyname>Ghose</keyname><forenames>Saugata</forenames><affiliation>Carnegie Mellon University</affiliation></author><author><keyname>Wang</keyname><forenames>Hui</forenames><affiliation>Beihang University</affiliation></author><author><keyname>Meza</keyname><forenames>Justin</forenames><affiliation>Carnegie Mellon University</affiliation></author><author><keyname>Ren</keyname><forenames>Jinglei</forenames><affiliation>Tsinghua University</affiliation></author><author><keyname>Mutlu</keyname><forenames>Onur</forenames><affiliation>Carnegie Mellon University</affiliation></author></authors><title>Managing Hybrid Main Memories with a Page-Utility Driven Performance
  Model</title><categories>cs.AR</categories><report-no>SAFARI Technical Report No. 2015-010</report-no><acm-class>B.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hybrid memory systems comprised of dynamic random access memory (DRAM) and
non-volatile memory (NVM) have been proposed to exploit both the capacity
advantage of NVM and the latency and dynamic energy advantages of DRAM. An
important problem for such systems is how to place data between DRAM and NVM to
improve system performance.
  In this paper, we devise the first mechanism, called UBM (page Utility Based
hybrid Memory management), that systematically estimates the system performance
benefit of placing a page in DRAM versus NVM and uses this estimate to guide
data placement. UBM's estimation method consists of two major components.
First, it estimates how much an application's stall time can be reduced if the
accessed page is placed in DRAM. To do this, UBM comprehensively considers
access frequency, row buffer locality, and memory level parallelism (MLP) to
estimate the application's stall time reduction. Second, UBM estimates how much
each application's stall time reduction contributes to overall system
performance. Based on this estimation method, UBM can determine and place the
most critical data in DRAM to directly optimize system performance.
Experimental results show that UBM improves system performance by 14% on
average (and up to 39%) compared to the best of three state-of-the-art
mechanisms for a large number of data-intensive workloads from the SPEC CPU2006
and Yahoo Cloud Serving Benchmark (YCSB) suites.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03304</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03304</id><created>2015-07-12</created><updated>2015-11-05</updated><authors><author><keyname>Hague</keyname><forenames>Matthew</forenames></author><author><keyname>Kochems</keyname><forenames>Jonathan</forenames></author><author><keyname>Ong</keyname><forenames>C. -H. Luke</forenames></author></authors><title>Unboundedness and Downward Closures of Higher-Order Pushdown Automata</title><categories>cs.FL</categories><acm-class>F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show the diagonal problem for higher-order pushdown automata (HOPDA), and
hence the simultaneous unboundedness problem, is decidable. From recent work by
Zetzsche this means that we can construct the downward closure of the set of
words accepted by a given HOPDA. This also means we can construct the downward
closure of the Parikh image of a HOPDA. Both of these consequences play an
important role in verifying concurrent higher-order programs expressed as HOPDA
or safe higher-order recursion schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03314</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03314</id><created>2015-07-12</created><authors><author><keyname>Olensky</keyname><forenames>Marlies</forenames></author><author><keyname>Schmidt</keyname><forenames>Marion</forenames></author><author><keyname>van Eck</keyname><forenames>Nees Jan</forenames></author></authors><title>Evaluation of the citation matching algorithms of CWTS and iFQ in
  comparison to Web of Science</title><categories>cs.DL</categories><comments>28 pages, 7 tables, 5 figures. The paper is accepted for publication
  in the Journal of the Association for Information Science and Technology
  (JASIST)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The results of bibliometric studies provided by bibliometric research groups,
e.g. the Centre for Science and Technology Studies (CWTS) and the Institute for
Research Information and Quality Assurance (iFQ), are often used in the process
of research assessment. Their databases use Web of Science (WoS) citation data,
which they match according to their own matching algorithms - in the case of
CWTS for standard usage in their studies and in the case of iFQ on an
experimental basis. Since the problem of non-matched citations in WoS persists
because of inaccuracies in the references or inaccuracies introduced in the
data extraction process, it is important to ascertain how well these
inaccuracies are rectified in these citation matching algorithms. This paper
evaluates the algorithms of CWTS and iFQ in comparison to WoS in a quantitative
and a qualitative analysis. The analysis builds upon the methodology and the
manually verified corpus of a previous study. The algorithm of CWTS performs
best, closely followed by that of iFQ. The WoS algorithm still performs quite
well (F1 score: 96.41 percent), but shows deficits in matching references
containing inaccuracies. An additional problem is posed by incorrectly provided
cited reference information in source articles by WoS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03323</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03323</id><created>2015-07-13</created><authors><author><keyname>Li</keyname><forenames>Bo</forenames></author><author><keyname>Qi</keyname><forenames>Hongsheng</forenames></author><author><keyname>Shi</keyname><forenames>Guodong</forenames></author></authors><title>Randomized Boolean Gossiping</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose and study a randomized Boolean gossiping process,
where nodes taking value from $\{0,1\}$ pairwise meet over an underlying graph
in a random manner at each time step and the two interacting nodes update their
states by random logistic rules drawn from the set $\{{\it AND}, {\it OR}\}$.
This model is a generalization of the classical gossiping process and serves as
a simplified probabilistic Boolean network. First of all, using standard
theories from Markov chain we show that almost surely, the network state
asymptotically converge to a consensus. We also establish a characterization of
the distribution of this limit for large-scale networks with all-to-all
communication in light of mean-field approximation methods. Next, we study how
the number of communication classes in the network state space relates to the
topology of the underlying interaction graph and obtain a full
characterization: A line interaction graph with $n$ nodes generates $2n$
communication classes; A cycle graph with $2n$ nodes generates $n+3$
communication classes, and a cycle graph with $2n+1$ nodes generates $n+2$
communication classes; For any connected graph which is not a line or a cycle,
the number of communication classes is either five or three, where three is
achieved if and only if the graph contains an odd cycle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03325</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03325</id><created>2015-07-13</created><authors><author><keyname>Zhang</keyname><forenames>Zhao</forenames></author><author><keyname>Barbary</keyname><forenames>Kyle</forenames></author><author><keyname>Nothaft</keyname><forenames>Frank Austin</forenames></author><author><keyname>Sparks</keyname><forenames>Evan</forenames></author><author><keyname>Zahn</keyname><forenames>Oliver</forenames></author><author><keyname>Franklin</keyname><forenames>Michael J.</forenames></author><author><keyname>Patterson</keyname><forenames>David A.</forenames></author><author><keyname>Perlmutter</keyname><forenames>Saul</forenames></author></authors><title>Scientific Computing Meets Big Data Technology: An Astronomy Use Case</title><categories>cs.DC</categories><acm-class>D.1.3; J.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scientific analyses commonly compose multiple single-process programs into a
dataflow. An end-to-end dataflow of single-process programs is known as a
many-task application. Typically, tools from the HPC software stack are used to
parallelize these analyses. In this work, we investigate an alternate approach
that uses Apache Spark -- a modern big data platform -- to parallelize
many-task applications. We present Kira, a flexible and distributed astronomy
image processing toolkit using Apache Spark. We then use the Kira toolkit to
implement a Source Extractor application for astronomy images, called Kira SE.
With Kira SE as the use case, we study the programming flexibility, dataflow
richness, scheduling capacity and performance of Apache Spark running on the
EC2 cloud. By exploiting data locality, Kira SE achieves a 3.7x speedup over an
equivalent C program when analyzing a 1TB dataset using 512 cores on the Amazon
EC2 cloud. Furthermore, we show that by leveraging software originally designed
for big data infrastructure, Kira SE achieves competitive performance to the C
implementation running on the NERSC Edison supercomputer. Our experience with
Kira indicates that emerging Big Data platforms such as Apache Spark are a
performant alternative for many-task scientific applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03328</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03328</id><created>2015-07-13</created><authors><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Li</keyname><forenames>Fu</forenames></author><author><keyname>Lin</keyname><forenames>Tian</forenames></author><author><keyname>Rubinstein</keyname><forenames>Aviad</forenames></author></authors><title>Combining Traditional Marketing and Viral Marketing with Amphibious
  Influence Maximization</title><categories>cs.SI</categories><comments>An extended abstract appeared in the Proceedings of the 16th ACM
  Conference on Economics and Computation (EC), 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose the amphibious influence maximization (AIM) model
that combines traditional marketing via content providers and viral marketing
to consumers in social networks in a single framework. In AIM, a set of content
providers and consumers form a bipartite network while consumers also form
their social network, and influence propagates from the content providers to
consumers and among consumers in the social network following the independent
cascade model. An advertiser needs to select a subset of seed content providers
and a subset of seed consumers, such that the influence from the seed providers
passing through the seed consumers could reach a large number of consumers in
the social network in expectation.
  We prove that the AIM problem is NP-hard to approximate to within any
constant factor via a reduction from Feige's k-prover proof system for 3-SAT5.
We also give evidence that even when the social network graph is trivial (i.e.
has no edges), a polynomial time constant factor approximation for AIM is
unlikely. However, when we assume that the weighted bi-adjacency matrix that
describes the influence of content providers on consumers is of constant rank,
a common assumption often used in recommender systems, we provide a
polynomial-time algorithm that achieves approximation ratio of
$(1-1/e-\epsilon)^3$ for any (polynomially small) $\epsilon &gt; 0$. Our
algorithmic results still hold for a more general model where cascades in
social network follow a general monotone and submodular function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03331</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03331</id><created>2015-07-13</created><updated>2016-03-03</updated><authors><author><keyname>Magron</keyname><forenames>Victor</forenames></author><author><keyname>Constantinides</keyname><forenames>George</forenames></author><author><keyname>Donaldson</keyname><forenames>Alastair</forenames></author></authors><title>Certified Roundoff Error Bounds Using Semidefinite Programming</title><categories>cs.NA</categories><comments>32 pages, 8 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Roundoff errors cannot be avoided when implementing numerical programs with
finite precision. The ability to reason about rounding is especially important
if one wants to explore a range of potential representations, for instance for
FPGAs or custom hardware implementation. This problem becomes challenging when
the program does not employ solely linear operations as non-linearities are
inherent to many interesting computational problems in real-world applications.
  Existing solutions to reasoning are limited in the presence of nonlinear
correlations between variables, leading to either imprecise bounds or high
analysis time. Furthermore, while it is easy to implement a straightforward
method such as interval arithmetic, sophisticated techniques are less
straightforward to implement in a formal setting. Thus there is a need for
methods which output certificates that can be formally validated inside a proof
assistant.
  We present a framework to provide upper bounds on absolute roundoff errors.
This framework is based on optimization techniques employing semidefinite
programming and sums of squares certificates, which can be formally checked
inside the Coq theorem prover. Our tool covers a wide range of nonlinear
programs, including polynomials and transcendental operations as well as
conditional statements. We illustrate the efficiency and precision of this tool
on non-trivial programs coming from biology, optimization and space control.
Our tool produces more precise error bounds for 37 percent of all programs and
yields better performance in 73 percent of all programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03338</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03338</id><created>2015-07-13</created><authors><author><keyname>Saroufim</keyname><forenames>Mark</forenames></author></authors><title>Aren't we all nearest neighbors: Spatial trees, high dimensional
  reductions and batch nearest neighbor search</title><categories>cs.CG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We start with a review of the pervasiveness of the nearest neighbor search
problem and techniques used to solve it along with some experimental results.
In the second chapter, we show reductions between two different classes of geo-
metric proximity problems: the nearest neighbor problems to solve the Euclidean
minimum spanning tree problem and the farthest neighbor problems to solve the
k-centers problem. In the third chapter, we unify spatial partitioning trees
un- der one framework the meta-tree. Finally, we propose a dual tree algorithm
for Bichromatic Closest Pair and measure the complexity of batch nearest
neighbor search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03340</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03340</id><created>2015-07-13</created><authors><author><keyname>Ansari</keyname><forenames>Zahid</forenames></author><author><keyname>Azeem</keyname><forenames>M. F.</forenames></author><author><keyname>Ahmed</keyname><forenames>Waseem</forenames></author><author><keyname>Babu</keyname><forenames>A. Vinaya</forenames></author></authors><title>Quantitative Evaluation of Performance and Validity Indices for
  Clustering the Web Navigational Sessions</title><categories>cs.LG cs.SI</categories><journal-ref>World of Computer Science and Information Technology Journal pp.
  217-226, Vol. 1, No. 5, June 2011. (ISSN: 2221- 0741, WCSIT Publisher, Unites
  States)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clustering techniques are widely used in Web Usage Mining to capture similar
interests and trends among users accessing a Web site. For this purpose, web
access logs generated at a particular web site are preprocessed to discover the
user navigational sessions. Clustering techniques are then applied to group the
user session data into user session clusters, where intercluster similarities
are minimized while the intra cluster similarities are maximized. Since the
application of different clustering algorithms generally results in different
sets of cluster formation, it is important to evaluate the performance of these
methods in terms of accuracy and validity of the clusters, and also the time
required to generate them, using appropriate performance measures. This paper
describes various validity and accuracy measures including Dunn's Index, Davies
Bouldin Index, C Index, Rand Index, Jaccard Index, Silhouette Index, Fowlkes
Mallows and Sum of the Squared Error (SSE). We conducted the performance
evaluation of the following clustering techniques: k-Means, k-Medoids, Leader,
Single Link Agglomerative Hierarchical and DBSCAN. These techniques are
implemented and tested against the Web user navigational data. Finally their
performance results are presented and compared.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03344</identifier>
 <datestamp>2015-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03344</id><created>2015-07-13</created><authors><author><keyname>Wang</keyname><forenames>Yong</forenames></author></authors><title>Entanglement in Reversible Quantum Computing</title><categories>cs.LO</categories><comments>arXiv admin note: substantial text overlap with arXiv:1501.05260,
  arXiv:1404.0665, arXiv:1311.2960, arXiv:1410.5131</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explicitly model entanglement as a kind of parallelism under the framework
of reversible quantum computing. A sound and complete axiomatization is
founded. Different to modeling of entanglement in quantum computing, the shadow
constant is unnecessary. And also, as a kind of parallelism, entanglement merge
is also quite different.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03348</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03348</id><created>2015-07-13</created><updated>2015-12-28</updated><authors><author><keyname>Chandoo</keyname><forenames>Maurice</forenames></author></authors><title>Deciding Circular-Arc Graph Isomorphism in Parameterized Logspace</title><categories>cs.DS cs.DM</categories><comments>14 pages, 3 figures</comments><acm-class>G.2.2</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We compute a canonical circular-arc representation for a given circular-arc
(CA) graph which implies solving the isomorphism and recognition problem for
this class. To accomplish this we split the class of CA graphs into uniform and
non-uniform ones and employ a generalized version of the argument given by
K\&quot;obler et al (2013) that has been used to show that the subclass of Helly CA
graphs can be canonized in logspace. For uniform CA graphs our approach works
in logspace and in addition to that Helly CA graphs are a strict subset of
uniform CA graphs. Thus our result is a generalization of the canonization
result for Helly CA graphs. In the non-uniform case a specific set of ambiguous
vertices arises. By choosing the parameter to be the cardinality of this set
the obstacle can be solved by brute force. This leads to an O(k + log n) space
algorithm to compute a canonical representation for non-uniform and therefore
all CA graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03351</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03351</id><created>2015-07-13</created><authors><author><keyname>Ray</keyname><forenames>Nicolas</forenames></author><author><keyname>Sokolov</keyname><forenames>Dmitry</forenames></author></authors><title>On Smooth 3D Frame Field Design</title><categories>cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze actual methods that generate smooth frame fields both in 2D and in
3D. We formalize the 2D problem by representing frames as functions (as it was
done in 3D), and show that the derived optimization problem is the one that
previous work obtain via &quot;representation vectors.&quot; We show (in 2D) why this non
linear optimization problem is easier to solve than directly minimizing the
rotation angle of the field, and observe that the 2D algorithm is able to find
good fields.
  Now, the 2D and the 3D optimization problems are derived from the same
formulation (based on representing frames by functions). Their energies share
some similarities from an optimization point of view (smoothness, local minima,
bounds of partial derivatives, etc.), so we applied the 2D resolution mechanism
to the 3D problem. Our evaluation of all existing 3D methods suggests to
initialize the field by this new algorithm, but possibly use another method for
further smoothing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03352</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03352</id><created>2015-07-13</created><authors><author><keyname>S&#xe1;nchez</keyname><forenames>Jos&#xe9; Manuel</forenames></author><author><keyname>Yahia</keyname><forenames>Imen Grida Ben</forenames></author><author><keyname>Crespi</keyname><forenames>Noel</forenames></author></authors><title>Self-Modeling Based Diagnosis of Software-Defined Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Networks built using SDN (Software-Defined Networks) and NFV (Network
Functions Virtualization) approaches are expected to face several challenges
such as scalability, robustness and resiliency. In this paper, we propose a
self-modeling based diagnosis to enable resilient networks in the context of
SDN and NFV. We focus on solving two major problems: On the one hand, we lack
today of a model or template that describes the managed elements in the context
of SDN and NFV. On the other hand, the highly dynamic networks enabled by the
softwarisation require the generation at runtime of a diagnosis model from
which the root causes can be identified. In this paper, we propose finer
granular templates that do not only model network nodes but also their
sub-components for a more detailed diagnosis suitable in the SDN and NFV
context. In addition, we specify and validate a self-modeling based diagnosis
using Bayesian Networks. This approach differs from the state of the art in the
discovery of network and service dependencies at run-time and the building of
the diagnosis model of any SDN infrastructure using our templates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03360</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03360</id><created>2015-07-13</created><authors><author><keyname>Gaur</keyname><forenames>Charu</forenames></author><author><keyname>Mohan</keyname><forenames>Baranidharan</forenames></author><author><keyname>Khare</keyname><forenames>Kedar</forenames></author></authors><title>Sparsity assisted solution to the twin image problem in phase retrieval</title><categories>cs.CV physics.optics</categories><doi>10.1364/JOSAA.32.001922</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The iterative phase retrieval problem for complex-valued objects from Fourier
transform magnitude data is known to suffer from the twin image problem. In
particular, when the object support is centro-symmetric, the iterative solution
often stagnates such that the resultant complex image contains the features of
both the desired solution and its inverted and complex-conjugated replica. The
conventional approach to address the twin image problem is to modify the object
support during initial iterations which can possibly lead to elimination of one
of the twin images. However, at present there seems to be no deterministic
procedure to make sure that the twin image will always be very weak or absent.
In this work we make an important observation that the ideal solution without
the twin image is typically more sparse (in some suitable transform domain) as
compared to the stagnated solution containing the twin image. We further show
that introducing a sparsity enhancing step in the iterative algorithm can
address the twin image problem without the need to change the object support
throughout the iterative process even when the object support is
centro-symmetric. In a simulation study, we use binary and gray-scale pure
phase objects and illustrate the effectiveness of the sparsity assisted phase
recovery in the context of the twin image problem. The results have important
implications for a wide range of topics in Physics where the phase retrieval
problem plays a central role.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03372</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03372</id><created>2015-07-13</created><updated>2015-12-28</updated><authors><author><keyname>Martino</keyname><forenames>Giovanni Da San</forenames></author><author><keyname>Navarin</keyname><forenames>Nicol&#xf2;</forenames></author><author><keyname>Sperduti</keyname><forenames>Alessandro</forenames></author></authors><title>Ordered Decompositional DAG Kernels Enhancements</title><categories>cs.LG</categories><comments>Paper accepted for publication in Neurocomputing</comments><journal-ref>Neurocomputing, 2016</journal-ref><doi>10.1016/j.neucom.2015.12.110</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we show how the Ordered Decomposition DAGs (ODD) kernel
framework, a framework that allows the definition of graph kernels from tree
kernels, allows to easily define new state-of-the-art graph kernels. Here we
consider a fast graph kernel based on the Subtree kernel (ST), and we propose
various enhancements to increase its expressiveness. The proposed DAG kernel
has the same worst-case complexity as the one based on ST, but an improved
expressivity due to an augmented set of features. Moreover, we propose a novel
weighting scheme for the features, which can be applied to other kernels of the
ODD framework. These improvements allow the proposed kernels to improve on the
classification performances of the ST-based kernel for several real-world
datasets, reaching state-of-the-art performances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03376</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03376</id><created>2015-07-13</created><authors><author><keyname>M&#xe9;tivier</keyname><forenames>Y.</forenames></author><author><keyname>Robson</keyname><forenames>J. M.</forenames></author><author><keyname>Zemmari</keyname><forenames>A.</forenames></author></authors><title>A Distributed Enumeration Algorithm and Applications to All Pairs
  Shortest Paths, Diameter</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the standard message passing model; we assume the system is fully
synchronous: all processes start at the same time and time proceeds in
synchronised rounds. In each round each vertex can transmit a different message
of size $O(1)$ to each of its neighbours. This paper proposes and analyses a
distributed enumeration algorithm of vertices of a graph having a distinguished
vertex which satisfies that two vertices with consecutive numbers are at
distance at most $3$. We prove that its time complexity is $O(n)$ where $n$ is
the number of vertices of the graph. Furthermore, the size of each message is
$O(1)$ thus its bit complexity is also $O(n).$ We provide some links between
this enumeration and Hamiltonian graphs from which we deduce that this
enumeration is optimal in the sense that there does not exist an enumeration
which satisfies that two vertices with consecutive numbers are at distance at
most $2$.
  We deduce from this enumeration algorithms which compute all pairs shortest
paths and the diameter with a time complexity and a bit complexity equal to
$O(n)$. This improves the best known distributed algorithms (under the same
hypotheses) for computing all pairs shortest paths or the diameter presented in
\cite{PRT12,HW12} having a time complexity equal to $O(n)$ and which use
messages of size $O(\log n)$ bits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03388</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03388</id><created>2015-07-13</created><authors><author><keyname>Rodriguez</keyname><forenames>Saul</forenames></author><author><keyname>Ollmar</keyname><forenames>Stig</forenames></author><author><keyname>Waqar</keyname><forenames>Muhammad</forenames></author><author><keyname>Rusu</keyname><forenames>Ana</forenames></author></authors><title>A Batteryless Sensor ASIC for Implantable Bio-impedance Applications</title><categories>cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The measurement of the biological tissue's electrical impedance is an active
research field that has attracted a lot of attention during the last decades.
Bio-impedances are closely related to a large variety of physiological
conditions; therefore, they are useful for diagnosis and monitoring in many
medical applications. Measuring living tissues, however, is a challenging task
that poses countless technical and practical problems, in particular if the
tissues need to be measured under the skin. This paper presents a bio-impedance
sensor ASIC targeting a battery-free, miniature size, implantable device, which
performs accurate 4-point complex impedance extraction in the frequency range
from 2 kHz to 2 MHz. The ASIC is fabricated in 150 nm CMOS, has a size of 1.22
mm x 1.22 mm and consumes 165 uA from a 1.8 V power supply. The ASIC is
embedded in a prototype which communicates with, and is powered by an external
reader device through inductive coupling. The prototype is validated by
measuring the impedances of different combinations of discrete components,
measuring the electrochemical impedance of physiological solution, and
performing ex vivo measurements on animal organs. The proposed ASIC is able to
extract complex impedances with around 1 Ohm resolution; therefore enabling
accurate wireless tissue measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03395</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03395</id><created>2015-07-13</created><authors><author><keyname>Bazzi</keyname><forenames>Louay</forenames></author><author><keyname>Abou-Faycal</keyname><forenames>Ibrahim</forenames></author></authors><title>LP decoding excess over symmetric channels</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of Linear Programming (LP) decoding of binary linear
codes. The LP excess lemma was introduced by the first author, B. Ghazi, and R.
Urbanke (IEEE Trans. Inf. Th., 2014) as a technique to trade crossover
probability for &quot;LP excess&quot; over the Binary Symmetric Channel. We generalize
the LP excess lemma to discrete, binary-input, Memoryless, Symmetric and
LLR-Bounded (MSB) channels. As an application, we extend a result by the first
author and H. Audah (IEEE Trans. Inf. Th., 2015) on the impact of redundant
checks on LP decoding to discrete MSB channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03403</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03403</id><created>2015-07-13</created><authors><author><keyname>Korman</keyname><forenames>Matias</forenames></author><author><keyname>Mulzer</keyname><forenames>Wolfgang</forenames></author><author><keyname>van Renssen</keyname><forenames>Andre</forenames></author><author><keyname>Roeloffzen</keyname><forenames>Marcel</forenames></author><author><keyname>Seiferth</keyname><forenames>Paul</forenames></author><author><keyname>Stein</keyname><forenames>Yannik</forenames></author></authors><title>Time-Space Trade-offs for Triangulations and Voronoi Diagrams</title><categories>cs.CG cs.DS</categories><comments>17 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $S$ be a planar $n$-point set. A triangulation for $S$ is a maximal plane
straight-line graph with vertex set $S$. The Voronoi diagram for $S$ is the
subdivision of the plane into cells such that each cell has the same nearest
neighbors in $S$. Classically, both structures can be computed in $O(n \log n)$
time and $O(n)$ space. We study the situation when the available workspace is
limited: given a parameter $s \in \{1, \dots, n\}$, an $s$-workspace algorithm
has read-only access to an input array with the points from $S$ in arbitrary
order, and it may use only $O(s)$ additional words of $\Theta(\log n)$ bits for
reading and writing intermediate data. The output should then be written to a
write-only structure. We describe a deterministic $s$-workspace algorithm for
computing a triangulation of $S$ in time $O(n^2/s + n \log n \log s )$ and a
randomized $s$-workspace algorithm for finding the Voronoi diagram of $S$ in
expected time $O((n^2/s) \log s + n \log s \log^*s)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03407</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03407</id><created>2015-07-13</created><authors><author><keyname>Br&#xe1;zdil</keyname><forenames>Tom&#xe1;&#x161;</forenames></author><author><keyname>Hlin&#x11b;n&#xfd;</keyname><forenames>Petr</forenames></author><author><keyname>Ku&#x10d;era</keyname><forenames>Anton&#xed;n</forenames></author><author><keyname>&#x158;eh&#xe1;k</keyname><forenames>Vojt&#x11b;ch</forenames></author><author><keyname>Abaffy</keyname><forenames>Mat&#xfa;&#x161;</forenames></author></authors><title>Strategy Synthesis in Adversarial Patrolling Games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Patrolling is one of the central problems in operational security. Formally,
a patrolling problem is specified by a set $U$ of nodes (admissible defender's
positions), a set $T \subseteq U$ of vulnerable targets, an admissible
defender's moves over $U$, and a function which to every target assigns the
time needed to complete an intrusion at it. The goal is to design an optimal
strategy for a defender who is moving from node to node and aims at detecting
possible intrusions at the targets. The goal of the attacker is to maximize the
probability of a successful attack. We assume that the attacker is adversarial,
i.e., he knows the strategy of the defender and can observe her moves.
  We prove that the defender has an optimal strategy for every patrolling
problem. Further, we show that for every $\varepsilon$ &gt; 0, there exists a
finite-memory $\varepsilon$-optimal strategy for the defender constructible in
exponential time, and we observe that such a strategy cannot be computed in
polynomial time unless P=NP.
  Then we focus ourselves to unrestricted defender's moves. Here, a patrolling
problem is fully determined by its signature, the number of targets of each
attack length. We bound the maximal probability of successfully defended
attacks. Then, we introduce a decomposition method which allows to split a
given patrolling problem $G$ into smaller subproblems and construct a
defender's strategy for $G$ by &quot;composing&quot; the strategies constructed for these
subproblems.
  Finally, for patrolling problems with $T = U$ and a well-formed signature, we
give an exact classification of all sufficiently connected environments where
the defender can achieve the same value as in the fully connected uniform
environment. This result is useful for designing &quot;good&quot; environments where the
defender can act optimally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03409</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03409</id><created>2015-07-13</created><updated>2015-09-20</updated><authors><author><keyname>Liang</keyname><forenames>Zhujin</forenames></author><author><keyname>Ding</keyname><forenames>Shengyong</forenames></author><author><keyname>Lin</keyname><forenames>Liang</forenames></author></authors><title>Unconstrained Facial Landmark Localization with Backbone-Branches
  Fully-Convolutional Networks</title><categories>cs.CV</categories><comments>This paper has been withdrawn by the author due to some errors in the
  paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates how to rapidly and accurately localize facial
landmarks in unconstrained, cluttered environments rather than in the well
segmented face images. We present a novel Backbone-Branches Fully-Convolutional
Neural Network (BB-FCN), which produces facial landmark response maps directly
from raw images without relying on pre-process or sliding window approaches.
BB-FCN contains one backbone and a number of network branches with each
corresponding to one landmark type, and it operates in a progressive manner.
Specifically, the backbone roughly detects the locations of facial landmarks by
taking the whole image as input, and the branches further refine the
localizations based on a local observation from the backbone's intermediate
feature map. Moreover, our backbone-branches architecture does not contain
full-connection layers for location regression, leading to efficient learning
and inference. Our extensive experiments show that our model achieves superior
performances over other state-of-the-arts under both the constrained (i.e. with
face regions) and the &quot;in the wild&quot; scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03415</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03415</id><created>2015-07-13</created><authors><author><keyname>Leitner</keyname><forenames>Maria</forenames></author><author><keyname>Ma</keyname><forenames>Zhendong</forenames></author><author><keyname>Rinderle-Ma</keyname><forenames>Stefanie</forenames></author></authors><title>A Cross-Layer Security Analysis for Process-Aware Information Systems</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information security in Process-aware Information System (PAIS) relies on
many factors, including security of business process and the underlying system
and technologies. Moreover, humans can be the weakest link that creates pathway
to vulnerabilities, or the worst enemy that compromises a well-defended system.
Since a system is as secure as its weakest link, information security can only
be achieved in PAIS if all factors are secure. In this paper, we address two
research questions: how to conduct a cross-layer security analysis that couple
security concerns at business process layer as well as at the technical layer;
and how to include human factor into the security analysis for the
identification of human-oriented vulnerabilities and threats. We propose a
methodology that supports the tracking of security interdependencies between
functional, technical, and human aspects which contribute to establish a
holistic approach to information security in PAIS. We demonstrate the
applicability with a scenario from the payment card industry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03418</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03418</id><created>2015-07-13</created><authors><author><keyname>Hu</keyname><forenames>Chuangqiang</forenames></author></authors><title>Explicit Construction of AG Codes from Generalized Hermitian Curves</title><categories>cs.IT math.IT</categories><comments>13 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present multi-point algebraic geometric codes overstepping the
Gilbert-Varshamov bound. The construction is based on the generalized Hermitian
curve introduced by A. Bassa, P. Beelen, A. Garcia, and H. Stichtenoth. These
codes are described in detail by constrcting a generator matrix. It turns out
that these codes have nice properties similar to those of Hermitian codes. It
is shown that the duals are also such codes and an explicit formula is given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03439</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03439</id><created>2015-07-13</created><authors><author><keyname>Etscheid</keyname><forenames>Michael</forenames></author><author><keyname>Kratsch</keyname><forenames>Stefan</forenames></author><author><keyname>Mnich</keyname><forenames>Matthias</forenames></author><author><keyname>R&#xf6;glin</keyname><forenames>Heiko</forenames></author></authors><title>Polynomial Kernels for Weighted Problems</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kernelization is a formalization of efficient preprocessing for NP-hard
problems using the framework of parameterized complexity. Among open problems
in kernelization it has been asked many times whether there are deterministic
polynomial kernelizations for Subset Sum and Knapsack when parameterized by the
number $n$ of items.
  We answer both questions affirmatively by using an algorithm for compressing
numbers due to Frank and Tardos (Combinatorica 1987). This result had been
first used by Marx and V\'egh (ICALP 2013) in the context of kernelization. We
further illustrate its applicability by giving polynomial kernels also for
weighted versions of several well-studied parameterized problems. Furthermore,
when parameterized by the different item sizes we obtain a polynomial
kernelization for Subset Sum and an exponential kernelization for Knapsack.
Finally, we also obtain kernelization results for polynomial integer programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03462</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03462</id><created>2015-07-13</created><authors><author><keyname>Aldabe</keyname><forenames>Itziar</forenames></author><author><keyname>de Lacalle</keyname><forenames>Oier Lopez</forenames></author><author><keyname>Lopez-Gazpio</keyname><forenames>I&#xf1;igo</forenames></author><author><keyname>Maritxalar</keyname><forenames>Montse</forenames></author></authors><title>Supervised Hierarchical Classification for Student Answer Scoring</title><categories>cs.CL</categories><comments>5 pages with references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a hierarchical system that predicts one label at a time
for automated student response analysis. For the task, we build a
classification binary tree that delays more easily confused labels to later
stages using hierarchical processes. In particular, the paper describes how the
hierarchical classifier has been built and how the classification task has been
broken down into binary subtasks. It finally discusses the motivations and
fundamentals of such an approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03466</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03466</id><created>2015-07-13</created><authors><author><keyname>Besselink</keyname><forenames>B.</forenames></author><author><keyname>Turri</keyname><forenames>V.</forenames></author><author><keyname>van de Hoef</keyname><forenames>S. H.</forenames></author><author><keyname>Liang</keyname><forenames>K. -Y.</forenames></author><author><keyname>Alam</keyname><forenames>A.</forenames></author><author><keyname>M&#xe5;rtensson</keyname><forenames>J.</forenames></author><author><keyname>Johansson</keyname><forenames>K. H.</forenames></author></authors><title>Cyber-physical Control of Road Freight Transport</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Freight transportation is of outmost importance for our society and is
continuously increasing. At the same time, transporting goods on roads accounts
for about 26% of all energy consumption and 18% of all greenhouse gas emissions
in the European Union. Despite the influence the transportation system has on
our energy consumption and the environment, road transportation is mainly done
by individual long-haulage trucks with no real-time coordination or global
optimization. In this paper, we review how modern information and communication
technology supports a cyber-physical transportation system architecture with an
integrated logistic system coordinating fleets of trucks traveling together in
vehicle platoons. From the reduced air drag, platooning trucks traveling close
together can save about 10% of their fuel consumption. Utilizing road grade
information and vehicle-to-vehicle communication, a safe and fuel-optimized
cooperative look-ahead control strategy is implemented on top of the existing
cruise controller. By optimizing the interaction between vehicles and platoons
of vehicles, it is shown that significant improvements can be achieved. An
integrated transport planning and vehicle routing in the fleet management
system allows both small and large fleet owners to benefit from the
collaboration. A realistic case study with 200 heavy-duty vehicles performing
transportation tasks in Sweden is described. Simulations show overall fuel
savings at more than 5% thanks to coordinated platoon planning. It is also
illustrated how well the proposed cooperative look-ahead controller for
heavy-duty vehicle platoons manages to optimize the velocity profiles of the
vehicles over a hilly segment of the considered road network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03467</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03467</id><created>2015-07-13</created><authors><author><keyname>De Santis</keyname><forenames>Alfredo</forenames></author><author><keyname>De Maio</keyname><forenames>Giancarlo</forenames></author><author><keyname>Petrillo</keyname><forenames>Umberto Ferraro</forenames></author></authors><title>Using HTML5 to Prevent Detection of Drive-by-Download Web Malware</title><categories>cs.CR</categories><comments>This is the pre-peer reviewed version of the article: \emph{Using
  HTML5 to Prevent Detection of Drive-by-Download Web Malware}, which has been
  published in final form at \url{http://dx.doi.org/10.1002/sec.1077}. This
  article may be used for non-commercial purposes in accordance with Wiley
  Terms and Conditions for Self-Archiving</comments><journal-ref>Security and Communication Networks, Volume 8, Issue 7, pages
  1237-1255, 10 May 2015</journal-ref><doi>10.1002/sec.1077</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The web is experiencing an explosive growth in the last years. New
technologies are introduced at a very fast-pace with the aim of narrowing the
gap between web-based applications and traditional desktop applications. The
results are web applications that look and feel almost like desktop
applications while retaining the advantages of being originated from the web.
However, these advancements come at a price. The same technologies used to
build responsive, pleasant and fully-featured web applications, can also be
used to write web malware able to escape detection systems. In this article we
present new obfuscation techniques, based on some of the features of the
upcoming HTML5 standard, which can be used to deceive malware detection
systems. The proposed techniques have been experimented on a reference set of
obfuscated malware. Our results show that the malware rewritten using our
obfuscation techniques go undetected while being analyzed by a large number of
detection systems. The same detection systems were able to correctly identify
the same malware in its original unobfuscated form. We also provide some hints
about how the existing malware detection systems can be modified in order to
cope with these new techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03471</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03471</id><created>2015-07-13</created><authors><author><keyname>Zilka</keyname><forenames>Lukas</forenames></author><author><keyname>Jurcicek</keyname><forenames>Filip</forenames></author></authors><title>Incremental LSTM-based Dialog State Tracker</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A dialog state tracker is an important component in modern spoken dialog
systems. We present an incremental dialog state tracker, based on LSTM
networks. It directly uses automatic speech recognition hypotheses to track the
state. We also present the key non-standard aspects of the model that bring its
performance close to the state-of-the-art and experimentally analyze their
contribution: including the ASR confidence scores, abstracting scarcely
represented values, including transcriptions in the training data, and model
averaging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03474</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03474</id><created>2015-07-13</created><authors><author><keyname>Peters</keyname><forenames>Dominik</forenames></author><author><keyname>Elkind</keyname><forenames>Edith</forenames></author></authors><title>Simple Causes of Complexity in Hedonic Games</title><categories>cs.GT</categories><comments>7+9 pages, long version of a paper in IJCAI 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hedonic games provide a natural model of coalition formation among
self-interested agents. The associated problem of finding stable outcomes in
such games has been extensively studied. In this paper, we identify simple
conditions on expressivity of hedonic games that are sufficient for the problem
of checking whether a given game admits a stable outcome to be computationally
hard. Somewhat surprisingly, these conditions are very mild and intuitive. Our
results apply to a wide range of stability concepts (core stability, individual
stability, Nash stability, etc.) and to many known formalisms for hedonic games
(additively separable games, games with W-preferences, fractional hedonic
games, etc.), and unify and extend known results for these formalisms. They
also have broader applicability: for several classes of hedonic games whose
computational complexity has not been explored in prior work, we show that our
framework immediately implies a number of hardness results for them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03480</identifier>
 <datestamp>2015-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03480</id><created>2015-07-13</created><authors><author><keyname>Bao</keyname><forenames>Wansu</forenames></author><author><keyname>Huang</keyname><forenames>Heliang</forenames></author></authors><title>Middle-Solving Grobner bases algorithm for cryptanalysis over finite
  fields</title><categories>cs.CR cs.SC</categories><comments>arXiv admin note: text overlap with arXiv:1310.2332</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Algebraic cryptanalysis usually requires to recover the secret key by solving
polynomial equations. Grobner bases algorithm is a well-known method to solve
this problem. However, a serious drawback exists in the Grobner bases based
algebraic attacks, namely, any information won't be got if we couldn't work out
the Grobner bases of the polynomial equations system. In this paper, firstly, a
generalized model of Grobner basis algorithms is presented, which provides us a
platform to analyze and solve common problems of the algorithms. Secondly, we
give and prove the degree bound of the polynomials appeared during the
computation of Grobner basis after field polynomials is added. Finally, by
detecting the temporary basis during the computation of Grobner bases and then
extracting the univariate polynomials contained unique solution in the
temporary basis, a heuristic strategy named Middle-Solving is presented to
solve these polynomials at each iteration of the algorithm. Farther, two
specific application mode of Middle-Solving strategy for the incremental and
non-incremental Grobner bases algorithms are presented respectively. By using
the Middle-Solving strategy, even though we couldn't work out the final Grobner
bases, some information of the variables still leak during the computational
process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03482</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03482</id><created>2015-07-13</created><authors><author><keyname>Hernando-Gallego</keyname><forenames>Francisco</forenames></author><author><keyname>Art&#xe9;s-Rodr&#xed;guez</keyname><forenames>Antonio</forenames></author></authors><title>Individual performance calibration using physiological stress signals</title><categories>cs.HC cs.CY stat.ML</categories><comments>5 pages, 12 figures, Workshop of Shimmer sensors, IEEE Body Sensor
  Networks Conference 2015 MIT Lab Boston (USA)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The relation between performance and stress is described by the Yerkes-Dodson
Law but varies significantly between individuals. This paper describes a method
for determining the individual optimal performance as a function of
physiological signals. The method is based on attention and reasoning tests of
increasing complexity under monitoring of three physiological signals: Galvanic
Skin Response (GSR), Heart Rate (HR), and Electromyogram (EMG). Based on the
test results with 15 different individuals, we first show that two of the
signals, GSR and HR, have enough discriminative power to distinguish between
relax and stress periods. We then show a positive correlation between the
complexity level of the tests and the GSR and HR signals, and we finally
determine the optimal performance point as the signal level just before a
performance decrease. We also discuss the differences among signals depending
on the type of test.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03490</identifier>
 <datestamp>2015-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03490</id><created>2015-07-13</created><updated>2015-09-27</updated><authors><author><keyname>Sayama</keyname><forenames>Hiroki</forenames></author><author><keyname>Cramer</keyname><forenames>Catherine</forenames></author><author><keyname>Porter</keyname><forenames>Mason A.</forenames></author><author><keyname>Sheetz</keyname><forenames>Lori</forenames></author><author><keyname>Uzzo</keyname><forenames>Stephen</forenames></author></authors><title>What are essential concepts about networks?</title><categories>cs.SI math.CO nlin.AO physics.data-an physics.soc-ph</categories><comments>21 pages, 8 figures, 5 tables. To appear in Journal of Complex
  Networks</comments><doi>10.1093/comnet/cnv028</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Networks have become increasingly relevant to everyday life as human society
has become increasingly connected. Attaining a basic understanding of networks
has thus become a necessary form of literacy for people (and for youths in
particular). At the NetSci 2014 conference, we initiated a year-long process to
develop an educational resource that concisely summarizes essential concepts
about networks that can be used by anyone of school age or older. The process
involved several brainstorming sessions on one key question: &quot;What should every
person living in the 21st century know about networks by the time he/she
finishes secondary education?&quot; Different sessions reached diverse participants,
which included professional researchers in network science, educators, and
high-school students. The generated ideas were connected by the students to
construct a concept network. We examined community structure in the concept
network to group ideas into a set of important themes, which we refined through
discussion into seven essential concepts. The students played a major role in
this development process by providing insights and perspectives that were often
unrecognized by researchers and educators. The final result, &quot;Network Literacy:
Essential Concepts and Core Ideas&quot;, is now available as a booklet in several
different languages from http://tinyurl.com/networkliteracy .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03498</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03498</id><created>2015-07-13</created><authors><author><keyname>Antonsen</keyname><forenames>Christine</forenames></author></authors><title>Selfish Routing on Dynamic Flows</title><categories>cs.GT</categories><comments>Oberlin College Computer Science Honors Thesis. Supervisor: Alexa
  Sharp, Oberlin College</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Selfish routing on dynamic flows over time is used to model scenarios that
vary with time in which individual agents act in their best interest. In this
paper we provide a survey of a particular dynamic model, the deterministic
queuing model, and discuss how the model can be adjusted and applied to
different real-life scenarios. We then examine how these adjustments affect the
computability, optimality, and existence of selfish routings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03509</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03509</id><created>2015-07-13</created><authors><author><keyname>Shermer</keyname><forenames>Thomas C.</forenames></author></authors><title>A Combinatorial Bound for Beacon-based Routing in Orthogonal Polygons</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Beacon attraction is a movement system whereby a robot (modeled as a point in
2D) moves in a free space so as to always locally minimize its Euclidean
distance to an activated beacon (which is also a point). This results in the
robot moving directly towards the beacon when it can, and otherwise sliding
along the edge of an obstacle. When a robot can reach the activated beacon by
this method, we say that the beacon attracts the robot. A beacon routing from
$p$ to $q$ is a sequence $b_1, b_2,$ ..., $b_{k}$ of beacons such that
activating the beacons in order will attract a robot from $p$ to $b_1$ to $b_2$
... to $b_{k}$ to $q$, where $q$ is considered to be a beacon. A routing set of
beacons is a set $B$ of beacons such that any two points $p, q$ in the free
space have a beacon routing with the intermediate beacons $b_1, b_2,$ ...,
$b_{k}$ all chosen from $B$. Here we address the question of &quot;how large must
such a $B$ be?&quot; in orthogonal polygons, and show that the answer is &quot;sometimes
as large as $[(n-4)/3]$, but never larger.&quot;
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03510</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03510</id><created>2015-07-13</created><updated>2015-07-14</updated><authors><author><keyname>Lim</keyname><forenames>Bang Hui</forenames></author><author><keyname>Lu</keyname><forenames>Dongyuan</forenames></author><author><keyname>Chen</keyname><forenames>Tao</forenames></author><author><keyname>Kan</keyname><forenames>Min-Yen</forenames></author></authors><title>#mytweet via Instagram: Exploring User Behaviour across Multiple Social
  Networks</title><categories>cs.SI</categories><comments>IEEE/ACM International Conference on Advances in Social Networks
  Analysis and Mining, 2015. This is the pre-peer reviewed version and the
  final version is available at
  http://wing.comp.nus.edu.sg/publications/2015/lim-et-al-15.pdf</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study how users of multiple online social networks (OSNs) employ and share
information by studying a common user pool that use six OSNs - Flickr, Google+,
Instagram, Tumblr, Twitter, and YouTube. We analyze the temporal and topical
signature of users' sharing behaviour, showing how they exhibit distinct
behaviorial patterns on different networks. We also examine cross-sharing
(i.e., the act of user broadcasting their activity to multiple OSNs
near-simultaneously), a previously-unstudied behaviour and demonstrate how
certain OSNs play the roles of originating source and destination sinks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03512</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03512</id><created>2015-07-13</created><updated>2015-10-07</updated><authors><author><keyname>Bapst</keyname><forenames>Victor</forenames></author><author><keyname>Coja-Oghlan</keyname><forenames>Amin</forenames></author></authors><title>The condensation phase transition in the regular $k$-SAT model</title><categories>math.PR cs.DM math.CO</categories><comments>Revised version based on arXiv:1504.03975, version 3</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Much of the recent work on random constraint satisfaction problems has been
inspired by ingenious but non-rigorous approaches from physics. The physics
predictions typically come in the form of distributional fixed point problems
that are intended to mimic Belief Propagation, a message passing algorithm,
applied to the random CSP. In this paper we propose a novel method for
harnessing Belief Propagation directly to obtain a rigorous proof of such a
prediction, namely the existence and location of a condensation phase
transition in the random regular $k$-SAT model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03513</identifier>
 <datestamp>2015-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03513</id><created>2015-07-13</created><updated>2015-11-24</updated><authors><author><keyname>Yang</keyname><forenames>Jean</forenames></author><author><keyname>Hance</keyname><forenames>Travis</forenames></author><author><keyname>Austin</keyname><forenames>Thomas H.</forenames></author><author><keyname>Solar-Lezama</keyname><forenames>Armando</forenames></author><author><keyname>Flanagan</keyname><forenames>Cormac</forenames></author><author><keyname>Chong</keyname><forenames>Stephen</forenames></author></authors><title>Precise, Dynamic Information Flow for Database-Backed Applications</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an approach for dynamic information flow control across the
application and database. Our approach reduces the amount of policy code
required, yields formal guarantees across the application and database, works
with existing relational database implementations, and scales for realistic
applications. In this paper, we present a programming model that factors out
information flow policies from application code and database queries, a dynamic
semantics for the underlying {\lambda}^JDB core language, and proofs of
termination-insensitive non-interference and policy compliance for the
semantics. We implement these ideas in Jacqueline, a Python web framework, and
demonstrate feasibility through three application case studies: a course
manager, a health record system, and a conference management system used to run
an academic workshop. We show that in comparison to traditional applications
with hand-coded policy checks, Jacqueline applications have 1) a smaller
trusted computing base, 2) fewer lines of policy code, and 2) reasonable, often
negligible, additional overheads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03518</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03518</id><created>2015-07-13</created><updated>2015-07-20</updated><authors><author><keyname>Alemzadeh</keyname><forenames>Homa</forenames></author><author><keyname>Iyer</keyname><forenames>Ravishankar K.</forenames></author><author><keyname>Kalbarczyk</keyname><forenames>Zbigniew</forenames></author><author><keyname>Leveson</keyname><forenames>Nancy</forenames></author><author><keyname>Raman</keyname><forenames>Jaishankar</forenames></author></authors><title>Adverse Events in Robotic Surgery: A Retrospective Study of 14 Years of
  FDA Data</title><categories>cs.RO cs.CR</categories><comments>Presented as the J. Maxwell Chamberlain Memorial Paper for adult
  cardiac surgery at the 50th Annual Meeting of the Society of Thoracic
  Surgeons in January. See Appendix for more detailed results, discussions, and
  related work. Updated the headers</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the causes and patient impacts of surgical adverse events will
help improve systems and operational practices to avoid incidents in the
future. We analyzed the adverse events data related to robotic systems and
instruments used in minimally invasive surgery, reported to the U.S. FDA MAUDE
database from January 2000 to December 2013. We determined the number of events
reported per procedure and per surgical specialty, the most common types of
device malfunctions and their impact on patients, and the causes for
catastrophic events such as major complications, patient injuries, and deaths.
During the study period, 144 deaths (1.4% of the 10,624 reports), 1,391 patient
injuries (13.1%), and 8,061 device malfunctions (75.9%) were reported. The
numbers of injury and death events per procedure have stayed relatively
constant since 2007 (mean = 83.4, 95% CI, 74.2-92.7). Surgical specialties, for
which robots are extensively used, such as gynecology and urology, had lower
number of injuries, deaths, and conversions per procedure than more complex
surgeries, such as cardiothoracic and head and neck (106.3 vs. 232.9, Risk
Ratio = 2.2, 95% CI, 1.9-2.6). Device and instrument malfunctions, such as
falling of burnt/broken pieces of instruments into the patient (14.7%),
electrical arcing of instruments (10.5%), unintended operation of instruments
(8.6%), system errors (5%), and video/imaging problems (2.6%), constituted a
major part of the reports. Device malfunctions impacted patients in terms of
injuries or procedure interruptions. In 1,104 (10.4%) of the events, the
procedure was interrupted to restart the system (3.1%), to convert the
procedure to non-robotic techniques (7.3%), or to reschedule it to a later time
(2.5%). Adoption of advanced techniques in design and operation of robotic
surgical systems may reduce these preventable incidents in the future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03528</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03528</id><created>2015-07-13</created><authors><author><keyname>Eshghi</keyname><forenames>Soheil</forenames></author><author><keyname>Sarkar</keyname><forenames>Saswati</forenames></author><author><keyname>Venkatesh</keyname><forenames>Santosh S.</forenames></author></authors><title>Visibility-Aware Optimal Contagion of Malware Epidemics</title><categories>cs.CR cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent innovations in the design of computer viruses have led to new
trade-offs for the attacker. Multiple variants of a malware may spread at
different rates and have different levels of visibility to the network. In this
work we examine the optimal strategies for the attacker so as to trade off the
extent of spread of the malware against the need for stealth. We show that in
the mean-field deterministic regime, this spread-stealth trade-off is optimized
by computationally simple single-threshold policies. Specifically, we show that
only one variant of the malware is spread by the attacker at each time, as
there exists a time up to which the attacker prioritizes maximizing the spread
of the malware, and after which she prioritizes stealth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03546</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03546</id><created>2015-07-13</created><updated>2016-01-30</updated><authors><author><keyname>Liu</keyname><forenames>Zi-Wen</forenames></author><author><keyname>Perry</keyname><forenames>Christopher</forenames></author><author><keyname>Zhu</keyname><forenames>Yechao</forenames></author><author><keyname>Koh</keyname><forenames>Dax Enshan</forenames></author><author><keyname>Aaronson</keyname><forenames>Scott</forenames></author></authors><title>Doubly infinite separation of quantum information and communication</title><categories>quant-ph cs.CC cs.IT math.IT</categories><comments>16 pages, 2 figures. This version: minor errors fixed; close to
  published version</comments><report-no>MIT-CTP/4692</report-no><journal-ref>Phys. Rev. A 93, 012347 (2016)</journal-ref><doi>10.1103/PhysRevA.93.012347</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove the existence of (one-way) communication tasks with a subconstant
versus superconstant asymptotic gap, which we call &quot;doubly infinite,&quot; between
their quantum information and communication complexities. We do so by studying
the exclusion game [C. Perry et al., Phys. Rev. Lett. 115, 030504 (2015)] for
which there exist instances where the quantum information complexity tends to
zero as the size of the input $n$ increases. By showing that the quantum
communication complexity of these games scales at least logarithmically in $n$,
we obtain our result. We further show that the established lower bounds and
gaps still hold even if we allow a small probability of error. However in this
case, the $n$-qubit quantum message of the zero-error strategy can be
compressed polynomially.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03549</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03549</id><created>2015-07-13</created><updated>2015-07-16</updated><authors><author><keyname>de Klerk</keyname><forenames>Etienne</forenames></author><author><keyname>Vallentin</keyname><forenames>Frank</forenames></author></authors><title>On the Turing model complexity of interior point methods for
  semidefinite programming</title><categories>math.OC cs.DS</categories><comments>(v2) some comments added, 16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is known that one can solve semidefinite programs to within fixed accuracy
in polynomial time using the ellipsoid method (under some assumptions). In this
paper it is shown that the same holds true when one uses the short-step, primal
interior point method. The main idea of the proof is to employ Diophantine
approximation at each iteration to bound the intermediate bit-sizes of
iterates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03558</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03558</id><created>2015-07-13</created><updated>2016-01-21</updated><authors><author><keyname>Canonne</keyname><forenames>Cl&#xe9;ment L.</forenames></author><author><keyname>Diakonikolas</keyname><forenames>Ilias</forenames></author><author><keyname>Gouleakis</keyname><forenames>Themis</forenames></author><author><keyname>Rubinfeld</keyname><forenames>Ronitt</forenames></author></authors><title>Testing Shape Restrictions of Discrete Distributions</title><categories>cs.DS cs.CC math.PR math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the question of testing structured properties (classes) of discrete
distributions. Specifically, given sample access to an arbitrary distribution
$D$ over $[n]$ and a property $\mathcal{P}$, the goal is to distinguish between
$D\in\mathcal{P}$ and $\ell_1(D,\mathcal{P})&gt;\varepsilon$. We develop a general
algorithm for this question, which applies to a large range of
&quot;shape-constrained&quot; properties, including monotone, log-concave, $t$-modal,
piecewise-polynomial, and Poisson Binomial distributions. Moreover, for all
cases considered, our algorithm has near-optimal sample complexity with regard
to the domain size and is computationally efficient. For most of these classes,
we provide the first non-trivial tester in the literature. In addition, we also
describe a generic method to prove lower bounds for this problem, and use it to
show our upper bounds are nearly tight. Finally, we extend some of our
techniques to tolerant testing, deriving nearly-tight upper and lower bounds
for the corresponding questions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03559</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03559</id><created>2015-07-13</created><authors><author><keyname>Darais</keyname><forenames>David</forenames></author><author><keyname>Van Horn</keyname><forenames>David</forenames></author></authors><title>Mechanically Verified Calculational Abstract Interpretation</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Calculational abstract interpretation, long advocated by Cousot, is a
technique for deriving correct-by-construction abstract interpreters from the
formal semantics of programming languages.
  This paper addresses the problem of deriving correct-by-verified-construction
abstract interpreters with the use of a proof assistant. We identify several
technical challenges to overcome with the aim of supporting verified
calculational abstract interpretation that is faithful to existing
pencil-and-paper proofs, supports calculation with Galois connections
generally, and enables the extraction of verified static analyzers from these
proofs. To meet these challenges, we develop a theory of Galois connections in
monadic style that include a specification effect. Effectful calculations may
reason classically, while pure calculations have extractable computational
content. Moving between the worlds of specification and implementation is
enabled by our metatheory.
  To validate our approach, we give the first mechanically verified proof of
correctness for Cousot's &quot;Calculational design of a generic abstract
interpreter.&quot; Our proof &quot;by calculus&quot; closely follows the original
paper-and-pencil proof and supports the extraction of a verified static
analyzer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03562</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03562</id><created>2015-07-13</created><authors><author><keyname>Soualhia</keyname><forenames>Mbarka</forenames></author><author><keyname>Khomh</keyname><forenames>Foutse</forenames></author><author><keyname>Tahar</keyname><forenames>Sofiene</forenames></author></authors><title>Predicting Scheduling Failures in the Cloud</title><categories>cs.DC cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud Computing has emerged as a key technology to deliver and manage
computing, platform, and software services over the Internet. Task scheduling
algorithms play an important role in the efficiency of cloud computing services
as they aim to reduce the turnaround time of tasks and improve resource
utilization. Several task scheduling algorithms have been proposed in the
literature for cloud computing systems, the majority relying on the
computational complexity of tasks and the distribution of resources. However,
several tasks scheduled following these algorithms still fail because of
unforeseen changes in the cloud environments. In this paper, using tasks
execution and resource utilization data extracted from the execution traces of
real world applications at Google, we explore the possibility of predicting the
scheduling outcome of a task using statistical models. If we can successfully
predict tasks failures, we may be able to reduce the execution time of jobs by
rescheduling failed tasks earlier (i.e., before their actual failing time). Our
results show that statistical models can predict task failures with a precision
up to 97.4%, and a recall up to 96.2%. We simulate the potential benefits of
such predictions using the tool kit GloudSim and found that they can improve
the number of finished tasks by up to 40%. We also perform a case study using
the Hadoop framework of Amazon Elastic MapReduce (EMR) and the jobs of a gene
expression correlations analysis study from breast cancer research. We find
that when extending the scheduler of Hadoop with our predictive models, the
percentage of failed jobs can be reduced by up to 45%, with an overhead of less
than 5 minutes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03577</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03577</id><created>2015-07-13</created><authors><author><keyname>Jeon</keyname><forenames>Jinseong</forenames></author><author><keyname>Qiu</keyname><forenames>Xiaokang</forenames></author><author><keyname>Foster</keyname><forenames>Jeffrey S.</forenames></author><author><keyname>Solar-Lezama</keyname><forenames>Armando</forenames></author></authors><title>JSKETCH: Sketching for Java</title><categories>cs.PL</categories><comments>This research was supported in part by NSF CCF-1139021, CCF- 1139056,
  CCF-1161775, and the partnership between UMIACS and the Laboratory for
  Telecommunication Sciences</comments><acm-class>I.2.2; F.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sketch-based synthesis, epitomized by the SKETCH tool, lets developers
synthesize software starting from a partial program, also called a sketch or
template. This paper presents JSKETCH, a tool that brings sketch-based
synthesis to Java. JSKETCH's input is a partial Java program that may include
holes, which are unknown constants, expression generators, which range over
sets of expressions, and class generators, which are partial classes. JSKETCH
then translates the synthesis problem into a SKETCH problem; this translation
is complex because SKETCH is not object-oriented. Finally, JSKETCH synthesizes
an executable Java program by interpreting the output of SKETCH.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03614</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03614</id><created>2015-07-13</created><updated>2016-02-23</updated><authors><author><keyname>Sarkis</keyname><forenames>Gabi</forenames></author><author><keyname>Tal</keyname><forenames>Ido</forenames></author><author><keyname>Giard</keyname><forenames>Pascal</forenames></author><author><keyname>Vardy</keyname><forenames>Alexander</forenames></author><author><keyname>Thibeault</keyname><forenames>Claude</forenames></author><author><keyname>Gross</keyname><forenames>Warren J.</forenames></author></authors><title>Flexible and Low-Complexity Encoding and Decoding of Systematic Polar
  Codes</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Communications, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we present hardware and software implementations of flexible
polar systematic encoders and decoders. The proposed implementations operate on
polar codes of any length less than a maximum and of any rate. We describe the
low-complexity, highly parallel, and flexible systematic-encoding algorithm
that we use and prove its correctness. Our hardware implementation results show
that the overhead of adding code rate and length flexibility is little, and the
impact on operation latency minor compared to code-specific versions. Finally,
the flexible software encoder and decoder implementations are also shown to be
able to maintain high throughput and low latency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03632</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03632</id><created>2015-07-13</created><authors><author><keyname>Chonev</keyname><forenames>Ventsislav</forenames></author><author><keyname>Ouaknine</keyname><forenames>Joel</forenames></author><author><keyname>Worrell</keyname><forenames>James</forenames></author></authors><title>On the Decidability of the Continuous Infinite Zeros Problem</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the Continuous Infinite Zeros Problem, which asks whether a
real-valued function $f$ satisfying a given ordinary linear differential
equation has infinitely many zeros on $\mathbb{R}_{\geq 0}$. We consider also
the closely related Unbounded Continuous Skolem Problem, which asks whether $f$
has a zero in a given unbounded subinterval of $\mathbb{R}_{\geq 0}$. These are
fundamental reachability problems arising in the analysis of continuous linear
dynamical systems, including linear hybrid automata and continuous-time Markov
chains.
  Our main decidability result is that if the ordinary differential equation
satisfied by $f$ is of order at most $7$ or if the imaginary parts of its
characteristic roots are all rational multiples of one another, then the
Infinite Zeros Problem is decidable, and moreover, if $f$ has only finitely
many zeros, then an upper bound $T$ may be found such that $f(t)=0$ entails
$t\leq T$. On the other hand, our main hardness results is that if the Infinite
Zeros Problem is decidable for ordinary differential equations of order at
least $9$, then this would entail a major breakthrough in Diophantine
Approximation, specifically, the computability of the Lagrange constant
$L_{\infty}(x)$ for all real algebraic $x$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03634</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03634</id><created>2015-07-13</created><authors><author><keyname>Shulman</keyname><forenames>Michael</forenames></author></authors><title>Idempotents in intensional type theory</title><categories>math.LO cs.PL math.CT</categories><comments>28 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study idempotents in intensional Martin-L\&quot;of type theory, and in
particular the question of when and whether they split. We show that in the
presence of propositional truncation and Voevodsky's univalence axiom, there
exist idempotents that do not split; thus in plain MLTT not all idempotents can
be proven to split. On the other hand, assuming only function extensionality,
an idempotent can be split if and only if its witness of idempotency satisfies
one extra coherence condition. Both proofs are inspired by parallel results of
Lurie in higher category theory, showing that ideas from higher category theory
and homotopy theory can have applications even in ordinary MLTT.
  Finally, we show that although the witness of idempotency can be recovered
from a splitting, the one extra coherence condition cannot in general; and we
construct &quot;the type of fully coherent idempotents&quot;, by splitting an idempotent
on the type of partially coherent ones. Our results have been formally verified
in the proof assistant Coq.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03638</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03638</id><created>2015-07-13</created><updated>2016-02-16</updated><authors><author><keyname>Costanzo</keyname><forenames>Giuseppe Tommaso</forenames></author><author><keyname>Iacovella</keyname><forenames>Sandro</forenames></author><author><keyname>Ruelens</keyname><forenames>Frederik</forenames></author><author><keyname>Leurs</keyname><forenames>T.</forenames></author><author><keyname>Claessens</keyname><forenames>Bert</forenames></author></authors><title>Experimental analysis of data-driven control for a building heating
  system</title><categories>cs.AI</categories><comments>12 pages, 8 figures, pending for publication in Elsevier SEGAN</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Driven by the opportunity to harvest the flexibility related to building
climate control for demand response applications, this work presents a
data-driven control approach building upon recent advancements in reinforcement
learning. More specifically, model assisted batch reinforcement learning is
applied to the setting of building climate control subjected to a dynamic
pricing. The underlying sequential decision making problem is cast on a markov
decision problem, after which the control algorithm is detailed. In this work,
fitted Q-iteration is used to construct a policy from a batch of experimental
tuples. In those regions of the state space where the experimental sample
density is low, virtual support samples are added using an artificial neural
network. Finally, the resulting policy is shaped using domain knowledge. The
control approach has been evaluated quantitatively using a simulation and
qualitatively in a living lab. From the quantitative analysis it has been found
that the control approach converges in approximately 20 days to obtain a
control policy with a performance within 90% of the mathematical optimum. The
experimental analysis confirms that within 10 to 20 days sensible policies are
obtained that can be used for di?erent outside temperature regimes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03641</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03641</id><created>2015-07-13</created><authors><author><keyname>Durrett</keyname><forenames>Greg</forenames></author><author><keyname>Klein</keyname><forenames>Dan</forenames></author></authors><title>Neural CRF Parsing</title><categories>cs.CL cs.NE</categories><comments>Accepted for publication at ACL 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a parsing model that combines the exact dynamic
programming of CRF parsing with the rich nonlinear featurization of neural net
approaches. Our model is structurally a CRF that factors over anchored rule
productions, but instead of linear potential functions based on sparse
features, we use nonlinear potentials computed via a feedforward neural
network. Because potentials are still local to anchored rules, structured
inference (CKY) is unchanged from the sparse case. Computing gradients during
learning involves backpropagating an error signal formed from standard CRF
sufficient statistics (expected rule counts). Using only dense features, our
neural CRF already exceeds a strong baseline CRF model (Hall et al., 2014). In
combination with sparse features, our system achieves 91.1 F1 on section 23 of
the Penn Treebank, and more generally outperforms the best prior single parser
results on a range of languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03648</identifier>
 <datestamp>2015-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03648</id><created>2015-07-13</created><authors><author><keyname>Hasan</keyname><forenames>Cengis</forenames></author><author><keyname>Haas</keyname><forenames>Zygmunt J.</forenames></author></authors><title>Deadline-aware Power Management in Data Centers</title><categories>cs.DS cs.DC cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the dynamic power optimization problem in data centers. We formulate
and solve the following offline problem: in which slot which server has to be
assigned to which job; and in which slot which server has to be switched ON or
OFF so that the total power is optimal for some time horizon. We show that the
offline problem is a new version of generalized assignment problem including
new constraints issuing from deadline characteristics of jobs and difference of
activation energy of servers. We propose an online algorithm that solves the
problem heuristically and compare it to randomized routing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03650</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03650</id><created>2015-07-13</created><authors><author><keyname>Shah</keyname><forenames>Neil</forenames></author><author><keyname>Song</keyname><forenames>Yang</forenames></author></authors><title>S-index: Towards Better Metrics for Quantifying Research Impact</title><categories>cs.DL cs.IR cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ongoing growth in the volume of scientific literature available today
precludes researchers from efficiently discerning the relevant from irrelevant
content. Researchers are constantly interested in impactful papers, authors and
venues in their respective fields. Moreover, they are interested in the
so-called recent &quot;rising stars&quot; of these contexts which may lead to attractive
directions for future work, collaborations or impactful publication venues. In
this work, we address the problem of quantifying research impact in each of
these contexts, in order to better direct attention of researchers and
streamline the processes of comparison, ranking and evaluation of contribution.
Specifically, we begin by outlining intuitive underlying assumptions that
impact quantification methods should obey and evaluate when current
state-of-the-art methods fail to satisfy these properties. To this end, we
introduce the s-index metric which quantifies research impact through influence
propagation over a heterogeneous citation network. s-index is tailored from
these intuitive assumptions and offers a number of desirable qualities
including robustness, natural temporality and straightforward extensibility
from the paper impact to broader author and venue impact contexts. We evaluate
its effectiveness on the publicly available Microsoft Academic Search citation
graph with over 119 million papers and 1 billion citation edges with 103
million and 21 thousand associated authors and venues respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03659</identifier>
 <datestamp>2015-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03659</id><created>2015-07-13</created><updated>2015-07-22</updated><authors><author><keyname>Ravindra</keyname><forenames>Swaran S.</forenames></author><author><keyname>Chandra</keyname><forenames>Rohitash</forenames></author><author><keyname>Dhenesh</keyname><forenames>Virallikattur S.</forenames></author></authors><title>A Study of the Management of Electronic Medical Records in Fijian
  Hospitals</title><categories>cs.CY</categories><comments>26 PAGES- single spaced, including all tables, figures, references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite having a number of benefits for healthcare settings, the successful
implementation of health information systems (HIS) continues to be a challenge
in many developing countries. This paper examines the current state of health
information systems in government hospitals in Fiji. It also investigates if
the general public as well as medical practitioners in Fiji have interest in
having web based electronic medical records systems that allow patients to
access their reports and make online bookings for their appointments. Nausori
Health Centre was used as a case study to examine the information systems in a
government hospital in Fiji.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03663</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03663</id><created>2015-07-13</created><authors><author><keyname>Slimane</keyname><forenames>Khaled Skander Ben</forenames></author><author><keyname>Comte</keyname><forenames>Alexis</forenames></author><author><keyname>Gasquet</keyname><forenames>Olivier</forenames></author><author><keyname>Heba</keyname><forenames>Abdelwahab</forenames></author><author><keyname>Lezaud</keyname><forenames>Olivier</forenames></author><author><keyname>Maris</keyname><forenames>Frederic</forenames></author><author><keyname>Valais</keyname><forenames>Mael</forenames></author></authors><title>Twist your logic with TouIST</title><categories>cs.CY cs.AI</categories><comments>Proceedings of the Fourth International Conference on Tools for
  Teaching Logic (TTL2015), Rennes, France, June 9-12, 2015. Editors: M.
  Antonia Huertas, Jo\~ao Marcos, Mar\'ia Manzano, Sophie Pinchinat,
  Fran\c{c}ois Schwarzentruber</comments><proxy>Joao Marcos</proxy><acm-class>K.3.2; I.2.8</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  SAT provers are powerful tools for solving real-sized logic problems, but
using them requires solid programming knowledge and may be seen w.r.t.\ logic
like assembly language w.r.t.\ programming. Something like a high level
language was missing to ease various users to take benefit of these tools. {\sc
\texttt {TouIST}}\ aims at filling this gap. It is devoted to propositional
logic and its main features are 1) to offer a high-level logic langage for
expressing succintly complex formulas (e.g.\ formulas describing Sudoku rules,
planification problems,\ldots) and 2) to find models to these formulas by using
the adequate powerful prover, which the user has no need to know about. It
consists in a friendly interface that offers several syntactic facilities and
which is connected with some sufficiently powerful provers allowing to
automatically solve big instances of difficult problems (such as time-tables or
Sudokus). It can interact with various provers: pure SAT solver but also SMT
provers (SAT modulo theories - like linear theory of reals, etc) and thus may
also be used by beginners for experiencing with pure propositional problems up
to graduate students or even researchers for solving planification problems
involving big sets of fluents and numerical constraints on them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03664</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03664</id><created>2015-07-13</created><updated>2015-07-16</updated><authors><author><keyname>Castro-Manzano</keyname><forenames>Jos&#xe9; Mart&#xed;n</forenames></author><author><keyname>Reyes-Meza</keyname><forenames>Ver&#xf3;nica</forenames></author><author><keyname>Medina-Delgadillo</keyname><forenames>Jorge</forenames></author></authors><title>{dasasap}, an App for Syllogisms</title><categories>cs.CY cs.LO</categories><comments>Proceedings of the Fourth International Conference on Tools for
  Teaching Logic (TTL2015), Rennes, France, June 9-12, 2015. Editors: M.
  Antonia Huertas, Jo\~ao Marcos, Mar\'ia Manzano, Sophie Pinchinat,
  Fran\c{c}ois Schwarzentruber</comments><proxy>Joao Marcos</proxy><acm-class>F.4.1; K.8</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The main goal of this contribution is to introduce a cross-platform
application to learn-teach syllogistic. We call this application
\textit{dasasap} for \textit{develop all syllogisms as soon as possible}. To
introduce this application we show the logical foundations for the game with a
system we call $\mathcal{L}_\square$, and its interface developed with
LiveCode.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03665</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03665</id><created>2015-07-13</created><authors><author><keyname>Duparc</keyname><forenames>Jacques</forenames></author></authors><title>Easy Proofs of L\&quot;owenheim-Skolem Theorems by Means of Evaluation Games</title><categories>cs.CY cs.LO</categories><comments>Proceedings of the Fourth International Conference on Tools for
  Teaching Logic (TTL2015), Rennes, France, June 9-12, 2015. Editors: M.
  Antonia Huertas, Jo\~ao Marcos, Mar\'ia Manzano, Sophie Pinchinat,
  Fran\c{c}ois Schwarzentruber</comments><proxy>Joao Marcos</proxy><acm-class>F.4.1</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We propose a proof of the downward L\&quot;owenheim-Skolem that relies on
strategies deriving from evaluation games instead of the Skolem normal forms.
This proof is simpler, and easily understood by the students, although it
requires, when defining the semantics of first-order logic to introduce first a
few notions inherited from game theory such as the one of an evaluation game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03666</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03666</id><created>2015-07-13</created><authors><author><keyname>Ehle</keyname><forenames>Arno</forenames></author><author><keyname>Hundeshagen</keyname><forenames>Norbert</forenames></author><author><keyname>Lange</keyname><forenames>Martin</forenames></author></authors><title>The Sequent Calculus Trainer - Helping Students to Correctly Construct
  Proofs</title><categories>cs.CY cs.LO</categories><comments>Proceedings of the Fourth International Conference on Tools for
  Teaching Logic (TTL2015), Rennes, France, June 9-12, 2015. Editors: M.
  Antonia Huertas, Jo\~ao Marcos, Mar\'ia Manzano, Sophie Pinchinat,
  Fran\c{c}ois Schwarzentruber</comments><proxy>Joao Marcos</proxy><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present the Sequent Calculus Trainer, a tool that supports students in
learning how to correctly construct proofs in the sequent calculus for
first-order logic with equality. It is a proof assistant fostering the
understanding of all the syntactic principles that need to be obeyed in
constructing correct proofs. It does not provide any help in finding good proof
strategies. Instead it aims at understanding the sequent calculus on a lower
syntactic level that needs to be mastered before one can consider proof
strategies. Its main feature is a proper feedback system embedded in a
graphical user interface. We also report on some empirical findings that
indicate how the Sequent Calculus Trainer can improve the students' success in
learning sequent calculus for full first-order logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03667</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03667</id><created>2015-07-13</created><authors><author><keyname>Guallart</keyname><forenames>Nino</forenames></author><author><keyname>Nepomuceno-Fernandez</keyname><forenames>Angel</forenames></author></authors><title>Set theory and tableaux for teaching propositional logic</title><categories>cs.CY cs.LO</categories><comments>Proceedings of the Fourth International Conference on Tools for
  Teaching Logic (TTL2015), Rennes, France, June 9-12, 2015. Editors: M.
  Antonia Huertas, Jo\~ao Marcos, Mar\'ia Manzano, Sophie Pinchinat,
  Fran\c{c}ois Schwarzentruber</comments><proxy>Joao Marcos</proxy><acm-class>F.4.1</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this work we suggest the use of a set-theoretical interpretation of
semantic tableaux for teaching propositional logic. If the student has previous
notions of basic set theory, this approach to semantical tableaux can clarify
her the way semantic trees operate, linking the syntactical and semantical
sides of the process. Also, it may be useful for the introduction of more
advanced topics in logic, like modal logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03668</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03668</id><created>2015-07-13</created><authors><author><keyname>Joray</keyname><forenames>Pierre</forenames></author></authors><title>Teaching Le\'sniewski's Prothetic with a Natural Deduction System</title><categories>cs.CY cs.LO</categories><comments>Presented at the Fourth International Conference on Tools for
  Teaching Logic (TTL2015), Rennes, France, June 9-12, 2015</comments><proxy>Joao Marcos</proxy><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Protothetic is one of the most stimulating systems for propositional logic.
Including quantifiers and an inference rule for definitions, it is a very
interesting mean for the study of many questions of metalogic. Unfortunately,
it only exists in an axiomatic version, far too complicated and unusual to be
easily understood by nowadays students in logic. In this paper, we present a
system which is a natural deduction (in Fitch-Ja\'skowski's style) version of
protothetic. According to us, this system is adequate for teaching
Le\'sniewski's logic to students accustomed to natural deduction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03670</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03670</id><created>2015-07-13</created><authors><author><keyname>Kyrilov</keyname><forenames>Angelo</forenames></author><author><keyname>Noelle</keyname><forenames>David</forenames></author></authors><title>Using Automated Theorem Provers to Teach Knowledge Representation in
  First-Order Logic</title><categories>cs.CY cs.LO</categories><comments>Proceedings of the Fourth International Conference on Tools for
  Teaching Logic (TTL2015), Rennes, France, June 9-12, 2015. Editors: M.
  Antonia Huertas, Jo\~ao Marcos, Mar\'ia Manzano, Sophie Pinchinat,
  Fran\c{c}ois Schwarzentruber</comments><proxy>Joao Marcos</proxy><acm-class>K.3</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Undergraduate students of artificial intelligence often struggle with
representing knowledge as logical sentences. This is a skill that seems to
require extensive practice to obtain, suggesting a teaching strategy that
involves the assignment of numerous exercises involving the formulation of some
bit of knowledge, communicated using a natural language such as English, as a
sentence in some logic. The number of such exercises needed to master this
skill is far too large to allow typical artificial intelligence course teaching
teams to provide prompt feedback on student efforts. Thus, an automated
assessment system for such exercises is needed to ensure that students receive
an adequate amount of practice, with the rapid delivery of feedback allowing
students to identify errors in their understanding and correct them. This paper
describes an automated grading system for knowledge representation exercises
using first-order logic. A resolution theorem prover, \textit{Prover9}, is used
to check if a student-submitted formula is logically equivalent to a solution
provided by the instructor. This system has been used by students enrolled in
undergraduate artificial intelligence classes for several years. Use of this
teaching tool resulted in a statistically significant improvement on
first-order logic knowledge representation questions appearing on the course
final examination. This article explains how this system works, provides an
analysis of changes in student learning outcomes, and explores potential
enhancements of this system, including the possibility of providing rich
formative feedback by replacing the resolution theorem prover with a
tableaux-based method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03671</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03671</id><created>2015-07-13</created><authors><author><keyname>Lodder</keyname><forenames>Josje</forenames></author><author><keyname>Heeren</keyname><forenames>Bastiaan</forenames></author><author><keyname>Jeuring</keyname><forenames>Johan</forenames></author></authors><title>A pilot study of the use of LogEx, lessons learned</title><categories>cs.CY cs.LO</categories><comments>Proceedings of the Fourth International Conference on Tools for
  Teaching Logic (TTL2015), Rennes, France, June 9-12, 2015. Editors: M.
  Antonia Huertas, Jo\~ao Marcos, Mar\'ia Manzano, Sophie Pinchinat,
  Fran\c{c}ois Schwarzentruber</comments><proxy>Joao Marcos</proxy><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  LogEx is a learning environment that supports students in rewriting
propositional logical formulae, using standard equivalences. We organized a
pilot study to prepare a large scale evaluation of the learning environment. In
this paper we describe this study, together with the outcomes, which teach us
valuable lessons for the large scale evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03672</identifier>
 <datestamp>2015-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03672</id><created>2015-07-13</created><authors><author><keyname>Makowsky</keyname><forenames>Johann</forenames></author></authors><title>Teaching Logic for Computer Science: Are We Teaching the Wrong
  Narrative?</title><categories>cs.CY cs.LO</categories><comments>Proceedings of the Fourth International Conference on Tools for
  Teaching Logic (TTL2015), Rennes, France, June 9-12, 2015. Editors: M.
  Antonia Huertas, Jo\~ao Marcos, Mar\'ia Manzano, Sophie Pinchinat,
  Fran\c{c}ois Schwarzentruber</comments><proxy>Joao Marcos</proxy><acm-class>K.3.2</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper I discuss what, according to my long experience, every computer
scientist should know from logic. We concentrate on issues of modeling,
interpretability and levels of abstraction. We discuss what the minimal toolbox
of logic tools should look like for a computer scientist who is involved in
designing and analyzing reliable systems. We shall conclude that many classical
topics dear to logicians are less important than usually presented, and that
less-known ideas from logic may be more useful for the working computer
scientist.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03673</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03673</id><created>2015-07-13</created><updated>2015-07-17</updated><authors><author><keyname>Marcos</keyname><forenames>Jo&#xe3;o</forenames></author></authors><title>Fail better: What formalized math can teach us about learning</title><categories>cs.CY cs.LO</categories><comments>Proceedings of the Fourth International Conference on Tools for
  Teaching Logic (TTL2015), Rennes, France, June 9-12, 2015. Editors: M.
  Antonia Huertas, Jo\~ao Marcos, Mar\'ia Manzano, Sophie Pinchinat,
  Fran\c{c}ois Schwarzentruber</comments><proxy>Joao Marcos</proxy><acm-class>K.3.1; F.4.1</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Real-life conjectures do not come with instructions saying whether they they
should be proven or, instead, refuted. Yet, as we now know, in either case the
final argument produced had better be not just convincing but actually
verifiable in as much detail as our need for eliminating risk might require.
For those who do not happen to have direct access to the realm of mathematical
truths, the modern field of formalized mathematics has quite a few lessons to
contribute, and one might pay heed to what it has to say, for instance, about:
the importance of employing proof strategies; the fine control of automation in
unraveling the structure of a certain proof object; reasoning forward from the
givens and backward from the goals, in developing proof scripts; knowing when
and how definitions and identities apply in a helpful way, and when they do not
apply; seeing proofs [and refutations] as dynamical objects, not reflected by
the static derivation trees that Proof Theory wants them to be. I believe that
the great challenge for teachers and learners resides currently less on the
availability of suitable generic tools than in combining them wisely in view of
their preferred education paradigms and introducing them in a way that best
fits their specific aims, possibly with the help of intelligent online
interactive tutoring systems. As a proof of concept, a computerized proof
assistant that makes use of several successful tools already freely available
on the market and that takes into account some of the above findings about
teaching and learning Logic is hereby introduced. To fully account for our
informed intuitions on the subject it would seem that a little bit extra
technology would still be inviting, but no major breakthrough is really needed:
We are talking about tools that are already within our reach to develop, as the
fruits of collaborative effort.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03674</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03674</id><created>2015-07-13</created><authors><author><keyname>Huang</keyname><forenames>Heliang</forenames></author><author><keyname>Bao</keyname><forenames>Wansu</forenames></author></authors><title>Algorithm for Solving Massively Underdefined Systems of Multivariate
  Quadratic Equations over Finite Fields</title><categories>cs.CR cs.SC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Solving systems of m multivariate quadratic equations in n variables
(MQ-problem) over finite fields is NP-hard. The security of many cryptographic
systems is based on this problem. Up to now, the best algorithm for solving the
underdefined MQ-problem is Hiroyuki Miura et al.'s algorithm, which is a
polynomial-time algorithm when \[n \ge m(m + 3)/2\] and the characteristic of
the field is even. In order to get a wider applicable range, we reduce the
underdefined MQ-problem to the problem of finding square roots over finite
field, and then combine with the guess and determine method. In this way, the
applicable range is extended to \[n \ge m(m + 1)/2\], which is the widest range
until now. Theory analysis indicates that the complexity of our algorithm is
\[O(q{n^\omega }m{(\log {\kern 1pt} {\kern 1pt} q)^2}){\kern 1pt} \] when
characteristic of the field is even and \[O(q{2^m}{n^\omega }m{(\log {\kern
1pt} {\kern 1pt} q)^2})\] when characteristic of the field is odd, where \[2
\le \omega \le 3\] is the complexity of Gaussian elimination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03675</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03675</id><created>2015-07-13</created><authors><author><keyname>Materzok</keyname><forenames>Marek</forenames></author></authors><title>Easyprove: a tool for teaching precise reasoning</title><categories>cs.CY cs.LO</categories><comments>Proceedings of the Fourth International Conference on Tools for
  Teaching Logic (TTL2015), Rennes, France, June 9-12, 2015. Editors: M.
  Antonia Huertas, Jo\~ao Marcos, Mar\'ia Manzano, Sophie Pinchinat,
  Fran\c{c}ois Schwarzentruber</comments><proxy>Joao Marcos</proxy><acm-class>F.4.1</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Teaching precise mathematical reasoning can be very hard. It is very easy for
a student to make a subtle mistake in a proof which invalidates it, but it is
often hard for the teacher to pinpoint and explain the problem in the (often
chaotically written) student's proof. We present Easyprove, an interactive
proof assistant aimed at first year computer science students and high school
students, intended as a supplementary tool for teaching logical reasoning. The
system is a Web application with a natural, mouse-oriented user interface.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03676</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03676</id><created>2015-07-13</created><authors><author><keyname>Michelini</keyname><forenames>Juan</forenames></author><author><keyname>Tasistro</keyname><forenames>Alvaro</forenames></author></authors><title>Presentation of Classical Propositional Tableaux on Program Design
  Premises</title><categories>cs.CY cs.LO</categories><comments>Proceedings of the Fourth International Conference on Tools for
  Teaching Logic (TTL2015), Rennes, France, June 9-12, 2015. Editors: M.
  Antonia Huertas, Jo\~ao Marcos, Mar\'ia Manzano, Sophie Pinchinat,
  Fran\c{c}ois Schwarzentruber</comments><proxy>Joao Marcos</proxy><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We propose a presentation of classical propositional tableaux elaborated by
application of methods that are noteworthy in program design, namely program
derivation with separation of concerns. We start by deriving from a
straightforward specification an algorithm given as a set of recursive
equations for computing all models of a finite set of formulae. Thereafter we
discuss the employment of data structures, mainly with regard to an easily
traceable manual execution of the algorithm. This leads to the kinds of trees
given usually as constituting the tableaux. The whole development strives at
avoiding gaps, both of logical and motivational nature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03677</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03677</id><created>2015-07-13</created><authors><author><keyname>Minic&#x103;</keyname><forenames>&#x15e;tefan</forenames></author></authors><title>RAESON: A Tool for Reasoning Tasks Driven by Interactive Visualization
  of Logical Structure</title><categories>cs.CY cs.LO</categories><comments>Proceedings of the Fourth International Conference on Tools for
  Teaching Logic (TTL2015), Rennes, France, June 9-12, 2015. Editors: M.
  Antonia Huertas, Jo\~ao Marcos, Mar\'ia Manzano, Sophie Pinchinat,
  Fran\c{c}ois Schwarzentruber</comments><proxy>Joao Marcos</proxy><acm-class>I.2; I.2.4; K.3; K.3.1</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The paper presents a software tool for analysis and interactive engagement in
various logical reasoning tasks. A first feature of the program consists in
providing an interface for working with logic-specific repositories of formal
knowledge. A second feature provides the means to intuitively visualize and
interactively generate the underlying logical structure that propels customary
logical reasoning tasks. Starting from this we argue that both aspects have
didactic potential and can be integrated in teaching activities to provide an
engaging learning experience.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03678</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03678</id><created>2015-07-13</created><authors><author><keyname>Miranda-Perea</keyname><forenames>Favio E.</forenames></author><author><keyname>Linares-Ar&#xe9;valo</keyname><forenames>P. Selene</forenames></author><author><keyname>Aliseda</keyname><forenames>Atocha</forenames></author></authors><title>How to prove it in Natural Deduction: A Tactical Approach</title><categories>cs.CY cs.LO</categories><comments>Proceedings of the Fourth International Conference on Tools for
  Teaching Logic (TTL2015), Rennes, France, June 9-12, 2015. Editors: M.
  Antonia Huertas, Jo\~ao Marcos, Mar\'ia Manzano, Sophie Pinchinat,
  Fran\c{c}ois Schwarzentruber</comments><proxy>Joao Marcos</proxy><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The motivation for this paper comes out of our experience with teaching
natural deduction (ND) and with the way this formal system is implemented by
the \textsc{Coq} proof assistant, namely by means of so-called tactics, which
are heuristics that transform a goal formula into a sequence of subgoals whose
provability implies that of the original formula. We aim at capturing some of
these tactics into a system of ND for minimal logic. Our goal is twofold:
formal and didactic. The former delivers a formal system with its underlying
heuristics to build proofs, which in turn serves our latter purpose, that of
making an ideal system for the teaching of ND at an undergraduate level in a
computer science program.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03679</identifier>
 <datestamp>2015-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03679</id><created>2015-07-13</created><authors><author><keyname>Oller</keyname><forenames>Carlos</forenames></author><author><keyname>Coul&#xf3;</keyname><forenames>Ana</forenames></author></authors><title>Why teach an introductory course in Mathematical Logic in the Philosophy
  curriculum?</title><categories>cs.CY cs.LO</categories><comments>Proceedings of the Fourth International Conference on Tools for
  Teaching Logic (TTL2015), Rennes, France, June 9-12, 2015. Editors: M.
  Antonia Huertas, Jo\~ao Marcos, Mar\'ia Manzano, Sophie Pinchinat,
  Fran\c{c}ois Schwarzentruber</comments><proxy>Joao Marcos</proxy><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper tries to justify the relevance of an introductory course in
Mathematical Logic in the Philosophy curriculum for analyzing philosophical
arguments in natural language. It is argued that the representation of the
structure of natural language arguments in Freeman's diagramming system can
provide an intuitive foundation for the inferential processes involved in the
use of First Order Logic natural deduction rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03680</identifier>
 <datestamp>2015-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03680</id><created>2015-07-13</created><authors><author><keyname>Resek</keyname><forenames>Diane</forenames></author><author><keyname>Fendel</keyname><forenames>Dan</forenames></author></authors><title>Transitioning to Proof</title><categories>cs.CY cs.LO</categories><comments>Proceedings of the Fourth International Conference on Tools for
  Teaching Logic (TTL2015), Rennes, France, June 9-12, 2015. Editors: M.
  Antonia Huertas, Jo\~ao Marcos, Mar\'ia Manzano, Sophie Pinchinat,
  Fran\c{c}ois Schwarzentruber</comments><proxy>Joao Marcos</proxy><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper describes some strategies used in a `transition' course. Such
courses help undergraduate mathematics majors move from learning procedures to
learning to function as critical mathematicians in order to understand and work
with abstract concepts. One of the co-authors of this paper was a student of
Leon Henkin. His influence on her helped shape the strategies used in the
course, and is described at the end of the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03681</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03681</id><created>2015-07-13</created><authors><author><keyname>Seligman</keyname><forenames>Jeremy</forenames></author><author><keyname>Thompson</keyname><forenames>Declan</forenames></author></authors><title>Teaching natural deduction in the right order with Natural Deduction
  Planner</title><categories>cs.CY cs.LO</categories><comments>Proceedings of the Fourth International Conference on Tools for
  Teaching Logic (TTL2015), Rennes, France, June 9-12, 2015. Editors: M.
  Antonia Huertas, Jo\~ao Marcos, Mar\'ia Manzano, Sophie Pinchinat,
  Fran\c{c}ois Schwarzentruber</comments><proxy>Joao Marcos</proxy><acm-class>K.3.1</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We describe a strategy-based approach to teaching natural deduction using a
notation that emphasises the order in which deductions are constructed,
together with a {\LaTeX} package and Java app to aid in the production of
teaching resources and classroom demonstrations. Our approach is aimed at
students with little exposure to mathematical method and has been developed
while teaching undergraduate classes for philosophy students over the last ten
years.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03682</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03682</id><created>2015-07-13</created><authors><author><keyname>Silva</keyname><forenames>Nailton</forenames></author><author><keyname>Terrematte</keyname><forenames>Patrick</forenames></author><author><keyname>Moura</keyname><forenames>Jos&#xe9;</forenames></author></authors><title>ARG: Virtual Tool to Teaching Argumentation Theory</title><categories>cs.CY</categories><comments>Proceedings of the Fourth International Conference on Tools for
  Teaching Logic (TTL2015), Rennes, France, June 9-12, 2015. Editors: M.
  Antonia Huertas, Jo\~ao Marcos, Mar\'ia Manzano, Sophie Pinchinat,
  Fran\c{c}ois Schwarzentruber</comments><proxy>Joao Marcos</proxy><acm-class>K.3.1</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Researchers look for new virtual instruments that can improve and maximize
traditional forms of teaching and learning. In this paper, we present the ARG
system, a virtual tool developed to help the teaching/learning process in
argumentation theory, especially in the field of Law. ARG was developed based
on Araucaria by Reed and Rowe, Room 5 by Ronald P. Loui, as well on systems
such as Argue!-System and ArguMed by Bart Verheij. ARG is a platform for online
collaboration and applies the theory of Stephen Toulmin to produce arguments
that are more concise, precise, minimally structured and more resistant to
criticism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03683</identifier>
 <datestamp>2015-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03683</id><created>2015-07-13</created><authors><author><keyname>Slaney</keyname><forenames>John</forenames></author></authors><title>Logic considered fun</title><categories>cs.CY cs.LO</categories><comments>Proceedings of the Fourth International Conference on Tools for
  Teaching Logic (TTL2015), Rennes, France, June 9-12, 2015. Editors: M.
  Antonia Huertas, Jo\~ao Marcos, Mar\'ia Manzano, Sophie Pinchinat,
  Fran\c{c}ois Schwarzentruber</comments><proxy>Joao Marcos</proxy><acm-class>K.3.1</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This report describes the development and use of an online teaching tool
giving students exercises in logical modelling, or \emph{formalisation} as it
is called in the older literature. The original version of the site, `Logic for
Fun', dates from 2001, though it was little used except by small groups of
students at the Australian National University. It is currently in the process
of being replaced by a new version, free to all Internet users, intended to be
promoted widely as a useful addition to both online and traditional logic
courses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03684</identifier>
 <datestamp>2015-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03684</id><created>2015-07-13</created><authors><author><keyname>Takemura</keyname><forenames>Ryo</forenames></author></authors><title>Euler diagrams as an introduction to set-theoretical models</title><categories>cs.CY cs.LO</categories><comments>Proceedings of the Fourth International Conference on Tools for
  Teaching Logic (TTL2015), Rennes, France, June 9-12, 2015. Editors: M.
  Antonia Huertas, Jo\~ao Marcos, Mar\'ia Manzano, Sophie Pinchinat,
  Fran\c{c}ois Schwarzentruber</comments><proxy>Joao Marcos</proxy><acm-class>I.2.0</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Understanding the notion of a model is not always easy in logic courses.
Hence, tools such as Euler diagrams are frequently applied as informal
illustrations of set-theoretical models. We formally investigate Euler diagrams
as an introduction to set-theoretical models. We show that the model-theoretic
notions of validity and invalidity are characterized by Euler diagrams, and, in
particular, that model construction can be described as a manipulation of Euler
diagrams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03685</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03685</id><created>2015-07-13</created><authors><author><keyname>Terrematte</keyname><forenames>Patrick</forenames></author><author><keyname>Marcos</keyname><forenames>Jo&#xe3;o</forenames></author></authors><title>TryLogic tutorial: an approach to Learning Logic by proving and refuting</title><categories>cs.CY cs.LO</categories><comments>Proceedings of the Fourth International Conference on Tools for
  Teaching Logic (TTL2015), Rennes, France, June 9-12, 2015. Editors: M.
  Antonia Huertas, Jo\~ao Marcos, Mar\'ia Manzano, Sophie Pinchinat,
  Fran\c{c}ois Schwarzentruber</comments><proxy>Joao Marcos</proxy><acm-class>K.3.1; F.4.1</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Aiming to offer a framework for blended learning to the teaching of proof
theory, the present paper describes an interactive tutorial, called
\textsc{TryLogic}, teaching how to solve logical conjectures either by proofs
or refutations. The paper also describes the integration of our infrastructure
with the Virtual Learning Environment \texttt{Moodle} through the IMS Learning
Tools Interoperability specification, and evaluates the tool we have developed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03686</identifier>
 <datestamp>2015-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03686</id><created>2015-07-13</created><authors><author><keyname>Villemaire</keyname><forenames>Roger</forenames></author></authors><title>Logic Modelling</title><categories>cs.CY cs.LO</categories><comments>Proceedings of the Fourth International Conference on Tools for
  Teaching Logic (TTL2015), Rennes, France, June 9-12, 2015. Editors: M.
  Antonia Huertas, Jo\~ao Marcos, Mar\'ia Manzano, Sophie Pinchinat,
  Fran\c{c}ois Schwarzentruber</comments><proxy>Joao Marcos</proxy><acm-class>F.4.1; I.2.2; I.2.3; I.2.4</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This is a reflection on the author's experience in teaching logic at the
graduate level in a computer science department. The main lesson is that model
building and the process of modelling must be placed at the centre stage of
logic teaching. Furthermore, effective use must be supported with adequate
tools. Finally, logic is the methodology underlying many applications, it is
hence paramount to pass on its principles, methods and concepts to computer
science audiences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03687</identifier>
 <datestamp>2015-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03687</id><created>2015-07-13</created><authors><author><keyname>Zamansky</keyname><forenames>Anna</forenames></author><author><keyname>Farchi</keyname><forenames>Eitan</forenames></author></authors><title>Teaching Logic to Information Systems Students: Challenges and
  Opportunities</title><categories>cs.CY cs.LO</categories><comments>Proceedings of the Fourth International Conference on Tools for
  Teaching Logic (TTL2015), Rennes, France, June 9-12, 2015. Editors: M.
  Antonia Huertas, Jo\~ao Marcos, Mar\'ia Manzano, Sophie Pinchinat,
  Fran\c{c}ois Schwarzentruber</comments><proxy>Joao Marcos</proxy><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In contrast to Computer Science, where the fundamental role of Logic is
widely recognized, it plays a practically non-existent role in Information
Systems curricula. In this paper we argue that instead of Logic's exclusion
from the IS curriculum, a significant adaptation of the contents, as well as
teaching methodologies, is required for an alignment with the needs of IS
practitioners. We present our vision for such adaptation and report on concrete
steps towards its implementation in the design and teaching of a course for
graduate IS students at the University of Haifa. We discuss the course plan and
present some data on the students' feedback on the course.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03691</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03691</id><created>2015-07-13</created><authors><author><keyname>Chen</keyname><forenames>Fang</forenames></author><author><keyname>Yang</keyname><forenames>Bo</forenames></author><author><keyname>Han</keyname><forenames>Qiaoni</forenames></author><author><keyname>Chen</keyname><forenames>Cailian</forenames></author><author><keyname>Guan</keyname><forenames>Xinping</forenames></author></authors><title>Dynamic Sleep Control in Green Relay-Assisted Networks for Energy Saving
  and QoS Improving</title><categories>cs.NI</categories><comments>7 papers, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the relay station (RS) sleep control mechanism targeting on reducing
energy consumption while improving users' quality of service (QoS) in green
relay-assisted cellular networks, where the base station (BS) is powered by
grid power and the RSs are powered by renewable energy. By adopting green RSs,
the grid power consumption of the BS is greatly reduced. But due to the
uncertainty and stochastic characteristics of the renewable energy, power
supply for RSs is not always sufficient. Thus the harvested energy needs to be
scheduled appropriately to cater to the dynamic traffic so as to minimize the
energy saving in the long term. An optimization problem is formulated to find
the optimal sleep ratio of RSs to match the time variation of energy harvesting
and traffic arrival. To fully use the renewable energy, green-RS-first
principle is adopted in the user association process. The optimal RS sleeping
policy is obtained through dynamic programming (DP) approach, which divides the
original optimization problem into per-stage subproblems. A reduced DP
algorithm and a greedy algorithm are further proposed to greatly reduce the
computation complexity. By simulations, the reduced DP algorithm outperforms
the greedy algorithm in achieving satisfactory energy saving and QoS
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03698</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03698</id><created>2015-07-13</created><updated>2016-01-08</updated><authors><author><keyname>D&#xed;az</keyname><forenames>Ra&#xfa;l</forenames></author><author><keyname>Lee</keyname><forenames>Minhaeng</forenames></author><author><keyname>Schubert</keyname><forenames>Jochen</forenames></author><author><keyname>Fowlkes</keyname><forenames>Charless C.</forenames></author></authors><title>Lifting GIS Maps into Strong Geometric Context for Scene Understanding</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contextual information can have a substantial impact on the performance of
visual tasks such as semantic segmentation, object detection, and geometric
estimation. Data stored in Geographic Information Systems (GIS) offers a rich
source of contextual information that has been largely untapped by computer
vision. We propose to leverage such information for scene understanding by
combining GIS resources with large sets of unorganized photographs using
Structure from Motion (SfM) techniques. We present a pipeline to quickly
generate strong 3D geometric priors from 2D GIS data using SfM models aligned
with minimal user input. Given an image resectioned against this model, we
generate robust predictions of depth, surface normals, and semantic labels. We
show that the precision of the predicted geometry is substantially more
accurate other single-image depth estimation methods. We then demonstrate the
utility of these contextual constraints for re-scoring pedestrian detections,
and use these GIS contextual features alongside object detection score maps to
improve a CRF-based semantic segmentation framework, boosting accuracy over
baseline models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03699</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03699</id><created>2015-07-13</created><authors><author><keyname>Liao</keyname><forenames>Guojun G</forenames></author><author><keyname>Chen</keyname><forenames>Xi</forenames></author><author><keyname>Cai</keyname><forenames>Xianxin</forenames></author><author><keyname>Hildebrand</keyname><forenames>Ben</forenames></author><author><keyname>Fleitas</keyname><forenames>Dion</forenames></author></authors><title>A New Method for Triangular Mesh Generation</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computational mathematics plays an increasingly important role in
computational fluid dynamics (CFD). The aeronautics and aerospace re- search
community is working on next generation of CFD capacity that is accurate,
automatic, and fast. A key component of the next generation of CFD is a greatly
enhanced capacity for mesh generation and adaptivity of the mesh according to
solution and geometry. In this paper, we propose a new method that generates
triangular meshes on domains of curved boundary. The method deforms a Cartesian
mesh that covers the domain to generate a mesh with prescribed boundary nodes.
The deformation fields are generated by a system of divergence and curl
equations which are solved effectively by the least square finite element
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03701</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03701</id><created>2015-07-13</created><authors><author><keyname>L&#xe9;vesque</keyname><forenames>Martin</forenames></author></authors><title>Http-Burst: Improving HTTP Efficiency in the Era of Bandwidth Hungry Web
  Applications</title><categories>cs.NI</categories><comments>5 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Hypertext Transfer Protocol (HTTP), a key building block of the World
Wide Web, has succeeded to enable information exchange worldwide. Since its
first version in 1996, HTTP/1.0, the average number of inlined objects and
average total bytes per webpage have been increasing significantly for desktops
and mobiles, from 1-10 objects in 1996 to more than 100 objects in June 2014.
Even if the retrieving of inlined objects can be parallelized as a given
Hypertext Markup Language (HTML) document is streamed, a maximum number of
connections is allocated, and thus as the number of inlined objects increases,
the overall webpage load duration grows, and the HTTP servers loading also gets
higher. To overcome this issue, we propose a new HTTP method called BURST,
which allows to retrieve the missing inlined objects of a webpage efficiently
by requesting sets of web objects. We experimentally demonstrate the potential
via a proof-of-concept demonstration, by comparing the regular HTTP to proposed
HTTP-Burst using a virtual private server and real HTTP client and server over
the Internet. The results indicate a latency reduction of webpage load duration
compared to HTTP as high as 52 % under the considered configurations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03707</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03707</id><created>2015-07-13</created><authors><author><keyname>Cai</keyname><forenames>Jian-Feng</forenames></author><author><keyname>Liu</keyname><forenames>Suhui</forenames></author><author><keyname>Xu</keyname><forenames>Weiyu</forenames></author></authors><title>Projected Wirtinger Gradient Descent for Low-Rank Hankel Matrix
  Completion in Spectral Compressed Sensing</title><categories>cs.IT cs.LG math.IT math.OC</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers reconstructing a spectrally sparse signal from a small
number of randomly observed time-domain samples. The signal of interest is a
linear combination of complex sinusoids at $R$ distinct frequencies. The
frequencies can assume any continuous values in the normalized frequency domain
$[0,1)$. After converting the spectrally sparse signal recovery into a low rank
structured matrix completion problem, we propose an efficient feasible point
approach, named projected Wirtinger gradient descent (PWGD) algorithm, to
efficiently solve this structured matrix completion problem. We further
accelerate our proposed algorithm by a scheme inspired by FISTA. We give the
convergence analysis of our proposed algorithms. Extensive numerical
experiments are provided to illustrate the efficiency of our proposed
algorithm. Different from earlier approaches, our algorithm can solve problems
of very large dimensions very efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03715</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03715</id><created>2015-07-14</created><authors><author><keyname>Chen</keyname><forenames>Xi</forenames></author><author><keyname>Liao</keyname><forenames>Guojun</forenames></author></authors><title>New Variational Method of Grid Generation with Prescribed Jacobian
  Determinant and Prescribed Curl</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adaptive grid generation is an active research topic for numer- ical solution
of di?erential equations. In this paper, we propose a variational method which
generates transformations with prescribed Jacobian determinant and curl. Then
we use this transformation to achieve adaptive grid generation task, and show
the importance of curl in a transformation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03716</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03716</id><created>2015-07-14</created><authors><author><keyname>Burger</keyname><forenames>Jens</forenames></author><author><keyname>Goudarzi</keyname><forenames>Alireza</forenames></author><author><keyname>Stefanovic</keyname><forenames>Darko</forenames></author><author><keyname>Teuscher</keyname><forenames>Christof</forenames></author></authors><title>Computational Capacity and Energy Consumption of Complex Resistive
  Switch Networks</title><categories>cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Resistive switches are a class of emerging nanoelectronics devices that
exhibit a wide variety of switching characteristics closely resembling
behaviors of biological synapses. Assembled into random networks, such
resistive switches produce emerging behaviors far more complex than that of
individual devices. This was previously demonstrated in simulations that
exploit information processing within these random networks to solve tasks that
require nonlinear computation as well as memory. Physical assemblies of such
networks manifest complex spatial structures and basic processing capabilities
often related to biologically-inspired computing. We model and simulate random
resistive switch networks and analyze their computational capacities. We
provide a detailed discussion of the relevant design parameters and establish
the link to the physical assemblies by relating the modeling parameters to
physical parameters. More globally connected networks and an increased network
switching activity are means to increase the computational capacity linearly at
the expense of exponentially growing energy consumption. We discuss a new
modular approach that exhibits higher computational capacities and energy
consumption growing linearly with the number of networks used. The results show
how to optimize the trade-off between computational capacity and energy
efficiency and are relevant for the design and fabrication of novel computing
architectures that harness random assemblies of emerging nanodevices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03719</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03719</id><created>2015-07-14</created><authors><author><keyname>Barbosa</keyname><forenames>Rafael da Ponte</forenames></author><author><keyname>Ene</keyname><forenames>Alina</forenames></author><author><keyname>Nguyen</keyname><forenames>Huy L.</forenames></author><author><keyname>Ward</keyname><forenames>Justin</forenames></author></authors><title>A New Framework for Distributed Submodular Maximization</title><categories>cs.DS cs.AI cs.DC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A wide variety of problems in machine learning, including exemplar
clustering, document summarization, and sensor placement, can be cast as
constrained submodular maximization problems. A lot of recent effort has been
devoted to developing distributed algorithms for these problems. However, these
results suffer from high number of rounds, suboptimal approximation ratios, or
both. We develop a framework for bringing existing algorithms in the sequential
setting to the distributed setting, achieving near optimal approximation ratios
for many settings in only a constant number of MapReduce rounds. Our techniques
also give a fast sequential algorithm for non-monotone maximization subject to
a matroid constraint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03727</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03727</id><created>2015-07-14</created><authors><author><keyname>Lingelbach</keyname><forenames>Frank</forenames></author></authors><title>On Probabilistic Completeness of Probabilistic Cell Decomposition</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic Cell Decomposition (PCD) is a probabilistic path planning
method combining the concepts of approximate cell decomposition with
probabilistic sampling. It has been shown that the use of lazy evaluation
techniques and supervised sampling in important areas result in a high
performance path planning method. Even if it was postulated before that PCD is
probabilistically complete, we present a detailed proof of probabilistic
completeness here for the first time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03729</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03729</id><created>2015-07-14</created><authors><author><keyname>Yan</keyname><forenames>Shihao</forenames></author><author><keyname>Yang</keyname><forenames>Nan</forenames></author><author><keyname>Geraci</keyname><forenames>Giovanni</forenames></author><author><keyname>Malaney</keyname><forenames>Robert</forenames></author><author><keyname>Yuan</keyname><forenames>Jinhong</forenames></author></authors><title>Optimization of Code Rates in SISOME Wiretap Channels</title><categories>cs.IT math.IT</categories><comments>13 pages, 12 figures, accepted by IEEE Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new framework for determining the wiretap code rates of
single-input single-output multi-antenna eavesdropper (SISOME) wiretap channels
when the capacity of the eavesdropper's channel is not available at the
transmitter. In our framework we introduce the effective secrecy throughput
(EST) as a new performance metric that explicitly captures the two key features
of wiretap channels, namely, reliability and secrecy. Notably, the EST measures
the average rate of the confidential information transmitted from the
transmitter to the intended receiver without being eavesdropped on. We provide
easy-to-implement methods to determine the wiretap code rates for two
transmission schemes: 1) adaptive transmission scheme in which the capacity of
the main channel is available at the transmitter and 2) fixed-rate transmission
scheme in which the capacity of the main channel is not available at the
transmitter. Such determinations are further extended into an absolute-passive
eavesdropping scenario where even the average signal-to-noise ratio of the
eavesdropper's channel is not available at the transmitter. Notably, our
solutions for the wiretap code rates do not require us to set reliability or
secrecy constraints for the transmission within wiretap channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03738</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03738</id><created>2015-07-14</created><authors><author><keyname>Fomin</keyname><forenames>Fedor V.</forenames></author><author><keyname>Golovnev</keyname><forenames>Alexander</forenames></author><author><keyname>Kulikov</keyname><forenames>Alexander S.</forenames></author><author><keyname>Mihajlin</keyname><forenames>Ivan</forenames></author></authors><title>Tight Bounds for Subgraph Isomorphism and Graph Homomorphism</title><categories>cs.DS</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that unless Exponential Time Hypothesis (ETH) fails, deciding if
there is a homomorphism from graph $G$ to graph $H$ cannot be done in time
$|V(H)|^{o(|V(G)|)}$. Combined with the reduction of Cygan, Pachocki, and
Soca{\l}a, our result rules out (subject to ETH) a possibility of
$|V(G)|^{o(|V(G)|)}$-time algorithm deciding if graph $H$ is a subgraph of $G$.
For both problems our lower bounds asymptotically match the running time of
brute-force algorithms trying all possible mappings of one graph into another.
Thus, our work closes the gap in the known complexity of these fundamental
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03751</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03751</id><created>2015-07-14</created><authors><author><keyname>Harringer</keyname><forenames>Manfred</forenames></author></authors><title>Closed Curves and Elementary Visual Object Identification</title><categories>cs.CV cs.LG q-bio.NC</categories><comments>13 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For two closed curves on a plane (discrete version) and local criteria for
similarity of points on the curves one gets a potential, which describes the
similarity between curve points. This is the base for a global similarity
measure of closed curves (Fr\'echet distance). I use borderlines of handwritten
digits to demonstrate an area of application. I imagine, measuring the
similarity of closed curves is an essential and elementary task performed by a
visual system. This approach to similarity measures may be used by visual
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03754</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03754</id><created>2015-07-14</created><authors><author><keyname>de Lima</keyname><forenames>Carlos H. M.</forenames></author><author><keyname>Nardelli</keyname><forenames>Pedro H. J.</forenames></author><author><keyname>Alves</keyname><forenames>Hirley</forenames></author><author><keyname>Latva-aho</keyname><forenames>Matti</forenames></author></authors><title>Contention-based Geographic Forwarding Strategies for Wireless Sensors
  Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies combined relay selection and opportunistic geographic
routing strategies for autonomous wireless sensor networks where transmissions
occur over multiple hops. The proposed solution is built upon three constituent
parts: (i) relay selection algorithm, (ii) contention resolution mechanism, and
(iii) geographic forwarding strategy. Using probability generating function and
spatial point process as the theoretic background, we propose an auction-based
algorithm for selecting the relay node that relies on the network topology as
side-information. Our results show that geographic solutions that iteratively
exploit the local knowledge of the topology to ponder the algorithm operation
outperforms established random approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03761</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03761</id><created>2015-07-14</created><authors><author><keyname>de Lima</keyname><forenames>Carlos H. M.</forenames></author><author><keyname>Alves</keyname><forenames>Hirley</forenames></author><author><keyname>Nardelli</keyname><forenames>Pedro. H. J.</forenames></author><author><keyname>Latva-aho</keyname><forenames>Matti</forenames></author></authors><title>Effects of Relay Selection Strategies on the Spectral Efficiency of
  Wireless Systems with Half- and Full-duplex Nodes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work proposes an analytical framework to study how relay selection
strategies perform in half- and full-duplex deployments by combining renewal
theory and stochastic geometry. Specifically, we assume that the network nodes
-- operating in either half- or full-duplex mode -- are scattered according to
a two-dimensional homogeneous Poisson point process to compute the relay
selection cost by using a semi-Markov process. Our results show: ($i$) fixed
relay outperforms the reactive option in either cases, ($ii$) the performance
of both reactive and fixed relay strategies depends on the self-interference
attenuation in full-duplex scenarios, evincing when they outperform the
half-duplex option, and ($iii$) the reactive relay selection suffers from
selecting relays at hop basis, while the fixed relay selection benefits most
from the full-duplex communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03762</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03762</id><created>2015-07-14</created><authors><author><keyname>Bergel</keyname><forenames>Itsik</forenames></author><author><keyname>Perets</keyname><forenames>Yona</forenames></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames></author></authors><title>Uplink Downlink Rate Balancing and throughput scaling in FDD Massive
  MIMO Systems</title><categories>cs.IT math.IT</categories><comments>Submitted to the IEEE Transactions on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we extend the concept of uplink-downlink rate balancing to
frequency division duplex (FDD) massive MIMO systems. We consider a base
station with large number antennas serving many single antenna users. We first
show that any unused capacity in the uplink can be traded off for higher
throughput in the downlink in a system that uses either dirty paper (DP) coding
or linear zero-forcing (ZF) precoding. We then also study the scaling of the
system throughput with the number of antennas in cases of linear Beamforming
(BF) Precoding, ZF Precoding, and DP coding. We show that the downlink
throughput is proportional to the logarithm of the number of antennas. While,
this logarithmic scaling is lower than the linear scaling of the rate in the
uplink, it can still bring significant throughput gains. For example, we
demonstrate through analysis and simulation that increasing the number of
antennas from 4 to 128 will increase the throughput by more than a factor of 5.
We also show that a logarithmic scaling of downlink throughput as a function of
the number of receive antennas can be achieved even when the number of transmit
antennas only increases logarithmically with the number of receive antennas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03765</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03765</id><created>2015-07-14</created><updated>2015-09-17</updated><authors><author><keyname>Wu</keyname><forenames>Chenmiao</forenames></author><author><keyname>Yang</keyname><forenames>Li</forenames></author></authors><title>A complete Classification of Quantum Public-key Encryption Protocols</title><categories>quant-ph cs.CR</categories><comments>14 pages, no figure</comments><doi>10.1117/12.2194067</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a classification of quantum public-key encryption protocols. There
are six elements in quantum public-key encryption: plaintext, ciphertext,
public-key, private-key, encryption algorithm and decryption algorithm.
According to the property of each element which is either quantum or classical,
the quantum public-key encryption protocols can be divided into 64 kinds. Among
64 kinds of protocols, 8 kinds have already been constructed, 52 kinds can be
proved to be impossible to construct and the remaining 4 kinds have not been
presented effectively yet. This indicates that the research on quantum
public-key encryption protocol should be focus on the existed kinds and the
unproposed kinds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03767</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03767</id><created>2015-07-14</created><updated>2015-09-17</updated><authors><author><keyname>Graziotin</keyname><forenames>Daniel</forenames></author><author><keyname>Wang</keyname><forenames>Xiaofeng</forenames></author><author><keyname>Abrahamsson</keyname><forenames>Pekka</forenames></author></authors><title>Understanding the Affect of Developers: Theoretical Background and
  Guidelines for Psychoempirical Software Engineering</title><categories>cs.SE cs.HC</categories><comments>9 pages, 2 figures</comments><acm-class>D.2.9; H.1.2; J.4</acm-class><journal-ref>7th Intl. Workshop on Social Software Engineering, pp. 25-32, 2015</journal-ref><doi>10.1145/2804381.2804386</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Affects---emotions and moods---have an impact on cognitive processing
activities and the working performance of individuals. It has been established
that software development tasks are undertaken through cognitive processing
activities. Therefore, we have proposed to employ psychology theory and
measurements in software engineering (SE) research. We have called it
&quot;psychoempirical software engineering&quot;. However, we found out that existing SE
research has often fallen into misconceptions about the affect of developers,
lacking in background theory and how to successfully employ psychological
measurements in studies. The contribution of this paper is threefold. (1) It
highlights the challenges to conduct proper affect-related studies with
psychology; (2) it provides a comprehensive literature review in affect theory;
and (3) it proposes guidelines for conducting psychoempirical software
engineering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03770</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03770</id><created>2015-07-14</created><authors><author><keyname>Khosravian</keyname><forenames>Alireza</forenames></author><author><keyname>Trumpf</keyname><forenames>Jochen</forenames></author><author><keyname>Mahony</keyname><forenames>Robert</forenames></author><author><keyname>Lageman</keyname><forenames>Christian</forenames></author></authors><title>Observers for invariant systems on Lie groups with biased input
  measurements and homogeneous outputs</title><categories>cs.SY</categories><comments>11 pages</comments><msc-class>93C10 (Primary), 93C40</msc-class><journal-ref>Automatica 55 (2015) 19-26</journal-ref><doi>10.1016/j.automatica.2015.02.030</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides a new observer design methodology for invariant systems
whose state evolves on a Lie group with outputs in a collection of related
homogeneous spaces and where the measurement of system input is corrupted by an
unknown constant bias. The key contribution of the paper is to study the
combined state and input bias estimation problem in the general setting of Lie
groups, a question for which only case studies of specific Lie groups are
currently available. We show that any candidate observer (with the same state
space dimension as the observed system) results in non-autonomous error
dynamics, except in the trivial case where the Lie-group is Abelian. This
precludes the application of the standard non-linear observer design
methodologies available in the literature and leads us to propose a new design
methodology based on employing invariant cost functions and general gain
mappings. We provide a rigorous and general stability analysis for the case
where the underlying Lie group allows a faithful matrix representation. We
demonstrate our theory in the example of rigid body pose estimation and show
that the proposed approach unifies two competing pose observers published in
prior literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03773</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03773</id><created>2015-07-14</created><authors><author><keyname>Mochaourab</keyname><forenames>Rami</forenames></author><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Bengtsson</keyname><forenames>Mats</forenames></author></authors><title>Pilot Clustering in Asymmetric Massive MIMO Networks</title><categories>cs.IT math.IT</categories><comments>Published in Proc. of IEEE International Workshop on Signal
  Processing Advances in Wireless Communications (SPAWC '15), 5 pages, 1
  tables, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the uplink of a cellular massive MIMO network. Since the spectral
efficiency of these networks is limited by pilot contamination, the pilot
allocation across cells is of paramount importance. However, finding efficient
pilot reuse patterns is non-trivial especially in practical asymmetric base
station deployments. In this paper, we approach this problem using coalitional
game theory. Each cell has its own unique pilots and can form coalitions with
other cells to gain access to more pilots. We develop a low-complexity
distributed algorithm and prove convergence to an individually stable coalition
structure. Simulations reveal fast algorithmic convergence and substantial
performance gains over one-cell coalitions and full pilot reuse.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03780</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03780</id><created>2015-07-14</created><authors><author><keyname>Gupta</keyname><forenames>Udit</forenames></author></authors><title>Monitoring in IOT enabled devices</title><categories>cs.NI</categories><comments>6 pages, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As network size continues to grow exponentially, there has been a
proportionate increase in the number of nodes in the corresponding network.
With the advent of Internet of things (IOT), it is assumed that many more
devices will be connected to the existing network infrastructure. As a result,
monitoring is expected to get more complex for administrators as networks tend
to become more heterogeneous. Moreover, the addressing for IOTs would be more
complex given the scale at which devices will be added to the network and hence
monitoring is bound to become an uphill task due to management of larger range
of addresses. This paper will throw light on what kind of monitoring mechanisms
can be deployed in internet of things (IOTs) and their overall effectiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03786</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03786</id><created>2015-07-14</created><updated>2015-09-21</updated><authors><author><keyname>Brihaye</keyname><forenames>Thomas</forenames></author><author><keyname>Geeraerts</keyname><forenames>Gilles</forenames></author><author><keyname>Haddad</keyname><forenames>Axel</forenames></author><author><keyname>Lefaucheux</keyname><forenames>Engel</forenames></author><author><keyname>Monmege</keyname><forenames>Benjamin</forenames></author></authors><title>Simple Priced Timed Games Are Not That Simple</title><categories>cs.GT</categories><acm-class>D.2.4; F.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Priced timed games are two-player zero-sum games played on priced timed
automata (whose locations and transitions are labeled by weights modeling the
costs of spending time in a state and executing an action, respectively). The
goals of the players are to minimise and maximise the cost to reach a target
location, respectively. We consider priced timed games with one clock and
arbitrary (positive and negative) weights and show that, for an important
subclass of theirs (the so-called simple priced timed games), one can compute,
in exponential time, the optimal values that the players can achieve, with
their associated optimal strategies. As side results, we also show that
one-clock priced timed games are determined and that we can use our result on
simple priced timed games to solve the more general class of so-called
reset-acyclic priced timed games (with arbitrary weights and one-clock).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03804</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03804</id><created>2015-07-14</created><authors><author><keyname>Bergel</keyname><forenames>Itsik</forenames></author><author><keyname>Yellin</keyname><forenames>Daniel</forenames></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames></author></authors><title>A lower bound on the data rate of dirty paper coding in general noise
  and interference</title><categories>cs.IT math.IT</categories><comments>Published in the IEEE Wireless Communications Letters</comments><doi>10.1109/LWC.2014.2323355</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dirty paper coding (DPC) allows a transmitter to send information to a
receiver in the presence of interference that is known (non-causally) to the
transmitter. The original version of DPC was derived for the case where the
noise and the interference are statistically independent Gaussian random
sequences. More recent works extended this approach to the case where the noise
and the interference are mutually independent and at least one of them is
Gaussian. In this letter we further extend the DPC scheme by relaxing the
Gaussian and statistical independence assumptions. We provide lower bounds on
the achievable data rates in a DPC setting for the case of possibly dependent
noise, interference and input signals. Also, the interference and noise terms
are allowed to have arbitrary probability distributions. The bounds are
relatively simple, are phrased in terms of second-order statistics, and are
tight when the actual noise distribution is close to Gaussian.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03811</identifier>
 <datestamp>2015-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03811</id><created>2015-07-14</created><authors><author><keyname>Presti</keyname><forenames>Liliana Lo</forenames></author><author><keyname>La Cascia</keyname><forenames>Marco</forenames></author></authors><title>Ensemble of Hankel Matrices for Face Emotion Recognition</title><categories>cs.CV cs.HC cs.RO</categories><comments>Paper to appear in Proc. of ICIAP 2015. arXiv admin note: text
  overlap with arXiv:1506.05001</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a face emotion is considered as the result of the composition
of multiple concurrent signals, each corresponding to the movements of a
specific facial muscle. These concurrent signals are represented by means of a
set of multi-scale appearance features that might be correlated with one or
more concurrent signals. The extraction of these appearance features from a
sequence of face images yields to a set of time series. This paper proposes to
use the dynamics regulating each appearance feature time series to recognize
among different face emotions. To this purpose, an ensemble of Hankel matrices
corresponding to the extracted time series is used for emotion classification
within a framework that combines nearest neighbor and a majority vote schema.
Experimental results on a public available dataset shows that the adopted
representation is promising and yields state-of-the-art accuracy in emotion
classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03819</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03819</id><created>2015-07-14</created><authors><author><keyname>Buchin</keyname><forenames>Kevin</forenames></author><author><keyname>Ophelders</keyname><forenames>Tim</forenames></author><author><keyname>Speckmann</keyname><forenames>Bettina</forenames></author></authors><title>Computing the Similarity Between Moving Curves</title><categories>cs.CG</categories><acm-class>I.3.5</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper we study similarity measures for moving curves which can, for
example, model changing coastlines or retreating glacier termini. Points on a
moving curve have two parameters, namely the position along the curve as well
as time. We therefore focus on similarity measures for surfaces, specifically
the Fr\'echet distance between surfaces. While the Fr\'echet distance between
surfaces is not even known to be computable, we show for variants arising in
the context of moving curves that they are polynomial-time solvable or
NP-complete depending on the restrictions imposed on how the moving curves are
matched. We achieve the polynomial-time solutions by a novel approach for
computing a surface in the so-called free-space diagram based on max-flow
min-cut duality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03823</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03823</id><created>2015-07-14</created><authors><author><keyname>Grimm</keyname><forenames>Carsten</forenames></author></authors><title>A Lower Bound on Supporting Predecessor Search in $k$ sorted Arrays</title><categories>cs.DS</categories><comments>This work was presented at the Young Researcher Workshop on Automata,
  Languages and Programming (YR-ICALP 2015), July 5th, 2015 in Kyoto, Japan</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We seek to perform efficient queries for the predecessor among $n$ values
stored in $k$ sorted arrays. Evading the $\Omega(n \log k)$ lower bound from
merging $k$ arrays, we support predecessor queries in $O(\log n)$ time after
$O(n \log(\frac{k}{\log n}))$ construction time. By applying Ben-Or's
technique, we establish that this is optimal for strict predecessor queries,
i.e., every data structure supporting $O(\log n)$-time strict predecessor
queries requires $\Omega(n \log(\frac{k}{\log n}))$ construction time. Our
approach generalizes as a template for deriving similar lower bounds on the
construction time of data structures with some desired query time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03826</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03826</id><created>2015-07-14</created><authors><author><keyname>Nunes</keyname><forenames>Davide</forenames></author><author><keyname>Antunes</keyname><forenames>Luis</forenames></author></authors><title>Modelling Structured Societies: a Multi-relational Approach to Context
  Permeability</title><categories>cs.MA cs.SI physics.soc-ph</categories><journal-ref>doi:10.1016/j.artint.2015.08.003</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The structure of social relations is fundamental for the construction of
plausible simulation scenarios. It shapes the way actors interact and create
their identity within overlapping social contexts. Each actor interacts in
multiple contexts within different types of social relations that constitute
their social space. In this article, we present an approach to model structured
agent societies with multiple coexisting social networks. We study the notion
of context permeability, using a game in which agents try to achieve global
consensus. We design and analyse two different models of permeability. In the
first model, agents interact concurrently in multiple social networks. In the
second, we introduce a context switching mechanism which adds a dynamic
temporal component to agent interaction in the model. Agents switch between the
different networks spending more or less time in each one. We compare these
models and analyse the influence of different social networks regarding the
speed of convergence to consensus. We conduct a series of experiments that show
the impact of different configurations for coexisting social networks. This
approach unveils both the limitations of the current modelling approaches and
possible research directions for complex social space simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03834</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03834</id><created>2015-07-14</created><updated>2015-08-08</updated><authors><author><keyname>Dai</keyname><forenames>Liyun</forenames></author><author><keyname>Han</keyname><forenames>Jingjun</forenames></author><author><keyname>Hong</keyname><forenames>Hoon</forenames></author><author><keyname>Xia</keyname><forenames>Bican</forenames></author></authors><title>Open Weak CAD and Its Applications</title><categories>cs.SC</categories><comments>31 pages. arXiv admin note: substantial text overlap with
  arXiv:1401.4953</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The concept of open weak CAD is introduced. Every open CAD is an open weak
CAD. On the contrary, an open weak CAD is not necessarily an open CAD. An
algorithm for computing open weak CADs is proposed. The key idea is to compute
the intersection of projection factor sets produced by different projection
orders. The resulting open weak CAD often has smaller number of sample points
than open CADs.
  The algorithm can be used for computing sample points for all open connected
components of $ f\ne 0$ for a given polynomial $f$. It can also be used for
many other applications, such as testing semi-definiteness of polynomials and
copositive problems. In fact, we solved several difficult semi-definiteness
problems efficiently by using the algorithm. Furthermore, applying the
algorithm to copositive problems, we find an explicit expression of the
polynomials producing open weak CADs under some conditions, which significantly
improves the efficiency of solving copositive problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03838</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03838</id><created>2015-07-14</created><authors><author><keyname>Elmusrati</keyname><forenames>Mohammed</forenames></author></authors><title>Towards Green and Infinite Capacity in Wireless Communication Networks:
  Beyond The Shannon Theorem</title><categories>cs.IT math.IT</categories><comments>Version of this paper has been submitted to IEEE Transaction on
  Wireless Communication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  New and novel way for resources allocation in wireless communication has been
proposed in this paper. Under this new method, it has been shown that the
required power budget becomes independent of the number of served terminals in
the downlink. However, the required power depends only of the coverage area,
i.e. the channel losses at the cell boarder. Therefore, huge number
(theoretically any number) of terminals could be supported concurrently at
finite and small downlink power budget. This could be very useful to support
the downlink signalling channels in HSPA+, LTE, and 5G. It can be very useful
also to support huge D2D communication downlinks. Moreover, and based on the
same concept, a new system configuration for a single link point-to-point
communication has been presented. With this new configuration, the achieved
data rate becomes independent of the required transmit power. This means that
any data rate can be achieved at the target BER and with small and finite
transmit power. This seems violating with some major results of the Shannon
theorem. This issue will be discussed in details in this article.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03839</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03839</id><created>2015-07-14</created><authors><author><keyname>Lakshminarayana</keyname><forenames>Subhash</forenames></author><author><keyname>Assaad</keyname><forenames>Mohamad</forenames></author><author><keyname>Debbah</keyname><forenames>Merouane</forenames></author></authors><title>Transmit Power Minimization in Small Cell Networks Under Time Average
  QoS Constraints</title><categories>cs.IT math.IT</categories><comments>in Journal on Selected Areas of Communications (JSAC), 2015</comments><doi>10.1109/JSAC.2015.2435312</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a small cell network (SCN) consisting of N cells, with the small
cell base stations (SCBSs) equipped with Nt \geq 1 antennas each, serving K
single antenna user terminals (UTs) per cell. Under this set up, we address the
following question: given certain time average quality of service (QoS) targets
for the UTs, what is the minimum transmit power expenditure with which they can
be met? Our motivation to consider time average QoS constraint comes from the
fact that modern wireless applications such as file sharing, multi-media etc.
allow some flexibility in terms of their delay tolerance. Time average QoS
constraints can lead to greater transmit power savings as compared to
instantaneous QoS constraints since it provides the flexibility to dynamically
allocate resources over the fading channel states. We formulate the problem as
a stochastic optimization problem whose solution is the design of the downlink
beamforming vectors during each time slot. We solve this problem using the
approach of Lyapunov optimization and characterize the performance of the
proposed algorithm. With this algorithm as the reference, we present two main
contributions that incorporate practical design considerations in SCNs. First,
we analyze the impact of delays incurred in information exchange between the
SCBSs. Second, we impose channel state information (CSI) feedback constraints,
and formulate a joint CSI feedback and beamforming strategy. In both cases, we
provide performance bounds of the algorithm in terms of satisfying the QoS
constraints and the time average power expenditure. Our simulation results show
that solving the problem with time average QoS constraints provide greater
savings in the transmit power as compared to the instantaneous QoS constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03840</identifier>
 <datestamp>2015-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03840</id><created>2015-07-14</created><authors><author><keyname>Zerpa</keyname><forenames>Levis</forenames></author></authors><title>Using interrogative logic to teach classical logic</title><categories>cs.CY cs.LO</categories><comments>Proceedings of the Fourth International Conference on Tools for
  Teaching Logic (TTL2015), Rennes, France, June 9-12, 2015. Editors: M.
  Antonia Huertas, Jo\~ao Marcos, Mar\'ia Manzano, Sophie Pinchinat,
  Fran\c{c}ois Schwarzentruber</comments><proxy>Joao Marcos</proxy><acm-class>F.4.1</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In the paper I discuss a tool for helping students in their symbolizations of
natural language sentences using the formal language of classical first order
logic (CFOL). The tool is an extension of Hintikka's concept of (Inquirer's)
range of attention in the context of interrogative games. Any given text is
reconstructed as the answer to a &quot;big&quot; or principal question obtained through
the answers of a series of &quot;small&quot; or operative questions. The tool brings some
&quot;narrative flavor&quot; to the symbolization and offers a convenient mold that can
be used by students in many different contexts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03843</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03843</id><created>2015-07-14</created><authors><author><keyname>Yang</keyname><forenames>Wei</forenames></author><author><keyname>Durisi</keyname><forenames>Giuseppe</forenames></author><author><keyname>Polyanskiy</keyname><forenames>Yury</forenames></author></authors><title>Minimum Energy to Send $k$ Bits Over Multiple-Antenna Fading Channels</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the minimum energy required to transmit $k$
information bits with a given reliability over a multiple-antenna Rayleigh
block-fading channel, with and without channel state information (CSI) at the
receiver. No feedback is assumed. It is well known that the ratio between the
minimum energy per bit and the noise level converges to $-1.59$ dB as $k$ goes
to infinity, regardless of whether CSI is available at the receiver or not.
This paper shows that lack of CSI at the receiver causes a slowdown in the
speed of convergence to $-1.59$ dB as $k\to\infty$ compared to the case of
perfect receiver CSI. Specifically, we show that, in the no-CSI case, the gap
to $-1.59$ dB is proportional to $((\log k) /k)^{1/3}$, whereas when perfect
CSI is available at the receiver, this gap is proportional to $1/\sqrt{k}$. In
both cases, the gap to $-1.59$ dB is independent of the number of transmit
antennas and of the channel's coherence time. Numerically, we observe that,
when the receiver is equipped with a single antenna, to achieve an energy per
bit of $ - 1.5$ dB in the no-CSI case, one needs to transmit at least $7\times
10^7$ information bits, whereas $6\times 10^4$ bits suffice for the case of
perfect CSI at the receiver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03844</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03844</id><created>2015-07-14</created><authors><author><keyname>Dias</keyname><forenames>Elis&#xe2;ngela Silva</forenames></author><author><keyname>Castonguay</keyname><forenames>Diane</forenames></author></authors><title>Polynomial recognition of cluster algebras of finite type</title><categories>math.AC cs.CC</categories><comments>14 pages</comments><msc-class>13F60, 68Q25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cluster algebras are a recent topic of study and have been shown to be a
useful tool to characterize structures in several knowledge fields. An
important problem is to establish whether or not a given cluster algebra is of
finite type. Using the standard definition, the problem is infeasible since it
uses mutations that can lead to an infinite process. Barot, Geiss and
Zelevinsky (2006) presented an easier way to verify if a given algebra is of
finite type, by testing that all chordless cycles of the graph related to the
algebra are cyclically oriented and that there exists a positive quasi-Cartan
companion of the skew-symmetrizable matrix related to the algebra. We develop
an algorithm that verifies these conditions and decides whether or not a
cluster algebra is of finite type in polynomial time. The second part of the
algorithm is used to prove that the more general problem to decide if a matrix
has a positive quasi-Cartan companion is in NP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03851</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03851</id><created>2015-07-14</created><updated>2015-08-03</updated><authors><author><keyname>Brockschmidt</keyname><forenames>Marc</forenames></author><author><keyname>Larraz</keyname><forenames>Daniel</forenames></author><author><keyname>Oliveras</keyname><forenames>Albert</forenames></author><author><keyname>Rodriguez-Carbonell</keyname><forenames>Enric</forenames></author><author><keyname>Rubio</keyname><forenames>Albert</forenames></author></authors><title>Compositional Safety Verification with Max-SMT</title><categories>cs.LO</categories><comments>Extended technical report version of the conference paper at FMCAD'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an automated compositional program verification technique for
safety properties based on conditional inductive invariants. For a given
program part (e.g., a single loop) and a postcondition $\varphi$, we show how
to, using a Max-SMT solver, an inductive invariant together with a precondition
can be synthesized so that the precondition ensures the validity of the
invariant and that the invariant implies $\varphi$. From this, we build a
bottom-up program verification framework that propagates preconditions of small
program parts as postconditions for preceding program parts. The method
recovers from failures to prove the validity of a precondition, using the
obtained intermediate results to restrict the search space for further proof
attempts.
  As only small program parts need to be handled at a time, our method is
scalable and distributable. The derived conditions can be viewed as implicit
contracts between different parts of the program, and thus enable an
incremental program analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03857</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03857</id><created>2015-07-14</created><updated>2016-01-05</updated><authors><author><keyname>Lesieur</keyname><forenames>Thibault</forenames></author><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author><author><keyname>Zdeborov&#xe1;</keyname><forenames>Lenka</forenames></author></authors><title>MMSE of probabilistic low-rank matrix estimation: Universality with
  respect to the output channel</title><categories>cs.IT cond-mat.stat-mech math.IT stat.ML</categories><comments>10 pages, Allerton Conference on Communication, Control, and
  Computing 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers probabilistic estimation of a low-rank matrix from
non-linear element-wise measurements of its elements. We derive the
corresponding approximate message passing (AMP) algorithm and its state
evolution. Relying on non-rigorous but standard assumptions motivated by
statistical physics, we characterize the minimum mean squared error (MMSE)
achievable information theoretically and with the AMP algorithm. Unlike in
related problems of linear estimation, in the present setting the MMSE depends
on the output channel only trough a single parameter - its Fisher information.
We illustrate this striking finding by analysis of submatrix localization, and
of detection of communities hidden in a dense stochastic block model. For this
example we locate the computational and statistical boundaries that are not
equal for rank larger than four.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03867</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03867</id><created>2015-07-14</created><authors><author><keyname>Ge</keyname><forenames>Rong</forenames></author><author><keyname>Zou</keyname><forenames>James</forenames></author></authors><title>Rich Component Analysis</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many settings, we have multiple data sets (also called views) that capture
different and overlapping aspects of the same phenomenon. We are often
interested in finding patterns that are unique to one or to a subset of the
views. For example, we might have one set of molecular observations and one set
of physiological observations on the same group of individuals, and we want to
quantify molecular patterns that are uncorrelated with physiology. Despite
being a common problem, this is highly challenging when the correlations come
from complex distributions. In this paper, we develop the general framework of
Rich Component Analysis (RCA) to model settings where the observations from
different views are driven by different sets of latent components, and each
component can be a complex, high-dimensional distribution. We introduce
algorithms based on cumulant extraction that provably learn each of the
components without having to model the other components. We show how to
integrate RCA with stochastic gradient descent into a meta-algorithm for
learning general models, and demonstrate substantial improvement in accuracy on
several synthetic and real datasets in both supervised and unsupervised tasks.
Our method makes it possible to learn latent variable models when we don't have
samples from the true model but only samples after complex perturbations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03885</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03885</id><created>2015-07-14</created><authors><author><keyname>Balodis</keyname><forenames>Kaspars</forenames></author><author><keyname>Iraids</keyname><forenames>J&#x101;nis</forenames></author></authors><title>Quantum Lower Bound for Graph Collision Implies Lower Bound for Triangle
  Detection</title><categories>quant-ph cs.CC</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We show that an improvement to the best known quantum lower bound for
GRAPH-COLLISION problem implies an improvement to the best known lower bound
for TRIANGLE problem in the quantum query complexity model. In GRAPH-COLLISION
we are given free access to a graph $(V,E)$ and access to a function
$f:V\rightarrow \{0,1\}$ as a black box. We are asked to determine if there
exist $(u,v) \in E$, such that $f(u)=f(v)=1$. In TRIANGLE we have a black box
access to an adjacency matrix of a graph and we have to determine if the graph
contains a triangle. For both of these problems the known lower bounds are
trivial ($\Omega(\sqrt{n})$ and $\Omega(n)$, respectively) and there is no
known matching upper bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03920</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03920</id><created>2015-07-14</created><authors><author><keyname>Alviano</keyname><forenames>Mario</forenames></author><author><keyname>Penaloza</keyname><forenames>Rafael</forenames></author></authors><title>Fuzzy Answer Set Computation via Satisfiability Modulo Theories</title><categories>cs.AI</categories><acm-class>I.2</acm-class><journal-ref>Theory and Practice of Logic Programming 15 (2015) 588-603</journal-ref><doi>10.1017/S1471068415000241</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fuzzy answer set programming (FASP) combines two declarative frameworks,
answer set programming and fuzzy logic, in order to model reasoning by default
over imprecise information. Several connectives are available to combine
different expressions; in particular the \Godel and \Luka fuzzy connectives are
usually considered, due to their properties. Although the \Godel conjunction
can be easily eliminated from rule heads, we show through complexity arguments
that such a simplification is infeasible in general for all other connectives.
%, even if bodies are restricted to \Luka or \Godel conjunctions. The paper
analyzes a translation of FASP programs into satisfiability modulo
theories~(SMT), which in general produces quantified formulas because of the
minimality of the semantics. Structural properties of many FASP programs allow
to eliminate the quantification, or to sensibly reduce the number of quantified
variables. Indeed, integrality constraints can replace recursive rules commonly
used to force Boolean interpretations, and completion subformulas can guarantee
minimality for acyclic programs with atomic heads. Moreover, head cycle free
rules can be replaced by shifted subprograms, whose structure depends on the
eliminated head connective, so that ordered completion may replace the
minimality check if also \Luka disjunction in rule bodies is acyclic. The paper
also presents and evaluates a prototype system implementing these translations.
  To appear in Theory and Practice of Logic Programming (TPLP), Proceedings of
ICLP 2015.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03922</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03922</id><created>2015-07-14</created><authors><author><keyname>Alviano</keyname><forenames>Mario</forenames></author><author><keyname>Leone</keyname><forenames>Nicola</forenames></author></authors><title>Complexity and Compilation of GZ-Aggregates in Answer Set Programming</title><categories>cs.AI</categories><acm-class>I.2</acm-class><journal-ref>Theory and Practice of Logic Programming 15 (2015) 574-587</journal-ref><doi>10.1017/S147106841500023X</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gelfond and Zhang recently proposed a new stable model semantics based on
Vicious Circle Principle in order to improve the interpretation of logic
programs with aggregates. The paper focuses on this proposal, and analyzes the
complexity of both coherence testing and cautious reasoning under the new
semantics. Some surprising results highlight similarities and differences
versus mainstream stable model semantics for aggregates. Moreover, the paper
reports on the design of compilation techniques for implementing the new
semantics on top of existing ASP solvers, which eventually lead to realize a
prototype system that allows for experimenting with Gelfond-Zhang's aggregates.
  To appear in Theory and Practice of Logic Programming (TPLP), Proceedings of
ICLP 2015.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03923</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03923</id><created>2015-07-14</created><authors><author><keyname>Alviano</keyname><forenames>Mario</forenames></author><author><keyname>Faber</keyname><forenames>Wolfgang</forenames></author><author><keyname>Gebser</keyname><forenames>Martin</forenames></author></authors><title>Rewriting recursive aggregates in answer set programming: back to
  monotonicity</title><categories>cs.AI</categories><acm-class>I.2</acm-class><journal-ref>Theory and Practice of Logic Programming 15 (2015) 559-573</journal-ref><doi>10.1017/S1471068415000228</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Aggregation functions are widely used in answer set programming for
representing and reasoning on knowledge involving sets of objects collectively.
Current implementations simplify the structure of programs in order to optimize
the overall performance. In particular, aggregates are rewritten into simpler
forms known as monotone aggregates. Since the evaluation of normal programs
with monotone aggregates is in general on a lower complexity level than the
evaluation of normal programs with arbitrary aggregates, any faithful
translation function must introduce disjunction in rule heads in some cases.
However, no function of this kind is known. The paper closes this gap by
introducing a polynomial, faithful, and modular translation for rewriting
common aggregation functions into the simpler form accepted by current solvers.
A prototype system allows for experimenting with arbitrary recursive
aggregates, which are also supported in the recent version 4.5 of the grounder
\textsc{gringo}, using the methods presented in this paper.
  To appear in Theory and Practice of Logic Programming (TPLP), Proceedings of
ICLP 2015.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03924</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03924</id><created>2015-07-14</created><updated>2015-12-17</updated><authors><author><keyname>Chakrabarty</keyname><forenames>Ankush</forenames></author><author><keyname>Buzzard</keyname><forenames>Gregery T.</forenames></author><author><keyname>Zak</keyname><forenames>Stanislaw H.</forenames></author><author><keyname>Zhu</keyname><forenames>Fanglai</forenames></author><author><keyname>Rundell</keyname><forenames>Ann E.</forenames></author></authors><title>Simultaneous state and exogenous input estimation for nonlinear systems
  using boundary-layer sliding mode observers</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While sliding mode observers (SMOs) using discontinuous relays are widely
analyzed, most SMOs are implemented computationally using a continuous
approximation of the discontinuous relays. This approximation results in the
formation of a boundary layer in a neighborhood of the sliding manifold in the
observer error space. Therefore, it becomes necessary to develop methods for
attenuating the effect of the boundary layer and guaranteeing performance
bounds on the resulting state estimation error. In this paper, a method is
proposed for constructing boundary-layer SMOs (BL-SMOs) with prescribed state
estimation error bounds. The BL-SMO formulation is then extended to
simultaneously estimate exogenous inputs (disturbance signals in the state and
output vector fields), along with the system state. Two numerical examples are
presented to illustrate the effectiveness of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03927</identifier>
 <datestamp>2015-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03927</id><created>2015-07-14</created><updated>2015-11-17</updated><authors><author><keyname>Chen</keyname><forenames>Houwu</forenames></author><author><keyname>Shu</keyname><forenames>Jiwu</forenames></author></authors><title>SkyHash: a Hash Opinion Dynamics Model</title><categories>cs.DC</categories><comments>arXiv admin note: substantial text overlap with arXiv:1501.06238</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes the first hash opinion dynamics model, named SkyHash,
that can help a P2P network quickly reach consensus on hash opinion. The model
consists of a bit layer and a hash layer, each time when a node shapes its new
opinion, the bit layer is to determine each bit of a pseudo hash, and the hash
layer is to choose a hash opinion with minimum Hamming distance to the pseudo
hash. With simulations, we conducted a comprehensive study on the convergence
speed of the model by taking into account impacts of various configurations
such as network size, node degree, hash size, and initial hash density.
Evaluation demonstrates that using our model, consensus can be quickly reached
even in large networks. We also developed a denial-of-service (DoS) proof
extension for our model. Experiments on the SNAP dataset of the Wikipedia
who-votes-on-whom network demonstrate that besides the ability to refuse known
ill-behaved nodes, the DoS-proof extended model also outperforms Bitcoin by
producing consensus in 45 seconds, and tolerating DoS attack committed by up to
0.9% top influential nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03928</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03928</id><created>2015-07-14</created><authors><author><keyname>Diaz</keyname><forenames>Fernando</forenames></author></authors><title>Pseudo-Query Reformulation</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic query reformulation refers to rewriting a user's original query in
order to improve the ranking of retrieval results compared to the original
query. We present a general framework for automatic query reformulation based
on discrete optimization. Our approach, referred to as pseudo-query
reformulation, treats automatic query reformulation as a search problem over
the graph of unweighted queries linked by minimal transformations (e.g. term
additions, deletions). This framework allows us to test existing performance
prediction methods as heuristics for the graph search process. We demonstrate
the effectiveness of the approach on several publicly available datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03934</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03934</id><created>2015-07-14</created><updated>2015-11-21</updated><authors><author><keyname>Sun</keyname><forenames>Kai</forenames></author><author><keyname>Xie</keyname><forenames>Qizhe</forenames></author><author><keyname>Yu</keyname><forenames>Kai</forenames></author></authors><title>Recurrent Polynomial Network for Dialogue State Tracking</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dialogue state tracking (DST) is a process to estimate the distribution of
the dialogue states as a dialogue progresses. Recent studies on constrained
Markov Bayesian polynomial (CMBP) framework take the first step towards
bridging the gap between rule-based and statistical approaches for DST. In this
paper, the gap is further bridged by a novel framework -- recurrent polynomial
network (RPN). RPN's unique structure enables the framework to have all the
advantages of CMBP including efficiency, portability and interpretability.
Additionally, RPN achieves more properties of statistical approaches than CMBP.
RPN was evaluated on the data corpora of the second and the third Dialog State
Tracking Challenge (DSTC-2/3). Experiments showed that RPN can significantly
outperform both traditional rule-based approaches and statistical approaches
with similar feature set. Compared with the state-of-the-art statistical DST
approaches with a lot richer features, RPN is also competitive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03955</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03955</id><created>2015-07-14</created><authors><author><keyname>Kazemipour</keyname><forenames>Abbas</forenames></author><author><keyname>Wu</keyname><forenames>Min</forenames></author><author><keyname>Babadi</keyname><forenames>Behtash</forenames></author></authors><title>Robust Estimation of Self-Exciting Point Process Models with Application
  to Neuronal Modeling</title><categories>cs.NE cs.IT cs.SY math.IT math.OC stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of estimating self-exciting point process models from
limited binary observations, where the history of the process serves as the
covariate. We analyze the performance of two classes of estimators:
$\ell_1$-regularized maximum likelihood and greedy estimation for a discrete
version of the Hawkes process and characterize the sampling tradeoffs required
for stable recovery in the non-asymptotic regime. Our results extend those of
compressed sensing for linear and generalized linear models with i.i.d.
covariates to point processes with highly inter-dependent covariates. We
further provide simulation studies as well as application to real spiking data
from mouse's lateral geniculate nucleus which chime in accordance with our
theoretical predictions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03958</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03958</id><created>2015-07-14</created><updated>2015-09-19</updated><authors><author><keyname>Basu</keyname><forenames>Saugata</forenames></author><author><keyname>Rizzie</keyname><forenames>Anthony</forenames></author></authors><title>Multi-degree bounds on the Betti numbers of real varieties and
  semi-algebraic sets and applications</title><categories>math.AG cs.CG math.CO</categories><comments>65 pages, 3 figures. Minor corrections. New references added</comments><msc-class>Primary 14P10, 14P25, Secondary 68W30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove new bounds on the Betti numbers of real varieties and semi-algebraic
sets that have a more refined dependence on the degrees of the polynomials
defining them than results known before. Our method also unifies several
different types of results under a single framework, such as bounds depending
on the total degrees, on multi-degrees, as well as in the case of quadratic and
partially quadratic polynomials. The bounds we present in the case of partially
quadratic polynomials offer a significant improvement over what was previously
known. We give several applications of our results, including a generalization
of the polynomial partitioning theorem due to Guth and Katz, which has become a
very important tool in discrete geometry in the multi-degree setting, and give
an application of this result a theorem that interpolates between two different
kinds of partitions of the plane. Finally, we extend a result of Barone and
Basu on bounding the number of connected components of real varieties defined
by two polynomials of differing degrees to the sum of all Betti numbers, thus
making progress on an open problem posed in that paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03960</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03960</id><created>2015-07-14</created><updated>2015-07-15</updated><authors><author><keyname>Florinsky</keyname><forenames>I. V.</forenames></author><author><keyname>Pankratov</keyname><forenames>A. N.</forenames></author></authors><title>Digital terrain modeling with the Chebyshev polynomials</title><categories>physics.geo-ph cs.CE cs.CG math.DG</categories><comments>10 pages, 1 figure, references improved</comments><msc-class>41A10</msc-class><acm-class>G.1.2</acm-class><journal-ref>Machine Learning and Data Analysis, 2015, Vol. 1, No. 12, pp.
  1647-1659</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mathematical problems of digital terrain analysis include interpolation of
digital elevation models (DEMs), DEM generalization and denoising, and
computation of morphometric variables by calculation of partial derivatives of
elevation. Traditionally, these procedures are based on numerical treatments of
two-variable discrete functions of elevation. We developed a spectral
analytical method and algorithm based on high-order orthogonal expansions using
the Chebyshev polynomials of the first kind with the subsequent Fejer
summation. The method and algorithm are intended for DEM analytical treatment,
such as, DEM global approximation, denoising, and generalization as well as
computation of morphometric variables by analytical calculation of partial
derivatives. To test the method and algorithm, we used a DEM of the Northern
Andes including 230,880 points (the elevation matrix 480 $\times$ 481). DEMs
were reconstructed with 480, 240, 120, 60, and 30 expansion coefficients. The
first and second partial derivatives of elevation were analytically calculated
from the reconstructed DEMs. Models of horizontal curvature ($k_h$) were then
computed with the derivatives. A set of elevation and $k_h$ maps related to
different number of expansion coefficients well illustrates data generalization
effects, denoising, and removal of artifacts contained in the original DEM. The
test results demonstrated a good performance of the developed method and
algorithm. They can be utilized as a universal tool for analytical treatment in
digital terrain modeling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03969</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03969</id><created>2015-07-14</created><authors><author><keyname>Pi</keyname><forenames>Zhouyue</forenames></author><author><keyname>Choi</keyname><forenames>Junil</forenames></author><author><keyname>Heath</keyname><forenames>Robert</forenames><suffix>Jr</suffix></author></authors><title>Millimeter-wave Gbps Broadband Evolution towards 5G: Fixed Access and
  Backhaul</title><categories>cs.IT math.IT</categories><comments>6 pages, 4 figures, 2 tables, submitted to IEEE Communications
  Magazine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As wireless communication evolves towards 5G, both fixed broadband and mobile
broadband will play a crucial part in providing the Gbps infrastructure for a
connected society. This paper proposes a Millimeter-wave Gbps Broadband (MGB)
system as the solution to two critical problems in this evolution: last-mile
access for fixed broadband and small cell backhaul for mobile broadband. The
key idea is to use spectrum that is already available in the millimeter wave
bands for fixed wireless access with optimized dynamic beamforming and massive
MIMO infrastructure to achieve high capacity with wide area coverage. This
paper explains the MGB concept and describes potential array architectures for
realizing the system. Simulations demonstrate that with 500 MHz of bandwidth
(at 39 GHz band) and 28 dBm transmission power (55 dBm EIRP), it is possible to
provide more than 11 Gbps backhaul capacity for 96 small cells within 1-km
radius.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03979</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03979</id><created>2015-07-14</created><authors><author><keyname>Zhou</keyname><forenames>Neng-Fa</forenames></author><author><keyname>Bartak</keyname><forenames>Roman</forenames></author><author><keyname>Dovier</keyname><forenames>Agostino</forenames></author></authors><title>Planning as Tabled Logic Programming</title><categories>cs.AI</categories><comments>27 pages in TPLP 2015</comments><acm-class>D.3.2</acm-class><journal-ref>Theory and Practice of Logic Programming 15 (2015) 543-558</journal-ref><doi>10.1017/S1471068415000216</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes Picat's planner, its implementation, and planning models
for several domains used in International Planning Competition (IPC) 2014.
Picat's planner is implemented by use of tabling. During search, every state
encountered is tabled, and tabled states are used to effectively perform
resource-bounded search. In Picat, structured data can be used to avoid
enumerating all possible permutations of objects, and term sharing is used to
avoid duplication of common state data. This paper presents several modeling
techniques through the example models, ranging from designing state
representations to facilitate data sharing and symmetry breaking, encoding
actions with operations for efficient precondition checking and state updating,
to incorporating domain knowledge and heuristics. Broadly, this paper
demonstrates the effectiveness of tabled logic programming for planning, and
argues the importance of modeling despite recent significant progress in
domain-independent PDDL planners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.03989</identifier>
 <datestamp>2015-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.03989</id><created>2015-07-14</created><authors><author><keyname>Momcheva</keyname><forenames>Ivelina</forenames></author><author><keyname>Tollerud</keyname><forenames>Erik</forenames></author></authors><title>Software Use in Astronomy: an Informal Survey</title><categories>astro-ph.IM cs.CY</categories><comments>Available on Authorea:
  https://www.authorea.com/users/10533/articles/18046; Interactive
  visualization:
  http://eteq.github.io/software_survey_analysis/software_tools_heirarchy_d3vis.html</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report on an informal survey about the use of software in the worldwide
astronomical community. The survey was carried out between December 2014 and
February 2015, collecting responses from 1142 astronomers, spanning all career
levels. We find that all participants use software in their research. The vast
majority of participants, 90%, write at least some of their own software. Even
though writing software is so wide-spread among the survey participants, only
8% of them report that they have received substantial training in software
development. Another 49% of the participants have received &quot;little&quot; training.
The remaining 43% have received no training. We also find that astronomers'
software stack is fairly narrow. The 10 most popular tools among astronomers
are (from most to least popular): Python, shell scripting, IDL, C/C++, Fortran,
IRAF, spreadsheets, HTML/CSS, SQL and Supermongo. Across all participants the
most common programing language is Python ($67\pm 2\%$), followed by IDL
($44\pm 2\%$), C/C++ ($37\pm 2\%$) and Fortran ($28\pm 2\%$). IRAF is used
frequently by $24\pm 1\%$ of participants. We show that all trends are largely
independent of career stage, area of research and geographic location.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04001</identifier>
 <datestamp>2015-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04001</id><created>2015-07-14</created><authors><author><keyname>Newman</keyname><forenames>M. E. J.</forenames></author><author><keyname>Clauset</keyname><forenames>Aaron</forenames></author></authors><title>Structure and inference in annotated networks</title><categories>cs.SI physics.data-an physics.soc-ph stat.ML</categories><comments>16 pages, 7 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For many networks of scientific interest we know both the connections of the
network and information about the network nodes, such as the age or gender of
individuals in a social network, geographic location of nodes in the Internet,
or cellular function of nodes in a gene regulatory network. Here we demonstrate
how this &quot;metadata&quot; can be used to improve our analysis and understanding of
network structure. We focus in particular on the problem of community detection
in networks and develop a mathematically principled approach that combines a
network and its metadata to detect communities more accurately than can be done
with either alone. Crucially, the method does not assume that the metadata are
correlated with the communities we are trying to find. Instead the method
learns whether a correlation exists and correctly uses or ignores the metadata
depending on whether they contain useful information. The learned correlations
are also of interest in their own right, allowing us to make predictions about
the community membership of nodes whose network connections are unknown. We
demonstrate our method on synthetic networks with known structure and on
real-world networks, large and small, drawn from social, biological, and
technological domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04002</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04002</id><created>2015-07-14</created><authors><author><keyname>Villadsen</keyname><forenames>J&#xf8;rgen</forenames></author><author><keyname>Jensen</keyname><forenames>Alexander Birch</forenames></author><author><keyname>Schlichtkrull</keyname><forenames>Anders</forenames></author></authors><title>NaDeA: A Natural Deduction Assistant with a Formalization in Isabelle</title><categories>cs.CY cs.LO</categories><comments>Proceedings of the Fourth International Conference on Tools for
  Teaching Logic (TTL2015), Rennes, France, June 9-12, 2015. Editors: M.
  Antonia Huertas, Jo\~ao Marcos, Mar\'ia Manzano, Sophie Pinchinat,
  Fran\c{c}ois Schwarzentruber</comments><proxy>Joao Marcos</proxy><acm-class>F.4.1</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present a new software tool for teaching logic based on natural deduction.
Its proof system is formalized in the proof assistant Isabelle such that its
definition is very precise. Soundness of the formalization has been proved in
Isabelle. The tool is open source software developed in TypeScript / JavaScript
and can thus be used directly in a browser without any further installation.
Although developed for undergraduate computer science students who are used to
study and program concrete computer code in a programming language we consider
the approach relevant for a broader audience and for other proof systems as
well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04004</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04004</id><created>2015-07-14</created><authors><author><keyname>Marin</keyname><forenames>Mike A.</forenames></author><author><keyname>Hauder</keyname><forenames>Matheus</forenames></author></authors><title>Case Management: a data set of definitions</title><categories>cs.SE</categories><comments>Data set used as part of a study reported at: Marin, M.A., Hauder, M,
  &amp; Matthes, F. (accepted for publication), Case Management: An Evaluation of
  Existing Approaches for Knowledge-Intensive Processes. In AdaptiveCM 2015 -
  4th International Workshop on Adaptive Case Management and other non-workflow
  approaches to BPM. 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowledge-intensive processes (KiPs) are becoming increasingly important for
organizations with the rise of the knowledge society. Due to their
unpredictable and emergent characteristic worklfow management solutions are not
suitable to support KiPs. Various case management related approaches have been
proposed by researchers and practitioners to support characteristics of KiPs.
In this paper we provide a comprehensive list of definitions available on case
management, e.g. case handling, adaptive case management, dynamic case
management, production case management. For every definition we present the
explicit definition, paragraphs that better describe and summarize the case
management approach, or extracted sequences that define the term in the
referenced publication. All of these definitions are compared against
characteristics of KiPs in order to get about understanding of the domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04019</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04019</id><created>2015-07-14</created><authors><author><keyname>Kumar</keyname><forenames>D. S. Pavan</forenames></author></authors><title>Feature Normalisation for Robust Speech Recognition</title><categories>cs.CL cs.SD</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Speech recognition system performance degrades in noisy environments. If the
acoustic models are built using features of clean utterances, the features of a
noisy test utterance would be acoustically mismatched with the trained model.
This gives poor likelihoods and poor recognition accuracy. Model adaptation and
feature normalisation are two broad areas that address this problem. While the
former often gives better performance, the latter involves estimation of lesser
number of parameters, making the system feasible for practical implementations.
  This research focuses on the efficacies of various subspace, statistical and
stereo based feature normalisation techniques. A subspace projection based
method has been investigated as a standalone and adjunct technique involving
reconstruction of noisy speech features from a precomputed set of clean speech
building-blocks. The building blocks are learned using non-negative matrix
factorisation (NMF) on log-Mel filter bank coefficients, which form a basis for
the clean speech subspace. The work provides a detailed study on how the method
can be incorporated into the extraction process of Mel-frequency cepstral
coefficients. Experimental results show that the new features are robust to
noise, and achieve better results when combined with the existing techniques.
  The work also proposes a modification to the training process of SPLICE
algorithm for noise robust speech recognition. It is based on feature
correlations, and enables this stereo-based algorithm to improve the
performance in all noise conditions, especially in unseen cases. Further, the
modified framework is extended to work for non-stereo datasets where clean and
noisy training utterances, but not stereo counterparts, are required. An
MLLR-based computationally efficient run-time noise adaptation method in SPLICE
framework has been proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04024</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04024</id><created>2015-07-14</created><updated>2015-07-16</updated><authors><author><keyname>Tavassoli</keyname><forenames>Sude</forenames></author><author><keyname>Zweig</keyname><forenames>Katharina Anna</forenames></author></authors><title>Analyzing the activity of a person in a chat by combining network
  analysis and fuzzy logic</title><categories>cs.SI</categories><comments>This paper will be appeared in the proceedings of the 2015 IEEE/ACM
  International Conference on Advances in Social Networks Analysis and Mining</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Chat-log data that contains information about sender and receiver of the
statements sent around in the chat can be readily turned into a directed
temporal multi-network representation. In the resulting network, the activity
of a chat member can, for example, be operationalized as his degree (number of
distinct interaction partners) or his strength (total number of interactions).
However, the data itself contains more information that is not readily
representable in the network, e.g., the total number of words used by a member
or the reaction time to what the members said. As degree and strength, these
values can be seen as a way to operationalize the idea of activity of a
chat-log member. This paper deals with the question of how the overall activity
of a member can be assessed, given multiple and probably opposing criteria by
using a fuzzy operator. We then present a new way of visualizing the results
and show how to apply it to the network representation of chat-log data.
Finally, we discuss how this approach can be used to deal with other
conflicting situations, like the different rankings produced by different
centrality indices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04027</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04027</id><created>2015-07-14</created><authors><author><keyname>Chen</keyname><forenames>Mingming</forenames></author><author><keyname>Szymanski</keyname><forenames>Boleslaw K.</forenames></author></authors><title>Fuzzy Overlapping Community Quality Metrics</title><categories>cs.SI physics.soc-ph</categories><comments>40 pages in Social Network Analysis and Mining 5(1) 2015</comments><doi>10.1007/s13278-015-0279-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modularity is widely used to effectively measure the strength of the disjoint
community structure found by community detection algorithms. Several
overlapping extensions of modularity were proposed to measure the quality of
overlapping community structure. However, all these extensions differ just in
the way they define the belonging coefficient and belonging function. Yet,
there is lack of systematic comparison of different extensions. To fill this
gap, we overview overlapping extensions of modularity and generalize them with
a uniform definition enabling application of different belonging coefficients
and belonging functions to select the best. In addition, we extend localized
modularity, modularity density, and eight local community quality metrics to
enable their usages for overlapping communities. The experimental results on a
large number of real networks and synthetic networks using overlapping
extensions of modularity, overlapping modularity density, and local metrics
show that the best results are obtained when the product of the belonging
coefficients of two nodes is used as the belonging function. Moreover, the
results may be used to guide researchers on which metrics to adopt when
measuring the quality of overlapping community structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04029</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04029</id><created>2015-07-14</created><authors><author><keyname>Portegys</keyname><forenames>Thomas E.</forenames></author></authors><title>Training artificial neural networks to learn a nondeterministic game</title><categories>cs.LG</categories><comments>ICAI'15: The 2015 International Conference on Artificial
  Intelligence, Las Vegas, NV, USA, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that artificial neural networks (ANNs) can learn
deterministic automata. Learning nondeterministic automata is another matter.
This is important because much of the world is nondeterministic, taking the
form of unpredictable or probabilistic events that must be acted upon. If ANNs
are to engage such phenomena, then they must be able to learn how to deal with
nondeterminism. In this project the game of Pong poses a nondeterministic
environment. The learner is given an incomplete view of the game state and
underlying deterministic physics, resulting in a nondeterministic game. Three
models were trained and tested on the game: Mona, Elman, and Numenta's NuPIC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04039</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04039</id><created>2015-07-14</created><updated>2015-09-18</updated><authors><author><keyname>Potvin</keyname><forenames>Pascal</forenames></author><author><keyname>Gamardo</keyname><forenames>Hanen Garcia</forenames></author><author><keyname>Nguyen</keyname><forenames>Kim-Khoa</forenames></author><author><keyname>Cheriet</keyname><forenames>Mohamed</forenames></author></authors><title>Hyper Heterogeneous Cloud-based IMS Software Architecture: A
  Proof-of-Concept and Empirical Analysis</title><categories>cs.DC</categories><comments>12 pages, 9 figures, 1 table. Accepted for oral presentation at S2CT
  2015 in Toronto. Latest Version is Camera Ready</comments><acm-class>C.2.4; D.1.3; D.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The IP Multimedia Subsystem (IMS) defined by the 3GPP has been mainly
developed and deployed by telephony vendors on vendor-specific hardware. Recent
advances in Network Function Virtualisation (NFV) technology paved the way for
virtualized hardware and telephony function elasticity. As such, Telecom
vendors have started to embrace the cloud as a deployment platform, usually
selecting a privileged virtualization platform. Operators would like to deploy
telecom functionality on their already existing IT cloud platforms. Achieving
such flexibility would require the telecom vendors to adopt a software
architecture allowing deployment on many cloud platforms or even heterogeneous
cloud platforms. We propose a distributed software architecture enabling the
deployment of a single software version on multiple cloud platforms thus
allowing for a solution-based deployment. We also present a prototype we
developed to study the characteristics of this architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04046</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04046</id><created>2015-07-14</created><authors><author><keyname>Alstrup</keyname><forenames>Stephen</forenames></author><author><keyname>G&#xf8;rtz</keyname><forenames>Inge Li</forenames></author><author><keyname>Halvorsen</keyname><forenames>Esben Bistrup</forenames></author><author><keyname>Porat</keyname><forenames>Ely</forenames></author></authors><title>Distance labeling schemes for trees</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider distance labeling schemes for trees: given a tree with $n$ nodes,
label the nodes with binary strings such that, given the labels of any two
nodes, one can determine, by looking only at the labels, the distance in the
tree between the two nodes.
  A lower bound by Gavoille et. al. (J. Alg. 2004) and an upper bound by Peleg
(J. Graph Theory 2000) establish that labels must use $\Theta(\log^2 n)$
bits\footnote{Throughout this paper we use $\log$ for $\log_2$.}. Gavoille et.
al. (ESA 2001) show that for very small approximate stretch, labels use
$\Theta(\log n \log \log n)$ bits. Several other papers investigate various
variants such as, for example, small distances in trees (Alstrup et. al.,
SODA'03).
  We improve the known upper and lower bounds of exact distance labeling by
showing that $\frac{1}{4} \log^2 n$ bits are needed and that $\frac{1}{2}
\log^2 n$ bits are sufficient. We also give ($1+\epsilon$)-stretch labeling
schemes using $\Theta(\log n)$ bits for constant $\epsilon&gt;0$.
($1+\epsilon$)-stretch labeling schemes with polylogarithmic label size have
previously been established for doubling dimension graphs by Talwar (STOC
2004).
  In addition, we present matching upper and lower bounds for distance labeling
for caterpillars, showing that labels must have size $2\log n - \Theta(\log\log
n)$. For simple paths with $k$ nodes and edge weights in $[1,n]$, we show that
labels must have size $\frac{k-1}{k}\log n+\Theta(\log k)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04047</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04047</id><created>2015-07-14</created><updated>2015-12-23</updated><authors><author><keyname>Fachada</keyname><forenames>Nuno</forenames></author><author><keyname>Lopes</keyname><forenames>Vitor V.</forenames></author><author><keyname>Martins</keyname><forenames>Rui C.</forenames></author><author><keyname>Rosa</keyname><forenames>Agostinho C.</forenames></author></authors><title>Parallelization Strategies for Spatial Agent-Based Models</title><categories>cs.DC</categories><acm-class>D.1.3; D.2.4; G.3; I.6.6</acm-class><journal-ref>International Journal of Parallel Programming, 2016</journal-ref><doi>10.1007/s10766-015-0399-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Agent-based modeling (ABM) is a bottom-up modeling approach, where each
entity of the system being modeled is uniquely represented as an independent
decision-making agent. Large scale emergent behavior in ABMs is population
sensitive. As such, the number of agents in a simulation should be able to
reflect the reality of the system being modeled, which can be in the order of
millions or billions of individuals in certain domains. A natural solution to
reach acceptable scalability in commodity multi-core processors consists of
decomposing models such that each component can be independently processed by a
different thread in a concurrent manner. In this paper we present a
multithreaded Java implementation of the PPHPC ABM, with two goals in mind: 1)
compare the performance of this implementation with an existing NetLogo
implementation; and, 2) study how different parallelization strategies impact
simulation performance on a shared memory architecture. Results show that: 1)
model parallelization can yield considerable performance gains; 2) distinct
parallelization strategies offer specific trade-offs in terms of performance
and simulation reproducibility; and, 3) PPHPC is a valid reference model for
comparing distinct implementations or parallelization strategies, from both
performance and statistical accuracy perspectives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04050</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04050</id><created>2015-07-14</created><authors><author><keyname>Ntougias</keyname><forenames>Konstantinos</forenames></author><author><keyname>Ntaikos</keyname><forenames>Dimitrios</forenames></author><author><keyname>Papadias</keyname><forenames>Constantinos B.</forenames></author></authors><title>Reducing Complexity in Next-Generation MU-MIMO Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, several advanced multi-antenna radio communications technologies
have emerged to meet the increased capacity demands in wireless multi-user
networks. Despite their great potential, the extent of these techniques'
practical applicability still remains questionable, since they have to face
either backhaul limitations or cost and hardware constraints. In this paper, we
propose a new system solution which includes network architecture, antenna
technology and radio transmission protocol to reduce drastically the hardware
complexity and cost as well as the channel state information / user data
feedback requirements of multi-user multi-antenna wireless networks. We focus
on the forward link of an interference channel in a cloud radio access network
setup wherein an arbitrary number of remote radio heads are each equipped with
a single radio frequency module parasitic antenna array and wish to send data
to their respective single-antenna user terminals, while co-existing in time
and frequency. Base stations select cooperatively the optimal combination of
pre-determined beams prior transmission. Our proposed approach is able to
achieve the aforementioned goals, while offering significant downlink sum-rate
gains due to the available spatial degrees of freedom.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04060</identifier>
 <datestamp>2015-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04060</id><created>2015-07-14</created><authors><author><keyname>Albehadili</keyname><forenames>Hayder</forenames></author><author><keyname>Islam</keyname><forenames>Naz</forenames></author></authors><title>Unsupervised Decision Forest for Data Clustering and Density Estimation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An algorithm to improve performance parameter for unsupervised decision
forest clustering and density estimation is presented. Specifically, a dual
assignment parameter is introduced as a density estimator by combining Random
Forest and Gaussian Mixture Model. The Random Forest method has been
specifically applied to construct a robust affinity graph that provides
information on the underlying structure of data objects used in clustering. The
proposed algorithm differs from the commonly used spectral clustering methods
where the computed distance metric is used to find similarities between data
points. Experiments were conducted using five datasets. A comparison with six
other state-of-the-art methods shows that our model is superior to existing
approaches. Efficiency of the proposed model is in capturing the underlying
structure for a given set of data points. The proposed method is also robust,
and can discriminate between the complex features of data points among
different clusters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04062</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04062</id><created>2015-07-14</created><authors><author><keyname>Wang</keyname><forenames>Wei</forenames></author><author><keyname>Tang</keyname><forenames>Ming</forenames></author><author><keyname>Zhang</keyname><forenames>Hai-Feng</forenames></author><author><keyname>Lai</keyname><forenames>Ying-Cheng</forenames></author></authors><title>Dynamics of social contagions with memory of non-redundant information</title><categories>physics.soc-ph cs.SI</categories><comments>13 pages, 9 figures</comments><journal-ref>Physical Review E 92, 012820 (2015)</journal-ref><doi>10.1103/PhysRevE.92.012820</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A key ingredient in social contagion dynamics is reinforcement, as adopting a
certain social behavior requires verification of its credibility and
legitimacy. Memory of non-redundant information plays an important role in
reinforcement, which so far has eluded theoretical analysis. We first propose a
general social contagion model with reinforcement derived from non-redundant
information memory. Then, we develop a unified edge-based compartmental theory
to analyze this model, and a remarkable agreement with numerics is obtained on
some specific models. Using a spreading threshold model as a specific example
to understand the memory effect, in which each individual adopts a social
behavior only when the cumulative pieces of information that the individual
received from his/her neighbors exceeds an adoption threshold. Through analysis
and numerical simulations, we find that the memory characteristic markedly
affects the dynamics as quantified by the final adoption size. Strikingly, we
uncover a transition phenomenon in which the dependence of the final adoption
size on some key parameters, such as the transmission probability, can change
from being discontinuous to being continuous. The transition can be triggered
by proper parameters and structural perturbations to the system, such as
decreasing individuals' adoption threshold, increasing initial seed size, or
enhancing the network heterogeneity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04065</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04065</id><created>2015-07-14</created><updated>2015-07-21</updated><authors><author><keyname>Zhang</keyname><forenames>Simpson</forenames></author><author><keyname>van der Schaar</keyname><forenames>Mihaela</forenames></author></authors><title>Reputational Learning and Network Dynamics</title><categories>q-fin.EC cs.GT cs.SI physics.soc-ph</categories><comments>Fixed typos and modified Figure 1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many real world networks agents are initially unsure of each other's
qualities and learn about each other over time via repeated interactions. This
paper is the first to provide a methodology for studying the formation of such
networks, taking into account that agents differ from each other, that they
begin with incomplete information, and that they must learn through
observations which connections/links to form and which to break. The network
dynamics in our model vary drastically from the dynamics emerging in models of
complete information. With incomplete information and learning, agents who
provide high benefits will develop high reputations and remain in the network,
while agents who provide low benefits will drop in reputation and become
ostracized. We show, among many other things, that the information to which
agents have access and the speed at which they learn and act can have
tremendous impact on the resulting network dynamics. Using our model, we can
also compute the ex ante social welfare given an arbitrary initial network,
which allows us to characterize the socially optimal network structures for
different sets of agents. Importantly, we show through examples that the
optimal network structure depends sharply on both the initial beliefs of the
agents, as well as the rate of learning by the agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04086</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04086</id><created>2015-07-15</created><authors><author><keyname>Kumar</keyname><forenames>Vinit</forenames></author><author><keyname>Agarwal</keyname><forenames>Ajay</forenames></author></authors><title>HT-Ring Paxos: Theory of High Throughput State-Machine Replication for
  Clustered Data Centers</title><categories>cs.DC</categories><comments>18 pages. arXiv admin note: substantial text overlap with
  arXiv:1407.1237</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Implementations of state-machine replication (SMR) prevalently use the
variants of Paxos. Some of the recent variants of Paxos like, Ring Paxos,
Multi-Ring Paxos, S-Paxos and HT-Paxos achieve significantly high throughput.
However, to meet the growing demand of high throughput, we are proposing
HT-Ring Paxos, a variant of Paxos that is basically derived from the classical
Paxos. Moreover, it also adopts some fundamental concepts of Ring Paxos,
S-Paxos and HT-Paxos for increasing throughput. Furthermore, HT-Ring Paxos is
best suitable for clustered data centers and achieves comparatively high
throughput among all variants of Paxos. However, similar to Ring Paxos, latency
of the HT-Ring Paxos is quite high as compared with other variants of Paxos.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04089</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04089</id><created>2015-07-15</created><authors><author><keyname>Lu</keyname><forenames>Hua</forenames></author><author><keyname>Peterson</keyname><forenames>Jack</forenames></author></authors><title>False shares in verifiable secret sharing with finite field commitments</title><categories>cs.CR</categories><comments>1 page</comments><acm-class>D.4.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Verifiable secret sharing (VSS) is designed to allow parties to collaborate
to keep secrets. We describe here a method of fabricating false secret shares
that appear to other parties to be legitimate, which can prevent assembly of
the decryption key. This vulnerability affects VSS schemes using verification
commitments bounded to a finite field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04091</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04091</id><created>2015-07-15</created><authors><author><keyname>Zhou</keyname><forenames>Kuang</forenames><affiliation>DRUID</affiliation></author><author><keyname>Martin</keyname><forenames>Arnaud</forenames><affiliation>DRUID</affiliation></author><author><keyname>Pan</keyname><forenames>Quan</forenames></author><author><keyname>Liu</keyname><forenames>Zhun-Ga</forenames></author></authors><title>Evidential relational clustering using medoids</title><categories>cs.AI</categories><comments>in The 18th International Conference on Information Fusion, July
  2015, Washington, DC, USA , Jul 2015, Washington, United States</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In real clustering applications, proximity data, in which only pairwise
similarities or dissimilarities are known, is more general than object data, in
which each pattern is described explicitly by a list of attributes.
Medoid-based clustering algorithms, which assume the prototypes of classes are
objects, are of great value for partitioning relational data sets. In this
paper a new prototype-based clustering method, named Evidential C-Medoids
(ECMdd), which is an extension of Fuzzy C-Medoids (FCMdd) on the theoretical
framework of belief functions is proposed. In ECMdd, medoids are utilized as
the prototypes to represent the detected classes, including specific classes
and imprecise classes. Specific classes are for the data which are distinctly
far from the prototypes of other classes, while imprecise classes accept the
objects that may be close to the prototypes of more than one class. This soft
decision mechanism could make the clustering results more cautious and reduce
the misclassification rates. Experiments in synthetic and real data sets are
used to illustrate the performance of ECMdd. The results show that ECMdd could
capture well the uncertainty in the internal data structure. Moreover, it is
more robust to the initializations compared with FCMdd.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04094</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04094</id><created>2015-07-15</created><authors><author><keyname>You</keyname><forenames>Changsheng</forenames></author><author><keyname>Huang</keyname><forenames>Kaibin</forenames></author><author><keyname>Chae</keyname><forenames>Hyukjin</forenames></author></authors><title>Energy Efficient Mobile Cloud Computing Powered by Wireless Energy
  Transfer</title><categories>cs.IT math.IT</categories><comments>single column, double spacing, 30 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Achieving long battery lives or even self sustainability has been a long
standing challenge for designing mobile devices. This paper presents a novel
solution that seamlessly integrates two technologies, mobile cloud computing
and microwave power transfer (MPT), to enable computation in passive
low-complexity devices such as sensors and wearable computing devices.
Specifically, considering a single-user system, a base station (BS) either
transfers power to or offloads computation from a mobile to the cloud; the
mobile uses harvested energy to compute given data either locally or by
offloading. A framework for energy efficient computing is proposed that
comprises a set of policies for controlling CPU cycles for the mode of local
computing, time division between MPT and offloading for the other mode of
offloading, and mode selection. Given the CPU-cycle statistics information and
channel state information (CSI), the policies aim at maximizing the probability
of successfully computing given data, called computing probability, under the
energy harvesting and deadline constraints. The policy optimization is
translated into the equivalent problems of minimizing the mobile energy
consumption for local computing and maximizing the mobile energy savings for
offloading which are solved using convex optimization theory. The structures of
the resultant policies are characterized in closed form. Furthermore, given
non-causal CSI, the said analytical framework is further developed to support
computation load allocation over multiple channel realizations, which further
increases computing probability. Last, simulation demonstrates the feasibility
of wirelessly powered mobile cloud computing and the gain of its optimal
control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04110</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04110</id><created>2015-07-15</created><updated>2015-11-23</updated><authors><author><keyname>Khan</keyname><forenames>Khalid</forenames></author><author><keyname>Lobiyal</keyname><forenames>D. K.</forenames></author><author><keyname>Kilicman</keyname><forenames>Adem</forenames></author></authors><title>A de Casteljau Algorithm for Bernstein type Polynomials based on
  (p,q)-integers</title><categories>cs.GR</categories><comments>16 pages, 7 figures, basis function revised. arXiv admin note:
  substantial text overlap with arXiv:1505.01810</comments><msc-class>65D17, 41A10, 41A25, 41A36</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a de Casteljau algorithm to compute (p,q)-Bernstein Bezier
curves based on (p,q)-integers is introduced. We study the nature of degree
elevation and degree reduction for (p,q)-Bezier Bernstein functions. The new
curves have some properties similar to q-Bezier curves. Moreover, we construct
the corresponding tensor product surfaces over the rectangular domain (u, v)
\in [0, 1] \times [0, 1] depending on four parameters. We also study the de
Casteljau algorithm and degree evaluation properties of the surfaces for these
generalization over the rectangular domain. Furthermore, some fundamental
properties for (p,q)-Bernstein Bezier curves are discussed. We get q-Bezier
curves and surfaces for (u, v) \in [0, 1] \times [0, 1] when we set the
parameter p1 = p2 = 1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04113</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04113</id><created>2015-07-15</created><authors><author><keyname>Angelini</keyname><forenames>Maria Chiara</forenames></author><author><keyname>Caltagirone</keyname><forenames>Francesco</forenames></author><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author><author><keyname>Zdeborov&#xe1;</keyname><forenames>Lenka</forenames></author></authors><title>Spectral Detection on Sparse Hypergraphs</title><categories>cs.SI cond-mat.stat-mech cs.IR physics.soc-ph</categories><comments>8 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of the assignment of nodes into communities from a
set of hyperedges, where every hyperedge is a noisy observation of the
community assignment of the adjacent nodes. We focus in particular on the
sparse regime where the number of edges is of the same order as the number of
vertices. We propose a spectral method based on a generalization of the
non-backtracking Hashimoto matrix into hypergraphs. We analyze its performance
on a planted generative model and compare it with other spectral methods and
with Bayesian belief propagation (which was conjectured to be asymptotically
optimal for this model). We conclude that the proposed spectral method detects
communities whenever belief propagation does, while having the important
advantages to be simpler, entirely nonparametric, and to be able to learn the
rule according to which the hyperedges were generated without prior
information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04116</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04116</id><created>2015-07-15</created><authors><author><keyname>Mariano</keyname><forenames>Angelo</forenames></author><author><keyname>Parisi</keyname><forenames>Giorgio</forenames></author><author><keyname>Pascazio</keyname><forenames>Saverio</forenames></author></authors><title>Language discrimination and clustering via a neural network approach</title><categories>cond-mat.dis-nn cs.CL cs.NE physics.soc-ph</categories><comments>10 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We classify twenty-one Indo-European languages starting from written text. We
use neural networks in order to define a distance among different languages,
construct a dendrogram and analyze the ultrametric structure that emerges. Four
or five subgroups of languages are identified, according to the &quot;cut&quot; of the
dendrogram, drawn with an entropic criterion. The results and the method are
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04121</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04121</id><created>2015-07-15</created><authors><author><keyname>Leike</keyname><forenames>Jan</forenames></author><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author></authors><title>Solomonoff Induction Violates Nicod's Criterion</title><categories>cs.LG cs.AI math.ST stat.TH</categories><comments>ALT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nicod's criterion states that observing a black raven is evidence for the
hypothesis H that all ravens are black. We show that Solomonoff induction does
not satisfy Nicod's criterion: there are time steps in which observing black
ravens decreases the belief in H. Moreover, while observing any computable
infinite string compatible with H, the belief in H decreases infinitely often
when using the unnormalized Solomonoff prior, but only finitely often when
using the normalized Solomonoff prior. We argue that the fault is not with
Solomonoff induction; instead we should reject Nicod's criterion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04124</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04124</id><created>2015-07-15</created><authors><author><keyname>Leike</keyname><forenames>Jan</forenames></author><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author></authors><title>On the Computability of Solomonoff Induction and Knowledge-Seeking</title><categories>cs.AI cs.LG</categories><comments>ALT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Solomonoff induction is held as a gold standard for learning, but it is known
to be incomputable. We quantify its incomputability by placing various flavors
of Solomonoff's prior M in the arithmetical hierarchy. We also derive
computability bounds for knowledge-seeking agents, and give a limit-computable
weakly asymptotically optimal reinforcement learning agent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04125</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04125</id><created>2015-07-15</created><authors><author><keyname>Landesa-V&#xe1;zquez</keyname><forenames>Iago</forenames></author><author><keyname>Alba-Castro</keyname><forenames>Jos&#xe9; Luis</forenames></author></authors><title>Revisiting AdaBoost for Cost-Sensitive Classification. Part I:
  Theoretical Perspective</title><categories>cs.CV cs.AI cs.LG</categories><comments>Preprint submitted to Pattern Recognition (July 13, 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Boosting algorithms have been widely used to tackle a plethora of problems.
In the last few years, a lot of approaches have been proposed to provide
standard AdaBoost with cost-sensitive capabilities, each with a different
focus. However, for the researcher, these algorithms shape a tangled set with
diffuse differences and properties, lacking a unifying analysis to jointly
compare, classify, evaluate and discuss those approaches on a common basis. In
this series of two papers we aim to revisit the various proposals, both from
theoretical (Part I) and practical (Part II) perspectives, in order to analyze
their specific properties and behavior, with the final goal of identifying the
algorithm providing the best and soundest results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04126</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04126</id><created>2015-07-15</created><authors><author><keyname>Landesa-V&#xe1;zquez</keyname><forenames>Iago</forenames></author><author><keyname>Alba-Castro</keyname><forenames>Jos&#xe9; Luis</forenames></author></authors><title>Revisiting AdaBoost for Cost-Sensitive Classification. Part II:
  Empirical Analysis</title><categories>cs.CV cs.AI cs.LG</categories><comments>Submitted to Pattern Recognition (July 13, 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A lot of approaches, each following a different strategy, have been proposed
in the literature to provide AdaBoost with cost-sensitive properties. In the
first part of this series of two papers, we have presented these algorithms in
a homogeneous notational framework, proposed a clustering scheme for them and
performed a thorough theoretical analysis of those approaches with a fully
theoretical foundation. The present paper, in order to complete our analysis,
is focused on the empirical study of all the algorithms previously presented
over a wide range of heterogeneous classification problems. The results of our
experiments, confirming the theoretical conclusions, seem to reveal that the
simplest approach, just based on cost-sensitive weight initialization, is the
one showing the best and soundest results, despite having been recurrently
overlooked in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04133</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04133</id><created>2015-07-15</created><authors><author><keyname>Karetsos</keyname><forenames>George T.</forenames></author><author><keyname>Rouskas</keyname><forenames>Angelos</forenames></author><author><keyname>Foukalas</keyname><forenames>Fotis</forenames></author></authors><title>Energy-efficient Traffic Bypassing in LTE HetNets with Mobile Relays</title><categories>cs.NI</categories><comments>6 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the core technologies being standardized by 3GPP for LTE-A is the
introduction of Relay Nodes (RNs). RNs are intended for ensuring coverage at
cell edges as well as for the provision of enhanced capacity at hot spot areas.
An extension to this concept is the Mobile Relay (MR). MRs can be mounted on
vehicles and the original idea is to serve users inside high speed trains thus
counter fighting the inherent severe fading and vehicle penetration loss. In
this work we present a framework for exploiting Mobile Relay (MRs) even at low
speeds in urban environments for bypassing traffic from nearby users, either
within or outside the vehicles. In particular we show that apart from increased
capacity and good quality coverage this approach achieves important energy
savings for the mobile terminals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04155</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04155</id><created>2015-07-15</created><authors><author><keyname>Orhan</keyname><forenames>Cem</forenames></author><author><keyname>Ta&#x15f;tan</keyname><forenames>&#xd6;znur</forenames></author></authors><title>ALEVS: Active Learning by Statistical Leverage Sampling</title><categories>cs.LG stat.ML</categories><comments>4 pages, presented as contributed talk in ICML 2015 Active Learning
  Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Active learning aims to obtain a classifier of high accuracy by using fewer
label requests in comparison to passive learning by selecting effective
queries. Many active learning methods have been developed in the past two
decades, which sample queries based on informativeness or representativeness of
unlabeled data points. In this work, we explore a novel querying criterion
based on statistical leverage scores. The statistical leverage scores of a row
in a matrix are the squared row-norms of the matrix containing its (top) left
singular vectors and is a measure of influence of the row on the matrix.
Leverage scores have been used for detecting high influential points in
regression diagnostics and have been recently shown to be useful for data
analysis and randomized low-rank matrix approximation algorithms. We explore
how sampling data instances with high statistical leverage scores perform in
active learning. Our empirical comparison on several binary classification
datasets indicate that querying high leverage points is an effective strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04159</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04159</id><created>2015-07-15</created><authors><author><keyname>Zhu</keyname><forenames>Xudong</forenames></author><author><keyname>Dai</keyname><forenames>Linglong</forenames></author><author><keyname>Wang</keyname><forenames>Zhaocheng</forenames></author></authors><title>Graph Coloring Based Pilot Allocation to Mitigate Pilot Contamination
  for Multi-Cell Massive MIMO Systems</title><categories>cs.IT math.IT</categories><comments>This paper is under the second-round review for an IEEE journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A massive multiple-input multiple-output (MIMO) system, which utilizes a
large number of base station (BS) antennas to serve a set of users, suffers
from pilot contamination due to the inter-cell interference (ICI). In this
letter, a graph coloring based pilot allocation (GC-PA) scheme is proposed to
mitigate pilot contamination for multi-cell massive MIMO systems. Specifically,
by exploiting the large-scale characteristics of fading channels, an
interference graph is firstly constructed to describe the potential ICI
relationship of all users. Then, with the limited pilot resource, the proposed
GC-PA scheme aims to mitigate the potential ICI by efficiently allocating
pilots among users in the interference graph. The performance gain of the
proposed scheme is verified by simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04177</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04177</id><created>2015-07-15</created><authors><author><keyname>Agaev</keyname><forenames>Rafig</forenames></author><author><keyname>Chebotarev</keyname><forenames>Pavel</forenames></author></authors><title>The projection method for continuous-time consensus seeking</title><categories>cs.SY math.OC</categories><comments>10 pages, to appear in Automation and Remote Control, 2015, No.8</comments><msc-class>93A14, 68T42, 15B51, 05C50, 05C05, 60J22</msc-class><journal-ref>Automation and Remote Control. 2015. No. 8. P. 1436-1445</journal-ref><doi>10.1134/S0005117915080081</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the case where the dependency digraph has no spanning in-tree, we
characterize the region of convergence of the basic continuous-time distributed
consensus algorithm and show that consensus can be achieved by employing the
method of orthogonal projection, which has been proposed for the discrete-time
coordination problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04180</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04180</id><created>2015-07-15</created><authors><author><keyname>Ismayilov</keyname><forenames>Ali</forenames></author><author><keyname>Kontokostas</keyname><forenames>Dimitris</forenames></author><author><keyname>Auer</keyname><forenames>S&#xf6;ren</forenames></author><author><keyname>Lehmann</keyname><forenames>Jens</forenames></author><author><keyname>Hellmann</keyname><forenames>Sebastian</forenames></author></authors><title>Wikidata through the Eyes of DBpedia</title><categories>cs.DB</categories><comments>8 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  DBpedia is one of the first and most prominent nodes of the Linked Open Data
cloud. It provides structured data for more than 100 Wikipedia language
editions as well as Wikimedia Commons, has a mature ontology and a stable and
thorough Linked Data publishing lifecycle. Wikidata, on the other hand, has
recently emerged as a user curated source for structured information which is
included in Wikipedia. In this paper, we present how Wikidata is incorporated
in the DBpedia ecosystem. Enriching DBpedia with structured information from
Wikidata provides added value for a number of usage scenarios. We outline those
scenarios and describe the structure and conversion process of the
DBpediaWikidata dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04188</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04188</id><created>2015-07-15</created><authors><author><keyname>Thaler</keyname><forenames>Justin</forenames></author></authors><title>Stream Verification</title><categories>cs.DS</categories><comments>A significantly abridged version of this article is to appear in the
  Springer Encyclopedia of Algorithms</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We survey models and algorithms for stream verification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04192</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04192</id><created>2015-07-15</created><authors><author><keyname>Alshamsi</keyname><forenames>Aamena</forenames></author><author><keyname>Pianesi</keyname><forenames>Fabio</forenames></author><author><keyname>Lepri</keyname><forenames>Bruno</forenames></author><author><keyname>Pentland</keyname><forenames>Alex</forenames></author><author><keyname>Rahwan</keyname><forenames>Iyad</forenames></author></authors><title>Beyond Contagion: Reality Mining Reveals Complex Patterns of Social
  Influence</title><categories>cs.SI physics.soc-ph</categories><doi>10.1371/journal.pone.0135740</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contagion, a concept from epidemiology, has long been used to characterize
social influence on people's behavior and affective (emotional) states. While
it has revealed many useful insights, it is not clear whether the contagion
metaphor is sufficient to fully characterize the complex dynamics of
psychological states in a social context. Using wearable sensors that capture
daily face-to-face interaction, combined with three daily experience sampling
surveys, we collected the most comprehensive data set of personality and
emotion dynamics of an entire community of work. From this high-resolution data
about actual (rather than self-reported) face-to-face interaction, a complex
picture emerges where contagion (that can be seen as adaptation of behavioral
responses to the behavior of other people) cannot fully capture the dynamics of
transitory states. We found that social influence has two opposing effects on
states: \emph{adaptation} effects that go beyond mere contagion, and
\emph{complementarity} effects whereby individuals' behaviors tend to
complement the behaviors of others. Surprisingly, these effects can exhibit
completely different directions depending on the stable personality or
emotional dispositions (stable traits) of target individuals. Our findings
provide a foundation for richer models of social dynamics, and have
implications on organizational engineering and workplace well-being.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04201</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04201</id><created>2015-07-15</created><authors><author><keyname>Pavlidis</keyname><forenames>Nicos</forenames></author><author><keyname>Hofmeyr</keyname><forenames>David</forenames></author><author><keyname>Tasoulis</keyname><forenames>Sotiris</forenames></author></authors><title>Minimum Density Hyperplane: An Unsupervised and Semi-Supervised
  Classifier</title><categories>stat.ML cs.LG</categories><msc-class>62H30, 68T10</msc-class><acm-class>I.5.0; I.5.3; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Associating distinct groups of objects (clusters) with contiguous regions of
high probability density (high-density clusters), is a central assumption in
statistical and machine learning approaches for the classification of
unlabelled data. In unsupervised classification this cluster definition
underlies a nonparametric approach known as density clustering. In
semi-supervised classification, class boundaries are assumed to lie in regions
of low density, which is equivalent to assuming that high-density clusters are
associated with a single class. We propose a novel hyperplane classifier for
unlabelled data that avoids splitting high-density clusters. The minimum
density hyperplane minimises the integral of the empirical probability density
function along a hyperplane. The link between this approach and density
clustering is immediate. We are able to establish a link between the minimum
density and the maximum margin hyperplanes, thus linking this approach to
maximum margin clustering and semi-supervised support vector machine
classifiers. We propose a globally convergent algorithm for the estimation of
minimum density hyperplanes for unsupervised and semi-supervised
classification. The performance of the proposed approach for unsupervised and
semi-supervised classification is evaluated on a number of benchmark datasets
and is shown to be very promising.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04203</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04203</id><created>2015-07-15</created><authors><author><keyname>Maulat</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Salvy</keyname><forenames>Bruno</forenames></author></authors><title>Formulas for Continued Fractions. An Automated Guess and Prove Approach</title><categories>cs.SC</categories><comments>Maple worksheet attached</comments><acm-class>I.1.2</acm-class><journal-ref>ISSAC 15: Proceedings of the 2015 ACM International Symposium on
  Symbolic and Algebraic Computation, pp. 275--282</journal-ref><doi>10.1145/2755996.2756660</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a simple method that produces automatically closed forms for the
coefficients of continued fractions expansions of a large number of special
functions. The function is specified by a non-linear differential equation and
initial conditions. This is used to generate the first few coefficients and
from there a conjectured formula. This formula is then proved automatically
thanks to a linear recurrence satisfied by some remainder terms. Extensive
experiments show that this simple approach and its straightforward
generalization to difference and $q$-difference equations capture a large part
of the formulas in the literature on continued fractions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04204</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04204</id><created>2015-07-15</created><authors><author><keyname>Zhu</keyname><forenames>Xudong</forenames></author><author><keyname>Wang</keyname><forenames>Zhaocheng</forenames></author><author><keyname>Dai</keyname><forenames>Linglong</forenames></author><author><keyname>Qian</keyname><forenames>Chen</forenames></author></authors><title>Smart Pilot Assignment for Massive MIMO</title><categories>cs.IT math.IT</categories><comments>4 pages, 3 figures</comments><journal-ref>IEEE Communications Letters, vol. 19 , no. 8, Aug. 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A massive multiple-input multiple-output (MIMO) system, which utilizes a
large number of antennas at the base station (BS) to serve multiple users,
suffers from pilot contamination due to inter-cell interference. A smart pilot
assignment (SPA) scheme is proposed in this letter to improve the performance
of users with severe pilot contamination. Specifically, by exploiting the
large-scale characteristics of fading channels, the BS firstly measures the
inter-cell interference of each pilot sequence caused by the users with the
same pilot sequence in other adjacent cells. Then, in contrast to the
conventional schemes which assign the pilot sequences to the users randomly,
the proposed SPA method assigns the pilot sequence with the smallest inter-cell
interference to the user having the worst channel quality in a sequential way
to improve its performance. Simulation results verify the performance gain of
the proposed scheme in typical massive MIMO systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04207</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04207</id><created>2015-07-15</created><authors><author><keyname>Bern&#xe1;th</keyname><forenames>Attila</forenames></author><author><keyname>Kir&#xe1;ly</keyname><forenames>Tam&#xe1;s</forenames></author></authors><title>Blocking optimal $k$-arborescences</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a digraph $D=(V,A)$ and a positive integer $k$, an arc set $F\subseteq
A$ is called a \textbf{$k$-arborescence} if it is the disjoint union of $k$
spanning arborescences. The problem of finding a minimum cost $k$-arborescence
is known to be polynomial-time solvable using matroid intersection. In this
paper we study the following problem: find a minimum cardinality subset of arcs
that contains at least one arc from every minimum cost $k$-arborescence. For
$k=1$, the problem was solved in [A. Bern\'ath, G. Pap , Blocking optimal
arborescences, IPCO 2013]. In this paper we give an algorithm for general $k$
that has polynomial running time if $k$ is fixed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04208</identifier>
 <datestamp>2015-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04208</id><created>2015-07-15</created><updated>2015-11-17</updated><authors><author><keyname>Kveton</keyname><forenames>Branislav</forenames></author><author><keyname>Wen</keyname><forenames>Zheng</forenames></author><author><keyname>Ashkan</keyname><forenames>Azin</forenames></author><author><keyname>Szepesvari</keyname><forenames>Csaba</forenames></author></authors><title>Combinatorial Cascading Bandits</title><categories>cs.LG stat.ML</categories><comments>Advances in Neural Information Processing Systems 28</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose combinatorial cascading bandits, a class of partial monitoring
problems where at each step a learning agent chooses a tuple of ground items
subject to constraints and receives a reward if and only if the weights of all
chosen items are one. The weights of the items are binary, stochastic, and
drawn independently of each other. The agent observes the index of the first
chosen item whose weight is zero. This observation model arises in network
routing, for instance, where the learning agent may only observe the first link
in the routing path which is down, and blocks the path. We propose a UCB-like
algorithm for solving our problems, CombCascade; and prove gap-dependent and
gap-free upper bounds on its $n$-step regret. Our proofs build on recent work
in stochastic combinatorial semi-bandits but also address two novel challenges
of our setting, a non-linear reward function and partial observability. We
evaluate CombCascade on two real-world problems and show that it performs well
even when our modeling assumptions are violated. We also demonstrate that our
setting requires a new learning algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04213</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04213</id><created>2015-07-15</created><authors><author><keyname>Zhu</keyname><forenames>Xudong</forenames></author><author><keyname>Wang</keyname><forenames>Zhaocheng</forenames></author><author><keyname>Qian</keyname><forenames>Chen</forenames></author><author><keyname>Dai</keyname><forenames>Linglong</forenames></author><author><keyname>Chen</keyname><forenames>Jinhui</forenames></author><author><keyname>Chen</keyname><forenames>Sheng</forenames></author><author><keyname>Hanzo</keyname><forenames>Lajos</forenames></author></authors><title>Soft Pilot Reuse and Multi-Cell Block Diagonalization Precoding for
  Massive MIMO Systems</title><categories>cs.IT math.IT</categories><comments>13 pages, 12 figures, accepted for publication in IEEE Transactions
  on Vehicular Technology, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The users at cell edge of a massive multiple-input multiple-output (MIMO)
system suffer from severe pilot contamination, which leads to poor quality of
service (QoS). In order to enhance the QoS for these edge users, soft pilot
reuse (SPR) combined with multi-cell block diagonalization (MBD) precoding are
proposed. Specifically, the users are divided into two groups according to
their large-scale fading coefficients, referred to as the center users, who
only suffer from modest pilot contamination and the edge users, who suffer from
severe pilot contamination. Based on this distinction, the SPR scheme is
proposed for improving the QoS for the edge users, whereby a cell-center pilot
group is reused for all cell-center users in all cells, while a cell-edge pilot
group is applied for the edge users in the adjacent cells. By extending the
classical block diagonalization precoding to a multi-cell scenario, the MBD
precoding scheme projects the downlink transmit signal onto the null space of
the subspace spanned by the inter-cell channels of the edge users in adjacent
cells. Thus, the inter-cell interference contaminating the edge users' signals
in the adjacent cells can be efficiently mitigated and hence the QoS of these
edge users can be further enhanced. Our theoretical analysis and simulation
results demonstrate that both the uplink and downlink rates of the edge users
are significantly improved, albeit at the cost of the slightly decreased rate
of center users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04214</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04214</id><created>2015-07-15</created><authors><author><keyname>Mersinli</keyname><forenames>Umit</forenames></author></authors><title>Associative Measures and Multi-word Unit Extraction in Turkish</title><categories>cs.CL</categories><journal-ref>Associative Measures and Multi-word Unit Extraction in Turkish.
  Dil ve Edebiyat Dergisi. 12(1). 43-61</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Associative measures are &quot;mathematical formulas determining the strength of
association between two or more words based on their occurrences and
cooccurrences in a text corpus&quot; (Pecina, 2010, p. 138). The purpose of this
paper is to test the 12 associative measures that Text-NSP (Banerjee &amp;
Pedersen, 2003) contains on a 10-million-word subcorpus of Turkish National
Corpus (TNC) (Aksan et.al., 2012). A statistical comparison of those measures
is out of the scope of the study, and the measures will be evaluated according
to the linguistic relevance of the rankings they provide. The focus of the
study is basically on optimizing the corpus data, before applying the measures
and then, evaluating the rankings produced by these measures as a whole, not on
the linguistic relevance of individual n-grams. The findings include
intra-linguistically relevant associative measures for a comma delimited,
sentence splitted, lower-cased, well-balanced, representative, 10-million-word
corpus of Turkish.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04215</identifier>
 <datestamp>2015-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04215</id><created>2015-07-15</created><updated>2015-07-21</updated><authors><author><keyname>Mendon&#xe7;a</keyname><forenames>Israel</forenames><affiliation>LIA</affiliation></author><author><keyname>Figueiredo</keyname><forenames>Rosa</forenames><affiliation>LIA</affiliation></author><author><keyname>Labatut</keyname><forenames>Vincent</forenames><affiliation>LIA</affiliation></author><author><keyname>Michelon</keyname><forenames>Philippe</forenames><affiliation>LIA</affiliation></author></authors><title>Relevance of Negative Links in Graph Partitioning: A Case Study Using
  Votes From the European Parliament</title><categories>cs.SI cs.RO math.OC physics.soc-ph</categories><comments>in 2nd European Network Intelligence Conference (ENIC), Sep 2015,
  Karlskrona, Sweden</comments><proxy>ccsd</proxy><doi>10.1109/ENIC.2015.25</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we want to study the informative value of negative links in
signed complex networks. For this purpose, we extract and analyze a collection
of signed networks representing voting sessions of the European Parliament
(EP). We first process some data collected by the VoteWatch Europe Website for
the whole 7 th term (2009-2014), by considering voting similarities between
Members of the EP to define weighted signed links. We then apply a selection of
community detection algorithms, designed to process only positive links, to
these data. We also apply Parallel Iterative Local Search (Parallel ILS), an
algorithm recently proposed to identify balanced partitions in signed networks.
Our results show that, contrary to the conclusions of a previous study focusing
on other data, the partitions detected by ignoring or considering the negative
links are indeed remarkably different for these networks. The relevance of
negative links for graph partitioning therefore is an open question which
should be further explored.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04220</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04220</id><created>2015-07-15</created><authors><author><keyname>Hartmann</keyname><forenames>Guido</forenames></author></authors><title>A numerical analysis of Quicksort: How many cases are bad cases?</title><categories>cs.DS</categories><comments>29 pages, 13 figures</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present numerical results for the probability of bad cases for Quicksort,
i.e. cases of input data for which the sorting cost considerably exceeds that
of the average. Dynamic programming was used to compute solutions of the
recurrence for the frequency distributions of comparisons. From these
solutions, probabilities of numbers of comparisons above certain thresholds
relative to the average were extracted. Computations were done for array sizes
up to n = 500 elements and for several methods to select the partitioning
element, from a simple random selection to what we call &quot;recursive median of
three medians.&quot; We found that the probability strongly depends on the selection
method: for n = 500 and a theshold 25% above the average number of comparisons
it ranges from 2.2*10^(-3) to 3.0*10^(-23). A version of Quicksort based on the
recursive median of medians approach is proposed, for which our data suggest a
worst case time complexity of O(n^1.37).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04221</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04221</id><created>2015-07-15</created><authors><author><keyname>Trossen</keyname><forenames>Dirk</forenames></author><author><keyname>Reed</keyname><forenames>Martin J.</forenames></author><author><keyname>Riihij&#xe4;rvi</keyname><forenames>Janne</forenames></author><author><keyname>Georgiades</keyname><forenames>Michael</forenames></author><author><keyname>Fotiou</keyname><forenames>Nikos</forenames></author><author><keyname>Xylomenos</keyname><forenames>George</forenames></author></authors><title>IP Over ICN - The Better IP? An Unusual Take on Information-Centric
  Networking</title><categories>cs.NI</categories><comments>5 pages in EUCNC 2015 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a proposition for informationcentric networking (ICN)
that lies outside the typical trajectory of aiming for a wholesale replacement
of IP as the internetworking layer of the Internet. Instead, we propose that a
careful exploitation of key ICN benefits, expanding previously funded ICN
efforts, will enable individual operators to improve the performance of their
IP-based services along many dimensions. Alongside the main motivation for our
work, we present an early strawman architecture for such an IP-over-ICN
proposition, which will ultimately be implemented and trialed in a recently
started H2020 research effort.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04222</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04222</id><created>2015-07-15</created><authors><author><keyname>Mamageishvili</keyname><forenames>Akaki</forenames></author><author><keyname>Mihalak</keyname><forenames>Matus</forenames></author></authors><title>Multicast Network Design Game on a Ring</title><categories>cs.GT</categories><comments>12 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study quality measures of different solution concepts for
the multicast network design game on a ring topology. We recall from the
literature a lower bound of 4/3 and prove a matching upper bound for the price
of stability, which is the ratio of the social costs of a best Nash equilibrium
and of a general optimum. Therefore, we answer an open question posed by
Fanelli et al. in [12]. We prove an upper bound of 2 for the ratio of the costs
of a potential optimizer and of an optimum, provide a construction of a lower
bound, and give a computer-assisted argument that it reaches $2$ for any
precision. We then turn our attention to players arriving one by one and
playing myopically their best response. We provide matching lower and upper
bounds of 2 for the myopic sequential price of anarchy (achieved for a
worst-case order of the arrival of the players). We then initiate the study of
myopic sequential price of stability and for the multicast game on the ring we
construct a lower bound of 4/3, and provide an upper bound of 26/19. To the
end, we conjecture and argue that the right answer is 4/3.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04227</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04227</id><created>2015-07-15</created><updated>2015-08-03</updated><authors><author><keyname>Makarychev</keyname><forenames>Konstantin</forenames></author><author><keyname>Makarychev</keyname><forenames>Yury</forenames></author><author><keyname>Sviridenko</keyname><forenames>Maxim</forenames></author><author><keyname>Ward</keyname><forenames>Justin</forenames></author></authors><title>A bi-criteria approximation algorithm for $k$ Means</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the classical $k$-means clustering problem in the setting
bi-criteria approximation, in which an algoithm is allowed to output $\beta k &gt;
k$ clusters, and must produce a clustering with cost at most $\alpha$ times the
to the cost of the optimal set of $k$ clusters. We argue that this approach is
natural in many settings, for which the exact number of clusters is a priori
unknown, or unimportant up to a constant factor. We give new bi-criteria
approximation algorithms, based on linear programming and local search,
respectively, which attain a guarantee $\alpha(\beta)$ depending on the number
$\beta k$ of clusters that may be opened. Our gurantee $\alpha(\beta)$ is
always at most $9 + \epsilon$ and improves rapidly with $\beta$ (for example:
$\alpha(2)&lt;2.59$, and $\alpha(3) &lt; 1.4$). Moreover, our algorithms have only
polynomial dependence on the dimension of the input data, and so are applicable
in high-dimensional settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04229</identifier>
 <datestamp>2015-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04229</id><created>2015-07-15</created><authors><author><keyname>Ferber</keyname><forenames>Asaf</forenames></author><author><keyname>Pfister</keyname><forenames>Pascal</forenames></author></authors><title>Strong games played on random graphs</title><categories>cs.DM cs.GT math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a strong game played on the edge set of a graph G there are two players,
Red and Blue, alternating turns in claiming previously unclaimed edges of G
(with Red playing first). The winner is the first one to claim all the edges of
some target structure (such as a clique, a perfect matching, a Hamilton cycle,
etc.). It is well known that Red can always ensure at least a draw in any
strong game, but finding explicit winning strategies is a difficult and a quite
rare task. We consider strong games played on the edge set of a random graph G
~ G(n,p) on n vertices. We prove, for sufficiently large $n$ and a fixed
constant 0 &lt; p &lt; 1, that Red can w.h.p win the perfect matching game on a
random graph G ~ G(n,p).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04230</identifier>
 <datestamp>2015-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04230</id><created>2015-07-15</created><authors><author><keyname>Huang</keyname><forenames>Jiaji</forenames></author><author><keyname>Qiu</keyname><forenames>Qiang</forenames></author><author><keyname>Calderbank</keyname><forenames>Robert</forenames></author></authors><title>The Role of Principal Angles in Subspace Classification</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Subspace models play an important role in a wide range of signal processing
tasks, and this paper explores how the pairwise geometry of subspaces
influences the probability of misclassification. When the mismatch between the
signal and the model is vanishingly small, the probability of misclassification
is determined by the product of the sines of the principal angles between
subspaces. When the mismatch is more significant, the probability of
misclassification is determined by the sum of the squares of the sines of the
principal angles. Reliability of classification is derived in terms of the
distribution of signal energy across principal vectors. Larger principal angles
lead to smaller classification error, motivating a linear transform that
optimizes principal angles. The transform presented here (TRAIT) preserves some
specific characteristic of each individual class, and this approach is shown to
be complementary to a previously developed transform (LRT) that enlarges
inter-class distance while suppressing intra-class dispersion. Theoretical
results are supported by demonstration of superior classification accuracy on
synthetic and measured data even in the presence of significant model mismatch.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04234</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04234</id><created>2015-07-15</created><updated>2016-01-21</updated><authors><author><keyname>Vyavahare</keyname><forenames>Pooja</forenames></author><author><keyname>Diwan</keyname><forenames>Nutan Limaye Ajit A.</forenames></author><author><keyname>Manjunath</keyname><forenames>D.</forenames></author></authors><title>On the Maximum Rate of Networked Computation in a Capacitated Network</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a capacitated communication network $\mathcal{N}$ and a function f that
needs to be computed on $\mathcal{N},$ we study the problem of generating a
computation and communication schedule in $\mathcal{N}$ to maximize the rate of
computation of f. Shah et. al.[IEEE Journal of Selected Areas in Communication,
2013] studied this problem when the computation schema $\mathcal{G}$ for f is a
tree. We define the notion of a schedule when $\mathcal{G}$ is a general DAG
and show that finding an optimal schedule is equivalent to finding the solution
of a packing LP. We prove that approximating the maximum rate is MAX SNP-hard
by looking at the packing LP. For this packing LP we prove that solving the
separation oracle of its dual is equivalent to solving the LP. The separation
oracle of the dual reduces to the problem of finding minimum cost embedding
given $\mathcal{N},\mathcal{G},$ which we prove to be MAX SNP-hard even when
$\mathcal{G}$ has bounded degree and bounded edge weights and $\mathcal{N}$ has
just three vertices. We present a polynomial time algorithm to compute the
maximum rate of function computation when $\mathcal{N}$ has two vertices by
reducing the problem to a version of submodular function minimization problem.
For the general $\mathcal{N}$ we study restricted class of schedules and its
equivalent packing LP. We observe that for this packing LP also the separation
oracle of its dual reduces to finding minimum cost embedding. A version of this
minimum cost embedding problem has been studied in literature. We present a
quadratic integer program for the minimum cost embedding problem and its linear
programming relaxation based on earthmover metric. We also present some
approximate algorithms for special classes of $\mathcal{G}.$
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04240</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04240</id><created>2015-07-15</created><authors><author><keyname>Zhang</keyname><forenames>Jiayi</forenames></author><author><keyname>Dai</keyname><forenames>Linglong</forenames></author><author><keyname>Zhang</keyname><forenames>Yu</forenames></author><author><keyname>Wang</keyname><forenames>Zhaocheng</forenames></author></authors><title>Unified Performance Analysis of Mixed Radio Frequency/Free-Space Optical
  Dual-Hop Transmission Systems</title><categories>cs.IT math.IT</categories><comments>7 pages, 5 figures, IEEE JLT</comments><journal-ref>IEEE/OSA Journal of Lightwave Technology, vol. 33, no. 11, pp.
  2286-2293, June 2015</journal-ref><doi>10.1109/JLT.2015.2409570</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The mixed radio frequency (RF)/free-space optical (FSO) relaying is a
promising technology for coverage improvement, while there lacks unified
expressions to describe its performance. In this paper, a unified performance
analysis framework of a dual-hop relay system over asymmetric RF/FSO links is
presented. More specifically, we consider the RF link follows generalized
$\kappa$-$\mu$ or $\eta$-$\mu$ distributions, while the FSO link experiences
the gamma-gamma distribution, respectively. Novel analytical expressions of the
probability density function and cumulative distribution function are derived.
We then capitalize on these results to provide new exact analytical expressions
of the outage probability and bit error rate (BER). Furthermore, the outage
probability for high signal-to-noise ratios and the BER for different
modulation schemes are deduced to provide useful insights into the impact of
system and channel parameters of the overall system performance. These accurate
expressions are general, since they correspond to generalized fading in the RF
link and account for pointing errors, atmospheric turbulence and different
modulation schemes in the FSO link. The links between derived results and
previous results are presented. Finally, numerical and Monte-Carlo simulation
results are provided to demonstrate the validity of the proposed unified
expressions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04243</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04243</id><created>2015-07-15</created><authors><author><keyname>Zhang</keyname><forenames>Jiayi</forenames></author><author><keyname>Dai</keyname><forenames>Linglong</forenames></author><author><keyname>Wang</keyname><forenames>Zhaocheng</forenames></author><author><keyname>Ng</keyname><forenames>Derrick Wing Kwan</forenames></author><author><keyname>Gerstacker</keyname><forenames>Wolfgang H.</forenames></author></authors><title>Effective Rate Analysis of MISO Systems over $\alpha$-$\mu$ Fading
  Channels</title><categories>cs.IT math.IT</categories><comments>6 pages, 3 figures, IEEE Globecom 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The effective rate is an important performance metric of real-time
applications in next generation wireless networks. In this paper, we present an
analysis of the effective rate of multiple-input single-output (MISO) systems
over $\alpha$-$\mu$ fading channels under a maximum delay constraint. More
specifically, novel and highly accurate closed-form approximate expressions of
the effective rate are derived for such systems assuming the generalized
$\alpha$-$\mu$ channel model. In order to examine the impact of system and
channel parameters on the effective rate, we also derive closed-form
expressions of the effective rate in asymptotically high and low
signal-to-noise ratio (SNR) regimes. Furthermore, connections between our
derived results and existing results from the literature are revealed for the
sake of completeness. Our results demonstrate that the effective rate is a
monotonically increasing function of channel fading parameters $\alpha$ and
$\mu$, as well as the number of transmit antennas, while it decreases to zero
when the delay constraint becomes stringent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04244</identifier>
 <datestamp>2015-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04244</id><created>2015-07-15</created><updated>2015-11-27</updated><authors><author><keyname>Zhang</keyname><forenames>Jiayi</forenames></author><author><keyname>Dai</keyname><forenames>Linglong</forenames></author><author><keyname>Zhang</keyname><forenames>Xinling</forenames></author><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Wang</keyname><forenames>Zhaocheng</forenames></author></authors><title>Achievable Rate of Rician Large-Scale MIMO Channels with Transceiver
  Hardware Impairments</title><categories>cs.IT math.IT</categories><comments>7 pages, 1 table, 3 figures, accepted to appear in IEEE Transactions
  on Vehicular Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transceiver hardware impairments (e.g., phase noise,
in-phase/quadrature-phase (I/Q) imbalance, amplifier non-linearities, and
quantization errors) have obvious degradation effects on the performance of
wireless communications. While prior works have improved our knowledge on the
influence of hardware impairments of single-user multiple-input multiple-output
(MIMO) systems over Rayleigh fading channels, an analysis encompassing the
Rician fading channel is not yet available. In this paper, we pursue a detailed
analysis of regular and large-scale (LS) MIMO systems over Rician fading
channels by deriving new, closed-form expressions for the achievable rate to
provide several important insights for practical system design. More
specifically, for regular MIMO systems with hardware impairments, there is
always a finite achievable rate ceiling, which is irrespective of the transmit
power and fading conditions. For LS-MIMO systems, it is interesting to find
that the achievable rate loss depends on the Rician $K$-factor, which reveals
that the favorable propagation in LS-MIMO systems can remove the influence of
hardware impairments. However, we show that the non-ideal LS-MIMO system can
still achieve high spectral efficiency due to its huge degrees of freedom.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04246</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04246</id><created>2015-07-15</created><updated>2015-07-20</updated><authors><author><keyname>Zhang</keyname><forenames>Jiayi</forenames></author><author><keyname>Dai</keyname><forenames>Linglong</forenames></author><author><keyname>Gerstacker</keyname><forenames>Wolfgang H.</forenames></author><author><keyname>Wang</keyname><forenames>Zhaocheng</forenames></author></authors><title>Effective capacity of communication systems over $\kappa$-$\mu$ shadowed
  fading channels</title><categories>cs.IT math.IT</categories><comments>2 pages, 2 figures, submitted to a journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The effective capacity of communication systems over generalized
$\kappa$-$\mu$ shadowed fading channels is investigated in this letter. A novel
and analytical expression for the exact effective capacity is derived in terms
of extended generalized bivariate Meijer's-$G$ function. To intuitively reveal
the impact of the system and channel parameters on the effective capacity, we
also derive closed-form expressions for the effective capacity in the
asymptotically high signal-to-noise ratio regime. Our results demonstrate that
the effective capacity is a monotonically increasing function of channel fading
parameters $\kappa$ and $\mu$ as well as the shadowing parameter $m$, while it
decays to zero when the delay constraint $\theta \rightarrow \infty$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04271</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04271</id><created>2015-07-15</created><authors><author><keyname>Ichkov</keyname><forenames>Aleksandar</forenames></author><author><keyname>Atanasovski</keyname><forenames>Vladimir</forenames></author><author><keyname>Gavrilovska</keyname><forenames>Liljana</forenames></author></authors><title>Hybrid access control with modified SINR association for future
  heterogeneous networks</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 figures, submitted to IEEE GLOBECOM 2015 Workshop on 5G
  Heterogeneous and Small Cell Networks. arXiv admin note: text overlap with
  arXiv:1412.5340</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The heterogeneity in cellular networks that comprise multiple base stations
types imposes new challenges in network planning and deployment of future
generation of cellular networks. The Radio Resource Management (RRM)
techniques, such as dynamic sharing of the available resources and advanced
user association strategies, determine the overall network capacity and
efficiency. This paper evaluates the downlink performance of a two tier
heterogeneous network (consisting of macro and femto tiers) in terms of rate
distribution, i.e. the percentage of users that achieve certain rate in the
system. The paper specifically addresses the femto tier RRM by randomization of
the allocated resources and the user association process by introducing a
modified SINR association strategy with bias factor for load balancing. Also,
the paper introduces hybrid access control mechanism at the femto tier that
allows the authorized users of the femtocell, which are part of the Closed
Subscriber Group (CSG) on the femtocell, to achieve higher data rates up to 10
times compared to the other regular users associated in the access.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04277</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04277</id><created>2015-07-15</created><updated>2016-01-29</updated><authors><author><keyname>Dobson</keyname><forenames>Ian</forenames></author><author><keyname>Carreras</keyname><forenames>Benjamin A.</forenames></author><author><keyname>Newman</keyname><forenames>David E.</forenames></author><author><keyname>Reynolds-Barredo</keyname><forenames>Jose M.</forenames></author></authors><title>Obtaining statistics of cascading line outages spreading in an electric
  transmission network from standard utility data</title><categories>physics.soc-ph cs.SY</categories><comments>Revised and expanded version with minor corrections and a new section
  illustrating use of data to validate a simulation. Will appear in IEEE
  Transactions on Power Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how to use standard transmission line outage historical data to
obtain the network topology in such a way that cascades of line outages can be
easily located on the network. Then we obtain statistics quantifying how
cascading outages typically spread on the network. Processing real outage data
is fundamental for understanding cascading and for evaluating the validity of
the many different models and simulations that have been proposed for cascading
in power networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04285</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04285</id><created>2015-07-15</created><authors><author><keyname>Bolander</keyname><forenames>Thomas</forenames></author><author><keyname>Gierasimczuk</keyname><forenames>Nina</forenames></author></authors><title>Learning Action Models: Qualitative Approach</title><categories>cs.LG cs.AI cs.LO</categories><comments>18 pages, accepted for LORI-V: The Fifth International Conference on
  Logic, Rationality and Interaction, October 28-31, 2015, National Taiwan
  University, Taipei, Taiwan</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In dynamic epistemic logic, actions are described using action models. In
this paper we introduce a framework for studying learnability of action models
from observations. We present first results concerning propositional action
models. First we check two basic learnability criteria: finite identifiability
(conclusively inferring the appropriate action model in finite time) and
identifiability in the limit (inconclusive convergence to the right action
model). We show that deterministic actions are finitely identifiable, while
non-deterministic actions require more learning power-they are identifiable in
the limit. We then move on to a particular learning method, which proceeds via
restriction of a space of events within a learning-specific action model. This
way of learning closely resembles the well-known update method from dynamic
epistemic logic. We introduce several different learning methods suited for
finite identifiability of particular types of deterministic actions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04292</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04292</id><created>2015-07-15</created><authors><author><keyname>Alzahrani</keyname><forenames>Bander A.</forenames></author><author><keyname>Reed</keyname><forenames>Martin J.</forenames></author><author><keyname>Vassilakis</keyname><forenames>Vassilios G.</forenames></author></authors><title>Resistance against brute-force attacks on stateless forwarding in
  information centric networking</title><categories>cs.NI cs.CR</categories><comments>ACM/IEEE Symposium on Architectures for Networking and Communications
  Systems (ANCS), 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Line Speed Publish/Subscribe Inter-networking (LIPSIN) is one of the proposed
forwarding mechanisms in Information Centric Networking (ICN). It is a
stateless source-routing approach based on Bloom filters. However, it has been
shown that LIPSIN is vulnerable to brute-force attacks which may lead to
distributed denial-of-service (DDoS) attacks and unsolicited messages. In this
work, we propose a new forwarding approach that maintains the advantages of
Bloom filter based forwarding while allowing forwarding nodes to statelessly
verify if packets have been previously authorized, thus preventing attacks on
the forwarding mechanism. Analysis of the probability of attack, derived
analytically, demonstrates that the technique is highly-resistant to
brute-force attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04296</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04296</id><created>2015-07-15</created><updated>2015-07-16</updated><authors><author><keyname>Nair</keyname><forenames>Arun</forenames></author><author><keyname>Srinivasan</keyname><forenames>Praveen</forenames></author><author><keyname>Blackwell</keyname><forenames>Sam</forenames></author><author><keyname>Alcicek</keyname><forenames>Cagdas</forenames></author><author><keyname>Fearon</keyname><forenames>Rory</forenames></author><author><keyname>De Maria</keyname><forenames>Alessandro</forenames></author><author><keyname>Panneershelvam</keyname><forenames>Vedavyas</forenames></author><author><keyname>Suleyman</keyname><forenames>Mustafa</forenames></author><author><keyname>Beattie</keyname><forenames>Charles</forenames></author><author><keyname>Petersen</keyname><forenames>Stig</forenames></author><author><keyname>Legg</keyname><forenames>Shane</forenames></author><author><keyname>Mnih</keyname><forenames>Volodymyr</forenames></author><author><keyname>Kavukcuoglu</keyname><forenames>Koray</forenames></author><author><keyname>Silver</keyname><forenames>David</forenames></author></authors><title>Massively Parallel Methods for Deep Reinforcement Learning</title><categories>cs.LG cs.AI cs.DC cs.NE</categories><comments>Presented at the Deep Learning Workshop, International Conference on
  Machine Learning, Lille, France, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the first massively distributed architecture for deep
reinforcement learning. This architecture uses four main components: parallel
actors that generate new behaviour; parallel learners that are trained from
stored experience; a distributed neural network to represent the value function
or behaviour policy; and a distributed store of experience. We used our
architecture to implement the Deep Q-Network algorithm (DQN). Our distributed
algorithm was applied to 49 games from Atari 2600 games from the Arcade
Learning Environment, using identical hyperparameters. Our performance
surpassed non-distributed DQN in 41 of the 49 games and also reduced the
wall-time required to achieve these results by an order of magnitude on most
games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04299</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04299</id><created>2015-07-15</created><authors><author><keyname>Andoni</keyname><forenames>Alexandr</forenames></author><author><keyname>Razenshteyn</keyname><forenames>Ilya</forenames></author></authors><title>Tight Lower Bounds for Data-Dependent Locality-Sensitive Hashing</title><categories>cs.DS cs.CC cs.CG</categories><comments>16 pages, no figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove a tight lower bound for the exponent $\rho$ for data-dependent
Locality-Sensitive Hashing schemes, recently used to design efficient solutions
for the $c$-approximate nearest neighbor search. In particular, our lower bound
matches the bound of $\rho\le \frac{1}{2c-1}+o(1)$ for the $\ell_1$ space,
obtained via the recent algorithm from [Andoni-Razenshteyn, STOC'15].
  In recent years it emerged that data-dependent hashing is strictly superior
to the classical Locality-Sensitive Hashing, when the hash function is
data-independent. In the latter setting, the best exponent has been already
known: for the $\ell_1$ space, the tight bound is $\rho=1/c$, with the upper
bound from [Indyk-Motwani, STOC'98] and the matching lower bound from
[O'Donnell-Wu-Zhou, ITCS'11].
  We prove that, even if the hashing is data-dependent, it must hold that
$\rho\ge \frac{1}{2c-1}-o(1)$. To prove the result, we need to formalize the
exact notion of data-dependent hashing that also captures the complexity of the
hash functions (in addition to their collision properties). Without restricting
such complexity, we would allow for obviously infeasible solutions such as the
Voronoi diagram of a dataset. To preclude such solutions, we require our hash
functions to be succinct. This condition is satisfied by all the known
algorithmic results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04300</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04300</id><created>2015-07-15</created><authors><author><keyname>Srinivasan</keyname><forenames>Seshadhri</forenames></author><author><keyname>Buonopane</keyname><forenames>Furio</forenames></author><author><keyname>Ramaswamy</keyname><forenames>Srini</forenames></author><author><keyname>Vain</keyname><forenames>Juri</forenames></author></authors><title>Verifying Response Times in Networked Automation Systems Using Jitter
  Bounds</title><categories>cs.SY</categories><comments>5 Pages; 7 Figures in The 25th IEEE International Symposium on
  Software Reliability Engineering (ISSRE), 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Networked Automation Systems (NAS) have to meet stringent response time
during operation. Verifying response time of automation is an important step
during design phase before deployment. Timing discrepancies due to hardware,
software and communication components of NAS affect the response time. This
investigation uses model templates for verifying the response time in NAS.
First, jitter bounds model the timing fluctuations of NAS components. These
jitter bounds are the inputs to model templates that are formal models of
timing fluctuations. The model templates are atomic action patterns composed of
three composition operators- sequential, alternative, and parallel and embedded
in time wrapper that specifies clock driven activation conditions. Model
templates in conjunction with formal model of technical process offer an easier
way to verify the response time. The investigation demonstrates the proposed
verification method using an industrial steam boiler with typical NAS
components in plant floor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04308</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04308</id><created>2015-07-15</created><authors><author><keyname>Chen</keyname><forenames>Mingming</forenames></author><author><keyname>Nguyen</keyname><forenames>Tommy</forenames></author><author><keyname>Szymanski</keyname><forenames>Boleslaw K.</forenames></author></authors><title>A New Metric for Quality of Network Community Structure</title><categories>cs.SI physics.soc-ph</categories><comments>15 pages, 7 figures</comments><journal-ref>ASE Human Journal 2(4), September 2013, pp. 226-240</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modularity is widely used to effectively measure the strength of the
community structure found by community detection algorithms. However,
modularity maximization suffers from two opposite yet coexisting problems: in
some cases, it tends to favor small communities over large ones while in
others, large communities over small ones. The latter tendency is known in the
literature as the resolution limit problem. To address them, we propose to
modify modularity by subtracting from it the fraction of edges connecting nodes
of different communities and by including community density into modularity. We
refer to the modified metric as Modularity Density and we demonstrate that it
indeed resolves both problems mentioned above. We describe the motivation for
introducing this metric by using intuitively clear and simple examples. We also
prove that this new metric solves the resolution limit problem. Finally, we
discuss the results of applying this metric, modularity, and several other
popular community quality metrics to two real dynamic networks. The results
imply that Modularity Density is consistent with all the community quality
measurements but not modularity, which suggests that Modularity Density is an
improved measurement of the community quality compared to modularity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04309</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04309</id><created>2015-07-15</created><updated>2016-02-10</updated><authors><author><keyname>Zakablukov</keyname><forenames>Dmitry</forenames></author></authors><title>Application of Permutation Group Theory in Reversible Logic Synthesis</title><categories>cs.ET</categories><comments>In English, 15 pages, 2 figures, 7 tables. Submission to the RC 2016
  conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper discusses various applications of the permutation group theory in
the synthesis of reversible logic circuits, consisting of Toffoli gates with
negative control lines. Asymptotically optimal synthesis algorithm for circuits
consisting of gates from NCT library is described. An algorithm for the gate
complexity reduction, based on equivalent replacements of gates compositions,
is introduced. New approach for combining a group theory based synthesis
algorithm with a Reed-Muller spectra based synthesis algorithm is described.
Experimental results are presented to show that proposed synthesis techniques
allow to reduce input lines count, gate complexity or quantum cost of
reversible circuits for various benchmark functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04310</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04310</id><created>2015-07-11</created><updated>2016-01-02</updated><authors><author><keyname>Franek</keyname><forenames>Peter</forenames></author><author><keyname>Kr&#x10d;&#xe1;l</keyname><forenames>Marek</forenames></author></authors><title>Persistence of Zero Sets</title><categories>math.AT cs.CG math.GT</categories><msc-class>57R90</msc-class><acm-class>G.1.5; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study robust properties of zero sets of continuous maps
$f:X\to\mathbb{R}^n$. Formally, we analyze the family
$Z_r(f)=\{g^{-1}(0):\,\,\|g-f\|&lt;r\}$ of all zero sets of all continuous maps
$g$ closer to $f$ than $r$ in the max-norm. The fundamental geometric property
of $Z_r(f)$ is that all its zero sets lie outside of $A:=\{x:\,|f(x)|\ge r\}$.
We claim that once the space $A$ is fixed, $Z_r(f)$ is \emph{fully} determined
by an element of a so-called cohomotopy group which---by a recent result---is
computable whenever the dimension of $X$ is at most $2n-3$. More explicitly,
the element is a homotopy class of a map from $A$ or $X/A$ into a sphere.
  By considering all $r&gt;0$ simultaneously, the pointed cohomotopy groups form a
persistence module---a structure leading to the persistence diagrams as in the
case of \emph{persistent homology} or \emph{well groups}. Eventually, we get a
descriptor of persistent robust properties of zero sets that has better
descriptive power (Theorem A) and better computability status (Theorem B) than
the established well diagrams. Moreover, if we endow every point of each zero
set with gradients of the perturbation, the robust description of the zero sets
by elements of cohomotopy groups is in some sense the best possible (Theorem
C).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04314</identifier>
 <datestamp>2015-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04314</id><created>2015-07-15</created><authors><author><keyname>Kayes</keyname><forenames>Imrul</forenames></author><author><keyname>Kourtellis</keyname><forenames>Nicolas</forenames></author><author><keyname>Quercia</keyname><forenames>Daniele</forenames></author><author><keyname>Iamnitchi</keyname><forenames>Adriana</forenames></author><author><keyname>Bonchi</keyname><forenames>Francesco</forenames></author></authors><title>The Social World of Content Abusers in Community Question Answering</title><categories>cs.SI physics.soc-ph</categories><comments>Published in the proceedings of the 24th International World Wide Web
  Conference (WWW 2015)</comments><acm-class>K.4.2</acm-class><doi>10.1145/2736277.2741674</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community-based question answering platforms can be rich sources of
information on a variety of specialized topics, from finance to cooking. The
usefulness of such platforms depends heavily on user contributions (questions
and answers), but also on respecting the community rules. As a crowd-sourced
service, such platforms rely on their users for monitoring and flagging content
that violates community rules.
  Common wisdom is to eliminate the users who receive many flags. Our analysis
of a year of traces from a mature Q&amp;A site shows that the number of flags does
not tell the full story: on one hand, users with many flags may still
contribute positively to the community. On the other hand, users who never get
flagged are found to violate community rules and get their accounts suspended.
This analysis, however, also shows that abusive users are betrayed by their
network properties: we find strong evidence of homophilous behavior and use
this finding to detect abusive users who go under the community radar. Based on
our empirical observations, we build a classifier that is able to detect
abusive users with an accuracy as high as 83%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04319</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04319</id><created>2015-07-15</created><authors><author><keyname>Mixon</keyname><forenames>Dustin G.</forenames></author><author><keyname>Peterson</keyname><forenames>Jesse</forenames></author></authors><title>Learning Boolean functions with concentrated spectra</title><categories>cs.LG cs.IT math.FA math.IT</categories><doi>10.1117/12.2189112</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses the theory and application of learning Boolean functions
that are concentrated in the Fourier domain. We first estimate the VC dimension
of this function class in order to establish a small sample complexity of
learning in this case. Next, we propose a computationally efficient method of
empirical risk minimization, and we apply this method to the MNIST database of
handwritten digits. These results demonstrate the effectiveness of our model
for modern classification tasks. We conclude with a list of open problems for
future investigation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04330</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04330</id><created>2015-07-15</created><updated>2015-07-16</updated><authors><author><keyname>Censor-Hillel</keyname><forenames>Keren</forenames></author><author><keyname>Haramaty</keyname><forenames>Elad</forenames></author><author><keyname>Karnin</keyname><forenames>Zohar</forenames></author></authors><title>Optimal Dynamic Distributed MIS</title><categories>cs.DC</categories><comments>19 pages including appendix and references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding a maximal independent set (MIS) in a graph is a cornerstone task in
distributed computing. The local nature of an MIS allows for fast solutions in
a static distributed setting, which are logarithmic in the number of nodes or
in their degrees. The result trivially applies for the dynamic distributed
model, in which edges or nodes may be inserted or deleted. In this paper, we
take a different approach which exploits locality to the extreme, and show how
to update an MIS in a dynamic distributed setting, either \emph{synchronous} or
\emph{asynchronous}, with only \emph{a single adjustment} and in a single
round, in expectation. These strong guarantees hold for the \emph{complete
fully dynamic} setting: Insertions and deletions, of edges as well as nodes,
gracefully and abruptly. This strongly separates the static and dynamic
distributed models, as super-constant lower bounds exist for computing an MIS
in the former.
  Our results are obtained by a novel analysis of the surprisingly simple
solution of carefully simulating the greedy \emph{sequential} MIS algorithm
with a random ordering of the nodes. As such, our algorithm has a direct
application as a $3$-approximation algorithm for correlation clustering. This
adds to the important toolbox of distributed graph decompositions, which are
widely used as crucial building blocks in distributed computing.
  Finally, our algorithm enjoys a useful \emph{history-independence} property,
meaning the output is independent of the history of topology changes that
constructed that graph. This means the output cannot be chosen, or even biased,
by the adversary in case its goal is to prevent us from optimizing some
objective function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04374</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04374</id><created>2015-07-15</created><updated>2015-10-01</updated><authors><author><keyname>Li</keyname><forenames>Sen</forenames></author><author><keyname>Zhang</keyname><forenames>Wei</forenames></author></authors><title>Uniform-Price Mechanism Design for a Large Population of Dynamic Agents</title><categories>cs.SY cs.GT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on the coordination of a large population of dynamic
agents with private information over multiple periods. Each agent maximizes the
individual utility, while the coordinator determines the market rule to achieve
group objectives. The coordination problem is formulated as a dynamic mechanism
design problem. A mechanism is proposed based on the competitive equilibrium of
the large population game. We derive the conditions for the general nonlinear
dynamic systems under which the proposed mechanism is incentive compatible and
can implement the social choice function in $\epsilon$-Nash equilibrium. In
addition, we show that for linear quadratic problems with bounded parameters,
the proposed mechanism can maximize the social welfare subject to a total
resource constraint in $\epsilon$-dominant strategy equilibrium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04380</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04380</id><created>2015-07-15</created><authors><author><keyname>Herzog</keyname><forenames>Alexander</forenames></author><author><keyname>Rotella</keyname><forenames>Nicholas</forenames></author><author><keyname>Schaal</keyname><forenames>Stefan</forenames></author><author><keyname>Righetti</keyname><forenames>Ludovic</forenames></author></authors><title>Trajectory generation for multi-contact momentum-control</title><categories>cs.RO</categories><doi>10.1109/HUMANOIDS.2015.7363464</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simplified models of the dynamics such as the linear inverted pendulum model
(LIPM) have proven to perform well for biped walking on flat ground. However,
for more complex tasks the assumptions of these models can become limiting. For
example, the LIPM does not allow for the control of contact forces
independently, is limited to co-planar contacts and assumes that the angular
momentum is zero. In this paper, we propose to use the full momentum equations
of a humanoid robot in a trajectory optimization framework to plan its center
of mass, linear and angular momentum trajectories. The model also allows for
planning desired contact forces for each end-effector in arbitrary contact
locations. We extend our previous results on LQR design for momentum control by
computing the (linearized) optimal momentum feedback law in a receding horizon
fashion. The resulting desired momentum and the associated feedback law are
then used in a hierarchical whole body control approach. Simulation experiments
show that the approach is computationally fast and is able to generate plans
for locomotion on complex terrains while demonstrating good tracking
performance for the full humanoid control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04383</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04383</id><created>2015-07-15</created><authors><author><keyname>Mirsoleimani</keyname><forenames>S. Ali</forenames></author><author><keyname>Plaat</keyname><forenames>Aske</forenames></author><author><keyname>Herik</keyname><forenames>Jaap van den</forenames></author><author><keyname>Vermaseren</keyname><forenames>Jos</forenames></author></authors><title>Scaling Monte Carlo Tree Search on Intel Xeon Phi</title><categories>cs.DC</categories><comments>8 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many algorithms have been parallelized successfully on the Intel Xeon Phi
coprocessor, especially those with regular, balanced, and predictable data
access patterns and instruction flows. Irregular and unbalanced algorithms are
harder to parallelize efficiently. They are, for instance, present in
artificial intelligence search algorithms such as Monte Carlo Tree Search
(MCTS). In this paper we study the scaling behavior of MCTS, on a highly
optimized real-world application, on real hardware. The Intel Xeon Phi allows
shared memory scaling studies up to 61 cores and 244 hardware threads. We
compare work-stealing (Cilk Plus and TBB) and work-sharing (FIFO scheduling)
approaches. Interestingly, we find that a straightforward thread pool with a
work-sharing FIFO queue shows the best performance. A crucial element for this
high performance is the controlling of the grain size, an approach that we call
Grain Size Controlled Parallel MCTS. Our subsequent comparing with the Xeon
CPUs shows an even more comprehensible distinction in performance between
different threading libraries. We achieve, to the best of our knowledge, the
fastest implementation of a parallel MCTS on the 61 core Intel Xeon Phi using a
real application (47 relative to a sequential run).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04391</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04391</id><created>2015-07-15</created><authors><author><keyname>Fotakis</keyname><forenames>Dimitris</forenames></author><author><keyname>Lampis</keyname><forenames>Michael</forenames></author><author><keyname>Paschos</keyname><forenames>Vangelis Th.</forenames></author></authors><title>Sub-exponential Approximation Schemes for CSPs: from Dense to Almost
  Sparse</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has long been known, since the classical work of (Arora, Karger,
Karpinski, JCSS~99), that \MC\ admits a PTAS on dense graphs, and more
generally, \kCSP\ admits a PTAS on &quot;dense&quot; instances with $\Omega(n^k)$
constraints. In this paper we extend and generalize their exhaustive sampling
approach, presenting a framework for $(1-\eps)$-approximating any \kCSP\
problem in \emph{sub-exponential} time while significantly relaxing the
denseness requirement on the input instance. Specifically, we prove that for
any constants $\delta \in (0, 1]$ and $\eps &gt; 0$, we can approximate \kCSP\
problems with $\Omega(n^{k-1+\delta})$ constraints within a factor of
$(1-\eps)$ in time $2^{O(n^{1-\delta}\ln n /\eps^3)}$. The framework is quite
general and includes classical optimization problems, such as \MC, {\sc
Max}-DICUT, \kSAT, and (with a slight extension) $k$-{\sc Densest Subgraph}, as
special cases. For \MC\ in particular (where $k=2$), it gives an approximation
scheme that runs in time sub-exponential in $n$ even for &quot;almost-sparse&quot;
instances (graphs with $n^{1+\delta}$ edges). We prove that our results are
essentially best possible, assuming the ETH. First, the density requirement
cannot be relaxed further: there exists a constant $r &lt; 1$ such that for all
$\delta &gt; 0$, \kSAT\ instances with $O(n^{k-1})$ clauses cannot be approximated
within a ratio better than $r$ in time $2^{O(n^{1-\delta})}$. Second, the
running time of our algorithm is almost tight \emph{for all densities}. Even
for \MC\ there exists $r&lt;1$ such that for all $\delta' &gt; \delta &gt;0$, \MC\
instances with $n^{1+\delta}$ edges cannot be approximated within a ratio
better than $r$ in time $2^{n^{1-\delta'}}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04394</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04394</id><created>2015-07-15</created><authors><author><keyname>Elmenreich</keyname><forenames>Wilfried</forenames></author></authors><title>Time-triggered smart transducer networks</title><categories>cs.NI</categories><journal-ref>IEEE Transactions on Industrial Informatics; 2(3):192-199; 2006</journal-ref><doi>10.1109/TII.2006.873991</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The time-triggered approach is a well-suited approach for building
distributed hard real-time systems. Since many applications of transducer
networks have real-time requirements, a time-triggered communication interface
for smart transducers is desirable, however such a time-triggered interface
must still support features for monitoring, maintenance, plug-and-play, etc.
The approach of the OMG Smart Transducer Interface consists of clusters of
time-triggered smart transducer nodes that contain special interfaces
supporting configuration, diagnostics, and maintenance without affecting the
deterministic real-time communication. This paper discusses the applicability
of the time-triggered approach for smart transducer networks and presents a
case study application of a time-triggered smart transducer network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04396</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04396</id><created>2015-07-15</created><authors><author><keyname>Kondor</keyname><forenames>Risi</forenames></author><author><keyname>Teneva</keyname><forenames>Nedelina</forenames></author><author><keyname>Mudrakarta</keyname><forenames>Pramod K.</forenames></author></authors><title>Parallel MMF: a Multiresolution Approach to Matrix Computation</title><categories>cs.NA cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiresolution Matrix Factorization (MMF) was recently introduced as a
method for finding multiscale structure and defining wavelets on
graphs/matrices. In this paper we derive pMMF, a parallel algorithm for
computing the MMF factorization. Empirically, the running time of pMMF scales
linearly in the dimension for sparse matrices. We argue that this makes pMMF a
valuable new computational primitive in its own right, and present experiments
on using pMMF for two distinct purposes: compressing matrices and
preconditioning large sparse linear systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04400</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04400</id><created>2015-07-15</created><authors><author><keyname>Morrison</keyname><forenames>Greg</forenames></author><author><keyname>Dudte</keyname><forenames>Levi</forenames></author><author><keyname>Mahadevan</keyname><forenames>L.</forenames></author></authors><title>Generalized Erdos Numbers for network analysis</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the concept of `closeness' between nodes in a
weighted network that can be defined topologically even in the absence of a
metric. The Generalized Erd\H{o}s Numbers (GENs) satisfy a number of desirable
properties as a measure of topological closeness when nodes share a finite
resource between nodes as they are real-valued and non-local, and can be used
to create an asymmetric matrix of connectivities. We show that they can be used
to define a personalized measure of the importance of nodes in a network with a
natural interpretation that leads to a new global measure of centrality and is
highly correlated with Page Rank. The relative asymmetry of the GENs (due to
their non-metric definition) is linked also to the asymmetry in the mean first
passage time between nodes in a random walk, and we use a linearized form of
the GENs to develop a continuum model for `closeness' in spatial networks. As
an example of their practicality, we deploy them to characterize the structure
of static networks and show how it relates to dynamics on networks in such
situations as the spread of an epidemic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04401</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04401</id><created>2015-07-15</created><authors><author><keyname>Rotella</keyname><forenames>Nicholas</forenames></author><author><keyname>Herzog</keyname><forenames>Alexander</forenames></author><author><keyname>Schaal</keyname><forenames>Stefan</forenames></author><author><keyname>Righetti</keyname><forenames>Ludovic</forenames></author></authors><title>Humanoid Momentum Estimation Using Sensed Contact Wrenches</title><categories>cs.RO</categories><comments>Submitted to the 15th IEEE RAS Humanoids Conference, to be held in
  Seoul, Korea on November 3 - 5, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents approaches for the estimation of quantities important for
the control of the momentum of a humanoid robot. In contrast to previous
approaches which use simplified models such as the Linear Inverted Pendulum
Model, we present estimators based on the momentum dynamics of the robot. By
using this simple yet dynamically-consistent model, we avoid the issues of
using simplified models for estimation. We develop an estimator for the center
of mass and full momentum which can be reformulated to estimate center of mass
offsets as well as external wrenches applied to the robot. The observability of
these estimators is investigated and their performance is evaluated in
comparison to previous approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04405</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04405</id><created>2015-07-15</created><authors><author><keyname>McCune</keyname><forenames>Robert Ryan</forenames></author><author><keyname>Weninger</keyname><forenames>Tim</forenames></author><author><keyname>Madey</keyname><forenames>Gregory</forenames></author></authors><title>Thinking Like a Vertex: a Survey of Vertex-Centric Frameworks for
  Distributed Graph Processing</title><categories>cs.DC</categories><acm-class>D.1.3; D.2.11; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The vertex-centric programming model is an established computational paradigm
recently incorporated into distributed processing frameworks to address
challenges in large-scale graph processing. Billion-node graphs that exceed the
memory capacity of standard machines are not well-supported by popular Big Data
tools like MapReduce, which are notoriously poor-performing for iterative graph
algorithms such as PageRank. In response, a new type of framework challenges
one to Think Like A Vertex (TLAV) and implements user-defined programs from the
perspective of a vertex rather than a graph. Such an approach improves
locality, demonstrates linear scalability, and provides a natural way to
express and compute many iterative graph algorithms. These frameworks are
simple to program and widely applicable, but, like an operating system, are
composed of several intricate, interdependent components, of which a thorough
understanding is necessary in order to elicit top performance at scale. To this
end, the first comprehensive survey of TLAV frameworks is presented. In this
survey, the vertex-centric approach to graph processing is overviewed, TLAV
frameworks are deconstructed into four main components and respectively
analyzed, and TLAV implementations are reviewed and categorized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04410</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04410</id><created>2015-07-15</created><authors><author><keyname>Singh</keyname><forenames>P.</forenames></author><author><keyname>Sreenivasan</keyname><forenames>S.</forenames></author><author><keyname>Szymanski</keyname><forenames>B. K.</forenames></author><author><keyname>Korniss</keyname><forenames>G.</forenames></author></authors><title>Competing Effects of Social Balance and Influence</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a three-state (leftist, rightist, centrist) model that couples the
dynamics of social balance with an external deradicalizing field. The
mean-field analysis shows that there exists a critical value of the external
field $p_c$ such that for a weak external field ($p$$&lt;$$p_c$), the system
exhibits a metastable fixed point and a saddle point in addition to a stable
fixed point. However, if the strength of the external field is sufficiently
large ($p$$&gt;$$p_c$), there is only one (stable) fixed point which corresponds
to an all-centrist consensus state (absorbing state). In the weak-field regime,
the convergence time to the absorbing state is evaluated using the
quasi-stationary distribution and is found to be in agreement with the results
obtained by numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04412</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04412</id><created>2015-07-15</created><authors><author><keyname>Liu</keyname><forenames>Xiwei</forenames></author></authors><title>Bridge the gap between network-based inference method and global ranking
  method in personal recommendation</title><categories>cs.SI cs.IR</categories><comments>13 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the relationship between the network-based inference
method and global ranking method in personal recommendation. By some
theoretical analysis, we prove that the recommendation result under the global
ranking method is the limit of applying network-based inference method with
infinity times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04417</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04417</id><created>2015-07-15</created><authors><author><keyname>Lamichhane</keyname><forenames>Bishnu P.</forenames></author></authors><title>A quadrilateral 'mini' finite element for the Stokes problem using a
  single bubble function</title><categories>cs.NA math.NA</categories><comments>11 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a quadrilateral 'mini' finite element for approximating the
solution of Stokes equations using a quadrilateral mesh. We use the standard
bilinear finite element space enriched with element-wise defined bubble
functions for the velocity and the standard bilinear finite element space for
the pressure space. With a simple modification of the standard bubble function
we show that a single bubble function is sufficient to ensure the inf-sup
condition. We have thus improved an earlier result on the quadrilateral 'mini'
element, where more than one bubble function are used to get the stability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04420</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04420</id><created>2015-07-15</created><authors><author><keyname>Kirby</keyname><forenames>James</forenames></author><author><keyname>Sonderegger</keyname><forenames>Morgan</forenames></author></authors><title>Bias and population structure in the actuation of sound change</title><categories>cs.CL physics.soc-ph</categories><comments>30 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Why do human languages change at some times, and not others? We address this
longstanding question from a computational perspective, focusing on the case of
sound change. Sound change arises from the pronunciation variability ubiquitous
in every speech community, but most such variability does not lead to change.
Hence, an adequate model must allow for stability as well as change. Existing
theories of sound change tend to emphasize factors at the level of individual
learners promoting one outcome or the other, such as channel bias (which favors
change) or inductive bias (which favors stability). Here, we consider how the
interaction of these biases can lead to both stability and change in a
population setting. We find that population structure itself can act as a
source of stability, but that both stability and change are possible only when
both types of bias are active, suggesting that it is possible to understand why
sound change occurs at some times and not others as the population-level result
of the interplay between forces promoting each outcome in individual speakers.
In addition, if it is assumed that learners learn from two or more teachers,
the transition from stability to change is marked by a phase transition,
consistent with the abrupt transitions seen in many empirical cases of sound
change. The predictions of multiple-teacher models thus match empirical cases
of sound change better than the predictions of single-teacher models,
underscoring the importance of modeling language change in a population
setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04437</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04437</id><created>2015-07-15</created><authors><author><keyname>Zhong</keyname><forenames>Guoqiang</forenames></author><author><keyname>Yang</keyname><forenames>Pan</forenames></author><author><keyname>Wang</keyname><forenames>Sijiang</forenames></author><author><keyname>Dong</keyname><forenames>Junyu</forenames></author></authors><title>A Deep Hashing Learning Network</title><categories>cs.CV</categories><comments>7 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hashing-based methods seek compact and efficient binary codes that preserve
the neighborhood structure in the original data space. For most existing
hashing methods, an image is first encoded as a vector of hand-crafted visual
feature, followed by a hash projection and quantization step to get the compact
binary vector. Most of the hand-crafted features just encode the low-level
information of the input, the feature may not preserve the semantic
similarities of images pairs. Meanwhile, the hashing function learning process
is independent with the feature representation, so the feature may not be
optimal for the hashing projection. In this paper, we propose a supervised
hashing method based on a well designed deep convolutional neural network,
which tries to learn hashing code and compact representations of data
simultaneously. The proposed model learn the binary codes by adding a compact
sigmoid layer before the loss layer. Experiments on several image data sets
show that the proposed model outperforms other state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04438</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04438</id><created>2015-07-15</created><authors><author><keyname>Bhattacharya</keyname><forenames>Binay</forenames></author><author><keyname>&#x106;usti&#x107;</keyname><forenames>Ante</forenames></author><author><keyname>Rafiey</keyname><forenames>Akbar</forenames></author><author><keyname>Rafiey</keyname><forenames>Arash</forenames></author><author><keyname>Sokol</keyname><forenames>Vladyslav</forenames></author></authors><title>Approximation Algorithms for Generalized MST and TSP in Grid Clusters</title><categories>cs.DM cs.CG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a special case of the generalized minimum spanning tree problem
(GMST) and the generalized travelling salesman problem (GTSP) where we are
given a set of points inside the integer grid (in Euclidean plane) where each
grid cell is $1 \times 1$. In the MST version of the problem, the goal is to
find a minimum tree that contains exactly one point from each non-empty grid
cell (cluster). Similarly, in the TSP version of the problem, the goal is to
find a minimum weight cycle containing one point from each non-empty grid cell.
We give a $(1+4\sqrt{2}+\epsilon)$ and $(1.5+8\sqrt{2}+\epsilon)$-approximation
algorithm for these two problems in the described setting, respectively.
  Our motivation is based on the problem posed in [7] for a constant
approximation algorithm. The authors designed a PTAS for the more special case
of the GMST where non-empty cells are connected end dense enough. However,
their algorithm heavily relies on this connectivity restriction and is
unpractical. Our results develop the topic further.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04441</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04441</id><created>2015-07-15</created><authors><author><keyname>Ma</keyname><forenames>Keng-Teck</forenames></author><author><keyname>Xu</keyname><forenames>Qianli</forenames></author><author><keyname>Li</keyname><forenames>Liyuan</forenames></author><author><keyname>Sim</keyname><forenames>Terence</forenames></author><author><keyname>Kankanhalli</keyname><forenames>Mohan</forenames></author><author><keyname>Lim</keyname><forenames>Rosary</forenames></author></authors><title>Eye-2-I: Eye-tracking for just-in-time implicit user profiling</title><categories>cs.HC</categories><comments>10 pages, 7 Figures</comments><acm-class>H.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For many applications, such as targeted advertising and content
recommendation, knowing users' traits and interests is a prerequisite. User
profiling is a helpful approach for this purpose. However, current methods,
i.e. self-reporting, web-activity monitoring and social media mining are either
intrusive or require data over long periods of time. Recently, there is growing
evidence in cognitive science that a variety of users' profile is significantly
correlated with eye-tracking data. We propose a novel just-in-time implicit
profiling method, Eye-2-I, which learns the user's interests, demographic and
personality traits from the eye-tracking data while the user is watching
videos. Although seemingly conspicuous by closely monitoring the user's eye
behaviors, our method is unobtrusive and privacy-preserving owing to its unique
characteristics, including (1) fast speed - the profile is available by the
first video shot, typically few seconds, and (2) self-contained - not relying
on historical data or functional modules. As a proof-of-concept, our method is
evaluated in a user study with 51 subjects. It achieved a mean accuracy of 0.89
on 37 attributes of user profile with 9 minutes of eye-tracking data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04443</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04443</id><created>2015-07-15</created><authors><author><keyname>Gao</keyname><forenames>Xinyu</forenames></author><author><keyname>Dai</keyname><forenames>Linglong</forenames></author><author><keyname>Ma</keyname><forenames>Yongkui</forenames></author><author><keyname>Wang</keyname><forenames>Zhaocheng</forenames></author></authors><title>Low-complexity near-optimal signal detection for uplink large-scale MIMO
  systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Minimum mean square error (MMSE) signal detection algorithm is near- optimal
for uplink multi-user large-scale multiple input multiple output (MIMO)
systems, but involves matrix inversion with high complexity. In this letter, we
firstly prove that the MMSE filtering matrix for large- scale MIMO is symmetric
positive definite, based on which we propose a low-complexity near-optimal
signal detection algorithm by exploiting the Richardson method to avoid the
matrix inversion. The complexity can be reduced from O(K3) to O(K2), where K is
the number of users. We also provide the convergence proof of the proposed
algorithm. Simulation results show that the proposed signal detection algorithm
converges fast, and achieves the near-optimal performance of the classical MMSE
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04450</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04450</id><created>2015-07-16</created><authors><author><keyname>Alaka</keyname><forenames>S. P.</forenames></author><author><keyname>Narasimhan</keyname><forenames>T. Lakshmi</forenames></author><author><keyname>Chockalingam</keyname><forenames>A.</forenames></author></authors><title>Coded Index Modulation for Non-DC-Biased OFDM in Multiple LED Visible
  Light Communication</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Use of multiple light emitting diodes (LED) is an attractive way to increase
spectral efficiency in visible light communications (VLC). A non-DC-biased OFDM
(NDC OFDM) scheme that uses two LEDs has been proposed in the literature
recently. NDC OFDM has been shown to perform better than other OFDM schemes for
VLC like DC-biased OFDM (DCO OFDM) and asymmetrically clipped OFDM (ACO OFDM)
in multiple LEDs settings. In this paper, we propose an efficient multiple LED
OFDM scheme for VLC which uses {\em coded index modulation}. The proposed
scheme uses two transmitter blocks, each having a pair of LEDs. Within each
block, NDC OFDM signaling is done. The selection of which block is activated in
a signaling interval is decided by information bits (i.e., index bits). In
order to improve the reliability of the index bits at the receiver (which is
critical because of high channel correlation in multiple LEDs settings), we
propose to use coding on the index bits alone. We call the proposed scheme as
CI-NDC OFDM (coded index NDC OFDM) scheme. Simulation results show that, for
the same spectral efficiency, CI-NDC OFDM that uses LDPC coding on the index
bits performs better than NDC OFDM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04452</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04452</id><created>2015-07-16</created><updated>2016-02-10</updated><authors><author><keyname>Choi</keyname><forenames>Junil</forenames></author><author><keyname>Mo</keyname><forenames>Jianhua</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>Near Maximum-Likelihood Detector and Channel Estimator for Uplink
  Multiuser Massive MIMO Systems with One-Bit ADCs</title><categories>cs.IT math.IT</categories><comments>13 pages, 8 figures, 2 tables, submitted to IEEE Transactions on
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In massive multiple-input multiple-output (MIMO) systems, it may not be power
efficient to have a high-resolution analog-to-digital converter (ADC) for each
antenna element. In this paper, a near maximum likelihood (nML) detector for
uplink multiuser massive MIMO systems is proposed where each antenna is
connected to a pair of one-bit ADCs, i.e., one for each real and imaginary
component of the baseband signal. The exhaustive search over all the possible
transmitted vectors required in the original maximum likelihood (ML) detection
problem is relaxed to formulate an ML estimation problem. Then, the ML
estimation problem is converted into a convex optimization problem which can be
efficiently solved. Using the solution, the base station can perform simple
symbol-by-symbol detection for the transmitted signals from multiple users. To
further improve detection performance, we also develop a two-stage nML detector
that exploits the structures of both the original ML and the proposed
(one-stage) nML detectors. Numerical results show that the proposed nML
detectors are efficient enough to simultaneously support multiple uplink users
adopting higher-order constellations, e.g., 16 quadrature amplitude modulation.
Since our detectors exploit the channel state information as part of the
detection, an ML channel estimation technique with one-bit ADCs that shares the
same structure with our proposed nML detector is also developed. The proposed
detectors and channel estimator provide a complete low power solution for the
uplink of a massive MIMO system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04457</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04457</id><created>2015-07-16</created><authors><author><keyname>Park</keyname><forenames>Dohyung</forenames></author><author><keyname>Neeman</keyname><forenames>Joe</forenames></author><author><keyname>Zhang</keyname><forenames>Jin</forenames></author><author><keyname>Sanghavi</keyname><forenames>Sujay</forenames></author><author><keyname>Dhillon</keyname><forenames>Inderjit S.</forenames></author></authors><title>Preference Completion: Large-scale Collaborative Ranking from Pairwise
  Comparisons</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the collaborative ranking setting: a pool of users
each provides a small number of pairwise preferences between $d$ possible
items; from these we need to predict preferences of the users for items they
have not yet seen. We do so by fitting a rank $r$ score matrix to the pairwise
data, and provide two main contributions: (a) we show that an algorithm based
on convex optimization provides good generalization guarantees once each user
provides as few as $O(r\log^2 d)$ pairwise comparisons -- essentially matching
the sample complexity required in the related matrix completion setting (which
uses actual numerical as opposed to pairwise information), and (b) we develop a
large-scale non-convex implementation, which we call AltSVM, that trains a
factored form of the matrix via alternating minimization (which we show reduces
to alternating SVM problems), and scales and parallelizes very well to large
problem settings. It also outperforms common baselines on many moderately large
popular collaborative filtering datasets in both NDCG and in other measures of
ranking performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04461</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04461</id><created>2015-07-16</created><authors><author><keyname>Afrati</keyname><forenames>Foto</forenames></author><author><keyname>Dolev</keyname><forenames>Shlomi</forenames></author><author><keyname>Korach</keyname><forenames>Ephraim</forenames></author><author><keyname>Sharma</keyname><forenames>Shantanu</forenames></author><author><keyname>Ullman</keyname><forenames>Jeffrey D.</forenames></author></authors><title>Assignment Problems of Different-Sized Inputs in MapReduce</title><categories>cs.DB cs.CC cs.DC</categories><comments>Preliminary versions of this paper have appeared in the proceedings
  of DISC 2014 and BeyondMR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A MapReduce algorithm can be described by a mapping schema, which assigns
inputs to a set of reducers, such that for each required output there exists a
reducer that receives all the inputs that participate in the computation of
this output. Reducers have a capacity, which limits the sets of inputs that
they can be assigned. However, individual inputs may vary in terms of size. We
consider, for the first time, mapping schemas where input sizes are part of the
considerations and restrictions. One of the significant parameters to optimize
in any MapReduce job is communication cost between the map and reduce phases.
The communication cost can be optimized by minimizing the number of copies of
inputs sent to the reducers. The communication cost is closely related to the
number of reducers of constrained capacity that are used to accommodate
appropriately the inputs, so that the requirement of how the inputs must meet
in a reducer is satisfied. In this work, we consider a family of problems where
it is required that each input meets with each other input in at least one
reducer. We also consider a slightly different family of problems in which,
each input of a set, $X$, is required to meet each input of another set, $Y$,
in at least one reducer. We prove that finding an optimal mapping schema for
these families of problems is NP-hard, and present a bin-packing-based
approximation algorithm for finding a near optimal mapping schema.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04462</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04462</id><created>2015-07-16</created><authors><author><keyname>Sharma</keyname><forenames>Shantanu</forenames></author><author><keyname>Singh</keyname><forenames>Awadhesh Kumar</forenames></author></authors><title>On Detecting Termination in Cognitive Radio Networks</title><categories>cs.NI cs.DC</categories><comments>Accepted in Wiley International Journal of Network Management
  (Wiley-IJNM) 2014, 12 figures</comments><journal-ref>International Journal of Network Management 24.6 (2014): 499-527</journal-ref><doi>10.1002/nem.1870</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The cognitive radio networks are an emerging wireless communication and
computing paradigm. The cognitive radio nodes execute computations on multiple
heterogeneous channels in the absence of licensed users (a.k.a. primary users)
of those bands. Termination detection is a fundamental and non-trivial problem
in distributed systems. In this paper, we propose a termination detection
protocol for multi-hop cognitive radio networks where the cognitive radio nodes
are allowed to tune to channels that are not currently occupied by primary
users and to move to different locations during the protocol execution. The
proposed protocol applies credit distribution and aggregation approach and
maintains a new kind of logical structure, called the virtual tree-like
structure. The virtual tree-like structure helps in decreasing the latency
involved in announcing termination. Unlike conventional tree structures, the
virtual tree-like structure does not require a specific node to act as the root
node that has to stay involved in the computation until termination
announcement; hence, the root node may become idle soon after finishing its
computation. Also, the protocol is able to detect the presence of licensed
users and announce strong or weak termination, whichever is possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04464</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04464</id><created>2015-07-16</created><authors><author><keyname>Shi</keyname><forenames>Sulong</forenames></author><author><keyname>Yang</keyname><forenames>Longxiang</forenames></author><author><keyname>Zhu</keyname><forenames>Hongbo</forenames></author></authors><title>Outage Balancing in Downlink Non-Orthogonal Multiple Access With
  Statistical Channel State Information</title><categories>cs.IT math.IT</categories><comments>13pages, 7figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a downlink non-orthogonal multiple access (NOMA) system
where the source intends to transmit independent information to the users at
targeted data rates under statistical channel state information at the
transmitter. The problem of outage balancing among the users is studied with
the issues of power allocation, decoding order selection, and user grouping
being taken into account. Specifically, with regard to the max-min fairness
criterion, we derive the optimal power allocation in closed-form and prove the
corresponding optimal decoding order for the elementary downlink NOMA system.
By assigning a weighting factor for each user, the analytical results can be
used to evaluate the outage performance of the downlink NOMA system under
various fairness constraints. Further, we investigate the case with user
grouping, in which each user group can be treated as an elementary downlink
NOMA system. The associated problems of power and resource allocation among
different user groups are solved. The implementation complexity issue of NOMA
is also considered with focus on that caused by successive interference
cancellation and user grouping. The complexity and performance tradeoff is
analyzed by simulations, which provides fruitful insights for the practical
application of NOMA. The simulation results substantiate our analysis and show
considerable performance gain of NOMA when compared with orthogonal multiple
access.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04489</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04489</id><created>2015-07-16</created><updated>2015-08-04</updated><authors><author><keyname>Geigl</keyname><forenames>Florian</forenames></author><author><keyname>Lamprecht</keyname><forenames>Daniel</forenames></author><author><keyname>Hofmann-Wellenhof</keyname><forenames>Rainer</forenames></author><author><keyname>Walk</keyname><forenames>Simon</forenames></author><author><keyname>Strohmaier</keyname><forenames>Markus</forenames></author><author><keyname>Helic</keyname><forenames>Denis</forenames></author></authors><title>Random Surfers on a Web Encyclopedia</title><categories>cs.SI physics.soc-ph</categories><comments>8 pages</comments><doi>10.1145/2809563.2809598</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The random surfer model is a frequently used model for simulating user
navigation behavior on the Web. Various algorithms, such as PageRank, are based
on the assumption that the model represents a good approximation of users
browsing a website. However, the way users browse the Web has been drastically
altered over the last decade due to the rise of search engines. Hence, new
adaptations for the established random surfer model might be required, which
better capture and simulate this change in navigation behavior. In this article
we compare the classical uniform random surfer to empirical navigation and page
access data in a Web Encyclopedia. Our high level contributions are (i) a
comparison of stationary distributions of different types of the random surfer
to quantify the similarities and differences between those models as well as
(ii) new insights into the impact of search engines on traditional user
navigation. Our results suggest that the behavior of the random surfer is
almost similar to those of users - as long as users do not use search engines.
We also find that classical website navigation structures, such as navigation
hierarchies or breadcrumbs, only exercise limited influence on user navigation
anymore. Rather, a new kind of navigational tools (e.g., recommendation
systems) might be needed to better reflect the changes in browsing behavior of
existing users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04491</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04491</id><created>2015-07-16</created><authors><author><keyname>Dolev</keyname><forenames>Shlomi</forenames></author><author><keyname>Krzywiecki</keyname><forenames>&#x141;ukasz</forenames></author><author><keyname>Panwar</keyname><forenames>Nisha</forenames></author><author><keyname>Segal</keyname><forenames>Michael</forenames></author></authors><title>Vehicle Authentication via Monolithically Certified Public Key and
  Attributes</title><categories>cs.CR cs.NI</categories><comments>Accepted in Wireless Networks June 2015, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vehicular networks are used to coordinate actions among vehicles in traffic
by the use of wireless transceivers (pairs of transmitters and receivers).
Unfortunately, the wireless communication among vehicles is vulnerable to
security threats that may lead to very serious safety hazards. In this work, we
propose a viable solution for coping with Man-in-the-Middle attacks.
Conventionally, Public Key Infrastructure (PKI) is utilized for a secure
communication with the pre-certified public key. However, a secure
vehicle-to-vehicle communication requires additional means of verification in
order to avoid impersonation attacks. To the best of our knowledge, this is the
first work that proposes to certify both the public key and out-of-band
sense-able static attributes to enable mutual authentication of the
communicating vehicles. Vehicle owners are bound to preprocess (periodically) a
certificate for both a public key and a list of fixed unchangeable attributes
of the vehicle. Furthermore, the proposed approach is shown to be adaptable
with regards to the existing authentication protocols. We illustrate the
security verification of the proposed protocol using a detailed proof in Spi
calculus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04492</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04492</id><created>2015-07-16</created><authors><author><keyname>Loke</keyname><forenames>Seng W.</forenames></author></authors><title>The Internet of Flying-Things: Opportunities and Challenges with
  Airborne Fog Computing and Mobile Cloud in the Clouds</title><categories>cs.CY</categories><comments>5 pages, submitted to a journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on services and applications provided to mobile users
using airborne computing infrastructure. We present concepts such as
drones-as-a-service and fly-in,fly-out infrastructure, and note data management
and system design issues that arise in these scenarios. Issues of Big Data
arising from such applications, optimising the configuration of airborne and
ground infrastructure to provide the best QoS and QoE, situation-awareness,
scalability, reliability, scheduling for efficiency, interaction with users and
drones using physical annotations are outlined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04497</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04497</id><created>2015-07-16</created><updated>2016-01-15</updated><authors><author><keyname>Biason</keyname><forenames>Alessandro</forenames></author><author><keyname>Zorzi</keyname><forenames>Michele</forenames></author></authors><title>Joint Transmission and Energy Transfer Policies for Energy Harvesting
  Devices with Finite Batteries</title><categories>cs.IT math.IT</categories><comments>16 pages, 12 figures</comments><journal-ref>IEEE Journal on Selected Areas in Communications, vol. 33, no. 12,
  pp. 2626-2640, Dec. 2015</journal-ref><doi>10.1109/JSAC.2015.2481214</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the main concerns in traditional Wireless Sensor Networks (WSNs) is
energy efficiency. In this work, we analyze two techniques that can extend
network lifetime. The first is Ambient \emph{Energy Harvesting} (EH), i.e., the
capability of the devices to gather energy from the environment, whereas the
second is Wireless \emph{Energy Transfer} (ET), that can be used to exchange
energy among devices. We study the combination of these techniques, showing
that they can be used jointly to improve the system performance. We consider a
transmitter-receiver pair, showing how the ET improvement depends upon the
statistics of the energy arrivals and the energy consumption of the devices.
With the aim of maximizing a reward function, e.g., the average transmission
rate, we find performance upper bounds with and without ET, define both online
and offline optimization problems, and present results based on realistic
energy arrivals in indoor and outdoor environments. We show that ET can
significantly improve the system performance even when a sizable fraction of
the transmitted energy is wasted and that, in some scenarios, the online
approach can obtain close to optimal performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04499</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04499</id><created>2015-07-16</created><authors><author><keyname>Ald&#xe0;</keyname><forenames>Francesco</forenames></author></authors><title>Efficient Private Query Release via Polynomial Approximation</title><categories>cs.DS cs.CR</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of privately answering queries on databases
consisting of points in $[0,1]^{\ell}$. We prove the following results. First,
we show that there exists a computationally efficient
$\varepsilon$-differentially private mechanism that releases a query class
parametrized by additively separable H\&quot;older continuous functions. Second, we
show that, if the query class is instead parametrized by additively separable
analytic functions, the accuracy can be significantly boosted. Moreover, both
our mechanisms operate in the non-interactive setting, i.e. they output a fixed
data structure which can be used to answer all the queries of interest without
further accessing the sensitive database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04500</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04500</id><created>2015-07-16</created><authors><author><keyname>Fearnley</keyname><forenames>John</forenames></author><author><keyname>Savani</keyname><forenames>Rahul</forenames></author></authors><title>The Complexity of All-switches Strategy Improvement</title><categories>cs.DS cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Strategy improvement is a widely-used and well-studied class of algorithms
for solving graph-based infinite games. These algorithms are parametrized by a
switching rule, and one of the most natural rules is &quot;all switches&quot; which
switches as many edges as possible in each iteration. Continuing a recent line
of work, we study all-switches strategy improvement from the perspective of
computational complexity. We consider two natural decision problems, both of
which have as input a game G, a starting strategy s, and an edge e. The
problems are: 1. The edge switch problem, namely, is the edge e, ever switched
by all-switches strategy improvement when it is started from s on game G? 2.
The optimal strategy problem, namely, is the edge e used in the final strategy
that is found by strategy improvement when it is started from s on game G? We
show PSPACE-completeness of the edge switch problem and optimal strategy
problem for the following settings: Parity games with the discrete strategy
improvement algorithm of Voge and Jurdzinski; mean-payoff games with the
gain-bias algorithm; and discounted-payoff games and simple stochastic games
with their standard strategy improvement algorithms. We also show
PSPACE-completeness of an analogous problem to edge switch for the
bottom-antipodal algorithm for Acyclic Unique Sink Orientations on Cubes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04502</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04502</id><created>2015-07-16</created><authors><author><keyname>Kirk</keyname><forenames>Nicholas H.</forenames></author><author><keyname>Dianov</keyname><forenames>Ilya</forenames></author></authors><title>Towards Predicting First Daily Departure Times: a Gaussian Modeling
  Approach for Load Shift Forecasting</title><categories>cs.LG</categories><comments>2015 IEEE International Conference on Systems, Man and Cybernetics
  [accepted]</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work provides two statistical Gaussian forecasting methods for
predicting First Daily Departure Times (FDDTs) of everyday use electric
vehicles. This is important in smart grid applications to understand
disconnection times of such mobile storage units, for instance to forecast
storage of non dispatchable loads (e.g. wind and solar power). We provide a
review of the relevant state-of-the-art driving behavior features towards FDDT
prediction, to then propose an approximated Gaussian method which qualitatively
forecasts how many vehicles will depart within a given time frame, by assuming
that departure times follow a normal distribution. This method considers
sampling sessions as Poisson distributions which are superimposed to obtain a
single approximated Gaussian model. Given the Gaussian distribution assumption
of the departure times, we also model the problem with Gaussian Mixture Models
(GMM), in which the priorly set number of clusters represents the desired time
granularity. Evaluation has proven that for the dataset tested, low error and
high confidence ($\approx 95\%$) is possible for 15 and 10 minute intervals,
and that GMM outperforms traditional modeling but is less generalizable across
datasets, as it is a closer fit to the sampling data. Conclusively we discuss
future possibilities and practical applications of the discussed model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04507</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04507</id><created>2015-07-16</created><authors><author><keyname>Luo</keyname><forenames>Jianxi</forenames></author><author><keyname>Whitney</keyname><forenames>Daniel E.</forenames></author></authors><title>Asymmetry in in-degree and out-degree distributions of large-scale
  industrial networks</title><categories>cs.SI physics.soc-ph</categories><comments>in Structure and Dynamics, 8, 2015</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Many natural, physical and social networks commonly exhibit power-law degree
distributions. In this paper, we discover previously unreported asymmetrical
patterns in the degree distributions of incoming and outgoing links in the
investigation of large-scale industrial networks, and provide interpretations.
In industrial networks, nodes are firms and links are directed
supplier-customer relationships. While both in- and out-degree distributions
have &quot;power law&quot; regimes, out-degree distribution decays faster than in-degree
distribution and crosses it at a consistent nodal degree. It implies that, as
link degree increases, the constraints to the capacity for designing, producing
and transmitting artifacts out to others grow faster than and surpasses those
for acquiring, absorbing and synthesizing artifacts provided from others. We
further discover that this asymmetry in decaying rates of in-degree and
out-degree distributions is smaller in networks that process and transmit more
decomposable artifacts, e.g. informational artifacts in contrast with physical
artifacts. This asymmetry in in-degree and out-degree distributions is likely
to hold for other directed networks, but to different degrees, depending on the
decomposability of the processed and transmitted artifacts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04512</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04512</id><created>2015-07-16</created><authors><author><keyname>Zhu</keyname><forenames>Hongyuan</forenames></author><author><keyname>Lu</keyname><forenames>Shijian</forenames></author><author><keyname>Cai</keyname><forenames>Jianfei</forenames></author><author><keyname>Lee</keyname><forenames>Quangqing</forenames></author></authors><title>Diagnosing State-Of-The-Art Object Proposal Methods</title><categories>cs.CV</categories><comments>Accepted to BMVC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Object proposal has become a popular paradigm to replace exhaustive sliding
window search in current top-performing methods in PASCAL VOC and ImageNet.
Recently, Hosang et al. conduct the first unified study of existing methods' in
terms of various image-level degradations. On the other hand, the vital
question &quot;what object-level characteristics really affect existing methods'
performance?&quot; is not yet answered. Inspired by Hoiem et al.'s work in
categorical object detection, this paper conducts the first meta-analysis of
various object-level characteristics' impact on state-of-the-art object
proposal methods. Specifically, we examine the effects of object size, aspect
ratio, iconic view, color contrast, shape regularity and texture. We also
analyse existing methods' localization accuracy and latency for various PASCAL
VOC object classes. Our study reveals the limitations of existing methods in
terms of non-iconic view, small object size, low color contrast, shape
regularity etc. Based on our observations, lessons are also learned and shared
with respect to the selection of existing object proposal technologies as well
as the design of the future ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04518</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04518</id><created>2015-07-16</created><authors><author><keyname>Sakaguchi</keyname><forenames>Kei</forenames></author><author><keyname>Mohamed</keyname><forenames>Ehab Mahmoud</forenames></author><author><keyname>Kusano</keyname><forenames>Hideyuki</forenames></author><author><keyname>Mizukami</keyname><forenames>Makoto</forenames></author><author><keyname>Miyamoto</keyname><forenames>Shinichi</forenames></author><author><keyname>Rezagah</keyname><forenames>Roya</forenames></author><author><keyname>Takinami</keyname><forenames>Koji</forenames></author><author><keyname>Takahashi</keyname><forenames>Kazuaki</forenames></author><author><keyname>Shirakata</keyname><forenames>Naganori</forenames></author><author><keyname>Peng</keyname><forenames>Hailan</forenames></author><author><keyname>Yamamoto</keyname><forenames>Toshiaki</forenames></author><author><keyname>Namba</keyname><forenames>Shinobu</forenames></author></authors><title>Millimeter-wave Wireless LAN and its Extension toward 5G Heterogeneous
  Networks</title><categories>cs.NI</categories><comments>18 pages, 24 figures, accepted, invited paper,</comments><journal-ref>IEICE Transactions on Communications, Vol.E98-B, No.10, Oct. 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Millimeter-wave (mmw) frequency bands, especially 60 GHz unlicensed band, are
considered as a promising solution for gigabit short range wireless
communication systems. IEEE standard 802.11ad, also known as WiGig, is
standardized for the usage of the 60 GHz unlicensed band for wireless local
area networks (WLANs). By using this mmw WLAN, multi-Gbps rate can be achieved
to support bandwidth-intensive multimedia applications. Exhaustive search along
with beamforming (BF) is usually used to overcome 60 GHz channel propagation
loss and accomplish data transmissions in such mmw WLANs. Because of its short
range transmission with a high susceptibility to path blocking, multiple number
of mmw access points (APs) should be used to fully cover a typical target
environment for future high capacity multi-Gbps WLANs. Therefore, coordination
among mmw APs is highly needed to overcome packet collisions resulting from
un-coordinated exhaustive search BF and to increase the total capacity of mmw
WLANs. In this paper, we firstly give the current status of mmw WLANs with our
developed WiGig AP prototype. Then, we highlight the great need for coordinated
transmissions among mmw APs as a key enabler for future high capacity mmw
WLANs. Two different types of coordinated mmw WLAN architecture are introduced.
One is the distributed antenna type architecture to realize centralized
coordination, while the other is an autonomous coordination with the assistance
of legacy Wi-Fi signaling. Moreover, two heterogeneous network (HetNet)
architectures are also introduced to efficiently extend the coordinated mmw
WLANs to be used for future 5th Generation (5G) cellular networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04523</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04523</id><created>2015-07-16</created><authors><author><keyname>Carpentier</keyname><forenames>Alexandra</forenames></author><author><keyname>Lazaric</keyname><forenames>Alessandro</forenames></author><author><keyname>Ghavamzadeh</keyname><forenames>Mohammad</forenames></author><author><keyname>Munos</keyname><forenames>R&#xe9;mi</forenames></author><author><keyname>Auer</keyname><forenames>Peter</forenames></author><author><keyname>Antos</keyname><forenames>Andr&#xe1;s</forenames></author></authors><title>Upper-Confidence-Bound Algorithms for Active Learning in Multi-Armed
  Bandits</title><categories>cs.LG</categories><comments>30 pages, 2 Postscript figures, uses elsarticle.cls, earlier, shorter
  version published in Proceedings of the 22nd International Conference,
  Algorithmic Learning Theory</comments><acm-class>G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the problem of estimating uniformly well the mean
values of several distributions given a finite budget of samples. If the
variance of the distributions were known, one could design an optimal sampling
strategy by collecting a number of independent samples per distribution that is
proportional to their variance. However, in the more realistic case where the
distributions are not known in advance, one needs to design adaptive sampling
strategies in order to select which distribution to sample from according to
the previously observed samples. We describe two strategies based on pulling
the distributions a number of times that is proportional to a high-probability
upper-confidence-bound on their variance (built from previous observed samples)
and report a finite-sample performance analysis on the excess estimation error
compared to the optimal allocation. We show that the performance of these
allocation strategies depends not only on the variances but also on the full
shape of the distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04537</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04537</id><created>2015-07-16</created><authors><author><keyname>Schwentick</keyname><forenames>Thomas</forenames></author><author><keyname>Vortmeier</keyname><forenames>Nils</forenames></author><author><keyname>Zeume</keyname><forenames>Thomas</forenames></author></authors><title>Static Analysis for Logic-Based Dynamic Programs</title><categories>cs.LO</categories><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A dynamic program, as introduced by Patnaik and Immerman (1994), maintains
the result of a fixed query for an input database which is subject to tuple
insertions and deletions. It can use an auxiliary database whose relations are
updated via first-order formulas upon modifications of the input database. This
paper studies static analysis problems for dynamic programs and investigates,
more specifically, the decidability of the following three questions. Is the
answer relation of a given dynamic program always empty? Does a program
actually maintain a query? Is the content of auxiliary relations independent of
the modification sequence that lead to an input database? In general, all these
problems can easily be seen to be undecidable for full first-order programs.
Therefore the paper aims at pinpointing the exact decidability borderline for
programs with restricted arity (of the input and/or auxiliary database) and
restricted quantification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04540</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04540</id><created>2015-07-16</created><updated>2016-02-22</updated><authors><author><keyname>Xie</keyname><forenames>Tianpei</forenames></author><author><keyname>Nasrabadi</keyname><forenames>Nasser M.</forenames></author><author><keyname>Hero</keyname><forenames>Alfred O.</forenames></author></authors><title>Learning to classify with possible sensor failures</title><categories>cs.LG cs.IT math.IT stat.ML</categories><comments>13 pages, submitted to IEEE Transaction of Signal Processing, Feb
  2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a general framework to learn a robust large-margin
binary classifier when corrupt measurements, called anomalies, caused by sensor
failure might be present in the training set. The goal is to minimize the
generalization error of the classifier on non-corrupted measurements while
controlling the false alarm rate associated with anomalous samples. By
incorporating a non-parametric regularizer based on an empirical entropy
estimator, we propose a Geometric-Entropy-Minimization regularized Maximum
Entropy Discrimination (GEM-MED) method to learn to classify and detect
anomalies in a joint manner. We demonstrate using simulated data and a real
multimodal data set. Our GEM-MED method can yield improved performance over
previous robust classification methods in terms of both classification accuracy
and anomaly detection rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04550</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04550</id><created>2015-07-16</created><authors><author><keyname>de Arruda</keyname><forenames>Guilherme Ferraz</forenames></author><author><keyname>Cozzo</keyname><forenames>Emanuele</forenames></author><author><keyname>Moreno</keyname><forenames>Yamir</forenames></author><author><keyname>Rodrigues</keyname><forenames>Francisco A.</forenames></author></authors><title>On degree-degree correlations in multilayer networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>8 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a generalization of the concept of assortativity based on the
tensorial representation of multilayer networks, covering the definitions given
in terms of Pearson and Spearman coefficients. Our approach can also be applied
to weighted networks and provides information about correlations considering
pairs of layers. By analyzing the multilayer representation of the airport
transportation network, we show that contrasting results are obtained when the
layers are analyzed independently or as an interconnected system. Finally, we
study the impact of the level of assortativity and heterogeneity between layers
on the spreading of diseases. Our results highlight the need of studying
degree-degree correlations on multilayer systems, instead of on aggregated
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04554</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04554</id><created>2015-07-16</created><authors><author><keyname>Yang</keyname><forenames>Han-Xin</forenames></author><author><keyname>Tang</keyname><forenames>Ming</forenames></author><author><keyname>Lai</keyname><forenames>Ying-Cheng</forenames></author></authors><title>Traffic-driven epidemic spreading in correlated networks</title><categories>physics.soc-ph cs.SI</categories><comments>6 pagea, 6 gigures</comments><journal-ref>Phys. Rev. E 91, 062817 (2015)</journal-ref><doi>10.1103/PhysRevE.91.062817</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In spite of the extensive previous efforts on traffic dynamics and epidemic
spreading in complex networks, the problem of traffic-driven epidemic spreading
on {\em correlated} networks has not been addressed. Interestingly, we find
that the epidemic threshold, a fundamental quantity underlying the spreading
dynamics, exhibits a non-monotonic behavior in that it can be minimized for
some critical value of the assortativity coefficient, a parameter
characterizing the network correlation. To understand this phenomenon, we use
the degree-based mean-field theory to calculate the traffic-driven epidemic
threshold for correlated networks. The theory predicts that the threshold is
inversely proportional to the packet-generation rate and the largest eigenvalue
of the betweenness matrix. We obtain consistency between theory and numerics.
Our results may provide insights into the important problem of
controlling/harnessing real-world epidemic spreading dynamics driven by traffic
flows.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04571</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04571</id><created>2015-07-16</created><updated>2015-11-10</updated><authors><author><keyname>Kranich</keyname><forenames>Stefan</forenames></author></authors><title>GPU-based visualization of domain-coloured algebraic Riemann surfaces</title><categories>cs.GR math.AG math.CV</categories><comments>revised version; 21 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine an algorithm for the visualization of domain-coloured Riemann
surfaces of plane algebraic curves. The approach faithfully reproduces the
topology and the holomorphic structure of the Riemann surface. We discuss how
the algorithm can be implemented efficiently in OpenGL with geometry shaders,
and (less efficiently) even in WebGL with multiple render targets and floating
point textures. While the generation of the surface takes noticeable time in
both implementations, the visualization of a cached Riemann surface mesh is
possible with interactive performance. This allows us to visually explore
otherwise almost unimaginable mathematical objects. As examples, we look at the
complex square root and the folium of Descartes. For the folium of Descartes,
the visualization reveals features of the algebraic curve which are not obvious
from its equation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04576</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04576</id><created>2015-07-16</created><updated>2016-01-13</updated><authors><author><keyname>Aghaei</keyname><forenames>Maedeh</forenames></author><author><keyname>Dimiccoli</keyname><forenames>Mariella</forenames></author><author><keyname>Radeva</keyname><forenames>Petia</forenames></author></authors><title>Multi-Face Tracking by Extended Bag-of-Tracklets in Egocentric Videos</title><categories>cs.CV</categories><comments>27 pages, 18 figures, submitted to computer vision and image
  understanding journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wearable cameras offer a hands-free way to record egocentric images of daily
experiences, where social events are of special interest. The first step
towards detection of social events is to track the appearance of multiple
persons involved in it. In this paper, we propose a novel method to find
correspondences of multiple faces in low temporal resolution egocentric videos
acquired through a wearable camera. This kind of photo-stream imposes
additional challenges to the multi-tracking problem with respect to
conventional videos. Due to the free motion of the camera and to its low
temporal resolution, abrupt changes in the field of view, in illumination
condition and in the target location are highly frequent. To overcome such
difficulties, we propose a multi-face tracking method that generates a set of
tracklets through finding correspondences along the whole sequence for each
detected face and takes advantage of the tracklets redundancy to deal with
unreliable ones. Similar tracklets are grouped into the so called extended
bag-of-tracklets (eBoT), which is aimed to correspond to a specific person.
Finally, a prototype tracklet is extracted for each eBoT, where the occurred
occlusions are estimated by relying on a new measure of confidence. We
validated our approach over an extensive dataset of egocentric photo-streams
and compared it to state of the art methods, demonstrating its effectiveness
and robustness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04577</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04577</id><created>2015-07-16</created><authors><author><keyname>Spiwack</keyname><forenames>Arnaud</forenames></author></authors><title>Notes on axiomatising Hurkens's Paradox</title><categories>cs.LO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  An axiomatisation of Hurkens's paradox in dependent type theory is given
without assuming any impredicative feature of said type theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04578</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04578</id><created>2015-07-16</created><authors><author><keyname>Gor</keyname><forenames>Bela</forenames></author><author><keyname>Aspinall</keyname><forenames>David</forenames></author></authors><title>Accessible Banking: Experiences and Future Directions</title><categories>cs.CY</categories><comments>3 pages, presented at Workshop on Inclusive Privacy and Security
  (WIPS): Privacy and Security for Everyone, Anytime, Anywhere, held as part of
  Symposium on Usable Privacy and Security (SOUPS) 2015, July 22-24, 2015,
  Ottawa, Canada</comments><acm-class>H.1.2; K.4.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is a short position paper drawing on experience working with the UK
banking industry and their disabled and ageing customers in the Business
Disability Forum, a UK non-profit member organisation funded by a large body of
UK private and public sector businesses. We describe some commonly reported
problems of disabled customers who use modern banking technologies, relating
them to UK law and best practice. We describe some of the recent banking
industry innovations and the hope they may offer for improved inclusive and
accessible multi-channel banking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04580</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04580</id><created>2015-07-16</created><authors><author><keyname>Ahmadi</keyname><forenames>Hamed</forenames></author><author><keyname>Finn</keyname><forenames>Danny</forenames></author><author><keyname>Razavi</keyname><forenames>Rouzbeh</forenames></author><author><keyname>Claussen</keyname><forenames>Holger</forenames></author><author><keyname>DaSilva</keyname><forenames>Luiz A.</forenames></author></authors><title>Optimization of Demand Hotspot Capacities using Switched Multi-Element
  Antenna Equipped Small Cells</title><categories>cs.NI</categories><comments>Accepted for publication at VTC2015-fall</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents switched Multi- Element Antennas (MEAs) as a simple, yet
effective, method of enhancing the performance of small cell heterogeneous
networks and compensating for the small cell base station sub-optimal
placement. The switched MEA system is a low-cost system which enables the small
cell to dynamically direct its transmission power toward locations of high user
density, in other words demand hotspots. Our simulation results show that small
cell base stations equipped with switched MEA systems offer greater performance
than base stations equipped with omni-directional antennas in terms of both the
number of users that can be served (and hence offloaded from the macrocell
network) and in terms of overall network capacity. We also compare the
performance of the switched MEA with fixed directional antennas and show that
fixed-directional antennas can only outperform the switched MEA if the
misalignment between their direction of transmission and the direction to the
demand hotspot is less than 22.5 degrees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04585</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04585</id><created>2015-07-16</created><authors><author><keyname>Moreira</keyname><forenames>Angel Torres</forenames></author><author><keyname>Igartua</keyname><forenames>Monica Aguilar</forenames></author><author><keyname>Puglisi</keyname><forenames>Silvia</forenames></author></authors><title>Design and implementation of an Android application to anonymously
  analyse locations of the citizens in Barcelona</title><categories>cs.CY cs.CR</categories><comments>M.Sc. Thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The MobilitApp application is able to obtain mobility data and type of
activity performed by a person. It runs in the background and stores the
information in synchronous periodic locations. The main work of this project
was to develop tools that facilitate the exploitation of the information
obtained, add elements that make it attractive to use the application and
spread it to a wider audience. With these new procedures, we manage to increase
the number of connected users, improve security with which sensitive user
information is managed and establish channels that will be used to add
additional functionality to the application in future works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04587</identifier>
 <datestamp>2015-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04587</id><created>2015-07-04</created><authors><author><keyname>Teixeira</keyname><forenames>Jose</forenames></author><author><keyname>Robles</keyname><forenames>Gregorio</forenames></author><author><keyname>Gonz&#xe1;lez-Barahona</keyname><forenames>Jes&#xfa;s</forenames></author></authors><title>Lessons Learned from Applying Social Network Analysis on an Industrial
  Free/Libre/Open Source Software Ecosystem</title><categories>cs.SI cs.SE</categories><comments>As accepted by the Journal of Internet Services and Applications
  (JISA)</comments><acm-class>D.2.9; K.6.3; K.6.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many software projects are no longer done in-house by a single organization.
Instead, we are in a new age where software is developed by a networked
community of individuals and organizations, which base their relations to each
other on mutual interest. Paradoxically, recent research suggests that software
development can actually be jointly-developed by rival firms. For instance, it
is known that the mobile-device makers Apple and Samsung kept collaborating in
open source projects while running expensive patent wars in the court. Taking a
case study approach, we explore how rival firms collaborate in the open source
arena by employing a multi-method approach that combines qualitative analysis
of archival data (QA) with mining software repositories (MSR) and Social
Network Analysis (SNA). While exploring collaborative processes within the
OpenStack ecosystem, our research contributes to Software Engineering research
by exploring the role of groups, sub-communities and business models within a
high-networked open source ecosystem. Surprising results point out that
competition for the same revenue model (i.e., operating conflicting business
models) does not necessary affect collaboration within the ecosystem. Moreover,
while detecting the different sub-communities of the OpenStack community, we
found out that the expected social tendency of developers to work with
developers from same firm (i.e., homophony) did not hold within the OpenStack
ecosystem. Furthermore, while addressing a novel, complex and unexplored open
source case, this research also contributes to the management literature in
coopetition strategy and high-tech entrepreneurship with a rich description on
how heterogeneous actors within a high-networked ecosystem (involving
individuals, startups, established firms and public organizations)
joint-develop a complex infrastructure for big-data in the open source arena.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04588</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04588</id><created>2015-07-16</created><authors><author><keyname>Gao</keyname><forenames>Xinyu</forenames></author><author><keyname>Dai</keyname><forenames>Linglong</forenames></author><author><keyname>Hu</keyname><forenames>Yuting</forenames></author><author><keyname>Wang</keyname><forenames>Zhongxu</forenames></author><author><keyname>Wang</keyname><forenames>Zhaocheng</forenames></author></authors><title>Matrix Inversion-Less Signal Detection Using SOR Method for Uplink
  Large-Scale MIMO Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For uplink large-scale MIMO systems, linear minimum mean square error (MMSE)
signal detection algorithm is near-optimal but involves matrix inversion with
high complexity. In this paper, we propose a low-complexity signal detection
algorithm based on the successive overrelaxation (SOR) method to avoid the
complicated matrix inversion. We first prove a special property that the MMSE
filtering matrix is symmetric positive definite for uplink large-scale MIMO
systems, which is the premise for the SOR method. Then a low-complexity
iterative signal detection algorithm based on the SOR method as well as the
convergence proof is proposed. The analysis shows that the proposed scheme can
reduce the computational complexity from O(K3) to O(K2), where K is the number
of users. Finally, we verify through simulation results that the proposed
algorithm outperforms the recently proposed Neumann series approximation
algorithm, and achieves the near-optimal performance of the classical MMSE
algorithm with a small number of iterations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04589</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04589</id><created>2015-07-16</created><authors><author><keyname>Spiwack</keyname><forenames>Arnaud</forenames></author></authors><title>The tree machine</title><categories>cs.LO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A variant of Turing machines is introduced where the tape is replaced by a
single tree which can be manipulated in a style akin to purely functional
programming. This yields two benefits: first, the extra structure on the tape
can be leveraged to write explicit constructions of machines much more easily
than with Turing machines. Second, this new kind of machines models finely the
asymptotic complexity of functional programming languages, and may allow to
answer questions such as &quot;is this problem inherently slower in functional
languages&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1507.04592</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1507.04592</id><created>2015-07-16</created><authors><author><keyname>Gao</keyname><forenames>Xinyu</forenames></author><author><keyname>Dai</keyname><forenames>Linglong</forenames></author><author><keyname>Han</keyname><forenames>Shuangfeng</forenames></author><author><keyname>I</keyname><forenames>Chih-Lin</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>Energy-Efficient Hybrid Analog and Digital Precoding for mmWave MIMO
  Systems with Large Antenna Arrays</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Millimeter wave (mmWave) MIMO will likely use hybrid analog and digital
precoding, which uses a small number of RF chains to avoid energy consumption
associated with mixed signal components like analog-to-digital components not
to mention baseband processing complexity. However, most hybrid precoding
techniques consider a fully-connected architecture requiring a large number of
phase shifters, which is also energyintensive. In this paper, we focus on the
more energy-efficient hybrid precoding with sub-connected architecture, and
propose a successive interference cancelation (SIC)-based hybrid precoding with
near-optimal performance and low complexity. Inspired by the idea of SIC for
multi-user signal detection, we first propose to decompose the total achievable
rate optimization problem with non-convex constraints into a series of simple
sub-rate optimization problems, each of which only considers one sub-antenna
array. Then, we prove that maximizing the achievable sub-rate of each
sub-antenna array is equivalent to simply seeking a precoding vector
sufficiently close (in terms of Euclidean distance) to the unconstrained
optimal solution. Finally, we propose a low-complexity algorithm to realize
SICbased hybrid precoding, which can avoid the need for the singular value
decomposition (SVD) and matrix inversion. Complexity evaluation shows that the
complexity of SIC-based hybrid precoding is only about 10% as complex as that
of the recently proposed spatially sparse precoding in typical mmWave MIMO
systems. Simulation results verify the near-optimal performance of SIC-based
hybrid precoding.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="80000" completeListSize="102538">1122234|81001</resumptionToken>
</ListRecords>
</OAI-PMH>
