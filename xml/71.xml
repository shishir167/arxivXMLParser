<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T03:40:04Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|70001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4171</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4171</id><created>2014-12-12</created><authors><author><keyname>Krishnamurthy</keyname><forenames>Vikram</forenames></author><author><keyname>Hoiles</keyname><forenames>William</forenames></author></authors><title>Information Diffusion in Social Sensing</title><categories>cs.SI physics.soc-ph</categories><comments>arXiv admin note: text overlap with arXiv:1405.1129</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistical inference using social sensors is an area that has witnessed
remarkable progress in the last decade. It is relevant in a variety of
applications including localizing events for targeted advertising, mar- keting,
localization of natural disasters and predicting sentiment of investors in
financial markets. This paper presents a tutorial description of three
important aspects of sensing-based information diffusion in social networks
from a communications/signal processing perspective. First, diffusion models
for information exchange in large scale social networks together with social
sensing via social media networks such as Twitter is considered. Second,
Bayesian social learning models in online reputation systems are presented.
Finally, the principle of revealed preferences arising in micro-economics
theory is used to parse datasets to determine if social sensors are utility
maximizers and then determine their utility functions. All three topics are
explained in the context of actual experimental datasets from health networks,
social media and psychological experiments. Also, algorithms are given that
exploit the above models to infer underlying events based on social sensing.
The overview, insights, models and algorithms presented in this paper stem from
recent developments in computer-science, economics, psychology and electrical
engineering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4172</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4172</id><created>2014-12-12</created><authors><author><keyname>Jayasumana</keyname><forenames>Sadeep</forenames></author><author><keyname>Hartley</keyname><forenames>Richard</forenames></author><author><keyname>Salzmann</keyname><forenames>Mathieu</forenames></author><author><keyname>Li</keyname><forenames>Hongdong</forenames></author><author><keyname>Harandi</keyname><forenames>Mehrtash</forenames></author></authors><title>Kernel Methods on the Riemannian Manifold of Symmetric Positive Definite
  Matrices</title><categories>cs.CV</categories><comments>Published in CVPR 2013. arXiv admin note: substantial text overlap
  with arXiv:1412.0265</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Symmetric Positive Definite (SPD) matrices have become popular to encode
image information. Accounting for the geometry of the Riemannian manifold of
SPD matrices has proven key to the success of many algorithms. However, most
existing methods only approximate the true shape of the manifold locally by its
tangent plane. In this paper, inspired by kernel methods, we propose to map SPD
matrices to a high dimensional Hilbert space where Euclidean geometry applies.
To encode the geometry of the manifold in the mapping, we introduce a family of
provably positive definite kernels on the Riemannian manifold of SPD matrices.
These kernels are derived from the Gaussian ker- nel, but exploit different
metrics on the manifold. This lets us extend kernel-based algorithms developed
for Euclidean spaces, such as SVM and kernel PCA, to the Riemannian manifold of
SPD matrices. We demonstrate the benefits of our approach on the problems of
pedestrian detection, ob- ject categorization, texture analysis, 2D motion
segmentation and Diffusion Tensor Imaging (DTI) segmentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4174</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4174</id><created>2014-12-12</created><authors><author><keyname>Jayasumana</keyname><forenames>Sadeep</forenames></author><author><keyname>Salzmann</keyname><forenames>Mathieu</forenames></author><author><keyname>Li</keyname><forenames>Hongdong</forenames></author><author><keyname>Harandi</keyname><forenames>Mehrtash</forenames></author></authors><title>A Framework for Shape Analysis via Hilbert Space Embedding</title><categories>cs.CV</categories><comments>Published in ICCV 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a framework for 2D shape analysis using positive definite kernels
defined on Kendall's shape manifold. Different representations of 2D shapes are
known to generate different nonlinear spaces. Due to the nonlinearity of these
spaces, most existing shape classification algorithms resort to nearest
neighbor methods and to learning distances on shape spaces. Here, we propose to
map shapes on Kendall's shape manifold to a high dimensional Hilbert space
where Euclidean geometry applies. To this end, we introduce a kernel on this
manifold that permits such a mapping, and prove its positive definiteness. This
kernel lets us extend kernel-based algorithms developed for Euclidean spaces,
such as SVM, MKL and kernel PCA, to the shape manifold. We demonstrate the
benefits of our approach over the state-of-the-art methods on shape
classification, clustering and retrieval.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4175</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4175</id><created>2014-12-12</created><authors><author><keyname>Jayasumana</keyname><forenames>Sadeep</forenames></author><author><keyname>Hartley</keyname><forenames>Richard</forenames></author><author><keyname>Salzmann</keyname><forenames>Mathieu</forenames></author><author><keyname>Li</keyname><forenames>Hongdong</forenames></author><author><keyname>Harandi</keyname><forenames>Mehrtash</forenames></author></authors><title>Optimizing Over Radial Kernels on Compact Manifolds</title><categories>cs.CV</categories><comments>Published in CVPR 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We tackle the problem of optimizing over all possible positive definite
radial kernels on Riemannian manifolds for classification. Kernel methods on
Riemannian manifolds have recently become increasingly popular in computer
vision. However, the number of known positive definite kernels on manifolds
remain very limited. Furthermore, most kernels typically depend on at least one
parameter that needs to be tuned for the problem at hand. A poor choice of
kernel, or of parameter value, may yield significant performance drop-off.
Here, we show that positive definite radial kernels on the unit n-sphere, the
Grassmann manifold and Kendall's shape manifold can be expressed in a simple
form whose parameters can be automatically optimized within a support vector
machine framework. We demonstrate the benefits of our kernel learning algorithm
on object, face, action and shape recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4179</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4179</id><created>2014-12-12</created><authors><author><keyname>Letiche</keyname><forenames>Terrence</forenames></author><author><keyname>Lissack</keyname><forenames>Michael</forenames></author></authors><title>What About Feedback?</title><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The role of immediate feedback in-group conversations has received scant
attention in the recent literature. While studies from the early 1990's
suggested that &quot;added information&quot; in the form of non-verbal cues would allow
video conferencing to &quot;augment&quot; the audio-only conference in terms of
effectiveness, stunningly little follow-on research has been done reflective of
the current state of computer mediated communication, video conferencing, &quot;live
walls&quot;, etc. This article contrasts three studies of immediate feedback in
in-person settings as the basis for suggesting a new research program -
research to look at potential effects of augmenting video-conferencing with an
immediate feedback channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4181</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4181</id><created>2014-12-12</created><updated>2015-06-28</updated><authors><author><keyname>Hallman</keyname><forenames>Sam</forenames></author><author><keyname>Fowlkes</keyname><forenames>Charless C.</forenames></author></authors><title>Oriented Edge Forests for Boundary Detection</title><categories>cs.CV</categories><comments>updated to include contents of CVPR version + new figure showing
  example segmentation results</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a simple, efficient model for learning boundary detection based on
a random forest classifier. Our approach combines (1) efficient clustering of
training examples based on simple partitioning of the space of local edge
orientations and (2) scale-dependent calibration of individual tree output
probabilities prior to multiscale combination. The resulting model outperforms
published results on the challenging BSDS500 boundary detection benchmark.
Further, on large datasets our model requires substantially less memory for
training and speeds up training time by a factor of 10 over the structured
forest model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4182</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4182</id><created>2014-12-12</created><authors><author><keyname>Steinhardt</keyname><forenames>Jacob</forenames></author><author><keyname>Wager</keyname><forenames>Stefan</forenames></author><author><keyname>Liang</keyname><forenames>Percy</forenames></author></authors><title>The Statistics of Streaming Sparse Regression</title><categories>math.ST cs.LG stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a sparse analogue to stochastic gradient descent that is
guaranteed to perform well under similar conditions to the lasso. In the linear
regression setup with irrepresentable noise features, our algorithm recovers
the support set of the optimal parameter vector with high probability, and
achieves a statistically quasi-optimal rate of convergence of Op(k log(d)/T),
where k is the sparsity of the solution, d is the number of features, and T is
the number of training examples. Meanwhile, our algorithm does not require any
more computational resources than stochastic gradient descent. In our
experiments, we find that our method substantially out-performs existing
streaming algorithms on both real and simulated data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4183</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4183</id><created>2014-12-12</created><authors><author><keyname>Borovikov</keyname><forenames>Eugene</forenames></author></authors><title>A survey of modern optical character recognition techniques</title><categories>cs.CV</categories><comments>Technical report surveying OCR/ICR and document understanding methods
  as of 2004.It contains 38 pages, numerous figures, 93 references, and
  provides a table of contents</comments><msc-class>62-04</msc-class><acm-class>I.7.5; I.4.1; I.5.4; I.4.1; I.4.3; I.4.6; I.4.7</acm-class><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  This report explores the latest advances in the field of digital document
recognition. With the focus on printed document imagery, we discuss the major
developments in optical character recognition (OCR) and document image
enhancement/restoration in application to Latin and non-Latin scripts. In
addition, we review and discuss the available technologies for hand-written
document recognition. In this report, we also provide some company-accumulated
benchmark results on available OCR engines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4184</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4184</id><created>2014-12-12</created><authors><author><keyname>Kratchanov</keyname><forenames>Kostadin</forenames></author><author><keyname>Golemanova</keyname><forenames>Emilia</forenames></author><author><keyname>Golemanov</keyname><forenames>Tzanko</forenames></author><author><keyname>Ercan</keyname><forenames>Tuncay</forenames></author><author><keyname>Ekici</keyname><forenames>Burak</forenames></author></authors><title>Procedural and Non-Procedural Implementation of Search Strategies in
  Control Network Programming</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report presents the general picture of how Control Network Programming
can be effectively used for implementing various search strategies, both blind
and informed. An interesting possibility is non - procedural solutions that can
be developed for most local search algorithms. A generic solution is described
for procedural implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4186</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4186</id><created>2014-12-12</created><authors><author><keyname>Borovikov</keyname><forenames>Eugene</forenames></author></authors><title>An Evaluation of Support Vector Machines as a Pattern Recognition Tool</title><categories>cs.LG</categories><comments>A short (6 page) report on evaluation of the SVM as a pattern
  classification tool, as of 1999</comments><msc-class>62-07</msc-class><acm-class>I.5.1; I.5.2; I.5.4; I.2.6</acm-class><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  The purpose of this report is in examining the generalization performance of
Support Vector Machines (SVM) as a tool for pattern recognition and object
classification. The work is motivated by the growing popularity of the method
that is claimed to guarantee a good generalization performance for the task in
hand. The method is implemented in MATLAB. SVMs based on various kernels are
tested for classifying data from various domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4188</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4188</id><created>2014-12-12</created><updated>2016-01-11</updated><authors><author><keyname>Kyn&#x10d;l</keyname><forenames>Jan</forenames></author><author><keyname>Lidick&#xfd;</keyname><forenames>Bernard</forenames></author><author><keyname>Vysko&#x10d;il</keyname><forenames>Tom&#xe1;&#x161;</forenames></author></authors><title>Irreversible 2-conversion set in graphs of bounded degree</title><categories>cs.DM cs.CC math.CO</categories><comments>16 pages, 12 figures, added references to overlapping results</comments><msc-class>05C85, 05C07, 05B35</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An irreversible $k$-threshold process (also a $k$-neighbor bootstrap
percolation) is a dynamic process on a graph where vertices change color from
white to black if they have at least $k$ black neighbors. An irreversible
$k$-conversion set of a graph $G$ is a subset $S$ of vertices of $G$ such that
the irreversible $k$-threshold process starting with $S$ black eventually
changes all vertices of $G$ to black. We show that deciding the existence of an
irreversible 2-conversion set of a given size is NP-complete, even for graphs
of maximum degree 4, which answers a question of Dreyer and Roberts.
Conversely, we show that for graphs of maximum degree 3, the minimum size of an
irreversible 2-conversion set can be computed in polynomial time. Moreover, we
find an optimal irreversible 3-conversion set for the toroidal grid,
simplifying constructions of Pike and Zou.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4192</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4192</id><created>2014-12-12</created><updated>2015-11-29</updated><authors><author><keyname>Baghaie</keyname><forenames>Ahmadreza</forenames></author><author><keyname>Moghaddam</keyname><forenames>Hamid Abrishami</forenames></author></authors><title>Finite Element Method Based Modeling of Cardiac Deformation Estimation
  under Abnormal Ventricular Muscle Conditions</title><categories>cs.CE</categories><comments>Some grammatical errors are corrected in the text</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deformation modeling of cardiac muscle is an important issue in the field of
cardiac analysis. Many approaches have been developed to better estimate the
cardiac muscle deformation, and to obtain a practical model to be used in
diagnostic procedures. But there are some conditions, like in case of
myocardial infarction, in which the regular modeling approaches are not useful.
In this article, using a point-wise approach, we try to estimate the
deformation under some abnormal conditions of cardiac muscle. First, the
endocardial and epicardial contour points are ordered with respect to the
center of gravity of the endocardial contour and displacement vectors of
boundary points are extracted. Then to solve the governing equations of the
deformation, which is an elliptic equation, we apply boundary conditions in
accordance with the computed displacement vectors and then the Finite Element
method (FEM) will be used to solve the governing equations. Using the obtained
displacement field of the cardiac muscle, strain map is extracted to show the
mechanical behavior of the cardiac muscle. Several tests are conducted using
phantom and real cardiac data in order to show the validity of the proposed
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4196</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4196</id><created>2014-12-13</created><authors><author><keyname>Hu</keyname><forenames>Yuan-Ting</forenames></author><author><keyname>Lin</keyname><forenames>Yen-Yu</forenames></author><author><keyname>Chen</keyname><forenames>Hsin-Yi</forenames></author><author><keyname>Hsu</keyname><forenames>Kuang-Jui</forenames></author><author><keyname>Chen</keyname><forenames>Bing-Yu</forenames></author></authors><title>Descriptor Ensemble: An Unsupervised Approach to Descriptor Fusion in
  the Homography Space</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the aim to improve the performance of feature matching, we present an
unsupervised approach to fuse various local descriptors in the space of
homographies. Inspired by the observation that the homographies of correct
feature correspondences vary smoothly along the spatial domain, our approach
stands on the unsupervised nature of feature matching, and can select a good
descriptor for matching each feature point. Specifically, the homography space
serves as the common domain, in which a correspondence obtained by any
descriptor is considered as a point, for integrating various heterogeneous
descriptors. Both geometric coherence and spatial continuity among
correspondences are considered via computing their geodesic distances in the
space. In this way, mutual verification across different descriptors is
allowed, and correct correspondences will be highlighted with a high degree of
consistency (i.e., short geodesic distances here). It follows that one-class
SVM can be applied to identifying these correct correspondences, and boosts the
performance of feature matching. The proposed approach is comprehensively
compared with the state-of-the-art approaches, and evaluated on four benchmarks
of image matching. The promising results manifest its effectiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4198</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4198</id><created>2014-12-13</created><updated>2016-01-07</updated><authors><author><keyname>Brandt</keyname><forenames>Felix</forenames></author><author><keyname>Brill</keyname><forenames>Markus</forenames></author><author><keyname>Suksompong</keyname><forenames>Warut</forenames></author></authors><title>An Ordinal Minimax Theorem</title><categories>cs.GT</categories><comments>10 pages, 2 figures</comments><journal-ref>Games and Economic Behavior (2016) 95:107-112</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the early 1950s Lloyd Shapley proposed an ordinal and set-valued solution
concept for zero-sum games called \emph{weak saddle}. We show that all weak
saddles of a given zero-sum game are interchangeable and equivalent. As a
consequence, every such game possesses a unique set-based value.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4203</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4203</id><created>2014-12-13</created><authors><author><keyname>Kariotoglou</keyname><forenames>Nikolaos</forenames></author><author><keyname>Margellos</keyname><forenames>Kostas</forenames></author><author><keyname>Lygeros</keyname><forenames>John</forenames></author></authors><title>On the computational complexity and generalization properties of
  multi-stage and recursive scenario programs</title><categories>math.OC cs.CC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss the computational complexity and feasibility properties of
scenario based techniques for uncertain optimization programs. We consider
different solution alternatives ranging from the standard scenario approach to
recursive variants, and compare feasibility as a function of the total
computation burden. We identify trade-offs between the different methods
depending on the problem structure and the desired probability of constraint
satisfaction. Our motivation for this work stems from the applicability and
complexity reduction when making decisions by means of recursive algorithms. We
illustrate our results on an example from the area of approximate dynamic
programming
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4205</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4205</id><created>2014-12-13</created><authors><author><keyname>Zhao</keyname><forenames>Xiaosha</forenames></author><author><keyname>Liu</keyname><forenames>Mandan</forenames></author></authors><title>The application of the Bayes Ying Yang harmony based GMMs in on-line
  signature verification</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this contribution, a Bayes Ying Yang(BYY) harmony based approach for
on-line signature verification is presented. In the proposed method, a simple
but effective Gaussian Mixture Models(GMMs) is used to represent for each
user's signature model based on the prior information collected. Different from
the early works, in this paper, we use the Bayes Ying Yang machine combined
with the harmony function to achieve Automatic Model Selection(AMS) during the
parameter learning for the GMMs, so that a better approximation of the user
model is assured. Experiments on a database from the First International
Signature Verification Competition(SVC 2004) confirm that this combined
algorithm yields quite satisfactory results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4210</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4210</id><created>2014-12-13</created><updated>2016-01-08</updated><authors><author><keyname>Banerjee</keyname><forenames>Arunava</forenames></author></authors><title>Learning Precise Spike Train to Spike Train Transformations in
  Multilayer Feedforward Neuronal Networks</title><categories>cs.NE</categories><comments>31 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive a synaptic weight update rule for learning temporally precise spike
train to spike train transformations in multilayer feedforward networks of
spiking neurons. The framework, aimed at seamlessly generalizing error
backpropagation to the deterministic spiking neuron setting, is based strictly
on spike timing and avoids invoking concepts pertaining to spike rates or
probabilistic models of spiking. The derivation is founded on two innovations.
First, an error functional is proposed that compares the spike train emitted by
the output neuron of the network to the desired spike train by way of their
putative impact on a virtual postsynaptic neuron. This formulation sidesteps
the need for spike alignment and leads to closed form solutions for all
quantities of interest. Second, virtual assignment of weights to spikes rather
than synapses enables a perturbation analysis of individual spike times and
synaptic weights of the output as well as all intermediate neurons in the
network, which yields the gradients of the error functional with respect to the
said entities. Learning proceeds via a gradient descent mechanism that
leverages these quantities. Simulation experiments demonstrate the efficacy of
the proposed learning framework. The experiments also highlight asymmetries
between synapses on excitatory and inhibitory neurons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4213</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4213</id><created>2014-12-13</created><authors><author><keyname>Cao</keyname><forenames>Yangjie</forenames></author><author><keyname>Sun</keyname><forenames>Hongyang</forenames></author><author><keyname>Qian</keyname><forenames>Depei</forenames></author><author><keyname>Wu</keyname><forenames>Weiguo</forenames></author></authors><title>Scalable Hierarchical Scheduling for Malleable Parallel Jobs on
  Multiprocessor-based Systems</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The proliferation of multi-core and multiprocessor-based computer systems has
led to explosive development of parallel applications and hence the need for
efficient schedulers. In this paper, we study hierarchical scheduling for
malleable parallel jobs on multiprocessor-based systems, which appears in many
distributed and multilayered computing environments. We propose a hierarchical
scheduling algorithm, named AC-DS, that consists of a feedback-driven adaptive
scheduler, a desire aggregation scheme and an efficient resource allocation
policy. From theoretical perspective, we show that AC-DS has scalable
performance regardless of the number of hierarchical levels. In particular, we
prove that AC-DS achieves $O(1)$-competitiveness with respect to the overall
completion time of the jobs, or the makespan. A detailed malleable job model is
developed to experimentally evaluate the effectiveness of the proposed
scheduling algorithm. The results verify the scalability of AC-DS and
demonstrate that AC-DS outperforms other strategies for a wide range of
parallel workloads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4217</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4217</id><created>2014-12-13</created><authors><author><keyname>Hakro</keyname><forenames>Dil Nawaz</forenames></author><author><keyname>Talib</keyname><forenames>A. Z.</forenames></author><author><keyname>Bhatti</keyname><forenames>Zeeshan</forenames></author><author><keyname>Moja</keyname><forenames>G. N.</forenames></author></authors><title>A Study of Sindhi Related and Arabic Script Adapted languages
  Recognition</title><categories>cs.CV</categories><comments>11 pages, 8 Figures, Sindh Univ. Res. Jour. (Sci. Ser.)</comments><journal-ref>Sindh University Research Journal (Science Series) Vol. 46 (3)
  323-334 (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A large number of publications are available for the Optical Character
Recognition (OCR). Significant researches, as well as articles are present for
the Latin, Chinese and Japanese scripts. Arabic script is also one of mature
script from OCR perspective. The adaptive languages which share Arabic script
or its extended characters; still lacking the OCRs for their language. In this
paper we present the efforts of researchers on Arabic and its related and
adapted languages. This survey is organized in different sections, in which
introduction is followed by properties of Sindhi Language. OCR process
techniques and methods used by various researchers are presented. The last
section is dedicated for future work and conclusion is also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4218</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4218</id><created>2014-12-13</created><authors><author><keyname>Lam</keyname><forenames>Ho Tat</forenames></author><author><keyname>Szeto</keyname><forenames>Kwok Yip</forenames></author></authors><title>Optimization of Reliability of Network of Given Connectivity using
  Genetic Algorithm</title><categories>physics.soc-ph cs.NE cs.SI</categories><comments>9 pages, 10 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reliability is one of the important measures of how well the system meets its
design objective, and mathematically is the probability that a system will
perform satisfactorily for at least a given period of time. When the system is
described by a connected network of N components (nodes) and their L connection
(links), the reliability of the system becomes a difficult network design
problem which solutions are of great practical interest in science and
engineering. This paper discusses the numerical method of finding the most
reliable network for a given N and L using genetic algorithm. For a given
topology of the network, the reliability is numerically computed using
adjacency matrix. For a search in the space of all possible topologies of the
connected network with N nodes and L links, genetic operators such as mutation
and crossover are applied to the adjacency matrix through a string
representation. In the context of graphs, the mutation of strings in genetic
algorithm corresponds to the rewiring of graphs, while crossover corresponds to
the interchange of the sub-graphs. For small networks where the most reliable
network can be found by exhaustive search, genetic algorithm is very efficient.
For larger networks, our results not only demonstrate the efficiency of our
algorithm, but also suggest that the most reliable network will have high
symmetry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4224</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4224</id><created>2014-12-13</created><authors><author><keyname>He</keyname><forenames>Jiguang</forenames></author><author><keyname>Kim</keyname><forenames>Taejoon</forenames></author><author><keyname>Ghauch</keyname><forenames>Hadi</forenames></author><author><keyname>Liu</keyname><forenames>Kunpeng</forenames></author><author><keyname>Wang</keyname><forenames>Guangjian</forenames></author></authors><title>Millimeter Wave MIMO Channel Tracking Systems</title><categories>cs.IT math.IT</categories><comments>6 pages, 3 figures, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider channel/subspace tracking systems for temporally correlated
millimeter wave (e.g., E-band) multiple-input multiple-output (MIMO) channels.
Our focus is given to the tracking algorithm in the non-line-of-sight (NLoS)
environment, where the transmitter and the receiver are equipped with hybrid
analog/digital precoder and combiner, respectively. In the absence of
straightforward time-correlated channel model in the millimeter wave MIMO
literature, we present a temporal MIMO channel evolution model for NLoS
millimeter wave scenarios. Considering that conventional MIMO channel tracking
algorithms in microwave bands are not directly applicable, we propose a new
channel tracking technique based on sequentially updating the precoder and
combiner. Numerical results demonstrate the superior channel tracking ability
of the proposed technique over independent sounding approach in the presented
channel model and the spatial channel model (SCM) adopted in 3GPP
specification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4234</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4234</id><created>2014-12-13</created><updated>2016-02-04</updated><authors><author><keyname>Longo</keyname><forenames>Riccardo</forenames></author><author><keyname>Marcolla</keyname><forenames>Chiara</forenames></author><author><keyname>Sala</keyname><forenames>Massimiliano</forenames></author></authors><title>Key-Policy Multi-Authority Attribute-Based Encryption</title><categories>cs.CR</categories><comments>12 pages</comments><msc-class>11T71</msc-class><journal-ref>Algebraic Informatics, Springer Verlag, 2015, p. 152-164</journal-ref><doi>10.1007/978-3-319-23021-4_14</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bilinear groups are often used to create Attribute-Based Encryption (ABE)
algorithms. In particular, they have been used to create an ABE system with
multi authorities, but limited to the ciphertext-policy instance. Here, for the
first time, we propose a multi-authority key-policy ABE system. In our
proposal, the authorities may be set up in any moment and without any
coordination. A party can simply act as an ABE authority by creating its own
public parameters and issuing private keys to the users. A user can thus
encrypt data choosing both a set of attributes and a set of trusted
authorities, maintaining full control unless all his chosen authorities collude
against him. We prove our system secure under the bilinear Diffie-Hellman
assumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4237</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4237</id><created>2014-12-13</created><authors><author><keyname>Burger</keyname><forenames>Martin</forenames></author><author><keyname>Sawatzky</keyname><forenames>Alex</forenames></author><author><keyname>Steidl</keyname><forenames>Gabriele</forenames></author></authors><title>First order algorithms in variational image processing</title><categories>math.OC cs.CV stat.ML</categories><comments>60 pages, 33 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Variational methods in imaging are nowadays developing towards a quite
universal and flexible tool, allowing for highly successful approaches on tasks
like denoising, deblurring, inpainting, segmentation, super-resolution,
disparity, and optical flow estimation. The overall structure of such
approaches is of the form ${\cal D}(Ku) + \alpha {\cal R} (u) \rightarrow
\min_u$ ; where the functional ${\cal D}$ is a data fidelity term also
depending on some input data $f$ and measuring the deviation of $Ku$ from such
and ${\cal R}$ is a regularization functional. Moreover $K$ is a (often linear)
forward operator modeling the dependence of data on an underlying image, and
$\alpha$ is a positive regularization parameter. While ${\cal D}$ is often
smooth and (strictly) convex, the current practice almost exclusively uses
nonsmooth regularization functionals. The majority of successful techniques is
using nonsmooth and convex functionals like the total variation and
generalizations thereof or $\ell_1$-norms of coefficients arising from scalar
products with some frame system. The efficient solution of such variational
problems in imaging demands for appropriate algorithms. Taking into account the
specific structure as a sum of two very different terms to be minimized,
splitting algorithms are a quite canonical choice. Consequently this field has
revived the interest in techniques like operator splittings or augmented
Lagrangians. Here we shall provide an overview of methods currently developed
and recent results as well as some computational studies providing a comparison
of different methods and also illustrating their success in applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4246</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4246</id><created>2014-12-13</created><authors><author><keyname>Baudel</keyname><forenames>Thomas</forenames></author></authors><title>A Canonical Representation of Data-Linear Visualization Algorithms</title><categories>cs.GR cs.SE</categories><comments>10 pages, extended version of the original technical report</comments><report-no>ILOG Technical report 05-003. 04/12/2005</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce linear-state dataflows, a canonical model for a large set of
visualization algorithms that we call data-linear visualizations. Our model
defines a fixed dataflow architecture: partitioning and subpartitioning of
input data, ordering, graphic primitives, and graphic attributes generation.
Local variables and accumulators are specific concepts that extend the
expressiveness of the dataflow to support features of visualization algorithms
that require state handling. We first show the flexibility of our model: it
enables the declarative construction of many common algorithms with just a few
mappings. Furthermore, the model enables easy mixing of visual mappings, such
as creating treemaps of histograms and 2D plots, plots of histograms...
Finally, we introduce our model in a more formal way and present some of its
important properties. We have implemented this model in a visualization
framework built around the concept of linear-state dataflows.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4259</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4259</id><created>2014-12-13</created><authors><author><keyname>Blondin</keyname><forenames>Michael</forenames></author><author><keyname>Finkel</keyname><forenames>Alain</forenames></author><author><keyname>G&#xf6;ller</keyname><forenames>Stefan</forenames></author><author><keyname>Haase</keyname><forenames>Christoph</forenames></author><author><keyname>McKenzie</keyname><forenames>Pierre</forenames></author></authors><title>Reachability in Two-Dimensional Vector Addition Systems with States is
  PSPACE-complete</title><categories>cs.FL cs.CC cs.LO</categories><comments>27 pages, 8 figures</comments><acm-class>F.1.1; F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Determining the complexity of the reachability problem for vector addition
systems with states (VASS) is a long-standing open problem in computer science.
Long known to be decidable, the problem to this day lacks any complexity upper
bound whatsoever. In this paper, reachability for two-dimensional VASS is shown
PSPACE-complete. This improves on a previously known doubly exponential time
bound established by Howell, Rosier, Huynh and Yen in 1986. The coverability
and boundedness problems are also noted to be PSPACE-complete. In addition,
some complexity results are given for the reachability problem in
two-dimensional VASS and in integer VASS when numbers are encoded in unary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4261</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4261</id><created>2014-12-13</created><authors><author><keyname>Alsan</keyname><forenames>Mine</forenames></author></authors><title>Re-proving Channel Polarization Theorems: An Extremality and Robustness
  Analysis</title><categories>cs.IT math.IT</categories><comments>Preview of my PhD Thesis, EPFL, Lausanne, 2014. For the full version,
  see http://people.epfl.ch/mine.alsan/publications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The general subject considered in this thesis is a recently discovered coding
technique, polar coding, which is used to construct a class of error correction
codes with unique properties. In his ground-breaking work, Ar{\i}kan proved
that this class of codes, called polar codes, achieve the symmetric capacity
--- the mutual information evaluated at the uniform input distribution ---of
any stationary binary discrete memoryless channel with low complexity encoders
and decoders requiring in the order of $O(N\log N)$ operations in the
block-length $N$. This discovery settled the long standing open problem left by
Shannon of finding low complexity codes achieving the channel capacity.
  Polar coding settled an open problem in information theory, yet opened plenty
of challenging problems that need to be addressed. A significant part of this
thesis is dedicated to advancing the knowledge about this technique in two
directions. The first one provides a better understanding of polar coding by
generalizing some of the existing results and discussing their implications,
and the second one studies the robustness of the theory over communication
models introducing various forms of uncertainty or variations into the
probabilistic model of the channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4265</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4265</id><created>2014-12-13</created><updated>2014-12-21</updated><authors><author><keyname>Chen</keyname><forenames>Wenjian</forenames></author><author><keyname>Zhang</keyname><forenames>Haizhang</forenames></author></authors><title>Exponential Approximation of Multivariate Bandlimited Functions from
  Average Oversampling</title><categories>cs.IT math.IT</categories><msc-class>41A25, 62D05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Instead of sampling a function at a single point, average sampling takes the
weighted sum of function values around the point. Such a sampling strategy is
more practical and more stable. In this note, we present an explicit method
with an exponentially-decaying approximation error to reconstruct a
multivariate bandlimited function from its finite average oversampling data.
The key problem in our analysis is how to extend a function so that its Fourier
transform decays at an optimal rate to zero at infinity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4271</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4271</id><created>2014-12-13</created><updated>2015-06-18</updated><authors><author><keyname>Nobandegani</keyname><forenames>Ardavan Salehi</forenames></author><author><keyname>Psaromiligkos</keyname><forenames>Ioannis N.</forenames></author></authors><title>Multi-Context Models for Reasoning under Partial Knowledge: Generative
  Process and Inference Grammar</title><categories>cs.AI math.LO math.PR stat.ML</categories><comments>To appear in the Proceedings of the 31st Conference on Uncertainty in
  Artificial Intelligence (UAI 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Arriving at the complete probabilistic knowledge of a domain, i.e., learning
how all variables interact, is indeed a demanding task. In reality, settings
often arise for which an individual merely possesses partial knowledge of the
domain, and yet, is expected to give adequate answers to a variety of posed
queries. That is, although precise answers to some queries, in principle,
cannot be achieved, a range of plausible answers is attainable for each query
given the available partial knowledge. In this paper, we propose the
Multi-Context Model (MCM), a new graphical model to represent the state of
partial knowledge as to a domain. MCM is a middle ground between Probabilistic
Logic, Bayesian Logic, and Probabilistic Graphical Models. For this model we
discuss: (i) the dynamics of constructing a contradiction-free MCM, i.e., to
form partial beliefs regarding a domain in a gradual and probabilistically
consistent way, and (ii) how to perform inference, i.e., to evaluate a
probability of interest involving some variables of the domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4273</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4273</id><created>2014-12-13</created><updated>2015-09-12</updated><authors><author><keyname>Drwal</keyname><forenames>Maciej</forenames></author><author><keyname>Rischke</keyname><forenames>Roman</forenames></author></authors><title>Complexity of interval minmax regret scheduling on parallel identical
  machines with total completion time criterion</title><categories>cs.DS cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the problem of scheduling jobs on parallel
identical machines, where the processing times of jobs are uncertain: only
interval bounds of processing times are known. The optimality criterion of a
schedule is the total completion time. In order to cope with the uncertainty we
consider the maximum regret objective and we seek a schedule which performs
well under all possible instantiations of processing times. Although the
deterministic version of the considered problem is solvable in polynomial time,
the minmax regret version is known to be weakly NP-hard even for a single
machine, and strongly NP-hard for parallel unrelated machines. In this paper we
show that the problem is strongly NP-hard also in the case of parallel
identical machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4299</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4299</id><created>2014-12-13</created><updated>2015-06-18</updated><authors><author><keyname>Jiang</keyname><forenames>Bo</forenames></author><author><keyname>Zhang</keyname><forenames>Zhi-Li</forenames></author><author><keyname>Towsley</keyname><forenames>Don</forenames></author></authors><title>Reciprocity in Social Networks with Capacity Constraints</title><categories>cs.SI physics.soc-ph</categories><acm-class>G.2.2; H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Directed links -- representing asymmetric social ties or interactions (e.g.,
&quot;follower-followee&quot;) -- arise naturally in many social networks and other
complex networks, giving rise to directed graphs (or digraphs) as basic
topological models for these networks. Reciprocity, defined for a digraph as
the percentage of edges with a reciprocal edge, is a key metric that has been
used in the literature to compare different directed networks and provide
&quot;hints&quot; about their structural properties: for example, are reciprocal edges
generated randomly by chance or are there other processes driving their
generation? In this paper we study the problem of maximizing achievable
reciprocity for an ensemble of digraphs with the same prescribed in- and
out-degree sequences. We show that the maximum reciprocity hinges crucially on
the in- and out-degree sequences, which may be intuitively interpreted as
constraints on some &quot;social capacities&quot; of nodes and impose fundamental limits
on achievable reciprocity. We show that it is NP-complete to decide the
achievability of a simple upper bound on maximum reciprocity, and provide
conditions for achieving it. We demonstrate that many real networks exhibit
reciprocities surprisingly close to the upper bound, which implies that users
in these social networks are in a sense more &quot;social&quot; than suggested by the
empirical reciprocity alone in that they are more willing to reciprocate,
subject to their &quot;social capacity&quot; constraints. We find some surprising linear
relationships between empirical reciprocity and the bound. We also show that a
particular type of small network motifs that we call 3-paths are the major
source of loss in reciprocity for real networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4303</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4303</id><created>2014-12-13</created><authors><author><keyname>Tang</keyname><forenames>Mingjie</forenames></author><author><keyname>Tahboub</keyname><forenames>Ruby Y.</forenames></author><author><keyname>Aref</keyname><forenames>Walid G.</forenames></author><author><keyname>Malluhi</keyname><forenames>Qutaibah M.</forenames></author><author><keyname>Ouzzani</keyname><forenames>Mourad</forenames></author></authors><title>On Order-independent Semantics of the Similarity Group-By Relational
  Database Operator</title><categories>cs.DB</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Similarity group-by (SGB, for short) has been proposed as a relational
database operator to match the needs of emerging database applications. Many
SGB operators that extend SQL have been proposed in the literature, e.g.,
similarity operators in the one-dimensional space. These operators have various
semantics. Depending on how these operators are implemented, some of the
implementations may lead to different groupings of the data. Hence, if SQL code
is ported from one database system to another, it is not guaranteed that the
code will produce the same results. In this paper, we investigate the various
semantics for the relational similarity group-by operators in the
multi-dimensional space. We define the class of order-independent SGB operators
that produce the same results regardless of the order in which the input data
is presented to them. Using the notion of interval graphs borrowed from graph
theory, we prove that, for certain SGB operators, there exist order-independent
implementations. For each of these operators, we provide a sample algorithm
that is order-independent. Also, we prove that for other SGB operators, there
does not exist an order-independent implementation for them, and hence these
SGB operators are ill-defined and should not be adopted in extensions to SQL to
realize similarity group-by. In this paper, we introduce an SGB operator,
namely SGB-All, for grouping multi-dimensional data using similarity. SGB-All
forms groups such that a data item, say O, belongs to a group, say G, if and
only if O is within a user-defined threshold from all other data items in G. In
other words, each group in SGB-All forms a clique of nearby data items in the
multi-dimensional space. We prove that SGB-All are order-independent, i.e.,
there is at least one algorithm for each option that is independent of the
presentation order of the input data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4311</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4311</id><created>2014-12-13</created><authors><author><keyname>Salimi</keyname><forenames>Babak</forenames></author><author><keyname>Bertossi</keyname><forenames>Leopoldo</forenames></author></authors><title>From Causes for Database Queries to Repairs and Model-Based Diagnosis
  and Back</title><categories>cs.DB</categories><comments>Extended version of paper to appear in Proceedings of ICDT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we establish and investigate connections between causality for
query answers in databases, database repairs wrt. denial constraints, and
consistency-based diagnosis. The first two are relatively new problems in
databases, and the third one is an established subject in knowledge
representation. We show how to obtain database repairs from causes and the
other way around. Causality problems are formulated as diagnosis problems, and
the diagnoses provide causes and their responsibilities. The vast body of
research on database repairs can be applied to the newer problem of determining
actual causes for query answers and their responsibilities. These connections,
which are interesting per se, allow us, after a transition -inspired by
consistency-based diagnosis- to computational problems on hitting sets and
vertex covers in hypergraphs, to obtain several new algorithmic and complexity
results for causality in databases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4313</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4313</id><created>2014-12-14</created><updated>2014-12-15</updated><authors><author><keyname>Cogswell</keyname><forenames>Michael</forenames></author><author><keyname>Lin</keyname><forenames>Xiao</forenames></author><author><keyname>Purushwalkam</keyname><forenames>Senthil</forenames></author><author><keyname>Batra</keyname><forenames>Dhruv</forenames></author></authors><title>Combining the Best of Graphical Models and ConvNets for Semantic
  Segmentation</title><categories>cs.CV</categories><comments>13 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a two-module approach to semantic segmentation that incorporates
Convolutional Networks (CNNs) and Graphical Models. Graphical models are used
to generate a small (5-30) set of diverse segmentations proposals, such that
this set has high recall. Since the number of required proposals is so low, we
can extract fairly complex features to rank them. Our complex feature of choice
is a novel CNN called SegNet, which directly outputs a (coarse) semantic
segmentation. Importantly, SegNet is specifically trained to optimize the
corpus-level PASCAL IOU loss function. To the best of our knowledge, this is
the first CNN specifically designed for semantic segmentation. This two-module
approach achieves $52.5\%$ on the PASCAL 2012 segmentation challenge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4314</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4314</id><created>2014-12-14</created><updated>2014-12-22</updated><authors><author><keyname>Chang</keyname><forenames>Joseph Chee</forenames></author><author><keyname>Lin</keyname><forenames>Chu-Cheng</forenames></author></authors><title>Recurrent-Neural-Network for Language Detection on Twitter
  Code-Switching Corpus</title><categories>cs.NE cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mixed language data is one of the difficult yet less explored domains of
natural language processing. Most research in fields like machine translation
or sentiment analysis assume monolingual input. However, people who are capable
of using more than one language often communicate using multiple languages at
the same time. Sociolinguists believe this &quot;code-switching&quot; phenomenon to be
socially motivated. For example, to express solidarity or to establish
authority. Most past work depend on external tools or resources, such as
part-of-speech tagging, dictionary look-up, or named-entity recognizers to
extract rich features for training machine learning models. In this paper, we
train recurrent neural networks with only raw features, and use word embedding
to automatically learn meaningful representations. Using the same
mixed-language Twitter corpus, our system is able to outperform the best
SVM-based systems reported in the EMNLP'14 Code-Switching Workshop by 1% in
accuracy, or by 17% in error rate reduction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4316</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4316</id><created>2014-12-14</created><authors><author><keyname>Mazonka</keyname><forenames>Oleg</forenames></author><author><keyname>Popov</keyname><forenames>Vlad</forenames></author></authors><title>Hasq Hash Chains</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a particular hash-based records linking chain scheme.
This scheme is simple conceptually and easy to implement in software. It allows
for a simple and secure way to transfer ownership of digital objects between
peers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4318</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4318</id><created>2014-12-14</created><authors><author><keyname>Chowdhury</keyname><forenames>Mostafa Zaman</forenames></author></authors><title>Adaptive Resource Management for Multimedia Applications in
  Femtocellular and Macrocellular Networks</title><categories>cs.NI cs.MM</categories><comments>PhD Dessertation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing demands of various high data rate wireless applications have
been seen in the recent years and it will continue in the future. To fulfill
these demands, the limited existing wireless resources should be utilized
properly or new wireless technology should be developed. Therefore, we propose
some novel idea to manage the wireless resources and deployment of
femtocellular network technology. The study was mainly divided into two parts:
(a) femtocellular network deployment and resource allocation and (b) resource
management for macrocellular networks. The femtocellular network deployment
scenarios, integrated femtocell/macrocell network architectures, cost-effective
frequency planning, and mobility management schemes are presented in first
part. In the second part, we provide a CAC based on adaptive bandwidth
allocation for the wireless network in. The proposed CAC relies on adaptive
multi-level bandwidth-allocation scheme for non-real-time calls. We propose
video service provisioning over wireless networks. We provide a QoS adaptive
radio resource allocation as well as popularity based bandwidth allocation
schemes for scalable videos over wireless cellular networks. All the proposed
schemes are verified through several numerical and simulation results. The
research results presented in this dissertation clearly imply the advantages of
our proposed schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4320</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4320</id><created>2014-12-14</created><authors><author><keyname>Lupei</keyname><forenames>Daniel</forenames></author><author><keyname>Koch</keyname><forenames>Christoph</forenames></author><author><keyname>Tannen</keyname><forenames>Val</forenames></author></authors><title>Incremental View Maintenance for Nested-Relational Databases</title><categories>cs.DB</categories><comments>34 pages (12 pages plus appendix)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Incremental view maintenance is an essential tool for speeding up the
processing of large, locally changing workloads. Its fundamental challenge is
to ensure that changes are propagated from input to output more efficiently
than via recomputation. We formalize this requirement for positive nested
relational algebra (NRA+) on bags and we propose a transformation deriving
deltas for any expression in the language.
  The main difficulty in maintaining nested queries lies in the inability to
express within NRA+ the efficient updating of inner bags, i.e., without
completely replacing the tuples that contain them. To address this problem, we
first show how to efficiently incrementalize IncNRA+, a large fragment of NRA+
whose deltas never generate inner bag updates. We then provide a
semantics-preserving transformation that takes any nested query into a
collection of IncNRA+ queries. This constitutes the first static solution for
the efficient incremental processing of languages with nested collections.
Furthermore, we show that the state-of-the-art technique of recursive IVM,
originally developed for positive relational algebra with aggregation, also
extends to nested queries.
  Finally, we generalize our static approach for the efficient
incrementalization of NRA+ to a family of simply-typed lambda calculi, given
that its primitives are themselves efficiently incrementalizable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4321</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4321</id><created>2014-12-14</created><authors><author><keyname>Chowdhury</keyname><forenames>Mostafa Zaman</forenames></author><author><keyname>Saha</keyname><forenames>Nirzhar</forenames></author><author><keyname>Chae</keyname><forenames>Sung Hun</forenames></author><author><keyname>Jang</keyname><forenames>Yeong Min</forenames></author></authors><title>Handover Call Admission Control for Mobile Femtocells with Free-Space
  Optical and Macrocellular Backbone Networks</title><categories>cs.NI</categories><journal-ref>Journal of Advanced Smart Convergence Vol.1 No.1 1-10 (2012)</journal-ref><doi>10.7236/JASC2012.1.1.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The deployment of mobile femtocellular networks can enhance the service
quality for the users inside the vehicles. The deployment of mobile femtocells
generates a lot of handover calls. Also, numbers of group handover scenarios
are found in mobile femtocellular network deployment. The ability to seamlessly
switch between the femtocells and the macrocell networks is a key concern for
femtocell network deployment. However, until now there is no effective and
complete handover scheme for the mobile femtocell network deployment. Also
handover between the backhaul networks is a major concern for the mobile
femtocellular network deployment. In this paper, we propose handover control
between the access networks for the individual handover cases. Call flows for
the handover between the backhaul networks of the macrocell-to-macrocell case
are proposed in this paper. We also propose the link switching for the FSO
based backhaul networks. The proposed resource allocation scheme ensures the
negligible handover call dropping probability as well as higher bandwidth
utilization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4322</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4322</id><created>2014-12-14</created><authors><author><keyname>Chowdhury</keyname><forenames>Mostafa Zaman</forenames></author><author><keyname>Jang</keyname><forenames>Yeong Min</forenames></author><author><keyname>Haas</keyname><forenames>Zygmunt J.</forenames></author></authors><title>Priority based Bandwidth Adaptation for Multi-class Traffic in Wireless
  Networks</title><categories>cs.NI</categories><journal-ref>International Journal of Multimedia and Ubiquitous Engineering,
  vol. 7, no. 2, pp. 445-450, April, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The bandwidth adaptation is the technique that allows the flexibility in
bandwidth allocation for a call. Using the bandwidth adaptation technique, the
number of call admission in the system can be increased significantly. In this
paper we propose a priority based bandwidth adaptation scheme that can release
multi-level of bandwidth from the existing calls to accept call requests. The
amount of released bandwidth is based on the number of existing bandwidth
adaptive calls and the priority of requesting traffic call. This priority
scheme does not reduce the bandwidth utilization. Moreover, the proposed
bandwidth adaptation strategy provides significantly reduced call blocking
probability for the higher priority traffic calls. The performance analyses
show the improvement of the proposed scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4324</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4324</id><created>2014-12-14</created><updated>2015-03-14</updated><authors><author><keyname>Shoukry</keyname><forenames>Yasser</forenames></author><author><keyname>Nuzzo</keyname><forenames>Pierluigi</forenames></author><author><keyname>Puggelli</keyname><forenames>Alberto</forenames></author><author><keyname>Sangiovanni-Vincentelli</keyname><forenames>Alberto L.</forenames></author><author><keyname>Seshia</keyname><forenames>Sanjit A.</forenames></author><author><keyname>Tabuada</keyname><forenames>Paulo</forenames></author></authors><title>Secure State Estimation For Cyber Physical Systems Under Sensor Attacks:
  A Satisfiability Modulo Theory Approach</title><categories>math.OC cs.CR cs.IT cs.SY math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of detecting and mitigating the effect of malicious
attacks to the sensors of a linear dynamical system. We develop a novel,
efficient algorithm that uses a Satisfiability-Modulo-Theory approach to
isolate the compromised sensors and estimate the system state despite the
presence of the attack, thus harnessing the intrinsic combinatorial complexity
of the problem. By leveraging results from formal methods over real numbers, we
provide guarantees on the soundness and completeness of our algorithm. We then
report simulation results to compare its runtime performance with alternative
techniques. Finally, we demonstrate its application to the problem of
controlling an unmanned ground vehicle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4353</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4353</id><created>2014-12-14</created><authors><author><keyname>Mohamed</keyname><forenames>Aridj</forenames></author></authors><title>LH*TH: New fast Scalable Distributed Data Structures (SDDS)</title><categories>cs.DC</categories><journal-ref>International Journal of Computer Science Issues,(IJCSI) Volume
  11, Issue 6, No 2, pp 123-128 November 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Proposed in 1993 the Scalable Distributed Data Structures (SDDSs) became a
profile of basis for the data management on Multi computer. In this paper we
propose an organization of a LH* bucket based on the trie hashing in order to
improve times of different access request.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4361</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4361</id><created>2014-12-14</created><updated>2015-01-25</updated><authors><author><keyname>Abbar</keyname><forenames>Sofiane</forenames></author><author><keyname>Mejova</keyname><forenames>Yelena</forenames></author><author><keyname>Weber</keyname><forenames>Ingmar</forenames></author></authors><title>You Tweet What You Eat: Studying Food Consumption Through Twitter</title><categories>cs.CY cs.SI physics.soc-ph</categories><acm-class>H.3.1; H.3.5; J.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Food is an integral part of our lives, cultures, and well-being, and is of
major interest to public health. The collection of daily nutritional data
involves keeping detailed diaries or periodic surveys and is limited in scope
and reach. Alternatively, social media is infamous for allowing its users to
update the world on the minutiae of their daily lives, including their eating
habits. In this work we examine the potential of Twitter to provide insight
into US-wide dietary choices by linking the tweeted dining experiences of 210K
users to their interests, demographics, and social networks. We validate our
approach by relating the caloric values of the foods mentioned in the tweets to
the state-wide obesity rates, achieving a Pearson correlation of 0.77 across
the 50 US states and the District of Columbia. We then build a model to predict
county-wide obesity and diabetes statistics based on a combination of
demographic variables and food names mentioned on Twitter. Our results show
significant improvement over previous CHI research (Culotta'14). We further
link this data to societal and economic factors, such as education and income,
illustrating that, for example, areas with higher education levels tweet about
food that is significantly less caloric. Finally, we address the somewhat
controversial issue of the social nature of obesity (first raised by Christakis
&amp; Fowler in 2007) by inducing two social networks using mentions and reciprocal
following relationships.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4365</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4365</id><created>2014-12-14</created><updated>2015-12-08</updated><authors><author><keyname>Nakashima</keyname><forenames>Norihiro</forenames></author><author><keyname>Matsui</keyname><forenames>Hajime</forenames></author></authors><title>Decoding of Projective Reed-Muller Codes by Dividing a Projective Space
  into Affine Spaces</title><categories>cs.IT math.IT</categories><comments>17 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A projective Reed-Muller (PRM) code, obtained by modifying a (classical)
Reed-Muller code with respect to a projective space, is a doubly extended
Reed-Solomon code when the dimension of the related projective space is equal
to 1. The minimum distance and dual code of a PRM code are known, and some
decoding examples have been represented for low-dimensional projective space.
In this study, we construct a decoding algorithm for all PRM codes by dividing
a projective space into a union of affine spaces. In addition, we determine the
computational complexity and the number of errors correctable of our algorithm.
Finally, we compare the codeword error rate of our algorithm with that of
minimum distance decoding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4369</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4369</id><created>2014-12-14</created><updated>2015-03-21</updated><authors><author><keyname>Fried</keyname><forenames>Daniel</forenames></author><author><keyname>Duh</keyname><forenames>Kevin</forenames></author></authors><title>Incorporating Both Distributional and Relational Semantics in Word
  Representations</title><categories>cs.CL</categories><comments>This is the long version of a short paper accepted as a workshop
  contribution at ICLR2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the hypothesis that word representations ought to incorporate
both distributional and relational semantics. To this end, we employ the
Alternating Direction Method of Multipliers (ADMM), which flexibly optimizes a
distributional objective on raw text and a relational objective on WordNet.
Preliminary results on knowledge base completion, analogy tests, and parsing
show that word representations trained on both objectives can give improvements
in some cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4378</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4378</id><created>2014-12-14</created><authors><author><keyname>Samanthula</keyname><forenames>Bharath K.</forenames></author><author><keyname>Rao</keyname><forenames>Fang-Yu</forenames></author><author><keyname>Bertino</keyname><forenames>Elisa</forenames></author><author><keyname>Yi</keyname><forenames>Xun</forenames></author><author><keyname>Liu</keyname><forenames>Dongxi</forenames></author></authors><title>Privacy-Preserving and Outsourced Multi-User k-Means Clustering</title><categories>cs.CR cs.DB</categories><comments>16 pages, 2 figures, 5 tables</comments><acm-class>D.4.6; E.3; H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many techniques for privacy-preserving data mining (PPDM) have been
investigated over the past decade. Often, the entities involved in the data
mining process are end-users or organizations with limited computing and
storage resources. As a result, such entities may want to refrain from
participating in the PPDM process. To overcome this issue and to take many
other benefits of cloud computing, outsourcing PPDM tasks to the cloud
environment has recently gained special attention. We consider the scenario
where n entities outsource their databases (in encrypted format) to the cloud
and ask the cloud to perform the clustering task on their combined data in a
privacy-preserving manner. We term such a process as privacy-preserving and
outsourced distributed clustering (PPODC). In this paper, we propose a novel
and efficient solution to the PPODC problem based on k-means clustering
algorithm. The main novelty of our solution lies in avoiding the secure
division operations required in computing cluster centers altogether through an
efficient transformation technique. Our solution builds the clusters securely
in an iterative fashion and returns the final cluster centers to all entities
when a pre-determined termination condition holds. The proposed solution
protects data confidentiality of all the participating entities under the
standard semi-honest model. To the best of our knowledge, ours is the first
work to discuss and propose a comprehensive solution to the PPODC problem that
incurs negligible cost on the participating entities. We theoretically estimate
both the computation and communication costs of the proposed protocol and also
demonstrate its practical value through experiments on a real dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4385</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4385</id><created>2014-12-14</created><updated>2015-04-15</updated><authors><author><keyname>Yang</keyname><forenames>Yi</forenames></author><author><keyname>Eisenstein</keyname><forenames>Jacob</forenames></author></authors><title>Unsupervised Domain Adaptation with Feature Embeddings</title><categories>cs.CL cs.LG</categories><comments>For more details, please refer to the long version of this paper:
  http://www.cc.gatech.edu/~jeisenst/papers/yang-naacl-2015.pdf</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Representation learning is the dominant technique for unsupervised domain
adaptation, but existing approaches often require the specification of &quot;pivot
features&quot; that generalize across domains, which are selected by task-specific
heuristics. We show that a novel but simple feature embedding approach provides
better performance, by exploiting the feature template structure common in NLP
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4388</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4388</id><created>2014-12-14</created><authors><author><keyname>Stanciu</keyname><forenames>Silviu</forenames></author><author><keyname>Dobrescu</keyname><forenames>Lidia</forenames></author></authors><title>A New System For Recording The Radiological Effective Doses For Patients
  Investigated by Imaging Methods</title><categories>cs.CE physics.med-ph</categories><comments>International Congress Of Radiology ICR 2014, Dubai, United Arab
  Emirates</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper the project of an integrated system for radiation safety and
security of the patients investigated by radiological imaging methods is
presented. The new system is based on smart cards and Public Key
Infrastructure. The new system allows radiation effective dose data storage and
a more accurate reporting system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4392</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4392</id><created>2014-12-14</created><authors><author><keyname>Liu</keyname><forenames>Chun-Hung</forenames></author></authors><title>Adaptive Downlink CoMP in Heterogeneous Cellular Networks with Imperfect
  Overhead Messaging</title><categories>cs.IT math.IT</categories><comments>6 pages, 4 figures, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coordinated multi-point (CoMP) transmission is an effective means of
improving network throughput in heterogeneous cellular networks (HetNets).
However, its performance is seriously weakened if imperfect coordination
happens between base stations (BSs). Many prior CoMP works do not consider
inter-cell overhead message delays such that a seemingly astonishing CoMP
throughput gain is attained. In this paper, the quantization error and delay
that actually exist in overhead messages was modeled and we developed a much
tractable SIR model based on the stochastic geometry framework. We proposed
adaptive CoMP that is applied to downlink zero-forcing beamforming (ZFBF) and
it can mitigate the interference from the coordinated cells with delayed
overhead messages. The bounds on the complementary cumulative distribution
function (CCDF) of the SIR of a user are characterized such that the average
throughput of a user is able to be analytically evaluated. Numerical results
show that the proposed adaptive CoMP scheme can make the throughput gain very
robust to the overhead delay and thus significantly increase the throughput
even when BSs are not perfectly coordinated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4395</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4395</id><created>2014-12-14</created><authors><author><keyname>Gauci</keyname><forenames>Rachel</forenames></author></authors><title>Dafny: Statically Verifying Functional Correctness</title><categories>cs.PL</categories><comments>12 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report presents the Dafny language and verifier, with a focus on
describing the main features of the language, including pre- and
postconditions, assertions, loop invariants, termination metrics, quantifiers,
predicates and frames. Examples of Dafny code are provided to illustrate the
use of each feature, and an overview of how Dafny translates programming code
into a mathematical proof of functional verification is presented. The report
also includes references to useful resources on Dafny, with mentions of related
works in the domain of specification languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4401</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4401</id><created>2014-12-14</created><authors><author><keyname>Enguehard</keyname><forenames>C.</forenames><affiliation>LINA</affiliation></author><author><keyname>Daille</keyname><forenames>B.</forenames></author><author><keyname>Morin</keyname><forenames>E.</forenames></author></authors><title>Tools for Terminology Processing</title><categories>cs.CY cs.CL</categories><proxy>ccsd</proxy><journal-ref>R. K. Arora, M. Kulkarni, H. Darbari. The Indo-European Conference
  on Multilingual Communications Technologies (IEMCT), Jun 2002, Pune, India.
  Tata McGraw-Hill, pp.218 - 229</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic terminology processing appeared 10 years ago when electronic
corpora became widely available. Such processing may be statistically or
linguistically based and produces terminology resources that can be used in a
number of applications : indexing, information retrieval, technology watch,
etc. We present the tools that have been developed in the IRIN Institute. They
all take as input texts (or collection of texts) and reflect different states
of terminology processing: term acquisition, term recognition and term
structuring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4406</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4406</id><created>2014-12-14</created><authors><author><keyname>Dobrescu</keyname><forenames>Lidia</forenames></author></authors><title>Domotic Embedded System</title><categories>cs.OH</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper presents an original domotic embedded system for room temperature
monitoring. The OpenRemote is the main software interface between the user and
the system, but other software components and communication protocols are used,
such as 1-Wire protocol for temperature monitoring devices, RS-232 for the
central PC unit and OWFS software for remote control using Android mobile
devices. The system architecture consists in hardware and software components
to remote control a room temperature parameter for energy efficiency
increasing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4411</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4411</id><created>2014-12-14</created><authors><author><keyname>Miller</keyname><forenames>Benjamin A.</forenames></author><author><keyname>Arcolano</keyname><forenames>Nicholas</forenames></author><author><keyname>Wolf</keyname><forenames>Michael M.</forenames></author><author><keyname>Bliss</keyname><forenames>Nadya T.</forenames></author></authors><title>Spectral Anomaly Detection in Very Large Graphs: Models, Noise, and
  Computational Complexity</title><categories>cs.SI physics.soc-ph</categories><comments>Extended abstract of a presentation at Dagstuhl seminar 14461,
  &quot;High-performance Graph Algorithms and Applications in Computational
  Science,&quot; held 9-14 November, 2014. 4 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Anomaly detection in massive networks has numerous theoretical and
computational challenges, especially as the behavior to be detected becomes
small in comparison to the larger network. This presentation focuses on recent
results in three key technical areas, specifically geared toward spectral
methods for detection. We first discuss recent models for network behavior, and
how their structure can be exploited for efficient computation of the principal
eigenspace of the graph. In addition to the stochasticity of background
activity, a graph of interest may be observed through a noisy or imperfect
mechanism, which may hinder the detection process. A few simple noise models
are discussed, and we demonstrate the ability to fuse multiple corrupted
observations and recover detection performance. Finally, we discuss the
challenges in scaling the spectral algorithms to large-scale high-performance
computing systems, and present preliminary recommendations to achieve good
performance with current parallel eigensolvers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4413</identifier>
 <datestamp>2015-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4413</id><created>2014-12-14</created><updated>2015-01-14</updated><authors><author><keyname>Bri&#xeb;t</keyname><forenames>Jop</forenames></author><author><keyname>Regev</keyname><forenames>Oded</forenames></author><author><keyname>Saket</keyname><forenames>Rishi</forenames></author></authors><title>Tight Hardness of the Non-commutative Grothendieck Problem</title><categories>cs.CC math.FA</categories><comments>19 pages. Changes compared to v1: strengthened results to NP-hardness</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that for any $\varepsilon &gt; 0$ it is NP-hard to approximate the
non-commutative Grothendieck problem to within a factor $1/2 + \varepsilon$,
which matches the approximation ratio of the algorithm of Naor, Regev, and
Vidick (STOC'13). Our proof uses an embedding of $\ell_2$ into the space of
matrices endowed with the trace norm with the property that the image of
standard basis vectors is longer than that of unit vectors with no large
coordinates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4422</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4422</id><created>2014-12-14</created><authors><author><keyname>Schneider</keyname><forenames>Klaus M.</forenames></author><author><keyname>Mast</keyname><forenames>Kai</forenames></author><author><keyname>Krieger</keyname><forenames>Udo R.</forenames></author></authors><title>CCN Forwarding Strategies for Multihomed Mobile Terminals</title><categories>cs.NI</categories><comments>to be published in the NetSys 2015 proceedings</comments><doi>10.1109/NetSys.2015.7089075</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current IP-based networks are unable to fully exploit the capabilities of the
increasing number of multihomed mobile terminals. We argue that Content-Centric
Networking (CCN), a novel networking architecture based on named information
objects, can fill the gap. In this paper, we elicit requirements for CCN packet
forwarding on multihomed mobile terminals. We categorize CCN forwarding
strategies according to their ability to fulfill these requirements and provide
a real-world performance evaluation in the current CCNx prototype
implementation. Moreover, we describe the initial design of an advanced
multipath forwarding strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4430</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4430</id><created>2014-12-14</created><authors><author><keyname>Chen</keyname><forenames>Yongxin</forenames></author><author><keyname>Georgiou</keyname><forenames>Tryphon</forenames></author><author><keyname>Pavon</keyname><forenames>Michele</forenames></author></authors><title>On the relation between optimal transport and Schr\&quot;odinger bridges: A
  stochastic control viewpoint</title><categories>cs.SY math-ph math.MP math.OC math.PR</categories><comments>28 pages</comments><msc-class>93E20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We take a new look at the relation between the optimal transport problem and
the Schr\&quot;{o}dinger bridge problem from the stochastic control perspective. We
show that the connections are richer and deeper than described in existing
literature. In particular: a) We give an elementary derivation of the
Benamou-Brenier fluid dynamics version of the optimal transport problem; b) We
provide a new fluid dynamics version of the Schr\&quot;{o}dinger bridge problem; c)
We observe that the latter provides an important connection with optimal
transport without zero noise limits; d) We propose and solve a fluid dynamic
version of optimal transport with prior; e) We can then view optimal transport
with prior as the zero noise limit of Schr\&quot;{o}dinger bridges when the prior is
any Markovian evolution. In particular, we work out the Gaussian case. A
numerical example of the latter convergence involving Brownian particles is
also provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4433</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4433</id><created>2014-12-14</created><updated>2015-03-16</updated><authors><author><keyname>Chen</keyname><forenames>Dai-Qiang</forenames></author></authors><title>Inexact Alternating Direction Method Based on Newton descent algorithm
  with Application to Poisson Image Deblurring</title><categories>cs.CV</categories><comments>23 pages, 7 figures</comments><msc-class>68U10, 90C90, 65T60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recovery of images from the observations that are degraded by a linear
operator and further corrupted by Poisson noise is an important task in modern
imaging applications such as astronomical and biomedical ones. Gradient-based
regularizers involve the popular total variation semi-norm have become standard
techniques for Poisson image restoration due to its edge-preserving ability.
Various efficient algorithms have been developed for solving the corresponding
minimization problem with non-smooth regularization terms. In this paper,
motivated by the idea of the alternating direction minimization algorithm and
the Newton's method with upper convergent rate, we further propose inexact
alternating direction methods utilizing the proximal Hessian matrix information
of the objective function, in a way reminiscent of Newton descent methods.
Besides, we also investigate the global convergence of the proposed algorithms
under certain conditions. Finally, we illustrate that the proposed algorithms
outperform the current state-of-the-art algorithms through numerical
experiments on Poisson image deblurring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4438</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4438</id><created>2014-12-14</created><authors><author><keyname>Chen</keyname><forenames>Dai-Qiang</forenames></author></authors><title>Fixed Point Algorithm Based on Quasi-Newton Method for Convex
  Minimization Problem with Application to Image Deblurring</title><categories>cs.CV</categories><msc-class>68U10, 90C90, 65T60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Solving an optimization problem whose objective function is the sum of two
convex functions has received considerable interests in the context of image
processing recently. In particular, we are interested in the scenario when a
non-differentiable convex function such as the total variation (TV) norm is
included in the objective function due to many variational models established
in image processing have this nature. In this paper, we propose a fast fixed
point algorithm based on the quasi-Newton method for solving this class of
problem, and apply it in the field of TV-based image deblurring. The novel
method is derived from the idea of the quasi-Newton method, and the fixed-point
algorithms based on the proximity operator, which were widely investigated very
recently. Utilizing the non-expansion property of the proximity operator we
further investigate the global convergence of the proposed algorithm. Numerical
experiments on image deblurring problem with additive or multiplicative noise
are presented to demonstrate that the proposed algorithm is superior to the
recently developed fixed-point algorithm in the computational efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4442</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4442</id><created>2014-12-14</created><authors><author><keyname>Peng</keyname><forenames>T.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author></authors><title>Adaptive Delay-Tolerant DSTBC in Opportunistic Relaying Cooperative MIMO
  Systems</title><categories>cs.IT math.IT</categories><comments>3 figures, 6 pages, ISWCS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An adaptive delay-tolerant distributed space-time coding (DSTC) scheme with
feedback is proposed for two-hop cooperative multiple-input multiple-output
(MIMO) networks using an amplify-and-forward strategy and opportunistic
relaying algorithms. Maximum likelihood receivers and adjustable code matrices
are considered subject to a power constraint. In the proposed delay-tolerant
DSTC scheme, an adjustable code matrix is employed to transform the space-time
coded matrices at the relay nodes. Stochastic gradient algorithms are developed
with reduced computational complexity to estimate the parameters of the code
matrix. Simulation results show that the proposed algorithms obtain significant
performance gains and address the delay issue in cooperative MIMO systems as
compared to existing DSTC schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4444</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4444</id><created>2014-12-14</created><authors><author><keyname>Kosut</keyname><forenames>Oliver</forenames></author><author><keyname>Sankar</keyname><forenames>Lalitha</forenames></author></authors><title>Asymptotics and Non-asymptotics for Universal Fixed-to-Variable Source
  Coding</title><categories>cs.IT math.IT</categories><comments>32 pages, 1 figure. Submitted to IEEE Transactions on Information
  Theory, Dec. 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Universal fixed-to-variable lossless source coding for memoryless sources is
studied in the finite blocklength and higher-order asymptotics regimes. Optimal
third-order coding rates are derived for general fixed-to-variable codes and
for prefix codes. It is shown that the non-prefix Type Size code, in which
codeword lengths are chosen in ascending order of type class size, achieves the
optimal third-order rate and outperforms classical Two-Stage codes. Converse
results are proved making use of a result on the distribution of the empirical
entropy and Laplace's approximation. Finally, the fixed-to-variable coding
problem without a prefix constraint is shown to be essentially the same as the
universal guessing problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4446</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4446</id><created>2014-12-14</created><updated>2015-02-09</updated><authors><author><keyname>Ajakan</keyname><forenames>Hana</forenames></author><author><keyname>Germain</keyname><forenames>Pascal</forenames></author><author><keyname>Larochelle</keyname><forenames>Hugo</forenames></author><author><keyname>Laviolette</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Marchand</keyname><forenames>Mario</forenames></author></authors><title>Domain-Adversarial Neural Networks</title><categories>stat.ML cs.LG cs.NE</categories><comments>The first version of this paper was accepted at the &quot;Second Workshop
  on Transfer and Multi-Task Learning: Theory meets Practice&quot; (NIPS 2014,
  Montreal, Canada). See: https://sites.google.com/site/multitaskwsnips2014/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new representation learning algorithm suited to the context of
domain adaptation, in which data at training and test time come from similar
but different distributions. Our algorithm is directly inspired by theory on
domain adaptation suggesting that, for effective domain transfer to be
achieved, predictions must be made based on a data representation that cannot
discriminate between the training (source) and test (target) domains. We
propose a training objective that implements this idea in the context of a
neural network, whose hidden layer is trained to be predictive of the
classification task, but uninformative as to the domain of the input. Our
experiments on a sentiment analysis classification benchmark, where the target
domain data available at training time is unlabeled, show that our neural
network for domain adaption algorithm has better performance than either a
standard neural network or an SVM, even if trained on input features extracted
with the state-of-the-art marginalized stacked denoising autoencoders of Chen
et al. (2012).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4451</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4451</id><created>2014-12-14</created><authors><author><keyname>Barber</keyname><forenames>Rina Foygel</forenames></author><author><keyname>Duchi</keyname><forenames>John C.</forenames></author></authors><title>Privacy and Statistical Risk: Formalisms and Minimax Bounds</title><categories>math.ST cs.IT math.IT stat.TH</categories><comments>29 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore and compare a variety of definitions for privacy and disclosure
limitation in statistical estimation and data analysis, including (approximate)
differential privacy, testing-based definitions of privacy, and posterior
guarantees on disclosure risk. We give equivalence results between the
definitions, shedding light on the relationships between different formalisms
for privacy. We also take an inferential perspective, where---building off of
these definitions---we provide minimax risk bounds for several estimation
problems, including mean estimation, estimation of the support of a
distribution, and nonparametric density estimation. These bounds highlight the
statistical consequences of different definitions of privacy and provide a
second lens for evaluating the advantages and disadvantages of different
techniques for disclosure limitation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4456</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4456</id><created>2014-12-14</created><updated>2015-02-04</updated><authors><author><keyname>Klimm</keyname><forenames>Max</forenames></author><author><keyname>Schmand</keyname><forenames>Daniel</forenames></author></authors><title>Sharing Non-Anonymous Costs of Multiple Resources Optimally</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In cost sharing games, the existence and efficiency of pure Nash equilibria
fundamentally depends on the method that is used to share the resources' costs.
We consider a general class of resource allocation problems in which a set of
resources is used by a heterogeneous set of selfish users. The cost of a
resource is a (non-decreasing) function of the set of its users. Under the
assumption that the costs of the resources are shared by uniform cost sharing
protocols, i.e., protocols that use only local information of the resource's
cost structure and its users to determine the cost shares, we exactly quantify
the inefficiency of the resulting pure Nash equilibria. Specifically, we show
tight bounds on prices of stability and anarchy for games with only submodular
and only supermodular cost functions, respectively, and an asymptotically tight
bound for games with arbitrary set-functions. While all our upper bounds are
attained for the well-known Shapley cost sharing protocol, our lower bounds
hold for arbitrary uniform cost sharing protocols and are even valid for games
with anonymous costs, i.e., games in which the cost of each resource only
depends on the cardinality of the set of its users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4458</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4458</id><created>2014-12-14</created><updated>2015-05-16</updated><authors><author><keyname>Li</keyname><forenames>Hang</forenames></author><author><keyname>Huang</keyname><forenames>Chuan</forenames></author><author><keyname>Alsaadi</keyname><forenames>Fuad</forenames></author><author><keyname>Dobaie</keyname><forenames>Abdullah M.</forenames></author><author><keyname>Cui</keyname><forenames>Shuguang</forenames></author></authors><title>Opportunistic Multi-Channel Access in Heterogeneous 5G Network with
  Renewable Energy Supplies</title><categories>cs.IT math.IT</categories><comments>28 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A heterogeneous system, where small networks (e.g., small cell or WiFi) boost
the system throughput under the umbrella of a large network (e.g., large cell),
is a promising architecture for the 5G wireless communication networks, where
green and sustainable communication is also a key aspect. Renewable energy
based communication via energy harvesting (EH) devices is one of such green
technology candidates. In this paper, we study an uplink transmission scenario
under a heterogeneous network hierarchy, where each mobile user (MU) is powered
by a sustainable energy supply, capable of both deterministic access to the
large network via one private channel, and dynamic access to a small network
with certain probability via one common channel shared by multiple MUs.
Considering a general EH model, i.e., energy arrivals are time-correlated, we
study an opportunistic transmission scheme and aim to maximize the average
throughput for each MU, which jointly exploits the statistics and current
states of the private channel, common channel, battery level, and EH rate.
Applying a simple yet efficient &quot;save-then-transmit&quot; scheme, the throughput
maximization problem is cast as a &quot;rate-of-return&quot; optimal stopping problem.
The optimal stopping rule is proved to has a time-dependent threshold-based
structure for the case with general Markovian system dynamics, and degrades to
a pure threshold policy for the case with independent and identically
distributed system dynamics. As performance benchmarks, the optimal power
allocation scheme with conventional power supplies is also examined. Finally,
numerical results are presented, and a new concept of &quot;EH diversity&quot; is
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4463</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4463</id><created>2014-12-15</created><updated>2015-04-27</updated><authors><author><keyname>Praveen</keyname><forenames>M.</forenames></author><author><keyname>Srivathsan</keyname><forenames>B.</forenames></author></authors><title>Defining relations on graphs: how hard is it in the presence of node
  partitions?</title><categories>cs.DB</categories><comments>Small corrections based on reviews from PODS 2015, results unchanged</comments><acm-class>F.4.3, H.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing query languages for graph structured data is an active field of
research. Evaluating a query on a graph results in a relation on the set of its
nodes. In other words, a query is a mechanism for defining relations on a
graph. Some relations may not be definable by any query in a given language.
This leads to the following question: given a graph, a query language and a
relation on the graph, does there exist a query in the language that defines
the relation? This is called the definability problem. When the given query
language is standard regular expressions, the definability problem is known to
be PSPACE-complete.
  The model of graphs can be extended by labeling nodes with values from an
infinite domain. These labels induce a partition on the set of nodes: two nodes
are equivalent if they are labeled by the same value. Query languages can also
be extended to make use of this equivalence. Two such extensions are Regular
Expressions with Memory (REM) and Regular Expressions with Equality (REE).
  In this paper, we study the complexity of the definability problem in this
extended model when the query language is either REM or REE. We show that the
definability problem is EXPSPACE-complete when the query language is REM, and
it is PSPACE-complete when the query language is REE. In addition, when the
query language is a union of conjunctive queries based on REM or REE, we show
coNP-completeness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4465</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4465</id><created>2014-12-15</created><authors><author><keyname>Sepahvand</keyname><forenames>Mostafa</forenames></author><author><keyname>Alikhajeh</keyname><forenames>Ghasem</forenames></author><author><keyname>Ghaffari</keyname><forenames>Meysam</forenames></author><author><keyname>Mirzaei</keyname><forenames>Abdolreza</forenames></author></authors><title>Generating Graphical Chain by Mutual Matching of Bayesian Network and
  Extracted Rules of Bayesian Network Using Genetic Algorithm</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the technology development, the need of analyze and extraction of useful
information is increasing. Bayesian networks contain knowledge from data and
experts that could be used for decision making processes But they are not
easily understandable thus the rule extraction methods have been used but they
have high computation costs. To overcome this problem we extract rules from
Bayesian network using genetic algorithm. Then we generate the graphical chain
by mutually matching the extracted rules and Bayesian network. This graphical
chain could shows the sequence of events that lead to the target which could
help the decision making process. The experimental results on small networks
show that the proposed method has comparable results with brute force method
which has a significantly higher computation cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4470</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4470</id><created>2014-12-15</created><authors><author><keyname>Mahdi</keyname><forenames>Walid</forenames></author><author><keyname>Chen</keyname><forenames>Liming</forenames></author><author><keyname>Ardebilian</keyname><forenames>Mohsen</forenames></author></authors><title>Automatic video scene segmentation based on spatial-temporal clues and
  rhythm</title><categories>cs.CV</categories><comments>25 pages, 12 figures</comments><report-no>ISBN-10: 1903996228 ISBN-10: 1903996228 ISBN-13: 9781903996225</report-no><journal-ref>Video DATA Hermes Science Publishing, 2002</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With ever increasing computing power and data storage capacity, the potential
for large digital video libraries is growing rapidly.However, the massive use
of video for the moment is limited by its opaque characteristics. Indeed, a
user who has to handle and retrieve sequentially needs too much time in order
to find out segments of interest within a video. Therefore, providing an
environment both convenient and efficient for video storing and retrieval,
especially for content-based searching as this exists in traditional textbased
database systems, has been the focus of recent and important efforts of a large
research community
  In this paper, we propose a new automatic video scene segmentation method
that explores two main video features; these are spatial-temporal relationship
and rhythm of shots. The experimental evidence we obtained from a 80
minutevideo showed that our prototype provides very high accuracy for video
segmentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4471</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4471</id><created>2014-12-15</created><updated>2015-05-01</updated><authors><author><keyname>Kosolobov</keyname><forenames>Dmitry</forenames></author></authors><title>Online Detection of Repetitions with Backtracking</title><categories>cs.DS</categories><comments>12 pages, 5 figures, accepted to CPM 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present two algorithms for the following problem: given a
string and a rational $e &gt; 1$, detect in the online fashion the earliest
occurrence of a repetition of exponent $\ge e$ in the string.
  1. The first algorithm supports the backtrack operation removing the last
letter of the input string. This solution runs in $O(n\log m)$ time and $O(m)$
space, where $m$ is the maximal length of a string generated during the
execution of a given sequence of $n$ read and backtrack operations.
  2. The second algorithm works in $O(n\log\sigma)$ time and $O(n)$ space,
where $n$ is the length of the input string and $\sigma$ is the number of
distinct letters. This algorithm is relatively simple and requires much less
memory than the previously known solution with the same working time and space.
a string generated during the execution of a given sequence of $n$ read and
backtrack operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4474</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4474</id><created>2014-12-15</created><authors><author><keyname>Thampi</keyname><forenames>Ajay</forenames></author><author><keyname>Liew</keyname><forenames>Soung Chang</forenames></author><author><keyname>Armour</keyname><forenames>Simon</forenames></author><author><keyname>Fan</keyname><forenames>Zhong</forenames></author><author><keyname>You</keyname><forenames>Lizhao</forenames></author><author><keyname>Kaleshi</keyname><forenames>Dritan</forenames></author></authors><title>Physical-layer Network Coding in Two-Way Heterogeneous Cellular Networks
  with Power Imbalance</title><categories>cs.NI cs.IT math.IT</categories><comments>Manuscript submitted to IEEE Transactions on Vehicular Technology</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The growing demand for high-speed data, quality of service (QoS) assurance
and energy efficiency has triggered the evolution of 4G LTE-A networks to 5G
and beyond. Interference is still a major performance bottleneck. This paper
studies the application of physical-layer network coding (PNC), a technique
that exploits interference, in heterogeneous cellular networks. In particular,
we propose a rate-maximising relay selection algorithm for a single cell with
multiple relays based on the decode-and-forward strategy. With nodes
transmitting at different powers, the proposed algorithm adapts the resource
allocation according to the differing link rates and we prove theoretically
that the optimisation problem is log-concave. The proposed technique is shown
to perform significantly better than the widely studied selection-cooperation
technique. We then undertake an experimental study on a software radio platform
of the decoding performance of PNC with unbalanced SNRs in the multiple-access
transmissions. This problem is inherent in cellular networks and it is shown
that with channel coding and decoders based on multiuser detection and
successive interference cancellation, the performance is better with power
imbalance. This paper paves the way for further research in multi-cell PNC,
resource allocation, and the implementation of PNC with higher-order
modulations and advanced coding techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4480</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4480</id><created>2014-12-15</created><updated>2015-04-21</updated><authors><author><keyname>Zheng</keyname><forenames>Long</forenames></author><author><keyname>Liao</keyname><forenames>Xiaofei</forenames></author><author><keyname>He</keyname><forenames>Bingsheng</forenames></author><author><keyname>Wu</keyname><forenames>Song</forenames></author><author><keyname>Jin</keyname><forenames>Hai</forenames></author></authors><title>On Performance Debugging of Unnecessary Lock Contentions on Multicore
  Processors: A Replay-based Approach</title><categories>cs.PL</categories><comments>18 pages, 19 figures, 3 tables</comments><acm-class>D.2.5; B.8.2; D.4.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Locks have been widely used as an effective synchronization mechanism among
processes and threads. However, we observe that a large number of false
inter-thread dependencies (i.e., unnecessary lock contentions) exist during the
program execution on multicore processors, thereby incurring significant
performance overhead. This paper presents a performance debugging framework,
PERFPLAY, to facilitate a comprehensive and in-depth understanding of the
performance impact of unnecessary lock contentions. The core technique of our
debugging framework is trace replay. Specifically, PERFPLAY records the program
execution trace, on the basis of which the unnecessary lock contentions can be
identified through trace analysis. We then propose a novel technique of trace
transformation to transform these identified unnecessary lock contentions in
the original trace into the correct pattern as a new trace free of unnecessary
lock contentions. Through replaying both traces, PERFPLAY can quantify the
performance impact of unnecessary lock contentions. To demonstrate the
effectiveness of our debugging framework, we study five real-world programs and
PARSEC benchmarks. Our experimental results demonstrate the significant
performance overhead of unnecessary lock contentions, and the effectiveness of
PERFPLAY in identifying the performance critical unnecessary lock contentions
in real applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4485</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4485</id><created>2014-12-15</created><authors><author><keyname>Rudolph</keyname><forenames>Sebastian</forenames></author><author><keyname>Thomazo</keyname><forenames>Micha&#xeb;l</forenames></author><author><keyname>Baget</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author><author><keyname>Mugnier</keyname><forenames>Marie-Laure</forenames></author></authors><title>Worst-case Optimal Query Answering for Greedy Sets of Existential Rules
  and Their Subclasses</title><categories>cs.AI cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The need for an ontological layer on top of data, associated with advanced
reasoning mechanisms able to exploit the semantics encoded in ontologies, has
been acknowledged both in the database and knowledge representation
communities. We focus in this paper on the ontological query answering problem,
which consists of querying data while taking ontological knowledge into
account. More specifically, we establish complexities of the conjunctive query
entailment problem for classes of existential rules (also called
tuple-generating dependencies, Datalog+/- rules, or forall-exists-rules. Our
contribution is twofold. First, we introduce the class of greedy
bounded-treewidth sets (gbts) of rules, which covers guarded rules, and their
most well-known generalizations. We provide a generic algorithm for query
entailment under gbts, which is worst-case optimal for combined complexity with
or without bounded predicate arity, as well as for data complexity and query
complexity. Secondly, we classify several gbts classes, whose complexity was
unknown, with respect to combined complexity (with both unbounded and bounded
predicate arity) and data complexity to obtain a comprehensive picture of the
complexity of existential rule fragments that are based on diverse guardedness
notions. Upper bounds are provided by showing that the proposed algorithm is
optimal for all of them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4510</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4510</id><created>2014-12-15</created><updated>2015-03-11</updated><authors><author><keyname>Tridenski</keyname><forenames>Sergey</forenames></author><author><keyname>Zamir</keyname><forenames>Ram</forenames></author></authors><title>Stochastic Interpretation for the Arimoto Algorithm</title><categories>cs.IT math.IT</categories><comments>5 pages, 1 figure, accepted for 2015 IEEE Information Theory
  Workshop, Jerusalem, Israel</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Arimoto algorithm computes the Gallager function $\max_Q
{E}_{0}^{}(\rho,Q)$ for a given channel ${P}_{}^{}(y \,|\, x)$ and parameter
$\rho$, by means of alternating maximization. Along the way, it generates a
sequence of input distributions ${Q}_{1}^{}(x)$, ${Q}_{2}^{}(x)$, ... , that
converges to the maximizing input ${Q}_{}^{*}(x)$. We propose a stochastic
interpretation for the Arimoto algorithm. We show that for a random (i.i.d.)
codebook with a distribution ${Q}_{k}^{}(x)$, the next distribution
${Q}_{k+1}^{}(x)$ in the Arimoto algorithm is equal to the type (${Q}'$) of the
feasible transmitted codeword that maximizes the conditional Gallager exponent
(conditioned on a specific transmitted codeword type ${Q}'$). This
interpretation is a first step toward finding a stochastic mechanism for
on-line channel input adaptation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4514</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4514</id><created>2014-12-15</created><authors><author><keyname>Zahavi</keyname><forenames>Daniel</forenames></author><author><keyname>Zhang</keyname><forenames>Lili</forenames></author><author><keyname>Maric</keyname><forenames>Ivana</forenames></author><author><keyname>Dabora</keyname><forenames>Ron</forenames></author><author><keyname>Goldsmith</keyname><forenames>Andrea J.</forenames></author><author><keyname>Cui</keyname><forenames>Shuguang</forenames></author></authors><title>Diversity-Multiplexing Tradeoff for the Interference Channel with a
  Relay</title><categories>cs.IT math.IT</categories><comments>Accepted to the IEEE Transactions on Information Theory</comments><doi>10.1109/TIT.2014.2381664</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the diversity-multiplexing tradeoff (DMT) for the slow fading
interference channel with a relay (ICR). We derive four inner bounds on the DMT
region: the first is based on the compress-and-forward (CF) relaying scheme,
the second is based on the decode-and-forward (DF) relaying scheme, and the
last two bounds are based on the half-duplex (HD) and full-duplex (FD)
amplify-and-forward (AF) schemes. For the CF and DF schemes, we find conditions
on the channel parameters and the multiplexing gains, under which the
corresponding inner bound achieves the optimal DMT region. We also identify
cases in which the DMT region of the ICR corresponds to that of two parallel
slow fading relay channels, implying that interference does not decrease the
DMT for each pair, and that a single relay can be DMT-optimal for two pairs
simultaneously. For the HD-AF scheme we derive conditions on the channel
coefficients under which the proposed scheme achieves the optimal DMT for the
AF-based relay channel. Lastly, we identify conditions under which adding a
relay strictly enlarges the DMT region relative to the interference channel
without a relay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4526</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4526</id><created>2014-12-15</created><updated>2014-12-15</updated><authors><author><keyname>Li</keyname><forenames>Hongsheng</forenames></author><author><keyname>Zhao</keyname><forenames>Rui</forenames></author><author><keyname>Wang</keyname><forenames>Xiaogang</forenames></author></authors><title>Highly Efficient Forward and Backward Propagation of Convolutional
  Neural Networks for Pixelwise Classification</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present highly efficient algorithms for performing forward and backward
propagation of Convolutional Neural Network (CNN) for pixelwise classification
on images. For pixelwise classification tasks, such as image segmentation and
object detection, surrounding image patches are fed into CNN for predicting the
classes of centered pixels via forward propagation and for updating CNN
parameters via backward propagation. However, forward and backward propagation
was originally designed for whole-image classification. Directly applying it to
pixelwise classification in a patch-by-patch scanning manner is extremely
inefficient, because surrounding patches of pixels have large overlaps, which
lead to a lot of redundant computation.
  The proposed algorithms eliminate all the redundant computation in
convolution and pooling on images by introducing novel d-regularly sparse
kernels. It generates exactly the same results as those by patch-by-patch
scanning. Convolution and pooling operations with such kernels are able to
continuously access memory and can run efficiently on GPUs. A fraction of
patches of interest can be chosen from each training image for backward
propagation by applying a mask to the error map at the last CNN layer. Its
computation complexity is constant with respect to the number of patches
sampled from the image. Experiments have shown that our proposed algorithms
speed up commonly used patch-by-patch scanning over 1500 times in both forward
and backward propagation. The speedup increases with the sizes of images and
patches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4535</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4535</id><created>2014-12-15</created><authors><author><keyname>Garcia-Saavedra</keyname><forenames>Andres</forenames></author><author><keyname>Banchs</keyname><forenames>Albert</forenames></author><author><keyname>Serrano</keyname><forenames>Pablo</forenames></author><author><keyname>Widmer</keyname><forenames>Joerg</forenames></author></authors><title>Adaptive Mechanism for Distributed Opportunistic Scheduling</title><categories>cs.NI cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed Opportunistic Scheduling (DOS) techniques have been recently
proposed to improve the throughput performance of wireless networks. With DOS,
each station contends for the channel with a certain access probability. If a
contention is successful, the station measures the channel conditions and
transmits in case the channel quality is above a certain threshold. Otherwise,
the station does not use the transmission opportunity, allowing all stations to
recontend. A key challenge with DOS is to design a distributed algorithm that
optimally adjusts the access probability and the threshold of each station. To
address this challenge, in this paper we first compute the configuration of
these two parameters that jointly optimizes throughput performance in terms of
proportional fairness. Then, we propose an adaptive algorithm based on control
theory that converges to the desired point of operation. Finally, we conduct a
control theoretic analysis of the algorithm to find a setting for its
parameters that provides a good tradeoff between stability and speed of
convergence. Simulation results validate the design of the proposed mechanism
and confirm its advantages over previous proposals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4538</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4538</id><created>2014-12-15</created><authors><author><keyname>Laursen</keyname><forenames>Johan S.</forenames></author><author><keyname>Buch</keyname><forenames>Jacob P.</forenames></author><author><keyname>S&#xf8;rensen</keyname><forenames>Lars C.</forenames></author><author><keyname>Kraft</keyname><forenames>Dirk</forenames></author><author><keyname>Ellekilde</keyname><forenames>Henrik G. Petersen Lars-Peter</forenames></author><author><keyname>Schultz</keyname><forenames>Ulrik P.</forenames></author></authors><title>Towards Error Handling in a DSL for Robot Assembly Tasks</title><categories>cs.RO</categories><comments>Presented at DSLRob 2014 (arXiv:cs/1411.7148)</comments><report-no>DSLRob/2014/03</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work-in-progress paper presents our work with a domain specific language
(DSL) for tackling the issue of programming robots for small-sized batch
production. We observe that as the complexity of assembly increases so does the
likelihood of errors, and these errors need to be addressed. Nevertheless, it
is essential that programming and setting up the assembly remains fast, allows
quick changeovers, easy adjustments and reconfigurations. In this paper we
present an initial design and implementation of extending an existing DSL for
assembly operations with error specification, error handling and advanced move
commands incorporating error tolerance. The DSL is used as part of a framework
that aims at tackling uncertainties through a probabilistic approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4542</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4542</id><created>2014-12-15</created><authors><author><keyname>Balatsoukas-Stimming</keyname><forenames>Alexios</forenames></author><author><keyname>Austin</keyname><forenames>Andrew Charles Mallory</forenames></author><author><keyname>Belanovic</keyname><forenames>Pavle</forenames></author><author><keyname>Burg</keyname><forenames>Andreas</forenames></author></authors><title>Baseband and RF Hardware Impairments in Full-Duplex Wireless Systems:
  Experimental Characterisation and Suppression</title><categories>cs.IT math.IT</categories><comments>Submitted to EURASIP Journal on Wireless Communications and
  Networking (Special Issue on &quot;Experimental Evaluation in Wireless
  Communications&quot;)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hardware imperfections can significantly reduce the performance of
full-duplex wireless systems by introducing non-idealities and random effects
that make it challenging to fully suppress self-interference. Previous research
has mostly focused on analyzing the impact of hardware imperfections on
full-duplex systems, based on simulations and theoretical models. In this
paper, we follow a measurement-based approach to experimentally identify and
isolate these hardware imperfections leading to residual self-interference in
full-duplex nodes. Our measurements show the important role of images arising
from in-phase and quadrature (IQ) imbalance in the mixers. We also observe
base-band non-linearities in the digital-to-analog converters (DAC), which can
introduce strong harmonic components that have not been previously considered.
A corresponding general mathematical model to suppress these components of the
self-interference signal arising from the hardware non-idealities is developed
from the observations and measurements. Results from a 10 MHz bandwidth
full-duplex OFDM system, operating at 2.48 GHz, show up to 13 dB additional
suppression, relative to state-of-the-art implementations can be achieved by
jointly compensating for IQ imbalance and DAC non-linearities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4550</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4550</id><created>2014-12-15</created><authors><author><keyname>Adalid</keyname><forenames>Damian</forenames></author><author><keyname>Gallardo</keyname><forenames>Maria del Mar</forenames></author><author><keyname>Titolo</keyname><forenames>Laura</forenames></author></authors><title>Modeling Hybrid Systems in Hy-tccp</title><categories>cs.PL cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Concurrent,reactive and hybrid systems require quality modeling languages to
be described and analyzed. The Timed Concurrent Constraint Language (tccp) was
introduced as a simple but powerful model for reactive systems. In this paper,
we present hybrid tccp (hy-tccp), an extension of tccp over continuous time
which includes new con- structs to model the continuous dynamics of hybrid
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4556</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4556</id><created>2014-12-15</created><authors><author><keyname>Varghese</keyname><forenames>Blesson</forenames></author><author><keyname>Barker</keyname><forenames>Adam</forenames></author></authors><title>Are Clouds Ready to Accelerate Ad hoc Financial Simulations?</title><categories>cs.DC cs.CE</categories><comments>Best paper nominee at the International Symposium on Big Data
  Computing (BDC 2014) in conjunction with IEEE/ACM Utility and Cloud Computing
  (UCC), 2014 London, UK</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Applications employed in the financial services industry to capture and
estimate a variety of risk metrics are underpinned by stochastic simulations
which are data, memory and computationally intensive. Many of these simulations
are routinely performed on production-based computing systems. Ad hoc
simulations in addition to routine simulations are required to obtain
up-to-date views of risk metrics. Such simulations are currently not performed
as they cannot be accommodated on production clusters, which are typically over
committed resources. Scalable, on-demand and pay-as-you go Virtual Machines
(VMs) offered by the cloud are a potential platform to satisfy the data, memory
and computational constraints of the simulation. However, &quot;Are clouds ready to
accelerate ad hoc financial simulations?&quot;
  The research reported in this paper aims to experimentally verify this
question by developing and deploying an important financial simulation,
referred to as 'Aggregate Risk Analysis' on the cloud. Parallel techniques to
improve efficiency and performance of the simulations are explored. Challenges
such as accommodating large input data on limited memory VMs and rapidly
processing data for real-time use are surmounted. The key result of this
investigation is that Aggregate Risk Analysis can be accommodated on cloud VMs.
Acceleration of up to 24x using multiple hardware accelerators over the
implementation on a single accelerator, 6x over a multiple core implementation
and approximately 60x over a baseline implementation was achieved on the cloud.
However, computational time is wasted for every dollar spent on the cloud due
to poor acceleration over multiple virtual cores. Interestingly, private VMs
can offer better performance than public VMs on comparable underlying hardware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4564</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4564</id><created>2014-12-15</created><updated>2015-06-21</updated><authors><author><keyname>Vedaldi</keyname><forenames>Andrea</forenames></author><author><keyname>Lenc</keyname><forenames>Karel</forenames></author></authors><title>MatConvNet - Convolutional Neural Networks for MATLAB</title><categories>cs.CV cs.LG cs.MS cs.NE</categories><comments>Updated for release v1.0-beta12</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MatConvNet is an implementation of Convolutional Neural Networks (CNNs) for
MATLAB. The toolbox is designed with an emphasis on simplicity and flexibility.
It exposes the building blocks of CNNs as easy-to-use MATLAB functions,
providing routines for computing linear convolutions with filter banks, feature
pooling, and many more. In this manner, MatConvNet allows fast prototyping of
new CNN architectures; at the same time, it supports efficient computation on
CPU and GPU allowing to train complex models on large datasets such as ImageNet
ILSVRC. This document provides an overview of CNNs and how they are implemented
in MatConvNet and gives the technical details of each computational block in
the toolbox.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4576</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4576</id><created>2014-12-15</created><authors><author><keyname>Azghani</keyname><forenames>Masoumeh</forenames></author><author><keyname>Karimi</keyname><forenames>Mostafa</forenames></author><author><keyname>Marvasti</keyname><forenames>Farokh</forenames></author></authors><title>Multi-Hypothesis Compressed Video Sensing Technique</title><categories>cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a compressive sampling and Multi-Hypothesis (MH)
reconstruction strategy for video sequences which has a rather simple encoder,
while the decoding system is not that complex. We introduce a convex cost
function that incorporates the MH technique with the sparsity constraint and
the Tikhonov regularization. Consequently, we derive a new iterative algorithm
based on these criteria. This algorithm surpasses its counterparts (Elasticnet
and Tikhonov) in the recovery performance. Besides it is computationally much
faster than the Elasticnet and comparable to the Tikhonov. Our extensive
simulation results confirm these claims.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4582</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4582</id><created>2014-12-15</created><authors><author><keyname>Mohammed</keyname><forenames>Bashir</forenames></author><author><keyname>Kiran</keyname><forenames>Mariam</forenames></author></authors><title>Experimental Report on Setting up a Cloud Computing Environment at the
  University of Bradford</title><categories>cs.DC</categories><comments>19 pages, 16 figures</comments><acm-class>C.0; C.1.4</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Cloud computing is increasingly attracting large attention in computing both
in academic research and in industrial initiatives. Emerging as a popular
paradigm and an attractive model of providing computing, information technology
(IT) infrastructure, network and storage to large and small enterprises both in
private and public sectors. This project was initiated and aimed at designing
and Setting up a basic Cloud lab Testbed running on Open stack under Virtual
box for experiments and Hosting Cloud Platforms in the networking laboratory at
the University of Bradford. This report presents the methodology of setting up
a cloud lab testbed for experiment running on open stack. Current resources, in
the Networking lab at the university were used and turned into virtual
platforms for cloud computing testing. This report serves as a practical
guideline, concentrating on the practical infrastructure related questions and
issues, on setting up a cloud lab for testing and proof of concept. Finally the
report proposes an experimental validation showing feasibility of migrating to
cloud.
  The primary focus of this report is to provide a brief background on
different theoretical concepts of cloud computing, particularly virtualisation,
and then it elaborates on the practical aspects concerning the setup and
implementation of a Cloud lab test bed using open source solutions. This
reports serves as a reference for institutions looking at the possibilities of
implementing cloud solutions, in order to benefit from getting the basics and a
view on the different aspects of cloud migration concepts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4586</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4586</id><created>2014-12-15</created><authors><author><keyname>Enqvist</keyname><forenames>Sebastian</forenames></author><author><keyname>Sourabh</keyname><forenames>Sumit</forenames></author></authors><title>Generalized Vietoris Bisimulations</title><categories>cs.LO math.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce and study bisimulations for coalgebras on Stone spaces [14]. Our
notion of bisimulation is sound and complete for behavioural equivalence, and
generalizes Vietoris bisimulations [4]. The main result of our paper is that
bisimulation for a $\mathbf{Stone}$ coalgebra is the topological closure of
bisimulation for the underlying $\mathbf{Set}$ coalgebra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4597</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4597</id><created>2014-12-15</created><authors><author><keyname>Rao</keyname><forenames>Xiongbin</forenames></author><author><keyname>Lau</keyname><forenames>Vincent K. N.</forenames></author></authors><title>Distributed Fronthaul Compression and Joint Signal Recovery in Cloud-RAN</title><categories>cs.IT math.IT</categories><comments>full version with detailed proof, accepted for publication in IEEE
  transactions on signal processing</comments><doi>10.1109/TSP.2014.2386290</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The cloud radio access network (C-RAN) is a promising network architecture
for future mobile communications, and one practical hurdle for its large scale
implementation is the stringent requirement of high capacity and low latency
fronthaul connecting the distributed remote radio heads (RRH) to the
centralized baseband pools (BBUs) in the C-RAN. To improve the scalability of
C-RAN networks, it is very important to take the fronthaul loading into
consideration in the signal detection, and it is very desirable to reduce the
fronthaul loading in C-RAN systems. In this paper, we consider uplink C-RAN
systems and we propose a distributed fronthaul compression scheme at the
distributed RRHs and a joint recovery algorithm at the BBUs by deploying the
techniques of distributed compressive sensing (CS). Different from conventional
distributed CS, the CS problem in C-RAN system needs to incorporate the
underlying effect of multi-access fading for the end-to-end recovery of the
transmitted signals from the users. We analyze the performance of the proposed
end-to-end signal recovery algorithm and we show that the aggregate measurement
matrix in C-RAN systems, which contains both the distributed fronthaul
compression and multiaccess fading, can still satisfy the restricted isometry
property with high probability. Based on these results, we derive tradeoff
results between the uplink capacity and the fronthaul loading in C-RAN systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4616</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4616</id><created>2014-12-15</created><authors><author><keyname>Weninger</keyname><forenames>Felix</forenames></author><author><keyname>Schuller</keyname><forenames>Bj&#xf6;rn</forenames></author><author><keyname>Eyben</keyname><forenames>Florian</forenames></author><author><keyname>W&#xf6;llmer</keyname><forenames>Martin</forenames></author><author><keyname>Rigoll</keyname><forenames>Gerhard</forenames></author></authors><title>A Broadcast News Corpus for Evaluation and Tuning of German LVCSR
  Systems</title><categories>cs.CL cs.SD</categories><comments>submitted to INTERSPEECH 2010 on May 3, 2010</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Transcription of broadcast news is an interesting and challenging application
for large-vocabulary continuous speech recognition (LVCSR). We present in
detail the structure of a manually segmented and annotated corpus including
over 160 hours of German broadcast news, and propose it as an evaluation
framework of LVCSR systems. We show our own experimental results on the corpus,
achieved with a state-of-the-art LVCSR decoder, measuring the effect of
different feature sets and decoding parameters, and thereby demonstrate that
real-time decoding of our test set is feasible on a desktop PC at 9.2% word
error rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4620</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4620</id><created>2014-12-15</created><updated>2014-12-15</updated><authors><author><keyname>Lawton</keyname><forenames>Neal</forenames></author></authors><title>An Important Corollary for the Fast Solution of Dynamic Maximal Clique
  Enumeration Problems</title><categories>cs.DM</categories><comments>5 pages</comments><msc-class>68R10</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we modify an algorithm for updating a maximal clique
enumeration after an edge insertion to provide an algorithm that runs in linear
time with respect to the number of cliques containing one of the edge's
endpoints, whereas existing algorithms take quadratic time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4626</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4626</id><created>2014-12-15</created><authors><author><keyname>Augot</keyname><forenames>Daniel</forenames><affiliation>LIX, INRIA Saclay - Ile de France</affiliation></author><author><keyname>Finiasz</keyname><forenames>Matthieu</forenames></author></authors><title>Direct Construction of Recursive MDS Diffusion Layers using Shortened
  BCH Codes</title><categories>cs.CR cs.IT math.IT</categories><comments>Best paper award; Carlos Cid and Christian Rechberger. 21st
  International Workshop on Fast Software Encryption, FSE 2014, Mar 2014,
  London, United Kingdom. springer</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MDS matrices allow to build optimal linear diffusion layers in block ciphers.
However, MDS matrices cannot be sparse and usually have a large description,
inducing costly software/hardware implementations. Recursive MDS matrices allow
to solve this problem by focusing on MDS matrices that can be computed as a
power of a simple companion matrix, thus having a compact description suitable
even for constrained environ- ments. However, up to now, finding recursive MDS
matrices required to perform an exhaustive search on families of companion
matrices, thus limiting the size of MDS matrices one could look for. In this
article we propose a new direct construction based on shortened BCH codes, al-
lowing to efficiently construct such matrices for whatever parameters.
Unfortunately, not all recursive MDS matrices can be obtained from BCH codes,
and our algorithm is not always guaranteed to find the best matrices for a
given set of parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4629</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4629</id><created>2014-12-15</created><authors><author><keyname>Estef&#xf3;</keyname><forenames>Pablo</forenames></author><author><keyname>Campusano</keyname><forenames>Miguel</forenames></author><author><keyname>Fabresse</keyname><forenames>Luc</forenames></author><author><keyname>Fabry</keyname><forenames>Johan</forenames></author><author><keyname>Laval</keyname><forenames>Jannik</forenames></author><author><keyname>Bouraqad</keyname><forenames>Noury</forenames></author></authors><title>Towards Live Programming in ROS with PhaROS and LRP</title><categories>cs.PL cs.RO</categories><comments>Presented at DSLRob 2014 (arXiv:cs/1411.7148)</comments><report-no>DSLRob/2014/06</report-no><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In traditional robot behavior programming, the
edit-compile-simulate-deploy-run cycle creates a large mental disconnect
between program creation and eventual robot behavior. This significantly slows
down behavior development because there is no immediate mental connection
between the program and the resulting behavior. With live programming the
development cycle is made extremely tight, realizing such an immediate
connection. In our work on programming of ROS robots in a more dynamic fashion
through PhaROS, we have experimented with the use of the Live Robot Programming
language. This has given rise to a number of requirements for such live
programming of robots. In this text we introduce these requirements and
illustrate them using an example robot behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4638</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4638</id><created>2014-12-15</created><authors><author><keyname>Skjegstad</keyname><forenames>Magnus</forenames></author><author><keyname>Madhavapeddy</keyname><forenames>Anil</forenames></author><author><keyname>Crowcroft</keyname><forenames>Jon</forenames></author></authors><title>Kadupul: Livin' on the Edge with Virtual Currencies and Time-Locked
  Puzzles</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Devices connected to the Internet today have a wide range of local
communication channels available, such as wireless Wifi, Bluetooth or NFC, as
well as wired backhaul. In densely populated areas it is possible to create
heterogeneous, multihop communication paths using a combination of these
technologies, and often transmit data with lower latency than via a wired
Internet connection. However, the potential for sharing meshed wireless radios
in this way has never been realised due to the lack of economic incentives to
do so on the part of individual nodes.
  In this paper, we explore how virtual currencies such as Bitcoin might be
used to provide an end-to-end incentive scheme to convince forwarding nodes
that it is profitable to send packets on via the lowest latency mechanism
available. Clients inject a small amount of money to transmit a datagram, and
forwarding engines compete to solve a time-locked puzzle that can be claimed by
the node that delivers the result in the lowest latency. This approach
naturally extends congestion control techniques to a surge pricing model when
available bandwidth is low. We conclude by discussing several latency-sensitive
applications that would benefit for this, such as video streaming and local
augmented reality systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4639</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4639</id><created>2014-12-15</created><authors><author><keyname>Gargiulo</keyname><forenames>Floriana</forenames></author><author><keyname>Bindi</keyname><forenames>Jacopo</forenames></author><author><keyname>Apolloni</keyname><forenames>Andrea</forenames></author></authors><title>The topology of a discussion: the #occupy case</title><categories>cs.SI physics.soc-ph</categories><comments>13 pages, 9 figures</comments><doi>10.1371/journal.pone.0137191</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyse a large sample of the Twitter activity developed around the social
movement 'Occupy Wall Street' to study the complex interactions between the
human communication activity and the semantic content of a discussion. We use a
network approach based on the analysis of the bipartite graph @Users-#Hashtags
and of its projections: the 'semantic network', whose nodes are hashtags, and
the 'users interest network', whose nodes are users In the first instance, we
find out that discussion topics (#hashtags) present a high heterogeneity, with
the distinct role of the communication hubs where most the 'opinion traffic'
passes through. In the second case, the self-organization process of users
activity leads to the emergence of two classes of communicators: the
'professionals' and the 'amateurs'. Moreover the network presents a strong
community structure, based on the differentiation of the semantic topics, and a
high level of structural robustness when a certain set of topics are censored
and/or accounts are removed. Analysing the characteristics the @Users-#Hashtags
network we can distinguish three phases of the discussion about the movement.
Each phase corresponds to specific moment of the movement: from declaration of
intent, organisation and development and the final phase of political
reactions. Each phase is characterised by the presence of specific #hashtags in
the discussion. Keywords: Twitter, Network analysis
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4643</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4643</id><created>2014-12-15</created><updated>2015-05-26</updated><authors><author><keyname>DeDeo</keyname><forenames>Simon</forenames></author></authors><title>Wrong side of the tracks: Big Data and Protected Categories</title><categories>cs.IT cs.CY math.IT physics.soc-ph stat.ME</categories><comments>9 pages. Extended discussion</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When we use machine learning for public policy, we find that many useful
variables are associated with others on which it would be ethically problematic
to base decisions. This problem becomes particularly acute in the Big Data era,
when predictions are often made in the absence of strong theories for
underlying causal mechanisms. We describe the dangers to democratic
decision-making when high-performance algorithms fail to provide an explicit
account of causation. We then demonstrate how information theory allows us to
degrade predictions so that they decorrelate from protected variables with
minimal loss of accuracy. Enforcing total decorrelation is at best a near-term
solution, however. The role of causal argument in ethical debate urges the
development of new, interpretable machine-learning algorithms that reference
causal mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4646</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4646</id><created>2014-12-15</created><updated>2015-12-23</updated><authors><author><keyname>Crochemore</keyname><forenames>Maxime</forenames></author><author><keyname>Mercas</keyname><forenames>Robert</forenames></author></authors><title>Fewer runs than word length</title><categories>cs.DM cs.DS cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The work takes another look at the number of runs that a string might contain
and provides an alternative proof for the bound. We also propose another
stronger conjecture that states that, for a fixed order on the alphabet, within
every factor of a word there are at most as many occurrences of Lyndon roots
corresponding to runs in a word as the length of the factor (only first such
occurrences for each run are considered).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4648</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4648</id><created>2014-12-15</created><updated>2014-12-16</updated><authors><author><keyname>Garousi</keyname><forenames>Vahid</forenames></author><author><keyname>Co&#x15f;kun&#xe7;ay</keyname><forenames>Ahmet</forenames></author><author><keyname>Betin-Can</keyname><forenames>Aysu</forenames></author><author><keyname>Demir&#xf6;rs</keyname><forenames>Onur</forenames></author></authors><title>A Survey of Software Engineering Practices in Turkey (extended version)</title><categories>cs.SE</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Context: Understanding the types of software engineering practices and
techniques used in the industry is important. There is a wide spectrum in terms
of the types and maturity of software engineering practices conducted in each
software team and company. To characterize the type of software engineering
practices conducted in software firms, a variety of surveys have been conducted
in different countries and regions. Turkey has a vibrant software industry and
it is important to characterize and understand the state of software
engineering practices in this industry. Objective: Our objective is to
characterize and grasp a high-level view on type of software engineering
practices in the Turkish software industry. Among the software engineering
practices that we have surveyed in this study are the followings: software
requirements, design, development, testing, maintenance, configuration
management, release planning and support practices. The current survey is the
most comprehensive of its type ever conducted in the context of Turkish
software industry. Method: To achieve the above objective, we systematically
designed an online survey with 46 questions based on our past experience in the
Canadian and Turkish contexts and using the Software Engineering Body of
Knowledge (SWEBOK). 202 practicing software engineers from the Turkish software
industry participated in the survey. We analyze and report in this paper the
results of the questions. Whenever possible, we also compare the trends and
results of our survey with the results of a similar 2010 survey conducted in
the Canadian software industry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4659</identifier>
 <datestamp>2015-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4659</id><created>2014-12-15</created><updated>2015-11-23</updated><authors><author><keyname>Qu</keyname><forenames>Qing</forenames></author><author><keyname>Sun</keyname><forenames>Ju</forenames></author><author><keyname>Wright</keyname><forenames>John</forenames></author></authors><title>Finding a sparse vector in a subspace: Linear sparsity using alternating
  directions</title><categories>cs.IT cs.CV cs.LG math.IT math.OC stat.ML</categories><comments>Submitted to IEEE Trans. Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of recovering the sparsest vector in a generic
subspace $\mathcal{S} \subseteq \mathbb{R}^p$ with $\mathrm{dim}(\mathcal{S})=
n &lt; p$. This problem can be considered a homogeneous variant of the sparse
recovery problem, and finds applications in sparse dictionary learning, sparse
PCA, and many other problems in signal processing and machine learning. Simple
convex heuristics for this problem provably break down when the fraction of
nonzero entries in the target sparse vector substantially exceeds
$O(1/\sqrt{n})$. In contrast, we exhibit a relatively simple nonconvex approach
based on alternating directions, which provably succeeds even when the fraction
of nonzero entries is $\Omega(1)$. To the best of our knowledge, this is the
first practical algorithm to achieve this linear scaling. This result assumes a
planted sparse model for the subspace, in which the target sparse vector is
embedded in an otherwise random subspace. Empirically, our proposed algorithm
also succeeds in more challenging data models, e.g., sparse dictionary
learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4682</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4682</id><created>2014-12-15</created><authors><author><keyname>Tromp</keyname><forenames>Erik</forenames></author><author><keyname>Pechenizkiy</keyname><forenames>Mykola</forenames></author></authors><title>Rule-based Emotion Detection on Social Media: Putting Tweets on
  Plutchik's Wheel</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study sentiment analysis beyond the typical granularity of polarity and
instead use Plutchik's wheel of emotions model. We introduce RBEM-Emo as an
extension to the Rule-Based Emission Model algorithm to deduce such emotions
from human-written messages. We evaluate our approach on two different datasets
and compare its performance with the current state-of-the-art techniques for
emotion detection, including a recursive auto-encoder. The results of the
experimental study suggest that RBEM-Emo is a promising approach advancing the
current state-of-the-art in emotion detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4690</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4690</id><created>2014-12-15</created><updated>2015-05-22</updated><authors><author><keyname>Searson</keyname><forenames>Dominic P.</forenames></author></authors><title>GPTIPS 2: an open-source software platform for symbolic data mining</title><categories>cs.MS cs.NE</categories><comments>26 pages, accepted for publication in the Springer Handbook of
  Genetic Programming Applications (2015, in press)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  GPTIPS is a free, open source MATLAB based software platform for symbolic
data mining (SDM). It uses a multigene variant of the biologically inspired
machine learning method of genetic programming (MGGP) as the engine that drives
the automatic model discovery process. Symbolic data mining is the process of
extracting hidden, meaningful relationships from data in the form of symbolic
equations. In contrast to other data-mining methods, the structural
transparency of the generated predictive equations can give new insights into
the physical systems or processes that generated the data. Furthermore, this
transparency makes the models very easy to deploy outside of MATLAB. The
rationale behind GPTIPS is to reduce the technical barriers to using,
understanding, visualising and deploying GP based symbolic models of data,
whilst at the same time remaining highly customisable and delivering robust
numerical performance for power users. In this chapter, notable new features of
the latest version of the software are discussed with these aims in mind.
Additionally, a simplified variant of the MGGP high level gene crossover
mechanism is proposed. It is demonstrated that the new functionality of GPTIPS
2 (a) facilitates the discovery of compact symbolic relationships from data
using multiple approaches, e.g. using novel gene-centric visualisation analysis
to mitigate horizontal bloat and reduce complexity in multigene symbolic
regression models (b) provides numerous methods for visualising the properties
of symbolic models (c) emphasises the generation of graphically navigable
libraries of models that are optimal in terms of the Pareto trade off surface
of model performance and complexity and (d) expedites real world applications
by the simple, rapid and robust deployment of symbolic models outside the
software environment they were developed in.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4707</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4707</id><created>2014-12-15</created><authors><author><keyname>Diaz</keyname><forenames>Jesus</forenames></author><author><keyname>Arroyo</keyname><forenames>David</forenames></author><author><keyname>Rodriguez</keyname><forenames>Francisco B.</forenames></author></authors><title>Fair anonymity for the Tor network</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current anonymizing networks have become an important tool for guaranteeing
users' privacy. However, these platforms can be used to perform illegitimate
actions, which sometimes makes service providers see traffic coming from these
networks as a probable threat. In order to solve this problem, we propose to
add support for fairness mechanisms to the Tor network. Specifically, by
introducing a slight modification to the key negotiation process with the entry
and exit nodes, in the shape of group signatures. By means of these signatures,
we set up an access control method to prevent misbehaving users to make use of
the Tor network. Additionally, we establish a predefined method for denouncing
illegitimate actions, which impedes the application of the proposed fairness
mechanisms as a threat eroding users' privacy. As a direct consequence, traffic
coming from Tor would be considered less suspicious by service providers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4709</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4709</id><created>2014-12-15</created><authors><author><keyname>Miyazawa</keyname><forenames>Fl&#xe1;vio K.</forenames></author><author><keyname>Pedrosa</keyname><forenames>Lehilton L. C.</forenames></author><author><keyname>Schouery</keyname><forenames>Rafael C. S.</forenames></author><author><keyname>Sviridenko</keyname><forenames>Maxim</forenames></author><author><keyname>Wakabayashi</keyname><forenames>Yoshiko</forenames></author></authors><title>Polynomial-Time Approximation Schemes for Circle and Other Packing
  Problems</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an asymptotic approximation scheme (APTAS) for the problem of packing
a set of circles into a minimum number of unit square bins. To obtain rational
solutions, we use augmented bins of height $1+\gamma$, for some arbitrarily
small number $\gamma &gt; 0$. Our algorithm is polynomial on $\log 1/\gamma$, and
thus $\gamma$ is part of the problem input. For the special case that $\gamma$
is constant, we give a (one dimensional) resource augmentation scheme, that is,
we obtain a packing into bins of unit width and height $1+\gamma$ using no more
than the number of bins in an optimal packing. Additionally, we obtain an APTAS
for the circle strip packing problem, whose goal is to pack a set of circles
into a strip of unit width and minimum height. These are the first
approximation and resource augmentation schemes for these problems.
  Our algorithm is based on novel ideas of iteratively separating small and
large items, and may be extended to a wide range of packing problems that
satisfy certain conditions. These extensions comprise problems with different
kinds of items, such as regular polygons, or with bins of different shapes,
such as circles and spheres. As an example, we obtain APTAS's for the problems
of packing d-dimensional spheres into hypercubes under the $L_p$-norm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4714</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4714</id><created>2014-12-15</created><authors><author><keyname>Adam</keyname><forenames>Sorin</forenames></author><author><keyname>Schultz</keyname><forenames>Ulrik Pagh</forenames></author></authors><title>Towards Interactive, Incremental Programming of ROS Nodes</title><categories>cs.RO cs.SE</categories><comments>Presented at DSLRob 2014 (arXiv:cs/1411.7148)</comments><report-no>DSLRob/2014/05</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Writing software for controlling robots is a complex task, usually demanding
command of many programming languages and requiring significant
experimentation. We believe that a bottom-up development process that
complements traditional component- and MDSD-based approaches can facilitate
experimentation. We propose the use of an internal DSL providing both a tool to
interactively create ROS nodes and a behaviour-replacement mechanism to
interactively reshape existing ROS nodes by wrapping the external interfaces
(the publish/subscribe topics), dynamically controlled using the Python command
line interface.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4718</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4718</id><created>2014-12-15</created><authors><author><keyname>Ramos</keyname><forenames>Marlon</forenames></author><author><keyname>Shao</keyname><forenames>Jia</forenames></author><author><keyname>Reis</keyname><forenames>Saulo D. S.</forenames></author><author><keyname>Anteneodo</keyname><forenames>Celia</forenames></author><author><keyname>Andrade</keyname><forenames>Jos&#xe9; S.</forenames><suffix>Jr</suffix></author><author><keyname>Havlin</keyname><forenames>Shlomo</forenames></author><author><keyname>Makse</keyname><forenames>Hern&#xe1;n A.</forenames></author></authors><title>How does public opinion become extreme?</title><categories>physics.soc-ph cs.SI</categories><comments>28 pages, 5 figures</comments><journal-ref>Scientific Reports 5, Article number: 10032 (2015)</journal-ref><doi>10.1038/srep10032</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the emergence of extreme opinion trends in society by
employing statistical physics modeling and analysis on polls that inquire about
a wide range of issues such as religion, economics, politics, abortion,
extramarital sex, books, movies, and electoral vote. The surveys lay out a
clear indicator of the rise of extreme views. The precursor is a nonlinear
relation between the fraction of individuals holding a certain extreme view and
the fraction of individuals that includes also moderates, e.g., in politics,
those who are &quot;very conservative&quot; versus &quot;moderate to very conservative&quot; ones.
We propose an activation model of opinion dynamics with interaction rules based
on the existence of individual &quot;stubbornness&quot; that mimics empirical
observations. According to our modeling, the onset of nonlinearity can be
associated to an abrupt bootstrap-percolation transition with cascades of
extreme views through society. Therefore, it represents an early-warning signal
to forecast the transition from moderate to extreme views. Moreover, by means
of a phase diagram we can classify societies according to the percolative
regime they belong to, in terms of critical fractions of extremists and
people's ties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4719</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4719</id><created>2014-12-15</created><authors><author><keyname>Bhowmick</keyname><forenames>Abhishek</forenames></author><author><keyname>Lovett</keyname><forenames>Shachar</forenames></author></authors><title>Nonclassical polynomials as a barrier to polynomial lower bounds</title><categories>cs.CC</categories><msc-class>68Qxx</msc-class><acm-class>F.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of constructing explicit functions which cannot be approximated
by low degree polynomials has been extensively studied in computational
complexity, motivated by applications in circuit lower bounds,
pseudo-randomness, constructions of Ramsey graphs and locally decodable codes.
Still, most of the known lower bounds become trivial for polynomials of
super-logarithmic degree. Here, we suggest a new barrier explaining this
phenomenon. We show that many of the existing lower bound proof techniques
extend to nonclassical polynomials, an extension of classical polynomials which
arose in higher order Fourier analysis. Moreover, these techniques are tight
for nonclassical polynomials of logarithmic degree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4726</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4726</id><created>2014-12-15</created><authors><author><keyname>Tagiew</keyname><forenames>Rustam</forenames></author><author><keyname>Ignatov</keyname><forenames>Dmitry I.</forenames></author><author><keyname>Amroush</keyname><forenames>Fadi</forenames></author></authors><title>Experimental economics for web mining</title><categories>cs.CE cs.CY</categories><comments>3 pages, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper offers a step towards research infrastructure, which makes data
from experimental economics efficiently usable for analysis of web data. We
believe that regularities of human behavior found in experimental data also
emerge in real world web data. A format for data from experiments is suggested,
which enables its publication as open data. Once standardized datasets of
experiments are available on-line, web mining can take advantages from this
data. Further, the questions about the order of causalities arisen from web
data analysis can inspire new experiment setups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4729</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4729</id><created>2014-12-15</created><updated>2015-04-30</updated><authors><author><keyname>Venugopalan</keyname><forenames>Subhashini</forenames></author><author><keyname>Xu</keyname><forenames>Huijuan</forenames></author><author><keyname>Donahue</keyname><forenames>Jeff</forenames></author><author><keyname>Rohrbach</keyname><forenames>Marcus</forenames></author><author><keyname>Mooney</keyname><forenames>Raymond</forenames></author><author><keyname>Saenko</keyname><forenames>Kate</forenames></author></authors><title>Translating Videos to Natural Language Using Deep Recurrent Neural
  Networks</title><categories>cs.CV cs.CL</categories><comments>NAACL-HLT 2015 camera ready</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Solving the visual symbol grounding problem has long been a goal of
artificial intelligence. The field appears to be advancing closer to this goal
with recent breakthroughs in deep learning for natural language grounding in
static images. In this paper, we propose to translate videos directly to
sentences using a unified deep neural network with both convolutional and
recurrent structure. Described video datasets are scarce, and most existing
methods have been applied to toy domains with a small vocabulary of possible
words. By transferring knowledge from 1.2M+ images with category labels and
100,000+ images with captions, our method is able to create sentence
descriptions of open-domain videos with large vocabularies. We compare our
approach with recent work using language generation metrics, subject, verb, and
object prediction accuracy, and a human evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4736</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4736</id><created>2014-12-15</created><updated>2015-02-17</updated><authors><author><keyname>Helmbold</keyname><forenames>David P.</forenames></author><author><keyname>Long</keyname><forenames>Philip M.</forenames></author></authors><title>On the Inductive Bias of Dropout</title><categories>cs.LG cs.AI cs.NE math.ST stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dropout is a simple but effective technique for learning in neural networks
and other settings. A sound theoretical understanding of dropout is needed to
determine when dropout should be applied and how to use it most effectively. In
this paper we continue the exploration of dropout as a regularizer pioneered by
Wager, et.al. We focus on linear classification where a convex proxy to the
misclassification loss (i.e. the logistic loss used in logistic regression) is
minimized. We show: (a) when the dropout-regularized criterion has a unique
minimizer, (b) when the dropout-regularization penalty goes to infinity with
the weights, and when it remains bounded, (c) that the dropout regularization
can be non-monotonic as individual weights increase from 0, and (d) that the
dropout regularization penalty may not be convex. This last point is
particularly surprising because the combination of dropout regularization with
any convex loss proxy is always a convex function.
  In order to contrast dropout regularization with $L_2$ regularization, we
formalize the notion of when different sources are more compatible with
different regularizers. We then exhibit distributions that are provably more
compatible with dropout regularization than $L_2$ regularization, and vice
versa. These sources provide additional insight into how the inductive biases
of dropout and $L_2$ regularization differ. We provide some similar results for
$L_1$ regularization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4737</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4737</id><created>2014-12-11</created><updated>2015-09-25</updated><authors><author><keyname>Diekert</keyname><forenames>Volker</forenames></author><author><keyname>Martin</keyname><forenames>Florent</forenames></author><author><keyname>Senizergues</keyname><forenames>Geraud</forenames></author><author><keyname>Silva</keyname><forenames>Pedro V.</forenames></author></authors><title>Equations over free inverse monoids with idempotent variables</title><categories>cs.LO</categories><comments>28 pages. The conference version of this paper appeared in the
  proceedings of 10th International Computer Science Symposium in Russia, CSR
  2015, Listvyanka, Russia, July 13-17, 2015. Springer LNCS 9139, pp. 173-188
  (2015)</comments><msc-class>20M18, 20F70, 03D40</msc-class><acm-class>F.4; F.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the notion of idempotent variables for studying equations in
inverse monoids.
  It is proved that it is decidable in singly exponential time (DEXPTIME)
whether a system of equations in idempotent variables over a free inverse
monoid has a solution. The result is proved by a direct reduction to solve
language equations with one-sided concatenation and a known complexity result
by Baader and Narendran: Unification of concept terms in description logics,
2001. We also show that the problem becomes DEXPTIME hard , as soon as the
quotient group of the free inverse monoid has rank at least two.
  Decidability for systems of typed equations over a free inverse monoid with
one irreducible variable and at least one unbalanced equation is proved with
the same complexity for the upper bound.
  Our results improve known complexity bounds by Deis, Meakin, and Senizergues:
Equations in free inverse monoids, 2007.
  Our results also apply to larger families of equations where no decidability
has been previously known.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4738</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4738</id><created>2014-12-11</created><authors><author><keyname>Caldwell</keyname><forenames>James</forenames><affiliation>University of Wyoming</affiliation></author><author><keyname>H&#xf6;lzenspies</keyname><forenames>Philip</forenames><affiliation>University of Twente</affiliation></author><author><keyname>Achten</keyname><forenames>Peter</forenames><affiliation>Radboud University Nijmegen</affiliation></author></authors><title>Proceedings 3rd International Workshop on Trends in Functional
  Programming in Education</title><categories>cs.CY cs.DS cs.PL</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 170, 2014</journal-ref><doi>10.4204/EPTCS.170</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of TFPIE is to gather researchers, professors, teachers, and all
professionals interested in functional programming in education. This includes
the teaching of functional programming, but also the application of functional
programming as a tool for teaching other topics. The post-workshop review
process received 13 submissions, which were vetted by the program committee,
assuming scientific journal standards of publication. The six articles in this
volume were selected for publication as the result of this process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4754</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4754</id><created>2014-12-15</created><authors><author><keyname>Dong</keyname><forenames>Yuxiao</forenames></author><author><keyname>Johnson</keyname><forenames>Reid A.</forenames></author><author><keyname>Chawla</keyname><forenames>Nitesh V.</forenames></author></authors><title>Will This Paper Increase Your h-index? Scientific Impact Prediction</title><categories>cs.SI cs.DL physics.soc-ph</categories><comments>Proc. of the 8th ACM International Conference on Web Search and Data
  Mining (WSDM'15)</comments><acm-class>H.2.8; H.3.7</acm-class><doi>10.1145/2684822.2685314</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scientific impact plays a central role in the evaluation of the output of
scholars, departments, and institutions. A widely used measure of scientific
impact is citations, with a growing body of literature focused on predicting
the number of citations obtained by any given publication. The effectiveness of
such predictions, however, is fundamentally limited by the power-law
distribution of citations, whereby publications with few citations are
extremely common and publications with many citations are relatively rare.
Given this limitation, in this work we instead address a related question asked
by many academic researchers in the course of writing a paper, namely: &quot;Will
this paper increase my h-index?&quot; Using a real academic dataset with over 1.7
million authors, 2 million papers, and 8 million citation relationships from
the premier online academic service ArnetMiner, we formalize a novel scientific
impact prediction problem to examine several factors that can drive a paper to
increase the primary author's h-index. We find that the researcher's authority
on the publication topic and the venue in which the paper is published are
crucial factors to the increase of the primary author's h-index, while the
topic popularity and the co-authors' h-indices are of surprisingly little
relevance. By leveraging relevant factors, we find a greater than 87.5%
potential predictability for whether a paper will contribute to an author's
h-index within five years. As a further experiment, we generate a
self-prediction for this paper, estimating that there is a 76% probability that
it will contribute to the h-index of the co-author with the highest current
h-index in five years. We conclude that our findings on the quantification of
scientific impact can help researchers to expand their influence and more
effectively leverage their position of &quot;standing on the shoulders of giants.&quot;
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4802</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4802</id><created>2014-12-01</created><authors><author><keyname>Patrascu</keyname><forenames>Vasile</forenames></author></authors><title>Neutrosophic information in the framework of multi-valued representation</title><categories>cs.AI</categories><doi>10.13140/2.1.4717.2169</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents some steps for multi-valued representation of neutrosophic
information. These steps are provided in the framework of multi-valued logics
using the following logical value: true, false, neutral, unknown and saturated.
Also, this approach provides some calculus formulae for the following
neutrosophic features: truth, falsity, neutrality, ignorance,
under-definedness, over-definedness, saturation and entropy. In addition, it
was defined net truth, definedness and neutrosophic score.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4813</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4813</id><created>2014-12-15</created><authors><author><keyname>Khosravirad</keyname><forenames>Saeed R.</forenames></author><author><keyname>Szczecinski</keyname><forenames>Leszek</forenames></author><author><keyname>Labeau</keyname><forenames>Fabrice</forenames></author></authors><title>Opportunistic Relaying without CSI: Optimizing Variable-Rate HARQ</title><categories>cs.IT math.IT</categories><comments>31 pages, 8figures, submitted to IEEE Trans. on Veh. Tech</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the opportunistic relaying based on HARQ transmission over the
block-fading channel with absence of channel state information (CSI) at the
transmitter nodes. We assume that both the source and the relay are allowed to
vary their transmission rate between the HARQ transmission rounds. We solve the
problem of throughput maximization with respect to the transmission rates using
double-recursive Dynamic Programming. Simplifications are also proposed to
diminish the complexity of the optimization. The numerical results confirm that
the variable-rate HARQ can increase the throughput significantly comparing to
its fixed-rate counterpart.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4825</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4825</id><created>2014-12-15</created><authors><author><keyname>Mahani</keyname><forenames>Alireza S.</forenames></author><author><keyname>Sharabiani</keyname><forenames>Mansour T. A.</forenames></author></authors><title>Efficient SIMD RNG for Varying-Parameter Streams: C++ Class BatchRNG</title><categories>stat.CO cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Single-Instruction, Multiple-Data (SIMD) random number generators (RNGs) take
advantage of vector units to offer significant performance gain over
non-vectorized libraries, but they often rely on batch production of deviates
from distributions with fixed parameters. In many statistical applications such
as Gibbs sampling, parameters of sampled distributions change from one
iteration to the next, requiring that random deviates be generated
one-at-a-time. This situation can render vectorized RNGs inefficient, and even
inferior to their scalar counterparts. The C++ class BatchRNG uses buffers of
base distributions such uniform, Gaussian and exponential to take advantage of
vector units while allowing for sequences of deviates to be generated with
varying parameters. These small buffers are consumed and replenished as needed
during a program execution. Performance tests using Intel Vector Statistical
Library (VSL) on various probability distributions illustrates the
effectiveness of the proposed batching strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4832</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4832</id><created>2014-12-15</created><authors><author><keyname>Foster</keyname><forenames>Dean</forenames></author><author><keyname>Karloff</keyname><forenames>Howard</forenames></author><author><keyname>Thaler</keyname><forenames>Justin</forenames></author></authors><title>Variable Selection is Hard</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Variable selection for sparse linear regression is the problem of finding,
given an m x p matrix B and a target vector y, a sparse vector x such that Bx
approximately equals y. Assuming a standard complexity hypothesis, we show that
no polynomial-time algorithm can find a k'-sparse x with ||Bx-y||^2&lt;=h(m,p),
where k'=k*2^{log^{1-delta} p} and h(m,p)&lt;=p^(C_1)*m^(1-C_2), where delta&gt;0,
C_1&gt;0,C_2&gt;0 are arbitrary. This is true even under the promise that there is an
unknown k-sparse vector x^* satisfying Bx^*=y. We prove a similar result for a
statistical version of the problem in which the data are corrupted by noise.
  To the authors' knowledge, these are the first hardness results for sparse
regression that apply when the algorithm simultaneously has k'&gt;k and h(m,p)&gt;0.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4840</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4840</id><created>2014-12-15</created><updated>2014-12-17</updated><authors><author><keyname>Daskalakis</keyname><forenames>Constantinos</forenames></author><author><keyname>Pan</keyname><forenames>Qinxuan</forenames></author></authors><title>A Counter-Example to Karlin's Strong Conjecture for Fictitious Play</title><categories>cs.GT</categories><comments>55th IEEE Symposium on Foundations of Computer Science</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fictitious play is a natural dynamic for equilibrium play in zero-sum games,
proposed by [Brown 1949], and shown to converge by [Robinson 1951]. Samuel
Karlin conjectured in 1959 that fictitious play converges at rate
$O(1/\sqrt{t})$ with the number of steps $t$. We disprove this conjecture
showing that, when the payoff matrix of the row player is the $n \times n$
identity matrix, fictitious play may converge with rate as slow as
$\Omega(t^{-1/n})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4842</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4842</id><created>2014-12-15</created><authors><author><keyname>Tang</keyname><forenames>Mingjie</forenames></author><author><keyname>Tahboub</keyname><forenames>Ruby Y.</forenames></author><author><keyname>Are</keyname><forenames>Walid G.</forenames></author><author><keyname>Atallah</keyname><forenames>Mikhail J.</forenames></author><author><keyname>Malluhi</keyname><forenames>Qutaibah M.</forenames></author><author><keyname>Ouzzani</keyname><forenames>Mourad</forenames></author><author><keyname>Silva</keyname><forenames>Yasin N.</forenames></author></authors><title>Similarity Group-by Operators for Multi-dimensional Relational Data</title><categories>cs.DB</categories><comments>submit to TKDE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The SQL group-by operator plays an important role in summarizing and
aggregating large datasets in a data analytic stack.While the standard group-by
operator, which is based on equality, is useful in several applications,
allowing similarity aware grouping provides a more realistic view on real-world
data that could lead to better insights. The Similarity SQL-based Group-By
operator (SGB, for short) extends the semantics of the standard SQL Group-by by
grouping data with similar but not necessarily equal values. While existing
similarity-based grouping operators efficiently materialize this approximate
semantics, they primarily focus on one-dimensional attributes and treat
multidimensional attributes independently. However, correlated attributes, such
as in spatial data, are processed independently, and hence, groups in the
multidimensional space are not detected properly. To address this problem, we
introduce two new SGB operators for multidimensional data. The first operator
is the clique (or distance-to-all) SGB, where all the tuples in a group are
within some distance from each other. The second operator is the
distance-to-any SGB, where a tuple belongs to a group if the tuple is within
some distance from any other tuple in the group. We implement and test the new
SGB operators and their algorithms inside PostgreSQL. The overhead introduced
by these operators proves to be minimal and the execution times are comparable
to those of the standard Group-by. The experimental study, based on TPC-H and a
social check-in data, demonstrates that the proposed algorithms can achieve up
to three orders of magnitude enhancement in performance over baseline methods
developed to solve the same problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4846</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4846</id><created>2014-12-15</created><updated>2015-01-07</updated><authors><author><keyname>Lin</keyname><forenames>Ruokuang</forenames></author><author><keyname>Ma</keyname><forenames>Qianli D. Y.</forenames></author><author><keyname>Bian</keyname><forenames>Chunhua</forenames></author></authors><title>Scaling laws in human speech, decreasing emergence of new words and a
  generalized model</title><categories>cs.CL physics.data-an</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human language, as a typical complex system, its organization and evolution
is an attractive topic for both physical and cultural researchers. In this
paper, we present the first exhaustive analysis of the text organization of
human speech. Two important results are that: (i) the construction and
organization of spoken language can be characterized as Zipf's law and Heaps'
law, as observed in written texts; (ii) word frequency vs. rank distribution
and the growth of distinct words with the increase of text length shows
significant differences between book and speech. In speech word frequency
distribution are more concentrated on higher frequency words, and the emergence
of new words decreases much rapidly when the content length grows. Based on
these observations, a new generalized model is proposed to explain these
complex dynamical behaviors and the differences between speech and book.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4847</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4847</id><created>2014-12-15</created><authors><author><keyname>Paikan</keyname><forenames>Ali</forenames></author><author><keyname>Metta</keyname><forenames>Giorgio</forenames></author><author><keyname>Natale</keyname><forenames>Lorenzo</forenames></author></authors><title>A representation of robotic behaviors using component port arbitration</title><categories>cs.RO cs.SE</categories><comments>Presented at DSLRob 2014 (arXiv:cs/1411.7148)</comments><report-no>DSLRob/2014/01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Developing applications considering reactiveness, scalability and
re-usability has always been at the center of attention of robotic researchers.
Behavior-based architectures have been proposed as a programming paradigm to
develop robust and complex behaviors as integration of simpler modules whose
activities are directly modulated by sensory feedback or input from other
models. The design of behavior based systems, however, becomes increasingly
difficult as the complexity of the application grows. This article proposes an
approach for modeling and coordinating behaviors in distributed architectures
based on port arbitration which clearly separates representation of the
behaviors from the composition of the software components. Therefore, based on
different behavioral descriptions, the same software components can be reused
to implement different applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4861</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4861</id><created>2014-12-15</created><authors><author><keyname>Yao</keyname><forenames>Yong</forenames></author><author><keyname>Xu</keyname><forenames>Jia</forenames></author><author><keyname>Yang</keyname><forenames>Lu</forenames></author></authors><title>A Successive Resultant Projection for Cylindrical Algebraic
  Decomposition</title><categories>cs.SC</categories><comments>6 pages</comments><msc-class>12Y05, 13P15, 14P10, 68W30</msc-class><acm-class>G.1.5</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This note shows the equivalence of two projection operators which both can be
used in cylindrical algebraic decomposition (CAD) . One is known as Brown's
Projection (C. W. Brown (2001)); the other was proposed by Lu Yang in his
earlier work (L.Yang and S.~H. Xia (2000)) that is sketched as follows: given a
polynomial $f$ in $x_1,\,x_2,\,\cdots$, by $f_1$ denote the resultant of $f$
and its partial derivative with respect to $x_1$ (removing the multiple
factors), by $f_2$ denote the resultant of $f_1$ and its partial derivative
with respect to $x_2$, (removing the multiple factors), $\cdots$, repeat this
procedure successively until the last resultant becomes a univariate
polynomial. Making use of an identity, the equivalence of these two projection
operators is evident.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4862</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4862</id><created>2014-12-15</created><authors><author><keyname>Tsotsos</keyname><forenames>Konstantine</forenames></author><author><keyname>Chiuso</keyname><forenames>Alessandro</forenames></author><author><keyname>Soatto</keyname><forenames>Stefano</forenames></author></authors><title>Robust Inference for Visual-Inertial Sensor Fusion</title><categories>cs.RO</categories><comments>Submitted to ICRA 2015, Manuscript #2912. Video results available at:
  http://youtu.be/5JSF0-DbIRc</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inference of three-dimensional motion from the fusion of inertial and visual
sensory data has to contend with the preponderance of outliers in the latter.
Robust filtering deals with the joint inference and classification task of
selecting which data fits the model, and estimating its state. We derive the
optimal discriminant and propose several approximations, some used in the
literature, others new. We compare them analytically, by pointing to the
assumptions underlying their approximations, and empirically. We show that the
best performing method improves the performance of state-of-the-art
visual-inertial sensor fusion systems, while retaining the same computational
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4863</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4863</id><created>2014-12-15</created><authors><author><keyname>Li</keyname><forenames>Changsheng</forenames></author><author><keyname>Liu</keyname><forenames>Qingshan</forenames></author><author><keyname>Dong</keyname><forenames>Weishan</forenames></author><author><keyname>Zhang</keyname><forenames>Xin</forenames></author><author><keyname>Yang</keyname><forenames>Lin</forenames></author></authors><title>Max-Margin based Discriminative Feature Learning</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new max-margin based discriminative feature
learning method. Specifically, we aim at learning a low-dimensional feature
representation, so as to maximize the global margin of the data and make the
samples from the same class as close as possible. In order to enhance the
robustness to noise, a $l_{2,1}$ norm constraint is introduced to make the
transformation matrix in group sparsity. In addition, for multi-class
classification tasks, we further intend to learn and leverage the correlation
relationships among multiple class tasks for assisting in learning
discriminative features. The experimental results demonstrate the power of the
proposed method against the related state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4864</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4864</id><created>2014-12-15</created><authors><author><keyname>Bachman</keyname><forenames>Philip</forenames></author><author><keyname>Alsharif</keyname><forenames>Ouais</forenames></author><author><keyname>Precup</keyname><forenames>Doina</forenames></author></authors><title>Learning with Pseudo-Ensembles</title><categories>stat.ML cs.LG cs.NE</categories><comments>To appear in Advances in Neural Information Processing Systems 27
  (NIPS 2014), Advances in Neural Information Processing Systems 27, Dec. 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We formalize the notion of a pseudo-ensemble, a (possibly infinite)
collection of child models spawned from a parent model by perturbing it
according to some noise process. E.g., dropout (Hinton et. al, 2012) in a deep
neural network trains a pseudo-ensemble of child subnetworks generated by
randomly masking nodes in the parent network. We present a novel regularizer
based on making the behavior of a pseudo-ensemble robust with respect to the
noise process generating it. In the fully-supervised setting, our regularizer
matches the performance of dropout. But, unlike dropout, our regularizer
naturally extends to the semi-supervised setting, where it produces
state-of-the-art results. We provide a case study in which we transform the
Recursive Neural Tensor Network of (Socher et. al, 2013) into a
pseudo-ensemble, which significantly improves its performance on a real-world
sentiment analysis benchmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4875</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4875</id><created>2014-12-15</created><updated>2015-05-05</updated><authors><author><keyname>Holme</keyname><forenames>Petter</forenames></author><author><keyname>Takaguchi</keyname><forenames>Taro</forenames></author></authors><title>Time evolution of predictability of epidemics on networks</title><categories>q-bio.PE cs.SI physics.soc-ph</categories><journal-ref>Phys. Rev. E 91, 042811 (2015)</journal-ref><doi>10.1103/PhysRevE.91.042811</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Epidemic outbreaks of new pathogens, or known pathogens in new populations,
cause a great deal of fear because they are hard to predict. For theoretical
models of disease spreading, on the other hand, quantities characterizing the
outbreak converge to deterministic functions of time. Our goal in this paper is
to shed some light on this apparent discrepancy. We measure the diversity of
(and, thus, the predictability of) outbreak sizes and extinction times as
functions of time given different scenarios of the amount of information
available. Under the assumption of perfect information -- i.e., knowing the
state of each individual with respect to the disease -- the predictability
decreases exponentially, or faster, with time. The decay is slowest for
intermediate values of the per-contact transmission probability. With a weaker
assumption on the information available, assuming that we know only the
fraction of currently infectious, recovered, or susceptible individuals, the
predictability also decreases exponentially most of the time. There are,
however, some peculiar regions in this scenario where the predictability
decreases. In other words, to predict its final size with a given accuracy, we
would need increasingly more information about the outbreak.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4877</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4877</id><created>2014-12-16</created><authors><author><keyname>Ishii</keyname><forenames>Yuki</forenames><affiliation>Ochanomizu University, Tokyo, Japan</affiliation></author><author><keyname>Asai</keyname><forenames>Kenichi</forenames><affiliation>Ochanomizu University, Tokyo, Japan</affiliation></author></authors><title>Report on a User Test and Extension of a Type Debugger for Novice
  Programmers</title><categories>cs.PL cs.SE</categories><comments>In Proceedings TFPIE 2014, arXiv:1412.4738</comments><proxy>EPTCS</proxy><acm-class>D.2.5</acm-class><journal-ref>EPTCS 170, 2014, pp. 1-18</journal-ref><doi>10.4204/EPTCS.170.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A type debugger interactively detects the expressions that cause type errors.
It asks users whether they intend the types of identifiers to be those that the
compiler inferred. However, it seems that novice programmers often get in
trouble when they think about how to fix type errors by reading the messages
given by the type debugger. In this paper, we analyze the user tests of a type
debugger and report problems of the current type debugger. We then extend the
type debugger to address these problems. Specifically, we introduce
expression-specific error messages and language levels. Finally, we show type
errors that we think are difficult to explain to novice programmers. The
subjects of the user tests were 40 novice students belonging to the department
of information science at Ochanomizu University.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4878</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4878</id><created>2014-12-16</created><authors><author><keyname>Moraz&#xe1;n</keyname><forenames>Marco T.</forenames><affiliation>Seton Hall University</affiliation></author><author><keyname>Antunez</keyname><forenames>Rosario</forenames><affiliation>City College of New York</affiliation></author></authors><title>Functional Automata - Formal Languages for Computer Science Students</title><categories>cs.FL cs.CY</categories><comments>In Proceedings TFPIE 2014, arXiv:1412.4738</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 170, 2014, pp. 19-32</journal-ref><doi>10.4204/EPTCS.170.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An introductory formal languages course exposes advanced undergraduate and
early graduate students to automata theory, grammars, constructive proofs,
computability, and decidability. Programming students find these topics to be
challenging or, in many cases, overwhelming and on the fringe of Computer
Science. The existence of this perception is not completely absurd since
students are asked to design and prove correct machines and grammars without
being able to experiment nor get immediate feedback, which is essential in a
learning context. This article puts forth the thesis that the theory of
computation ought to be taught using tools for actually building computations.
It describes the implementation and the classroom use of a library, FSM,
designed to provide students with the opportunity to experiment and test their
designs using state machines, grammars, and regular expressions. Students are
able to perform random testing before proceeding with a formal proof of
correctness. That is, students can test their designs much like they do in a
programming course. In addition, the library easily allows students to
implement the algorithms they develop as part of the constructive proofs they
write. Providing students with this ability ought to be a new trend in the
formal languages classroom.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4879</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4879</id><created>2014-12-16</created><authors><author><keyname>Olmer</keyname><forenames>Tim</forenames><affiliation>Open University of the Netherlands</affiliation></author><author><keyname>Heeren</keyname><forenames>Bastiaan</forenames><affiliation>Open University of the Netherlands</affiliation></author><author><keyname>Jeuring</keyname><forenames>Johan</forenames><affiliation>Universiteit Utrecht and Open University of the Netherlands</affiliation></author></authors><title>Evaluating Haskell expressions in a tutoring environment</title><categories>cs.CY cs.PL</categories><comments>In Proceedings TFPIE 2014, arXiv:1412.4738</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 170, 2014, pp. 50-66</journal-ref><doi>10.4204/EPTCS.170.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A number of introductory textbooks for Haskell use calculations right from
the start to give the reader insight into the evaluation of expressions and the
behavior of functional programs. Many programming concepts that are important
in the functional programming paradigm, such as recursion, higher-order
functions, pattern-matching, and lazy evaluation, can be partially explained by
showing a stepwise computation. A student gets a better understanding of these
concepts if she performs these evaluation steps herself. Tool support for
experimenting with the evaluation of Haskell expressions is currently lacking.
In this paper we present a prototype implementation of a stepwise evaluator for
Haskell expressions that supports multiple evaluation strategies, specifically
targeted at education. Besides performing evaluation steps the tool also
diagnoses steps that are submitted by a student, and provides feedback.
Instructors can add or change function definitions without knowledge of the
tool's internal implementation. We discuss some preliminary results of a small
survey about the tool.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4880</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4880</id><created>2014-12-16</created><authors><author><keyname>Walck</keyname><forenames>Scott N.</forenames><affiliation>Lebanon Valley College, Annville, Pennsylvania, USA</affiliation></author></authors><title>Learn Physics by Programming in Haskell</title><categories>cs.CY cs.PL physics.ed-ph</categories><comments>In Proceedings TFPIE 2014, arXiv:1412.4738</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 170, 2014, pp. 67-77</journal-ref><doi>10.4204/EPTCS.170.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a method for deepening a student's understanding of basic physics
by asking the student to express physical ideas in a functional programming
language. The method is implemented in a second-year course in computational
physics at Lebanon Valley College. We argue that the structure of Newtonian
mechanics is clarified by its expression in a language (Haskell) that supports
higher-order functions, types, and type classes. In electromagnetic theory, the
type signatures of functions that calculate electric and magnetic fields
clearly express the functional dependency on the charge and current
distributions that produce the fields. Many of the ideas in basic physics are
well-captured by a type or a function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4881</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4881</id><created>2014-12-16</created><authors><author><keyname>Winter</keyname><forenames>Victor</forenames><affiliation>University of Nebraska at Omaha, USA</affiliation></author></authors><title>Bricklayer: An Authentic Introduction to the Functional Programming
  Language SML</title><categories>cs.CY cs.PL</categories><comments>In Proceedings TFPIE 2014, arXiv:1412.4738</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 170, 2014, pp. 33-49</journal-ref><doi>10.4204/EPTCS.170.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Functional programming languages are seen by many as instrumental to
effectively utilizing the computational power of multi-core platforms. As a
result, there is growing interest to introduce functional programming and
functional thinking as early as possible within the computer science
curriculum. Bricklayer is an API, written in SML, that provides a set of
abstractions for creating LEGO artifacts which can be viewed using LEGO Digital
Designer. The goal of Bricklayer is to create a problem space (i.e., a set of
LEGO artifacts) that is accessible and engaging to programmers (especially
novice programmers) while providing an authentic introduction to the functional
programming language SML.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4882</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4882</id><created>2014-12-16</created><authors><author><keyname>Ragde</keyname><forenames>Prabhakar</forenames><affiliation>University of Waterloo, Waterloo, Ontario, Canada</affiliation></author></authors><title>Simple Balanced Binary Search Trees</title><categories>cs.PL cs.DS</categories><comments>In Proceedings TFPIE 2014, arXiv:1412.4738</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 170, 2014, pp. 78-87</journal-ref><doi>10.4204/EPTCS.170.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient implementations of sets and maps (dictionaries) are important in
computer science, and balanced binary search trees are the basis of the best
practical implementations. Pedagogically, however, they are often quite
complicated, especially with respect to deletion. I present complete code (with
justification and analysis not previously available in the literature) for a
purely-functional implementation based on AA trees, which is the simplest
treatment of the subject of which I am aware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4904</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4904</id><created>2014-12-16</created><authors><author><keyname>Klauck</keyname><forenames>Hartmut</forenames></author><author><keyname>Podder</keyname><forenames>Supartha</forenames></author></authors><title>New Bounds for the Garden-Hose Model</title><categories>cs.CC</categories><comments>In FSTTCS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show new results about the garden-hose model. Our main results include
improved lower bounds based on non-deterministic communication complexity
(leading to the previously unknown $\Theta(n)$ bounds for Inner Product mod 2
and Disjointness), as well as an $O(n\cdot \log^3 n)$ upper bound for the
Distributed Majority function (previously conjectured to have quadratic
complexity). We show an efficient simulation of formulae made of AND, OR, XOR
gates in the garden-hose model, which implies that lower bounds on the
garden-hose complexity $GH(f)$ of the order $\Omega(n^{2+\epsilon})$ will be
hard to obtain for explicit functions. Furthermore we study a time-bounded
variant of the model, in which even modest savings in time can lead to
exponential lower bounds on the size of garden-hose protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4920</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4920</id><created>2014-12-16</created><authors><author><keyname>Palmer</keyname><forenames>T. N.</forenames></author><author><keyname>O'Shea</keyname><forenames>M.</forenames></author></authors><title>Neuronal noise as a physical resource for human cognition</title><categories>q-bio.NC cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new class of energy-efficient digital microprocessor is being developed
which is susceptible to thermal noise and consequently operates in
probabilistic rather than conventional deterministic mode. Hybrid computing
systems which combine probabilistic and deterministic processors can provide
robust and efficient tools for computational problems that hitherto would be
intractable by conventional deterministic algorithm. These developments suggest
a revised perspective on the consequences of ion-channel noise in slender
axons, often regarded as a hindrance to neuronal computations. It is proposed
that the human brain is such an energy-efficient hybrid computational system
whose remarkable characteristics emerge from constructive synergies between
probabilistic and deterministic modes of operation. In particular, the capacity
for intuition and creative problem solving appears to arise naturally from such
a hybrid system. Bearing in mind that physical thermal noise is both pure and
available at no cost, our proposal has implications for attempts to emulate the
energy-efficient human brain on conventional energy-intensive deterministic
supercomputers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4927</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4927</id><created>2014-12-16</created><authors><author><keyname>Jing</keyname><forenames>Gangshan</forenames></author><author><keyname>Zheng</keyname><forenames>Yuanshi</forenames></author><author><keyname>Wang</keyname><forenames>Long</forenames></author></authors><title>Consensus of Multi-agent Systems Under State-dependent Information
  Transmission</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the consensus problem for continuous-time and
discrete-time multi-agent systems in state-dependent switching networks. In
each case, we first consider the networks with fixed connectivity, in which the
communication between adjacent agents always exists but the influence could
possibly become negligible if the transmission distance is long enough. It is
obtained that consensus can be reached under a restriction of either the
decaying rate of the transmission weight or the initial states of the agents.
After then we investigate the networks with state-dependent connectivity, in
which the information transmission between adjacent agents gradually vanishes
if their distance exceeds a fixed range. In such networks, we prove that the
realization of consensus requires the validity of some initial conditions.
Finally, the conclusions are applied to models with the transmission law of C-S
model, opinion dynamics and the rendezvous problem, the corresponding
simulations are also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4930</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4930</id><created>2014-12-16</created><updated>2015-04-08</updated><authors><author><keyname>Lebret</keyname><forenames>R&#xe9;mi</forenames></author><author><keyname>Collobert</keyname><forenames>Ronan</forenames></author></authors><title>Rehabilitation of Count-based Models for Word Vector Representations</title><categories>cs.CL</categories><comments>A. Gelbukh (Ed.), Springer International Publishing Switzerland</comments><journal-ref>CICLing 2015, Part I, LNCS 9041, pp. 417-429, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent works on word representations mostly rely on predictive models.
Distributed word representations (aka word embeddings) are trained to optimally
predict the contexts in which the corresponding words tend to appear. Such
models have succeeded in capturing word similarties as well as semantic and
syntactic regularities. Instead, we aim at reviving interest in a model based
on counts. We present a systematic study of the use of the Hellinger distance
to extract semantic representations from the word co-occurence statistics of
large text corpora. We show that this distance gives good performance on word
similarity and analogy tasks, with a proper type and size of context, and a
dimensionality reduction based on a stochastic low-rank approximation. Besides
being both simple and intuitive, this method also provides an encoding function
which can be used to infer unseen words or phrases. This becomes a clear
advantage compared to predictive models which must train these new words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4933</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4933</id><created>2014-12-16</created><authors><author><keyname>Dutta</keyname><forenames>Sankha Baran</forenames></author><author><keyname>McLeod</keyname><forenames>Robert</forenames></author><author><keyname>Friesen</keyname><forenames>Marcia</forenames></author></authors><title>GPU accelerated Nature Inspired Methods for Modelling Large Scale
  Bi-Directional Pedestrian Movement</title><categories>cs.DC</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Pedestrian movement, although ubiquitous and well-studied, is still not that
well understood due to the complicating nature of the embedded social dynamics.
Interest among researchers in simulating pedestrian movement and interactions
has grown significantly in part due to increased computational and
visualization capabilities afforded by high power computing. Different
approaches have been adopted to simulate pedestrian movement under various
circumstances and interactions. In the present work, bi-directional crowd
movement is simulated where an equal numbers of individuals try to reach the
opposite sides of an environment. Two movement methods are considered. First a
Least Effort Model (LEM) is investigated where agents try to take an optimal
path with as minimal changes from their intended path as possible. Following
this, a modified form of Ant Colony Optimization (ACO) is proposed, where
individuals are guided by a goal of reaching the other side in a least effort
mode as well as a pheromone trail left by predecessors. The basic idea is to
increase agent interaction, thereby more closely reflecting a real world
scenario. The methodology utilizes Graphics Processing Units (GPUs) for general
purpose computing using the CUDA platform. Because of the inherent parallel
properties associated with pedestrian movement such as proximate interactions
of individuals on a 2D grid, GPUs are well suited. The main feature of the
implementation undertaken here is that the parallelism is data driven. The data
driven implementation leads to a speedup up to 18x compared to its sequential
counterpart running on a single threaded CPU. The numbers of pedestrians
considered in the model ranged from 2K to 100K representing numbers typical of
mass gathering events. A detailed discussion addresses implementation
challenges faced and averted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4940</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4940</id><created>2014-12-16</created><authors><author><keyname>Marchesotti</keyname><forenames>Luca</forenames></author><author><keyname>Murray</keyname><forenames>Naila</forenames></author><author><keyname>Perronnin</keyname><forenames>Florent</forenames></author></authors><title>Discovering beautiful attributes for aesthetic image analysis</title><categories>cs.CV</categories><comments>IJCV, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Aesthetic image analysis is the study and assessment of the aesthetic
properties of images. Current computational approaches to aesthetic image
analysis either provide accurate or interpretable results. To obtain both
accuracy and interpretability by humans, we advocate the use of learned and
nameable visual attributes as mid-level features. For this purpose, we propose
to discover and learn the visual appearance of attributes automatically, using
a recently introduced database, called AVA, which contains more than 250,000
images together with their aesthetic scores and textual comments given by
photography enthusiasts. We provide a detailed analysis of these annotations as
well as the context in which they were given. We then describe how these three
key components of AVA - images, scores, and comments - can be effectively
leveraged to learn visual attributes. Lastly, we show that these learned
attributes can be successfully used in three applications: aesthetic quality
prediction, image tagging and retrieval.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4944</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4944</id><created>2014-12-16</created><authors><author><keyname>Irofti</keyname><forenames>Paul</forenames></author></authors><title>Efficient GPU Implementation for Single Block Orthogonal Dictionary
  Learning</title><categories>cs.CV cs.DC</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dictionary training for sparse representations involves dealing with large
chunks of data and complex algorithms that determine time consuming
implementations. SBO is an iterative dictionary learning algorithm based on
constructing unions of orthonormal bases via singular value decomposition, that
represents each data item through a single best fit orthobase. In this paper we
present a GPGPU approach of implementing SBO in OpenCL. We provide a lock-free
solution that ensures full-occupancy of the GPU by following the map-reduce
model for the sparse-coding stage and by making use of the Partitioned Global
Address Space (PGAS) model for developing parallel dictionary updates. The
resulting implementation achieves a favourable trade-off between algorithm
complexity and data representation quality compared to PAK-SVD which is the
standard overcomplete dictionary learning approach. We present and discuss
numerical results showing a significant acceleration of the execution time for
the dictionary learning process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4957</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4957</id><created>2014-12-16</created><authors><author><keyname>Georgiou</keyname><forenames>Orestis</forenames></author><author><keyname>Bocus</keyname><forenames>Mohammud Z.</forenames></author><author><keyname>Rahman</keyname><forenames>Mohammed R.</forenames></author><author><keyname>Dettmann</keyname><forenames>Carl P.</forenames></author><author><keyname>Coon</keyname><forenames>Justin P.</forenames></author></authors><title>Network connectivity in non-convex domains with reflections</title><categories>cs.NI cond-mat.dis-nn cs.IT math.IT</categories><comments>4 pages, 4 figures, letter</comments><journal-ref>IEEE Commun. Lett. 19 427-430 (2015)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent research has demonstrated the importance of boundary effects on the
overall connection probability of wireless networks but has largely focused on
convex deployment regions. We consider here a scenario of practical importance
to wireless communications, in which one or more nodes are located outside the
convex space where the remaining nodes reside. We call these `external nodes',
and assume that they play some essential role in the macro network
functionality e.g. a gateway to a dense self-contained mesh network cloud.
Conventional approaches with the underlying assumption of only line-of-sight
(LOS) or direct connections between nodes, fail to provide the correct analysis
for such a network setup. To this end we present a novel analytical framework
that accommodates for the non-convexity of the domain and explicitly considers
the effects of non-LOS nodes through reflections from the domain boundaries. We
obtain analytical expressions in 2D and 3D which are confirmed numerically for
Rician channel fading statistics and discuss possible extensions and
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4958</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4958</id><created>2014-12-16</created><updated>2015-08-27</updated><authors><author><keyname>Tyagi</keyname><forenames>Himanshu</forenames></author><author><keyname>Vardy</keyname><forenames>Alexander</forenames></author></authors><title>Universal Hashing for Information Theoretic Security</title><categories>cs.IT math.IT</categories><comments>This is a slightly extended version of a review article to appear in
  the Proceedings of IEEE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The information theoretic approach to security entails harnessing the
correlated randomness available in nature to establish security. It uses tools
from information theory and coding and yields provable security, even against
an adversary with unbounded computational power. However, the feasibility of
this approach in practice depends on the development of efficiently
implementable schemes. In this article, we review a special class of practical
schemes for information theoretic security that are based on 2-universal hash
families. Specific cases of secret key agreement and wiretap coding are
considered, and general themes are identified. The scheme presented for wiretap
coding is modular and can be implemented easily by including an extra
pre-processing layer over the existing transmission codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4963</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4963</id><created>2014-12-16</created><updated>2015-05-22</updated><authors><author><keyname>Roy</keyname><forenames>Shibdas</forenames></author><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author><author><keyname>Huntington</keyname><forenames>Elanor H.</forenames></author></authors><title>Robust Adaptive Quantum Phase Estimation</title><categories>quant-ph cs.SY math.OC physics.optics</categories><comments>25 pages, 8 figures, Journal version (accepted)</comments><journal-ref>New Journal of Physics 17 (2015), 063020</journal-ref><doi>10.1088/1367-2630/17/6/063020</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum parameter estimation is central to many fields such as quantum
computation, communications and metrology. Optimal estimation theory has been
instrumental in achieving the best accuracy in quantum parameter estimation,
which is possible when we have very precise knowledge of and control over the
model. However, uncertainties in key parameters underlying the system are
unavoidable and may impact the quality of the estimate. We show here how
quantum optical phase estimation of a squeezed state of light exhibits
improvement when using a robust fixed-interval smoother designed with
uncertainties explicitly introduced in parameters underlying the phase noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4967</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4967</id><created>2014-12-16</created><authors><author><keyname>Knittel</keyname><forenames>Anthony</forenames></author><author><keyname>Blair</keyname><forenames>Alan</forenames></author></authors><title>Sparse, guided feature connections in an Abstract Deep Network</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a technique for developing a network of re-used features, where
the topology is formed using a coarse learning method, that allows
gradient-descent fine tuning, known as an Abstract Deep Network (ADN). New
features are built based on observed co-occurrences, and the network is
maintained using a selection process related to evolutionary algorithms. This
allows coarse ex- ploration of the problem space, effective for irregular
domains, while gradient descent allows pre- cise solutions. Accuracy on
standard UCI and Protein-Structure Prediction problems is comparable with
benchmark SVM and optimized GBML approaches, and shows scalability for
addressing large problems. The discrete implementation is symbolic, allowing
interpretability, while the continuous method using fine-tuning shows improved
accuracy. The binary multiplexer problem is explored, as an irregular domain
that does not support gradient descent learning, showing solution to the bench-
mark 135-bit problem. A convolutional implementation is demonstrated on image
classification, showing an error-rate of 0.79% on the MNIST problem, without a
pre-defined topology. The ADN system provides a method for developing a very
sparse, deep feature topology, based on observed relationships between
features, that is able to find solutions in irregular domains, and initialize a
network prior to gradient descent learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4972</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4972</id><created>2014-12-16</created><updated>2015-10-04</updated><authors><author><keyname>Park</keyname><forenames>Sejun</forenames></author><author><keyname>Shin</keyname><forenames>Jinwoo</forenames></author></authors><title>Max-Product Belief Propagation for Linear Programming: Applications to
  Combinatorial Optimization</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Max-product belief propagation (BP) is a popular message-passing algorithm
for computing a maximum-a-posteriori (MAP) assignment in a joint distribution
represented by a graphical model (GM). It has been shown that BP can solve a
few classes of Linear Programming (LP) formulations to combinatorial
optimization problems including maximum weight matching and shortest path,
i.e., BP can be a distributed solver for certain LPs. However, those LPs and
corresponding BP analysis are very sensitive to underlying problem setups, and
it has been not clear what extent these results can be generalized to. In this
paper, we obtain a generic criteria that BP converges to the optimal solution
of given LP, and show that it is satisfied in LP formulations associated to
many classical combinatorial optimization problems including maximum weight
perfect matching, shortest path, traveling salesman, cycle packing and vertex
cover. More importantly, our criteria can guide the BP design to compute
fractional LP solutions, while most prior results focus on integral ones. Our
results provide new tools on BP analysis and new directions on efficient
solvers for large-scale LPs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4973</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4973</id><created>2014-12-16</created><updated>2014-12-18</updated><authors><author><keyname>Dreier</keyname><forenames>Jan</forenames></author><author><keyname>Kuinke</keyname><forenames>Philipp</forenames></author><author><keyname>Przybylski</keyname><forenames>Rafael</forenames></author><author><keyname>Reidl</keyname><forenames>Felix</forenames></author><author><keyname>Rossmanith</keyname><forenames>Peter</forenames></author><author><keyname>Sikdar</keyname><forenames>Somnath</forenames></author></authors><title>Overlapping Communities in Social Networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex networks can be typically broken down into groups or modules.
Discovering this &quot;community structure&quot; is an important step in studying the
large-scale structure of networks. Many algorithms have been proposed for
community detection and benchmarks have been created to evaluate their
performance. Typically algorithms for community detection either partition the
graph (non-overlapping communities) or find node covers (overlapping
communities).
  In this paper, we propose a particularly simple semi-supervised learning
algorithm for finding out communities. In essence, given the community
information of a small number of &quot;seed nodes&quot;, the method uses random walks
from the seed nodes to uncover the community information of the whole network.
The algorithm runs in time $O(k \cdot m \cdot \log n)$, where $m$ is the number
of edges; $n$ the number of links; and $k$ the number of communities in the
network. In sparse networks with $m = O(n)$ and a constant number of
communities, this running time is almost linear in the size of the network.
Another important feature of our algorithm is that it can be used for either
non-overlapping or overlapping communities.
  We test our algorithm using the LFR benchmark created by Lancichinetti,
Fortunato, and Radicchi specifically for the purpose of evaluating such
algorithms. Our algorithm can compete with the best of algorithms for both
non-overlapping and overlapping communities as found in the comprehensive study
of Lancichinetti and Fortunato.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4980</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4980</id><created>2014-12-16</created><authors><author><keyname>Wang</keyname><forenames>Huandong</forenames></author><author><keyname>Li</keyname><forenames>Yong</forenames></author><author><keyname>Zhang</keyname><forenames>Ying</forenames></author><author><keyname>Jin</keyname><forenames>Depeng</forenames></author></authors><title>Virtual Machine Migration Planning in Software-Defined Networks</title><categories>cs.NI</categories><comments>To appear at Infocom 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we examine the problem of how to schedule the migrations and
how to allocate network resources for migration when multiple VMs need to be
migrated at the same time. We consider the problem in the Software-defined
Network (SDN) context since it provides flexible control on routing. More
specifically, we propose a method that computes the optimal migration sequence
and network bandwidth used for each migration. We formulate this problem as a
mixed integer programming, which is NP-hard. To make it computationally
feasible for large scale data centers, we propose an approximation scheme via
linear approximation plus fully polynomial time approximation, and obtain its
theoretical performance bound. Through extensive simulations, we demonstrate
that our fully polynomial time approximation (FPTA) algorithm has a good
performance compared with the optimal solution and two state of-the-art
algorithms. That is, our proposed FPTA algorithm approaches to the optimal
solution with less than 10% variation and much less computation time.
Meanwhile, it reduces the total migration time and the service downtime by up
to 40% and 20% compared with the state-of-the-art algorithms, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4986</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4986</id><created>2014-12-16</created><authors><author><keyname>Yu</keyname><forenames>Hsiang-Fu</forenames></author><author><keyname>Hsieh</keyname><forenames>Cho-Jui</forenames></author><author><keyname>Yun</keyname><forenames>Hyokun</forenames></author><author><keyname>Vishwanathan</keyname><forenames>S. V. N</forenames></author><author><keyname>Dhillon</keyname><forenames>Inderjit S.</forenames></author></authors><title>A Scalable Asynchronous Distributed Algorithm for Topic Modeling</title><categories>cs.DC cs.IR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning meaningful topic models with massive document collections which
contain millions of documents and billions of tokens is challenging because of
two reasons: First, one needs to deal with a large number of topics (typically
in the order of thousands). Second, one needs a scalable and efficient way of
distributing the computation across multiple machines. In this paper we present
a novel algorithm F+Nomad LDA which simultaneously tackles both these problems.
In order to handle large number of topics we use an appropriately modified
Fenwick tree. This data structure allows us to sample from a multinomial
distribution over $T$ items in $O(\log T)$ time. Moreover, when topic counts
change the data structure can be updated in $O(\log T)$ time. In order to
distribute the computation across multiple processor we present a novel
asynchronous framework inspired by the Nomad algorithm of
\cite{YunYuHsietal13}. We show that F+Nomad LDA significantly outperform
state-of-the-art on massive problems which involve millions of documents,
billions of words, and thousands of topics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4988</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4988</id><created>2014-12-16</created><authors><author><keyname>Burton</keyname><forenames>Benjamin A.</forenames></author><author><keyname>de Verdi&#xe8;re</keyname><forenames>&#xc9;ric Colin</forenames></author><author><keyname>de Mesmay</keyname><forenames>Arnaud</forenames></author></authors><title>On the Complexity of Immersed Normal Surfaces</title><categories>math.GT cs.CG</categories><comments>17 pages, under journal submission</comments><msc-class>68U05, 57M50, 68Q25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Normal surface theory, a tool to represent surfaces in a triangulated
3-manifold combinatorially, is ubiquitous in computational 3-manifold theory.
In this paper, we investigate a relaxed notion of normal surfaces where we
remove the quadrilateral conditions. This yields normal surfaces that are no
longer embedded. We prove that it is NP-hard to decide whether such a surface
is immersed. Our proof uses a reduction from Boolean constraint satisfaction
problems where every variable appears in at most two clauses, using a
classification theorem of Feder. We also investigate variants, and provide a
polynomial-time algorithm to test for a local version of this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.4999</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.4999</id><created>2014-12-16</created><updated>2014-12-17</updated><authors><author><keyname>Elsayed</keyname><forenames>Medhat H. M.</forenames></author><author><keyname>El-Sherif</keyname><forenames>Amr A.</forenames></author><author><keyname>Mohamed</keyname><forenames>Amr</forenames></author></authors><title>DDSAT: Distributed Dynamic Spectrum Access Protocol Implementation Using
  GNURadio and USRP</title><categories>cs.NI</categories><comments>6 pages, 9 figures, conference paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Frequency spectrum is one of the valuable resources in wireless
communications. Using cognitive radio, spectrum efficiency will increase by
making use of the spectrum holes. Dynamic Spectrum Access techniques allows
secondary users to transmit on an empty channel not used by a primary user for
a given time. In this paper, a Distributed Dynamic Spectrum Access based TDMA
protocol (DDSAT) is designed and implemented on USRP. The proposed protocol
performs two main functions: Spectrum Sensing, and Spectrum Management.
Spectrum Sensing is performed to find spectrum holes in a co-operative manner
using the contributing secondary users. Spectrum Management works
distributively on the secondary users to allocate the spectrum holes in a
fairly and efficient utilization. The DDSAT protocol is implemented using
Software Defined Radio (SDR) and Universal Software Radio Peripheral (USRP).
Evaluation and performance tests are conducted to show throughput and fairness
of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5010</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5010</id><created>2014-12-16</created><authors><author><keyname>Ma&#xdf;berg</keyname><forenames>Jens</forenames></author></authors><title>The rectilinear Steiner tree problem with given topology and length
  restrictions</title><categories>cs.DS cs.CG</categories><comments>14 pages</comments><journal-ref>Computing and Combinatorics, Lecture Notes in Computer Science,
  Volume 9198, 2015, pp 445-456</journal-ref><doi>10.1007/978-3-319-21398-9_35</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of embedding the Steiner points of a Steiner tree
with given topology into the rectilinear plane. Thereby, the length of the path
between a distinguished terminal and each other terminal must not exceed given
length restrictions. We want to minimize the total length of the tree.
  The problem can be formulated as a linear program and therefore it is
solvable in polynomial time. In this paper we analyze the structure of feasible
embeddings and give a combinatorial polynomial time algorithm for the problem.
Our algorithm combines a dynamic programming approach and binary search and
relies on the total unimodularity of a matrix appearing in a sub-problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5012</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5012</id><created>2014-12-16</created><authors><author><keyname>Augot</keyname><forenames>Daniel</forenames><affiliation>LIX</affiliation></author><author><keyname>Levy-Dit-Vehel</keyname><forenames>Fran&#xe7;oise</forenames><affiliation>UMA</affiliation></author><author><keyname>Shikfa</keyname><forenames>Abdullatif</forenames></author></authors><title>A Storage-Efficient and Robust Private Information Retrieval Scheme
  Allowing Few Servers</title><categories>cs.CR</categories><proxy>ccsd</proxy><journal-ref>13th International Conference, CANS 2014, Heraklion, Crete,
  Greece, October 22-24, 2014. Proceedings, Oct 2014, Heraklion, Greece.
  springer, Lecture notes in computer science, 8813, pp.222 - 239, Cryptology
  and Network Security (CANS)</journal-ref><doi>10.1007/978-3-319-12280-9_15</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since the concept of locally decodable codes was introduced by Katz and
Trevisan in 2000, it is well-known that information the-oretically secure
private information retrieval schemes can be built using locally decodable
codes. In this paper, we construct a Byzantine ro-bust PIR scheme using the
multiplicity codes introduced by Kopparty et al. Our main contributions are on
the one hand to avoid full replica-tion of the database on each server; this
significantly reduces the global redundancy. On the other hand, to have a much
lower locality in the PIR context than in the LDC context. This shows that
there exists two different notions: LDC-locality and PIR-locality. This is made
possible by exploiting geometric properties of multiplicity codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5027</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5027</id><created>2014-12-08</created><authors><author><keyname>Borji</keyname><forenames>Ali</forenames></author></authors><title>What is a salient object? A dataset and a baseline model for salient
  object detection</title><categories>cs.CV</categories><comments>IEEE Transactions on Image Processing, 2014</comments><doi>10.1109/TIP.2014.2383320</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Salient object detection or salient region detection models, diverging from
fixation prediction models, have traditionally been dealing with locating and
segmenting the most salient object or region in a scene. While the notion of
most salient object is sensible when multiple objects exist in a scene, current
datasets for evaluation of saliency detection approaches often have scenes with
only one single object. We introduce three main contributions in this paper:
First, we take an indepth look at the problem of salient object detection by
studying the relationship between where people look in scenes and what they
choose as the most salient object when they are explicitly asked. Based on the
agreement between fixations and saliency judgments, we then suggest that the
most salient object is the one that attracts the highest fraction of fixations.
Second, we provide two new less biased benchmark datasets containing scenes
with multiple objects that challenge existing saliency models. Indeed, we
observed a severe drop in performance of 8 state-of-the-art models on our
datasets (40% to 70%). Third, we propose a very simple yet powerful model based
on superpixels to be used as a baseline for model evaluation and comparison.
While on par with the best models on MSRA-5K dataset, our model wins over other
models on our data highlighting a serious drawback of existing models, which is
convoluting the processes of locating the most salient object and its
segmentation. We also provide a review and statistical analysis of some labeled
scene datasets that can be used for evaluating salient object detection models.
We believe that our work can greatly help remedy the over-fitting of models to
existing biased datasets and opens new venues for future research in this
fast-evolving field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5034</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5034</id><created>2014-12-16</created><updated>2015-01-20</updated><authors><author><keyname>Abrahamsen</keyname><forenames>Mikkel</forenames></author></authors><title>Spiral Toolpaths for High-Speed Machining of 2D Pockets with or without
  Islands</title><categories>cs.CG</categories><comments>22 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe new methods for the construction of spiral toolpaths for
high-speed machining. In the simplest case, our method takes a polygon as input
and a number $\delta&gt;0$ and returns a spiral starting at a central point in the
polygon, going around towards the boundary while morphing to the shape of the
polygon. The spiral consists of linear segments and circular arcs, it is $G^1$
continuous, it has no self-intersections, and the distance from each point on
the spiral to each of the neighboring revolutions is at most $\delta$. Our
method has the advantage over previously described methods that it is easily
adjustable to the case where there is an island in the polygon to be avoided by
the spiral. In that case, the spiral starts at the island and morphs the island
to the outer boundary of the polygon. It is shown how to apply that method to
make significantly shorter spirals in polygons with no islands. Finally, we
show how to make a spiral in a polygon with multiple islands by connecting the
islands into one island.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5050</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5050</id><created>2014-12-09</created><authors><author><keyname>Schreiber</keyname><forenames>Michael</forenames></author></authors><title>Restricting the h-index to a citation time window: A case study of a
  timed Hirsch index</title><categories>cs.DL physics.soc-ph</categories><comments>6 pages, 4 figures</comments><journal-ref>Journal of Informetrics 9, 150-155 (2015)</journal-ref><doi>10.1016/j.joi.2014.12.005</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The h-index has been shown to increase in many cases mostly because of
citations to rather old publications. This inertia can be circumvented by
restricting the evaluation to a citation time window. Here I report results of
an empirical study analyzing the evolution of the thus defined timed h-index in
dependence on the length of the citation time window.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5052</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5052</id><created>2014-12-16</created><updated>2015-04-20</updated><authors><author><keyname>Schlamp</keyname><forenames>Johann</forenames></author><author><keyname>Gustafsson</keyname><forenames>Josef</forenames></author><author><keyname>W&#xe4;hlisch</keyname><forenames>Matthias</forenames></author><author><keyname>Schmidt</keyname><forenames>Thomas C.</forenames></author><author><keyname>Carle</keyname><forenames>Georg</forenames></author></authors><title>The Abandoned Side of the Internet: Hijacking Internet Resources When
  Domain Names Expire</title><categories>cs.NI cs.CR</categories><comments>Final version for TMA 2015</comments><proxy>Matthias W\~Ahlisch</proxy><acm-class>C.2.3; C.2.0</acm-class><journal-ref>Proceedings of TMA 2015, LNCS vol. 9053, pp. 188 - 201, 2015</journal-ref><doi>10.1007/978-3-319-17172-2_13</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The vulnerability of the Internet has been demonstrated by prominent IP
prefix hijacking events. Major outages such as the China Telecom incident in
2010 stimulate speculations about malicious intentions behind such anomalies.
Surprisingly, almost all discussions in the current literature assume that
hijacking incidents are enabled by the lack of security mechanisms in the
inter-domain routing protocol BGP. In this paper, we discuss an attacker model
that accounts for the hijacking of network ownership information stored in
Regional Internet Registry (RIR) databases. We show that such threats emerge
from abandoned Internet resources (e.g., IP address blocks, AS numbers). When
DNS names expire, attackers gain the opportunity to take resource ownership by
re-registering domain names that are referenced by corresponding RIR database
objects. We argue that this kind of attack is more attractive than conventional
hijacking, since the attacker can act in full anonymity on behalf of a victim.
Despite corresponding incidents have been observed in the past, current
detection techniques are not qualified to deal with these attacks. We show that
they are feasible with very little effort, and analyze the risk potential of
abandoned Internet resources for the European service region: our findings
reveal that currently 73 /24 IP prefixes and 7 ASes are vulnerable to be
stealthily abused. We discuss countermeasures and outline research directions
towards preventive solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5065</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5065</id><created>2014-12-16</created><updated>2015-06-05</updated><authors><author><keyname>Galiotto</keyname><forenames>Carlo</forenames></author><author><keyname>Pratas</keyname><forenames>Nuno K.</forenames></author><author><keyname>Marchetti</keyname><forenames>Nicola</forenames></author><author><keyname>Doyle</keyname><forenames>Linda</forenames></author></authors><title>A Stochastic Geometry Framework for LOS/NLOS Propagation in Dense Small
  Cell Networks</title><categories>cs.IT math.IT</categories><comments>Typo corrected in eq. (3); Typo corrected in legend of Fig. 1-2;
  Typos corrected and definitions of some variables added in Section III.E;
  Final result unchanged; Paper accepted to IEEE ICC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The need to carry out analytical studies of wireless systems often motivates
the usage of simplified models which, despite their tractability, can easily
lead to an overestimation of the achievable performance. In the case of dense
small cells networks, the standard single slope path-loss model has been shown
to provide interesting, but supposedly too optimistic, properties such as the
invariance of the outage/coverage probability and of the spectral efficiency to
the base station density. This paper seeks to explore the performance of dense
small cells networks when a more accurate path-loss model is taken into
account. We first propose a stochastic geometry based framework for small cell
networks where the signal propagation accounts for both the Line-of-Sight (LOS)
and Non-Line-Of-Sight (NLOS) components, such as the model provided by the 3GPP
for evaluation of pico-cells in Heterogeneous Networks. We then study the
performance of these networks and we show the dependency of some metrics such
as the outage/coverage probability, the spectral efficiency and Area Spectral
Efficiency (ASE) on the base station density and on the LOS likelihood of the
propagation environment. Specifically, we show that, with LOS/NLOS propagation,
dense networks still achieve large ASE gain but, at the same time, suffer from
high outage probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5067</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5067</id><created>2014-12-16</created><authors><author><keyname>Eremeev</keyname><forenames>A. V.</forenames></author><author><keyname>Kovalenko</keyname><forenames>Ju. V.</forenames></author></authors><title>Analysis of Optimal Recombination in Genetic Algorithm for a Scheduling
  Problem with Setups</title><categories>cs.NE</categories><comments>13 pages, in Russian</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we perform an experimental study of optimal recombination
operator for makespan minimization problem on single machine with
sequence-dependent setup times ($1|s_{vu}|C_{\max}$). The computational
experiment on benchmark problems from TSPLIB library indicates practical
applicability of optimal recombination in crossover operator of genetic
algorithm for $1|s_{vu}|C_{\max}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5068</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5068</id><created>2014-12-11</created><updated>2015-04-09</updated><authors><author><keyname>Gu</keyname><forenames>Shixiang</forenames></author><author><keyname>Rigazio</keyname><forenames>Luca</forenames></author></authors><title>Towards Deep Neural Network Architectures Robust to Adversarial Examples</title><categories>cs.LG cs.CV cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work has shown deep neural networks (DNNs) to be highly susceptible to
well-designed, small perturbations at the input layer, or so-called adversarial
examples. Taking images as an example, such distortions are often
imperceptible, but can result in 100% mis-classification for a state of the art
DNN. We study the structure of adversarial examples and explore network
topology, pre-processing and training strategies to improve the robustness of
DNNs. We perform various experiments to assess the removability of adversarial
examples by corrupting with additional noise and pre-processing with denoising
autoencoders (DAEs). We find that DAEs can remove substantial amounts of the
adversarial noise. How- ever, when stacking the DAE with the original DNN, the
resulting network can again be attacked by new adversarial examples with even
smaller distortion. As a solution, we propose Deep Contractive Network, a model
with a new end-to-end training procedure that includes a smoothness penalty
inspired by the contractive autoencoder (CAE). This increases the network
robustness to adversarial examples, without a significant performance penalty.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5071</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5071</id><created>2014-12-16</created><updated>2015-06-17</updated><authors><author><keyname>Harrison</keyname><forenames>Gavin</forenames></author><author><keyname>Johnson</keyname><forenames>Jeremy</forenames></author><author><keyname>Saunders</keyname><forenames>B. David</forenames></author></authors><title>Probabilistic analysis of Wiedemann's algorithm for minimal polynomial
  computation</title><categories>cs.SC</categories><comments>19 pages. To be published in the Journal of Symbolic Computation,
  please cite as &quot;Harrison, G., et al. Probabilistic analysis of Wiedemann's
  algorithm for minimal polynomial computation. J. Symb. Comput. (2015),
  http://dx.doi.org/10.1016/j.jsc.2015.06.005&quot;</comments><acm-class>F.2.1; I.1.2</acm-class><doi>10.1016/j.jsc.2015.06.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Blackbox algorithms for linear algebra problems start with projection of the
sequence of powers of a matrix to a sequence of vectors (Lanczos), a sequence
of scalars (Wiedemann) or a sequence of smaller matrices (block methods). Such
algorithms usually depend on the minimal polynomial of the resulting sequence
being that of the given matrix. Here exact formulas are given for the
probability that this occurs. They are based on the generalized Jordan normal
form (direct sum of companion matrices of the elementary divisors) of the
matrix. Sharp bounds follow from this for matrices of unknown elementary
divisors. The bounds are valid for all finite field sizes and show that a small
blocking factor can give high probability of success for all cardinalities and
matrix dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5075</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5075</id><created>2014-12-16</created><authors><author><keyname>Eppstein</keyname><forenames>David</forenames></author></authors><title>$k$-best enumeration</title><categories>cs.DS</categories><comments>17 pages. A significantly shorter version of this material appears in
  the Springer Encyclopedia of Algorithms, 2014</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We survey $k$-best enumeration problems and the algorithms for solving them,
including in particular the problems of finding the $k$ shortest paths, $k$
smallest spanning trees, and $k$ best matchings in weighted graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5077</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5077</id><created>2014-12-16</created><authors><author><keyname>&#x15e;ahin</keyname><forenames>R&#x131;dvan</forenames></author><author><keyname>Yi&#x11f;ider</keyname><forenames>Muhammed</forenames></author></authors><title>A Multi-criteria neutrosophic group decision making metod based TOPSIS
  for supplier selection</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The process of multiple criteria decision making (MCDM) is of determining the
best choice among all of the probable alternatives. The problem of supplier
selection on which decision maker has usually vague and imprecise knowledge is
a typical example of multi criteria group decision-making problem. The
conventional crisp techniques has not much effective for solving MCDM problems
because of imprecise or fuzziness nature of the linguistic assessments. To find
the exact values for MCDM problems is both difficult and impossible in more
cases in real world. So, it is more reasonable to consider the values of
alternatives according to the criteria as single valued neutrosophic sets
(SVNS). This paper deal with the technique for order preference by similarity
to ideal solution (TOPSIS) approach and extend the TOPSIS method to MCDM
problem with single valued neutrosophic information. The value of each
alternative and the weight of each criterion are characterized by single valued
neutrosophic numbers. Here, the importance of criteria and alternatives is
identified by aggregating individual opinions of decision makers (DMs) via
single valued neutrosophic weighted averaging (IFWA) operator. The proposed
method is, easy use, precise and practical for solving MCDM problem with single
valued neutrosophic data. Finally, to show the applicability of the developed
method, a numerical experiment for supplier choice is given as an application
of single valued neutrosophic TOPSIS method at end of this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5083</identifier>
 <datestamp>2015-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5083</id><created>2014-12-16</created><updated>2015-04-16</updated><authors><author><keyname>Qiu</keyname><forenames>Qiang</forenames></author><author><keyname>Sapiro</keyname><forenames>Guillermo</forenames></author><author><keyname>Bronstein</keyname><forenames>Alex</forenames></author></authors><title>Random Forests Can Hash</title><categories>cs.CV cs.IR cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hash codes are a very efficient data representation needed to be able to cope
with the ever growing amounts of data. We introduce a random forest semantic
hashing scheme with information-theoretic code aggregation, showing for the
first time how random forest, a technique that together with deep learning have
shown spectacular results in classification, can also be extended to
large-scale retrieval. Traditional random forest fails to enforce the
consistency of hashes generated from each tree for the same class data, i.e.,
to preserve the underlying similarity, and it also lacks a principled way for
code aggregation across trees. We start with a simple hashing scheme, where
independently trained random trees in a forest are acting as hashing functions.
We the propose a subspace model as the splitting function, and show that it
enforces the hash consistency in a tree for data from the same class. We also
introduce an information-theoretic approach for aggregating codes of individual
trees into a single hash code, producing a near-optimal unique hash for each
class. Experiments on large-scale public datasets are presented, showing that
the proposed approach significantly outperforms state-of-the-art hashing
methods for retrieval tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5090</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5090</id><created>2014-12-16</created><updated>2014-12-17</updated><authors><author><keyname>van Eijck</keyname><forenames>Jan</forenames></author><author><keyname>Renne</keyname><forenames>Bryan</forenames></author></authors><title>Belief as Willingness to Bet</title><categories>cs.LO cs.AI</categories><comments>Removed date from v1 to avoid confusion on citation/reference,
  otherwise identical to v1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate modal logics of high probability having two unary modal
operators: an operator $K$ expressing probabilistic certainty and an operator
$B$ expressing probability exceeding a fixed rational threshold $c\geq\frac
12$. Identifying knowledge with the former and belief with the latter, we may
think of $c$ as the agent's betting threshold, which leads to the motto &quot;belief
is willingness to bet.&quot; The logic $\mathsf{KB.5}$ for $c=\frac 12$ has an
$\mathsf{S5}$ $K$ modality along with a sub-normal $B$ modality that extends
the minimal modal logic $\mathsf{EMND45}$ by way of four schemes relating $K$
and $B$, one of which is a complex scheme arising out of a theorem due to
Scott. Lenzen was the first to use Scott's theorem to show that a version of
this logic is sound and complete for the probability interpretation. We
reformulate Lenzen's results and present them here in a modern and accessible
form. In addition, we introduce a new epistemic neighborhood semantics that
will be more familiar to modern modal logicians. Using Scott's theorem, we
provide the Lenzen-derivative properties that must be imposed on finite
epistemic neighborhood models so as to guarantee the existence of a probability
measure respecting the neighborhood function in the appropriate way for
threshold $c=\frac 12$. This yields a link between probabilistic and modal
neighborhood semantics that we hope will be of use in future work on modal
logics of qualitative probability. We leave open the question of which
properties must be imposed on finite epistemic neighborhood models so as to
guarantee existence of an appropriate probability measure for thresholds
$c\neq\frac 12$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5104</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5104</id><created>2014-12-16</created><authors><author><keyname>Kanazawa</keyname><forenames>Angjoo</forenames></author><author><keyname>Sharma</keyname><forenames>Abhishek</forenames></author><author><keyname>Jacobs</keyname><forenames>David</forenames></author></authors><title>Locally Scale-Invariant Convolutional Neural Networks</title><categories>cs.CV cs.LG cs.NE</categories><comments>Deep Learning and Representation Learning Workshop: NIPS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional Neural Networks (ConvNets) have shown excellent results on many
visual classification tasks. With the exception of ImageNet, these datasets are
carefully crafted such that objects are well-aligned at similar scales.
Naturally, the feature learning problem gets more challenging as the amount of
variation in the data increases, as the models have to learn to be invariant to
certain changes in appearance. Recent results on the ImageNet dataset show that
given enough data, ConvNets can learn such invariances producing very
discriminative features [1]. But could we do more: use less parameters, less
data, learn more discriminative features, if certain invariances were built
into the learning process? In this paper we present a simple model that allows
ConvNets to learn features in a locally scale-invariant manner without
increasing the number of model parameters. We show on a modified MNIST dataset
that when faced with scale variation, building in scale-invariance allows
ConvNets to learn more discriminative features with reduced chances of
over-fitting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5111</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5111</id><created>2014-12-16</created><authors><author><keyname>Doostmohammadian</keyname><forenames>Mohammadreza</forenames></author><author><keyname>Khan</keyname><forenames>Usman A.</forenames></author></authors><title>Measurement partitioning and observational equivalence in state
  estimation</title><categories>cs.IT math.IT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter studies measurement partitioning and equivalence in state
estimation based on graph-theoretic principles. We show that a set of critical
measurements (required to ensure LTI state-space observability) can be further
partitioned into two types:~$\alpha$ and~$\beta$. This partitioning is driven
by different graphical (or algebraic) methods used to define the corresponding
measurements. Subsequently, we describe observational equivalence, i.e. given
an~$\alpha$ (or~$\beta$) measurement, say~$y_i$, what is the set of
measurements equivalent to~$y_i$, such that only one measurement in this set is
required to ensure observability? Since~$\alpha$ and~$\beta$ measurements are
cast using different algebraic and graphical characteristics, their equivalence
sets are also derived using different algebraic and graph-theoretic principles.
We illustrate the related concepts on an appropriate system digraph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5126</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5126</id><created>2014-12-16</created><updated>2015-09-01</updated><authors><author><keyname>Minaee</keyname><forenames>Shervin</forenames></author><author><keyname>Yu</keyname><forenames>Haoping</forenames></author><author><keyname>Wang</keyname><forenames>Yao</forenames></author></authors><title>A Robust Regression Approach for Background/Foreground Segmentation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background/foreground segmentation has a lot of applications in image and
video processing. In this paper, a segmentation algorithm is proposed which is
mainly designed for text and line extraction in screen content. The proposed
method makes use of the fact that the background in each block is usually
smoothly varying and can be modeled well by a linear combination of a few
smoothly varying basis functions, while the foreground text and graphics create
sharp discontinuity. The algorithm separates the background and foreground
pixels by trying to fit pixel values in the block into a smooth function using
a robust regression method. The inlier pixels that can fit well will be
considered as background, while remaining outlier pixels will be considered
foreground. This algorithm has been extensively tested on several images from
HEVC standard test sequences for screen content coding, and is shown to have
superior performance over other methods, such as the k-means clustering based
segmentation algorithm in DjVu. This background/foreground segmentation can be
used in different applications such as: text extraction, separate coding of
background and foreground for compression of screen content and mixed content
documents, principle line extraction from palmprint and crease detection in
fingerprint images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5143</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5143</id><created>2014-12-15</created><updated>2015-08-18</updated><authors><author><keyname>Hague</keyname><forenames>Matthew</forenames></author><author><keyname>Lin</keyname><forenames>Anthony Widjaja</forenames></author><author><keyname>Ong</keyname><forenames>Luke</forenames></author></authors><title>Detecting Redundant CSS Rules in HTML5 Applications: A Tree-Rewriting
  Approach</title><categories>cs.LO cs.DB cs.PL cs.SE</categories><comments>50 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  HTML5 applications normally have a large set of CSS (Cascading Style Sheets)
rules for data display. Each CSS rule consists of a node selector (given in an
XPath-like query language) and a declaration block (assigning values to
selected nodes' display attributes). As web applications evolve, maintaining
CSS files can easily become problematic. Some CSS rules will be replaced by new
ones, but these obsolete (hence redundant) CSS rules often remain in the
applications. Not only does this &quot;bloat&quot; the applications, but it also
significantly increases web browsers' processing time. Most works on detecting
redundant CSS rules in HTML5 applications do not consider the dynamic behaviors
of HTML5 (specified in JavaScript); in fact, the only proposed method that
takes these into account is dynamic analysis (a.k.a. testing), which cannot
soundly prove redundancy of CSS rules. In this paper, we introduce an
abstraction of HTML5 applications based on monotonic tree-rewriting and study
its &quot;redundancy problem&quot;. We establish the precise complexity of the problem
and various subproblems of practical importance (ranging from P to EXP). In
particular, our algorithm relies on an efficient reduction to an analysis of
symbolic pushdown systems (for which highly optimised solvers are available),
which yields a fast method for checking redundancy in practice. We implemented
our algorithm and demonstrated its efficacy in detecting redundant CSS rules in
HTML5 applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5149</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5149</id><created>2014-12-16</created><updated>2014-12-17</updated><authors><author><keyname>Alston</keyname><forenames>Aubrey</forenames></author></authors><title>Polynomial-time Method of Determining Subset Sum Solutions</title><categories>cs.DS</categories><acm-class>F.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reducing the conditions under which a given set satisfies the stipulations of
the subset sum proposition to a set of linear relationships, the question of
whether a set satisfies subset sum may be answered in a polynomial number of
steps by forming, solving, and constraining a series of linear systems whose
dimensions and number are both polynomial with respect to the length of the
set. Given its demonstrated 100% accuracy rate and its demonstrable
justification, this algorithm may provide basis to reconsider the validity of
SSP-based and NP-hard-reliant cryptosystems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5150</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5150</id><created>2014-12-15</created><authors><author><keyname>Vassiliadis</keyname><forenames>Vassilis</forenames></author><author><keyname>Parasyris</keyname><forenames>Konstantinos</forenames></author><author><keyname>Chalios</keyname><forenames>Charalambos</forenames></author><author><keyname>Antonopoulos</keyname><forenames>Christos D.</forenames></author><author><keyname>Lalis</keyname><forenames>Spyros</forenames></author><author><keyname>Bellas</keyname><forenames>Nikolaos</forenames></author><author><keyname>Vandierendonck</keyname><forenames>Hans</forenames></author><author><keyname>Nikolopoulos</keyname><forenames>Dimitrios S.</forenames></author></authors><title>A Programming Model and Runtime System for Significance-Aware
  Energy-Efficient Computing</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reducing energy consumption is one of the key challenges in computing
technology. One factor that contributes to high energy consumption is that all
parts of the program are considered equally significant for the accuracy of the
end-result. However, in many cases, parts of computations can be performed in
an approximate way, or even dropped, without affecting the quality of the final
output to a significant degree.
  In this paper, we introduce a task-based programming model and runtime system
that exploit this observation to trade off the quality of program outputs for
increased energy-efficiency. This is done in a structured and flexible way,
allowing for easy exploitation of different execution points in the
quality/energy space, without code modifications and without adversely
affecting application performance. The programmer specifies the significance of
tasks, and optionally provides approximations for them. Moreover, she provides
hints to the runtime on the percentage of tasks that should be executed
accurately in order to reach the target quality of results. The runtime system
can apply a number of different policies to decide whether it will execute each
individual less-significant task in its accurate form, or in its approximate
version. Policies differ in terms of their runtime overhead but also the degree
to which they manage to execute tasks according to the programmer's
specification.
  The results from experiments performed on top of an Intel-based
multicore/multiprocessor platform show that, depending on the runtime policy
used, our system can achieve an energy reduction of up to 83% compared with a
fully accurate execution and up to 35% compared with an approximate version
employing loop perforation. At the same time, our approach always results in
graceful quality degradation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5153</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5153</id><created>2014-12-16</created><updated>2015-09-09</updated><authors><author><keyname>P&#xe9;rez-Lantero</keyname><forenames>Pablo</forenames></author></authors><title>Area and Perimeter of the Convex Hull of Stochastic Points</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set $P$ of $n$ points in the plane, we study the computation of the
probability distribution function of both the area and perimeter of the convex
hull of a random subset $S$ of $P$. The random subset $S$ is formed by drawing
each point $p$ of $P$ independently with a given rational probability $\pi_p$.
For both measures of the convex hull, we show that it is \#P-hard to compute
the probability that the measure is at least a given bound $w$. For
$\varepsilon\in(0,1)$, we provide an algorithm that runs in
$O(n^{6}/\varepsilon)$ time and returns a value that is between the probability
that the area is at least $w$, and the probability that the area is at least
$(1-\varepsilon)w$. For the perimeter, we show a similar algorithm running in
$O(n^{6}/\varepsilon)$ time. Finally, given $\varepsilon,\delta\in(0,1)$ and
for any measure, we show an $O(n\log n+ (n/\varepsilon^2)\log(1/\delta))$-time
Monte Carlo algorithm that returns a value that, with probability of success at
least $1-\delta$, differs at most $\varepsilon$ from the probability that the
measure is at least $w$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5159</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5159</id><created>2014-12-15</created><updated>2014-12-19</updated><authors><author><keyname>Klein</keyname><forenames>Felix</forenames></author></authors><title>Solving 3-Color Parity Games in $ O(n^2) $ Time</title><categories>cs.LO cs.FL cs.GT</categories><comments>This paper has been withdrawn by the author due to a crucial error in
  Lemma 1.7, which additionally needs the assumption that V' is a trap for
  Player 1-i. However, this assumption is not given for equation (1)
  invalidating Theorem 1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parity games are an expressive framework to consider realizability questions
for omega-regular languages. However, it is open whether they can be solved in
polynomial time, making them unamenable for practical usage. To overcome this
restriction, we consider 3-color parity games, which can be solved in
polynomial time. They still cover an expressive fragment of specifications, as
they include the classical B\&quot;uchi and co-B\&quot;uchi winning conditions as well as
their union and intersection. This already suffices to express many useful
combinations of safety and liveness properties, as for example the family of
GR(1). The best known algorithm for 3-color parity games solves a game with n
vertices in $ O(n^{2}\sqrt{n}) $ time. We improve on this result by presenting
a new algorithm, based on simple attractor constructions, which only needs time
$ O(n^2) $. As a result, we match the best known running times for solving
(co)-B\&quot;uchi games, showing that 3-color parity games are not harder to solve
in general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5202</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5202</id><created>2014-12-17</created><authors><author><keyname>&#x15e;ahin</keyname><forenames>R&#x131;dvan</forenames></author></authors><title>Multi-criteria neutrosophic decision making method based on score and
  accuracy functions under neutrosophic environment</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A neutrosophic set is a more general platform, which can be used to present
uncertainty, imprecise, incomplete and inconsistent. In this paper a score
function and an accuracy function for single valued neutrosophic sets is
firstly proposed to make the distinction between them. Then the idea is
extended to interval neutrosophic sets. A multi-criteria decision making method
based on the developed score-accuracy functions is established in which
criterion values for alternatives are single valued neutrosophic sets and
interval neutrosophic sets. In decision making process, the neutrosophic
weighted aggregation operators (arithmetic and geometric average operators) are
adopted to aggregate the neutrosophic information related to each alternative.
Thus, we can rank all alternatives and make the selection of the best of one(s)
according to the score-accuracy functions. Finally, some illustrative examples
are presented to verify the developed approach and to demonstrate its
practicality and effectiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5204</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5204</id><created>2014-12-16</created><authors><author><keyname>Gilboa</keyname><forenames>Shoni</forenames></author><author><keyname>Gueron</keyname><forenames>Shay</forenames></author><author><keyname>Morris</keyname><forenames>Ben</forenames></author></authors><title>How many queries are needed to distinguish a truncated random
  permutation from a random function?</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An oracle chooses a function $f$ from the set of $n$ bits strings to itself,
which is either a randomly chosen permutation or a randomly chosen function.
When queried by an $n$-bit string $w$, the oracle computes $f(w)$, truncates
the $m$ last bits, and returns only the first $n-m$ bits of $f(w)$. How many
queries does a querying adversary need to submit in order to distinguish the
truncated permutation from the (truncated) function?
  In 1998, Hall et al. showed an algorithm for determining (with high
probability) whether or not $f$ is a permutation, using $O(2^{\frac{m+n}{2}})$
queries. They also showed that if $m &lt; n/7$, a smaller number of queries will
not suffice. For $m &gt; n/7$, their method gives a weaker bound. In this note, we
first show how a modification of the approximation method used by Hall et al.
can solve the problem completely. It extends the result to practically any $m$,
showing that $\Omega(2^{\frac{m+n}{2}})$ queries are needed to get a
non-negligible distinguishing advantage. However, more surprisingly, a better
bound for the distinguishing advantage can be obtained from a result of Stam
published, in a different context, already in 1978. We also show that, at least
in some cases, Stam's bound is tight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5207</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5207</id><created>2014-12-16</created><authors><author><keyname>Acharya</keyname><forenames>Anish</forenames></author></authors><title>Are We Ready for Driver-less Vehicles? Security vs. Privacy- A Social
  Perspective</title><categories>cs.CY cs.HC</categories><comments>17 Pages, 5 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  At this moment Autonomous cars are probably the biggest and most talked about
technology in the Robotics Research Community. In spite of great technological
advances over past few years a full edged autonomous car is still far from
reality. This article talks about the existing system and discusses the
possibility of a Computer Vision enabled driving being superior than the LiDar
based system. A detailed overview of privacy violations that might arise from
autonomous driving has been discussed in detail both from a technical as well
as legal perspective. It has been proved through evidence and arguments that
efficient and accurate estimation and efficient solution of the constraint
satisfaction problem addressed in the case of autonomous cars are negatively
correlated with the preserving the privacy of the user. It is a very difficult
trade-off since both are very important aspects and has to be taken into
account. The fact that one cannot compromise with the safety issues of the car
makes it inevitable to run into serious privacy concerns that might have
adverse social and political effects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5212</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5212</id><created>2014-12-16</created><authors><author><keyname>&#x141;opuszy&#x144;ski</keyname><forenames>Micha&#x142;</forenames></author></authors><title>Application of Topic Models to Judgments from Public Procurement Domain</title><categories>cs.CL</categories><comments>&quot;Legal Knowledge and Information Systems, JURIX 2014: The
  Twenty-Seventh Annual Conference&quot;, series Frontiers in Artificial
  Intelligence and Applications, Volume 271, edited by Rinke Hoekstra,
  IOSPress, 2014</comments><doi>10.3233/978-1-61499-468-8-131</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, automatic analysis of themes contained in a large corpora of
judgments from public procurement domain is performed. The employed technique
is unsupervised latent Dirichlet allocation (LDA). In addition, it is proposed,
to use LDA in conjunction with recently developed method of unsupervised
keyword extraction. Such an approach improves the interpretability of the
automatically obtained topics and allows for better computational performance.
The described analysis illustrates a potential of the method in detecting
recurring themes and discovering temporal trends in lodged contract appeals.
These results may be in future applied to improve information retrieval from
repositories of legal texts or as auxiliary material for legal analyses carried
out by human experts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5215</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5215</id><created>2014-12-16</created><authors><author><keyname>Ezra</keyname><forenames>Esther</forenames></author></authors><title>Shallow Packings in Geometry</title><categories>cs.CG cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We refine the bound on the packing number, originally shown by Haussler, for
shallow geometric set systems. Specifically, let $\V$ be a finite set system
defined over an $n$-point set $X$; we view $\V$ as a set of indicator vectors
over the $n$-dimensional unit cube. A $\delta$-separated set of $\V$ is a
subcollection $\W$, s.t. the Hamming distance between each pair $\uu, \vv \in
\W$ is greater than $\delta$, where $\delta &gt; 0$ is an integer parameter. The
$\delta$-packing number is then defined as the cardinality of the largest
$\delta$-separated subcollection of $\V$. Haussler showed an asymptotically
tight bound of $\Theta((n/\delta)^d)$ on the $\delta$-packing number if $\V$
has VC-dimension (or \emph{primal shatter dimension}) $d$. We refine this bound
for the scenario where, for any subset, $X' \subseteq X$ of size $m \le n$ and
for any parameter $1 \le k \le m$, the number of vectors of length at most $k$
in the restriction of $\V$ to $X'$ is only $O(m^{d_1} k^{d-d_1})$, for a fixed
integer $d &gt; 0$ and a real parameter $1 \le d_1 \le d$ (this generalizes the
standard notion of \emph{bounded primal shatter dimension} when $d_1 = d$). In
this case when $\V$ is &quot;$k$-shallow&quot; (all vector lengths are at most $k$), we
show that its $\delta$-packing number is $O(n^{d_1} k^{d-d_1}/\delta^d)$,
matching Haussler's bound for the special cases where $d_1=d$ or $k=n$. As an
immediate consequence we conclude that set systems of halfspaces, balls, and
parallel slabs defined over $n$ points in $d$-space admit better packing
numbers when $k$ is smaller than $n$. Last but not least, we describe
applications to (i) spanning trees of low total crossing number, and (ii)
geometric discrepancy, based on previous work by the author.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5218</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5218</id><created>2014-12-16</created><authors><author><keyname>Grosse</keyname><forenames>Roger B.</forenames></author><author><keyname>Duvenaud</keyname><forenames>David K.</forenames></author></authors><title>Testing MCMC code</title><categories>cs.SE cs.LG stat.ML</categories><comments>Presented at the 2014 NIPS workshop on Software Engineering for
  Machine Learning</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Markov Chain Monte Carlo (MCMC) algorithms are a workhorse of probabilistic
modeling and inference, but are difficult to debug, and are prone to silent
failure if implemented naively. We outline several strategies for testing the
correctness of MCMC algorithms. Specifically, we advocate writing code in a
modular way, where conditional probability calculations are kept separate from
the logic of the sampler. We discuss strategies for both unit testing and
integration testing. As a running example, we show how a Python implementation
of Gibbs sampling for a mixture of Gaussians model can be tested.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5227</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5227</id><created>2014-12-16</created><updated>2015-01-04</updated><authors><author><keyname>Kim</keyname><forenames>Il-Min</forenames></author><author><keyname>Kim</keyname><forenames>Byoung-Hoon</forenames></author><author><keyname>Ahn</keyname><forenames>Joon Kui</forenames></author></authors><title>BER-Based Physical Layer Security with Finite Codelength: Combining
  Strong Converse and Error Amplification</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A bit error rate (BER)-based physical layer security approach is proposed for
finite blocklength. For secure communication in the sense of high BER, the
information-theoretic strong converse is combined with cryptographic error
amplification achieved by substitution permutation networks (SPNs) based on
confusion and diffusion. For discrete memoryless channels (DMCs), an analytical
framework is provided showing the tradeoffs among finite blocklength,
maximum/minimum possible transmission rates, and BER requirements for the
legitimate receiver and the eavesdropper. Also, the security gap is
analytically studied for Gaussian channels and the concept is extended to other
DMCs including binary symmetric channels (BSCs) and binary erasure channels
(BECs). For fading channels, the transmit power is optimized to minimize the
outage probability of the legitimate receiver subject to a BER threshold for
the eavesdropper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5231</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5231</id><created>2014-12-16</created><authors><author><keyname>Cai</keyname><forenames>Y.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author><author><keyname>Yang</keyname><forenames>L. L.</forenames></author><author><keyname>Zhao</keyname><forenames>M.</forenames></author></authors><title>Robust MMSE Precoding for Multiuser MIMO Relay Systems using Switched
  Relaying and Side Information</title><categories>cs.IT math.IT</categories><comments>7 figures, 11 pages</comments><journal-ref>IEEE Transactions on Vehicular Technology, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study proposes a novel precoding scheme for multiuser multiple-input
multiple-output (MIMO) relay systems in the presence of imperfect channel state
information (CSI). The base station (BS) and the MIMO relay station (RS) are
both equipped with the same codebook of unitary matrices. According to each
element of the codebook, we create a latent precoding matrix pair, namely a BS
precoding matrix and an RS precoding matrix. The RS precoding matrix is formed
by multiplying the appropriate unitary matrix from the codebook by a power
scaling factor. Based on the given CSI and a block of transmit symbols, the
optimum precoding matrix pair, within the class of all possible latent
precoding matrix pairs derived from the various unitary matrices, is selected
by a suitable selection mechanism for transmission, which is designed to
minimize the squared Euclidean distance between the pre-estimated received
vector and the true transmit symbol vector. We develop a minimum mean square
error (MMSE) design algorithm for the construction of the latent precoding
matrix pairs. In the proposed scheme, rather than sending the complete
processing matrix, only the index of the unitary matrix and its power scaling
factor are sent by the BS to the RS. This significantly reduces the overhead.
Simulation results show that compared to other recently reported precoding
algorithms the proposed precoding scheme is capable of providing improved
robustness against the effects of CSI estimation errors and multiuser
interference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5236</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5236</id><created>2014-12-16</created><authors><author><keyname>Dai</keyname><forenames>Andrew M.</forenames></author><author><keyname>Storkey</keyname><forenames>Amos J.</forenames></author></authors><title>The supervised hierarchical Dirichlet process</title><categories>stat.ML cs.LG</categories><comments>14 pages</comments><doi>10.1109/TPAMI.2014.2315802</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose the supervised hierarchical Dirichlet process (sHDP), a
nonparametric generative model for the joint distribution of a group of
observations and a response variable directly associated with that whole group.
We compare the sHDP with another leading method for regression on grouped data,
the supervised latent Dirichlet allocation (sLDA) model. We evaluate our method
on two real-world classification problems and two real-world regression
problems. Bayesian nonparametric regression models based on the Dirichlet
process, such as the Dirichlet process-generalised linear models (DP-GLM) have
previously been explored; these models allow flexibility in modelling nonlinear
relationships. However, until now, Hierarchical Dirichlet Process (HDP)
mixtures have not seen significant use in supervised problems with grouped data
since a straightforward application of the HDP on the grouped data results in
learnt clusters that are not predictive of the responses. The sHDP solves this
problem by allowing for clusters to be learnt jointly from the group structure
and from the label assigned to each group.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5238</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5238</id><created>2014-12-16</created><authors><author><keyname>Marazopoulou</keyname><forenames>Katerina</forenames></author><author><keyname>Arbour</keyname><forenames>David</forenames></author><author><keyname>Jensen</keyname><forenames>David</forenames></author></authors><title>Refining the Semantics of Social Influence</title><categories>cs.SI physics.soc-ph</categories><comments>Networks: From Graphs to Rich Data - NIPS Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the proliferation of network data, researchers are increasingly focusing
on questions investigating phenomena occurring on networks. This often includes
analysis of peer-effects, i.e., how the connections of an individual affect
that individual's behavior. This type of influence is not limited to direct
connections of an individual (such as friends), but also to individuals that
are connected through longer paths (for example, friends of friends, or friends
of friends of friends). In this work, we identify an ambiguity in the
definition of what constitutes the extended neighborhood of an individual. This
ambiguity gives rise to different semantics and supports different types of
underlying phenomena. We present experimental results, both on synthetic and
real networks, that quantify differences among the sets of extended neighbors
under different semantics. Finally, we provide experimental evidence that
demonstrates how the use of different semantics affects model selection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5240</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5240</id><created>2014-12-16</created><authors><author><keyname>Zhang</keyname><forenames>Shuai</forenames></author><author><keyname>Xin</keyname><forenames>Jack</forenames></author></authors><title>Minimization of Transformed $L_1$ Penalty: Closed Form Representation
  and Iterative Thresholding Algorithms</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The transformed $l_1$ penalty (TL1) functions are a one parameter family of
bilinear transformations composed with the absolute value function. When acting
on vectors, the TL1 penalty interpolates $l_0$ and $l_1$ similar to $l_p$ norm
($p \in (0,1)$). In our companion paper, we showed that TL1 is a robust
sparsity promoting penalty in compressed sensing (CS) problems for a broad
range of incoherent and coherent sensing matrices. Here we develop an explicit
fixed point representation for the TL1 regularized minimization problem. The
TL1 thresholding functions are in closed form for all parameter values. In
contrast, the $l_p$ thresholding functions ($p \in [0,1]$) are in closed form
only for $p=0,1,1/2,2/3$, known as hard, soft, half, and 2/3 thresholding
respectively. The TL1 threshold values differ in subcritical (supercritical)
parameter regime where the TL1 threshold functions are continuous
(discontinuous) similar to soft-thresholding (half-thresholding) functions. We
propose TL1 iterative thresholding algorithms and compare them with hard and
half thresholding algorithms in CS test problems. For both incoherent and
coherent sensing matrices, a proposed TL1 iterative thresholding algorithm with
adaptive subcritical and supercritical thresholds consistently performs the
best in sparse signal recovery with and without measurement noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5244</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5244</id><created>2014-12-16</created><authors><author><keyname>Li</keyname><forenames>Yujia</forenames></author><author><keyname>Swersky</keyname><forenames>Kevin</forenames></author><author><keyname>Zemel</keyname><forenames>Richard</forenames></author></authors><title>Learning unbiased features</title><categories>cs.LG cs.AI cs.NE stat.ML</categories><comments>Published in NIPS 2014 Workshop on Transfer and Multitask Learning,
  see http://nips.cc/Conferences/2014/Program/event.php?ID=4282</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A key element in transfer learning is representation learning; if
representations can be developed that expose the relevant factors underlying
the data, then new tasks and domains can be learned readily based on mappings
of these salient factors. We propose that an important aim for these
representations are to be unbiased. Different forms of representation learning
can be derived from alternative definitions of unwanted bias, e.g., bias to
particular tasks, domains, or irrelevant underlying data dimensions. One very
useful approach to estimating the amount of bias in a representation comes from
maximum mean discrepancy (MMD) [5], a measure of distance between probability
distributions. We are not the first to suggest that MMD can be a useful
criterion in developing representations that apply across multiple domains or
tasks [1]. However, in this paper we describe a number of novel applications of
this criterion that we have devised, all based on the idea of developing
unbiased representations. These formulations include: a standard domain
adaptation framework; a method of learning invariant representations; an
approach based on noise-insensitive autoencoders; and a novel form of
generative model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5249</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5249</id><created>2014-12-16</created><authors><author><keyname>Greenhill</keyname><forenames>Catherine</forenames></author></authors><title>The switch Markov chain for sampling irregular graphs</title><categories>cs.DS math.CO</categories><comments>9 pages, 5 figures, to appear in proceedings of SODA 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of efficiently sampling from a set of(undirected) graphs with a
given degree sequence has many applications. One approach to this problem uses
a simple Markov chain, which we call the switch chain, to perform the sampling.
The switch chain is known to be rapidly mixing for regular degree sequences. We
prove that the switch chain is rapidly mixing for any degree sequence with
minimum degree at least 1 and with maximum degree $d_{\max}$ which satisfies
$3\leq d_{\max}\leq \frac{1}{4}\, \sqrt{M}$, where $M$ is the sum of the
degrees. The mixing time bound obtained is only an order of $n$ larger than
that established in the regular case, where $n$ is the number of vertices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5263</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5263</id><created>2014-12-17</created><authors><author><keyname>Jindal</keyname><forenames>Alekh</forenames></author><author><keyname>Madden</keyname><forenames>Samuel</forenames></author><author><keyname>Castellanos</keyname><forenames>Malu</forenames></author><author><keyname>Hsu</keyname><forenames>Meichun</forenames></author></authors><title>Graph Analytics using the Vertica Relational Database</title><categories>cs.DB</categories><acm-class>H.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph analytics is becoming increasingly popular, with a deluge of new
systems for graph analytics having been proposed in the past few years. These
systems often start from the assumption that a new storage or query processing
system is needed, in spite of graph data being often collected and stored in a
relational database in the first place. In this paper, we study Vertica
relational database as a platform for graph analytics. We show that
vertex-centric graph analysis can be translated to SQL queries, typically
involving table scans and joins, and that modern column-oriented databases are
very well suited to running such queries. Specifically, we present an
experimental evaluation of the Vertica relational database system on a variety
of graph analytics, including iterative analysis, a combination of graph and
relational analyses, and more complex 1- hop neighborhood graph analytics,
showing that it is competitive to two popular vertex-centric graph analytics
systems, namely Giraph and GraphLab.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5267</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5267</id><created>2014-12-17</created><authors><author><keyname>Han</keyname><forenames>Bin</forenames></author><author><keyname>Zhao</keyname><forenames>Zhepeng</forenames></author><author><keyname>Zhuang</keyname><forenames>Xiaosheng</forenames></author></authors><title>Directional Tensor Product Complex Tight Framelets with Low Redundancy</title><categories>cs.IT math.IT</categories><msc-class>42C40, 42C15, 94A08, 68U10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Though high redundancy rate of a tight frame can improve performance in
applications, as the dimension increases, it also makes the computational cost
skyrocket and the storage of frame coefficients increase exponentially. This
seriously restricts the usefulness of such tight frames for problems in
moderately high dimensions such as video processing in dimension three.
Inspired by the directional tensor product complex tight framelets ${TP-CTF}_m$
with $m\ge 3$ in [14,18] and their impressive performance for image processing
in [18,30] in this paper we introduce a directional tensor product complex
tight framelet ${TP-CTF}^!_6$ (called reduced ${TP-CTF}_6$) with low
redundancy. Such ${TP-CTF}_6^!$ is a particular example of tight framelet
filter banks with mixed sampling factors. The ${TP-CTF}^!_6$ in $d$ dimensions
not only offers good directionality but also has the low redundancy rate
$\frac{3^d-1}{2^d-1}$ (e.g., the redundancy rates are $2,
2\mathord{\frac{2}{3}}, 3\mathord{\frac{5}{7}}, 5\mathord{\frac{1}{3}}$ and
$7\mathord{\frac{25}{31}}$ for dimension $d=1,..., 5$, respectively). Moreover,
our numerical experiments on image/video denoising and inpainting show that the
performance using our proposed ${TP-CTF}^!_6$ is often comparable or sometimes
better than several state-of-the-art frame-based methods which have much higher
redundancy rates than that of ${TPCTF}^!_6$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5272</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5272</id><created>2014-12-17</created><authors><author><keyname>Fan</keyname><forenames>Jun</forenames></author><author><keyname>Hu</keyname><forenames>Ting</forenames></author><author><keyname>Wu</keyname><forenames>Qiang</forenames></author><author><keyname>Zhou</keyname><forenames>Ding-Xuan</forenames></author></authors><title>Consistency Analysis of an Empirical Minimum Error Entropy Algorithm</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the consistency of an empirical minimum error entropy
(MEE) algorithm in a regression setting. We introduce two types of consistency.
The error entropy consistency, which requires the error entropy of the learned
function to approximate the minimum error entropy, is shown to be always true
if the bandwidth parameter tends to 0 at an appropriate rate. The regression
consistency, which requires the learned function to approximate the regression
function, however, is a complicated issue. We prove that the error entropy
consistency implies the regression consistency for homoskedastic models where
the noise is independent of the input variable. But for heteroskedastic models,
a counterexample is used to show that the two types of consistency do not
coincide. A surprising result is that the regression consistency is always
true, provided that the bandwidth parameter tends to infinity at an appropriate
rate. Regression consistency of two classes of special models is shown to hold
with fixed bandwidth parameter, which further illustrates the complexity of
regression consistency of MEE. Fourier transform plays crucial roles in our
analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5275</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5275</id><created>2014-12-17</created><authors><author><keyname>Nojavani</keyname><forenames>Ismail</forenames></author><author><keyname>Rezaeezade</keyname><forenames>Azade</forenames></author><author><keyname>Monadjemi</keyname><forenames>Amirhassan</forenames></author></authors><title>Iranian cashes recognition using mobile</title><categories>cs.CV</categories><comments>arXiv #133709</comments><journal-ref>International Journal of Computer Science &amp; Information
  Technology, volume 6, issue 6, pp.61-71, 2014</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In economical societies of today, using cash is an inseparable aspect of
human life. People use cashes for marketing, services, entertainments, bank
operations and so on. This huge amount of contact with cash and the necessity
of knowing the monetary value of it caused one of the most challenging problems
for visually impaired people. In this paper we propose a mobile phone based
approach to identify monetary value of a picture taken from cashes using some
image processing and machine vision techniques. While the developed approach is
very fast, it can recognize the value of cash by average accuracy of about 95%
and can overcome different challenges like rotation, scaling, collision,
illumination changes, perspective, and some others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5278</identifier>
 <datestamp>2015-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5278</id><created>2014-12-17</created><authors><author><keyname>Such</keyname><forenames>Jose M.</forenames></author><author><keyname>Rovatsos</keyname><forenames>Michael</forenames></author></authors><title>Privacy Policy Negotiation in Social Media</title><categories>cs.SI</categories><journal-ref>ACM Transactions on Autonomous and Adaptive Systems, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social Media involve many shared items, such as photos, which may concern
more than one user. The first challenge we address in this paper is to develop
a way for users of such items to take a decision on to whom to share these
items. This is not an easy problem, as users' privacy preferences for the same
item may conflict, so an approach that just merges in some way the users'
privacy preferences may provide unsatisfactory results. We propose a
negotiation mechanism for users to agree on a compromise for the conflicts
found. The second challenge we address in this paper relates to the exponential
complexity of such a negotiation mechanism, which could make it too slow to be
used in practice in a Social Media infrastructure. To address this, we propose
heuristics that reduce the complexity of the negotiation mechanism and show how
substantial benefits can be derived from the use of these heuristics through
extensive experimental evaluation that compares the performance of the
negotiation mechanism with and without these heuristics. Moreover, we show that
one such heuristic makes the negotiation mechanism produce results fast enough
to be used in actual Social Media infrastructures with near-optimal results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5302</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5302</id><created>2014-12-17</created><authors><author><keyname>Bundala</keyname><forenames>Daniel</forenames></author><author><keyname>Codish</keyname><forenames>Michael</forenames></author><author><keyname>Cruz-Filipe</keyname><forenames>Lu&#xed;s</forenames></author><author><keyname>Schneider-Kamp</keyname><forenames>Peter</forenames></author><author><keyname>Z&#xe1;vodn&#xfd;</keyname><forenames>Jakub</forenames></author></authors><title>Optimal-Depth Sorting Networks</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We solve a 40-year-old open problem on the depth optimality of sorting
networks. In 1973, Donald E. Knuth detailed, in Volume 3 of &quot;The Art of
Computer Programming&quot;, sorting networks of the smallest depth known at the time
for n =&lt; 16 inputs, quoting optimality for n =&lt; 8. In 1989, Parberry proved the
optimality of the networks with 9 =&lt; n =&lt; 10 inputs. In this article, we
present a general technique for obtaining such optimality results, and use it
to prove the optimality of the remaining open cases of 11 =&lt; n =&lt; 16 inputs. We
show how to exploit symmetry to construct a small set of two-layer networks on
n inputs such that if there is a sorting network on n inputs of a given depth,
then there is one whose first layers are in this set. For each network in the
resulting set, we construct a propositional formula whose satisfiability is
necessary for the existence of a sorting network of a given depth. Using an
off-the-shelf SAT solver we show that the sorting networks listed by Knuth are
optimal. For n =&lt; 10 inputs, our algorithm is orders of magnitude faster than
the prior ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5307</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5307</id><created>2014-12-17</created><updated>2015-08-31</updated><authors><author><keyname>Ardeshiri</keyname><forenames>Tohid</forenames></author><author><keyname>&#xd6;zkan</keyname><forenames>Emre</forenames></author><author><keyname>Orguner</keyname><forenames>Umut</forenames></author><author><keyname>Gustafsson</keyname><forenames>Fredrik</forenames></author></authors><title>Approximate Bayesian Smoothing with Unknown Process and Measurement
  Noise Covariances</title><categories>cs.SY</categories><comments>Derivations for the smoother can found here:
  http://urn.kb.se/resolve?urn=urn:nbn:se:liu:diva-120700</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an adaptive smoother for linear state-space models with unknown
process and measurement noise covariances. The proposed method utilizes the
variational Bayes technique to perform approximate inference. The resulting
smoother is computationally efficient, easy to implement, and can be applied to
high dimensional linear systems. The performance of the algorithm is
illustrated on a target tracking example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5310</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5310</id><created>2014-12-17</created><authors><author><keyname>Gadouleau</keyname><forenames>Maximilien</forenames></author><author><keyname>Richard</keyname><forenames>Adrien</forenames></author><author><keyname>Fanchon</keyname><forenames>Eric</forenames></author></authors><title>Reduction and Fixed Points of Boolean Networks and Linear Network Coding
  Solvability</title><categories>cs.IT cs.DM math.DS math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear network coding transmits data through networks by letting the
intermediate nodes combine the messages they receive and forward the
combinations towards their destinations. The solvability problem asks whether
the demands of all the destinations can be simultaneously satisfied by using
linear network coding. The guessing number approach converts this problem to
determining the number of fixed points of coding functions $f:A^n\to A^n$ over
a finite alphabet $A$ (usually referred to as Boolean networks if $A =
\{0,1\}$) with a given interaction graph, that describes which local functions
depend on which variables. In this paper, we generalise the so-called reduction
of coding functions in order to eliminate variables. We then determine the
maximum number of fixed points of a fully reduced coding function, whose
interaction graph has a loop on every vertex. Since the reduction preserves the
number of fixed points, we then apply these ideas and results to obtain four
main results on the linear network coding solvability problem. First, we prove
that non-decreasing coding functions cannot solve any more instances than
routing already does. Second, we show that triangle-free undirected graphs are
linearly solvable if and only if they are solvable by routing. This is the
first classification result for the linear network coding solvability problem.
Third, we exhibit a new class of non-linearly solvable graphs. Fourth, we
determine large classes of strictly linearly solvable graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5316</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5316</id><created>2014-12-17</created><authors><author><keyname>Latkin</keyname><forenames>Evgeny</forenames></author></authors><title>Twofolds in C and C++</title><categories>cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Here I propose C and C++ interfaces and experimental implementation for
twofolds arithmetic. I introduce twofolds in my previous article entitled
&quot;Twofold fast arithmetic&quot; for tracking floating-point inaccuracy. Testing
shows, plain C enables high-performance computing with twofolds. C++ interface
enables coding as easily as ordinary floating-point numbers. My goal is
convincing you to try twofolds; I think assuring accuracy of math computations
is worth its cost. Code and use examples available at my web site, references
inside.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5322</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5322</id><created>2014-12-17</created><authors><author><keyname>Patrascu</keyname><forenames>Vasile</forenames></author></authors><title>An Algebraical Model for Gray Level Images</title><categories>cs.CV</categories><comments>The 7th International Conference, Exhibition on Optimization of
  Electrical and Electronic Equipment, OPTIM 2000, Bra\c{s}ov, Rom\^ania 11-12
  May, 2000</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a new algebraical model for the gray level images.
It can be used for digital image processing. The model adresses to those images
which are generated in improper light conditions (very low or high level). The
vector space structure is able to illustrate some features into the image using
modified level of contrast and luminosity. Also, the defined structure could be
used in image enhancement. The general approach is presented with experimental
results to demonstrate image enhancement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5323</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5323</id><created>2014-12-17</created><authors><author><keyname>AlKindy</keyname><forenames>Bassam</forenames></author><author><keyname>Guyeux</keyname><forenames>Christophe</forenames></author><author><keyname>Couchot</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author><author><keyname>Salomon</keyname><forenames>Michel</forenames></author><author><keyname>Bahi</keyname><forenames>Jacques M.</forenames></author></authors><title>Gene Similarity-based Approaches for Determining Core-Genes of
  Chloroplasts</title><categories>cs.NE q-bio.GN</categories><comments>4 pages, IEEE International Conference on Bioinformatics and
  Biomedicine (BIBM 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In computational biology and bioinformatics, the manner to understand
evolution processes within various related organisms paid a lot of attention
these last decades. However, accurate methodologies are still needed to
discover genes content evolution. In a previous work, two novel approaches
based on sequence similarities and genes features have been proposed. More
precisely, we proposed to use genes names, sequence similarities, or both,
insured either from NCBI or from DOGMA annotation tools. Dogma has the
advantage to be an up-to-date accurate automatic tool specifically designed for
chloroplasts, whereas NCBI possesses high quality human curated genes (together
with wrongly annotated ones). The key idea of the former proposal was to take
the best from these two tools. However, the first proposal was limited by name
variations and spelling errors on the NCBI side, leading to core trees of low
quality. In this paper, these flaws are fixed by improving the comparison of
NCBI and DOGMA results, and by relaxing constraints on gene names while adding
a stage of post-validation on gene sequences. The two stages of similarity
measures, on names and sequences, are thus proposed for sequence clustering.
This improves results that can be obtained using either NCBI or DOGMA alone.
Results obtained with this quality control test are further investigated and
compared with previously released ones, on both computational and biological
aspects, considering a set of 99 chloroplastic genomes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5325</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5325</id><created>2014-12-17</created><authors><author><keyname>Patrascu</keyname><forenames>Vasile</forenames></author><author><keyname>Buzuloiu</keyname><forenames>Vasile</forenames></author></authors><title>Color Image Enhancement In the Framework of Logarithmic Models</title><categories>cs.CV</categories><comments>The 8th IEEE International Conference on Telecommunications, Vol. 1,
  pp. 199-204, IEEE ICT2001, June 4 - 7, 2001, Bucharest,Romania</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a mathematical model for color image processing. It
is a logarithmical one. We consider the cube (-1,1)x(-1,1)x(-1,1) as the set of
values for the color space. We define two operations: addition &lt;+&gt; and real
scalar multiplication &lt;x&gt;. With these operations the space of colors becomes a
real vector space. Then, defining the scalar product (.|.) and the norm || .
||, we obtain a (logarithmic) Euclidean space. We show how we can use this
model for color image enhancement and we present some experimental results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5328</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5328</id><created>2014-12-17</created><authors><author><keyname>Patrascu</keyname><forenames>Vasile</forenames></author><author><keyname>Buzuloiu</keyname><forenames>Vasile</forenames></author></authors><title>A Mathematical Model for Logarithmic Image Processing</title><categories>cs.CV</categories><comments>The 5th World Multi-Conference on Systemics, Cybernetics and
  Informatics, Vol 13, pp. 117-122, SCI2001, July 22-25, 2001, Orlando, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new mathematical model for image processing. It
is a logarithmical one. We consider the bounded interval (-1, 1) as the set of
gray levels. Firstly, we define two operations: addition &lt;+&gt; and real scalar
multiplication &lt;x&gt;. With these operations, the set of gray levels becomes a
real vector space. Then, defining the scalar product (.|.) and the norm || .
||, we obtain an Euclidean space of the gray levels. Secondly, we extend these
operations and functions for color images. We finally show the effect of
various simple operations on an image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5334</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5334</id><created>2014-12-17</created><authors><author><keyname>Patrascu</keyname><forenames>Vasile</forenames></author><author><keyname>Buzuloiu</keyname><forenames>Vasile</forenames></author></authors><title>The Affine Transforms for Image Enhancement in the Context of
  Logarithmic Models</title><categories>cs.CV</categories><comments>International Conference on Computer Vision and Graphics, ICCVG2002,
  25-29 September, 2002, Zakopane, Poland</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The logarithmic model offers new tools for image processing. An efficient
method for image enhancement is to use an affine transformation with the
logarithmic operations: addition and scalar multiplication. We define some
criteria for automatically determining the parameters of the processing and
this is done via mean and variance computed by logarithmic operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5335</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5335</id><created>2014-12-17</created><updated>2015-05-27</updated><authors><author><keyname>Mesnil</keyname><forenames>Gr&#xe9;goire</forenames></author><author><keyname>Mikolov</keyname><forenames>Tomas</forenames></author><author><keyname>Ranzato</keyname><forenames>Marc'Aurelio</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Ensemble of Generative and Discriminative Techniques for Sentiment
  Analysis of Movie Reviews</title><categories>cs.CL cs.IR cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sentiment analysis is a common task in natural language processing that aims
to detect polarity of a text document (typically a consumer review). In the
simplest settings, we discriminate only between positive and negative
sentiment, turning the task into a standard binary classification problem. We
compare several ma- chine learning approaches to this problem, and combine them
to achieve the best possible results. We show how to use for this task the
standard generative lan- guage models, which are slightly complementary to the
state of the art techniques. We achieve strong results on a well-known dataset
of IMDB movie reviews. Our results are easily reproducible, as we publish also
the code needed to repeat the experiments. This should simplify further advance
of the state of the art, as other researchers can combine their techniques with
ours with little effort.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5340</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5340</id><created>2014-12-17</created><authors><author><keyname>Smiljkovikj</keyname><forenames>Katerina</forenames></author><author><keyname>Ichkov</keyname><forenames>Aleksandar</forenames></author><author><keyname>Angjelicinoski</keyname><forenames>Marko</forenames></author><author><keyname>Atanasovski</keyname><forenames>Vladimir</forenames></author><author><keyname>Gavrilovska</keyname><forenames>Liljana</forenames></author></authors><title>Analysis of Two-Tier LTE Network with Randomized Resource Allocation and
  Proactive Offloading</title><categories>cs.IT cs.NI math.IT</categories><comments>6 pages; 7 figures; submitted to The 8th International WDN Workshop
  on Cooperative and Heterogeneous Cellular Networks (WDN-CN) at IEEE WCNC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The heterogeneity in cellular networks that comprise multiple base stations
types imposes new challenges in network planning and deployment. The Radio
Resource Management (RRM) techniques, such as dynamic sharing of the available
resources and advanced user association strategies determine the overall
network capacity and the network/spectrum efficiency. This paper evaluates the
downlink performance of a two-tier heterogeneous LTE network (consisting of
macro and femto tiers) in terms of rate distribution, i.e. the percentage of
users that achieve certain rate in the system. The paper specifically
addresses: (1) the femto tier RRM by randomization of the allocated resources;
(2) the user association process by introducing novel proactive offloading
scheme and (3) femto tier access control. System level simulation results show
that an optimal RRM strategy can be designed for different scenarios (e.g.
congested and uncongested networks). The proposed proactive offloading scheme
in the association phase improves the performance of congested networks by
efficiently utilizing the available femto tier resources. Finally, the
introduced hybrid access in femto tier is shown to perform nearly identical as
the open access.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5344</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5344</id><created>2014-12-17</created><authors><author><keyname>Meena</keyname><forenames>V.</forenames></author><author><keyname>Abhilash</keyname><forenames>G.</forenames></author></authors><title>Noise Resilient Recovery Algorithm for Compressed Sensing</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we discuss a novel greedy algorithm for the recovery of
compressive sampled signals under noisy conditions. Most of the greedy recovery
algorithms proposed in the literature require sparsity of the signal to be
known or they estimate sparsity, for a known representation basis, from the
number of measurements. These algorithms recover signals when noise level is
significantly low. We propose Entropy minimization based Matching Pursuit (EMP)
which has the capability to reject noise even when noise level is comparable to
that of signal level. The proposed algorithm can cater to compressible signals
and signals for which sparsity is not known in advance. Simulation study of the
proposed scheme shows improved robustness to white Gaussian noise in comparison
with the conventional greedy recovery algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5356</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5356</id><created>2014-12-17</created><authors><author><keyname>Xiang</keyname><forenames>Lin</forenames></author><author><keyname>Ge</keyname><forenames>Xiaohu</forenames></author><author><keyname>Wang</keyname><forenames>Cheng-Xiang</forenames></author><author><keyname>Li</keyname><forenames>Frank Y.</forenames></author><author><keyname>Reichert</keyname><forenames>Frank</forenames></author></authors><title>Energy Efficiency Evaluation of Cellular Networks Based on Spatial
  Distributions of Traffic Load and Power Consumption</title><categories>cs.NI</categories><journal-ref>IEEE Transactions on Wireless Communications, vol. 12, no. 3,
  pp.961-973, March. 2013</journal-ref><doi>10.1109/TWC.2013.011713.112157</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy efficiency has gained its significance when service providers'
operational costs burden with the rapidly growing data traffic demand in
cellular networks. In this paper, we propose an energy efficiency model for
Poisson-Voronoi tessellation (PVT) cellular networks considering spatial
distributions of traffic load and power consumption. The spatial distributions
of traffic load and power consumption are derived for a typical PVT cell, and
can be directly extended to the whole PVT cellular network based on the Palm
theory. Furthermore, the energy efficiency of PVT cellular networks is
evaluated by taking into account traffic load characteristics, wireless channel
effects and interference. Both numerical and Monte Carlo simulations are
conducted to evaluate the performance of the energy efficiency model in PVT
cellular networks. These simulation results demonstrate that there exist
maximal limits for energy efficiency in PVT cellular networks for given
wireless channel conditions and user intensity in a cell.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5366</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5366</id><created>2014-12-17</created><authors><author><keyname>Ge</keyname><forenames>Xiaohu</forenames></author><author><keyname>Huang</keyname><forenames>Kun</forenames></author><author><keyname>Wang</keyname><forenames>Cheng-Xiang</forenames></author><author><keyname>Hong</keyname><forenames>Xuemin</forenames></author><author><keyname>Yang</keyname><forenames>Xi</forenames></author></authors><title>Capacity analysis of a multi-cell multi-antenna cooperative cellular
  network with co-channel interference</title><categories>cs.NI cs.IT math.IT</categories><journal-ref>IEEE Transactions on Wireless Communications, vol. 10, no. 10,
  pp.3298-3309, Oct. 2011</journal-ref><doi>10.1109/TWC.2011.11.101551</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Characterization and modeling of co-channel interference is critical for the
design and performance evaluation of realistic multi-cell cellular networks. In
this paper, based on alpha stable processes, an analytical co-channel
interference model is proposed for multi-cell multiple-input multi-output
(MIMO) cellular networks. The impact of different channel parameters on the new
interference model is analyzed numerically. Furthermore, the exact normalized
downlink average capacity is derived for a multi-cell MIMO cellular network
with co-channel interference. Moreover, the closed-form normalized downlink
average capacity is derived for cell-edge users in the multi-cell
multiple-input single-output (MISO) cooperative cellular network with
co-channel interference. From the new co-channel interference model and
capacity, the impact of cooperative antennas and base stations on cell-edge
user performance in the multi-cell multi-antenna cellular network is
investigated by numerical methods. Numerical results show that cooperative
transmission can improve the capacity performance of multi-cell multi-antenna
cooperative cellular networks, especially in a scenario with a high density of
interfering base stations. The capacity performance gain is degraded with the
increased number of cooperative antennas or base stations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5372</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5372</id><created>2014-12-17</created><authors><author><keyname>Ge</keyname><forenames>Xiaohu</forenames></author><author><keyname>Han</keyname><forenames>Tao</forenames></author><author><keyname>Zhang</keyname><forenames>Yan</forenames></author><author><keyname>Mao</keyname><forenames>Guoqiang</forenames></author><author><keyname>Wang</keyname><forenames>Cheng-Xiang.</forenames></author><author><keyname>Zhang</keyname><forenames>Jing</forenames></author><author><keyname>Yang</keyname><forenames>Bin</forenames></author><author><keyname>Pan</keyname><forenames>Sheng</forenames></author></authors><title>Spectrum and Energy Efficiency Evaluation of Two-Tier Femtocell networks
  With Partially Open Channels</title><categories>cs.NI</categories><journal-ref>IEEE Transactions on Vehicular Technology, vol. 63, no. 3, pp.
  1306 - 1319, March 2014</journal-ref><doi>10.1109/TVT.2013.2292084</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two-tier femtocell networks is an efficient communication architecture that
significantly improves throughput in indoor environments with low power
consumption. Traditionally, a femtocell network is usually configured to be
either completely open or completely closed in that its channels are either
made available to all users or used by its own users only. This may limit
network flexibility and performance. It is desirable for owners of femtocell
base stations if a femtocell can partially open its channels for external users
access. In such scenarios, spectrum and energy efficiency becomes a critical
issue in the design of femtocell network protocols and structure. In this
paper, we conduct performance analysis for two-tier femtocell networks with
partially open channels. In particular, we build a Markov chain to model the
channel access in the femtocell network and then derive the performance metrics
in terms of the blocking probabilities. Based on stationary state probabilities
derived by Markov chain models, spectrum and energy efficiency are modeled and
analyzed under different scenarios characterized by critical parameters,
including number of femtocells in a macrocell, average number of users, and
number of open channels in a femtocell. Numerical and Monte-Carlo (MC)
simulation results indicate that the number of open channels in a femtocell has
an adverse impact on the spectrum and energy efficiency of two-tier femtocell
networks. Results in this paper provide guidelines for trading off spectrum and
energy efficiency of two-tier femtocell networks by configuring different
numbers of open channels in a femtocell.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5374</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5374</id><created>2014-12-17</created><updated>2015-08-05</updated><authors><author><keyname>Li</keyname><forenames>Cheuk Ting</forenames></author><author><keyname>Gamal</keyname><forenames>Abbas El</forenames></author></authors><title>Maximal Correlation Secrecy</title><categories>cs.IT cs.CR math.IT</categories><comments>16 pages, 2 figure, presented in part at IEEE International Symposium
  on Information Theory 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows that the Hirschfeld-Gebelein-R\'enyi maximal correlation
between the message and the ciphertext provides good secrecy guarantees for
cryptosystems that use short keys. We first establish a bound on the
eavesdropper's advantage in guessing functions of the message in terms of
maximal correlation and the R\'enyi entropy of the message. This result implies
that maximal correlation is stronger than the notion of entropic security
introduced by Russell and Wang. We then show that a small maximal correlation
$\rho$ can be achieved via a randomly generated cipher with key length
$\approx2\log(1/\rho)$, independent of the message length, and by a stream
cipher with key length $2\log(1/\rho)+\log n+2$ for a message of length $n$. We
establish a converse showing that these ciphers are close to optimal. This is
in contrast to entropic security for which there is a gap between the lower and
upper bounds. Finally, we show that a small maximal correlation implies secrecy
with respect to several mutual information based criteria but is not
necessarily implied by them. Hence, maximal correlation is a stronger and more
practically relevant measure of secrecy than mutual information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5381</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5381</id><created>2014-12-17</created><authors><author><keyname>Brunsch</keyname><forenames>Tobias</forenames></author><author><keyname>Gro&#xdf;wendt</keyname><forenames>Anna</forenames></author><author><keyname>R&#xf6;glin</keyname><forenames>Heiko</forenames></author></authors><title>Solving Totally Unimodular LPs with the Shadow Vertex Algorithm</title><categories>cs.DS</categories><comments>to be presented at STACS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the shadow vertex simplex algorithm can be used to solve linear
programs in strongly polynomial time with respect to the number $n$ of
variables, the number $m$ of constraints, and $1/\delta$, where $\delta$ is a
parameter that measures the flatness of the vertices of the polyhedron. This
extends our recent result that the shadow vertex algorithm finds paths of
polynomial length (w.r.t. $n$, $m$, and $1/\delta$) between two given vertices
of a polyhedron.
  Our result also complements a recent result due to Eisenbrand and Vempala who
have shown that a certain version of the random edge pivot rule solves linear
programs with a running time that is strongly polynomial in the number of
variables $n$ and $1/\delta$, but independent of the number $m$ of constraints.
Even though the running time of our algorithm depends on $m$, it is
significantly faster for the important special case of totally unimodular
linear programs, for which $1/\delta\le n$ and which have only $O(n^2)$
constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5384</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5384</id><created>2014-12-17</created><authors><author><keyname>Perina</keyname><forenames>Andre B.</forenames></author><author><keyname>Gois</keyname><forenames>Marcilyanne M.</forenames></author><author><keyname>Matias</keyname><forenames>Paulo</forenames></author><author><keyname>Cardoso</keyname><forenames>Joao M. P.</forenames></author><author><keyname>Delbem</keyname><forenames>Alexandre C. B.</forenames></author><author><keyname>Bonato</keyname><forenames>Vanderlei</forenames></author></authors><title>Representation of Evolutionary Algorithms in FPGA Cluster for Project of
  Large-Scale Networks</title><categories>cs.DC cs.NE</categories><comments>Preprint of a short paper published in the proceedings of ERAD-SP
  2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many problems are related to network projects, such as electric distribution,
telecommunication and others. Most of them can be represented by graphs, which
manipulate thousands or millions of nodes, becoming almost an impossible task
to obtain real-time solutions. Many efficient solutions use Evolutionary
Algorithms (EA), where researches show that performance of EAs can be
substantially raised by using an appropriate representation, such as the
Node-Depth Encoding (NDE). The objective of this work was to partition an
implementation on single-FPGA (Field-Programmable Gate Array) based on NDE from
512 nodes to a multi-FPGAs approach, expanding the system to 4096 nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5400</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5400</id><created>2014-12-17</created><updated>2014-12-28</updated><authors><author><keyname>Khedker</keyname><forenames>Uday P.</forenames></author></authors><title>Buffer Overflow Analysis for C</title><categories>cs.PL</categories><acm-class>F.3.1; F.3.2; D.2.4; D.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Buffer overflow detection and mitigation for C programs has been an important
concern for a long time. This paper defines a string buffer overflow analysis
for C programs. The key ideas of our formulation are (a) separating buffers
from the pointers that point to them, (b) modelling buffers in terms of sizes
and sets of positions of null characters, and (c) defining stateless functions
to compute the sets of null positions and mappings between buffers and
pointers.
  This exercise has been carried out to test the feasibility of describing such
an analysis in terms of lattice valued functions and relations to facilitate
automatic construction of an analyser without the user having to write
C/C++/Java code. This is facilitated by devising stateless formulations because
stateful formulations combine features through side effects in states raising a
natural requirement of C/C++/Java code to be written to describe them. Given
the above motivation, the focus of this paper is not to build good static
approximations for buffer overflow analysis but to show how given static
approximations could be formalized in terms of stateless formulations so that
they become amenable to automatic construction of analysers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5401</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5401</id><created>2014-12-17</created><authors><author><keyname>Osipov</keyname><forenames>Ilya V.</forenames></author><author><keyname>Volinsky</keyname><forenames>Alex A.</forenames></author><author><keyname>Grishin</keyname><forenames>Vadim V.</forenames></author></authors><title>Gamification, virality and retention in educational online platform.
  Measurable indicators and market entry strategy</title><categories>cs.CY cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper describes gamification, virality and retention in the freemium
educational online platform with 40,000 users as an example. Relationships
between virality and retention parameters as measurable metrics are calculated
and discussed using real examples. Virality and monetization can be both
competing and complementary mechanisms for the system growth. The K-growth
factor, which combines both virality and retention, is proposed as the metrics
of the overall freemium system performance in terms of the user base growth.
This approach can be tested using a small number of users to assess the system
potential performance. If the K-growth factor is less than one, the product
needs further development. If the K-growth factor it is greater than one, the
system retains existing and attracts new users, thus a large scale market
launch can be successful.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5404</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5404</id><created>2014-12-17</created><authors><author><keyname>Zuo</keyname><forenames>Yuan</forenames></author><author><keyname>Zhao</keyname><forenames>Jichang</forenames></author><author><keyname>Xu</keyname><forenames>Ke</forenames></author></authors><title>Word Network Topic Model: A Simple but General Solution for Short and
  Imbalanced Texts</title><categories>cs.CL cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The short text has been the prevalent format for information of Internet in
recent decades, especially with the development of online social media, whose
millions of users generate a vast number of short messages everyday. Although
sophisticated signals delivered by the short text make it a promising source
for topic modeling, its extreme sparsity and imbalance brings unprecedented
challenges to conventional topic models like LDA and its variants. Aiming at
presenting a simple but general solution for topic modeling in short texts, we
present a word co-occurrence network based model named WNTM to tackle the
sparsity and imbalance simultaneously. Different from previous approaches, WNTM
models the distribution over topics for each word instead of learning topics
for each document, which successfully enhance the semantic density of data
space without importing too much time or space complexity. Meanwhile, the rich
contextual information preserved in the word-word space also guarantees its
sensitivity in identifying rare topics with convincing quality. Furthermore,
employing the same Gibbs sampling with LDA makes WNTM easily to be extended to
various application scenarios. Extensive validations on both short and normal
texts testify the outperformance of WNTM as compared to baseline methods. And
finally we also demonstrate its potential in precisely discovering newly
emerging topics or unexpected events in Weibo at pretty early stages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5429</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5429</id><created>2014-12-17</created><authors><author><keyname>Flores</keyname><forenames>Ram&#xf3;n</forenames></author><author><keyname>Molina</keyname><forenames>Elisenda</forenames></author><author><keyname>Tejada</keyname><forenames>Juan</forenames></author></authors><title>The Shapley group value</title><categories>math.OC cs.GT</categories><comments>29 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Following the original interpretation of the Shapley value (Shapley, 1953a)
as a priori evaluation of the prospects of a player in a multi-person
interaction situation, we propose a group value, which we call the Shapley
group value, as a priori evaluation of the prospects of a group of players in a
coalitional game when acting as a unit. We study its properties and we give an
axiomatic characterization. Relaying on this valuation we analyze the
profitability of a group. We motivate our proposal by means of some relevant
applications of the Shapley group value, when it is used as an objective
function by a decisionmaker who is trying to identify an optimal group of
agents in a framework in which agents interact and the attained benefit can be
modeled bymeans of a transferable utility game. As an illustrative examplewe
analyze the problem of identifying the set of key agents in a terrorist
network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5448</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5448</id><created>2014-12-17</created><authors><author><keyname>Poussevin</keyname><forenames>Micka&#xeb;l</forenames></author><author><keyname>Guigue</keyname><forenames>Vincent</forenames></author><author><keyname>Gallinari</keyname><forenames>Patrick</forenames></author></authors><title>Extended Recommendation Framework: Generating the Text of a User Review
  as a Personalized Summary</title><categories>cs.IR cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose to augment rating based recommender systems by providing the user
with additional information which might help him in his choice or in the
understanding of the recommendation. We consider here as a new task, the
generation of personalized reviews associated to items. We use an extractive
summary formulation for generating these reviews. We also show that the two
information sources, ratings and items could be used both for estimating
ratings and for generating summaries, leading to improved performance for each
system compared to the use of a single source. Besides these two contributions,
we show how a personalized polarity classifier can integrate the rating and
textual aspects. Overall, the proposed system offers the user three
personalized hints for a recommendation: rating, text and polarity. We evaluate
these three components on two datasets using appropriate measures for each
task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5452</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5452</id><created>2014-12-17</created><updated>2014-12-26</updated><authors><author><keyname>Mezei</keyname><forenames>Jozsef</forenames></author><author><keyname>Sarlin</keyname><forenames>Peter</forenames></author></authors><title>Aggregation operators for the measurement of systemic risk</title><categories>q-fin.GN cs.CE q-fin.EC q-fin.RM</categories><comments>Submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The policy objective of safeguarding financial stability has stimulated a
wave of research on systemic risk analytics, yet it still faces challenges in
measurability. This paper models systemic risk by tapping into expert knowledge
of financial supervisors. We decompose systemic risk into a number of
interconnected segments, for which the level of vulnerability is measured. The
system is modeled in the form of a Fuzzy Cognitive Map (FCM), in which nodes
represent vulnerability in segments and links their interconnectedness. A main
problem tackled in this paper is the aggregation of values in different
interrelated nodes of the network to obtain an estimate systemic risk. To this
end, the Choquet integral is employed for aggregating expert evaluations of
measures, as it allows for the integration of interrelations among factors in
the aggregation process. The approach is illustrated through two applications
in a European setting. First, we provide an estimation of systemic risk with a
of pan-European set-up. Second, we estimate country-level risks, allowing for a
more granular decomposition. This sets a starting point for the use of the
rich, oftentimes tacit, knowledge in policy organizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5459</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5459</id><created>2014-12-17</created><authors><author><keyname>Kiran</keyname><forenames>Mariam</forenames></author><author><keyname>Liu</keyname><forenames>Wei</forenames></author></authors><title>Converting a Systems Dynamic Model to an Agent-based model for studying
  the Bicoid morphogen gradient in Drosophila embryo</title><categories>cs.MA cs.CE q-bio.QM</categories><comments>21 pages, 13 figures, technical report, exploratory study</comments><acm-class>C.4</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The concentration gradient of the Bicoid morphogen, which is established
during the early stages of a Drosophila melanogaster embryonic development,
determines the differential spatial patterns of gene expression and subsequent
cell fate determination. This is mainly achieved by diffusion elicited by the
different concentrations of the Bicoid protein in the embryo. Such chemical
dynamic progress can be simulated by stochastic models, particularly the
Gillespie alogrithm. However, as with various modelling approaches in biology,
each technique involves drawing assumptions and reducing the model complexity
sometimes limiting the model capability. This is mainly due to the complexity
of the software modelling approaches to construct these models. Agent-based
modelling is a technique which is becoming increasingly popular for modelling
the behaviour of individual molecules or cells in computational biology.
  This paper attempts to compare these two popular modelling techniques of
stochastic and agent-based modelling to show how the model can be studied in
detail using the different approaches. This paper presents how to use these
techniques with the advantages and disadvantages of using either of these.
Through various comparisons, such as computation complexity and results
obtained, we show that although the same model is implemented, both approaches
can give varying results. The results of the paper show that the stochastic
model is able to give smoother results compared to the agent-based model which
may need further analysis at a later stage. We discuss the reasons for these
results and how these could be rectified in systems biology research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5466</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5466</id><created>2014-12-17</created><updated>2016-03-01</updated><authors><author><keyname>Cardinali</keyname><forenames>Ilaria</forenames></author><author><keyname>Giuzzi</keyname><forenames>Luca</forenames></author></authors><title>Enumerative Coding for Line Polar Grassmannians with applications to
  codes</title><categories>cs.IT math.CO math.IT</categories><comments>26 pages; extensively revised with enumerators for both orthogonal
  and symplectic line polar Grassmannians</comments><msc-class>14M15, 94B27, 94B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A $k$-polar Grassmannian is the geometry having as pointset the set of all
$k$-dimensional subspaces of a vector space $V$ which are totally isotropic for
a given non-degenerate bilinear form $\mu$ defined on $V.$ Hence it can be
regarded as a subgeometry of the ordinary $k$-Grassmannian. In this paper we
deal with orthogonal line Grassmannians and with symplectic line Grassmannians,
i.e. we assume $k=2$ and $\mu$ a non-degenerate symmetric or alternating form.
We will provide a method to efficiently enumerate the pointsets of both
orthogonal and symplectic line Grassmannians. This has several nice
applications; among them, we shall discuss an efficient encoding/decoding/error
correction strategy for line polar Grassmann codes of both types.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5474</identifier>
 <datestamp>2015-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5474</id><created>2014-12-17</created><updated>2015-11-20</updated><authors><author><keyname>Jin</keyname><forenames>Jonghoon</forenames></author><author><keyname>Dundar</keyname><forenames>Aysegul</forenames></author><author><keyname>Culurciello</keyname><forenames>Eugenio</forenames></author></authors><title>Flattened Convolutional Neural Networks for Feedforward Acceleration</title><categories>cs.NE cs.LG</categories><comments>International Conference on Learning Representations (ICLR) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present flattened convolutional neural networks that are designed for fast
feedforward execution. The redundancy of the parameters, especially weights of
the convolutional filters in convolutional neural networks has been extensively
studied and different heuristics have been proposed to construct a low rank
basis of the filters after training. In this work, we train flattened networks
that consist of consecutive sequence of one-dimensional filters across all
directions in 3D space to obtain comparable performance as conventional
convolutional networks. We tested flattened model on different datasets and
found that the flattened layer can effectively substitute for the 3D filters
without loss of accuracy. The flattened convolution pipelines provide around
two times speed-up during feedforward pass compared to the baseline model due
to the significant reduction of learning parameters. Furthermore, the proposed
method does not require efforts in manual tuning or post processing once the
model is trained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5477</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5477</id><created>2014-12-17</created><authors><author><keyname>Raja</keyname><forenames>S V Kasmir</forenames></author><author><keyname>Rajitha</keyname><forenames>V</forenames></author><author><keyname>Meenakshi</keyname><forenames>Lakshmanan</forenames></author></authors><title>Computational Model to Generate Case-Inflected Forms of Masculine Nouns
  for Word Search in Sanskrit E-Text</title><categories>cs.CL</categories><journal-ref>Journal of Computer Science (ISSN Print: 1549-3636, ISSN Online:
  1552-6607), December 2014, Volume 10, Issue 11, Pages 2269-2283</journal-ref><doi>10.3844/jcssp.2014.2260.2268</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of word search in Sanskrit is inseparable from complexities that
include those caused by euphonic conjunctions and case-inflections. The
case-inflectional forms of a noun normally number 24 owing to the fact that in
Sanskrit there are eight cases and three numbers-singular, dual and plural. The
traditional method of generating these inflectional forms is rather elaborate
owing to the fact that there are differences in the forms generated between
even very similar words and there are subtle nuances involved. Further, it
would be a cumbersome exercise to generate and search for 24 forms of a word
during a word search in a large text, using the currently available
case-inflectional form generators. This study presents a new approach to
generating case-inflectional forms that is simpler to compute. Further, an
optimized model that is sufficient for generating only those word forms that
are required in a word search and is more than 80% efficient compared to the
complete case-inflectional forms generator, is presented in this study for the
first time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5484</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5484</id><created>2014-12-17</created><updated>2015-06-22</updated><authors><author><keyname>Devadas</keyname><forenames>Sheela</forenames></author><author><keyname>Rubinfeld</keyname><forenames>Ronitt</forenames></author></authors><title>A Self-Tester for Linear Functions over the Integers with an Elementary
  Proof of Correctness</title><categories>cs.CC cs.DS</categories><doi>10.1007/s00224-015-9639-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present simple, self-contained proofs of correctness for algorithms for
linearity testing and program checking of linear functions on finite subsets of
integers represented as n-bit numbers. In addition we explore a generalization
of self-testing to homomorphisms on a multidimensional vector space. We show
that our self-testing algorithm for the univariate case can be directly
generalized to vector space domains. The number of queries made by our
algorithms is independent of domain size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5488</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5488</id><created>2014-12-17</created><authors><author><keyname>Saha</keyname><forenames>Ashirbani</forenames></author><author><keyname>Wu</keyname><forenames>Q. M. Jonathan</forenames></author></authors><title>Full-reference image quality assessment by combining global and local
  distortion measures</title><categories>cs.CV</categories><comments>31 pages, 8 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Full-reference image quality assessment (FR-IQA) techniques compare a
reference and a distorted/test image and predict the perceptual quality of the
test image in terms of a scalar value representing an objective score. The
evaluation of FR-IQA techniques is carried out by comparing the objective
scores from the techniques with the subjective scores (obtained from human
observers) provided in the image databases used for the IQA. Hence, we
reasonably assume that the goal of a human observer is to rate the distortion
present in the test image. The goal oriented tasks are processed by the human
visual system (HVS) through top-down processing which actively searches for
local distortions driven by the goal. Therefore local distortion measures in an
image are important for the top-down processing. At the same time, bottom-up
processing also takes place signifying spontaneous visual functions in the HVS.
To account for this, global perceptual features can be used. Therefore, we
hypothesize that the resulting objective score for an image can be derived from
the combination of local and global distortion measures calculated from the
reference and test images. We calculate the local distortion by measuring the
local correlation differences from the gradient and contrast information. For
global distortion, dissimilarity of the saliency maps computed from a bottom-up
model of saliency is used. The motivation behind the proposed approach has been
thoroughly discussed, accompanied by an intuitive analysis. Finally,
experiments are conducted in six benchmark databases suggesting the
effectiveness of the proposed approach that achieves competitive performance
with the state-of-the-art methods providing an improvement in the overall
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5490</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5490</id><created>2014-12-17</created><updated>2014-12-17</updated><authors><author><keyname>Saha</keyname><forenames>Ashirbani</forenames></author><author><keyname>Wu</keyname><forenames>Q. M. Jonathan</forenames></author></authors><title>High Frequency Content based Stimulus for Perceptual Sharpness
  Assessment in Natural Images</title><categories>cs.CV</categories><comments>13 pages, 6 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A blind approach to evaluate the perceptual sharpness present in a natural
image is proposed. Though the literature demonstrates a set of variegated
visual cues to detect or evaluate the absence or presence of sharpness, we
emphasize in the current work that high frequency content and local standard
deviation can form strong features to compute perceived sharpness in any
natural image, and can be considered an able alternative for the existing cues.
Unsharp areas in a natural image happen to exhibit uniform intensity or lack of
sharp changes between regions. Sharp region transitions in an image are caused
by the presence of spatial high frequency content. Therefore, in the proposed
approach, we hypothesize that using the high frequency content as the principal
stimulus, the perceived sharpness can be quantified in an image. When an image
is convolved with a high pass filter, higher values at any pixel location
signify the presence of high frequency content at those locations. Considering
these values as the stimulus, the exponent of the stimulus is weighted by local
standard deviation to impart the contribution of the local contrast within the
formation of the sharpness map. The sharpness map highlights the relatively
sharper regions in the image and is used to calculate the perceived sharpness
score of the image. The advantages of the proposed method lie in its use of
simple visual cues of high frequency content and local contrast to arrive at
the perceptual score, and requiring no training with the images. The promise of
the proposed method is demonstrated by its ability to compute perceived
sharpness for within image and across image sharpness changes and for blind
evaluation of perceptual degradation resulting due to presence of blur.
Experiments conducted on several databases demonstrate improved performance of
the proposed method over that of the state-of-the-art techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5496</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5496</id><created>2014-12-16</created><authors><author><keyname>X&#xfa;</keyname><forenames>Shixin</forenames><affiliation>LURPA</affiliation></author><author><keyname>Anwer</keyname><forenames>Nabil</forenames><affiliation>LURPA</affiliation></author><author><keyname>Lavernhe</keyname><forenames>Sylvain</forenames><affiliation>LURPA</affiliation></author></authors><title>Conversion of G-code programs for milling into STEP-NC</title><categories>cs.CE</categories><comments>Joint Conference on Mechanical, Design Engineering \&amp; Advanced
  Manufacturing, Jun 2014, Toulouse, France</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  STEP-NC (ISO 14649) is becoming a promising standard to replace or supplement
the conventional G-code programs based on ISO 6983 due to its feature based
machine independent characteristics and its centric role to enable efficient
CAD/CAM/CNC interoperability. The re-use of G-code programs is important for
both manufacturing and capitalization of machining knowledge, nevertheless the
conversion is a tedious task when carried out manually and machining knowledge
is almost hidden in the low level G-code. Mapping G-code into STEP-NC should
benefit from more expressiveness of the manufacturing feature-based
characteristics of this new standard. The work presented here proposes an
overall method for G-code to STEP-NC conversion. First, G-code is converted
into canonical machining functions, this can make the method more applicable
and make subsequent processes easier to implement; then these functions are
parsed to generate the neutral format of STEP-NC Part21 toolpath file, this
turns G-code into object instances, and can facilitate company's usage of
legacy programs; and finally, also optionally, machining features are extracted
to generate Part21 CC2 (conformance class) file. The proposed extraction method
employs geometric information of cutting area inferred from toolpaths and
machining strategies, in addition to cutting tools' data and workpiece's
dimension data. This comprehensive use of available data makes the extraction
more accurate and reliable. The conversion method is holistic, and can be
extended to process a wide range of G-code programs (e.g. turning or mill-turn
codes) with as few user interventions as possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5498</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5498</id><created>2014-12-17</created><updated>2016-03-07</updated><authors><author><keyname>van Bevern</keyname><forenames>Ren&#xe9;</forenames></author><author><keyname>Komusiewicz</keyname><forenames>Christian</forenames></author><author><keyname>Niedermeier</keyname><forenames>Rolf</forenames></author><author><keyname>Sorge</keyname><forenames>Manuel</forenames></author><author><keyname>Walsh</keyname><forenames>Toby</forenames></author></authors><title>H-Index Manipulation by Merging Articles: Models, Theory, and
  Experiments</title><categories>cs.DL cs.DM cs.DS cs.SI</categories><comments>New kernelization results and experiments</comments><msc-class>91D30</msc-class><acm-class>G.2.1; G.2.2; F.2.2; H.3.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An author's profile on Google Scholar consists of indexed articles and
associated data, such as the number of citations and the H-index. The author is
allowed to merge articles, which may affect the H-index. We analyze the
(parameterized) computational complexity of maximizing the H-index using
article merges. Herein, to model realistic manipulation scenarios, we define a
compatibility graph whose edges correspond to plausible merges. Moreover, we
consider several different measures for computing the citation count of a
merged article. For the measure used by Google Scholar, we give an algorithm
that maximizes the H-index in linear time if the compatibility graph has
constant-size connected components. In contrast, if we allow to merge arbitrary
articles (that is, for arbitrary compatibility graphs), then already increasing
the H-index by one is NP-hard. Experiments on Google Scholar profiles of AI
researchers show that the H-index can be manipulated substantially only if one
merges articles with highly dissimilar titles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5501</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5501</id><created>2014-12-17</created><authors><author><keyname>Afisiadis</keyname><forenames>Orion</forenames></author><author><keyname>Balatsoukas-Stimming</keyname><forenames>Alexios</forenames></author><author><keyname>Burg</keyname><forenames>Andreas</forenames></author></authors><title>A Low-Complexity Improved Successive Cancellation Decoder for Polar
  Codes</title><categories>cs.IT math.IT</categories><comments>Presented at the Asilomar Conference on Signals, Systems, and
  Computers, Nov. 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Under successive cancellation (SC) decoding, polar codes are inferior to
other codes of similar blocklength in terms of frame error rate. While more
sophisticated decoding algorithms such as list- or stack-decoding partially
mitigate this performance loss, they suffer from an increase in complexity. In
this paper, we describe a new flavor of the SC decoder, called the SC flip
decoder. Our algorithm preserves the low memory requirements of the basic SC
decoder and adjusts the required decoding effort to the signal quality. In the
waterfall region, its average computational complexity is almost as low as that
of the SC decoder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5512</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5512</id><created>2014-12-17</created><updated>2015-01-04</updated><authors><author><keyname>Brough</keyname><forenames>Tara</forenames></author><author><keyname>Ciobanu</keyname><forenames>Laura</forenames></author><author><keyname>Elder</keyname><forenames>Murray</forenames></author></authors><title>Permutations of context-free and indexed languages</title><categories>cs.FL</categories><comments>13 pages, 5 figures</comments><msc-class>20F65, 68Q45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the cyclic closure of a language, and its generalisation to the
operators $C^k$ introduced by Brandst\&quot;adt. We prove that the cyclic closure of
an indexed language is indexed, and that if $L$ is a context-free language then
$C^k(L)$ is indexed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5513</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5513</id><created>2014-12-17</created><authors><author><keyname>Arouri</keyname><forenames>Cyrine</forenames></author><author><keyname>Nguifo</keyname><forenames>Engelbert Mephu</forenames></author><author><keyname>Aridhi</keyname><forenames>Sabeur</forenames></author><author><keyname>Roucelle</keyname><forenames>C&#xe9;cile</forenames></author><author><keyname>Bonnet-Loosli</keyname><forenames>Gaelle</forenames></author><author><keyname>Tsopz&#xe9;</keyname><forenames>Norbert</forenames></author></authors><title>Towards a constructive multilayer perceptron for regression task using
  non-parametric clustering. A case study of Photo-Z redshift reconstruction</title><categories>cs.NE cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The choice of architecture of artificial neuron network (ANN) is still a
challenging task that users face every time. It greatly affects the accuracy of
the built network. In fact there is no optimal method that is applicable to
various implementations at the same time. In this paper we propose a method to
construct ANN based on clustering, that resolves the problems of random and ad
hoc approaches for multilayer ANN architecture. Our method can be applied to
regression problems. Experimental results obtained with different datasets,
reveals the efficiency of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5514</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5514</id><created>2014-12-17</created><updated>2015-07-22</updated><authors><author><keyname>Zhao</keyname><forenames>Yun-Bin</forenames></author><author><keyname>XU</keyname><forenames>Chunlei</forenames></author></authors><title>1-Bit Compressive Sensing: Reformulation and RRSP-Based Sign Recovery
  Theory</title><categories>cs.IT math.IT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, the 1-bit compressive sensing (1-bit CS) has been studied in the
field of sparse signal recovery. Since the amplitude information of sparse
signals in 1-bit CS is not available, it is often the support or the sign of a
signal that can be exactly recovered with a decoding method. In this paper, we
first show that a necessary assumption (that has been overlooked in the
literature) should be made for some existing theories and discussions for 1-bit
CS. Without such an assumption, the found solution by some existing decoding
algorithms might be inconsistent with 1-bit measurements. This motivates us to
pursue a new direction to develop uniform and nonuniform recovery theories for
1-bit CS with a new decoding method which always generates a solution
consistent with 1-bit measurements. We focus on an extreme case of 1-bit CS, in
which the measurements capture only the sign of the product of a sensing matrix
and a signal. We show that the 1-bit CS model can be reformulated equivalently
as an $\ell_0$-minimization problem with linear constraints. This reformulation
naturally leads to a new linear-program-based decoding method, referred to as
the 1-bit basis pursuit, which is remarkably different from existing
formulations. It turns out that the uniqueness condition for the solution of
the 1-bit basis pursuit yields the so-called restricted range space property
(RRSP) of the transposed sensing matrix. This concept provides a basis to
develop sign recovery conditions for sparse signals through 1-bit measurements.
We prove that if the sign of a sparse signal can be exactly recovered from
1-bit measurements with 1-bit basis pursuit, then the sensing matrix must admit
a certain RRSP, and that if the sensing matrix admits a slightly enhanced RRSP,
then the sign of a $k$-sparse signal can be exactly recovered with 1-bit basis
pursuit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5517</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5517</id><created>2014-12-17</created><authors><author><keyname>De Herve</keyname><forenames>Jocelyn De Goer</forenames></author><author><keyname>Kang</keyname><forenames>Myoung-Ah</forenames></author><author><keyname>Bailly</keyname><forenames>Xavier</forenames></author><author><keyname>Nguifo</keyname><forenames>Engelbert Mephu</forenames></author></authors><title>A perceptual hash function to store and retrieve large scale DNA
  sequences</title><categories>cs.CE q-bio.GN</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel approach for storing and retrieving massive DNA
sequences.. The method is based on a perceptual hash function, commonly used to
determine the similarity between digital images, that we adapted for DNA
sequences. Perceptual hash function presented here is based on a Discrete
Cosine Transform Sign Only (DCT-SO). Each nucleotide is encoded as a fixed gray
level intensity pixel and the hash is calculated from its significant frequency
characteristics. This results to a drastic data reduction between the sequence
and the perceptual hash. Unlike cryptographic hash functions, perceptual hashes
are not affected by &quot;avalanche effect&quot; and thus can be compared. The similarity
distance between two hashes is estimated with the Hamming Distance, which is
used to retrieve DNA sequences. Experiments that we conducted show that our
approach is relevant for storing massive DNA sequences, and retrieving them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5524</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5524</id><created>2014-12-12</created><authors><author><keyname>Jiao</keyname><forenames>Lianmeng</forenames></author><author><keyname>Pan</keyname><forenames>Quan</forenames></author><author><keyname>Liang</keyname><forenames>Yan</forenames></author><author><keyname>Yang</keyname><forenames>Feng</forenames></author></authors><title>A nonlinear tracking algorithm with range-rate measurements based on
  unbiased measurement conversion</title><categories>cs.SY</categories><proxy>ccsd</proxy><journal-ref>2012 15th International Conference on Information Fusion (FUSION),
  Jul 2012, Singapore, Singapore. pp.1400-1405</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The three-dimensional CMKF-U with only position measurements is extended to
solve the nonlinear tracking problem with range-rate measurements in this
paper. A pseudo measurement is constructed by the product of range and
range-rate measurements to reduce the high nonlinearity of the range-rate
measurements with respect to the target state; then the mean and covariance of
the converted measurement errors are derived by the measurement conditioned
method, showing better consistency than the transitional nested conditioning
method; finally, the sequential filter was used to process the converted
position and range-rate measurements sequentially to reduce the approximation
error in the second-order EKF. Monte Carlo simulations show that the
performance of the new tracking algorithm is better than the traditional one
based on CMKF-D.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5526</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5526</id><created>2014-12-15</created><updated>2015-04-27</updated><authors><author><keyname>Xi</keyname><forenames>Chenguang</forenames></author><author><keyname>Wu</keyname><forenames>Qiong</forenames></author><author><keyname>Khan</keyname><forenames>Usman A.</forenames></author></authors><title>Distributed Mirror Descent over Directed Graphs</title><categories>math.OC cs.IT math.IT</categories><comments>This paper has been withdrawn by the author due to a crucial error</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose Distributed Mirror Descent (DMD) algorithm for
constrained convex optimization problems on a (strongly-)connected multi-agent
network. We assume that each agent has a private objective function and a
constraint set. The proposed DMD algorithm employs a locally designed Bregman
distance function at each agent, and thus can be viewed as a generalization of
the well-known Distributed Projected Subgradient (DPS) methods, which use
identical Euclidean distances at the agents. At each iteration of the DMD, each
agent optimizes its own objective adjusted with the Bregman distance function
while exchanging state information with its neighbors. To further generalize
DMD, we consider the case where the agent communication follows a
\emph{directed} graph and it may not be possible to design doubly-stochastic
weight matrices. In other words, we restrict the corresponding weight matrices
to be row-stochastic instead of doubly-stochastic. We study the convergence of
DMD in two cases: (i) when the constraint sets at the agents are the same; and,
(ii) when the constraint sets at the agents are different. By partially
following the spirit of our proof, it can be shown that a class of
consensus-based distributed optimization algorithms, restricted to
doubly-stochastic matrices, remain convergent with stochastic matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5538</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5538</id><created>2014-12-17</created><authors><author><keyname>Olofsson</keyname><forenames>Andreas</forenames></author><author><keyname>Nordstr&#xf6;m</keyname><forenames>Tomas</forenames></author><author><keyname>Ul-Abdin</keyname><forenames>Zain</forenames></author></authors><title>Kickstarting High-performance Energy-efficient Manycore Architectures
  with Epiphany</title><categories>cs.AR cs.DC</categories><comments>(to appear in Asilomar Conference on signals, systems, and computers
  2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce Epiphany as a high-performance energy-efficient
manycore architecture suitable for real-time embedded systems. This scalable
architecture supports floating point operations in hardware and achieves 50
GFLOPS/W in 28 nm technology, making it suitable for high performance streaming
applications like radio base stations and radar signal processing. Through an
efficient 2D mesh Network-on-Chip and a distributed shared memory model, the
architecture is scalable to thousands of cores on a single chip. An
Epiphany-based open source computer named Parallella was launched in 2012
through Kickstarter crowd funding and has now shipped to thousands of customers
around the world.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5551</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5551</id><created>2014-12-17</created><authors><author><keyname>Zefreh</keyname><forenames>Mahdi Ranjbar</forenames></author><author><keyname>Salehi</keyname><forenames>Jawad A.</forenames></author></authors><title>Statistical Modeling and Performance Characterization of an Ultrafast
  Digital Lightwave Communication System Using a Power-Cubic Optical Nonlinear
  Preprocessor (Extended Version)</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an analytical approach in obtaining the probability
density function (pdf) of the random decision variable Y, formed at the output
of power-cubic all-optical nonlinear preprocessor followed by the
photodetector. Our approach can be used to accurately evaluate the performance
of ultrafast pulse detection in the presence of Gaussian noise. Through
rigorous Monte-Carlo simulation, the accuracy of widely used Gaussian
approximation of decision variable Y is refuted. However, in this paper we show
that the so called Log-Pearson type-3 probability density function (LP3 pdf) is
an excellent representation for the decision variable Y . Three distinguishable
parameters of the LP3 pdf are obtained through analytical derivation of three
moments of the decision variable Y . Furthermore, toward a more realistic
model, in addition to ASE Gaussian noise, the effects of shot and thermal
noises are also included. Finally, using the presented analytical approach, it
is shown that power-cubic preprocessor outperforms its quadratic counterparts,
i.e., Second Harmonic Generation (SHG) and Two Photon Absorption (TPA) devices,
in high power regime where shot and thermal noises can be neglected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5557</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5557</id><created>2014-12-17</created><updated>2015-01-02</updated><authors><author><keyname>James</keyname><forenames>Doug</forenames></author><author><keyname>Wilkins-Diehr</keyname><forenames>Nancy</forenames></author><author><keyname>Stodden</keyname><forenames>Victoria</forenames></author><author><keyname>Colbry</keyname><forenames>Dirk</forenames></author><author><keyname>Rosales</keyname><forenames>Carlos</forenames></author><author><keyname>Fahey</keyname><forenames>Mark</forenames></author><author><keyname>Shi</keyname><forenames>Justin</forenames></author><author><keyname>Silva</keyname><forenames>Rafael F.</forenames></author><author><keyname>Lee</keyname><forenames>Kyo</forenames></author><author><keyname>Roskies</keyname><forenames>Ralph</forenames></author><author><keyname>Loewe</keyname><forenames>Laurence</forenames></author><author><keyname>Lindsey</keyname><forenames>Susan</forenames></author><author><keyname>Kooper</keyname><forenames>Rob</forenames></author><author><keyname>Barba</keyname><forenames>Lorena</forenames></author><author><keyname>Bailey</keyname><forenames>David</forenames></author><author><keyname>Borwein</keyname><forenames>Jonathan</forenames></author><author><keyname>Corcho</keyname><forenames>Oscar</forenames></author><author><keyname>Deelman</keyname><forenames>Ewa</forenames></author><author><keyname>Dietze</keyname><forenames>Michael</forenames></author><author><keyname>Gilbert</keyname><forenames>Benjamin</forenames></author><author><keyname>Harkes</keyname><forenames>Jan</forenames></author><author><keyname>Keele</keyname><forenames>Seth</forenames></author><author><keyname>Kumar</keyname><forenames>Praveen</forenames></author><author><keyname>Lee</keyname><forenames>Jong</forenames></author><author><keyname>Linke</keyname><forenames>Erika</forenames></author><author><keyname>Marciano</keyname><forenames>Richard</forenames></author><author><keyname>Marini</keyname><forenames>Luigi</forenames></author><author><keyname>Mattman</keyname><forenames>Chris</forenames></author><author><keyname>Mattson</keyname><forenames>Dave</forenames></author><author><keyname>McHenry</keyname><forenames>Kenton</forenames></author><author><keyname>McLay</keyname><forenames>Robert</forenames></author><author><keyname>Miguez</keyname><forenames>Sheila</forenames></author><author><keyname>Minsker</keyname><forenames>Barbara</forenames></author><author><keyname>Perez-Hernandez</keyname><forenames>Maria</forenames></author><author><keyname>Ryan</keyname><forenames>Dan</forenames></author><author><keyname>Rynge</keyname><forenames>Mats</forenames></author><author><keyname>Santana-Perez</keyname><forenames>Idafen</forenames></author><author><keyname>Satyanarayanan</keyname><forenames>Mahadev</forenames></author><author><keyname>Clair</keyname><forenames>Gloriana St.</forenames></author><author><keyname>Webster</keyname><forenames>Keith</forenames></author><author><keyname>Hovig</keyname><forenames>Elvind</forenames></author><author><keyname>Katz</keyname><forenames>Daniel S.</forenames></author><author><keyname>Kay</keyname><forenames>Sophie</forenames></author><author><keyname>Sandve</keyname><forenames>Geir</forenames></author><author><keyname>Skinner</keyname><forenames>David</forenames></author><author><keyname>Allen</keyname><forenames>Gabrielle</forenames></author><author><keyname>Cazes</keyname><forenames>John</forenames></author><author><keyname>Cho</keyname><forenames>Kym Won</forenames></author><author><keyname>Fonseca</keyname><forenames>Jim</forenames></author><author><keyname>Hwang</keyname><forenames>Lorraine</forenames></author><author><keyname>Koesterke</keyname><forenames>Lars</forenames></author><author><keyname>Patel</keyname><forenames>Pragnesh</forenames></author><author><keyname>Pouchard</keyname><forenames>Line</forenames></author><author><keyname>Seidel</keyname><forenames>Ed</forenames></author><author><keyname>Suriarachchi</keyname><forenames>Isuru</forenames></author></authors><title>Standing Together for Reproducibility in Large-Scale Computing: Report
  on reproducibility@XSEDE</title><categories>cs.DC</categories><msc-class>68N01</msc-class><acm-class>D.2.9</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the final report on reproducibility@xsede, a one-day workshop held in
conjunction with XSEDE14, the annual conference of the Extreme Science and
Engineering Discovery Environment (XSEDE). The workshop's discussion-oriented
agenda focused on reproducibility in large-scale computational research. Two
important themes capture the spirit of the workshop submissions and
discussions: (1) organizational stakeholders, especially supercomputer centers,
are in a unique position to promote, enable, and support reproducible research;
and (2) individual researchers should conduct each experiment as though someone
will replicate that experiment. Participants documented numerous issues,
questions, technologies, practices, and potentially promising initiatives
emerging from the discussion, but also highlighted four areas of particular
interest to XSEDE: (1) documentation and training that promotes reproducible
research; (2) system-level tools that provide build- and run-time information
at the level of the individual job; (3) the need to model best practices in
research collaborations involving XSEDE staff; and (4) continued work on
gateways and related technologies. In addition, an intriguing question emerged
from the day's interactions: would there be value in establishing an annual
award for excellence in reproducible research?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5567</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5567</id><created>2014-12-17</created><updated>2014-12-19</updated><authors><author><keyname>Hannun</keyname><forenames>Awni</forenames></author><author><keyname>Case</keyname><forenames>Carl</forenames></author><author><keyname>Casper</keyname><forenames>Jared</forenames></author><author><keyname>Catanzaro</keyname><forenames>Bryan</forenames></author><author><keyname>Diamos</keyname><forenames>Greg</forenames></author><author><keyname>Elsen</keyname><forenames>Erich</forenames></author><author><keyname>Prenger</keyname><forenames>Ryan</forenames></author><author><keyname>Satheesh</keyname><forenames>Sanjeev</forenames></author><author><keyname>Sengupta</keyname><forenames>Shubho</forenames></author><author><keyname>Coates</keyname><forenames>Adam</forenames></author><author><keyname>Ng</keyname><forenames>Andrew Y.</forenames></author></authors><title>Deep Speech: Scaling up end-to-end speech recognition</title><categories>cs.CL cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a state-of-the-art speech recognition system developed using
end-to-end deep learning. Our architecture is significantly simpler than
traditional speech systems, which rely on laboriously engineered processing
pipelines; these traditional systems also tend to perform poorly when used in
noisy environments. In contrast, our system does not need hand-designed
components to model background noise, reverberation, or speaker variation, but
instead directly learns a function that is robust to such effects. We do not
need a phoneme dictionary, nor even the concept of a &quot;phoneme.&quot; Key to our
approach is a well-optimized RNN training system that uses multiple GPUs, as
well as a set of novel data synthesis techniques that allow us to efficiently
obtain a large amount of varied data for training. Our system, called Deep
Speech, outperforms previously published results on the widely studied
Switchboard Hub5'00, achieving 16.0% error on the full test set. Deep Speech
also handles challenging noisy environments better than widely used,
state-of-the-art commercial speech systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5571</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5571</id><created>2014-12-17</created><authors><author><keyname>L&#xe9;vesque</keyname><forenames>Martin</forenames></author><author><keyname>B&#xe9;chet</keyname><forenames>Christophe</forenames></author><author><keyname>Suignard</keyname><forenames>Eric</forenames></author><author><keyname>Maier</keyname><forenames>Martin</forenames></author><author><keyname>Picault</keyname><forenames>Anne</forenames></author><author><keyname>Jo&#xf3;s</keyname><forenames>G&#xe9;za</forenames></author></authors><title>From Co- Toward Multi-Simulation of Smart Grids based on HLA and FMI
  Standards</title><categories>cs.NI</categories><comments>20 pages, 11 figures, 1 table</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  In this article, a multi-simulation model is proposed to measure the
performance of all Smart Grid perspectives as defined in the IEEE P2030
standard. As a preliminary implementation, a novel information technology (IT)
and communication multi-simulator is developed following an High Level
Architecture (HLA). To illustrate the usefulness of such a multi-simulator, a
case study of a distribution network operation application is presented using
real-world topology configurations with realistic communication traffic based
on IEC 61850. The multi-simulator allows to quantify, in terms of communication
delay and system reliability, the impacts of aggregating all traffic on a
low-capacity wireless link based on Digital Mobile Radio (DMR) when a Long Term
Evolution (LTE) network failure occurs. The case study illustrates that such a
multi-simulator can be used to experiment new smart grid mechanisms and verify
their impacts on all smart grid perspectives in an automated manner. Even more
importantly, multi-simulation can prevent problems before modifying/upgrading a
smart grid and thus potentially reduce utility costs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5583</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5583</id><created>2014-12-17</created><authors><author><keyname>Iaconesi</keyname><forenames>Salvatore</forenames></author><author><keyname>Persico</keyname><forenames>Oriana</forenames></author></authors><title>Visualising Emotional Landmarks in Cities</title><categories>cs.HC cs.CY</categories><comments>6 pages, 5 figures, IV2014 Conference Proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Different people and cultures associate different emotional states to
different parts and spaces of cities. These vary according to individuals,
their cultures and also to the time of day, day of week, season, special
occasions and more. Recurring patterns may occur in correspondence of the
places in which people work, study, entertain themselves, consume, relate, wait
or just take a break. What can we learn from these patterns? Trying to find
possible answers to this question passes through the possibility to visualize
and represent the configurations of emotional expressions in urban spaces,
across time, geography, theme, cultures and other dimensions. We have developed
ways in which it is possible to harvest people's geo-located (or geo-locatable)
emotional expressions from major social networks and to visualize them
according to a variety of different modalities. In this paper we will present a
series of these types of visualizations, and the ways in which they can be used
to gain better understandings of these emotional patterns as they arise, from
points of view which derive from anthropology, urbanism, sociology, politics
and also arts and poetics. The paper will focus on the ways in which the data
is harvested from different social networks, then categorized and annotated
with meta-data describing the emotional states, the languages in which people
express themselves, the geographic locations, the themes expressed. A
methodology for representing this information across a variety of domains
(time, space, emotion, theme) will then be presented in detail. A reflection on
possible usage cases for anthropology, urbanism, policy-making, arts and design
will end the contribution, as well as the description of series of open issues
and the indication of possible next-steps for research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5616</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5616</id><created>2014-12-08</created><authors><author><keyname>Torrieri</keyname><forenames>Don</forenames></author><author><keyname>Talarico</keyname><forenames>Salvatore</forenames></author><author><keyname>Valenti</keyname><forenames>Matthew C.</forenames></author></authors><title>Performance Analysis of Geographic Routing Protocols in Ad Hoc Networks</title><categories>cs.NI</categories><comments>6 pages, 7 figures, IEEE Military Commun. Conf. (MILCOM), 2014. arXiv
  admin note: substantial text overlap with arXiv:1309.3582</comments><doi>10.1109/MILCOM.2014.183</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Geographic routing protocols greatly reduce the requirements of topology
storage and provide flexibility in the accommodation of the dynamic behavior of
ad hoc networks. This paper presents performance evaluations and comparisons of
two geographic routing protocols and the popular AODV protocol. The trade-offs
among the average path reliabilities, average conditional delays, average
conditional number of hops, and area spectral efficiencies and the effects of
various parameters are illustrated for finite ad hoc networks with randomly
placed mobiles. This paper uses a dual method of closed-form analysis and
simple simulation that is applicable to most routing protocols and provides a
much more realistic performance evaluation than has previously been possible.
Some features included in the new analysis are shadowing, exclusion and guard
zones, and distance-dependent fading.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5617</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5617</id><created>2014-12-17</created><authors><author><keyname>Song</keyname><forenames>Shuang</forenames></author><author><keyname>Chaudhuri</keyname><forenames>Kamalika</forenames></author><author><keyname>Sarwate</keyname><forenames>Anand D.</forenames></author></authors><title>Learning from Data with Heterogeneous Noise using SGD</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider learning from data of variable quality that may be obtained from
different heterogeneous sources. Addressing learning from heterogeneous data in
its full generality is a challenging problem. In this paper, we adopt instead a
model in which data is observed through heterogeneous noise, where the noise
level reflects the quality of the data source. We study how to use stochastic
gradient algorithms to learn in this model. Our study is motivated by two
concrete examples where this problem arises naturally: learning with local
differential privacy based on data from multiple sources with different privacy
requirements, and learning from data with labels of variable quality.
  The main contribution of this paper is to identify how heterogeneous noise
impacts performance. We show that given two datasets with heterogeneous noise,
the order in which to use them in standard SGD depends on the learning rate. We
propose a method for changing the learning rate as a function of the
heterogeneity, and prove new regret bounds for our method in two cases of
interest. Experiments on real data show that our method performs better than
using a single learning rate and using only the less noisy of the two datasets
when the noise level is low to moderate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5619</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5619</id><created>2014-12-16</created><authors><author><keyname>Terasawa</keyname><forenames>Yoshihiro</forenames></author></authors><title>A Simple construction of the Pseudorandom Generator from Permutation</title><categories>cs.CR</categories><comments>in japanese</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A simple construction of pseudorandom generator is appear.This pseudorandom
generator is always passed by NIST statistical test.This paper reports a
pseudorandom number generator which has good property is able to construct
using only permutation and data rewriting by XOR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5622</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5622</id><created>2014-12-17</created><authors><author><keyname>Glebov</keyname><forenames>Roman</forenames></author><author><keyname>Hoppen</keyname><forenames>Carlos</forenames></author><author><keyname>Klimosova</keyname><forenames>Tereza</forenames></author><author><keyname>Kohayakawa</keyname><forenames>Yoshiharu</forenames></author><author><keyname>Kral</keyname><forenames>Daniel</forenames></author><author><keyname>Liu</keyname><forenames>Hong</forenames></author></authors><title>Large permutations and parameter testing</title><categories>cs.DM math.CO</categories><comments>14 pages</comments><msc-class>68R05 (Primary), 05A05 (Secondary)</msc-class><acm-class>G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A classical theorem of Erd\H{o}s, Lov\'{a}sz and Spencer asserts that the
densities of connected subgraphs in large graphs are independent. We prove an
analogue of this theorem for permutations and we then apply the methods used in
the proof to give an example of a finitely approximable permutation parameter
that is not finitely forcible. The latter answers a question posed by two of
the authors and Moreira and Sampaio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5627</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5627</id><created>2014-12-17</created><authors><author><keyname>Conque</keyname><forenames>Bruno Mendes Moro</forenames></author><author><keyname>Kashiwabara</keyname><forenames>Andr&#xe9; Yoshiaki</forenames></author><author><keyname>Lopes</keyname><forenames>Fabr&#xed;cio Martins</forenames></author></authors><title>Feature extraction from complex networks: A case of study in genomic
  sequences classification</title><categories>cs.CE cs.LG q-bio.QM</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents a new approach for classification of genomic sequences
from measurements of complex networks and information theory. For this, it is
considered the nucleotides, dinucleotides and trinucleotides of a genomic
sequence. For each of them, the entropy, sum entropy and maximum entropy values
are calculated.For each of them is also generated a network, in which the nodes
are the nucleotides, dinucleotides or trinucleotides and its edges are
estimated by observing the respective adjacency among them in the genomic
sequence. In this way, it is generated three networks, for which measures of
complex networks are extracted.These measures together with measures of
information theory comprise a feature vector representing a genomic sequence.
Thus, the feature vector is used for classification by methods such as SVM,
MultiLayer Perceptron, J48, IBK, Naive Bayes and Random Forest in order to
evaluate the proposed approach.It was adopted coding sequences, intergenic
sequences and TSS (Transcriptional Starter Sites) as datasets, for which the
better results were obtained by the Random Forest with 91.2%, followed by J48
with 89.1% and SVM with 84.8% of accuracy. These results indicate that the new
approach of feature extraction has its value, reaching good levels of
classification even considering only the genomic sequences, i.e., no other a
priori knowledge about them is considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5632</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5632</id><created>2014-12-17</created><authors><author><keyname>Loh</keyname><forenames>Po-Ling</forenames></author><author><keyname>Wainwright</keyname><forenames>Martin J.</forenames></author></authors><title>Support recovery without incoherence: A case for nonconvex
  regularization</title><categories>math.ST cs.IT math.IT stat.ML stat.TH</categories><comments>51 pages, 13 figures</comments><msc-class>62F12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We demonstrate that the primal-dual witness proof method may be used to
establish variable selection consistency and $\ell_\infty$-bounds for sparse
regression problems, even when the loss function and/or regularizer are
nonconvex. Using this method, we derive two theorems concerning support
recovery and $\ell_\infty$-guarantees for the regression estimator in a general
setting. Our results provide rigorous theoretical justification for the use of
nonconvex regularization: For certain nonconvex regularizers with vanishing
derivative away from the origin, support recovery consistency may be guaranteed
without requiring the typical incoherence conditions present in $\ell_1$-based
methods. We then derive several corollaries that illustrate the wide
applicability of our method to analyzing composite objective functions
involving losses such as least squares, nonconvex modified least squares for
errors-in variables linear regression, the negative log likelihood for
generalized linear models, and the graphical Lasso. We conclude with empirical
studies to corroborate our theoretical predictions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5655</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5655</id><created>2014-12-17</created><authors><author><keyname>Chen</keyname><forenames>Xi</forenames></author><author><keyname>Servedio</keyname><forenames>Rocco A.</forenames></author><author><keyname>Tan</keyname><forenames>Li-Yang</forenames></author></authors><title>New algorithms and lower bounds for monotonicity testing</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of testing whether an unknown Boolean function $f$ is
monotone versus $\epsilon$-far from every monotone function. The two main
results of this paper are a new lower bound and a new algorithm for this
well-studied problem.
  Lower bound: We prove an $\tilde{\Omega}(n^{1/5})$ lower bound on the query
complexity of any non-adaptive two-sided error algorithm for testing whether an
unknown Boolean function $f$ is monotone versus constant-far from monotone.
This gives an exponential improvement on the previous lower bound of
$\Omega(\log n)$ due to Fischer et al. [FLN+02]. We show that the same lower
bound holds for monotonicity testing of Boolean-valued functions over hypergrid
domains $\{1,\ldots,m\}^n$ for all $m\ge 2$.
  Upper bound: We give an $\tilde{O}(n^{5/6})\text{poly}(1/\epsilon)$-query
algorithm that tests whether an unknown Boolean function $f$ is monotone versus
$\epsilon$-far from monotone. Our algorithm, which is non-adaptive and makes
one-sided error, is a modified version of the algorithm of Chakrabarty and
Seshadhri [CS13a], which makes $\tilde{O}(n^{7/8})\text{poly}(1/\epsilon)$
queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5657</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5657</id><created>2014-12-17</created><authors><author><keyname>Chen</keyname><forenames>Xi</forenames></author><author><keyname>De</keyname><forenames>Anindya</forenames></author><author><keyname>Servedio</keyname><forenames>Rocco A.</forenames></author><author><keyname>Tan</keyname><forenames>Li-Yang</forenames></author></authors><title>Boolean function monotonicity testing requires (almost) $n^{1/2}$
  non-adaptive queries</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove a lower bound of $\Omega(n^{1/2 - c})$, for all $c&gt;0$, on the query
complexity of (two-sided error) non-adaptive algorithms for testing whether an
$n$-variable Boolean function is monotone versus constant-far from monotone.
This improves a $\tilde{\Omega}(n^{1/5})$ lower bound for the same problem that
was recently given in [CST14] and is very close to $\Omega(n^{1/2})$, which we
conjecture is the optimal lower bound for this model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5659</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5659</id><created>2014-12-17</created><authors><author><keyname>Dronen</keyname><forenames>Nicholas</forenames></author><author><keyname>Foltz</keyname><forenames>Peter W.</forenames></author><author><keyname>Habermehl</keyname><forenames>Kyle</forenames></author></authors><title>Effective sampling for large-scale automated writing evaluation systems</title><categories>cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automated writing evaluation (AWE) has been shown to be an effective
mechanism for quickly providing feedback to students. It has already seen wide
adoption in enterprise-scale applications and is starting to be adopted in
large-scale contexts. Training an AWE model has historically required a single
batch of several hundred writing examples and human scores for each of them.
This requirement limits large-scale adoption of AWE since human-scoring essays
is costly. Here we evaluate algorithms for ensuring that AWE models are
consistently trained using the most informative essays. Our results show how to
minimize training set sizes while maximizing predictive performance, thereby
reducing cost without unduly sacrificing accuracy. We conclude with a
discussion of how to integrate this approach into large-scale AWE systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5661</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5661</id><created>2014-12-17</created><updated>2015-06-01</updated><authors><author><keyname>Ouyang</keyname><forenames>Wanli</forenames></author><author><keyname>Wang</keyname><forenames>Xiaogang</forenames></author><author><keyname>Zeng</keyname><forenames>Xingyu</forenames></author><author><keyname>Qiu</keyname><forenames>Shi</forenames></author><author><keyname>Luo</keyname><forenames>Ping</forenames></author><author><keyname>Tian</keyname><forenames>Yonglong</forenames></author><author><keyname>Li</keyname><forenames>Hongsheng</forenames></author><author><keyname>Yang</keyname><forenames>Shuo</forenames></author><author><keyname>Wang</keyname><forenames>Zhe</forenames></author><author><keyname>Loy</keyname><forenames>Chen-Change</forenames></author><author><keyname>Tang</keyname><forenames>Xiaoou</forenames></author></authors><title>DeepID-Net: Deformable Deep Convolutional Neural Networks for Object
  Detection</title><categories>cs.CV cs.NE</categories><comments>CVPR15, arXiv admin note: substantial text overlap with
  arXiv:1409.3505</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose deformable deep convolutional neural networks for
generic object detection. This new deep learning object detection framework has
innovations in multiple aspects. In the proposed new deep architecture, a new
deformation constrained pooling (def-pooling) layer models the deformation of
object parts with geometric constraint and penalty. A new pre-training strategy
is proposed to learn feature representations more suitable for the object
detection task and with good generalization capability. By changing the net
structures, training strategies, adding and removing some key components in the
detection pipeline, a set of models with large diversity are obtained, which
significantly improves the effectiveness of model averaging. The proposed
approach improves the mean averaged precision obtained by RCNN
\cite{girshick2014rich}, which was the state-of-the-art, from 31\% to 50.3\% on
the ILSVRC2014 detection test set. It also outperforms the winner of
ILSVRC2014, GoogLeNet, by 6.1\%. Detailed component-wise analysis is also
provided through extensive experimental evaluation, which provide a global view
for people to understand the deep learning object detection pipeline.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5666</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5666</id><created>2014-12-17</created><updated>2015-04-13</updated><authors><author><keyname>Yancey</keyname><forenames>Matthew</forenames></author></authors><title>Bipartite Communities</title><categories>math.CO cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a given graph, $G$, let $A$ be the adjacency matrix, $D$ is the diagonal
matrix of degrees, $L' = D - A$ is the combinatorial Laplacian, and $L =
D^{-1/2}L'D^{-1/2}$ is the normalized Laplacian. Recently, the eigenvectors
corresponding to the smallest eigenvalues of $L$ and $L'$ have been of great
interest because of their application to community detection, which is a
nebulously defined problem that essentially seeks to find a vertex set $S$ such
that there are few edges incident with exactly one vertex of $S$. The
connection between community detection and the second smallest eigenvalue (and
the corresponding eigenvector) is well-known. The $k$ smallest eigenvalues have
been used heuristically to find multiple communities in the same graph, and a
justification with theoretical rigor for the use of $k \geq 3$ eigenpairs has
only been found very recently.
  The largest eigenpair of $L$ has been used more classically to solve the
MAX-CUT problem, which seeks to find a vertex set $S$ that maximizes the number
of edges incident with exactly one vertex of $S$. Very recently Trevisan
presented a connection between the largest eigenvalue of $L$ and a recursive
approach to the MAX-CUT problem that seeks to find a &quot;bipartite community&quot; at
each stage. This is related to Kleinberg's HITS algorithm that finds the
largest eigenvalue of $A^TA$. We will provide a justification with theoretical
rigor for looking at the $k$ largest eigenvalues of $L$ to find multiple
bipartite communities in the same graph, and then provide a heuristic algorithm
to find strong bipartite communities that is based on the intuition developed
by the theoretical methods. Finally, we will present the results of applying
our algorithm to various data-mining problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5669</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5669</id><created>2014-12-17</created><updated>2015-08-14</updated><authors><author><keyname>Rosenmann</keyname><forenames>Amnon</forenames></author></authors><title>The Timestamp of Timed Automata</title><categories>cs.FL</categories><comments>24 pages, 7 figures</comments><acm-class>F.1.1; D.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a non-deterministic timed automaton with silent transitions (eNTA), we
compute its timestamp: the set of all time values on which any observable
transition occurs, and also a deterministic timed automaton with the same
timestamp. The timestamp is eventually periodic and is constructed via a finite
periodic augmented region automaton. A consequence of this construction is the
periodicity of the language of timed automata with respect to suffixes.
Applications include the decidability of the 1-bounded language inclusion
problem for eNTA, and a partial method, not bounded by time or number of steps,
for the general language non-inclusion problem for eNTA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5673</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5673</id><created>2014-12-17</created><updated>2015-04-28</updated><authors><author><keyname>Ji</keyname><forenames>Yangfeng</forenames></author><author><keyname>Eisenstein</keyname><forenames>Jacob</forenames></author></authors><title>Entity-Augmented Distributional Semantics for Discourse Relations</title><categories>cs.CL cs.LG</categories><comments>Accepted as a workshop contribution at ICLR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discourse relations bind smaller linguistic elements into coherent texts.
However, automatically identifying discourse relations is difficult, because it
requires understanding the semantics of the linked sentences. A more subtle
challenge is that it is not enough to represent the meaning of each sentence of
a discourse relation, because the relation may depend on links between
lower-level elements, such as entity mentions. Our solution computes
distributional meaning representations by composition up the syntactic parse
tree. A key difference from previous work on compositional distributional
semantics is that we also compute representations for entity mentions, using a
novel downward compositional pass. Discourse relations are predicted not only
from the distributional representations of the sentences, but also of their
coreferent entity mentions. The resulting system obtains substantial
improvements over the previous state-of-the-art in predicting implicit
discourse relations in the Penn Discourse Treebank.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5675</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5675</id><created>2014-12-17</created><updated>2015-05-15</updated><authors><author><keyname>Heydari</keyname><forenames>Ali</forenames></author></authors><title>Stabilizing Value Iteration with and without Approximation Errors</title><categories>cs.SY math.OC stat.ML</categories><comments>In this revision the proof of Lemma 5 is updated. Initial submission
  date: 12/17/2014. (This study has overlaps on Theorem 6 and Lemma 5 with
  another work of the author available at arXiv:1412.6095)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adaptive optimal control using value iteration (VI) initiated from a
stabilizing policy is theoretically analyzed in various aspects including the
continuity of the result, the stability of the system operated using any
single/constant resulting control policy, the stability of the system operated
using the evolving/time-varying control policy, the convergence of the
algorithm, and the optimality of the limit function. Afterwards, the effect of
presence of approximation errors in the involved function approximation
processes is incorporated and another set of results for boundedness of the
approximate VI as well as stability of the system operated under the results
for both cases of applying a single policy or an evolving policy are derived. A
feature of the presented results is providing estimations of the region of
attraction so that if the initial condition is within the region, the whole
trajectory will remain inside it and hence, the function approximation results
will be reliable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5676</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5676</id><created>2014-12-17</created><authors><author><keyname>Heydari</keyname><forenames>Ali</forenames></author></authors><title>Optimal Triggering of Networked Control Systems</title><categories>cs.SY math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of resource allocation of nonlinear networked control systems is
investigated, where, unlike the well discussed case of triggering for
stability, the objective is optimal triggering. An approximate dynamic
programming approach is developed for solving problems with fixed final times
initially and then it is extended to infinite horizon problems. Different cases
including Zero-Order-Hold, Generalized Zero-Order-Hold, and stochastic networks
are investigated. Afterwards, the developments are extended to the case of
problems with unknown dynamics and a model-free scheme is presented for
learning the (approximate) optimal solution. After detailed analyses of
convergence, optimality, and stability of the results, the performance of the
method is demonstrated through different numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5681</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5681</id><created>2014-12-17</created><authors><author><keyname>Chen</keyname><forenames>Xi</forenames></author><author><keyname>Durfee</keyname><forenames>David</forenames></author><author><keyname>Orfanou</keyname><forenames>Anthi</forenames></author></authors><title>On the Complexity of Nash Equilibria in Anonymous Games</title><categories>cs.GT</categories><comments>full version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the problem of finding an {\epsilon}-approximate Nash
equilibrium in an anonymous game with seven pure strategies is complete in
PPAD, when the approximation parameter {\epsilon} is exponentially small in the
number of players.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5687</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5687</id><created>2014-12-17</created><authors><author><keyname>Bendale</keyname><forenames>Abhijit</forenames></author><author><keyname>Boult</keyname><forenames>Terrance</forenames></author></authors><title>Towards Open World Recognition</title><categories>cs.CV</categories><journal-ref>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
  (2015) 1893 - 1902</journal-ref><doi>10.1109/CVPR.2015.7298799</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the of advent rich classification models and high computational power
visual recognition systems have found many operational applications.
Recognition in the real world poses multiple challenges that are not apparent
in controlled lab environments. The datasets are dynamic and novel categories
must be continuously detected and then added. At prediction time, a trained
system has to deal with myriad unseen categories. Operational systems require
minimum down time, even to learn. To handle these operational issues, we
present the problem of Open World recognition and formally define it. We prove
that thresholding sums of monotonically decreasing functions of distances in
linearly transformed feature space can balance &quot;open space risk&quot; and empirical
risk. Our theory extends existing algorithms for open world recognition. We
present a protocol for evaluation of open world recognition systems. We present
the Nearest Non-Outlier (NNO) algorithm which evolves model efficiently, adding
object categories incrementally while detecting outliers and managing open
space risk. We perform experiments on the ImageNet dataset with 1.2M+ images to
validate the effectiveness of our method on large scale visual recognition
tasks. NNO consistently yields superior results on open world recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5694</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5694</id><created>2014-12-17</created><updated>2015-02-16</updated><authors><author><keyname>Pedarsani</keyname><forenames>Ramtin</forenames></author><author><keyname>Lee</keyname><forenames>Kangwook</forenames></author><author><keyname>Ramchandran</keyname><forenames>Kannan</forenames></author></authors><title>Capacity-Approaching PhaseCode for Low-Complexity Compressive Phase
  Retrieval</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1408.0034</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we tackle the general compressive phase retrieval problem. The
problem is to recover a K-sparse complex vector of length n, $x\in
\mathbb{C}^n$, from the magnitudes of m linear measurements, $y=|Ax|$, where $A
\in \mathbb{C}^{m \times n}$ can be designed, and the magnitudes are taken
component-wise for vector $Ax\in \mathbb{C}^m$. We propose a variant of the
PhaseCode algorithm, and show that, using an irregular left-degree sparse-graph
code construction, the algorithm can recover almost all the K non-zero signal
components using only slightly more than 4K measurements under some mild
assumptions, with optimal time and memory complexity of ${\cal O}(K)$. It is
known that the fundamental limit for the number of measurements in compressive
phase retrieval problem is $4K - o(K)$. To the best of our knowledge, this is
the first constructive capacity-approaching compressive phase retrieval
algorithm. As a second contribution, we propose another variant of the
PhaseCode algorithm that is based on a Compressive Sensing framework involving
sparse-graph codes. Our proposed algorithm is an instance of a more powerful
&quot;separation&quot; architecture that can be used to address the compressive
phase-retrieval problem in general. This modular design features a compressive
sensing outer layer, and a trigonometric-based phase-retrieval inner layer. The
compressive-sensing layer operates as a linear phase-aware compressive
measurement subsystem, while the trig-based phase-retrieval layer provides the
desired abstraction between the actually targeted nonlinear phase-retrieval
problem and the induced linear compressive-sensing problem. Invoking this
architecture based on the use of sparse-graph codes for the compressive sensing
layer, we show that we can exactly recover a signal from only the magnitudes of
its linear measurements using only slightly more than 6K measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5697</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5697</id><created>2014-12-17</created><authors><author><keyname>Rahmati</keyname><forenames>Zahed</forenames></author><author><keyname>Abam</keyname><forenames>Mohammad Ali</forenames></author><author><keyname>King</keyname><forenames>Valerie</forenames></author><author><keyname>Whitesides</keyname><forenames>Sue</forenames></author></authors><title>Kinetic $k$-Semi-Yao Graph and its Applications</title><categories>cs.CG</categories><comments>arXiv admin note: text overlap with arXiv:1307.2700, arXiv:1406.5554</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new proximity graph, called the $k$-Semi-Yao graph
($k$-SYG), on a set $P$ of points in $\mathbb{R}^d$, which is a supergraph of
the $k$-nearest neighbor graph ($k$-NNG) of $P$. We provide a kinetic data
structure (KDS) to maintain the $k$-SYG on moving points, where the trajectory
of each point is a polynomial function whose degree is bounded by some
constant. Our technique gives the first KDS for the theta graph (\ie, $1$-SYG)
in $\mathbb{R}^d$. It generalizes and improves on previous work on maintaining
the theta graph in $\mathbb{R}^2$.
  As an application, we use the kinetic $k$-SYG to provide the first KDS for
maintenance of all the $k$-nearest neighbors in $\mathbb{R}^d$, for any $k\geq
1$. Previous works considered the $k=1$ case only. Our KDS for all the
$1$-nearest neighbors is deterministic. The best previous KDS for all the
$1$-nearest neighbors in $ \mathbb{R}^d$ is randomized. Our structure and
analysis are simpler and improve on this work for the $k=1$ case. We also
provide a KDS for all the $(1+\epsilon)$-nearest neighbors, which in fact gives
better performance than previous KDS's for maintenance of all the exact
$1$-nearest neighbors.
  As another application, we present the first KDS for answering reverse
$k$-nearest neighbor queries on moving points in $ \mathbb{R}^d$, for any
$k\geq 1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5706</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5706</id><created>2014-12-17</created><authors><author><keyname>Vabishchevich</keyname><forenames>Petr N.</forenames></author></authors><title>Numerical solution of nonstationary problems for a space-fractional
  diffusion equation</title><categories>cs.NA math.NA</categories><comments>22 pages, 13 figures</comments><msc-class>26A33, 35R11, 65F60, 65M06</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An unsteady problem is considered for a space-fractional diffusion equation
in a bounded domain. A first-order evolutionary equation containing a
fractional power of an elliptic operator of second order is studied for general
boundary conditions of Robin type. Finite element approximation in space is
employed. To construct approximation in time, regularized two-level schemes are
used. The numerical implementation is based on solving the equation with the
fractional power of the elliptic operator using an auxiliary Cauchy problem for
a pseudo-parabolic equation. The results of numerical experiments are presented
for a model two-dimensional problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5707</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5707</id><created>2014-12-17</created><authors><author><keyname>Ikeda</keyname><forenames>Takuya</forenames></author><author><keyname>Nagahara</keyname><forenames>Masaaki</forenames></author></authors><title>Continuity of the Value Function in Sparse Optimal Control</title><categories>cs.SY math.OC</categories><comments>Submitted to ASCC2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove the continuity of the value function of the sparse optimal control
problem. The sparse optimal control is a control whose support is minimum among
all admissible controls. Under the normality assumption, it is known that a
sparse optimal control is given by L^1 optimal control. Furthermore, the value
function of the sparse optimal control problem is identical with that of the
L1-optimal control problem. From these properties, we prove the continuity of
the value function of the sparse optimal control problem by verifying that of
the L1-optimal control problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5710</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5710</id><created>2014-12-17</created><authors><author><keyname>Zhao</keyname><forenames>Jiaqi</forenames></author><author><keyname>Fernandes</keyname><forenames>Vitor Basto</forenames></author><author><keyname>Jiao</keyname><forenames>Licheng</forenames></author><author><keyname>Yevseyeva</keyname><forenames>Iryna</forenames></author><author><keyname>Maulana</keyname><forenames>Asep</forenames></author><author><keyname>Li</keyname><forenames>Rui</forenames></author><author><keyname>B&#xe4;ck</keyname><forenames>Thomas</forenames></author><author><keyname>Emmerich</keyname><forenames>Michael T. M.</forenames></author></authors><title>Multiobjective Optimization of Classifiers by Means of 3-D Convex Hull
  Based Evolutionary Algorithm</title><categories>cs.NE cs.LG</categories><comments>32 pages, 26 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Finding a good classifier is a multiobjective optimization problem with
different error rates and the costs to be minimized. The receiver operating
characteristic is widely used in the machine learning community to analyze the
performance of parametric classifiers or sets of Pareto optimal classifiers. In
order to directly compare two sets of classifiers the area (or volume) under
the convex hull can be used as a scalar indicator for the performance of a set
of classifiers in receiver operating characteristic space.
  Recently, the convex hull based multiobjective genetic programming algorithm
was proposed and successfully applied to maximize the convex hull area for
binary classification problems. The contribution of this paper is to extend
this algorithm for dealing with higher dimensional problem formulations. In
particular, we discuss problems where parsimony (or classifier complexity) is
stated as a third objective and multi-class classification with three different
true classification rates to be maximized.
  The design of the algorithm proposed in this paper is inspired by
indicator-based evolutionary algorithms, where first a performance indicator
for a solution set is established and then a selection operator is designed
that complies with the performance indicator. In this case, the performance
indicator will be the volume under the convex hull. The algorithm is tested and
analyzed in a proof of concept study on different benchmarks that are designed
for measuring its capability to capture relevant parts of a convex hull.
  Further benchmark and application studies on email classification and feature
selection round up the analysis and assess robustness and usefulness of the new
algorithm in real world settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5711</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5711</id><created>2014-12-17</created><authors><author><keyname>Budden</keyname><forenames>David M</forenames></author><author><keyname>Wang</keyname><forenames>Peter</forenames></author><author><keyname>Obst</keyname><forenames>Oliver</forenames></author><author><keyname>Prokopenko</keyname><forenames>Mikhail</forenames></author></authors><title>Simulation leagues: Enabling replicable and robust investigation of
  complex robotic systems</title><categories>cs.RO cs.DC</categories><comments>9 pages, 4 figures. arXiv admin note: text overlap with
  arXiv:1403.4023</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Physically-realistic simulated environments are powerful platforms for
enabling measurable, replicable and statistically-robust investigation of
complex robotic systems. Such environments are epitomised by the RoboCup
simulation leagues, which have been successfully utilised to conduct
massively-parallel experiments in topics including: optimisation of bipedal
locomotion, self-localisation from noisy perception data and planning complex
multi-agent strategies without direct agent-to-agent communication. Many of
these systems are later transferred to physical robots, making the simulation
leagues invaluable well-beyond the scope of simulated soccer matches. In this
study, we provide an overview of the RoboCup simulation leagues and describe
their properties as they pertain to replicable and robust robotics research. To
demonstrate their utility directly, we leverage the ability to run parallelised
experiments to evaluate different competition formats (e.g. round robin) for
the RoboCup 2D simulation league. Our results demonstrate that a
previously-proposed hybrid format minimises fluctuations from 'true'
(statistically-significant) team performance rankings within the time
constraints of the RoboCup world finals. Our experimental analysis would be
impossible with physical robots alone, and we encourage other researchers to
explore the potential for enriching their experimental pipelines with simulated
components, both to minimise experimental costsand enable others to replicate
and expand upon their results in a hardware-independent manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5716</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5716</id><created>2014-12-17</created><authors><author><keyname>Lou</keyname><forenames>Yang</forenames></author><author><keyname>Chen</keyname><forenames>Guanrong</forenames></author></authors><title>Naming game with learning errors in communications</title><categories>cs.SI physics.soc-ph</categories><comments>15 pages, 12 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Naming game simulates the process of naming an objective by a population of
agents organized in a certain communication network topology. By pair-wise
iterative interactions, the population reaches a consensus state
asymptotically. In this paper, we study naming game with communication errors
during pair-wise conversations, where errors are represented by error rates in
a uniform probability distribution. First, a model of naming game with learning
errors in communications (NGLE) is proposed. Then, a strategy for agents to
prevent learning errors is suggested. To that end, three typical topologies of
communication networks, namely random-graph, small-world and scale-free
networks with different parameters, are employed to investigate the effects of
various learning errors. Simulation results on these models show that 1)
learning errors slightly affect the convergence speed but distinctively
increase the requirement for memory of each agent during lexicon propagation;
2) the maximum number of different words held by the whole population increases
linearly as the value of the error rate increases; 3) without applying any
strategy to eliminate learning errors, there is a threshold value of the
learning errors which impairs the convergence. The new findings help to better
understand the role of learning errors in naming game as well as human language
development from a network science perspective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5718</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5718</id><created>2014-12-17</created><updated>2015-09-23</updated><authors><author><keyname>Golnari</keyname><forenames>Golshan</forenames></author><author><keyname>T.</keyname><forenames>Amir Asiaee</forenames></author><author><keyname>Banerjee</keyname><forenames>Arindam</forenames></author><author><keyname>Zhang</keyname><forenames>Zhi-Li</forenames></author></authors><title>Revisiting Non-Progressive Influence Models: Scalable Influence
  Maximization</title><categories>cs.SI physics.soc-ph</categories><comments>G. Golnari and A. Asiaee contributed equally to this work. Published
  in 31st Conference on Uncertainty in Artificial Intelligence (UAI) proceeding</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While influence maximization in social networks has been studied extensively
in computer science community for the last decade the focus has been on the
progressive influence models, such as independent cascade (IC) and Linear
threshold (LT) models, which cannot capture the reversibility of choices. In
this paper, we present the Heat Conduction (HC) model which is a
non-progressive influence model with real-world interpretations. We show that
HC unifies, generalizes, and extends the existing nonprogressive models, such
as the Voter model [1] and non-progressive LT [2]. We then prove that selecting
the optimal seed set of influential nodes is NP-hard for HC but by establishing
the submodularity of influence spread, we can tackle the influence maximization
problem with a scalable and provably near-optimal greedy algorithm. We are the
first to present a scalable solution for influence maximization under
nonprogressive LT model, as a special case of the HC model. In sharp contrast
to the other greedy influence maximization methods, our fast and efficient
C2GREEDY algorithm benefits from two analytically computable steps: closed-form
computation for finding the influence spread as well as the greedy seed
selection. Through extensive experiments on several large real and synthetic
networks, we show that C2GREEDY outperforms the state-of-the-art methods, in
terms of both influence spread and scalability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5720</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5720</id><created>2014-12-18</created><authors><author><keyname>Flannery</keyname><forenames>Madison</forenames></author><author><keyname>Budden</keyname><forenames>David M</forenames></author><author><keyname>Mendes</keyname><forenames>Alexandre</forenames></author></authors><title>FlexDM: Enabling robust and reliable parallel data mining using WEKA</title><categories>cs.MS cs.SE</categories><comments>4 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Performing massive data mining experiments with multiple datasets and methods
is a common task faced by most bioinformatics and computational biology
laboratories. WEKA is a machine learning package designed to facilitate this
task by providing tools that allow researchers to select from several
classification methods and specific test strategies. Despite its popularity,
the current WEKA environment for batch experiments, namely Experimenter, has
four limitations that impact its usability: the selection of value ranges for
methods options lacks flexibility and is not intuitive; there is no support for
parallelisation when running large-scale data mining tasks; the XML schema is
difficult to read, necessitating the use of the Experimenter's graphical user
interface for generation and modification; and robustness is limited by the
fact that results are not saved until the last test has concluded.
  FlexDM implements an interface to WEKA to run batch processing tasks in a
simple and intuitive way. In a short and easy-to-understand XML file, one can
define hundreds of tests to be performed on several datasets. FlexDM also
allows those tests to be executed asynchronously in parallel to take advantage
of multi-core processors, significantly increasing usability and productivity.
Results are saved incrementally for better robustness and reliability.
  FlexDM is implemented in Java and runs on Windows, Linux and OSX. As we
encourage other researchers to explore and adopt our software, FlexDM is made
available as a pre-configured bootable reference environment. All code,
supporting documentation and usage examples are also available for download at
http://sourceforge.net/projects/flexdm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5721</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5721</id><created>2014-12-18</created><updated>2015-02-23</updated><authors><author><keyname>Liberty</keyname><forenames>Edo</forenames></author><author><keyname>Sriharsha</keyname><forenames>Ram</forenames></author><author><keyname>Sviridenko</keyname><forenames>Maxim</forenames></author></authors><title>An Algorithm for Online K-Means Clustering</title><categories>cs.DS cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows that one can be competitive with the k-means objective while
operating online. In this model, the algorithm receives vectors v_1,...,v_n one
by one in an arbitrary order. For each vector the algorithm outputs a cluster
identifier before receiving the next one. Our online algorithm generates ~O(k)
clusters whose k-means cost is ~O(W*). Here, W* is the optimal k-means cost
using k clusters and ~O suppresses poly-logarithmic factors. We also show that,
experimentally, it is not much worse than k-means++ while operating in a
strictly more constrained computational model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5731</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5731</id><created>2014-12-18</created><authors><author><keyname>Lin</keyname><forenames>Yicheng</forenames></author><author><keyname>Bao</keyname><forenames>Wei</forenames></author><author><keyname>Yu</keyname><forenames>Wei</forenames></author><author><keyname>Liang</keyname><forenames>Ben</forenames></author></authors><title>Optimizing User Association and Spectrum Allocation in HetNets: A
  Utility Perspective</title><categories>cs.NI cs.IT math.IT</categories><comments>Accepted with minor revision for publication in the IEEE Journal on
  Selected Areas in Communications, Special Issue on Recent Advances in
  Heterogeneous Cellular Networks</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The joint user association and spectrum allocation problem is studied for
multi-tier heterogeneous networks (HetNets) in both downlink and uplink in the
interference-limited regime. Users are associated with base-stations (BSs)
based on the biased downlink received power. Spectrum is either shared or
orthogonally partitioned among the tiers. This paper models the placement of
BSs in different tiers as spatial point processes and adopts stochastic
geometry to derive the theoretical mean proportionally fair utility of the
network based on the coverage rate. By formulating and solving the network
utility maximization problem, the optimal user association bias factors and
spectrum partition ratios are analytically obtained for the multi-tier network.
The resulting analysis reveals that the downlink and uplink user associations
do not have to be symmetric. For uplink under spectrum sharing, if all tiers
have the same target signal-to-interference ratio (SIR), distance-based user
association is shown to be optimal under a variety of path loss and power
control settings. For both downlink and uplink, under orthogonal spectrum
partition, it is shown that the optimal proportion of spectrum allocated to
each tier should match the proportion of users associated with that tier.
Simulations validate the analytical results. Under typical system parameters,
simulation results suggest that spectrum partition performs better for downlink
in terms of utility, while spectrum sharing performs better for uplink with
power control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5732</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5732</id><created>2014-12-18</created><updated>2015-09-07</updated><authors><author><keyname>Li</keyname><forenames>Changsheng</forenames></author><author><keyname>Wei</keyname><forenames>Fan</forenames></author><author><keyname>Dong</keyname><forenames>Weishan</forenames></author><author><keyname>Liu</keyname><forenames>Qingshan</forenames></author><author><keyname>Wang</keyname><forenames>Xiangfeng</forenames></author><author><keyname>Zhang</keyname><forenames>Xin</forenames></author></authors><title>Dynamic Structure Embedded Online Multiple-Output Regression for Stream
  Data</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online multiple-output regression is an important machine learning technique
for modeling, predicting, and compressing multi-dimensional correlated data
streams. In this paper, we propose a novel online multiple-output regression
method, called MORES, for stream data. MORES can \emph{dynamically} learn the
structure of the coefficients change in each update step to facilitate the
model's continuous refinement. We observe that limited expressive ability of
the regression model, especially in the preliminary stage of online update,
often leads to the variables in the residual errors being dependent. In light
of this point, MORES intends to \emph{dynamically} learn and leverage the
structure of the residual errors to improve the prediction accuracy. Moreover,
we define three statistical variables to \emph{exactly} represent all the seen
samples for \emph{incrementally} calculating prediction loss in each online
update round, which can avoid loading all the training data into memory for
updating model, and also effectively prevent drastic fluctuation of the model
in the presence of noise. Furthermore, we introduce a forgetting factor to set
different weights on samples so as to track the data streams' evolving
characteristics quickly from the latest samples. Experiments on one synthetic
dataset and three real-world datasets validate the effectiveness of the
proposed method. In addition, the update speed of MORES is at least 2000
samples processed per second on the three real-world datasets, more than 15
times faster than the state-of-the-art online learning algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5744</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5744</id><created>2014-12-18</created><updated>2015-04-27</updated><authors><author><keyname>Golden</keyname><forenames>Richard M.</forenames></author></authors><title>Stochastic Descent Analysis of Representation Learning Algorithms</title><categories>stat.ML cs.LG</categories><comments>Version: April 27, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although stochastic approximation learning methods have been widely used in
the machine learning literature for over 50 years, formal theoretical analyses
of specific machine learning algorithms are less common because stochastic
approximation theorems typically possess assumptions which are difficult to
communicate and verify. This paper presents a new stochastic approximation
theorem for state-dependent noise with easily verifiable assumptions applicable
to the analysis and design of important deep learning algorithms including:
adaptive learning, contrastive divergence learning, stochastic descent
expectation maximization, and active learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5758</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5758</id><created>2014-12-18</created><updated>2015-04-01</updated><authors><author><keyname>Wang</keyname><forenames>Zhangyang</forenames></author><author><keyname>Yang</keyname><forenames>Jianchao</forenames></author><author><keyname>Jin</keyname><forenames>Hailin</forenames></author><author><keyname>Shechtman</keyname><forenames>Eli</forenames></author><author><keyname>Agarwala</keyname><forenames>Aseem</forenames></author><author><keyname>Brandt</keyname><forenames>Jonathan</forenames></author><author><keyname>Huang</keyname><forenames>Thomas S.</forenames></author></authors><title>Decomposition-Based Domain Adaptation for Real-World Font Recognition</title><categories>cs.CV</categories><comments>This paper has been withdrawn by the author due to project concerns</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a domain adaption framework to address a domain mismatch between
synthetic training and real-world testing data. We demonstrate our method on a
challenging fine-grain classification problem: recognizing a font style from an
image of text. In this task, it is very easy to generate lots of rendered font
examples but very hard to obtain real-world labeled images. This
real-to-synthetic domain gap caused poor generalization to new real data in
previous font recognition methods (Chen et al. (2014)). In this paper, we
introduce a Convolutional Neural Network decomposition approach, leveraging a
large training corpus of synthetic data to obtain effective features for
classification. This is done using an adaptation technique based on a Stacked
Convolutional Auto-Encoder that exploits a large collection of unlabeled
real-world text images combined with synthetic data preprocessed in a specific
way. The proposed DeepFont method achieves an accuracy of higher than 80%
(top-5) on a new large labeled real-world dataset we collected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5764</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5764</id><created>2014-12-18</created><authors><author><keyname>Patrascu</keyname><forenames>Vasile</forenames></author><author><keyname>Buzuloiu</keyname><forenames>Vasile</forenames></author></authors><title>Image Dynamic Range Enhancement in the Context of Logarithmic Models</title><categories>cs.CV</categories><comments>The 11th European Signal Processing Conference, EUSIPCO 2002,
  Toulouse, France, 03-06 september 2002</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Images of a scene observed under a variable illumination or with a variable
optical aperture are not identical. Does a privileged representant exist? In
which mathematical context? How to obtain it? The authors answer to such
questions in the context of logarithmic models for images. After a short
presentation of the model, the paper presents two image transforms: one
performs an optimal enhancement of the dynamic range, and the other does the
same for the mean dynamic range. Experimental results are shown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5769</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5769</id><created>2014-12-18</created><authors><author><keyname>Patrascu</keyname><forenames>Vasile</forenames></author></authors><title>Gray level image enhancement using the Bernstein polynomials</title><categories>cs.CV</categories><comments>Scientific Bulletin of the Politechnica, University of
  Timisoara,Transactions on Electronics and Communications, Vol. 47 (61), No:
  2,pp.121-126, June 2002</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a method for enhancing the gray level images. This
presented method takes part from the category of point operations and it is
based on piecewise linear functions. The interpolation nodes of these functions
are calculated using the Bernstein polynomials.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5787</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5787</id><created>2014-12-18</created><authors><author><keyname>Patrascu</keyname><forenames>Vasile</forenames></author></authors><title>Gray Level Image Enhancement Using Polygonal Functions</title><categories>cs.CV</categories><comments>The 13th International Conference on Automation, Quality and Testing,
  Robotics, Vol. Robotics, Image and Signal processing, pp. 129-134, May 23-25
  2002, Cluj-Napoca, Romania</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a method for enhancing the gray level images. This method
takes part from the category of point transforms and it is based on
interpolation functions. The latter have a graphic represented by polygonal
lines. The interpolation nodes of these functions are calculated taking into
account the statistics of gray levels belonging to the image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5795</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5795</id><created>2014-12-18</created><updated>2015-02-18</updated><authors><author><keyname>Thorne</keyname><forenames>Camilo</forenames></author></authors><title>The Expressive Power of DL-Lite</title><categories>cs.LO</categories><comments>7pp</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Description logics are knowledge representation formalisms that provide the
formal underpinning of the semantic web and in particular of the $\text{OWL}$
Ontology Web Language. In this paper we investigate the expressive power of
logic $\text{DL-Lite}_{R,\sqcap}$, and some of its computational properties. We
rely on simulations to characterize the absolute expressive power of
$\text{DL-Lite}_{R,\sqcap}$ as a concept language, and to show that disjunction
is not expressible. We also show that no simulation-based closure property
exists for $\text{DL-Lite}_{R,\sqcap}$ assertions. Finally, we show that query
answering of unions of conjunctive queries is $\text{NP-complete}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5796</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5796</id><created>2014-12-18</created><authors><author><keyname>Patrascu</keyname><forenames>Vasile</forenames></author></authors><title>Image Enhancement Using a Generalization of Homographic Function</title><categories>cs.CV</categories><comments>The IEEE International Conference COMMUNICATIONS 2002, pp. 429-434,
  December 5-7, 2002, Bucharest, Romania</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new method of gray level image enhancement, based on
point transforms. In order to define the transform function, it was used a
generalization of the homographic function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5797</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5797</id><created>2014-12-18</created><updated>2016-01-15</updated><authors><author><keyname>Camarero</keyname><forenames>Crist&#xf3;bal</forenames></author><author><keyname>Mart&#xed;nez</keyname><forenames>Carmen</forenames></author></authors><title>Quasi-perfect Lee Codes of Radius 2 and Arbitrarily Large Dimension</title><categories>cs.IT cs.DM math.CO math.IT</categories><comments>Available online at IEEE Transactions on Information Theory</comments><doi>10.1109/TIT.2016.2517069</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A construction of 2-quasi-perfect Lee codes is given over the space $\mathbb
Z_p^n$ for $p$ prime, $p\equiv \pm 5\pmod{12}$ and $n=2[\frac{p}{4}]$. It is
known that there are infinitely many such primes. Golomb and Welch conjectured
that perfect codes for the Lee-metric do not exist for dimension $n\geq 3$ and
radius $r\geq 2$. This conjecture was proved to be true for large radii as well
as for low dimensions. The codes found are very close to be perfect, which
exhibits the hardness of the conjecture. A series of computations show that
related graphs are Ramanujan, which could provide further connections between
Coding and Graph Theories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5802</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5802</id><created>2014-12-18</created><authors><author><keyname>Patrascu</keyname><forenames>Vasile</forenames></author></authors><title>Contour Detection Using Contrast Formulas in the Framework of
  Logarithmic Models</title><categories>cs.CV</categories><comments>The 8th International Conference, Exhibition on Optimization of
  Electrical and Electronic Equipment, OPTIM 2002, Vol III, pp 751-756, 16 - 17
  May 2002, Brasov, Romania</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we use a new logarithmic model of image representation,
developed in [1,2], for edge detection. In fact, in the framework of the new
model we obtain the formulas for computing the &quot;contrast of a pixel&quot; and the
&quot;contrast&quot; image is just the &quot;contour&quot; or edge image. In our setting the range
of values is preserved and the quality of the contour is good for high as well
as for low luminosity regions. We present the comparison of our results with
the results using classical edge detection operators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5808</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5808</id><created>2014-12-18</created><updated>2015-08-18</updated><authors><author><keyname>Niedermayer</keyname><forenames>Johannes</forenames></author><author><keyname>Kr&#xf6;ger</keyname><forenames>Peer</forenames></author></authors><title>Minimizing the Number of Matching Queries for Object Retrieval</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To increase the computational efficiency of interest-point based object
retrieval, researchers have put remarkable research efforts into improving the
efficiency of kNN-based feature matching, pursuing to match thousands of
features against a database within fractions of a second. However, due to the
high-dimensional nature of image features that reduces the effectivity of index
structures (curse of dimensionality), due to the vast amount of features stored
in image databases (images are often represented by up to several thousand
features), this ultimate goal demanded to trade query runtimes for query
precision. In this paper we address an approach complementary to indexing in
order to improve the runtimes of retrieval by querying only the most promising
keypoint descriptors, as this affects matching runtimes linearly and can
therefore lead to increased efficiency. As this reduction of kNN queries
reduces the number of tentative correspondences, a loss of query precision is
minimized by an additional image-level correspondence generation stage with a
computational performance independent of the underlying indexing structure. We
evaluate such an adaption of the standard recognition pipeline on a variety of
datasets using both SIFT and state-of-the-art binary descriptors. Our results
suggest that decreasing the number of queried descriptors does not necessarily
imply a reduction in the result quality as long as alternative ways of
increasing query recall (by thoroughly selecting k) and MAP (using image-level
correspondence generation) are considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5822</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5822</id><created>2014-12-18</created><updated>2015-04-29</updated><authors><author><keyname>Gunderson</keyname><forenames>Karen</forenames></author><author><keyname>Morrison</keyname><forenames>Natasha</forenames></author><author><keyname>Semeraro</keyname><forenames>Jason</forenames></author></authors><title>Bounding the Number of Hyperedges in Friendship $r$-Hypergraphs</title><categories>math.CO cs.DM</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For $r \ge 2$, an $r$-uniform hypergraph is called a friendship
$r$-hypergraph if every set $R$ of $r$ vertices has a unique 'friend' - that
is, there exists a unique vertex $x \notin R$ with the property that for each
subset $A \subseteq R$ of size $r-1$, the set $A \cup \{x\}$ is a hyperedge.
  We show that for $r \geq 3$, the number of hyperedges in a friendship
$r$-hypergraph is at least $\frac{r+1}{r} \binom{n-1}{r-1}$, and we
characterise those hypergraphs which achieve this bound. This generalises a
result given by Li and van Rees in the case when $r = 3$.
  We also obtain a new upper bound on the number of hyperedges in a friendship
$r$-hypergraph, which improves on a known bound given by Li, van Rees, Seo and
Singhi when $r=3$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5830</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5830</id><created>2014-12-18</created><authors><author><keyname>Camarero</keyname><forenames>Crist&#xf3;bal</forenames></author><author><keyname>Mart&#xed;nez</keyname><forenames>Carmen</forenames></author><author><keyname>Beivide</keyname><forenames>Ram&#xf3;n</forenames></author></authors><title>Identifying Codes of Degree 4 Cayley Graphs over Abelian Groups</title><categories>cs.IT cs.DM math.CO math.IT</categories><msc-class>Primary: 94B25, 94C12, Secondary: 05C69</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a wide family of identifying codes over regular Cayley graphs
of degree four which are built over finite Abelian groups is presented. Some of
the codes in this construction are also perfect. The graphs considered include
some well-known graphs such as tori, twisted tori and Kronecker products of two
cycles. Therefore, the codes can be used for identification in these graphs.
Finally, an example of how these codes can be applied for adaptive
identification over these graphs is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5831</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5831</id><created>2014-12-18</created><updated>2015-06-22</updated><authors><author><keyname>Noetzel</keyname><forenames>Janis</forenames></author><author><keyname>Swetly</keyname><forenames>Walter</forenames></author></authors><title>Telling Truth from Correlation</title><categories>cs.IT math.IT</categories><comments>21 pages, 1 figure. Grant numbers corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work is motivated by a question at the heart of unsupervised learning
approaches: Assume we are collecting a number K of (subjective) opinions about
some event E from K different agents. Can we infer E from them? Prima facie
this seems impossible, since the agents may be lying. We model this task by
letting the events be distributed according to some distribution p and the task
is to estimate p under unknown noise. Again, this is impossible without
additional assumptions. We report here the finding of very natural such
assumptions - the availability of multiple copies of the true data, each under
independent and invertible (in the sense of matrices) noise, is already
sufficient: If the true distribution and the observations are modeled on the
same finite alphabet, then the number of such copies needed to determine p to
the highest possible precision is exactly three! This result can be seen as a
counterpart to independent component analysis. Therefore, we call our approach
'dependent component analysis'. In addition, we present generalizations of the
model to different alphabet sizes at in- and output. A second result is found:
the 'activation' of invertibility through multiple parallel uses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5836</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5836</id><created>2014-12-18</created><updated>2015-03-21</updated><authors><author><keyname>Fried</keyname><forenames>Daniel</forenames></author><author><keyname>Duh</keyname><forenames>Kevin</forenames></author></authors><title>Incorporating Both Distributional and Relational Semantics in Word
  Representations</title><categories>cs.CL</categories><comments>Accepted as a workshop contribution at ICLR2015. Long version at:
  arXiv:1412.4369</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the hypothesis that word representations ought to incorporate
both distributional and relational semantics. To this end, we employ the
Alternating Direction Method of Multipliers (ADMM), which flexibly optimizes a
distributional objective on raw text and a relational objective on WordNet.
Preliminary results on knowledge base completion, analogy tests, and parsing
show that word representations trained on both objectives can give improvements
in some cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5847</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5847</id><created>2014-12-18</created><authors><author><keyname>Dorta</keyname><forenames>Antonio</forenames></author><author><keyname>Caon</keyname><forenames>Nicola</forenames></author><author><keyname>Prieto</keyname><forenames>Jorge Andres Perez</forenames></author></authors><title>ConGUSTo: (HT)Condor Graphical Unified Supervising Tool</title><categories>cs.DC</categories><comments>8 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  HTCondor is a distributed job scheduler developed by the University of
Wisconsin-Madison, which allows users to run their applications in other users'
machines when they are not being used, thus providing a considerably increase
in the overall computational power and a more efficient use of the computing
resources. Our institution has been successfully using HTCondor for more than
ten years, and HTCondor is nowadays the most used Supercomputing resource we
have. Although HTCondor provides a wide range of tools and options for its
management and administration, there are currently no tools that can show
detailed usage information and statistics in a clear, easy to interpret,
interactive set of graphics displays. For this reason, we have developed
ConGUSTo, a web-based tool that allows to collect HTCondor usage and statistics
data in an easy way, and present them using a variety of tabular and graphics
charts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5862</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5862</id><created>2014-12-18</created><authors><author><keyname>Jonke</keyname><forenames>Zeno</forenames></author><author><keyname>Habenschuss</keyname><forenames>Stefan</forenames></author><author><keyname>Maass</keyname><forenames>Wolfgang</forenames></author></authors><title>A theoretical basis for efficient computations with noisy spiking
  neurons</title><categories>cs.NE q-bio.NC</categories><comments>main paper: 21 pages, 5 figures supplemental paper: 11 pages, no
  figures</comments><msc-class>68Q10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network of neurons in the brain apply - unlike processors in our current
generation of computer hardware - an event-based processing strategy, where
short pulses (spikes) are emitted sparsely by neurons to signal the occurrence
of an event at a particular point in time. Such spike-based computations
promise to be substantially more power-efficient than traditional clocked
processing schemes. However it turned out to be surprisingly difficult to
design networks of spiking neurons that are able to carry out demanding
computations. We present here a new theoretical framework for organizing
computations of networks of spiking neurons. In particular, we show that a
suitable design enables them to solve hard constraint satisfaction problems
from the domains of planning - optimization and verification - logical
inference. The underlying design principles employ noise as a computational
resource. Nevertheless the timing of spikes (rather than just spike rates)
plays an essential role in the resulting computations. Furthermore, one can
demonstrate for the Traveling Salesman Problem a surprising computational
advantage of networks of spiking neurons compared with traditional artificial
neural networks and Gibbs sampling. The identification of such advantage has
been a well-known open problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5867</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5867</id><created>2014-12-18</created><authors><author><keyname>Dobrescu</keyname><forenames>Lidia</forenames></author></authors><title>Replacing ANSI C with other modern programming languages</title><categories>cs.CY cs.PL</categories><comments>IEEE International Symposium on Fundamentals of Electrical
  Engineering 2014, ISFEE 2014, Bucharest</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Replacing ANSI C language with other modern programming languages such as
Python or Java may be an actual debate topic in technical universities.
Researchers whose primary interests are not in programming area seem to prefer
modern and higher level languages. Keeping standard language ANSI C as a
primary tool for engineers and for microcontrollers programming, robotics and
data acquisition courses is another strong different opinion trend. Function
oriented versus object oriented languages may be another highlighted topic in
actual debates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5873</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5873</id><created>2014-12-18</created><authors><author><keyname>Henrion</keyname><forenames>Didier</forenames></author><author><keyname>Naldi</keyname><forenames>Simone</forenames></author><author><keyname>Din</keyname><forenames>Mohab Safey El</forenames></author></authors><title>Real root finding for determinants of linear matrices</title><categories>cs.SC math.AG</categories><acm-class>I.1; F.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\A_0, \A_1, \ldots, \A_n$ be given square matrices of size $m$ with
rational coefficients. The paper focuses on the exact computation of one point
in each connected component of the real determinantal variety $\{\X \in\RR^n \:
:\: \det(\A_0+x_1\A_1+\cdots+x_n\A_n)=0\}$. Such a problem finds applications
in many areas such as control theory, computational geometry, optimization,
etc. Using standard complexity results this problem can be solved using
$m^{O(n)}$ arithmetic operations. Under some genericity assumptions on the
coefficients of the matrices, we provide an algorithm solving this problem
whose runtime is essentially quadratic in ${{n+m}\choose{n}}^{3}$. We also
report on experiments with a computer implementation of this algorithm. Its
practical performance illustrates the complexity estimates. In particular, we
emphasize that for subfamilies of this problem where $m$ is fixed, the
complexity is polynomial in $n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5889</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5889</id><created>2014-12-18</created><authors><author><keyname>Bshouty</keyname><forenames>Nader H.</forenames></author></authors><title>Dense Testers: Almost Linear Time and Locally Explicit Constructions</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a new notion called $(1-\epsilon)$-tester for a set $M$ of
functions $f:A\to C$. A $(1-\epsilon)$-tester for $M$ maps each element $a\in
A$ to a finite number of elements $B_a=\{b_1,\ldots,b_t\}\subset B$ in a
smaller sub-domain $B\subset A$ where for every $f\in M$ if $f(a)\not=0$ then
$f(b)\not=0$ for at least $(1-\epsilon)$ fraction of the elements $b$ of $B_a$.
I.e., if $f(a)\not=0$ then $\Pr_{b\in B_a}[f(b)\not=0]\ge 1-\epsilon$. The {\it
size} of the $(1-\epsilon)$-tester is $\max_{a\in A}|B_a|$ and the goal is to
minimize this size, construct $B_a$ in deterministic almost linear time and
access and compute each map in poly-log time.
  We use tools from elementary algebra and algebraic function fields to build
$(1-\epsilon)$-testers of small size in deterministic almost linear time. We
also show that our constructions are locally explicit, i.e., one can find any
entry in the construction in time poly-log in the size of the construction and
the field size. We also prove lower bounds that show that the sizes of our
testers and the densities are almost optimal.
  Testers were used in [Bshouty, Testers and its application, ITCS 2014] to
construct almost optimal perfect hash families, universal sets, cover-free
families, separating hash functions, black box identity testing and hitting
sets. The dense testers in this paper shows that such constructions can be done
in almost linear time, are locally explicit and can be made to be dense.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5893</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5893</id><created>2014-12-18</created><authors><author><keyname>Hrube&#x161;</keyname><forenames>Pavel</forenames></author></authors><title>On families of anticommuting matrices</title><categories>cs.CC math.RT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $e_{1},\dots, e_{k}$ be complex $n\times n$ matrices such that
$e_{i}e_{j}=-e_{j}e_{i}$ whenever $i\not=j$. We conjecture that
$\hbox{rk}(e_{1}^{2})+\hbox{rk}(e_{2}^{2})+\cdots+\hbox{rk}(e_{k}^{2})\leq
O(n\log n)$, and prove some results in this direction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5896</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5896</id><created>2014-12-18</created><updated>2015-06-03</updated><authors><author><keyname>Giryes</keyname><forenames>Raja</forenames></author><author><keyname>Sapiro</keyname><forenames>Guillermo</forenames></author><author><keyname>Bronstein</keyname><forenames>Alex M.</forenames></author></authors><title>On the Stability of Deep Networks</title><categories>stat.ML cs.IT cs.LG cs.NE math.IT math.MG</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we study the properties of deep neural networks (DNN) with
random weights. We formally prove that these networks perform a
distance-preserving embedding of the data. Based on this we then draw
conclusions on the size of the training data and the networks' structure. A
longer version of this paper with more results and details can be found in
(Giryes et al., 2015). In particular, we formally prove in the longer version
that DNN with random Gaussian weights perform a distance-preserving embedding
of the data, with a special treatment for in-class and out-of-class data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5902</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5902</id><created>2014-12-07</created><authors><author><keyname>Qiu</keyname><forenames>Teng</forenames></author><author><keyname>Yang</keyname><forenames>Kaifu</forenames></author><author><keyname>Li</keyname><forenames>Chaoyi</forenames></author><author><keyname>Li</keyname><forenames>Yongjie</forenames></author></authors><title>A Physically Inspired Clustering Algorithm: to Evolve Like Particles</title><categories>cs.LG cs.CV</categories><comments>32 pages: text part(1-7), supplementary material(8-32)</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Clustering analysis is a method to organize raw data into categories based on
a measure of similarity. It has been successfully applied to diverse fields
from science to business and engineering. By endowing data points with physical
meaning like particles in the physical world and then leaning their evolving
tendency of moving from higher to lower potentials, data points in the proposed
clustering algorithm sequentially hop to the locations of their transfer points
and gather, after a few steps, at the locations of cluster centers with the
locally lowest potentials, where cluster members can be easily identified. The
whole clustering process is simple and efficient, and can be performed either
automatically or interactively, with reliable performances on test data of
diverse shapes, attributes, and dimensionalities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5903</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5903</id><created>2014-12-18</created><updated>2015-04-10</updated><authors><author><keyname>Jaderberg</keyname><forenames>Max</forenames></author><author><keyname>Simonyan</keyname><forenames>Karen</forenames></author><author><keyname>Vedaldi</keyname><forenames>Andrea</forenames></author><author><keyname>Zisserman</keyname><forenames>Andrew</forenames></author></authors><title>Deep Structured Output Learning for Unconstrained Text Recognition</title><categories>cs.CV</categories><comments>arXiv admin note: text overlap with arXiv:1406.2227</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a representation suitable for the unconstrained recognition of
words in natural images: the general case of no fixed lexicon and unknown
length.
  To this end we propose a convolutional neural network (CNN) based
architecture which incorporates a Conditional Random Field (CRF) graphical
model, taking the whole word image as a single input. The unaries of the CRF
are provided by a CNN that predicts characters at each position of the output,
while higher order terms are provided by another CNN that detects the presence
of N-grams. We show that this entire model (CRF, character predictor, N-gram
predictor) can be jointly optimised by back-propagating the structured output
loss, essentially requiring the system to perform multi-task learning, and
training uses purely synthetically generated data. The resulting model is a
more accurate system on standard real-world text recognition benchmarks than
character prediction alone, setting a benchmark for systems that have not been
trained on a particular lexicon. In addition, our model achieves
state-of-the-art accuracy in lexicon-constrained scenarios, without being
specifically modelled for constrained recognition. To test the generalisation
of our model, we also perform experiments with random alpha-numeric strings to
evaluate the method when no visual language model is applicable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5910</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5910</id><created>2014-12-18</created><authors><author><keyname>Schuster</keyname><forenames>Martin</forenames></author><author><keyname>Schwentick</keyname><forenames>Thomas</forenames></author></authors><title>Games for Active XML Revisited</title><categories>cs.DB cs.FL</categories><comments>To be published in ICDT 2015</comments><acm-class>F.2.m; F.4.2; H.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper studies the rewriting mechanisms for intensional documents in the
Active XML framework, abstracted in the form of active context-free games. The
safe rewriting problem studied in this paper is to decide whether the first
player, Juliet, has a winning strategy for a given game and (nested) word; this
corresponds to a successful rewriting strategy for a given intensional
document. The paper examines several extensions to active context-free games.
  The primary extension allows more expressive schemas (namely XML schemas and
regular nested word languages) for both target and replacement languages and
has the effect that games are played on nested words instead of (flat) words as
in previous studies. Other extensions consider validation of input parameters
of web services, and an alternative semantics based on insertion of service
call results.
  In general, the complexity of the safe rewriting problem is highly
intractable (doubly exponential time), but the paper identifies interesting
tractable cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5932</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5932</id><created>2014-12-18</created><authors><author><keyname>Benoit</keyname><forenames>Ga&#xeb;tan</forenames></author><author><keyname>Lemaitre</keyname><forenames>Claire</forenames></author><author><keyname>Lavenier</keyname><forenames>Dominique</forenames></author><author><keyname>Rizk</keyname><forenames>Guillaume</forenames></author></authors><title>Compression of high throughput sequencing data with probabilistic de
  Bruijn graph</title><categories>cs.DS q-bio.QM</categories><comments>21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivation: Data volumes generated by next-generation sequencing technolo-
gies is now a major concern, both for storage and transmission. This triggered
the need for more efficient methods than general purpose compression tools,
such as the widely used gzip. Most reference-free tools developed for NGS data
compression still use general text compression methods and fail to benefit from
algorithms already designed specifically for the analysis of NGS data. The goal
of our new method Leon is to achieve compression of DNA sequences of high
throughput sequencing data, without the need of a reference genome, with
techniques derived from existing assembly principles, that possibly better
exploit NGS data redundancy. Results: We propose a novel method, implemented in
the software Leon, for compression of DNA sequences issued from high throughput
sequencing technologies. This is a lossless method that does not need a
reference genome. Instead, a reference is built de novo from the set of reads
as a probabilistic de Bruijn Graph, stored in a Bloom filter. Each read is
encoded as a path in this graph, storing only an anchoring kmer and a list of
bifurcations indicating which path to follow in the graph. This new method will
allow to have compressed read files that also already contain its underlying de
Bruijn Graph, thus directly re-usable by many tools relying on this structure.
Leon achieved encoding of a C. elegans reads set with 0.7 bits/base,
outperforming state of the art reference-free methods. Availability: Open
source, under GNU affero GPL License, available for download at
http://gatb.inria.fr/software/leon/
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5937</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5937</id><created>2014-12-18</created><authors><author><keyname>Wu</keyname><forenames>Xuangou</forenames></author><author><keyname>Tang</keyname><forenames>Shaojie</forenames></author><author><keyname>Yang</keyname><forenames>Panlong</forenames></author></authors><title>Low-Complexity Cloud Image Privacy Protection via Matrix Perturbation</title><categories>cs.CR cs.NI</categories><comments>11 pages, 8 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud-assisted image services are widely used for various applications. Due
to the high computational complexity of existing image encryption technology,
it is extremely challenging to provide privacy preserving image services for
resource-constrained smart device. In this paper, we propose a novel
encrypressive cloud-assisted image service scheme, called eCIS. The key idea of
eCIS is to shift the high computational cost to the cloud allowing reduction in
complexity of encoder and decoder on resource-constrained device. This is done
via compressive sensing (CS) techniques, compared with existing approaches, we
are able to achieve privacy protection at no additional transmission cost. In
particular, we design an encryption matrix by taking care of image compression
and encryption simultaneously. Such that, the goal of our design is to minimize
the mutual information of original image and encrypted image. In addition to
the theoretical analysis that demonstrates the security properties and
complexity of our system, we also conduct extensive experiment to evaluate its
performance. The experiment results show that eCIS can effectively protect
image privacy and meet the user's adaptive secure demand. eCIS reduced the
system overheads by up to $4.1\times\sim6.8\times$ compared with the existing
CS based image processing approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5943</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5943</id><created>2014-12-17</created><updated>2015-03-01</updated><authors><author><keyname>Kouzapas</keyname><forenames>Dimitrios</forenames><affiliation>University of Glasgow</affiliation></author><author><keyname>Yoshida</keyname><forenames>Nobuko</forenames><affiliation>Imperial College London</affiliation></author></authors><title>Globally Governed Session Semantics</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 4 (December
  30, 2014) lmcs:775</journal-ref><doi>10.2168/LMCS-10(4:20)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a bisimulation theory based on multiparty session types
where a choreography specification governs the behaviour of session typed
processes and their observer. The bisimulation is defined with the observer
cooperating with the observed process in order to form complete global session
scenarios and usable for proving correctness of optimisations for globally
coordinating threads and processes. The induced bisimulation is strictly more
fine-grained than the standard session bisimulation. The difference between the
governed and standard bisimulations only appears when more than two interleaved
multiparty sessions exist. This distinct feature enables to reason real
scenarios in the large-scale distributed system where multiple choreographic
sessions need to be interleaved. The compositionality of the governed
bisimilarity is proved through the soundness and completeness with respect to
the governed reduction-based congruence. Finally, its usage is demonstrated by
a thread transformation governed under multiple sessions in a real usecase in
the large-scale cyberinfrustracture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5949</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5949</id><created>2014-12-18</created><authors><author><keyname>Xie</keyname><forenames>Pengtao</forenames></author><author><keyname>Xing</keyname><forenames>Eric</forenames></author></authors><title>Large Scale Distributed Distance Metric Learning</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In large scale machine learning and data mining problems with high feature
dimensionality, the Euclidean distance between data points can be
uninformative, and Distance Metric Learning (DML) is often desired to learn a
proper similarity measure (using side information such as example data pairs
being similar or dissimilar). However, high dimensionality and large volume of
pairwise constraints in modern big data can lead to prohibitive computational
cost for both the original DML formulation in Xing et al. (2002) and later
extensions. In this paper, we present a distributed algorithm for DML, and a
large-scale implementation on a parameter server architecture. Our approach
builds on a parallelizable reformulation of Xing et al. (2002), and an
asynchronous stochastic gradient descent optimization procedure. To our
knowledge, this is the first distributed solution to DML, and we show that, on
a system with 256 CPU cores, our program is able to complete a DML task on a
dataset with 1 million data points, 22-thousand features, and 200 million
labeled data pairs, in 15 hours; and the learned metric shows great
effectiveness in properly measuring distances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5967</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5967</id><created>2014-12-18</created><authors><author><keyname>Lan</keyname><forenames>Andrew S.</forenames></author><author><keyname>Studer</keyname><forenames>Christoph</forenames></author><author><keyname>Waters</keyname><forenames>Andrew E.</forenames></author><author><keyname>Baraniuk</keyname><forenames>Richard G.</forenames></author></authors><title>Tag-Aware Ordinal Sparse Factor Analysis for Learning and Content
  Analytics</title><categories>stat.ML cs.LG</categories><journal-ref>In Proc. 6th Intl. Conf. on Educational Data Mining, pages 90-97,
  July 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine learning offers novel ways and means to design personalized learning
systems wherein each student's educational experience is customized in real
time depending on their background, learning goals, and performance to date.
SPARse Factor Analysis (SPARFA) is a novel framework for machine learning-based
learning analytics, which estimates a learner's knowledge of the concepts
underlying a domain, and content analytics, which estimates the relationships
among a collection of questions and those concepts. SPARFA jointly learns the
associations among the questions and the concepts, learner concept knowledge
profiles, and the underlying question difficulties, solely based on the
correct/incorrect graded responses of a population of learners to a collection
of questions. In this paper, we extend the SPARFA framework significantly to
enable: (i) the analysis of graded responses on an ordinal scale (partial
credit) rather than a binary scale (correct/incorrect); (ii) the exploitation
of tags/labels for questions that partially describe the question{concept
associations. The resulting Ordinal SPARFA-Tag framework greatly enhances the
interpretability of the estimated concepts. We demonstrate using real
educational data that Ordinal SPARFA-Tag outperforms both SPARFA and existing
collaborative filtering techniques in predicting missing learner responses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5968</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5968</id><created>2014-12-18</created><authors><author><keyname>Lan</keyname><forenames>Andrew S.</forenames></author><author><keyname>Studer</keyname><forenames>Christoph</forenames></author><author><keyname>Baraniuk</keyname><forenames>Richard G.</forenames></author></authors><title>Quantized Matrix Completion for Personalized Learning</title><categories>stat.ML cs.LG</categories><journal-ref>In Proc. 7th Intl. Conf. on Educational Data Mining, pages
  280-283, July 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recently proposed SPARse Factor Analysis (SPARFA) framework for
personalized learning performs factor analysis on ordinal or binary-valued
(e.g., correct/incorrect) graded learner responses to questions. The underlying
factors are termed &quot;concepts&quot; (or knowledge components) and are used for
learning analytics (LA), the estimation of learner concept-knowledge profiles,
and for content analytics (CA), the estimation of question-concept associations
and question difficulties. While SPARFA is a powerful tool for LA and CA, it
requires a number of algorithm parameters (including the number of concepts),
which are difficult to determine in practice. In this paper, we propose
SPARFA-Lite, a convex optimization-based method for LA that builds on matrix
completion, which only requires a single algorithm parameter and enables us to
automatically identify the required number of concepts. Using a variety of
educational datasets, we demonstrate that SPARFALite (i) achieves comparable
performance in predicting unobserved learner responses to existing methods,
including item response theory (IRT) and SPARFA, and (ii) is computationally
more efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5980</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5980</id><created>2014-12-18</created><authors><author><keyname>Mahmud</keyname><forenames>Mohammad Murtaza</forenames></author><author><keyname>Shatabda</keyname><forenames>Swakkhar</forenames></author><author><keyname>Huda</keyname><forenames>Mohammad Nurul</forenames></author></authors><title>GraATP: A Graph Theoretic Approach for Automated Theorem Proving in
  Plane Geometry</title><categories>cs.AI</categories><comments>The 8th International Conference on Software, Knowledge, Information
  Management and Applications (SKIMA 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automated Theorem Proving (ATP) is an established branch of Artificial
Intelligence. The purpose of ATP is to design a system which can automatically
figure out an algorithm either to prove or disprove a mathematical claim, on
the basis of a set of given premises, using a set of fundamental postulates and
following the method of logical inference. In this paper, we propose GraATP, a
generalized framework for automated theorem proving in plane geometry. Our
proposed method translates the geometric entities into nodes of a graph and the
relations between them as edges of that graph. The automated system searches
for different ways to reach the conclusion for a claim via graph traversal by
which the validity of the geometric theorem is examined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.5984</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.5984</id><created>2014-12-18</created><authors><author><keyname>Hossain</keyname><forenames>Muktadir</forenames></author><author><keyname>Tasnim</keyname><forenames>Tajkia</forenames></author><author><keyname>Shatabda</keyname><forenames>Swakkhar</forenames></author><author><keyname>Farid</keyname><forenames>Dewan M.</forenames></author></authors><title>Stochastic Local Search for Pattern Set Mining</title><categories>cs.AI</categories><comments>The 8th International Conference on Software, Knowledge, Information
  Management and Applications (SKIMA 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Local search methods can quickly find good quality solutions in cases where
systematic search methods might take a large amount of time. Moreover, in the
context of pattern set mining, exhaustive search methods are not applicable due
to the large search space they have to explore. In this paper, we propose the
application of stochastic local search to solve the pattern set mining.
Specifically, to the task of concept learning. We applied a number of local
search algorithms on a standard benchmark instances for pattern set mining and
the results show the potentials for further exploration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6007</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6007</id><created>2014-12-18</created><authors><author><keyname>Braud-Santoni</keyname><forenames>Nicolas</forenames><affiliation>TU Graz</affiliation></author><author><keyname>Dubois</keyname><forenames>Swan</forenames><affiliation>INRIA</affiliation></author><author><keyname>Kaaouachi</keyname><forenames>Mohamed-Hamza</forenames><affiliation>INRIA</affiliation></author><author><keyname>Petit</keyname><forenames>Franck</forenames><affiliation>INRIA</affiliation></author></authors><title>The Next 700 Impossibility Results in Time-Varying Graphs</title><categories>cs.DC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address highly dynamic distributed systems modeled by time-varying graphs
(TVGs). We interest in proof of impossibility results that often use informal
arguments about convergence. First, we provide a distance among TVGs to define
correctly the convergence of TVG sequences. Next, we provide a general
framework that formally proves the convergence of the sequence of executions of
any deterministic algorithm over TVGs of any convergent sequence of TVGs.
Finally, we illustrate the relevance of the above result by proving that no
deterministic algorithm exists to compute the underlying graph of any
connected-over-time TVG, i.e., any TVG of the weakest class of long-lived TVGs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6011</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6011</id><created>2014-12-18</created><authors><author><keyname>Coxon</keyname><forenames>Nicholas</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author></authors><title>Montgomery's method of polynomial selection for the number field sieve</title><categories>cs.CR math.NT</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The number field sieve is the most efficient known algorithm for factoring
large integers that are free of small prime factors. For the polynomial
selection stage of the algorithm, Montgomery proposed a method of generating
polynomials which relies on the construction of small modular geometric
progressions. Montgomery's method is analysed in this paper and the existence
of suitable geometric progressions is considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6012</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6012</id><created>2014-12-15</created><authors><author><keyname>Leifert</keyname><forenames>Gundram</forenames></author><author><keyname>Gr&#xfc;ning</keyname><forenames>Tobias</forenames></author><author><keyname>Strau&#xdf;</keyname><forenames>Tobias</forenames></author><author><keyname>Labahn</keyname><forenames>Roger</forenames></author><author><keyname>CITlab</keyname><forenames>for the University of Rostock -</forenames></author></authors><title>CITlab ARGUS for historical data tables</title><categories>cs.CV cs.NE</categories><comments>arXiv admin note: text overlap with arXiv:1412.3949</comments><msc-class>68T05, 68T10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe CITlab's recognition system for the ANWRESH-2014 competition
attached to the 14. International Conference on Frontiers in Handwriting
Recognition, ICFHR 2014. The task comprises word recognition from segmented
historical documents. The core components of our system are based on
multi-dimensional recurrent neural networks (MDRNN) and connectionist temporal
classification (CTC). The software modules behind that as well as the basic
utility technologies are essentially powered by PLANET's ARGUS framework for
intelligent text recognition and image processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6014</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6014</id><created>2014-11-13</created><authors><author><keyname>Sheikholeslami</keyname><forenames>Azadeh</forenames></author><author><keyname>Goeckel</keyname><forenames>Dennis</forenames></author><author><keyname>Pishro-Nik</keyname><forenames>Hossein</forenames></author></authors><title>Jamming Based on an Ephemeral Key to Obtain Everlasting Security in
  Wireless Environments</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Secure communication over a wiretap channel is considered in the
disadvantaged wireless environment, where the eavesdropper channel is (possibly
much) better than the main channel. We present a method to exploit inherent
vulnerabilities of the eavesdropper's receiver to obtain everlasting secrecy.
Based on an ephemeral cryptographic key pre-shared between the transmitter
Alice and the intended recipient Bob, a random jamming signal is added to each
symbol. Bob can subtract the jamming signal before recording the signal, while
the eavesdropper Eve is forced to perform these non-commutative operations in
the opposite order. Thus, information-theoretic secrecy can be obtained, hence
achieving the goal of converting the vulnerable &quot;cheap&quot; cryptographic secret
key bits into &quot;valuable&quot; information-theoretic (i.e. everlasting) secure bits.
We evaluate the achievable secrecy rates for different settings, and show that,
even when the eavesdropper has perfect access to the output of the transmitter
(albeit through an imperfect analog-to-digital converter), the method can still
achieve a positive secrecy rate. Next we consider a wideband system, where
Alice and Bob perform frequency hopping in addition to adding the random
jamming to the signal, and we show the utility of such an approach even in the
face of substantial eavesdropper hardware capabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6016</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6016</id><created>2014-11-19</created><authors><author><keyname>Trujillo-Rasua</keyname><forenames>Rolando</forenames></author></authors><title>Complexity of distance fraud attacks in graph-based distance bounding</title><categories>cs.CR</categories><comments>Mobile and Ubiquitous Systems: Computing, Networking, and Services -
  10th International Conference, MOBIQUITOUS 2013</comments><doi>10.1007/978-3-319-11569-6_23</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distance bounding (DB) emerged as a countermeasure to the so-called
\emph{relay attack}, which affects several technologies such as RFID, NFC,
Bluetooth, and Ad-hoc networks. A prominent family of DB protocols are those
based on graphs, which were introduced in 2010 to resist both mafia and
distance frauds. The security analysis in terms of distance fraud is performed
by considering an adversary that, given a vertex labeled graph $G = (V, E)$ and
a vertex $v \in V$, is able to find the most frequent $n$-long sequence in $G$
starting from $v$ (MFS problem). However, to the best of our knowledge, it is
still an open question whether the distance fraud security can be computed
considering the aforementioned adversarial model. Our first contribution is a
proof that the MFS problem is NP-Hard even when the graph is constrained to
meet the requirements of a graph-based DB protocol. Although this result does
not invalidate the model, it does suggest that a \emph{too-strong} adversary is
perhaps being considered (i.e., in practice, graph-based DB protocols might
resist distance fraud better than the security model suggests.) Our second
contribution is an algorithm addressing the distance fraud security of the
tree-based approach due to Avoine and Tchamkerten. The novel algorithm improves
the computational complexity $O(2^{2^n+n})$ of the naive approach to
$O(2^{2n}n)$ where $n$ is the number of rounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6017</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6017</id><created>2014-12-07</created><authors><author><keyname>Meghanathan</keyname><forenames>Natarajan</forenames></author></authors><title>A Tutorial on Network Security: Attacks and Controls</title><categories>cs.CR cs.NI</categories><comments>21 pages, 19 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the phenomenal growth in the Internet, network security has become an
integral part of computer and information security. In order to come up with
measures that make networks more secure, it is important to learn about the
vulnerabilities that could exist in a computer network and then have an
understanding of the typical attacks that have been carried out in such
networks. The first half of this paper will expose the readers to the classical
network attacks that have exploited the typical vulnerabilities of computer
networks in the past and solutions that have been adopted since then to prevent
or reduce the chances of some of these attacks. The second half of the paper
will expose the readers to the different network security controls including
the network architecture, protocols, standards and software/ hardware tools
that have been adopted in modern day computer networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6018</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6018</id><created>2014-10-09</created><authors><author><keyname>Visessenee</keyname><forenames>Sirisak</forenames></author><author><keyname>Marukatat</keyname><forenames>Sanparith</forenames></author><author><keyname>Kongkachandra</keyname><forenames>Rachada</forenames></author></authors><title>Automatic Training Data Synthesis for Handwriting Recognition Using the
  Structural Crossing-Over Technique</title><categories>cs.CV cs.LG</categories><comments>8 pages, 6 figures</comments><journal-ref>International Journal of Artificial Intelligence &amp; Applications
  (IJAIA), Vol. 5, No. 5, September 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents a novel technique called &quot;Structural Crossing-Over&quot; to
synthesize qualified data for training machine learning-based handwriting
recognition. The proposed technique can provide a greater variety of patterns
of training data than the existing approaches such as elastic distortion and
tangent-based affine transformation. A couple of training characters are
chosen, then they are analyzed by their similar and different structures, and
finally are crossed over to generate the new characters. The experiments are
set to compare the performances of tangent-based affine transformation and the
proposed approach in terms of the variety of generated characters and percent
of recognition errors. The standard MNIST corpus including 60,000 training
characters and 10,000 test characters is employed in the experiments. The
proposed technique uses 1,000 characters to synthesize 60,000 characters, and
then uses these data to train and test the benchmark handwriting recognition
system that exploits Histogram of Gradient (HOG) as features and Support Vector
Machine (SVM) as recognizer. The experimental result yields 8.06% of errors. It
significantly outperforms the tangent-based affine transformation and the
original MNIST training data, which are 11.74% and 16.55%, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6029</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6029</id><created>2014-12-18</created><authors><author><keyname>Fu</keyname><forenames>Jie</forenames></author><author><keyname>Topcu</keyname><forenames>Ufuk</forenames></author></authors><title>Pareto efficiency in synthesizing shared autonomy policies with temporal
  logic constraints</title><categories>cs.RO cs.HC cs.SY</categories><comments>8 pages, 5 figures, submitted to ICRA 2015 conference</comments><msc-class>93E20</msc-class><acm-class>I.2.9; I.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In systems in which control authority is shared by an autonomous controller
and a human operator, it is important to find solutions that achieve a
desirable system performance with a reasonable workload for the human operator.
We formulate a shared autonomy system capable of capturing the interaction and
switching control between an autonomous controller and a human operator, as
well as the evolution of the operator's cognitive state during control
execution. To trade-off human's effort and the performance level, e.g.,
measured by the probability of satisfying the underlying temporal logic
specification, a two-stage policy synthesis algorithm is proposed for
generating Pareto efficient coordination and control policies with respect to
user specified weights. We integrate the Tchebychev scalarization method for
multi-objective optimization methods to obtain a better coverage of the set of
Pareto efficient solutions than linear scalarization methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6039</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6039</id><created>2014-12-18</created><updated>2015-02-22</updated><authors><author><keyname>Pu</keyname><forenames>Yunchen</forenames></author><author><keyname>Yuan</keyname><forenames>Xin</forenames></author><author><keyname>Carin</keyname><forenames>Lawrence</forenames></author></authors><title>Generative Deep Deconvolutional Learning</title><categories>stat.ML cs.LG</categories><comments>21 pages, 9 figures, revised version for ICLR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A generative Bayesian model is developed for deep (multi-layer) convolutional
dictionary learning. A novel probabilistic pooling operation is integrated into
the deep model, yielding efficient bottom-up and top-down probabilistic
learning. After learning the deep convolutional dictionary, testing is
implemented via deconvolutional inference. To speed up this inference, a new
statistical approach is proposed to project the top-layer dictionary elements
to the data level. Following this, only one layer of deconvolution is required
during testing. Experimental results demonstrate powerful capabilities of the
model to learn multi-layer features from images. Excellent classification
results are obtained on both the MNIST and Caltech 101 datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6041</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6041</id><created>2014-12-15</created><updated>2015-05-19</updated><authors><author><keyname>Liu</keyname><forenames>Chun-Hao</forenames></author><author><keyname>Azarfar</keyname><forenames>Arash</forenames></author><author><keyname>Frigon</keyname><forenames>Jean-Francois</forenames></author><author><keyname>Sanso</keyname><forenames>Brunilde</forenames></author><author><keyname>Cabric</keyname><forenames>Danijela</forenames></author></authors><title>Robust Cooperative Spectrum Sensing Scheduling Optimization in
  Multi-Channel Dynamic Spectrum Access Networks</title><categories>cs.IT math.IT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic spectrum access (DSA) enables secondary networks to find and
efficiently exploit spectrum opportunities. A key factor to design a DSA
network is the spectrum sensing algorithms for multiple channels with multiple
users. Multi-user cooperative channel sensing reduces the sensing time, and
thus it increases transmission throughput. However, in a multi-channel system,
the problem becomes more complex since the benefits of assigning users to sense
channels in parallel must also be considered. A sensing schedule, indicating to
each user the channel that it should sense at different sensing moments, must
be thus created to optimize system performance. In this paper, we formulate the
general sensing scheduling optimization problem and then propose several
sensing strategies to schedule the users according to network parameters with
homogeneous sensors. Later on we extend the results to heterogeneous sensors
and propose a robust scheduling design when we have traffic and channel
uncertainty. We propose three sensing strategies, and, within each one of them,
several solutions, striking a balance between throughput performance and
computational complexity, are proposed. In addition, we show that a sequential
channel sensing strategy is the one to be preferred when the sensing time is
small, the number of channels is large, and the number of users is small. For
all the other cases, a parallel channel sensing strategy is recommended in
terms of throughput performance. We also show that a proposed hybrid
sequential-parallel channel sensing strategy achieves the best performance in
all scenarios at the cost of extra memory and computation complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6043</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6043</id><created>2014-12-18</created><authors><author><keyname>Giard</keyname><forenames>Pascal</forenames></author><author><keyname>Sarkis</keyname><forenames>Gabi</forenames></author><author><keyname>Thibeault</keyname><forenames>Claude</forenames></author><author><keyname>Gross</keyname><forenames>Warren J.</forenames></author></authors><title>A 237 Gbps Unrolled Hardware Polar Decoder</title><categories>cs.AR</categories><comments>4 pages, 3 figures</comments><journal-ref>Electronics Lett., vol. 51, issue 10, May 2015, pp. 762-763</journal-ref><doi>10.1049/el.2014.4432</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter we present a new architecture for a polar decoder using a
reduced complexity successive cancellation decoding algorithm. This novel
fully-unrolled, deeply-pipelined architecture is capable of achieving a coded
throughput of over 237 Gbps for a (1024,512) polar code implemented using an
FPGA. This decoder is two orders of magnitude faster than state-of-the-art
polar decoders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6045</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6045</id><created>2014-12-18</created><updated>2014-12-19</updated><authors><author><keyname>Pi&#xf1;a</keyname><forenames>Luis Nieto</forenames></author><author><keyname>Johansson</keyname><forenames>Richard</forenames></author></authors><title>A Simple and Efficient Method To Generate Word Sense Representations</title><categories>cs.CL</categories><comments>5 pages, submission to ICLR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed representations of words have boosted the performance of many
Natural Language Processing tasks. However, usually only one representation per
word is obtained, not acknowledging the fact that some words have multiple
meanings. This has a negative effect on the individual word representations and
the language model as a whole. In this paper we present a simple model that
enables recent techniques for building word vectors to represent distinct
senses of polysemic words. In our assessment of this model we show that it is
able to effectively discriminate between words' senses and to do so in a
computationally efficient manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6049</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6049</id><created>2014-12-16</created><updated>2015-11-07</updated><authors><author><keyname>Liu</keyname><forenames>Qipeng</forenames></author><author><keyname>Zhao</keyname><forenames>Jiuhua</forenames></author><author><keyname>Wang</keyname><forenames>Xiaofan</forenames></author></authors><title>Distributed Detection via Bayesian Updates and Consensus</title><categories>stat.ME cs.MA cs.SY physics.data-an</categories><comments>6 pages, 3 figures. This paper has been submitted to Chinese Control
  Conference 2015 at Hangzhou, People's Republic of China</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we discuss a class of distributed detection algorithms which
can be viewed as implementations of Bayes' law in distributed settings. Some of
the algorithms are proposed in the literature most recently, and others are
first developed in this paper. The common feature of these algorithms is that
they all combine (i) certain kinds of consensus protocols with (ii) Bayesian
updates. They are different mainly in the aspect of the type of consensus
protocol and the order of the two operations. After discussing their
similarities and differences, we compare these distributed algorithms by
numerical examples. We focus on the rate at which these algorithms detect the
underlying true state of an object. We find that (a) The algorithms with
consensus via geometric average is more efficient than that via arithmetic
average; (b) The order of consensus aggregation and Bayesian update does not
apparently influence the performance of the algorithms; (c) The existence of
communication delay dramatically slows down the rate of convergence; (d) More
communication between agents with different signal structures improves the rate
of convergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6056</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6056</id><created>2014-12-18</created><updated>2015-09-08</updated><authors><author><keyname>Goroshin</keyname><forenames>Ross</forenames></author><author><keyname>Bruna</keyname><forenames>Joan</forenames></author><author><keyname>Tompson</keyname><forenames>Jonathan</forenames></author><author><keyname>Eigen</keyname><forenames>David</forenames></author><author><keyname>LeCun</keyname><forenames>Yann</forenames></author></authors><title>Unsupervised Learning of Spatiotemporally Coherent Metrics</title><categories>cs.CV</categories><comments>To appear at ICCV2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current state-of-the-art classification and detection algorithms rely on
supervised training. In this work we study unsupervised feature learning in the
context of temporally coherent video data. We focus on feature learning from
unlabeled video data, using the assumption that adjacent video frames contain
semantically similar information. This assumption is exploited to train a
convolutional pooling auto-encoder regularized by slowness and sparsity. We
establish a connection between slow feature learning to metric learning and
show that the trained encoder can be used to define a more temporally and
semantically coherent metric.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6058</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6058</id><created>2014-12-18</created><authors><author><keyname>Hong</keyname><forenames>Mingyi</forenames></author></authors><title>A Distributed, Asynchronous and Incremental Algorithm for Nonconvex
  Optimization: An ADMM Based Approach</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The alternating direction method of multipliers (ADMM) has been popular for
solving many signal processing problems, convex or nonconvex. In this paper, we
study an asynchronous implementation of the ADMM for solving a nonconvex
nonsmooth optimization problem, whose objective is the sum of a number of
component functions. The proposed algorithm allows the problem to be solved in
a distributed, asynchronous and incremental manner. First, the component
functions can be distributed to different computing nodes, who perform the
updates asynchronously without coordinating with each other. Two sources of
asynchrony are covered by our algorithm: one is caused by the heterogeneity of
the computational nodes, and the other arises from unreliable communication
links. Second, the algorithm can be viewed as implementing an incremental
algorithm where at each step the (possibly delayed) gradients of only a subset
of component functions are update d. We show that when certain bounds are put
on the level of asynchrony, the proposed algorithm converges to the set of
stationary solutions (resp. optimal solutions) for the nonconvex (resp. convex)
problem. To the best of our knowledge, the proposed ADMM implementation can
tolerate the highest degree of asynchrony, among all known asynchronous
variants of the ADMM. Moreover, it is the first ADMM implementation that can
deal with nonconvexity and asynchrony at the same time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6060</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6060</id><created>2014-12-12</created><authors><author><keyname>Madsen</keyname><forenames>Mark E.</forenames></author><author><keyname>Lipo</keyname><forenames>Carl P.</forenames></author></authors><title>Combinatorial Structure of the Deterministic Seriation Method with
  Multiple Subset Solutions</title><categories>cs.AI physics.soc-ph</categories><comments>8 pages, 2 figures</comments><doi>10.6084/m9.figshare.1269322</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Seriation methods order a set of descriptions given some criterion (e.g.,
unimodality or minimum distance between similarity scores). Seriation is thus
inherently a problem of finding the optimal solution among a set of
permutations of objects. In this short technical note, we review the
combinatorial structure of the classical seriation problem, which seeks a
single solution out of a set of objects. We then extend those results to the
iterative frequency seriation approach introduced by Lipo (1997), which finds
optimal subsets of objects which each satisfy the unimodality criterion within
each subset. The number of possible solutions across multiple solution subsets
is larger than $n!$, which underscores the need to find new algorithms and
heuristics to assist in the deterministic frequency seriation problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6061</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6061</id><created>2014-12-15</created><authors><author><keyname>Leifert</keyname><forenames>Gundram</forenames><affiliation>University of Rostock - CITlab</affiliation></author><author><keyname>Labahn</keyname><forenames>Roger</forenames><affiliation>University of Rostock - CITlab</affiliation></author><author><keyname>Strau&#xdf;</keyname><forenames>Tobias</forenames><affiliation>University of Rostock - CITlab</affiliation></author></authors><title>CITlab ARGUS for Arabic Handwriting</title><categories>cs.CV cs.NE</categories><comments>http://www.nist.gov/itl/iad/mig/upload/OpenHaRT2013_SysDesc_CITLAB.pdf</comments><msc-class>68T10, 68T05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the recent years it turned out that multidimensional recurrent neural
networks (MDRNN) perform very well for offline handwriting recognition tasks
like the OpenHaRT 2013 evaluation DIR. With suitable writing preprocessing and
dictionary lookup, our ARGUS software completed this task with an error rate of
26.27% in its primary setup.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6063</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6063</id><created>2014-10-29</created><authors><author><keyname>Rad</keyname><forenames>Jamal Amani</forenames></author><author><keyname>Parand</keyname><forenames>Kourosh</forenames></author><author><keyname>Abbasbandy</keyname><forenames>Saeid</forenames></author></authors><title>Local weak form meshless techniques based on the radial point
  interpolation (RPI) method and local boundary integral equation (LBIE) method
  to evaluate European and American options</title><categories>cs.CE q-fin.CP</categories><journal-ref>dx.doi.org/10.1016/j.cnsns.2014.07.015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the first time in mathematical finance field, we propose the local weak
form meshless methods for option pricing; especially in this paper we select
and analysis two schemes of them named local boundary integral equation method
(LBIE) based on moving least squares approximation (MLS) and local radial point
interpolation (LRPI) based on Wu's compactly supported radial basis functions
(WCS-RBFs). LBIE and LRPI schemes are the truly meshless methods, because, a
traditional non-overlapping, continuous mesh is not required, either for the
construction of the shape functions, or for the integration of the local
sub-domains. In this work, the American option which is a free boundary
problem, is reduced to a problem with fixed boundary using a Richardson
extrapolation technique. Then the $\theta$-weighted scheme is employed for the
time derivative. Stability analysis of the methods is analyzed and performed by
the matrix method. In fact, based on an analysis carried out in the present
paper, the methods are unconditionally stable for implicit Euler (\theta = 0)
and Crank-Nicolson (\theta = 0.5) schemes. It should be noted that LBIE and
LRPI schemes lead to banded and sparse system matrices. Therefore, we use a
powerful iterative algorithm named the Bi-conjugate gradient stabilized method
(BCGSTAB) to get rid of this system. Numerical experiments are presented
showing that the LBIE and LRPI approaches are extremely accurate and fast.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6064</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6064</id><created>2014-10-29</created><authors><author><keyname>Rad</keyname><forenames>Jamal Amani</forenames></author><author><keyname>Parand</keyname><forenames>Kourosh</forenames></author></authors><title>Numerical pricing of American options under two stochastic factor models
  with jumps using a meshless local Petrov-Galerkin method</title><categories>cs.CE q-fin.CP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The most recent update of financial option models is American options under
stochastic volatility models with jumps in returns (SVJ) and stochastic
volatility models with jumps in returns and volatility (SVCJ). To evaluate
these options, mesh-based methods are applied in a number of papers but it is
well-known that these methods depend strongly on the mesh properties which is
the major disadvantage of them. Therefore, we propose the use of the meshless
methods to solve the aforementioned options models, especially in this work we
select and analyze one scheme of them, named local radial point interpolation
(LRPI) based on Wendland's compactly supported radial basis functions
(WCS-RBFs) with C6, C4 and C2 smoothness degrees. The LRPI method which is a
special type of meshless local Petrov-Galerkin method (MLPG), offers several
advantages over the mesh-based methods, nevertheless it has never been applied
to option pricing, at least to the very best of our knowledge. These schemes
are the truly meshless methods, because, a traditional non-overlapping
continuous mesh is not required, neither for the construction of the shape
functions, nor for the integration of the local sub-domains. In this work, the
American option which is a free boundary problem, is reduced to a problem with
fixed boundary using a Richardson extrapolation technique. Then the
implicit-explicit (IMEX) time stepping scheme is employed for the time
derivative which allows us to smooth the discontinuities of the options'
payoffs. Stability analysis of the method is analyzed and performed. In fact,
according to an analysis carried out in the present paper, the proposed method
is unconditionally stable. Numerical experiments are presented showing that the
proposed approaches are extremely accurate and fast.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6065</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6065</id><created>2014-12-02</created><authors><author><keyname>Klein</keyname><forenames>Rolf</forenames><affiliation>University of Bonn, Germany, Institute of Computer Science I</affiliation></author><author><keyname>Langetepe</keyname><forenames>Elmar</forenames><affiliation>University of Bonn, Germany, Institute of Computer Science I</affiliation></author><author><keyname>Levcopoulos</keyname><forenames>Christos</forenames><affiliation>University of Lund, Sweden, Department of Computer Science</affiliation></author></authors><title>A Fire Fighter's Problem</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose that a circular fire spreads in the plane at unit speed. A fire
fighter can build a barrier at speed $v&gt;1$. How large must $v$ be to ensure
that the fire can be contained, and how should the fire fighter proceed? We
provide two results. First, we analyze the natural strategy where the fighter
keeps building a barrier along the frontier of the expanding fire. We prove
that this approach contains the fire if $v&gt;v_c=2.6144 \ldots$ holds. Second, we
show that any &quot;spiralling&quot; strategy must have speed $v&gt;1.618$ in order to
succeed.
  Keywords: Motion Planning, Dynamic Environments, Spiralling strategies, Lower
and upper bounds
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6067</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6067</id><created>2014-12-17</created><authors><author><keyname>Fabbri</keyname><forenames>Mattia</forenames></author><author><keyname>Callegari</keyname><forenames>Sergio</forenames></author></authors><title>Very Low Cost Entropy Source Based on Chaotic Dynamics Retrofittable on
  Networked Devices to Prevent RNG Attacks</title><categories>cs.CR cs.AR</categories><comments>4 pages, 6 figures. Pre-print from conference proceedings; IEEE 21th
  International Conference on Electronics, Circuits, and Systems (ICECS 2014),
  pp. 175-178, Dec. 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Good quality entropy sources are indispensable in most modern cryptographic
protocols. Unfortunately, many currently deployed networked devices do not
include them and may be vulnerable to Random Number Generator (RNG) attacks.
Since most of these systems allow firmware upgrades and have serial
communication facilities, the potential for retrofitting them with secure
hardware-based entropy sources exists. To this aim, very low-cost, robust, easy
to deploy solutions are required. Here, a retrofittable, sub 10$ entropy source
based on chaotic dynamics is illustrated, capable of a 32 kbit/s rate or more
and offering multiple serial communication options including USB, I2C, SPI or
USART. Operation is based on a loop built around the Analog to Digital
Converter (ADC) hosted on a standard microcontroller.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6069</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6069</id><created>2014-10-07</created><authors><author><keyname>Roorda</keyname><forenames>Dirk</forenames></author><author><keyname>Heuvel</keyname><forenames>Charles van den</forenames></author></authors><title>Annotation as a New Paradigm in Research Archiving</title><categories>cs.DL cs.CL</categories><comments>http://depot.knaw.nl/13026/</comments><journal-ref>Proceedings of the American Society for Information Science and
  Technology; Volume 49, Issue 1, pages 1-10, 2012</journal-ref><doi>10.1002/meet.14504901084</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We outline a paradigm to preserve results of digital scholarship, whether
they are query results, feature values, or topic assignments. This paradigm is
characterized by using annotations as multifunctional carriers and making them
portable. The testing grounds we have chosen are two significant enterprises,
one in the history of science, and one in Hebrew scholarship. The first one
(CKCC) focuses on the results of a project where a Dutch consortium of
universities, research institutes, and cultural heritage institutions
experimented for 4 years with language techniques and topic modeling methods
with the aim to analyze the emergence of scholarly debates. The data: a complex
set of about 20.000 letters. The second one (DTHB) is a multi-year effort to
express the linguistic features of the Hebrew bible in a text database, which
is still growing in detail and sophistication. Versions of this database are
packaged in commercial bible study software. We state that the results of these
forms of scholarship require new knowledge management and archive practices.
Only when researchers can build efficiently on each other's (intermediate)
results, they can achieve the aggregations of quality data by which new
questions can be answered, and hidden patterns visualized. Archives are
required to find a balance between preserving authoritative versions of sources
and supporting collaborative efforts in digital scholarship. Annotations are
promising vehicles for preserving and reusing research results. Keywords
annotation, portability, archiving, queries, features, topics, keywords,
Republic of Letters, Hebrew text databases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6071</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6071</id><created>2014-12-18</created><updated>2015-05-12</updated><authors><author><keyname>Graham</keyname><forenames>Benjamin</forenames></author></authors><title>Fractional Max-Pooling</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional networks almost always incorporate some form of spatial
pooling, and very often it is alpha times alpha max-pooling with alpha=2.
Max-pooling act on the hidden layers of the network, reducing their size by an
integer multiplicative factor alpha. The amazing by-product of discarding 75%
of your data is that you build into the network a degree of invariance with
respect to translations and elastic distortions. However, if you simply
alternate convolutional layers with max-pooling layers, performance is limited
due to the rapid reduction in spatial size, and the disjoint nature of the
pooling regions. We have formulated a fractional version of max-pooling where
alpha is allowed to take non-integer values. Our version of max-pooling is
stochastic as there are lots of different ways of constructing suitable pooling
regions. We find that our form of fractional max-pooling reduces overfitting on
a variety of datasets: for instance, we improve on the state-of-the art for
CIFAR-100 without even using dropout.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6072</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6072</id><created>2014-11-20</created><updated>2015-08-14</updated><authors><author><keyname>Boros</keyname><forenames>Endre</forenames></author><author><keyname>Elbassioni</keyname><forenames>Khaled</forenames></author><author><keyname>Gurvich</keyname><forenames>Vladimir</forenames></author><author><keyname>Makino</keyname><forenames>Kazuhisa</forenames></author></authors><title>A Nested Family of $k$-total Effective Rewards for Positional Games</title><categories>cs.DM cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider Gillette's two-person zero-sum stochastic games with perfect
information. For each $k \in \ZZ_+$ we introduce an effective reward function,
called $k$-total. For $k = 0$ and $1$ this function is known as {\it mean
payoff} and {\it total reward}, respectively. We restrict our attention to the
deterministic case. For all $k$, we prove the existence of a saddle point which
can be realized by uniformly optimal pure stationary strategies. We also
demonstrate that $k$-total reward games can be embedded into $(k+1)$-total
reward games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6073</identifier>
 <datestamp>2015-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6073</id><created>2014-11-19</created><updated>2015-01-15</updated><authors><author><keyname>Kunegis</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author></authors><title>Exploiting the Structure of Bipartite Graphs for Algebraic and Spectral
  Graph Theory Applications</title><categories>cs.DM cs.SI</categories><comments>37 pages; fixed references</comments><doi>10.1080/15427951.2014.958250</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we extend several algebraic graph analysis methods to
bipartite networks. In various areas of science, engineering and commerce, many
types of information can be represented as networks, and thus the discipline of
network analysis plays an important role in these domains. A powerful and
widespread class of network analysis methods is based on algebraic graph
theory, i.e., representing graphs as square adjacency matrices. However, many
networks are of a very specific form that clashes with that representation:
They are bipartite. That is, they consist of two node types, with each edge
connecting a node of one type with a node of the other type. Examples of
bipartite networks (also called \emph{two-mode networks}) are persons and the
social groups they belong to, musical artists and the musical genres they play,
and text documents and the words they contain. In fact, any type of feature
that can be represented by a categorical variable can be interpreted as a
bipartite network. Although bipartite networks are widespread, most literature
in the area of network analysis focuses on unipartite networks, i.e., those
networks with only a single type of node. The purpose of this article is to
extend a selection of important algebraic network analysis methods to bipartite
networks, showing that many methods from algebraic graph theory can be applied
to bipartite networks with only minor modifications. We show methods for
clustering, visualization and link prediction. Additionally, we introduce new
algebraic methods for measuring the bipartivity in near-bipartite graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6075</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6075</id><created>2014-10-22</created><authors><author><keyname>Koutis</keyname><forenames>Ioannis</forenames></author><author><keyname>Miller</keyname><forenames>Gary</forenames></author><author><keyname>Peng</keyname><forenames>Richard</forenames></author></authors><title>A Generalized Cheeger Inequality</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The generalized conductance $\phi(G,H)$ between two graphs $G$ and $H$ on the
same vertex set $V$ is defined as the ratio $$
  \phi(G,H) = \min_{S\subseteq V} \frac{cap_G(S,\bar{S})}{ cap_H(S,\bar{S})},
$$ where $cap_G(S,\bar{S})$ is the total weight of the edges crossing from $S$
to $\bar{S}=V-S$. We show that the minimum generalized eigenvalue
$\lambda(L_G,L_H)$ of the pair of Laplacians $L_G$ and $L_H$ satisfies $$
  \lambda(L_G,L_H) \geq \phi(G,H) \phi(G)/8, $$ where $\phi(G)$ is the usual
conductance of $G$. A generalized cut that meets this bound can be obtained
from the generalized eigenvector corresponding to $\lambda(L_G,L_H)$. The
inequality complements a recent proof that $\phi(G)$ cannot be replaced by
$\Theta(\phi(G,H))$ in the above inequality, unless the Unique Games Conjecture
is false.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6077</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6077</id><created>2014-12-02</created><updated>2015-09-07</updated><authors><author><keyname>Li</keyname><forenames>Jiawei</forenames></author><author><keyname>Kendall</keyname><forenames>Graham</forenames></author></authors><title>On Nash Equilibrium and Evolutionarily Stable States that Are Not
  Characterised by the Folk Theorem</title><categories>cs.GT q-bio.PE</categories><doi>10.1371/journal.pone.0136032</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In evolutionary game theory, evolutionarily stable states are characterised
by the folk theorem because exact solutions to the replicator equation are
difficult to obtain. It is generally assumed that the folk theorem, which is
the fundamental theory for non-cooperative games, defines all Nash equilibria
in infinitely repeated games. Here, we prove that Nash equilibria that are not
characterised by the folk theorem do exist. By adopting specific reactive
strategies, a group of players can be better off by coordinating their actions
in repeated games. We call it a type-k equilibrium when a group of k players
coordinate their actions and they have no incentive to deviate from their
strategies simultaneously. The existence and stability of the type-k
equilibrium in general games is discussed. This study shows that the sets of
Nash equilibria and evolutionarily stable states have greater cardinality than
classic game theory has predicted in many repeated games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6078</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6078</id><created>2014-10-16</created><authors><author><keyname>Sethuraman</keyname><forenames>Jay</forenames></author><author><keyname>Ye</keyname><forenames>Chun</forenames></author></authors><title>A Note on the Assignment Problem with Uniform Preferences</title><categories>cs.GT</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by a problem of scheduling unit-length jobs with weak preferences
over time-slots, the random assignment problem (also called the house
allocation problem) is considered on a uniform preference domain. For the
subdomain in which preferences are strict except possibly for the class of
unacceptable objects, Bogomolnaia and Moulin characterized the probabilistic
serial mechanism as the only mechanism satisfying equal treatment of equals,
strategyproofness, and ordinal efficiency. The main result in this paper is
that the natural extension of the probabilistic serial mechanism to the domain
of weak, but uniform, preferences fails strategyproofness, but so does every
other mechanism that is ordinally efficient and treats equals equally. If
envy-free assignments are required, then any (probabilistic or deterministic)
mechanism that guarantees an ex post efficient outcome must fail even a weak
form of strategyproofness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6079</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6079</id><created>2014-12-15</created><authors><author><keyname>Sadeghi</keyname><forenames>Fereshteh</forenames></author><author><keyname>Izadinia</keyname><forenames>Hamid</forenames></author></authors><title>Decoding the Text Encoding</title><categories>cs.HC</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Word clouds and text visualization is one of the recent most popular and
widely used types of visualizations. Despite the attractiveness and simplicity
of producing word clouds, they do not provide a thorough visualization for the
distribution of the underlying data. Therefore, it is important to redesign
word clouds for improving their design choices and to be able to do further
statistical analysis on data. In this paper we have proposed a fully automatic
redesigning algorithm for word cloud visualization. Our proposed method is able
to decode an input word cloud visualization and provides the raw data in the
form of a list of (word, value) pairs. To the best of our knowledge our work is
the first attempt to extract raw data from word cloud visualization. We have
tested our proposed method both qualitatively and quantitatively. The results
of our experiments show that our algorithm is able to extract the words and
their weights effectively with considerable low error rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6080</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6080</id><created>2014-12-15</created><authors><author><keyname>Augot</keyname><forenames>Daniel</forenames><affiliation>LIX</affiliation></author></authors><title>Generalization of Gabidulin Codes over Fields of Rational Functions</title><categories>cs.SC cs.IT math.IT</categories><comments>21st International Symposium on Mathematical Theory of Networks and
  Systems (MTNS 2014), Jul 2014, Groningen, Netherlands.
  https://fwn06.housing.rug.nl/mtns2014/</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We transpose the theory of rank metric and Gabidulin codes to the case of
fields which are not finite fields. The Frobenius automorphism is replaced by
any element of the Galois group of a cyclic algebraic extension of a base
field. We use our framework to define Gabidulin codes over the field of
rational functions using algebraic function fields with a cyclic Galois group.
This gives a linear subspace of matrices whose coefficients are rational
function, such that the rank of each of this matrix is lower bounded, where the
rank is comprised in term of linear combination with rational functions. We
provide two examples based on Kummer and Artin-Schreier extensions.The matrices
that we obtain may be interpreted as generating matrices of convolutional
codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6082</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6082</id><created>2014-11-28</created><authors><author><keyname>Botorek</keyname><forenames>Jan</forenames></author><author><keyname>Budikova</keyname><forenames>Petra</forenames></author><author><keyname>Zezula</keyname><forenames>Pavel</forenames></author></authors><title>Visual Concept Ontology for Image Annotations</title><categories>cs.IR cs.DL</categories><acm-class>I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In spite of the development of content-based data management, text-based
searching remains the primary means of multimedia retrieval in many areas.
Automatic creation of text metadata is thus a crucial tool for increasing the
findability of multimedia objects. Search-based annotation tools try to provide
content-descriptive keywords by exploiting web data, which are easily available
but unstructured and noisy. Such data need to be analyzed with the help of
semantic resources that provide knowledge about objects and relationships in a
given domain. In this paper, we focus on the task of general-purpose image
annotation and present the VCO, a new ontology of visual concepts developed as
a part of image annotation framework. The ontology is linked with the WordNet
lexical database, so the annotation tools can easily integrate information from
both these resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6083</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6083</id><created>2014-12-17</created><authors><author><keyname>Assem</keyname><forenames>Ali</forenames></author></authors><title>On Symmetrized Weight Compositions</title><categories>cs.IT math.IT math.RA</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A characterization of module alphabets with the Hamming weight EP
(abbreviation for Extension Property) had been settled. A thoughtfully
constructed example by J.A.Wood finished the tour. Frobenius bimodules were
proved to satisfy the EP with respect to symmetrized weight compositions. In 4,
the embeddability in the character group of the ambient ring R was found
sufficient for a module RA to satisfy the EP with respect to swc built on any
subgroup of AutR(A), while the necessity remained a question. A least action
trial suggests bridging to the already settled case, a trial that turns out to
be successful. Here, landing in a Midway, the necessity is proved by jumping to
Hamming weight. Corollary 1.11 declares a characterization of module alphabets
satisfying the EP with respect to swc
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6092</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6092</id><created>2014-12-18</created><authors><author><keyname>Patrascu</keyname><forenames>Vasile</forenames></author></authors><title>Image enhancement using the mean dynamic range maximization with
  logarithmic operations</title><categories>cs.CV</categories><comments>Periodica Politechnica, Transactions on Automatic Control and
  Computer Science, Vol.47 (61), 2002, ISSN 1224-600X, pp. 121-126, Timisoara,
  Romania. arXiv admin note: text overlap with arXiv:1412.5764</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we use a logarithmic model for gray level image enhancement. We
begin with a short presentation of the model and then, we propose a new formula
for the mean dynamic range. After that we present two image transforms: one
performs an optimal enhancement of the mean dynamic range using the logarithmic
addition, and the other does the same for positive and negative values using
the logarithmic scalar multiplication. We present the comparison of the results
obtained by dynamic ranges optimization with the results obtained using
classical image enhancement methods like gamma correction and histogram
equalization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6093</identifier>
 <datestamp>2014-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6093</id><created>2014-12-18</created><updated>2014-12-23</updated><authors><author><keyname>Goel</keyname><forenames>Kratarth</forenames></author><author><keyname>Vohra</keyname><forenames>Raunaq</forenames></author></authors><title>Learning Temporal Dependencies in Data Using a DBN-BLSTM</title><categories>cs.LG cs.NE</categories><comments>6 pages, 2 figures, 1 table, ICLR 2015 conference track submission
  under review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since the advent of deep learning, it has been used to solve various problems
using many different architectures. The application of such deep architectures
to auditory data is also not uncommon. However, these architectures do not
always adequately consider the temporal dependencies in data. We thus propose a
new generic architecture called the Deep Belief Network - Bidirectional Long
Short-Term Memory (DBN-BLSTM) network that models sequences by keeping track of
the temporal information while enabling deep representations in the data. We
demonstrate this new architecture by applying it to the task of music
generation and obtain state-of-the-art results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6095</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6095</id><created>2014-12-18</created><updated>2015-05-15</updated><authors><author><keyname>Heydari</keyname><forenames>Ali</forenames></author></authors><title>Theoretical and Numerical Analysis of Approximate Dynamic Programming
  with Approximation Errors</title><categories>cs.SY cs.LG math.OC stat.ML</categories><comments>This study is the counterpart of another work of the author
  (arXiv:1412.5675) which was for value iterations with initial stabilizing
  guess (with overlaps on Theorem 1 and Lemma 1). As for the revision on this
  work, some steps of proofs are updated and an explanation about the
  approximation error is included. Initial submission date: 12/18/2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study is aimed at answering the famous question of how the approximation
errors at each iteration of Approximate Dynamic Programming (ADP) affect the
quality of the final results considering the fact that errors at each iteration
affect the next iteration. To this goal, convergence of Value Iteration scheme
of ADP for deterministic nonlinear optimal control problems with undiscounted
cost functions is investigated while considering the errors existing in
approximating respective functions. The boundedness of the results around the
optimal solution is obtained based on quantities which are known in a general
optimal control problem and assumptions which are verifiable. Moreover, since
the presence of the approximation errors leads to the deviation of the results
from optimality, sufficient conditions for stability of the system operated by
the result obtained after a finite number of value iterations, along with an
estimation of its region of attraction, are derived in terms of a calculable
upper bound of the control approximation error. Finally, the process of
implementation of the method on an orbital maneuver problem is investigated
through which the assumptions made in the theoretical developments are verified
and the sufficient conditions are applied for guaranteeing stability and near
optimality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6115</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6115</id><created>2014-12-18</created><authors><author><keyname>Gong</keyname><forenames>Yunchao</forenames></author><author><keyname>Liu</keyname><forenames>Liu</forenames></author><author><keyname>Yang</keyname><forenames>Ming</forenames></author><author><keyname>Bourdev</keyname><forenames>Lubomir</forenames></author></authors><title>Compressing Deep Convolutional Networks using Vector Quantization</title><categories>cs.CV cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep convolutional neural networks (CNN) has become the most promising method
for object recognition, repeatedly demonstrating record breaking results for
image classification and object detection in recent years. However, a very deep
CNN generally involves many layers with millions of parameters, making the
storage of the network model to be extremely large. This prohibits the usage of
deep CNNs on resource limited hardware, especially cell phones or other
embedded devices. In this paper, we tackle this model storage issue by
investigating information theoretical vector quantization methods for
compressing the parameters of CNNs. In particular, we have found in terms of
compressing the most storage demanding dense connected layers, vector
quantization methods have a clear gain over existing matrix factorization
methods. Simply applying k-means clustering to the weights or conducting
product quantization can lead to a very good balance between model size and
recognition accuracy. For the 1000-category classification task in the ImageNet
challenge, we are able to achieve 16-24 times compression of the network with
only 1% loss of classification accuracy using the state-of-the-art CNN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6118</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6118</id><created>2014-12-04</created><updated>2015-01-26</updated><authors><author><keyname>Pereira</keyname><forenames>A. Amorim</forenames><suffix>Jr</suffix></author><author><keyname>Sampaio-Neto</keyname><forenames>R.</forenames></author></authors><title>A Random-List Based LAS Algorithm for Near-Optimal Detection in
  Large-Scale Uplink Multiuser MIMO Systems</title><categories>cs.IT cs.NI math.IT</categories><comments>6 pages, 6 figures, 1 table. Corrected typos; Replaced references
  [17-18] concerning SIC and MB-SIC detection</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive Multiple-input Multiple-output (MIMO) systems offer exciting
opportunities due to their high spectral efficiencies capabilities. On the
other hand, one major issue in these scenarios is the high-complexity detectors
of such systems. In this work, we present a low-complexity, near
maximum-likelihood (ML) performance achieving detector for the uplink in large
MIMO systems with tens to hundreds of antennas at the base station (BS) and
similar number of uplink users. The proposed algorithm is derived from the
likelihood-ascent search (LAS) algorithm and it is shown to achieve near ML
performance as well as to possess excellent complexity attribute. The presented
algorithm, termed as random-list based LAS (RLB-LAS), employs several iterative
LAS search procedures whose starting-points are in a list generated by random
changes in the matched filter detected vector and chooses the best LAS result.
Also, a stop criterion was proposed in order to maintain the algorithm's
complexity at low levels. Near-ML performance detection is demonstrated by
means of Monte Carlo simulations and it is shown that this performance is
achieved with complexity of just O(K^2) per symbol, where K denotes the number
of single-antenna uplink users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6122</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6122</id><created>2014-12-02</created><authors><author><keyname>Kak</keyname><forenames>Subhash</forenames></author></authors><title>Spread Unary Coding</title><categories>cs.NE cs.IT math.IT</categories><comments>5 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unary coding is useful but it is redundant in its standard form. Unary coding
can also be seen as spatial coding where the value of the number is determined
by its place in an array. Motivated by biological finding that several neurons
in the vicinity represent the same number, we propose a variant of unary
numeration in its spatial form, where each number is represented by several 1s.
We call this spread unary coding where the number of 1s used is the spread of
the code. Spread unary coding is associated with saturation of the Hamming
distance between code words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6124</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6124</id><created>2014-12-18</created><authors><author><keyname>Wang</keyname><forenames>Jianyu</forenames></author><author><keyname>Yuille</keyname><forenames>Alan</forenames></author></authors><title>Semantic Part Segmentation using Compositional Model combining Shape and
  Appearance</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the problem of semantic part segmentation for
animals. This is more challenging than standard object detection, object
segmentation and pose estimation tasks because semantic parts of animals often
have similar appearance and highly varying shapes. To tackle these challenges,
we build a mixture of compositional models to represent the object boundary and
the boundaries of semantic parts. And we incorporate edge, appearance, and
semantic part cues into the compositional model. Given part-level segmentation
annotation, we develop a novel algorithm to learn a mixture of compositional
models under various poses and viewpoints for certain animal classes.
Furthermore, a linear complexity algorithm is offered for efficient inference
of the compositional model using dynamic programming. We evaluate our method
for horse and cow using a newly annotated dataset on Pascal VOC 2010 which has
pixelwise part labels. Experimental results demonstrate the effectiveness of
our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6125</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6125</id><created>2014-11-30</created><authors><author><keyname>Joneidi</keyname><forenames>Mohsen</forenames></author><author><keyname>Khalilsarai</keyname><forenames>Mahdi Barzegar</forenames></author><author><keyname>Zaeemzadeh</keyname><forenames>Alireza</forenames></author><author><keyname>Rahnavard</keyname><forenames>Nazanin</forenames></author></authors><title>Matrix Coherency Graph: A Tool for Improving Sparse Coding Performance</title><categories>cs.IT math.IT</categories><comments>5 pages, 8 figures, going to be submitted to SampTA 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exact recovery of a sparse solution for an underdetermined system of linear
equations implies full search among all possible subsets of the dictionary,
which is computationally intractable, while l1 minimization will do the job
when a Restricted Isometry Property holds for the dictionary. Yet, practical
sparse recovery algorithms may fail to recover the vector of coefficients even
when the dictionary deviates from the RIP only slightly. To enjoy l1
minimization guarantees in a wider sense, a method based on a combination of
full-search and l1 minimization is presented. The idea is based on partitioning
the dictionary into atoms which are in some sense well-conditioned and those
which are ill-conditioned. Inspired by that, a matrix coherency graph is
introduced which is a tool extracted by the structure of the dictionary. This
tool can be used for decreasing the greediness of sparse coding algorithms so
that recovery will be more reliable. We have modified the IRLS algorithm by
applying the proposed method on it and simulation results show that the
modified version performs quite better than the original algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6126</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6126</id><created>2014-11-28</created><authors><author><keyname>Nam</keyname><forenames>Sung Sik</forenames></author><author><keyname>Choi</keyname><forenames>Seyeong</forenames></author><author><keyname>Cho</keyname><forenames>Sung Ho</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Technical Report: Finger Replacement Schemes for RAKE Receivers in the
  Soft Handover Region with Multiple BSs over I.N.D. Fading Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new finger replacement technique which is applicable for RAKE receivers in
the soft handover (SHO) region has been proposed and studied in [1], [2] under
the ideal assumption that the fading is both independent and identically
distributed from path to path. To supplement our previous work, we present a
general comprehensive framework for the performance assessment of the proposed
finger replacement schemes operating over independent and non-identically
distributed (i.n.d.) faded paths. To accomplish this object, we derive new
closed-form expressions for the target key statistics which are composed of
i.n.d. exponential random variables. With these new expressions, the
performance analysis of various wireless communication systems over more
practical environments can be possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6127</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6127</id><created>2014-11-26</created><authors><author><keyname>Foukalas</keyname><forenames>Fotis</forenames></author><author><keyname>Khattab</keyname><forenames>Tamer</forenames></author></authors><title>Multi-User Diversity with Optimal Power Allocation in Spectrum Sharing
  under Average Interference Power Constraint</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the performance of multi-user diversity (MUD)
with optimal power allocation (OPA) in spectrum sharing (SS) under average
interference power (AIP) constraint. In particular, OPA through average
transmit power constraint in conjunction with the AIP constraint is assumed to
maximize the ergodic secondary capacity. The solution of this problem requires
the calculation of two Lagrange multipliers instead of one as obtained for the
peak interference power (PIP) constraint and calculated using the well known
water-filling algorithm. To this end, an algorithm based on bisection method is
devised in order to calculate both Lagrange multipliers iteratively. Moreover,
Rayleigh and Nakagami-$m$ fading channels with one and multiple primary users
are considered to derive the required end-to-end SNR analysis. Numerical
results are depicted to corroborate our performance analysis and compare it
with the PIP case highlighting hence, the impact of the AIP constraint compared
to the PIP constraint application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6128</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6128</id><created>2014-11-25</created><authors><author><keyname>Jiang</keyname><forenames>Jing</forenames></author><author><keyname>Cheng</keyname><forenames>Minquan</forenames></author><author><keyname>Miao</keyname><forenames>Ying</forenames></author></authors><title>Strongly Separable Codes</title><categories>cs.IT math.IT</categories><comments>11 pages, submitted to Designs, Codes and Cryptography. arXiv admin
  note: text overlap with arXiv:1411.6841</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Binary $t$-frameproof codes ($t$-FPCs) are used in multimedia fingerprinting
schemes where the identification of authorized users taking part in the
averaging collusion attack is required. In this paper, a binary strongly
$\bar{t}$-separable code ($\bar{t}$-SSC) is introduced to improve such a scheme
based on a binary $t$-FPC. A binary $\bar{t}$-SSC has the same traceability as
a binary $t$-FPC but has more codewords than a binary $t$-FPC. A composition
construction for binary $\bar{t}$-SSCs from $q$-ary $\bar{t}$-SSCs is
described, which stimulates the research on $q$-ary $\bar{t}$-SSCs with short
length. Several infinite series of optimal $q$-ary $\bar{2}$-SSCs of length $2$
are derived from the fact that a $q$-ary $\bar{2}$-SSC of length $2$ is
equivalent to a $q$-ary $\bar{2}$-separable code of length $2$. Combinatorial
properties of $q$-ary $\bar{2}$-SSCs of length $3$ are investigated, and a
construction for $q$-ary $\bar{2}$-SSCs of length $3$ is provided. These
$\bar{2}$-SSCs of length $3$ have more than $12.5\%$ codewords than $2$-FPCs of
length $3$ could have.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6129</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6129</id><created>2014-11-24</created><authors><author><keyname>Suksmono</keyname><forenames>Andriyan B.</forenames></author></authors><title>The Sample Allocation Problem and Non-Uniform Compressive Sampling</title><categories>cs.IT math.IT stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses sample allocation problem (SAP) in frequency-domain
Compressive Sampling (CS) of time-domain signals. An analysis that is relied on
two fundamental CS principles; the Uniform Random Sampling (URS) and the
Uncertainty Principle (UP), is presented. We show that CS on a single- and
multi-band signals performs better if the URS is done only within the band and
suppress the out-band parts, compared to ordinary URS that ignore the band
limits. It means that sampling should only be done at the signal support, while
the non-support should be masked and suppressed in the reconstruction process.
We also show that for an N-length discrete time signal with K-number of
frequency components (Fourier coefficients), given the knowledge of the
spectrum, URS leads to exact sampling on the location of the K-spectral peaks.
These results are used to formulate a sampling scheme when the boundaries of
the bands are not sharply distinguishable, such as in a triangular- or a
stacked-band- spectral signals. When analyzing these cases, CS will face a
paradox; in which narrowing the band leads to a more number of required
samples, whereas widening it leads to lessen the number. Accordingly; instead
of signal analysis by dividing the signal's spectrum vertically into bands of
frequencies, slicing horizontally magnitude-wise yields less number of required
sample and better reconstruction results. Moreover, it enables sample reuse
that reduces the sample number even further. The horizontal slicing and sample
reuse methods imply non-uniform random sampling, where larger-magnitude part of
the spectrum should be allocated more sample than the lower ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6130</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6130</id><created>2014-12-17</created><authors><author><keyname>Ge</keyname><forenames>Xiaohu</forenames></author><author><keyname>Huang</keyname><forenames>Xi</forenames></author><author><keyname>Wang</keyname><forenames>Yuming</forenames></author><author><keyname>Chen</keyname><forenames>Min</forenames></author><author><keyname>Li</keyname><forenames>Qiang</forenames></author><author><keyname>Han</keyname><forenames>Tao</forenames></author><author><keyname>Wang</keyname><forenames>Cheng-Xiang</forenames></author></authors><title>Energy Efficiency Optimization for MIMO-OFDM Mobile Multimedia
  Communication Systems with QoS Constraints</title><categories>cs.IT cs.MM cs.NI math.IT</categories><journal-ref>IEEE Transactions on Vehicular Technology, vol. 63, no. 5, pp.
  2127 - 2138, June 2014</journal-ref><doi>10.1109/TVT.2014.2310773</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  It is widely recognized that besides the quality of service (QoS), the energy
efficiency is also a key parameter in designing and evaluating mobile
multimedia communication systems, which has catalyzed great interest in recent
literature. In this paper, an energy efficiency model is first proposed for
multiple-input multiple-output orthogonal-frequency-division-multiplexing
(MIMO-OFDM) mobile multimedia communication systems with statistical QoS
constraints. Employing the channel matrix singular-value-decomposition (SVD)
method, all subchannels are classified by their channel characteristics.
Furthermore, the multi-channel joint optimization problem in conventional
MIMO-OFDM communication systems is transformed into a multi-target single
channel optimization problem by grouping all subchannels. Therefore, a
closed-form solution of the energy efficiency optimization is derived for
MIMO-OFDM mobile mlutimedia communication systems. As a consequence, an
energy-efficiency optimized power allocation (EEOPA) algorithm is proposed to
improve the energy efficiency of MIMO-OFDM mobile multimedia communication
systems. Simulation comparisons validate that the proposed EEOPA algorithm can
guarantee the required QoS with high energy efficiency in MIMO-OFDM mobile
multimedia communication systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6131</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6131</id><created>2014-11-12</created><authors><author><keyname>Song</keyname><forenames>Tianyu</forenames></author><author><keyname>Kam</keyname><forenames>Pooi-Yuen</forenames></author></authors><title>A Robust and Efficient Detection Algorithm for The Photon-Counting
  Free-Space Optical System</title><categories>cs.IT math.IT physics.optics</categories><comments>3 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a Viterbi-type trellis-search algorithm to implement the FSO
photon-counting sequence receiver proposed in [1] more efficiently and a
selective-store strategy to overcome the error floor problem observed therein.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6133</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6133</id><created>2014-11-10</created><authors><author><keyname>Lo</keyname><forenames>Yuan-Hsun</forenames></author><author><keyname>Wong</keyname><forenames>Wing Shing</forenames></author><author><keyname>Fu</keyname><forenames>Hung-Lin</forenames></author></authors><title>Partially user-irrepressible sequence sets and conflict-avoiding codes</title><categories>cs.IT math.CO math.IT</categories><comments>15 pages, 4 figures. To appear in Designs, Codes and Cryptography</comments><msc-class>94B25, 94C15, 05B10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we give a partial shift version of user-irrepressible sequence
sets and conflict-avoiding codes. By means of disjoint difference sets, we
obtain an infinite number of such user-irrepressible sequence sets whose
lengths are shorter than known results in general. Subsequently, the newly
defined partially conflict-avoiding codes are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6134</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6134</id><created>2014-12-18</created><updated>2015-07-21</updated><authors><author><keyname>Qiu</keyname><forenames>Qiang</forenames></author><author><keyname>Thompson</keyname><forenames>Andrew</forenames></author><author><keyname>Calderbank</keyname><forenames>Robert</forenames></author><author><keyname>Sapiro</keyname><forenames>Guillermo</forenames></author></authors><title>Data Representation using the Weyl Transform</title><categories>cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Weyl transform is introduced as a rich framework for data representation.
Transform coefficients are connected to the Walsh-Hadamard transform of
multiscale autocorrelations, and different forms of dyadic periodicity in a
signal are shown to appear as different features in its Weyl coefficients. The
Weyl transform has a high degree of symmetry with respect to a large group of
multiscale transformations, which allows compact yet discriminative
representations to be obtained by pooling coefficients. The effectiveness of
the Weyl transform is demonstrated through the example of textured image
classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6135</identifier>
 <datestamp>2015-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6135</id><created>2014-10-15</created><updated>2015-05-19</updated><authors><author><keyname>Noel</keyname><forenames>Adam</forenames></author><author><keyname>Cheung</keyname><forenames>Karen C.</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Multi-Scale Stochastic Simulation for Diffusive Molecular Communication</title><categories>cs.ET cs.IT math.IT physics.comp-ph</categories><comments>7 pages, 2 tables, 6 figures. Will be presented at the 2015 IEEE
  International Conference on Communications (ICC) in June 2015</comments><doi>10.1109/ICC.2015.7248471</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, hybrid models have emerged that combine microscopic and mesoscopic
regimes in a single stochastic reaction-diffusion simulation. Microscopic
simulations track every individual molecule and are generally more accurate.
Mesoscopic simulations partition the environment into subvolumes, track when
molecules move between adjacent subvolumes, and are generally more
computationally efficient. In this paper, we present the foundation of a
multi-scale stochastic simulator from the perspective of molecular
communication, for both mesoscopic and hybrid models, where we emphasize
simulation accuracy at the receiver and efficiency in regions that are far from
the communication link. Our multi-scale models use subvolumes of different
sizes, between which we derive the diffusion event transition rate. Simulation
results compare the accuracy and efficiency of traditional approaches with that
of a regular hybrid method and with those of our proposed multi-scale methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6137</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6137</id><created>2014-10-08</created><authors><author><keyname>Ali</keyname><forenames>Anum</forenames></author><author><keyname>Masood</keyname><forenames>Mudassir</forenames></author><author><keyname>Sohail</keyname><forenames>Muhammad S.</forenames></author><author><keyname>Al-Ghadhban</keyname><forenames>Samir</forenames></author><author><keyname>Al-Naffouri</keyname><forenames>Tareq Y.</forenames></author></authors><title>Narrowband Interference Mitigation in SC-FDMA Using Bayesian Sparse
  Recovery</title><categories>cs.IT math.IT stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel narrowband interference (NBI) mitigation scheme
for SC-FDMA systems. The proposed NBI cancellation scheme exploits the
frequency domain sparsity of the unknown signal and adopts a low complexity
Bayesian sparse recovery procedure. At the transmitter, a few randomly chosen
sub-carriers are kept data free to sense the NBI signal at the receiver.
Further, it is noted that in practice, the sparsity of the NBI signal is
destroyed by a grid mismatch between NBI sources and the system under
consideration. Towards this end, first an accurate grid mismatch model is
presented that is capable of assuming independent offsets for multiple NBI
sources. Secondly, prior to NBI reconstruction, the sparsity of the unknown
signal is restored by employing a sparsifying transform. To improve the
spectral efficiency of the proposed scheme, a data-aided NBI recovery procedure
is outlined that relies on adaptively selecting a subset of data carriers and
uses them as additional measurements to enhance the NBI estimation. Finally,
the proposed scheme is extended to single-input multi-output systems by
performing a collaborative NBI support search over all antennas. Numerical
results are presented that depict the suitability of the proposed scheme for
NBI mitigation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6140</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6140</id><created>2014-12-18</created><authors><author><keyname>Hu</keyname><forenames>Yuheng</forenames></author><author><keyname>Farnham</keyname><forenames>Shelly D.</forenames></author><author><keyname>Monroy-Hernandez</keyname><forenames>Andres</forenames></author></authors><title>Whoo.ly: Facilitating Information Seeking For Hyperlocal Communities
  Using Social Media</title><categories>cs.HC cs.CY cs.SI</categories><comments>ACM CHI Conference on Human Factors in Computing Systems 2013, best
  paper honorable mention, 10 pages, 4 figures</comments><acm-class>H.5.3</acm-class><doi>10.1145/2470654.2466478</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social media systems promise powerful opportunities for people to connect to
timely, relevant information at the hyper local level. Yet, finding the
meaningful signal in noisy social media streams can be quite daunting to users.
In this paper, we present and evaluate Whoo.ly, a web service that provides
neighborhood-specific information based on Twitter posts that were
automatically inferred to be hyperlocal. Whoo.ly automatically extracts and
summarizes hyperlocal information about events, topics, people, and places from
these Twitter posts. We provide an overview of our design goals with Whoo.ly
and describe the system including the user interface and our unique event
detection and summarization algorithms. We tested the usefulness of the system
as a tool for finding neighborhood information through a comprehensive user
study. The outcome demonstrated that most participants found Whoo.ly easier to
use than Twitter and they would prefer it as a tool for exploring their
neighborhoods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6141</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6141</id><created>2014-10-30</created><authors><author><keyname>Kim</keyname><forenames>Song-Ju</forenames></author><author><keyname>Aono</keyname><forenames>Masashi</forenames></author><author><keyname>Nameda</keyname><forenames>Etsushi</forenames></author></authors><title>Efficient Decision-Making by Volume-Conserving Physical Object</title><categories>cs.AI cs.LG nlin.AO physics.data-an</categories><comments>5 pages, 3 figures</comments><doi>10.1088/1367-2630/17/8/083023</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We demonstrate that any physical object, as long as its volume is conserved
when coupled with suitable operations, provides a sophisticated decision-making
capability. We consider the problem of finding, as accurately and quickly as
possible, the most profitable option from a set of options that gives
stochastic rewards. These decisions are made as dictated by a physical object,
which is moved in a manner similar to the fluctuations of a rigid body in a
tug-of-war game. Our analytical calculations validate statistical reasons why
our method exhibits higher efficiency than conventional algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6143</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6143</id><created>2014-11-17</created><authors><author><keyname>Rayachoti</keyname><forenames>Eswaraiah</forenames></author><author><keyname>Edara</keyname><forenames>Sreenivasa Reddy</forenames></author></authors><title>Block Based Medical Image Watermarking Technique for Tamper Detection
  and Recovery</title><categories>cs.MM cs.CR</categories><comments>10 pages, 6 figures</comments><acm-class>I.4</acm-class><journal-ref>ijcsi.org/papers/IJCSI-11-5-1-31-40.pdf 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel fragile block based medical image
watermarking technique for embedding data of patient into medical image,
verifying the integrity of ROI (Region of Interest), detecting the tampered
blocks inside ROI and recovering original ROI with less size authentication and
recovery data and with simple mathematical calculations. In the proposed
method, the medical image is divided into three regions called ROI, RONI
(Region of Non Interest) and border pixels. Later, authentication data of ROI
and Electronic Patient Record (EPR) are compressed using Run Length Encoding
(RLE) technique and then embedded into ROI. Recovery information of ROI is
embedded inside RONI and information of ROI is embedded inside border pixels.
Results of experiments conducted on several medical images reveal that proposed
method produces high quality watermarked medical images, identifies tampered
areas inside ROI of watermarked medical images and recovers the original ROI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6144</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6144</id><created>2014-12-14</created><authors><author><keyname>Kovach</keyname><forenames>Daniel</forenames></author></authors><title>The Computational Theory of Intelligence: Applications to Genetic
  Programming and Turing Machines</title><categories>cs.NE</categories><comments>Total of 5 figures. This paper was originally presented at RAMSA 2013
  in Visakhaptnam, India in December 2013</comments><msc-class>92DXX, 68TXX, 03Dxx</msc-class><acm-class>I.2.0; J.3; F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we continue the efforts of the Computational Theory of
Intelligence (CTI) by extending concepts to include computational processes in
terms of Genetic Algorithms (GA's) and Turing Machines (TM's). Active, Passive,
and Hybrid Computational Intelligence processes are also introduced and
discussed. We consider the ramifications of the assumptions of CTI with regard
to the qualities of reproduction and virility. Applications to Biology,
Computer Science and Cyber Security are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6145</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6145</id><created>2014-12-01</created><authors><author><keyname>Skanderova</keyname><forenames>Lenka</forenames></author><author><keyname>Fabian</keyname><forenames>Tomas</forenames></author></authors><title>Study of the Influence of the Number Normalization Scheme Used in Two
  Chaotic Pseudo Random Number Generators Used as the Source of Randomness in
  Differential Evolution</title><categories>cs.NE cs.CR</categories><comments>Pre-print for Impact Factor Journal Natural Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many publications, authors showed that chaotic pseudo random number
generators (PRNGs) may improve performance of the evolutionary algorithms. In
this paper, we use two chaotic maps Gingerbread man and Tinkerbell as the
chaotic PRNGs instead of the classical PRNG in the differential evolution.
Numbers generated by this maps are normalized to the unit interval by three
different methods -- operation modulo, straightforward number normalization
where we know minimal and maximal generated number and arctangent of the two
variables $x$ and $y$, where numbers $x$ and $y$ are generated by the
Gingerbread man map and Tinkerbell map. The first goal of this paper is to show
whether the differential evolution convergence speed might be affected by the
way how we normalize number generated by the chaotic map. The second goal is to
find out the influence of the probability distribution function of the selected
chaotic PRNGs. The results mentioned below showed that the selected
normalization method may improve differential evolution convergence speed,
especially in the case of arctangent and straightforward number normalization,
where we know the minimal and maximal generated numbers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6147</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6147</id><created>2014-12-17</created><authors><author><keyname>Kolokolnikov</keyname><forenames>Theodore</forenames></author></authors><title>Maximizing algebraic connectivity for certain families of graphs</title><categories>cs.DM math.CO math.SP</categories><comments>to appear, Journal of Linear Algebra and Applications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the bounds on algebraic connectivity of graphs subject to
constraints on the number of edges, vertices, and topology. We show that the
algebraic connectivity for any tree on $n$ vertices and with maximum degree $d$
is bounded above by $2(d-2) \frac{1}{n}+O(\frac{\ln n}{n^{2}}) .$ We then
investigate upper bounds on algebraic connectivity for cubic graphs. We show
that algebraic connectivity of a cubic graph of girth $g$ is bounded above by
$3-2^{3/2}\cos(\pi/\lfloor g/2\rfloor) ,$ which is an improvement over the
bound found by Nilli [A. Nilli, Electron. J. Combin., 11(9), 2004]. Finally, we
propose several conjectures and open questions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6148</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6148</id><created>2014-12-03</created><authors><author><keyname>Kallitsis</keyname><forenames>Michael</forenames></author><author><keyname>Stoev</keyname><forenames>Stilian</forenames></author><author><keyname>Michailidis</keyname><forenames>George</forenames></author></authors><title>Hashing Pursuit for Online Identification of Heavy-Hitters in High-Speed
  Network Streams</title><categories>cs.CR cs.DS cs.NI</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed Denial of Service (DDoS) attacks have become more prominent
recently, both in frequency of occurrence, as well as magnitude. Such attacks
render key Internet resources unavailable and disrupt its normal operation. It
is therefore of paramount importance to quickly identify malicious Internet
activity. The DDoS threat model includes characteristics such as: (i)
heavy-hitters that transmit large volumes of traffic towards &quot;victims&quot;, (ii)
persistent-hitters that send traffic, not necessarily large, to specific
destinations to be used as attack facilitators, (iii) host and port scanning
for compiling lists of un-secure servers to be used as attack amplifiers, etc.
This conglomeration of problems motivates the development of space/time
efficient summaries of data traffic streams that can be used to identify
heavy-hitters associated with the above attack vectors. This paper presents a
hashing-based framework and fast algorithms that take into account the
large-dimensionality of the incoming network stream and can be employed to
quickly identify the culprits. The algorithms and data structures proposed
provide a synopsis of the network stream that is not taxing to fast-memory, and
can be efficiently implemented in hardware due to simple bit-wise operations.
The methods are evaluated using real-world Internet data from a large academic
network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6149</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6149</id><created>2014-11-25</created><authors><author><keyname>Hammoudi</keyname><forenames>Karim</forenames></author><author><keyname>Ajam</keyname><forenames>Nabil</forenames></author><author><keyname>Kasraoui</keyname><forenames>Mohamed</forenames></author><author><keyname>Dornaika</keyname><forenames>Fadi</forenames></author><author><keyname>Radhakrishnan</keyname><forenames>Karan</forenames></author><author><keyname>Bandi</keyname><forenames>Karthik</forenames></author><author><keyname>Cai</keyname><forenames>Qing</forenames></author><author><keyname>Liu</keyname><forenames>Sai</forenames></author></authors><title>Design, Implementation and Simulation of a Cloud Computing System for
  Enhancing Real-time Video Services by using VANET and Onboard Navigation
  Systems</title><categories>cs.NI cs.CV</categories><comments>paper accepted for publication in the proceedings of the &quot;17\`eme
  Colloque Compression et Repr\'esentation des Signaux Audiovisuels&quot; (CORESA),
  5p., Reims, France, 2014. (preprint)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a design for novel and experimental cloud computing
systems. The proposed system aims at enhancing computational, communicational
and annalistic capabilities of road navigation services by merging several
independent technologies, namely vision-based embedded navigation systems,
prominent Cloud Computing Systems (CCSs) and Vehicular Ad-hoc NETwork (VANET).
This work presents our initial investigations by describing the design of a
global generic system. The designed system has been experimented with various
scenarios of video-based road services. Moreover, the associated architecture
has been implemented on a small-scale simulator of an in-vehicle embedded
system. The implemented architecture has been experimented in the case of a
simulated road service to aid the police agency. The goal of this service is to
recognize and track searched individuals and vehicles in a real-time monitoring
system remotely connected to moving cars. The presented work demonstrates the
potential of our system for efficiently enhancing and diversifying real-time
video services in road environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6150</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6150</id><created>2014-10-13</created><authors><author><keyname>Dua</keyname><forenames>Deepika</forenames></author><author><keyname>Mishra</keyname><forenames>Atul</forenames></author></authors><title>Selective Watchdog Technique for Intrusion Detection in Mobile Ad-Hoc
  Network</title><categories>cs.NI cs.CR</categories><comments>12 pages,8 figures</comments><journal-ref>International Journal on Applications of Graph Theory in Wireless
  Ad hoc Networks and Sensor Networks (GRAPH-HOC) Vol.6, No.3, September 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile ad-hoc networks(MANET) is the collection of mobile nodes which are
self organizing and are connected by wireless links where nodes which are not
in the direct range communicate with each other relying on the intermediate
nodes. As a result of trusting other nodes in the route, a malicious node can
easily compromise the security of the network. A black-hole node is the
malicious node which drops the entire packet coming to it and always shows the
fresh route to the destination, even if the route to destination doesn't exist.
This paper describes a scheme that will detect the intrusion in the network in
the presence of black-hole node and its performance is compared with the
previous technique. This novel technique helps to increase the network
performance by reducing the overhead in the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6151</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6151</id><created>2014-11-11</created><authors><author><keyname>Le&#xe3;o</keyname><forenames>Lucas</forenames></author><author><keyname>Bianchini</keyname><forenames>David</forenames></author><author><keyname>Branquinho</keyname><forenames>Omar</forenames></author></authors><title>FLBRA: Fuzzy Logic Based Routing Algorithm for Indoor Wireless Sensor
  Networks</title><categories>cs.NI</categories><comments>12 pages</comments><journal-ref>International Journal of Computer Science &amp; Information Technology
  (IJCSIT) Vol 6, No 5, October 2014</journal-ref><doi>10.5121/ijcsit.2014.6507</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Considering the context of building management systems with wireless sensor
networks monitoring environmental features, this paper presents a proposal of a
Fuzzy Logic Based Routing Algorithm (FLBRA) to determine the cost of each link
and the identification of the best routes for packet forwarding. We describe
the parameters (Received Signal Strength Indicator - RSSI, Standard Deviation
of the RSSI and Packet Error Rate - PER) for the cost definition of each path,
the sequence of identifying best routes and the results obtained in simulation.
As expected in this proposal, the simulation results showed an increase in the
packet delivery rate compared to RSSI-based forward protocol (RBF).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6153</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6153</id><created>2014-09-10</created><authors><author><keyname>Krishnan</keyname><forenames>Arjun B.</forenames></author><author><keyname>Kollipara</keyname><forenames>Jayaram</forenames></author></authors><title>Intelligent Indoor Mobile Robot Navigation Using Stereo Vision</title><categories>cs.RO cs.AI cs.CV</categories><comments>9 pages, SIPIJ August 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Majority of the existing robot navigation systems, which facilitate the use
of laser range finders, sonar sensors or artificial landmarks, has the ability
to locate itself in an unknown environment and then build a map of the
corresponding environment. Stereo vision, while still being a rapidly
developing technique in the field of autonomous mobile robots, are currently
less preferable due to its high implementation cost. This paper aims at
describing an experimental approach for the building of a stereo vision system
that helps the robots to avoid obstacles and navigate through indoor
environments and at the same time remaining very much cost effective. This
paper discusses the fusion techniques of stereo vision and ultrasound sensors
which helps in the successful navigation through different types of complex
environments. The data from the sensor enables the robot to create the two
dimensional topological map of unknown environments and stereo vision systems
models the three dimension model of the same environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6154</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6154</id><created>2014-10-06</created><authors><author><keyname>Romero</keyname><forenames>Ana</forenames></author><author><keyname>Rubio</keyname><forenames>Julio</forenames></author><author><keyname>Sergeraert</keyname><forenames>Francis</forenames></author></authors><title>Effective persistent homology of digital images</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, three Computational Topology methods (namely effective
homology, persistent homology and discrete vector fields) are mixed together to
produce algorithms for homological digital image processing. The algorithms
have been implemented as extensions of the Kenzo system and have shown a good
performance when applied on some actual images extracted from a public dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6155</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6155</id><created>2014-12-16</created><updated>2016-01-19</updated><authors><author><keyname>Meghanathan</keyname><forenames>Natarajan</forenames></author></authors><title>Use of Eigenvalue and Eigenvectors to Analyze Bipartivity of Network
  Graphs</title><categories>cs.SI physics.soc-ph</categories><comments>A paper with a similar idea has been published earlier. I was not
  aware of this before. Now that, I have come to know about this, I am
  withdrawing my paper from arXiv</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the applications of Eigenvalues and Eigenvectors (as part
of spectral decomposition) to analyze the bipartivity index of graphs as well
as to predict the set of vertices that will constitute the two partitions of
graphs that are truly bipartite and those that are close to being bipartite.
Though the largest eigenvalue and the corresponding eigenvector (called the
principal eigenvalue and principal eigenvector) are typically used in the
spectral analysis of network graphs, we show that the smallest eigenvalue and
the smallest eigenvector (called the bipartite eigenvalue and the bipartite
eigenvector) could be used to predict the bipartite partitions of network
graphs. For each of the predictions, we hypothesize an expected partition for
the input graph and compare that with the predicted partitions. We also analyze
the impact of the number of frustrated edges (edges connecting the vertices
within a partition) and their location across the two partitions on the
bipartivity index. We observe that for a given number of frustrated edges, if
the frustrated edges are located in the larger of the two partitions of the
bipartite graph (rather than the smaller of the two partitions or equally
distributed across the two partitions), the bipartivity index is likely to be
relatively larger.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6156</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6156</id><created>2014-11-24</created><updated>2016-01-05</updated><authors><author><keyname>Hajek</keyname><forenames>Bruce</forenames></author><author><keyname>Wu</keyname><forenames>Yihong</forenames></author><author><keyname>Xu</keyname><forenames>Jiaming</forenames></author></authors><title>Achieving Exact Cluster Recovery Threshold via Semidefinite Programming</title><categories>stat.ML cs.DS math.PR</categories><comments>This paper was accepted to IEEE Transactions on Information Theory on
  January 3, 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The binary symmetric stochastic block model deals with a random graph of $n$
vertices partitioned into two equal-sized clusters, such that each pair of
vertices is connected independently with probability $p$ within clusters and
$q$ across clusters. In the asymptotic regime of $p=a \log n/n$ and $q=b \log
n/n$ for fixed $a,b$ and $n \to \infty$, we show that the semidefinite
programming relaxation of the maximum likelihood estimator achieves the optimal
threshold for exactly recovering the partition from the graph with probability
tending to one, resolving a conjecture of Abbe et al. \cite{Abbe14}.
Furthermore, we show that the semidefinite programming relaxation also achieves
the optimal recovery threshold in the planted dense subgraph model containing a
single cluster of size proportional to $n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6157</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6157</id><created>2014-12-12</created><updated>2015-11-13</updated><authors><author><keyname>Bonaccorsi</keyname><forenames>Stefano</forenames></author><author><keyname>Ottaviano</keyname><forenames>Stefania</forenames></author><author><keyname>Mugnolo</keyname><forenames>Delio</forenames></author><author><keyname>De Pellegrini</keyname><forenames>Francesco</forenames></author></authors><title>Epidemic Outbreaks in Networks with Equitable or Almost-Equitable
  Partitions</title><categories>cs.SI physics.soc-ph</categories><comments>21 pages, 9 figures</comments><journal-ref>SIAM J. Appl. Math., 75(6), 2421-2443, 2015</journal-ref><doi>10.1137/140995829</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the diffusion of epidemics on networks that are partitioned into
local communities. The gross structure of hierarchical networks of this kind
can be described by a quotient graph. The rationale of this approach is that
individuals infect those belonging to the same community with higher
probability than individuals in other communities. In community models the
nodal infection probability is thus expected to depend mainly on the
interaction of a few, large interconnected clusters. In this work, we describe
the epidemic process as a continuous-time individual-based
susceptible-infected-susceptible (SIS) model using a first-order mean-field
approximation. A key feature of our model is that the spectral radius of this
smaller quotient graph (which only captures the macroscopic structure of the
community network) is all we need to know in order to decide whether the
overall healthy-state defines a globally asymptotically stable or an unstable
equilibrium. Indeed, the spectral radius is related to the epidemic threshold
of the system. Moreover we prove that, above the threshold, another
steady-state exists that can be computed using a lower-dimensional dynamical
system associated with the evolution of the process on the quotient graph. Our
investigations are based on the graph-theoretical notion of equitable partition
and of its recent and rather flexible generalization, that of almost equitable
partition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6159</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6159</id><created>2014-10-06</created><updated>2015-01-06</updated><authors><author><keyname>Mortier</keyname><forenames>Richard</forenames></author><author><keyname>Haddadi</keyname><forenames>Hamed</forenames></author><author><keyname>Henderson</keyname><forenames>Tristan</forenames></author><author><keyname>McAuley</keyname><forenames>Derek</forenames></author><author><keyname>Crowcroft</keyname><forenames>Jon</forenames></author></authors><title>Human-Data Interaction: The Human Face of the Data-Driven Society</title><categories>cs.CY cs.HC cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing generation and collection of personal data has created a
complex ecosystem, often collaborative but sometimes combative, around
companies and individuals engaging in the use of these data. We propose that
the interactions between these agents warrants a new topic of study: Human-Data
Interaction (HDI). In this paper we discuss how HDI sits at the intersection of
various disciplines, including computer science, statistics, sociology,
psychology and behavioural economics. We expose the challenges that HDI raises,
organised into three core themes of legibility, agency and negotiability, and
we present the HDI agenda to open up a dialogue amongst interested parties in
the personal and big data ecosystems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6160</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6160</id><created>2014-12-15</created><authors><author><keyname>You</keyname><forenames>Seungil</forenames></author><author><keyname>Gattami</keyname><forenames>Ather</forenames></author></authors><title>H infinity Analysis Revisited</title><categories>math.OC cs.SY</categories><comments>Submitted to IEEE Transactions on Automatic Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a direct, and simple approach to the H infinity norm
calculation in more general settings. In contrast to the method based on the
Kalman-Yakubovich-Popov lemma, our approach does not require a controllability
assumption, and returns a sinusoidal input that achieves the H infinity norm of
the system including its frequency. In addition, using a semidefinite
programming duality, we present a new proof of the Kalman- Yakubovich-Popov
lemma, and make a connection between strong duality and controllability.
Finally, we generalize our approach towards the generalized
Kalman-Yakubovich-Popov lemma, which considers input signals within a finite
spectrum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6161</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6161</id><created>2014-11-14</created><authors><author><keyname>&#x130;lhan</keyname><forenames>Ferruh</forenames></author><author><keyname>Karabacak</keyname><forenames>&#xd6;zkan</forenames></author></authors><title>Graph-Based Minimum Dwell Time and Average Dwell Time Computations for
  Discrete-Time Switched Linear Systems</title><categories>cs.SY cs.PF nlin.AO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discrete-time switched linear systems where switchings are governed by a
digraph are considered. The minimum (or average) dwell time that guarantees the
asymptotic stability can be computed by calculating the maximum cycle ratio (or
maximum cycle mean) of a doubly weighted digraph where weights depend on the
eigenvalues and eigenvectors of subsystem matrices. The graph-based method is
applied to systems with defective subsystem matrices using Jordan
decomposition. In the case of bimodal switched systems scaling algorithms that
minimizes the condition number can be used to give a better minimum (or
average) dwell time estimates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6162</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6162</id><created>2014-11-07</created><authors><author><keyname>Fatemi</keyname><forenames>Mehdi</forenames></author><author><keyname>Setoodeh</keyname><forenames>Peyman</forenames></author><author><keyname>Haykin</keyname><forenames>Simon</forenames></author></authors><title>Improving Observability of Stochastic Complex Networks under the
  Supervision of Cognitive Dynamic Systems</title><categories>cs.SY math.OC</categories><comments>Submitted to IEEE Trans. Network Science and Engineering on October
  25, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Much has been said about observability in system theory and control; however,
it has been recently that observability in complex networks has seriously
attracted the attention of researchers. This paper examines the
state-of-the-art and discusses some issues raised due to &quot;complexity&quot; and
&quot;stochasticity&quot;. These unresolved issues call for a new practical methodology.
For stochastic systems, a degree of observability may be defined and the
observability problem is not a binary (i.e., yes-no) question anymore. Here, we
propose to employ a goal-seeking system to play a supervisory role in the
network. Hence, improving the degree of observability would be a valid
objective for the supervisory system. Towards this goal, the supervisor
dynamically optimizes the observation process by reconfiguring the sensory
parts in the network. A cognitive dynamic system is suggested as a proper
choice for the supervisory system. In this framework, the network itself is
viewed as the environment with which the cognitive dynamic system interacts.
Computer experiments confirm the potential of the proposed approach for
addressing some of the issues raised in networks due to complexity and
stochasticity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6163</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6163</id><created>2014-12-18</created><authors><author><keyname>Poddar</keyname><forenames>Piyush</forenames></author><author><keyname>Ahmidi</keyname><forenames>Narges</forenames></author><author><keyname>Vedula</keyname><forenames>S. Swaroop</forenames></author><author><keyname>Ishii</keyname><forenames>Lisa</forenames></author><author><keyname>Hager</keyname><forenames>Gregory D.</forenames></author><author><keyname>Ishii</keyname><forenames>Masaru</forenames></author></authors><title>Automated Objective Surgical Skill Assessment in the Operating Room
  Using Unstructured Tool Motion</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Previous work on surgical skill assessment using intraoperative tool motion
in the operating room (OR) has focused on highly-structured surgical tasks such
as cholecystectomy. Further, these methods only considered generic motion
metrics such as time and number of movements, which are of limited instructive
value. In this paper, we developed and evaluated an automated approach to the
surgical skill assessment of nasal septoplasty in the OR. The obstructed field
of view and highly unstructured nature of septoplasty precludes trainees from
efficiently learning the procedure. We propose a descriptive structure of
septoplasty consisting of two types of activity: (1) brushing activity directed
away from the septum plane characterizing the consistency of the surgeon's
wrist motion and (2) activity along the septal plane characterizing the
surgeon's coverage pattern. We derived features related to these two activity
types that classify a surgeon's level of training with an average accuracy of
about 72%. The features we developed provide surgeons with personalized,
actionable feedback regarding their tool motion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6164</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6164</id><created>2014-10-11</created><updated>2015-05-29</updated><authors><author><keyname>Sarkar</keyname><forenames>Soumic</forenames></author><author><keyname>Kar</keyname><forenames>Indra Narayan</forenames></author></authors><title>Formation of Multiple Groups of Mobile Robots Using Sliding Mode Control</title><categories>cs.SY cs.RO</categories><comments>8 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Formation control of multiple groups of agents finds application in large
area navigation by generating different geometric patterns and shapes, and also
in carrying large objects. In this paper, Centroid Based Transformation (CBT)
\cite{c39}, has been applied to decompose the combined dynamics of wheeled
mobile robots (WMRs) into three subsystems: intra and inter group shape
dynamics, and the dynamics of the centroid. Separate controllers have been
designed for each subsystem. The gains of the controllers are such chosen that
the overall system becomes singularly perturbed system. Then sliding mode
controllers are designed on the singularly perturbed system to drive the
subsystems on sliding surfaces in finite time. Negative gradient of a potential
based function has been added to the sliding surface to ensure collision
avoidance among the robots in finite time. The efficacy of the proposed
controller is established through simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6168</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6168</id><created>2014-12-18</created><authors><author><keyname>Bonifas</keyname><forenames>Nicolas</forenames></author><author><keyname>Dadush</keyname><forenames>Daniel</forenames></author></authors><title>Short Paths on the Voronoi Graph and the Closest Vector Problem with
  Preprocessing</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Improving on the Voronoi cell based techniques of Micciancio and Voulgaris
(SIAM J. Comp. 13), and Sommer, Feder and Shalvi (SIAM J. Disc. Math. 09), we
give a Las Vegas $\tilde{O}(2^n)$ expected time and space algorithm for CVPP
(the preprocessing version of the Closest Vector Problem, CVP). This improves
on the $\tilde{O}(4^n)$ deterministic runtime of the Micciancio Voulgaris
algorithm, henceforth MV, for CVPP (which also solves CVP) at the cost of a
polynomial amount of randomness (which only affects runtime, not correctness).
As in MV, our algorithm proceeds by computing a short path on the Voronoi graph
of the lattice, where lattice points are adjacent if their Voronoi cells share
a common facet, from the origin to a closest lattice vector.
  Our main technical contribution is a randomized procedure that given the
Voronoi relevant vectors of a lattice - the lattice vectors inducing facets of
the Voronoi cell - as preprocessing and any &quot;close enough&quot; lattice point to the
target, computes a path to a closest lattice vector of expected polynomial
size. This improves on the $\tilde{O}(4^n)$ path length given by the MV
algorithm. Furthermore, as in MV, each edge of the path can be computed using a
single iteration over the Voronoi relevant vectors. As a byproduct of our work,
we also give an optimal relationship between geometric and path distance on the
Voronoi graph, which we believe to be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6170</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6170</id><created>2014-12-18</created><authors><author><keyname>Lettich</keyname><forenames>Francesco</forenames></author><author><keyname>Orlando</keyname><forenames>Salvatore</forenames></author><author><keyname>Silvestri</keyname><forenames>Claudio</forenames></author></authors><title>Manycore processing of repeated k-NN queries over massive moving objects
  observations</title><categories>cs.DC cs.DB cs.DS</categories><acm-class>D.1.3; C.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to timely process significant amounts of continuously updated
spatial data is mandatory for an increasing number of applications. In this
paper we focus on a specific data-intensive problem concerning the repeated
processing of huge amounts of k nearest neighbours (k-NN) queries over massive
sets of moving objects, where the spatial extents of queries and the position
of objects are continuously modified over time. In particular, we propose a
novel hybrid CPU/GPU pipeline that significantly accelerate query processing
thanks to a combination of ad-hoc data structures and non-trivial memory access
patterns. To the best of our knowledge this is the first work that exploits
GPUs to efficiently solve repeated k-NN queries over massive sets of
continuously moving objects, even characterized by highly skewed spatial
distributions. In comparison with state-of-the-art sequential CPU-based
implementations, our method highlights significant speedups in the order of
10x-20x, depending on the datasets, even when considering cheap GPUs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6177</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6177</id><created>2014-12-18</created><updated>2015-03-31</updated><authors><author><keyname>Tsuchida</keyname><forenames>Tomoki</forenames></author><author><keyname>Cottrell</keyname><forenames>Garrison W.</forenames></author></authors><title>Example Selection For Dictionary Learning</title><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In unsupervised learning, an unbiased uniform sampling strategy is typically
used, in order that the learned features faithfully encode the statistical
structure of the training data. In this work, we explore whether active example
selection strategies - algorithms that select which examples to use, based on
the current estimate of the features - can accelerate learning. Specifically,
we investigate effects of heuristic and saliency-inspired selection algorithms
on the dictionary learning task with sparse activations. We show that some
selection algorithms do improve the speed of learning, and we speculate on why
they might work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6181</identifier>
 <datestamp>2014-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6181</id><created>2014-12-18</created><updated>2014-12-24</updated><authors><author><keyname>Xie</keyname><forenames>Pengtao</forenames></author><author><keyname>Bilenko</keyname><forenames>Misha</forenames></author><author><keyname>Finley</keyname><forenames>Tom</forenames></author><author><keyname>Gilad-Bachrach</keyname><forenames>Ran</forenames></author><author><keyname>Lauter</keyname><forenames>Kristin</forenames></author><author><keyname>Naehrig</keyname><forenames>Michael</forenames></author></authors><title>Crypto-Nets: Neural Networks over Encrypted Data</title><categories>cs.LG cs.CR cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem we address is the following: how can a user employ a predictive
model that is held by a third party, without compromising private information.
For example, a hospital may wish to use a cloud service to predict the
readmission risk of a patient. However, due to regulations, the patient's
medical files cannot be revealed. The goal is to make an inference using the
model, without jeopardizing the accuracy of the prediction or the privacy of
the data.
  To achieve high accuracy, we use neural networks, which have been shown to
outperform other learning models for many tasks. To achieve the privacy
requirements, we use homomorphic encryption in the following protocol: the data
owner encrypts the data and sends the ciphertexts to the third party to obtain
a prediction from a trained model. The model operates on these ciphertexts and
sends back the encrypted prediction. In this protocol, not only the data
remains private, even the values predicted are available only to the data
owner.
  Using homomorphic encryption and modifications to the activation functions
and training algorithms of neural networks, we show that it is protocol is
possible and may be feasible. This method paves the way to build a secure
cloud-based neural network prediction services without invading users' privacy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6201</identifier>
 <datestamp>2014-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6201</id><created>2014-12-18</created><authors><author><keyname>Kant&#xe9;</keyname><forenames>Mamadou Moustapha</forenames></author><author><keyname>Kwon</keyname><forenames>O-joung</forenames></author></authors><title>An Upper Bound on the Size of Obstructions for Bounded Linear Rank-Width</title><categories>math.CO cs.DM</categories><comments>28 pages, 1 figure</comments><msc-class>05C75</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a doubly exponential upper bound in $p$ on the size of forbidden
pivot-minors for symmetric or skew-symmetric matrices over a fixed finite field
$\mathbb{F}$ of linear rank-width at most $p$. As a corollary, we obtain a
doubly exponential upper bound in $p$ on the size of forbidden vertex-minors
for graphs of linear rank-width at most $p$. This solves an open question
raised by Jeong, Kwon, and Oum [Excluded vertex-minors for graphs of linear
rank-width at most $k$. European J. Combin., 41:242--257, 2014]. We also give a
doubly exponential upper bound in $p$ on the size of forbidden minors for
matroids representable over a fixed finite field of path-width at most $p$.
  Our basic tool is the pseudo-minor order used by Lagergren [Upper Bounds on
the Size of Obstructions and Interwines, Journal of Combinatorial Theory Series
B, 73:7--40, 1998] to bound the size of forbidden graph minors for bounded
path-width. To adapt this notion into linear rank-width, it is necessary to
well define partial pieces of graphs and merging operations that fit to
pivot-minors. Using the algebraic operations introduced by Courcelle and
Kant\'e, and then extended to (skew-)symmetric matrices by Kant\'e and Rao, we
define boundaried $s$-labelled graphs and prove similar structure theorems for
pivot-minor and linear rank-width.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6211</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6211</id><created>2014-12-18</created><authors><author><keyname>Hu</keyname><forenames>Xianfeng</forenames></author><author><keyname>Wang</keyname><forenames>Yang</forenames></author><author><keyname>Wu</keyname><forenames>Qiang</forenames></author></authors><title>Multiple Authors Detection: A Quantitative Analysis of Dream of the Red
  Chamber</title><categories>cs.LG cs.CL</categories><journal-ref>Advances in Adaptive Data Analysis, Article ID 1450012 (18 pages),
  2014</journal-ref><doi>10.1142/S1793536914500125</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by the authorship controversy of Dream of the Red Chamber and the
application of machine learning in the study of literary stylometry, we develop
a rigorous new method for the mathematical analysis of authorship by testing
for a so-called chrono-divide in writing styles. Our method incorporates some
of the latest advances in the study of authorship attribution, particularly
techniques from support vector machines. By introducing the notion of relative
frequency as a feature ranking metric our method proves to be highly effective
and robust.
  Applying our method to the Cheng-Gao version of Dream of the Red Chamber has
led to convincing if not irrefutable evidence that the first $80$ chapters and
the last $40$ chapters of the book were written by two different authors.
Furthermore, our analysis has unexpectedly provided strong support to the
hypothesis that Chapter 67 was not the work of Cao Xueqin either.
  We have also tested our method to the other three Great Classical Novels in
Chinese. As expected no chrono-divides have been found. This provides further
evidence of the robustness of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6216</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6216</id><created>2014-12-18</created><authors><author><keyname>Garg</keyname><forenames>Ankita</forenames></author></authors><title>An approach for improving the concept of Cyclomatic Complexity for
  Object-Oriented Programming</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Measuring software complexity plays an important role to meet the demands of
complex software. The cyclomatic complexity is one of most used and renowned
metric among the other three proposed and researched metrics that are namely:
Line of code, Halstead measure and cyclomatic complexity. Although cyclomatic
complexity is very popular but also serves some of the problems which has been
identified and showed in a tabular form in this research work. It lacks the
calculation of coupling between the object classes for object oriented
programming and only calculates the complexity on the basis of the conditional
statements. Thus there is requirement to improve the concept of cyclomatic
complexity based on coupling. Paper includes the proposed algorithm to handle
the above stated problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6223</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6223</id><created>2014-12-19</created><authors><author><keyname>Takeuchi</keyname><forenames>Keigo</forenames></author></authors><title>Iterative LMMSE Channel Estimation, Multiuser Detection, and Decoding
  via Spatial Coupling</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Trans. Inf. Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spatial coupling is utilized to improve the performance of iterative channel
estimation, multiuser detection, and decoding for multiple-input multiple-input
(MIMO) bit-interleaved coded modulation (BICM). Coupling is applied to both
coding and BICM---the encoder uses a protograph-based spatially-coupled
low-density parity-check (SC LDPC) code. Spatially and temporally coupled (STC)
BICM is proposed to enable iterative channel estimation via coupling. Linear
minimum mean-squared error (LMMSE) estimation is applied for both channel
estimation and detection to reduce the complexity. Tractable density evolution
(DE) equations are derived to analyze the convergence property of iterative
receivers in the large-system limit, via a tool developed in statistical
physics---replica method. The DE analysis implies that the STC BICM can improve
the performance of iterative channel estimation especially for higher-order
modulation. Numerical simulations show that the STC BICM can provide a
significant gain of the performance at high signal-to-noise ratios for 64
quadrature amplitude modulation (QAM), as well as an improvement in the
decoding threshold, compared to conventional BICM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6249</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6249</id><created>2014-12-19</created><updated>2015-04-16</updated><authors><author><keyname>Lin</keyname><forenames>Min</forenames></author><author><keyname>Li</keyname><forenames>Shuo</forenames></author><author><keyname>Luo</keyname><forenames>Xuan</forenames></author><author><keyname>Yan</keyname><forenames>Shuicheng</forenames></author></authors><title>Purine: A bi-graph based deep learning framework</title><categories>cs.NE cs.LG</categories><comments>Submitted to ICLR 2015 workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a novel deep learning framework, termed Purine.
In Purine, a deep network is expressed as a bipartite graph (bi-graph), which
is composed of interconnected operators and data tensors. With the bi-graph
abstraction, networks are easily solvable with event-driven task dispatcher. We
then demonstrate that different parallelism schemes over GPUs and/or CPUs on
single or multiple PCs can be universally implemented by graph composition.
This eases researchers from coding for various parallelization schemes, and the
same dispatcher can be used for solving variant graphs. Scheduled by the task
dispatcher, memory transfers are fully overlapped with other computations,
which greatly reduce the communication overhead and help us achieve approximate
linear acceleration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6254</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6254</id><created>2014-12-19</created><authors><author><keyname>Bendory</keyname><forenames>Tamir</forenames></author><author><keyname>Dekel</keyname><forenames>Shai</forenames></author><author><keyname>Feuer</keyname><forenames>Arie</forenames></author></authors><title>Exact recovery of non-uniform splines from the projection onto spaces of
  algebraic polynomials</title><categories>cs.IT math.IT math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we consider the problem of recovering non-uniform splines from
their projection onto spaces of algebraic polynomials. We show that under a
certain Chebyshev-type separation condition on its knots, a spline whose
inner-products with a polynomial basis and boundary conditions are known, can
be recovered using Total Variation norm minimization. The proof of the
uniqueness of the solution uses the method of `dual' interpolating polynomials
and is based on \cite{SR}, where the theory was developed for trigonometric
polynomials. We also show results for the multivariate case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6257</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6257</id><created>2014-12-19</created><authors><author><keyname>Kalmanovich</keyname><forenames>Alexander</forenames></author><author><keyname>Chechik</keyname><forenames>Gal</forenames></author></authors><title>Gradual training of deep denoising auto encoders</title><categories>cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stacked denoising auto encoders (DAEs) are well known to learn useful deep
representations, which can be used to improve supervised training by
initializing a deep network. We investigate a training scheme of a deep DAE,
where DAE layers are gradually added and keep adapting as additional layers are
added. We show that in the regime of mid-sized datasets, this gradual training
provides a small but consistent improvement over stacked training in both
reconstruction quality and classification error over stacked training on MNIST
and CIFAR datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6264</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6264</id><created>2014-12-19</created><authors><author><keyname>K</keyname><forenames>Taraka Rama</forenames></author></authors><title>Supertagging: Introduction, learning, and application</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Supertagging is an approach originally developed by Bangalore and Joshi
(1999) to improve the parsing efficiency. In the beginning, the scholars used
small training datasets and somewhat na\&quot;ive smoothing techniques to learn the
probability distributions of supertags. Since its inception, the applicability
of Supertags has been explored for TAG (tree-adjoining grammar) formalism as
well as other related yet, different formalisms such as CCG. This article will
try to summarize the various chapters, relevant to statistical parsing, from
the most recent edited book volume (Bangalore and Joshi, 2010). The chapters
were selected so as to blend the learning of supertags, its integration into
full-scale parsing, and in semantic parsing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6265</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6265</id><created>2014-12-19</created><updated>2015-04-04</updated><authors><author><keyname>Daniely</keyname><forenames>Amit</forenames></author><author><keyname>Schapira</keyname><forenames>Michael</forenames></author><author><keyname>Shahaf</keyname><forenames>Gal</forenames></author></authors><title>Inapproximability of Truthful Mechanisms via Generalizations of the VC
  Dimension</title><categories>cs.GT cs.CC cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Algorithmic mechanism design (AMD) studies the delicate interplay between
computational efficiency, truthfulness, and optimality. We focus on AMD's
paradigmatic problem: combinatorial auctions. We present a new generalization
of the VC dimension to multivalued collections of functions, which encompasses
the classical VC dimension, Natarajan dimension, and Steele dimension. We
present a corresponding generalization of the Sauer-Shelah Lemma and harness
this VC machinery to establish inapproximability results for deterministic
truthful mechanisms. Our results essentially unify all inapproximability
results for deterministic truthful mechanisms for combinatorial auctions to
date and establish new separation gaps between truthful and non-truthful
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6266</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6266</id><created>2014-12-19</created><authors><author><keyname>Guang</keyname><forenames>Xuan</forenames></author><author><keyname>Lu</keyname><forenames>Jiyong</forenames></author><author><keyname>Fu</keyname><forenames>Fang-Wei</forenames></author></authors><title>Small Field Size for Secure Network Coding</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in the IEEE Communications Letters. One
  column,10 pages</comments><doi>10.1109/LCOMM.2014.2385053</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In network coding, information transmission often encounters wiretapping
attacks. Secure network coding is introduced to prevent information from being
leaked to adversaries. For secure linear network codes (SLNCs), the required
field size is a very important index, because it largely determines the
computational and space complexities of a SLNC, and it is also very important
for the process of secure network coding from theoretical research to practical
application. In this letter, we further discuss the required field size of
SLNCs, and obtain a new lower bound. This bound shows that the field size of
SLNCs can be reduced further, and much smaller than the known results for
almost all cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6268</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6268</id><created>2014-12-19</created><authors><author><keyname>Li</keyname><forenames>Jiyou</forenames></author><author><keyname>Luo</keyname><forenames>Chu</forenames></author></authors><title>The simplified weighted sum function and its average sensitivity</title><categories>cs.CC</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we simplify the definition of the weighted sum Boolean function
which used to be inconvenient to compute and use. We show that the new function
has essentially the same properties as the previous one. In particular, the
bound on the average sensitivity of the weighted sum Boolean function remains
unchanged after the simplification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6271</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6271</id><created>2014-12-19</created><authors><author><keyname>Matsumoto</keyname><forenames>Tsutomu</forenames></author><author><keyname>Hoga</keyname><forenames>Morihisa</forenames></author><author><keyname>Ohyagi</keyname><forenames>Yasuyuki</forenames></author><author><keyname>Ishikawa</keyname><forenames>Mikio</forenames></author><author><keyname>Naruse</keyname><forenames>Makoto</forenames></author><author><keyname>Hanaki</keyname><forenames>Kenta</forenames></author><author><keyname>Suzuki</keyname><forenames>Ryosuke</forenames></author><author><keyname>Sekiguchi</keyname><forenames>Daiki</forenames></author><author><keyname>Tate</keyname><forenames>Naoya</forenames></author><author><keyname>Ohtsu</keyname><forenames>Motoichi</forenames></author></authors><title>Nano-artifact metrics based on random collapse of resist</title><categories>cs.CR cond-mat.mes-hall cs.ET</categories><journal-ref>Scientific Reports, Vol. 4, Article No. 6142 (2014)</journal-ref><doi>10.1038/srep06142</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Artifact metrics is an information security technology that uses the
intrinsic characteristics of a physical object for authentication and clone
resistance. Here, we demonstrate nano-artifact metrics based on silicon
nanostructures formed via an array of resist pillars that randomly collapse
when exposed to electron-beam lithography. The proposed technique uses
conventional and scalable lithography processes, and because of the random
collapse of resist, the resultant structure has extremely fine-scale morphology
with a minimum dimension below 10 nm, which is less than the resolution of
current lithography capabilities. By evaluating false match, false non-match
and clone-resistance rates, we clarify that the nanostructured patterns based
on resist collapse satisfy the requirements for high-performance security
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6277</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6277</id><created>2014-12-19</created><updated>2015-04-10</updated><authors><author><keyname>Lebret</keyname><forenames>R&#xe9;mi</forenames></author><author><keyname>Collobert</keyname><forenames>Ronan</forenames></author></authors><title>N-gram-Based Low-Dimensional Representation for Document Classification</title><categories>cs.CL</categories><comments>Accepted as a workshop contribution at ICLR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The bag-of-words (BOW) model is the common approach for classifying
documents, where words are used as feature for training a classifier. This
generally involves a huge number of features. Some techniques, such as Latent
Semantic Analysis (LSA) or Latent Dirichlet Allocation (LDA), have been
designed to summarize documents in a lower dimension with the least semantic
information loss. Some semantic information is nevertheless always lost, since
only words are considered. Instead, we aim at using information coming from
n-grams to overcome this limitation, while remaining in a low-dimension space.
Many approaches, such as the Skip-gram model, provide good word vector
representations very quickly. We propose to average these representations to
obtain representations of n-grams. All n-grams are thus embedded in a same
semantic space. A K-means clustering can then group them into semantic
concepts. The number of features is therefore dramatically reduced and
documents can be represented as bag of semantic concepts. We show that this
model outperforms LSA and LDA on a sentiment classification task, and yields
similar results than a traditional BOW-model with far less features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6279</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6279</id><created>2014-12-19</created><updated>2015-09-29</updated><authors><author><keyname>Gonzalez</keyname><forenames>Adriana</forenames></author><author><keyname>Delouille</keyname><forenames>V&#xe9;ronique</forenames></author><author><keyname>Jacques</keyname><forenames>Laurent</forenames></author></authors><title>Non-parametric PSF estimation from celestial transit solar images using
  blind deconvolution</title><categories>cs.CV astro-ph.SR</categories><comments>31 pages, 47 figures</comments><journal-ref>J. Space Weather Space Clim., 6, A1, 2016</journal-ref><doi>10.1051/swsc/2015040</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Context: Characterization of instrumental effects in astronomical imaging is
important in order to extract accurate physical information from the
observations. The measured image in a real optical instrument is usually
represented by the convolution of an ideal image with a Point Spread Function
(PSF). Additionally, the image acquisition process is also contaminated by
other sources of noise (read-out, photon-counting). The problem of estimating
both the PSF and a denoised image is called blind deconvolution and is
ill-posed.
  Aims: We propose a blind deconvolution scheme that relies on image
regularization. Contrarily to most methods presented in the literature, our
method does not assume a parametric model of the PSF and can thus be applied to
any telescope.
  Methods: Our scheme uses a wavelet analysis prior model on the image and weak
assumptions on the PSF. We use observations from a celestial transit, where the
occulting body can be assumed to be a black disk. These constraints allow us to
retain meaningful solutions for the filter and the image, eliminating trivial,
translated and interchanged solutions. Under an additive Gaussian noise
assumption, they also enforce noise canceling and avoid reconstruction
artifacts by promoting the whiteness of the residual between the blurred
observations and the cleaned data.
  Results: Our method is applied to synthetic and experimental data. The PSF is
estimated for the SECCHI/EUVI instrument using the 2007 Lunar transit, and for
SDO/AIA using the 2012 Venus transit. Results show that the proposed
non-parametric blind deconvolution method is able to estimate the core of the
PSF with a similar quality to parametric methods proposed in the literature. We
also show that, if these parametric estimations are incorporated in the
acquisition model, the resulting PSF outperforms both the parametric and
non-parametric methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6285</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6285</id><created>2014-12-19</created><authors><author><keyname>Bontempi</keyname><forenames>Gianluca</forenames></author><author><keyname>Flauder</keyname><forenames>Maxime</forenames></author></authors><title>From dependency to causality: a machine learning approach</title><categories>cs.LG cs.AI stat.ML</categories><comments>submitted to JMLR</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The relationship between statistical dependency and causality lies at the
heart of all statistical approaches to causal inference. Recent results in the
ChaLearn cause-effect pair challenge have shown that causal directionality can
be inferred with good accuracy also in Markov indistinguishable configurations
thanks to data driven approaches. This paper proposes a supervised machine
learning approach to infer the existence of a directed causal link between two
variables in multivariate settings with $n&gt;2$ variables. The approach relies on
the asymmetry of some conditional (in)dependence relations between the members
of the Markov blankets of two variables causally connected. Our results show
that supervised learning methods may be successfully used to extract causal
information on the basis of asymmetric statistical descriptors also for $n&gt;2$
variate distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6286</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6286</id><created>2014-12-19</created><updated>2015-03-30</updated><authors><author><keyname>B&#xf6;hmer</keyname><forenames>Wendelin</forenames></author><author><keyname>Obermayer</keyname><forenames>Klaus</forenames></author></authors><title>Regression with Linear Factored Functions</title><categories>cs.LG stat.ML</categories><comments>Under review as conference paper at ECML/PKDD 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many applications that use empirically estimated functions face a curse of
dimensionality, because the integrals over most function classes must be
approximated by sampling. This paper introduces a novel regression-algorithm
that learns linear factored functions (LFF). This class of functions has
structural properties that allow to analytically solve certain integrals and to
calculate point-wise products. Applications like belief propagation and
reinforcement learning can exploit these properties to break the curse and
speed up computation. We derive a regularized greedy optimization scheme, that
learns factored basis functions during training. The novel regression algorithm
performs competitively to Gaussian processes on benchmark tasks, and the
learned LFF functions are with 4-9 factored basis functions on average very
compact.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6291</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6291</id><created>2014-12-19</created><authors><author><keyname>Wielgus</keyname><forenames>Maciek</forenames></author></authors><title>Perona-Malik equation and its numerical properties</title><categories>cs.NA math.NA</categories><comments>My bachelor thesis, Faculty of Mathematics, Informatics and
  Mechanics, University of Warsaw, 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work concerns the Perona-Malik equation, which plays essential role in
image processing. The first part gives a survey of results on existance,
uniqueness and stability of solutions, the second part introduces
discretisations of equation and deals with an analysis of discrete problem. In
the last part I present some numerical results, in particular with algorithms
applied to real images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6293</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6293</id><created>2014-12-19</created><authors><author><keyname>Kone&#x10d;n&#xfd;</keyname><forenames>Jakub</forenames></author><author><keyname>Qu</keyname><forenames>Zheng</forenames></author><author><keyname>Richt&#xe1;rik</keyname><forenames>Peter</forenames></author></authors><title>Semi-Stochastic Coordinate Descent</title><categories>cs.NA math.OC</categories><comments>14 pages. The paper was accepted for presentation at the 2014 NIPS
  Optimization for Machine Learning workshop in a peer reviewed process</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel stochastic gradient method---semi-stochastic coordinate
descent (S2CD)---for the problem of minimizing a strongly convex function
represented as the average of a large number of smooth convex functions:
$f(x)=\tfrac{1}{n}\sum_i f_i(x)$. Our method first performs a deterministic
step (computation of the gradient of $f$ at the starting point), followed by a
large number of stochastic steps. The process is repeated a few times, with the
last stochastic iterate becoming the new starting point where the deterministic
step is taken. The novelty of our method is in how the stochastic steps are
performed. In each such step, we pick a random function $f_i$ and a random
coordinate $j$---both using nonuniform distributions---and update a single
coordinate of the decision vector only, based on the computation of the
$j^{th}$ partial derivative of $f_i$ at two different points. Each random step
of the method constitutes an unbiased estimate of the gradient of $f$ and
moreover, the squared norm of the steps goes to zero in expectation, meaning
that the stochastic estimate of the gradient progressively improves. The
complexity of the method is the sum of two terms: $O(n\log(1/\epsilon))$
evaluations of gradients $\nabla f_i$ and $O(\hat{\kappa}\log(1/\epsilon))$
evaluations of partial derivatives $\nabla_j f_i$, where $\hat{\kappa}$ is a
novel condition number.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6296</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6296</id><created>2014-12-19</created><updated>2015-04-09</updated><authors><author><keyname>Dai</keyname><forenames>Jifeng</forenames></author><author><keyname>Lu</keyname><forenames>Yang</forenames></author><author><keyname>Wu</keyname><forenames>Ying-Nian</forenames></author></authors><title>Generative Modeling of Convolutional Neural Networks</title><categories>cs.CV cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The convolutional neural networks (CNNs) have proven to be a powerful tool
for discriminative learning. Recently researchers have also started to show
interest in the generative aspects of CNNs in order to gain a deeper
understanding of what they have learned and how to further improve them. This
paper investigates generative modeling of CNNs. The main contributions include:
(1) We construct a generative model for the CNN in the form of exponential
tilting of a reference distribution. (2) We propose a generative gradient for
pre-training CNNs by a non-parametric importance sampling scheme, which is
fundamentally different from the commonly used discriminative gradient, and yet
has the same computational architecture and cost as the latter. (3) We propose
a generative visualization method for the CNNs by sampling from an explicit
parametric image distribution. The proposed visualization method can directly
draw synthetic samples for any given node in a trained CNN by the Hamiltonian
Monte Carlo (HMC) algorithm, without resorting to any extra hold-out images.
Experiments on the challenging ImageNet benchmark show that the proposed
generative gradient pre-training consistently helps improve the performances of
CNNs, and the proposed generative visualization method generates meaningful and
varied samples of synthetic images from a large-scale deep CNN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6306</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6306</id><created>2014-12-19</created><authors><author><keyname>Lita</keyname><forenames>Adrian-Ioan</forenames></author><author><keyname>Plotog</keyname><forenames>Ioan</forenames></author><author><keyname>Dobrescu</keyname><forenames>Lidia</forenames></author></authors><title>Multiprocessor System Dedicated to Multi-Rotor Mini-UAV Capable of 3D
  flying</title><categories>cs.CE</categories><comments>International Conference of Scientific Paper AFASES 2014 Brasov,
  22-24 May 2014</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The paper describes an electronic multiprocessor system that assures
functionality of a miniature UAV capable of 3D flying. The apparatus consists
of six independently controlled brushless DC motors, each having a propeller
attached to it. Since the brushless motor requires complex algorithms in order
to achieve maximum torque, efficiency and response time a DSP must be used. All
the motors are then controlled by a main microprocessor which is capable of
reading sensors (Inertial Measurement Unit (IMU)-orientation and GPS),
receiving input commands (remote controller or trajectory plan) and sending
independent commands to each of the six motors. The apparatus contains a total
of eight microcontrollers: the main unit, the IMU mathematical processor and
one microcontroller for each of the six brushless DC motors. Applications for
such an apparatus could include not only military, but also search-and-rescue,
geodetics, aerial photography and aerial assistance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6329</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6329</id><created>2014-12-19</created><authors><author><keyname>Li</keyname><forenames>Xiang</forenames></author><author><keyname>Zhang</keyname><forenames>Yi-Qing</forenames></author><author><keyname>Vasilakos</keyname><forenames>Athanasios V.</forenames></author></authors><title>Discovering and Predicting Temporal Patterns of WiFi-interactive Social
  Populations</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>11 pages, 10 pages</comments><journal-ref>Opportunistic Mobile Social Networks, 2014: 99</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Extensive efforts have been devoted to characterizing the rich connectivity
patterns among the nodes (components) of such complex networks (systems), and
in the course of development of research in this area, people have been
prompted to address on a fundamental question: How does the fascinating yet
complex topological features of a network affect or determine the collective
behavior and performance of the networked system? While elegant attempts to
address this core issue have been made, for example, from the viewpoints of
synchronization, epidemics, evolutionary cooperation, and the control of
complex networks, theoretically or empirically, this widely concerned key
question still remains open in the newly emergent field of network science.
Such fruitful advances also push the desire to understand (mobile) social
networks and characterize human social populations with the interdependent
collective dynamics as well as the behavioral patterns. Nowadays, a great deal
of digital technologies are unobtrusively embedded into the physical world of
human daily activities, which offer unparalleled opportunities to explosively
digitize human physical interactions, who is contacting with whom at what time.
Such powerful technologies include the Bluetooth, the active Radio Frequency
Identification (RFID) technology, wireless sensors and, more close to our
interest in this paper, the WiFi technology. As a snapshot of the modern
society, a university is in the coverage of WiFi signals, where the WiFi system
records the digital access logs of the authorized WiFi users when they access
the campus wireless services. Such WiFi access records, as the indirect proxy
data, work as the effective proxy of a large-scale population's social
interactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6334</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6334</id><created>2014-12-19</created><updated>2015-08-22</updated><authors><author><keyname>Soyer</keyname><forenames>Hubert</forenames></author><author><keyname>Stenetorp</keyname><forenames>Pontus</forenames></author><author><keyname>Aizawa</keyname><forenames>Akiko</forenames></author></authors><title>Leveraging Monolingual Data for Crosslingual Compositional Word
  Representations</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this work, we present a novel neural network based architecture for
inducing compositional crosslingual word representations. Unlike previously
proposed methods, our method fulfills the following three criteria; it
constrains the word-level representations to be compositional, it is capable of
leveraging both bilingual and monolingual data, and it is scalable to large
vocabularies and large quantities of data. The key component of our approach is
what we refer to as a monolingual inclusion criterion, that exploits the
observation that phrases are more closely semantically related to their
sub-phrases than to other randomly sampled phrases. We evaluate our method on a
well-established crosslingual document classification task and achieve results
that are either comparable, or greatly improve upon previous state-of-the-art
methods. Concretely, our method reaches a level of 92.7% and 84.4% accuracy for
the English to German and German to English sub-tasks respectively. The former
advances the state of the art by 0.9% points of accuracy, the latter is an
absolute improvement upon the previous state of the art by 7.7% points of
accuracy and an improvement of 33.0% in error reduction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6341</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6341</id><created>2014-12-19</created><authors><author><keyname>Daugulis</keyname><forenames>Peteris</forenames></author></authors><title>Connectome graphs and maximum flow problems</title><categories>q-bio.NC cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose to study maximum flow problems for connectome graphs. We suggest a
few computational problems: finding vertex pairs with maximal flow, finding new
edges which would increase the maximal flow. Initial computation results for
some publicly available connectome graphs are described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6346</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6346</id><created>2014-12-19</created><authors><author><keyname>Searson</keyname><forenames>Dominic P.</forenames></author><author><keyname>Willis</keyname><forenames>Mark J.</forenames></author><author><keyname>Wright</keyname><forenames>Allen</forenames></author></authors><title>Reverse Engineering Chemical Reaction Networks from Time Series Data</title><categories>cs.NE q-bio.MN</categories><comments>36 pages. In: Dehmer, M., Varmuza, K., Bonchev, D, ed. Statistical
  Modelling of Molecular Descriptors in QSAR/QSPR. Weinheim, Germany: Wiley-VCH
  Verlag GmbH, 2012, pp.327-348</comments><doi>10.1002/9783527645121.ch12</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The automated inference of physically interpretable (bio)chemical reaction
network models from measured experimental data is a challenging problem whose
solution has significant commercial and academic ramifications. It is
demonstrated, using simulations, how sets of elementary reactions comprising
chemical reaction networks, as well as their rate coefficients, may be
accurately recovered from non-equilibrium time series concentration data, such
as that obtained from laboratory scale reactors. A variant of an evolutionary
algorithm called differential evolution in conjunction with least squares
techniques is used to search the space of reaction networks in order to infer
both the reaction network topology and its rate parameters. Properties of the
stoichiometric matrices of trial networks are used to bias the search towards
physically realisable solutions. No other information, such as chemical
characterisation of the reactive species is required, although where available
it may be used to improve the search process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6357</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6357</id><created>2014-12-19</created><authors><author><keyname>Callegari</keyname><forenames>Sergio</forenames></author></authors><title>Achievement of Preassigned Spectra in the Synthesis of Band-Pass
  Constant-Envelope Signals by Rapidly Hopping through Discrete Frequencies</title><categories>physics.data-an cs.IT math.IT</categories><comments>4 pages, 5 figures. Pre-print from conference proceedings</comments><journal-ref>Proceedings of the 2014 IEEE International Symposium on Circuits
  and Systems (ISCAS), pp. 2776-2779, Jun. 2014</journal-ref><doi>10.1109/ISCAS.2014.6865749</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spread-spectrum signals are increasingly adopted in fields including
communications, testing of electronic systems, Electro-Magnetic Compatibility
(EMC) enhancement, ultrasonic non-destructive testing. This paper considers the
synthesis of constant-envelope band-pass wave-forms with preassigned spectra
via an FM technique using only a limited number of frequencies. In particular,
an optimization-based approach for the selection of appropriate modulation
parameters and statistical features of the modulating waveform is proposed. By
example, it is shown that the design problem generally admits multiple local
optima, but can still be managed with relative ease since the local optima can
typically be scanned by changing the initial setting of a single parameter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6359</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6359</id><created>2014-12-17</created><authors><author><keyname>Hoque</keyname><forenames>Mohammad Iftekharul</forenames></author><author><keyname>Ranga</keyname><forenames>Vijay Nag</forenames></author><author><keyname>Pedditi</keyname><forenames>Anurag Reddy</forenames></author><author><keyname>Srinath</keyname><forenames>Rachitha</forenames></author><author><keyname>Rana</keyname><forenames>Md Ali Ahsan</forenames></author><author><keyname>Islam</keyname><forenames>Md Eftakhairul</forenames></author><author><keyname>Somani</keyname><forenames>Afshin</forenames></author></authors><title>An Empirical Study on Refactoring Activity</title><categories>cs.SE</categories><comments>11 pages, 9 figures, 1 table</comments><acm-class>D.2; K.6; H.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reports an empirical study on refactoring activity in three Java
software systems. We investigated some questions on refactoring activity, to
confirm or disagree on conclusions that have been drawn from previous empirical
studies. Unlike previous empirical studies, our study found that it is not
always true that there are more refactoring activities before major project
release date than after. In contrast, we were able to confirm that software
developers perform different types of refactoring operations on test code and
production code, specific developers are responsible for refactorings in the
project, refactoring edits are not very well tested. Further, floss refactoring
is more popular among the developers, refactoring activity is frequent in the
projects, majority of bad smells once occurred they persist up to the latest
version of the system. By confirming assumptions by other researchers we can
have greater confidence that those research conclusions are generalizable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6361</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6361</id><created>2014-12-19</created><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>Pass</keyname><forenames>Rafael</forenames></author></authors><title>Sequential Equilibrium in Computational Games</title><categories>cs.GT</categories><comments>Appears in IJCAI 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine sequential equilibrium in the context of computational games,
where agents are charged for computation. In such games, an agent can
rationally choose to forget, so issues of imperfect recall arise. In this
setting, we consider two notions of sequential equilibrium. One is an ex ante
notion, where a player chooses his strategy before the game starts and is
committed to it, but chooses it in such a way that it remains optimal even off
the equilibrium path. The second is an interim notion, where a player can
reconsider at each information set whether he is doing the &quot;right&quot; thing, and
if not, can change his strategy. The two notions agree in games of perfect
recall, but not in games of imperfect recall. Although the interim notion seems
more appealing, \fullv{Halpern and Pass [2011] argue that there are some deep
conceptual problems with it in standard games of imperfect recall. We show that
the conceptual problems largely disappear in the computational setting.
Moreover, in this setting, under natural assumptions, the two notions coincide.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6367</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6367</id><created>2014-12-19</created><authors><author><keyname>Kieffer</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author><author><keyname>Ashiotis</keyname><forenames>Giannis</forenames></author></authors><title>PyFAI: a Python library for high performance azimuthal integration on
  GPU</title><categories>astro-ph.IM cs.DC cs.MS</categories><comments>Part of the Proceedings of the 7th European Conference on Python in
  Science (EuroSciPy 2014), Pierre de Buyl and Nelle Varoquaux editors, (2014)</comments><report-no>euroscipy-proceedings2014-02</report-no><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The pyFAI package has been designed to reduce X-ray diffraction images into
powder diffraction curves to be further processed by scientists. This
contribution describes how to convert an image into a radial profile using the
Numpy package, how the process was accelerated using Cython. The algorithm was
parallelised, needing a complete re-design to benefit from massively parallel
devices like graphical processing units or accelerators like the Intel Xeon Phi
using the PyOpenCL library.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6368</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6368</id><created>2014-12-19</created><updated>2015-09-09</updated><authors><author><keyname>Walter</keyname><forenames>Cl&#xe9;ment</forenames></author></authors><title>Point Process-based Monte Carlo estimation</title><categories>cs.CE stat.CO</categories><comments>13 pages + 4 pages of appendix, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the issue of estimating the expectation of a real-valued
random variable of the form $X = g(\mathbf{U})$ where $g$ is a deterministic
function and $\mathbf{U}$ can be a random finite- or infinite-dimensional
vector. Using recent results on rare event simulation, we propose a unified
framework for dealing with both probability and mean estimation for such random
variables, \emph{i.e.} linking algorithms such as Tootsie Pop Algorithm (TPA)
or Last Particle Algorithm with nested sampling. Especially, it extends nested
sampling as follows: first the random variable $X$ does not need to be bounded
any more: it gives the principle of an ideal estimator with an infinite number
of terms that is unbiased and always better than a classical Monte Carlo
estimator -- in particular it has a finite variance as soon as there exists $k
\in \mathbb{R} &gt; 1$ such that $\operatorname{E}[X^k] &lt; \infty$. Moreover we
address the issue of nested sampling termination and show that a random
truncation of the sum can preserve unbiasedness while increasing the variance
only by a factor up to 2 compared to the ideal case. We also build an unbiased
estimator with fixed computational budget which supports a Central Limit
Theorem and discuss parallel implementation of nested sampling, which can
dramatically reduce its computational cost. Finally we extensively study the
case where $X$ is heavy-tailed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6376</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6376</id><created>2014-12-19</created><authors><author><keyname>Chen</keyname><forenames>Zitan</forenames></author><author><keyname>Jaggi</keyname><forenames>Sidharth</forenames></author><author><keyname>Langberg</keyname><forenames>Michael</forenames></author></authors><title>A characterization of the capacity of online (causal) binary channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the binary online (or &quot;causal&quot;) channel coding model, a sender wishes to
communicate a message to a receiver by transmitting a codeword $\mathbf{x}
=(x_1,\ldots,x_n) \in \{0,1\}^n$ bit by bit via a channel limited to at most
$pn$ corruptions. The channel is &quot;online&quot; in the sense that at the $i$th step
of communication the channel decides whether to corrupt the $i$th bit or not
based on its view so far, i.e., its decision depends only on the transmitted
bits $(x_1,\ldots,x_i)$. This is in contrast to the classical adversarial
channel in which the error is chosen by a channel that has a full knowledge on
the sent codeword $\mathbf{x}$.
  In this work we study the capacity of binary online channels for two
corruption models: the {\em bit-flip} model in which the channel may flip at
most $pn$ of the bits of the transmitted codeword, and the {\em erasure} model
in which the channel may erase at most $pn$ bits of the transmitted codeword.
Specifically, for both error models we give a full characterization of the
capacity as a function of $p$.
  The online channel (in both the bit-flip and erasure case) has seen a number
of recent studies which present both upper and lower bounds on its capacity. In
this work, we present and analyze a coding scheme that improves on the
previously suggested lower bounds and matches the previously suggested upper
bounds thus implying a tight characterization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6378</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6378</id><created>2014-12-19</created><authors><author><keyname>Venthur</keyname><forenames>Bastian</forenames></author><author><keyname>Blankertz</keyname><forenames>Benjamin</forenames></author></authors><title>Wyrm, A Pythonic Toolbox for Brain-Computer Interfacing</title><categories>cs.HC</categories><comments>Part of the Proceedings of the 7th European Conference on Python in
  Science (EuroSciPy 2014), Pierre de Buyl and Nelle Varoquaux editors, (2014)</comments><report-no>euroscipy-proceedings2014-04</report-no><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  A Brain-Computer Interface (BCI) is a system that measures central nervous
system activity and translates the recorded data into an output suitable for a
computer to use as an input signal. Such a BCI system consists of three parts,
the signal acquisition, the signal processing and the feedback/stimulus
presentation. In this paper we present Wyrm, a signal processing toolbox for
BCI in Python. Wyrm is applicable to a broad range of neuroscientific problems
and capable for running online experiments in real time and off-line data
analysis and visualisation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6382</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6382</id><created>2014-12-19</created><authors><author><keyname>Mineraud</keyname><forenames>Julien</forenames></author><author><keyname>Wang</keyname><forenames>Liang</forenames></author><author><keyname>Balasubramaniam</keyname><forenames>Sasitharan</forenames></author><author><keyname>Kangasharju</keyname><forenames>Jussi</forenames></author></authors><title>Renewable Energy-Aware Information-Centric Networking</title><categories>cs.NI cs.DC cs.PF</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ICT industry today is placed as one of the major consumers of energy,
where recent reports have also shown that the industry is a major contributor
to global carbon emissions. While renewable energy-aware data centers have been
proposed, these solutions have certain limitations. The primary limitation is
due to the design of data centers which focus on large-size facilities located
in selected locations. This paper addresses this problem, by utilizing
in-network caching with each router having storage and being powered by
renewable energy sources (wind and solar). Besides placing contents closer to
end users, utilizing in-network caching could potentially increase probability
of capturing renewable energy in diverse geographical locations. Our proposed
solution is dual- layered: on the first layer a distributed gradient-based
routing protocol is used to discover the paths along routers that are powered
by the highest renewable energy, and on the second layer, a caching mechanism
will pull the contents from the data centre and place them on routers of the
paths that are discovered by our routing protocol. Through our experiments on a
testbed utilizing real meteorological data, our proposed solution has
demonstrated increased quantity of renewable energy consumption, while reducing
the workload on the data centers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6383</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6383</id><created>2014-12-19</created><authors><author><keyname>Pouzat</keyname><forenames>Christophe</forenames></author><author><keyname>Detorakis</keyname><forenames>Georgios Is.</forenames></author></authors><title>SPySort: Neuronal Spike Sorting with Python</title><categories>cs.CE q-bio.NC</categories><comments>Part of the Proceedings of the 7th European Conference on Python in
  Science (EuroSciPy 2014), Pierre de Buyl and Nelle Varoquaux editors, (2014)</comments><report-no>euroscipy-proceedings2014-05</report-no><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Extracellular recordings with multi-electrode arrays is one of the basic
tools of contemporary neuroscience. These recordings are mostly used to monitor
the activities, understood as sequences of emitted action potentials, of many
individual neurons. But the raw data produced by extracellular recordings are
most commonly a mixture of activities from several neurons. In order to get the
activities of the individual contributing neurons, a pre-processing step called
spike sorting is required. We present here a pure Python implementation of a
well tested spike sorting procedure. The latter was designed in a modular way
in order to favour a smooth transition from an interactive sorting, for
instance with IPython, to an automatic one. Surprisingly enough - or sadly
enough, depending on one's view point -, recoding our now 15 years old
procedure into Python was the occasion of major methodological improvements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6386</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6386</id><created>2014-12-19</created><authors><author><keyname>Cokelaer</keyname><forenames>Thomas</forenames></author><author><keyname>Saez-Rodriguez</keyname><forenames>Julio</forenames></author></authors><title>Using Python to Dive into Signalling Data with CellNOpt and BioServices</title><categories>cs.CE q-bio.MN</categories><comments>Part of the Proceedings of the 7th European Conference on Python in
  Science (EuroSciPy 2014), Pierre de Buyl and Nelle Varoquaux editors, (2014)</comments><report-no>euroscipy-proceedings2014-06</report-no><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Systems biology is an inter-disciplinary field that studies systems of
biological components at different scales, which may be molecules, cells or
entire organism. In particular, systems biology methods are applied to
understand functional deregulations within human cells (e.g., cancers). In this
context, we present several python packages linked to CellNOptR (R package),
which is used to build predictive logic models of signalling networks by
training networks (derived from literature) to signalling (phospho-proteomic)
data. The first package (cellnopt.wrapper) is a wrapper based on RPY2 that
allows a full access to CellNOptR functionalities within Python. The second one
(cellnopt.core) was designed to ease the manipulation and visualisation of data
structures used in CellNOptR, which was achieved by using Pandas, NetworkX and
matplotlib. Systems biology also makes extensive use of web resources and
services. We will give an overview and status of BioServices, which allows one
to access programmatically to web resources used in life science and how it can
be combined with CellNOptR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6388</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6388</id><created>2014-12-19</created><authors><author><keyname>&#x130;rsoy</keyname><forenames>Ozan</forenames></author><author><keyname>Alpayd&#x131;n</keyname><forenames>Ethem</forenames></author></authors><title>Distributed Decision Trees</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently proposed budding tree is a decision tree algorithm in which every
node is part internal node and part leaf. This allows representing every
decision tree in a continuous parameter space, and therefore a budding tree can
be jointly trained with backpropagation, like a neural network. Even though
this continuity allows it to be used in hierarchical representation learning,
the learned representations are local: Activation makes a soft selection among
all root-to-leaf paths in a tree. In this work we extend the budding tree and
propose the distributed tree where the children use different and independent
splits and hence multiple paths in a tree can be traversed at the same time.
This ability to combine multiple paths gives the power of a distributed
representation, as in a traditional perceptron layer. We show that distributed
trees perform comparably or better than budding and traditional hard trees on
classification and regression tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6391</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6391</id><created>2014-12-19</created><authors><author><keyname>Monari</keyname><forenames>Davide</forenames></author><author><keyname>Cenni</keyname><forenames>Francesco</forenames></author><author><keyname>Aertbeli&#xeb;n</keyname><forenames>Erwin</forenames></author><author><keyname>Desloovere</keyname><forenames>Kaat</forenames></author></authors><title>Py3DFreeHandUS: a library for voxel-array reconstruction using
  Ultrasonography and attitude sensors</title><categories>cs.CV cs.CE</categories><comments>Part of the Proceedings of the 7th European Conference on Python in
  Science (EuroSciPy 2014), Pierre de Buyl and Nelle Varoquaux editors, (2014)</comments><report-no>euroscipy-proceedings2014-07</report-no><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In medical imaging, there is a growing interest to provide real-time images
with good quality for large anatomical structures. To cope with this issue, we
developed a library that allows to replace, for some specific clinical
applications, more robust systems such as Computer Tomography (CT) and Magnetic
Resonance Imaging (MRI). Our python library Py3DFreeHandUS is a package for
processing data acquired simultaneously by ultra-sonographic systems (US) and
marker-based optoelectronic systems. In particular, US data enables to
visualize subcutaneous body structures, whereas the optoelectronic system is
able to collect the 3D position in space for reflective objects, that are
called markers. By combining these two measurement devices, it is possible to
reconstruct the real 3D morphology of body structures such as muscles, for
relevant clinical implications. In the present research work, the different
steps which allow to obtain a relevant 3D data set as well as the procedures
for calibrating the systems and for determining the quality of the
reconstruction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6392</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6392</id><created>2014-12-19</created><updated>2015-01-26</updated><authors><author><keyname>Mantripragada</keyname><forenames>Kiran</forenames></author><author><keyname>Binotto</keyname><forenames>Alecio</forenames></author><author><keyname>Tizzei</keyname><forenames>Leonardo P.</forenames></author></authors><title>A Self-adaptive Auto-scaling Method for Scientific Applications on HPC
  Environments and Clouds</title><categories>cs.DC</categories><comments>Part of ADAPT Workshop proceedings, 2015 (arXiv:1412.2347)</comments><report-no>ADAPT/2015/01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High intensive computation applications can usually take days to months to
finish an execution. During this time, it is common to have variations of the
available resources when considering that such hardware is usually shared among
a plurality of researchers/departments within an organization. On the other
hand, High Performance Clusters can take advantage of Cloud Computing bursting
techniques for the execution of applications together with the on-premise
resources. In order to meet deadlines, high intensive computational
applications can use the Cloud to boost their performance when they are data
and task parallel. This article presents an ongoing work towards the use of
extended resources of an HPC execution platform together with Cloud. We propose
an unified view of such heterogeneous environments and a method that monitors,
predicts the application execution time, and dynamically shifts part of the
domain -- previously running in local HPC hardware -- to be computed on the
Cloud, meeting then a specific deadline. The method is exemplified along with a
seismic application that, at runtime, adapts itself to move part of the
processing to the Cloud (in a movement called bursting) and also auto-scales
(the moved part) over cloud nodes. Our preliminary results show that there is
an expected overhead for performing this movement and for synchronizing
results, but our outcomes demonstrate it is an important feature for meeting
deadlines in the case an on-premise cluster is overloaded or cannot provide the
capacity needed for a particular project.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6395</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6395</id><created>2014-12-19</created><authors><author><keyname>Fuentes</keyname><forenames>Esteban</forenames></author><author><keyname>Martinez</keyname><forenames>Hector E.</forenames></author></authors><title>SClib, a hack for straightforward embedded C functions in Python</title><categories>cs.MS physics.comp-ph</categories><comments>Part of the Proceedings of the 7th European Conference on Python in
  Science (EuroSciPy 2014), Pierre de Buyl and Nelle Varoquaux editors, (2014)</comments><report-no>euroscipy-proceedings2014-08</report-no><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We present SClib, a simple hack that allows easy and straightforward
evaluation of C functions within Python code, boosting flexibility for better
trade-off between computation power and feature availability, such as
visualization and existing computation routines in SciPy. We also present two
cases were SClib has been used. In the first set of applications we use SClib
to write a port to Python of a Schr\&quot;odinger equation solver that has been
extensively used the literature, the resulting script presents a speed-up of
about 150x with respect to the original one. A review of the situations where
the speeded-up script has been used is presented. We also describe the solution
to the related problem of solving a set of coupled Schr\&quot;odinger-like equations
where SClib is used to implement the speed-critical parts of the code. We argue
that when using SClib within IPython we can use NumPy and Matplotlib for the
manipulation and visualization of the solutions in an interactive environment
with no performance compromise. The second case is an engineering application.
We use SClib to evaluate the control and system derivatives in a feedback
control loop for electrical motors. With this and the integration routines
available in SciPy, we can run simulations of the control loop a la Simulink.
The use of C code not only boosts the speed of the simulations, but also
enables to test the exact same code that we use in the test rig to get
experimental results. Again, integration with IPython gives us the flexibility
to analyze and visualize the data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6396</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6396</id><created>2014-12-19</created><authors><author><keyname>Tantau</keyname><forenames>Till</forenames></author></authors><title>Existential Second-Order Logic Over Graphs: A Complete
  Complexity-Theoretic Classification</title><categories>cs.LO cs.CC</categories><comments>Technical report version of a STACS 2015 paper</comments><msc-class>68Q19</msc-class><acm-class>F.1.3; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Descriptive complexity theory aims at inferring a problem's computational
complexity from the syntactic complexity of its description. A cornerstone of
this theory is Fagin's Theorem, by which a graph property is expressible in
existential second-order logic (ESO logic) if, and only if, it is in NP. A
natural question, from the theory's point of view, is which syntactic fragments
of ESO logic also still characterize NP. Research on this question has
culminated in a dichotomy result by Gottlob, Kolatis, and Schwentick: for each
possible quantifier prefix of an ESO formula, the resulting prefix class either
contains an NP-complete problem or is contained in P. However, the exact
complexity of the prefix classes inside P remained elusive. In the present
paper, we clear up the picture by showing that for each prefix class of ESO
logic, its reduction closure under first-order reductions is either FO, L, NL,
or NP. For undirected, self-loop-free graphs two containment results are
especially challenging to prove: containment in L for the prefix $\exists R_1
\cdots \exists R_n \forall x \exists y$ and containment in FO for the prefix
$\exists M \forall x \exists y$ for monadic $M$. The complex argument by
Gottlob, Kolatis, and Schwentick concerning polynomial time needs to be
carefully reexamined and either combined with the logspace version of
Courcelle's Theorem or directly improved to first-order computations. A
different challenge is posed by formulas with the prefix $\exists M \forall
x\forall y$: We show that they express special constraint satisfaction problems
that lie in L.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6399</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6399</id><created>2014-12-19</created><authors><author><keyname>Dean</keyname><forenames>Jamie A.</forenames></author><author><keyname>Welsh</keyname><forenames>Liam C.</forenames></author><author><keyname>Harrington</keyname><forenames>Kevin J.</forenames></author><author><keyname>Nutting</keyname><forenames>Christopher M.</forenames></author><author><keyname>Gulliford</keyname><forenames>Sarah L.</forenames></author></authors><title>Predictive Modelling of Toxicity Resulting from Radiotherapy Treatments
  of Head and Neck Cancer</title><categories>physics.med-ph cs.CE q-bio.QM</categories><comments>Part of the Proceedings of the 7th European Conference on Python in
  Science (EuroSciPy 2014), Pierre de Buyl and Nelle Varoquaux editors, (2014)</comments><report-no>euroscipy-proceedings2014-09</report-no><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In radiotherapy for head and neck cancer, the radiation dose delivered to the
pharyngeal mucosa (mucosal lining of the throat) is thought to be a major
contributing factor to dysphagia (swallowing dysfunction), the most commonly
reported severe toxicity. There is a variation in the severity of dysphagia
experienced by patients. Understanding the role of the dose distribution in
dysphagia would allow improvements in the radiotherapy technique to be
explored. The 3D dose distributions delivered to the pharyngeal mucosa of 249
patients treated as part of clinical trials were reconstructed. Pydicom was
used to extract DICOM (digital imaging and communications in medicine) data
(the standard file formats for medical imaging and radiotherapy data). NumPy
and SciPy were used to manipulate the data to generate 3D maps of the dose
distribution delivered to the pharyngeal mucosa and calculate metrics
describing the dose distribution. Multivariate predictive modelling of severe
dysphagia, including descriptions of the dose distribution and relevant
clinical factors, was performed using Pandas and SciKit-Learn. Matplotlib and
Mayavi were used for 2D and 3D data visualisation. A support vector
classification model, with feature selection using randomised logistic
regression, to predict radiation-induced severe dysphagia, was trained. When
this model was independently validated, the area under the receiver operating
characteristic curve was 0.54. The model has poor predictive power and work is
ongoing to improve the model through alternative feature engineering and
statistical modelling approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6402</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6402</id><created>2014-12-19</created><authors><author><keyname>Murphy</keyname><forenames>Rebecca R.</forenames></author><author><keyname>Jackson</keyname><forenames>Sophie E.</forenames></author><author><keyname>Klenerman</keyname><forenames>David</forenames></author></authors><title>pyFRET: A Python Library for Single Molecule Fluorescence Data Analysis</title><categories>cs.CE physics.bio-ph q-bio.BM</categories><comments>Part of the Proceedings of the 7th European Conference on Python in
  Science (EuroSciPy 2014), Pierre de Buyl and Nelle Varoquaux editors, (2014)</comments><report-no>euroscipy-proceedings2014-10</report-no><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Single molecule F\&quot;orster resonance energy transfer (smFRET) is a powerful
experimental technique for studying the properties of individual biological
molecules in solution. However, as adoption of smFRET techniques becomes more
widespread, the lack of available software, whether open source or commercial,
for data analysis, is becoming a significant issue. Here, we present pyFRET, an
open source Python package for the analysis of data from single-molecule
fluorescence experiments from freely diffusing biomolecules. The package
provides methods for the complete analysis of a smFRET dataset, from burst
selection and denoising, through data visualisation and model fitting. We
provide support for both continuous excitation and alternating laser excitation
(ALEX) data analysis. pyFRET is available as a package downloadable from the
Python Package Index (PyPI) under the open source three-clause BSD licence,
together with links to extensive documentation and tutorials, including example
usage and test data. Additional documentation including tutorials is hosted
independently on ReadTheDocs. The code is available from the free hosting site
Bitbucket. Through distribution of this software, we hope to lower the barrier
for the adoption of smFRET experiments by other research groups and we
encourage others to contribute modules for specific analysis needs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6407</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6407</id><created>2014-12-19</created><authors><author><keyname>Cimrman</keyname><forenames>Robert</forenames></author></authors><title>Enhancing SfePy with Isogeometric Analysis</title><categories>cs.MS cs.NA math.NA</categories><comments>Part of the Proceedings of the 7th European Conference on Python in
  Science (EuroSciPy 2014), Pierre de Buyl and Nelle Varoquaux editors, (2014)</comments><report-no>euroscipy-proceedings2014-11</report-no><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In the paper a recent enhancement to the open source package SfePy (Simple
Finite Elements in Python, http://sfepy.org) is introduced, namely the addition
of another numerical discretization scheme, the isogeometric analysis, to the
original implementation based on the nowadays standard and well-established
numerical solution technique, the finite element method. The isogeometric
removes the need of the solution domain approximation by a piece-wise polygonal
domain covered by the finite element mesh, and allows approximation of unknown
fields with a higher smoothness then the finite element method, which can be
advantageous in many applications. Basic numerical examples illustrating the
implementation and use of the isogeometric analysis in SfePy are shown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6410</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6410</id><created>2014-12-19</created><authors><author><keyname>Brasier</keyname><forenames>Steve</forenames></author><author><keyname>Pollard</keyname><forenames>Fred</forenames></author></authors><title>A Python-based Post-processing Toolset For Seismic Analyses</title><categories>cs.CE physics.geo-ph</categories><comments>Part of the Proceedings of the 7th European Conference on Python in
  Science (EuroSciPy 2014), Pierre de Buyl and Nelle Varoquaux editors, (2014)</comments><report-no>euroscipy-proceedings2014-12</report-no><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper discusses the design and implementation of a Python-based toolset
to aid in assessing the response of the UK's Advanced Gas Reactor nuclear power
stations to earthquakes. The seismic analyses themselves are carried out with a
commercial Finite Element solver, but understanding the raw model output this
produces requires customised post-processing and visualisation tools. Extending
the existing tools had become increasingly difficult and a decision was made to
develop a new, Python-based toolset. This comprises of a post-processing
framework (aftershock) which includes an embedded Python interpreter, and a
plotting package (afterplot) based on numpy and matplotlib. The new toolset had
to be significantly more flexible and easier to maintain than the existing
code-base, while allowing the majority of development to be carried out by
engineers with little training in software development. The resulting
architecture will be described with a focus on exploring how the design drivers
were met and the successes and challenges arising from the choices made.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6412</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6412</id><created>2014-12-19</created><authors><author><keyname>Luke&#x161;</keyname><forenames>Vladim&#xed;r</forenames></author><author><keyname>Ji&#x159;&#xed;k</keyname><forenames>Miroslav</forenames></author><author><keyname>Jon&#xe1;&#x161;ov&#xe1;</keyname><forenames>Alena</forenames></author><author><keyname>Rohan</keyname><forenames>Eduard</forenames></author><author><keyname>Bubl&#xed;k</keyname><forenames>Ond&#x159;ej</forenames></author><author><keyname>Cimrman</keyname><forenames>Robert</forenames></author></authors><title>Numerical simulation of liver perfusion: from CT scans to FE model</title><categories>cs.CE</categories><comments>Part of the Proceedings of the 7th European Conference on Python in
  Science (EuroSciPy 2014), Pierre de Buyl and Nelle Varoquaux editors, (2014)</comments><report-no>euroscipy-proceedings2014-13</report-no><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We use a collection of Python programs for numerical simulation of liver
perfusion. We have an application for semi-automatic generation of a finite
element mesh of the human liver from computed tomography scans and for
reconstruction of the liver vascular structure. When the real vascular trees
can not be obtained from the CT data we generate artificial trees using the
constructive optimization method. The generated FE mesh and vascular trees are
imported into SfePy (Simple Finite Elements in Python) and numerical
simulations are performed in order to get the pressure distribution and
perfusion flows in the liver tissue. In the post-processing steps we calculate
transport of a contrast fluid through the liver parenchyma.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6413</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6413</id><created>2014-11-24</created><authors><author><keyname>Ramaswamy</keyname><forenames>Gowri Shankar</forenames></author><author><keyname>Francis</keyname><forenames>F Sagayaraj</forenames></author></authors><title>Towards a Consistent, Sound and Complete Conceptual Knowledge</title><categories>cs.AI</categories><journal-ref>International Journal of Computer Trends and Technology (IJCTT)
  V17(2):61-63, Nov 2014</journal-ref><doi>10.14445/22312803/IJCTT-V17P112</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowledge is only good if it is sound, consistent and complete. The same
holds true for conceptual knowledge, which holds knowledge about concepts and
its association. Conceptual knowledge no matter what format they are
represented in, must be consistent, sound and complete in order to realise its
practical use. This paper discusses consistency, soundness and completeness in
the ambit of conceptual knowledge and the need to consider these factors as
fundamental to the development of conceptual knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6418</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6418</id><created>2014-12-19</created><updated>2015-04-16</updated><authors><author><keyname>Titov</keyname><forenames>Ivan</forenames></author><author><keyname>Khoddam</keyname><forenames>Ehsan</forenames></author></authors><title>Inducing Semantic Representation from Text by Jointly Predicting and
  Factorizing Relations</title><categories>cs.CL cs.LG stat.ML</categories><comments>Accepted as a workshop contribution at ICLR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose a new method to integrate two recent lines of work:
unsupervised induction of shallow semantics (e.g., semantic roles) and
factorization of relations in text and knowledge bases. Our model consists of
two components: (1) an encoding component: a semantic role labeling model which
predicts roles given a rich set of syntactic and lexical features; (2) a
reconstruction component: a tensor factorization model which relies on roles to
predict argument fillers. When the components are estimated jointly to minimize
errors in argument reconstruction, the induced roles largely correspond to
roles defined in annotated resources. Our method performs on par with most
accurate role induction methods on English, even though, unlike these previous
approaches, we do not incorporate any prior linguistic knowledge about the
language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6431</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6431</id><created>2014-10-30</created><authors><author><keyname>Ghannam</keyname><forenames>Adel</forenames></author><author><keyname>Nassar</keyname><forenames>Salwa</forenames></author><author><keyname>Deeb</keyname><forenames>Hesham El</forenames></author></authors><title>Managing shop floor systems Using RFID in the Middle East</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The controllability of a factory is highly dependent on the capability of a
given enterprise planning system to interact with the shop floor, and the
capability of any authorized user to review the operation plans, as well as the
status of any sales order on the shop floor. We have called the concept
E-manufacturing, which is a concept to improve the controllability of a factory
by connecting the Enterprise Resource Planning (ERP) system to the shop floor
control (SFC) system through the Internet. Business systems and plant systems
must be coupled to reduce decision cycle times and increase plant productivity,
as well as eliminating human intervention to improve accuracy and data
availability speed. This paper is the output of a project funded by the
Egyptian Ministry of Scientific Research and Technology, through the Electronic
Research Institute. The overall objective of the ministerial program is
bridging the gap between R&amp;D and manufacturing to fulfill the immediate
technological needs of the Egyptian industry and economy, and building a
national technology know-how expertise. This is achieved, in this project, by
addressing the development of an E-Manufacturing system using available RFID
state-of-the-art technology. The specific objective is developing an end
product for Egyptian industry that manages real-time interaction with back end
ERP systems to correct delays on the shop floor, improving real-time operation,
reducing cost, and allow real-time visibility, through the intelligent usage of
the RFID.The end product is a fully working system tested in the factory of the
manufacturing partner MOBICA. This report addresses the logical and technical
structure,as well as the technology constraints that should be considered in
such systems, and the implementation stages of the project
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6448</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6448</id><created>2014-12-19</created><updated>2015-04-03</updated><authors><author><keyname>Hill</keyname><forenames>Felix</forenames></author><author><keyname>Cho</keyname><forenames>Kyunghyun</forenames></author><author><keyname>Jean</keyname><forenames>Sebastien</forenames></author><author><keyname>Devin</keyname><forenames>Coline</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Embedding Word Similarity with Neural Machine Translation</title><categories>cs.CL</categories><comments>arXiv admin note: text overlap with arXiv:1410.0718</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural language models learn word representations, or embeddings, that
capture rich linguistic and conceptual information. Here we investigate the
embeddings learned by neural machine translation models, a recently-developed
class of neural language model. We show that embeddings from translation models
outperform those learned by monolingual models at tasks that require knowledge
of both conceptual similarity and lexical-syntactic role. We further show that
these effects hold when translating from both English to French and English to
German, and argue that the desirable properties of translation embeddings
should emerge largely independently of the source and target languages.
Finally, we apply a new method for training neural translation models with very
large vocabularies, and show that this vocabulary expansion algorithm results
in minimal degradation of embedding quality. Our embedding spaces can be
queried in an online demo and downloaded from our web page. Overall, our
analyses indicate that translation-based embeddings should be used in
applications that require concepts to be organised according to similarity
and/or lexical function, while monolingual embeddings are better suited to
modelling (nonspecific) inter-word relatedness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6451</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6451</id><created>2014-12-19</created><authors><author><keyname>Wernsdorfer</keyname><forenames>Mark</forenames></author><author><keyname>Schmid</keyname><forenames>Ute</forenames></author></authors><title>Grounding Hierarchical Reinforcement Learning Models for Knowledge
  Transfer</title><categories>cs.LG cs.AI cs.RO</categories><comments>14 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Methods of deep machine learning enable to to reuse low-level representations
efficiently for generating more abstract high-level representations.
Originally, deep learning has been applied passively (e.g., for classification
purposes). Recently, it has been extended to estimate the value of actions for
autonomous agents within the framework of reinforcement learning (RL). Explicit
models of the environment can be learned to augment such a value function.
Although &quot;flat&quot; connectionist methods have already been used for model-based
RL, up to now, only model-free variants of RL have been equipped with methods
from deep learning. We propose a variant of deep model-based RL that enables an
agent to learn arbitrarily abstract hierarchical representations of its
environment. In this paper, we present research on how such hierarchical
representations can be grounded in sensorimotor interaction between an agent
and its environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6452</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6452</id><created>2014-12-19</created><updated>2015-03-31</updated><authors><author><keyname>Nicolae</keyname><forenames>Maria-Irina</forenames></author><author><keyname>Sebban</keyname><forenames>Marc</forenames></author><author><keyname>Habrard</keyname><forenames>Amaury</forenames></author><author><keyname>Gaussier</keyname><forenames>&#xc9;ric</forenames></author><author><keyname>Amini</keyname><forenames>Massih-Reza</forenames></author></authors><title>Algorithmic Robustness for Learning via $(\epsilon, \gamma, \tau)$-Good
  Similarity Functions</title><categories>cs.LG</categories><comments>ICLR 2015 Workshop - accepted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The notion of metric plays a key role in machine learning problems such as
classification, clustering or ranking. However, it is worth noting that there
is a severe lack of theoretical guarantees that can be expected on the
generalization capacity of the classifier associated to a given metric. The
theoretical framework of $(\epsilon, \gamma, \tau)$-good similarity functions
(Balcan et al., 2008) has been one of the first attempts to draw a link between
the properties of a similarity function and those of a linear classifier making
use of it. In this paper, we extend and complete this theory by providing a new
generalization bound for the associated classifier based on the algorithmic
robustness framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6455</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6455</id><created>2014-12-19</created><updated>2015-04-21</updated><authors><author><keyname>Goldberg</keyname><forenames>Paul W.</forenames></author><author><keyname>Turchetta</keyname><forenames>Stefano</forenames></author></authors><title>Query Complexity of Approximate Equilibria in Anonymous Games</title><categories>cs.GT</categories><comments>38 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the computation of equilibria of anonymous games, via algorithms
that may proceed via a sequence of adaptive queries to the game's payoff
function, assumed to be unknown initially. The general topic we consider is
\emph{query complexity}, that is, how many queries are necessary or sufficient
to compute an exact or approximate Nash equilibrium. We present upper and lower
bounds for general anonymous games with $n$ players that share a constant
number of strategies $k$. We also consider the following subclasses: symmetric
games, self-anonymous games, and Lipschitz games.
  The basic kind of query is one that identifies the payoff to a single
specified player in response to a given pure-strategy profile. We compare this
kind of query with ones that respond with a collection of all payoffs in
response to an anonymized profile; we identify cases where this &quot;bundling&quot; of
information leads to loss of query efficiency.
  We show that exact equilibria cannot be found via query-efficient algorithms.
We also give an example of a 2-strategy, 3-player anonymous game that does not
have any exact Nash equilibrium in rational numbers, answering a question posed
in \citep{dp14,cdo14}. Our main results are in the context of general
two-strategy anonymous games. We provide a new randomized query-efficient
polynomial-time approximation scheme that finds a $O(\epsilon)$-approximate
Nash equilibrium querying $\tilde{O}(n^{11/8})$ payoffs for constant values of
$\epsilon$ and at most $\tilde{O}(n^{15/8})$ for any $\epsilon \geq 1 /
\sqrt[4]{n}$. Moreover, we prove that $\Omega(n \log{n})$ payoffs must be
queried in order to find any non-trivial $\epsilon$-well-supported Nash
equilibrium, even by randomized algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6462</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6462</id><created>2014-12-19</created><updated>2015-06-24</updated><authors><author><keyname>Kraker</keyname><forenames>Peter</forenames></author></authors><title>Educational Technology as Seen Through the Eyes of the Readers</title><categories>cs.DL cs.CY</categories><comments>Forthcoming article in the International Journal of Technology
  Enhanced Learning</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, I present the evaluation of a novel knowledge domain
visualization of educational technology. The interactive visualization is based
on readership patterns in the online reference management system Mendeley. It
comprises of 13 topic areas, spanning psychological, pedagogical, and
methodological foundations, learning methods and technologies, and social and
technological developments. The visualization was evaluated with (1) a
qualitative comparison to knowledge domain visualizations based on citations,
and (2) expert interviews. The results show that the co-readership
visualization is a recent representation of pedagogical and psychological
research in educational technology. Furthermore, the co-readership analysis
covers more areas than comparable visualizations based on co-citation patterns.
Areas related to computer science, however, are missing from the co-readership
visualization and more research is needed to explore the interpretations of
size and placement of research areas on the map.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6463</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6463</id><created>2014-12-19</created><authors><author><keyname>Moharir</keyname><forenames>Sharayu</forenames></author><author><keyname>Ghaderi</keyname><forenames>Javad</forenames></author><author><keyname>Sanghavi</keyname><forenames>Sujay</forenames></author><author><keyname>Shakkottai</keyname><forenames>Sanjay</forenames></author></authors><title>Serving Content with Unknown Demand:the High-Dimensional Regime</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we look at content placement in the high-dimensional regime:
there are n servers, and O(n) distinct types of content. Each server can store
and serve O(1) types at any given time. Demands for these content types arrive,
and have to be served in an online fashion; over time, there are a total of
O(n) of these demands. We consider the algorithmic task of content placement:
determining which types of content should be on which server at any given time,
in the setting where the demand statistics (i.e. the relative popularity of
each type of content) are not known a-priori, but have to be inferred from the
very demands we are trying to satisfy. This is the high- dimensional regime
because this scaling (everything being O(n)) prevents consistent estimation of
demand statistics; it models many modern settings where large numbers of users,
servers and videos/webpages interact in this way. We characterize the
performance of any scheme that separates learning and placement (i.e. which use
a portion of the demands to gain some estimate of the demand statistics, and
then uses the same for the remaining demands), showing it is order-wise
strictly suboptimal. We then study a simple adaptive scheme - which myopically
attempts to store the most recently requested content on idle servers - and
show it outperforms schemes that separate learning and placement. Our results
also generalize to the setting where the demand statistics change with time.
Overall, our results demonstrate that separating the estimation of demand, and
the subsequent use of the same, is strictly suboptimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6464</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6464</id><created>2014-12-19</created><authors><author><keyname>Napoli</keyname><forenames>Christian</forenames></author><author><keyname>Pappalardo</keyname><forenames>Giuseppe</forenames></author><author><keyname>Tramontana</keyname><forenames>Emiliano</forenames></author><author><keyname>Marsza&#x142;ek</keyname><forenames>Zbigniew</forenames></author><author><keyname>Po&#x142;ap</keyname><forenames>Dawid</forenames></author><author><keyname>Wo&#x17a;niak</keyname><forenames>Marcin</forenames></author></authors><title>Simplified firefly algorithm for 2D image key-points search</title><categories>cs.NE cs.AI cs.CV</categories><comments>Published version on: 2014 IEEE Symposium on Computational
  Intelligence for Human-like Intelligence</comments><msc-class>68T05, 68T10, 68T45, 68U10, 68W25, 68W99</msc-class><acm-class>I.2.6; I.2.10; I.4.8</acm-class><journal-ref>IEEE Symposium on Computational Intelligence for Human-like
  Intelligence, pp. 118-125, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to identify an object, human eyes firstly search the field of view
for points or areas which have particular properties. These properties are used
to recognise an image or an object. Then this process could be taken as a model
to develop computer algorithms for images identification. This paper proposes
the idea of applying the simplified firefly algorithm to search for key-areas
in 2D images. For a set of input test images the proposed version of firefly
algorithm has been examined. Research results are presented and discussed to
show the efficiency of this evolutionary computation method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6466</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6466</id><created>2014-12-19</created><updated>2015-05-20</updated><authors><author><keyname>Henzinger</keyname><forenames>Monika</forenames></author><author><keyname>Krinninger</keyname><forenames>Sebastian</forenames></author><author><keyname>Loitzenbauer</keyname><forenames>Veronika</forenames></author></authors><title>Finding 2-Edge and 2-Vertex Strongly Connected Components in Quadratic
  Time</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present faster algorithms for computing the 2-edge and 2-vertex strongly
connected components of a directed graph, which are straightforward
generalizations of strongly connected components. While in undirected graphs
the 2-edge and 2-vertex connected components can be found in linear time, in
directed graphs only rather simple $O(m n)$-time algorithms were known. We use
a hierarchical sparsification technique to obtain algorithms that run in time
$O(n^2)$. For 2-edge strongly connected components our algorithm gives the
first running time improvement in 20 years. Additionally we present an $O(m^2 /
\log{n})$-time algorithm for 2-edge strongly connected components, and thus
improve over the $O(m n)$ running time also when $m = O(n)$. Our approach
extends to k-edge and k-vertex strongly connected components for any constant k
with a running time of $O(n^2 \log^2 n)$ for edges and $O(n^3)$ for vertices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6477</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6477</id><created>2014-12-19</created><authors><author><keyname>Paradies</keyname><forenames>Marcus</forenames></author><author><keyname>Lehner</keyname><forenames>Wolfgang</forenames></author><author><keyname>Bornhoevd</keyname><forenames>Christof</forenames></author></authors><title>GRAPHITE: An Extensible Graph Traversal Framework for Relational
  Database Management Systems</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph traversals are a basic but fundamental ingredient for a variety of
graph algorithms and graph-oriented queries. To achieve the best possible query
performance, they need to be implemented at the core of a database management
system that aims at storing, manipulating, and querying graph data.
Increasingly, modern business applications demand native graph query and
processing capabilities for enterprise-critical operations on data stored in
relational database management systems. In this paper we propose an extensible
graph traversal framework (GRAPHITE) as a central graph processing component on
a common storage engine inside a relational database management system.
  We study the influence of the graph topology on the execution time of graph
traversals and derive two traversal algorithm implementations specialized for
different graph topologies and traversal queries. We conduct extensive
experiments on GRAPHITE for a large variety of real-world graph data sets and
input configurations. Our experiments show that the proposed traversal
algorithms differ by up to two orders of magnitude for different input
configurations and therefore demonstrate the need for a versatile framework to
efficiently process graph traversals on a wide range of different graph
topologies and types of queries. Finally, we highlight that the query
performance of our traversal implementations is competitive with those of two
native graph database management systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6482</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6482</id><created>2014-12-19</created><authors><author><keyname>Tsourtis</keyname><forenames>Anastasios</forenames></author><author><keyname>Pantazis</keyname><forenames>Yannis</forenames></author><author><keyname>Katsoulakis</keyname><forenames>Markos A.</forenames></author><author><keyname>Harmandaris</keyname><forenames>Vagelis</forenames></author></authors><title>Parametric Sensitivity Analysis for Stochastic Molecular Systems using
  Information Theoretic Metrics</title><categories>cs.IT math.IT physics.data-an</categories><comments>18 pages, Relative Entropy, Sensitivity Analysis, Fisher Information
  Matrix, Langevin dynamics, Methane Molecular Dynamics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we extend the parametric sensitivity analysis (SA) methodology
proposed in Ref. [Y. Pantazis and M. A. Katsoulakis, J. Chem. Phys. 138, 054115
(2013)] to continuous time and continuous space Markov processes represented by
stochastic differential equations and, particularly, stochastic molecular
dynamics as described by the Langevin equation. The utilized SA method is based
on the computation of the information-theoretic (and thermodynamic) quantity of
relative entropy rate (RER) and the associated Fisher information matrix (FIM)
between path distributions. A major advantage of the pathwise SA method is that
both RER and pathwise FIM depend only on averages of the force field therefore
they are tractable and computable as ergodic averages from a single run of the
molecular dynamics simulation both in equilibrium and in non-equilibrium steady
state regimes. We validate the performance of the extended SA method to two
different molecular stochastic systems, a standard Lennard-Jones fluid and an
all-atom methane liquid and compare the obtained parameter sensitivities with
parameter sensitivities on three popular and well-studied observable functions,
namely, the radial distribution function, the mean squared displacement and the
pressure. Results show that the RER-based sensitivities are highly correlated
with the observable-based sensitivities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6483</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6483</id><created>2014-12-19</created><authors><author><keyname>Leonard</keyname><forenames>Andrew</forenames></author><author><keyname>Morgan</keyname><forenames>Huw</forenames></author></authors><title>Temperature diagnostics of the solar atmosphere using SunPy</title><categories>astro-ph.SR cs.CE</categories><comments>Part of the Proceedings of the 7th European Conference on Python in
  Science (EuroSciPy 2014), Pierre de Buyl and Nelle Varoquaux editors, (2014)</comments><report-no>euroscipy-proceedings2014-03</report-no><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The solar atmosphere is a hot (about 1MK), magnetised plasma of great
interest to physicists. There have been many previous studies of the
temperature of the Sun's atmosphere (Plowman2012, Wit2012, Hannah2012,
Aschwanden2013, etc.). Almost all of these studies use the SolarSoft software
package written in the commercial Interactive Data Language (IDL), which has
been the standard language for solar physics. The SunPy project aims to provide
an open-source library for solar physics. This work presents (to the authors'
knowledge) the first study of its type to use SunPy rather than SolarSoft. This
work uses SunPy to process multi-wavelength solar observations made by the
Atmospheric Imaging Assembly (AIA) instrument aboard the Solar Dynamics
Observatory (SDO) and produce temperature maps of the Sun's atmosphere. The
method uses SunPy's utilities for querying databases of solar events,
downloading solar image data, storing and processing images as spatially aware
Map objects, and tracking solar features as the Sun rotates. An essential
consideration in developing this software is computational efficiency due to
the large amount of data collected by AIA/SDO, and in anticipating new solar
missions which will result in even larger sets of data. An overview of the
method and implementation is given, along with tests involving synthetic data
and examples of results using real data for various regions in the Sun's
atmosphere.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6493</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6493</id><created>2014-12-19</created><authors><author><keyname>Yang</keyname><forenames>Zichao</forenames></author><author><keyname>Smola</keyname><forenames>Alexander J.</forenames></author><author><keyname>Song</keyname><forenames>Le</forenames></author><author><keyname>Wilson</keyname><forenames>Andrew Gordon</forenames></author></authors><title>A la Carte - Learning Fast Kernels</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kernel methods have great promise for learning rich statistical
representations of large modern datasets. However, compared to neural networks,
kernel methods have been perceived as lacking in scalability and flexibility.
We introduce a family of fast, flexible, lightly parametrized and general
purpose kernel learning methods, derived from Fastfood basis function
expansions. We provide mechanisms to learn the properties of groups of spectral
frequencies in these expansions, which require only O(mlogd) time and O(m)
memory, for m basis functions and d input dimensions. We show that the proposed
methods can learn a wide class of kernels, outperforming the alternatives in
accuracy, speed, and memory consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6494</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6494</id><created>2014-12-18</created><authors><author><keyname>Albeladi</keyname><forenames>Abdulrhman</forenames></author><author><keyname>Almessabi</keyname><forenames>Ahmed</forenames></author><author><keyname>Abozkhar</keyname><forenames>Aber</forenames></author><author><keyname>Mohamed</keyname><forenames>Huda</forenames></author><author><keyname>Thomas</keyname><forenames>Jilson</forenames></author><author><keyname>Alomari</keyname><forenames>Zakaria</forenames></author></authors><title>Toward Refactoring of DMARF and GIPSY Case Studies a Team 7 SOEN6471-S14
  Project Report</title><categories>cs.SE</categories><comments>25 pages; 31 figures; 4 tables</comments><proxy>Serguei Mokhov</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software architecture is defined as the process of a well-structured solution
that meets all of the technical and operational requirements, as well as
improving the quality attributes of the system such as readability,
Reliability, maintainability, and performance. It involves a series of design
decisions that can have a considerable impact on the systems quality
attributes, and on the overall success of the application. In this work, we
start with analysis and investigation of two open source software (OSS)
platforms DMARF and GIPSY, predominantly implemented in Java. Many research
papers have been studied in order to gain more insights and clear background
about their architectures, enhancement, evolution, challenges, and features.
Subsequently, we extract and find their needs, high-level requirements, and
architectural structures which lead to important design decisions and thus
influence their quality attributes. Primarily, we reversed engineering each
system0s source code to reconstruct its domain model and class diagram model.
We tried to achieve the traceability between requirements and other design
artifacts to be consistent. Additionally, we conducted both manual and
automated refactoring techniques to get rid of some existing code smells to end
up with more readable and understandable code without affecting its observable
behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6496</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6496</id><created>2014-12-19</created><authors><author><keyname>Meunier</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Pradeau</keyname><forenames>Thomas</forenames></author></authors><title>Computing solutions of the multiclass network equilibrium problem with
  affine cost functions</title><categories>cs.GT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a nonatomic congestion game on a graph, with several classes of
players. Each player wants to go from its origin vertex to its destination
vertex at the minimum cost and all players of a given class share the same
characteristics: cost functions on each arc, and origin-destination pair. Under
some mild conditions, it is known that a Nash equilibrium exists, but the
computation of an equilibrium in the multiclass case is an open problem for
general functions. We consider the specific case where the cost functions are
affine. We show that this problem is polynomially solvable when the number of
vertices and the number of classes are fixed. In particular, it shows that the
parallel-link case with a fixed number of classes is polynomially solvable. On
a more practical side, we propose an extension of Lemke's algorithm able to
solve this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6502</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6502</id><created>2014-12-19</created><updated>2015-04-21</updated><authors><author><keyname>Pramod</keyname><forenames>Siddharth</forenames></author><author><keyname>Page</keyname><forenames>Adam</forenames></author><author><keyname>Mohsenin</keyname><forenames>Tinoosh</forenames></author><author><keyname>Oates</keyname><forenames>Tim</forenames></author></authors><title>Detecting Epileptic Seizures from EEG Data using Neural Networks</title><categories>cs.LG cs.NE q-bio.NC</categories><comments>This paper has been withdrawn by the authors due to an error
  discovered in the experiments</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the use of neural networks trained with dropout in predicting
epileptic seizures from electroencephalographic data (scalp EEG). The input to
the neural network is a 126 feature vector containing 9 features for each of
the 14 EEG channels obtained over 1-second, non-overlapping windows. The models
in our experiments achieved high sensitivity and specificity on patient records
not used in the training process. This is demonstrated using
leave-one-out-cross-validation across patient records, where we hold out one
patient's record as the test set and use all other patients' records for
training; repeating this procedure for all patients in the database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6504</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6504</id><created>2014-12-19</created><updated>2015-05-07</updated><authors><author><keyname>Fragkiadaki</keyname><forenames>Katerina</forenames></author><author><keyname>Arbelaez</keyname><forenames>Pablo</forenames></author><author><keyname>Felsen</keyname><forenames>Panna</forenames></author><author><keyname>Malik</keyname><forenames>Jitendra</forenames></author></authors><title>Learning to Segment Moving Objects in Videos</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We segment moving objects in videos by ranking spatio-temporal segment
proposals according to &quot;moving objectness&quot;: how likely they are to contain a
moving object. In each video frame, we compute segment proposals using multiple
figure-ground segmentations on per frame motion boundaries. We rank them with a
Moving Objectness Detector trained on image and motion fields to detect moving
objects and discard over/under segmentations or background parts of the scene.
We extend the top ranked segments into spatio-temporal tubes using random
walkers on motion affinities of dense point trajectories. Our final tube
ranking consistently outperforms previous segmentation methods in the two
largest video segmentation benchmarks currently available, for any number of
proposals. Further, our per frame moving object proposals increase the
detection rate up to 7\% over previous state-of-the-art static proposal
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6505</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6505</id><created>2014-12-19</created><updated>2015-05-06</updated><authors><author><keyname>Ryoo</keyname><forenames>M. S.</forenames></author><author><keyname>Rothrock</keyname><forenames>Brandon</forenames></author><author><keyname>Matthies</keyname><forenames>Larry</forenames></author></authors><title>Pooled Motion Features for First-Person Videos</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a new feature representation for first-person
videos. In first-person video understanding (e.g., activity recognition), it is
very important to capture both entire scene dynamics (i.e., egomotion) and
salient local motion observed in videos. We describe a representation framework
based on time series pooling, which is designed to abstract
short-term/long-term changes in feature descriptor elements. The idea is to
keep track of how descriptor values are changing over time and summarize them
to represent motion in the activity video. The framework is general, handling
any types of per-frame feature descriptors including conventional motion
descriptors like histogram of optical flows (HOF) as well as appearance
descriptors from more recent convolutional neural networks (CNN). We
experimentally confirm that our approach clearly outperforms previous feature
representations including bag-of-visual-words and improved Fisher vector (IFV)
when using identical underlying feature descriptors. We also confirm that our
feature representation has superior performance to existing state-of-the-art
features like local spatio-temporal features and Improved Trajectory Features
(originally developed for 3rd-person videos) when handling first-person videos.
Multiple first-person activity datasets were tested under various settings to
confirm these findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6506</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6506</id><created>2014-12-19</created><authors><author><keyname>Xie</keyname><forenames>Pengtao</forenames></author><author><keyname>Xing</keyname><forenames>Eric</forenames></author></authors><title>Cauchy Principal Component Analysis</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Principal Component Analysis (PCA) has wide applications in machine learning,
text mining and computer vision. Classical PCA based on a Gaussian noise model
is fragile to noise of large magnitude. Laplace noise assumption based PCA
methods cannot deal with dense noise effectively. In this paper, we propose
Cauchy Principal Component Analysis (Cauchy PCA), a very simple yet effective
PCA method which is robust to various types of noise. We utilize Cauchy
distribution to model noise and derive Cauchy PCA under the maximum likelihood
estimation (MLE) framework with low rank constraint. Our method can robustly
estimate the low rank matrix regardless of whether noise is large or small,
dense or sparse. We analyze the robustness of Cauchy PCA from a robust
statistics view and present an efficient singular value projection optimization
method. Experimental results on both simulated data and real applications
demonstrate the robustness of Cauchy PCA to various noise patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6507</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6507</id><created>2014-12-19</created><authors><author><keyname>Aaronson</keyname><forenames>Scott</forenames></author><author><keyname>Bouland</keyname><forenames>Adam</forenames></author><author><keyname>Fitzsimons</keyname><forenames>Joseph</forenames></author><author><keyname>Lee</keyname><forenames>Mitchell</forenames></author></authors><title>The space &quot;just above&quot; BQP</title><categories>quant-ph cs.CC</categories><comments>24 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the space &quot;just above&quot; BQP by defining a complexity class PDQP
(Product Dynamical Quantum Polynomial time) which is larger than BQP but does
not contain NP relative to an oracle. The class is defined by imagining that
quantum computers can perform measurements that do not collapse the
wavefunction. This (non-physical) model of computation can efficiently solve
problems such as Graph Isomorphism and Approximate Shortest Vector which are
believed to be intractable for quantum computers. Furthermore, it can search an
unstructured N-element list in $\tilde O$(N^{1/3}) time, but no faster than
{\Omega}(N^{1/4}), and hence cannot solve NP-hard problems in a black box
manner. In short, this model of computation is more powerful than standard
quantum computation, but only slightly so.
  Our work is inspired by previous work of Aaronson on the power of sampling
the histories of hidden variables. However Aaronson's work contains an error in
its proof of the lower bound for search, and hence it is unclear whether or not
his model allows for search in logarithmic time. Our work can be viewed as a
conceptual simplification of Aaronson's approach, with a provable polynomial
lower bound for search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6514</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6514</id><created>2014-12-19</created><updated>2015-04-19</updated><authors><author><keyname>Janzamin</keyname><forenames>Majid</forenames></author><author><keyname>Sedghi</keyname><forenames>Hanie</forenames></author><author><keyname>Anandkumar</keyname><forenames>Anima</forenames></author></authors><title>Score Function Features for Discriminative Learning</title><categories>cs.LG stat.ML</categories><comments>Accepted as a workshop contribution at ICLR 2015. A longer version of
  this work is also available on arXiv: http://arxiv.org/abs/1412.2863</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature learning forms the cornerstone for tackling challenging learning
problems in domains such as speech, computer vision and natural language
processing. In this paper, we consider a novel class of matrix and
tensor-valued features, which can be pre-trained using unlabeled samples. We
present efficient algorithms for extracting discriminative information, given
these pre-trained features and labeled samples for any related task. Our class
of features are based on higher-order score functions, which capture local
variations in the probability density function of the input. We establish a
theoretical framework to characterize the nature of discriminative information
that can be extracted from score-function features, when used in conjunction
with labeled samples. We employ efficient spectral decomposition algorithms (on
matrices and tensors) for extracting discriminative components. The advantage
of employing tensor-valued features is that we can extract richer
discriminative information in the form of an overcomplete representations.
Thus, we present a novel framework for employing generative models of the input
for discriminative learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6534</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6534</id><created>2014-12-19</created><updated>2015-02-10</updated><authors><author><keyname>Berisha</keyname><forenames>Visar</forenames></author><author><keyname>Wisler</keyname><forenames>Alan</forenames></author><author><keyname>Hero</keyname><forenames>Alfred O.</forenames></author><author><keyname>Spanias</keyname><forenames>Andreas</forenames></author></authors><title>Empirically Estimable Classification Bounds Based on a New Divergence
  Measure</title><categories>cs.IT math.IT stat.ML</categories><comments>12 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information divergence functions play a critical role in statistics and
information theory. In this paper we show that a non-parametric f-divergence
measure can be used to provide improved bounds on the minimum binary
classification probability of error for the case when the training and test
data are drawn from the same distribution and for the case where there exists
some mismatch between training and test distributions. We confirm the
theoretical results by designing feature selection algorithms using the
criteria from these bounds and by evaluating the algorithms on a series of
pathological speech classification tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6537</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6537</id><created>2014-12-19</created><updated>2015-02-25</updated><authors><author><keyname>Simo-Serra</keyname><forenames>Edgar</forenames></author><author><keyname>Trulls</keyname><forenames>Eduard</forenames></author><author><keyname>Ferraz</keyname><forenames>Luis</forenames></author><author><keyname>Kokkinos</keyname><forenames>Iasonas</forenames></author><author><keyname>Moreno-Noguer</keyname><forenames>Francesc</forenames></author></authors><title>Fracking Deep Convolutional Image Descriptors</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a novel framework for learning local image
descriptors in a discriminative manner. For this purpose we explore a siamese
architecture of Deep Convolutional Neural Networks (CNN), with a Hinge
embedding loss on the L2 distance between descriptors. Since a siamese
architecture uses pairs rather than single image patches to train, there exist
a large number of positive samples and an exponential number of negative
samples. We propose to explore this space with a stochastic sampling of the
training set, in combination with an aggressive mining strategy over both the
positive and negative samples which we denote as &quot;fracking&quot;. We perform a
thorough evaluation of the architecture hyper-parameters, and demonstrate large
performance gains compared to both standard CNN learning strategies,
hand-crafted image descriptors like SIFT, and the state-of-the-art on learned
descriptors: up to 2.5x vs SIFT and 1.5x vs the state-of-the-art in terms of
the area under the curve (AUC) of the Precision-Recall curve.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6544</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6544</id><created>2014-12-19</created><updated>2015-05-21</updated><authors><author><keyname>Goodfellow</keyname><forenames>Ian J.</forenames></author><author><keyname>Vinyals</keyname><forenames>Oriol</forenames></author><author><keyname>Saxe</keyname><forenames>Andrew M.</forenames></author></authors><title>Qualitatively characterizing neural network optimization problems</title><categories>cs.NE cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Training neural networks involves solving large-scale non-convex optimization
problems. This task has long been believed to be extremely difficult, with fear
of local minima and other obstacles motivating a variety of schemes to improve
optimization, such as unsupervised pretraining. However, modern neural networks
are able to achieve negligible training error on complex tasks, using only
direct training with stochastic gradient descent. We introduce a simple
analysis technique to look for evidence that such networks are overcoming local
optima. We find that, in fact, on a straight path from initialization to
solution, a variety of state of the art neural networks never encounter any
significant obstacles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6545</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6545</id><created>2014-12-19</created><authors><author><keyname>Fillottrani</keyname><forenames>Pablo R.</forenames></author><author><keyname>Keet</keyname><forenames>C. Maria</forenames></author></authors><title>KF metamodel formalization</title><categories>cs.AI cs.DB cs.LO</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The KF metamodel is a comprehensive unifying metamodel covering the static
structural entities and constraints of UML Class Diagrams (v2.4.1), ER, EER,
ORM, and ORM2, and intended to boost interoperability of common conceptual data
modelling languages. It was originally designed in UML with textual
constraints, and in this report we present its formalisations in FOL and OWL,
which accompanies the paper that describes, discusses, and analyses the KF
metamodel in detail. These new formalizations contribute to give a precise
meaning to the metamodel, to understand its complexity properties and to
provide a basis for future implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6546</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6546</id><created>2014-12-19</created><authors><author><keyname>Etesami</keyname><forenames>Seyed Rasoul</forenames></author><author><keyname>Basar</keyname><forenames>Tamer</forenames></author></authors><title>Game-Theoretic Analysis of the Hegselmann-Krause Model for Opinion
  Dynamics in Finite Dimensions</title><categories>cs.GT cs.DM cs.MA cs.RO math.OC</categories><comments>The paper is accepted in IEEE Transactions on Automatic Control and
  will appear soon</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the Hegselmann-Krause model for opinion dynamics and study the
evolution of the system under various settings. We first analyze the
termination time of the synchronous Hegselmann-Krause dynamics in arbitrary
finite dimensions and show that the termination time in general only depends on
the number of agents involved in the dynamics. To the best of our knowledge,
that is the sharpest bound for the termination time of such dynamics that
removes dependency of the termination time from the dimension of the ambient
space. This answers an open question in [1] on how to obtain a tighter upper
bound for the termination time. Furthermore, we study the asynchronous
Hegselmann-Krause model from a novel game-theoretic approach and show that the
evolution of an asynchronous Hegselmann-Krause model is equivalent to a
sequence of best response updates in a well-designed potential game. We then
provide a polynomial upper bound for the expected time and expected number of
switching topologies until the dynamic reaches an arbitrarily small
neighborhood of its equilibrium points, provided that the agents update
uniformly at random. This is a step toward analysis of heterogeneous
Hegselmann-Krause dynamics. Finally, we consider the heterogeneous
Hegselmann-Krause dynamics and provide a necessary condition for the finite
termination time of such dynamics. In particular, we sketch some future
directions toward more detailed analysis of the heterogeneous Hegselmann-Krause
model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6547</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6547</id><created>2014-12-19</created><updated>2015-07-05</updated><authors><author><keyname>Mineiro</keyname><forenames>Paul</forenames></author><author><keyname>Karampatziakis</keyname><forenames>Nikos</forenames></author></authors><title>Fast Label Embeddings via Randomized Linear Algebra</title><categories>cs.LG</categories><comments>To appear in the proceedings of the ECML/PKDD 2015 conference.
  Reference implementation available at https://github.com/pmineiro/randembed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many modern multiclass and multilabel problems are characterized by
increasingly large output spaces. For these problems, label embeddings have
been shown to be a useful primitive that can improve computational and
statistical efficiency. In this work we utilize a correspondence between rank
constrained estimation and low dimensional label embeddings that uncovers a
fast label embedding algorithm which works in both the multiclass and
multilabel settings. The result is a randomized algorithm whose running time is
exponentially faster than naive algorithms. We demonstrate our techniques on
two large-scale public datasets, from the Large Scale Hierarchical Text
Challenge and the Open Directory Project, where we obtain state of the art
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6548</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6548</id><created>2014-12-19</created><authors><author><keyname>Sengupta</keyname><forenames>Abhronil</forenames></author><author><keyname>Azim</keyname><forenames>Zubair Al</forenames></author><author><keyname>Fong</keyname><forenames>Xuanyao</forenames></author><author><keyname>Roy</keyname><forenames>Kaushik</forenames></author></authors><title>Spin-Orbit Torque Induced Spike-Timing Dependent Plasticity</title><categories>cs.ET</categories><doi>10.1063/1.4914111</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nanoelectronic devices that mimic the functionality of synapses are a crucial
requirement for performing cortical simulations of the brain. In this work we
propose a ferromagnet-heavy metal heterostructure that employs spin-orbit
torque to implement Spike-Timing Dependent Plasticity. The proposed device
offers the advantage of decoupled spike transmission and programming current
paths, thereby leading to reliable operation during online learning. Possible
arrangement of such devices in a crosspoint architecture can pave the way for
ultra-dense neural networks. Simulation studies indicate that the device has
the potential of achieving pico-Joule level energy consumption (maximum 2 pJ
per synaptic event) which is comparable to the energy consumption for synaptic
events in biological synapses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6550</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6550</id><created>2014-12-19</created><updated>2015-03-27</updated><authors><author><keyname>Romero</keyname><forenames>Adriana</forenames></author><author><keyname>Ballas</keyname><forenames>Nicolas</forenames></author><author><keyname>Kahou</keyname><forenames>Samira Ebrahimi</forenames></author><author><keyname>Chassang</keyname><forenames>Antoine</forenames></author><author><keyname>Gatta</keyname><forenames>Carlo</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>FitNets: Hints for Thin Deep Nets</title><categories>cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While depth tends to improve network performances, it also makes
gradient-based training more difficult since deeper networks tend to be more
non-linear. The recently proposed knowledge distillation approach is aimed at
obtaining small and fast-to-execute models, and it has shown that a student
network could imitate the soft output of a larger teacher network or ensemble
of networks. In this paper, we extend this idea to allow the training of a
student that is deeper and thinner than the teacher, using not only the outputs
but also the intermediate representations learned by the teacher as hints to
improve the training process and final performance of the student. Because the
student intermediate hidden layer will generally be smaller than the teacher's
intermediate hidden layer, additional parameters are introduced to map the
student hidden layer to the prediction of the teacher hidden layer. This allows
one to train deeper students that can generalize better or run faster, a
trade-off that is controlled by the chosen student capacity. For example, on
CIFAR-10, a deep student network with almost 10.4 times less parameters
outperforms a larger, state-of-the-art teacher network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6553</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6553</id><created>2014-12-19</created><updated>2015-04-24</updated><authors><author><keyname>Lebedev</keyname><forenames>Vadim</forenames></author><author><keyname>Ganin</keyname><forenames>Yaroslav</forenames></author><author><keyname>Rakhuba</keyname><forenames>Maksim</forenames></author><author><keyname>Oseledets</keyname><forenames>Ivan</forenames></author><author><keyname>Lempitsky</keyname><forenames>Victor</forenames></author></authors><title>Speeding-up Convolutional Neural Networks Using Fine-tuned
  CP-Decomposition</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a simple two-step approach for speeding up convolution layers
within large convolutional neural networks based on tensor decomposition and
discriminative fine-tuning. Given a layer, we use non-linear least squares to
compute a low-rank CP-decomposition of the 4D convolution kernel tensor into a
sum of a small number of rank-one tensors. At the second step, this
decomposition is used to replace the original convolutional layer with a
sequence of four convolutional layers with small kernels. After such
replacement, the entire network is fine-tuned on the training data using
standard backpropagation process.
  We evaluate this approach on two CNNs and show that it is competitive with
previous approaches, leading to higher obtained CPU speedups at the cost of
lower accuracy drops for the smaller of the two networks. Thus, for the
36-class character classification CNN, our approach obtains a 8.5x CPU speedup
of the whole network with only minor accuracy drop (1% from 91% to 90%). For
the standard ImageNet architecture (AlexNet), the approach speeds up the second
convolution layer by a factor of 4x at the cost of $1\%$ increase of the
overall top-5 classification error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6558</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6558</id><created>2014-12-19</created><updated>2015-02-27</updated><authors><author><keyname>Sussillo</keyname><forenames>David</forenames></author><author><keyname>Abbott</keyname><forenames>L. F.</forenames></author></authors><title>Random Walk Initialization for Training Very Deep Feedforward Networks</title><categories>cs.NE cs.LG stat.ML</categories><comments>10 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Training very deep networks is an important open problem in machine learning.
One of many difficulties is that the norm of the back-propagated error gradient
can grow or decay exponentially. Here we show that training very deep
feed-forward networks (FFNs) is not as difficult as previously thought. Unlike
when back-propagation is applied to a recurrent network, application to an FFN
amounts to multiplying the error gradient by a different random matrix at each
layer. We show that the successive application of correctly scaled random
matrices to an initial vector results in a random walk of the log of the norm
of the resulting vectors, and we compute the scaling that makes this walk
unbiased. The variance of the random walk grows only linearly with network
depth and is inversely proportional to the size of each layer. Practically,
this implies a gradient whose log-norm scales with the square root of the
network depth and shows that the vanishing gradient problem can be mitigated by
increasing the width of the layers. Mathematical analyses and experimental
results using stochastic gradient descent to optimize tasks related to the
MNIST and TIMIT datasets are provided to support these claims. Equations for
the optimal matrix scaling are provided for the linear and ReLU cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6563</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6563</id><created>2014-12-19</created><updated>2015-04-13</updated><authors><author><keyname>Warde-Farley</keyname><forenames>David</forenames></author><author><keyname>Rabinovich</keyname><forenames>Andrew</forenames></author><author><keyname>Anguelov</keyname><forenames>Dragomir</forenames></author></authors><title>Self-informed neural network structure learning</title><categories>stat.ML cs.CV cs.LG cs.NE</categories><comments>Updated with accepted workshop contribution header</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of large scale, multi-label visual recognition with a
large number of possible classes. We propose a method for augmenting a trained
neural network classifier with auxiliary capacity in a manner designed to
significantly improve upon an already well-performing model, while minimally
impacting its computational footprint. Using the predictions of the network
itself as a descriptor for assessing visual similarity, we define a
partitioning of the label space into groups of visually similar entities. We
then augment the network with auxilliary hidden layer pathways with
connectivity only to these groups of label units. We report a significant
improvement in mean average precision on a large-scale object recognition task
with the augmented model, while increasing the number of multiply-adds by less
than 3%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6564</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6564</id><created>2014-12-19</created><updated>2015-04-10</updated><authors><author><keyname>Maddison</keyname><forenames>Chris J.</forenames></author><author><keyname>Huang</keyname><forenames>Aja</forenames></author><author><keyname>Sutskever</keyname><forenames>Ilya</forenames></author><author><keyname>Silver</keyname><forenames>David</forenames></author></authors><title>Move Evaluation in Go Using Deep Convolutional Neural Networks</title><categories>cs.LG cs.NE</categories><comments>Minor edits and included captures in Figure 2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The game of Go is more challenging than other board games, due to the
difficulty of constructing a position or move evaluation function. In this
paper we investigate whether deep convolutional networks can be used to
directly represent and learn this knowledge. We train a large 12-layer
convolutional neural network by supervised learning from a database of human
professional games. The network correctly predicts the expert move in 55% of
positions, equalling the accuracy of a 6 dan human player. When the trained
convolutional network was used directly to play games of Go, without any
search, it beat the traditional search program GnuGo in 97% of games, and
matched the performance of a state-of-the-art Monte-Carlo tree search that
simulates a million positions per move.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6565</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6565</id><created>2014-12-19</created><authors><author><keyname>Bravo</keyname><forenames>Mario</forenames></author><author><keyname>Mertikopoulos</keyname><forenames>Panayotis</forenames></author></authors><title>On the Robustness of Learning in Games with Stochastically Perturbed
  Payoff Observations</title><categories>math.OC cs.GT math.PR stat.ML</categories><comments>29 pages, 4 figures</comments><msc-class>60H10, 91A26 (Primary) 60H30, 60J70, 91A22 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a general class of game-theoretic learning dynamics in the presence
of random payoff disturbances and observation noise, and we provide a unified
framework that extends several rationality properties of the (stochastic)
replicator dynamics and other game dynamics. In the unilateral case, we show
that the stochastic dynamics under study lead to no regret, irrespective of the
noise level. In the multi-player case, we find that dominated strategies become
extinct (a.s.) and strict Nash equilibria remain stochastically asymptotically
stable - again, independently of the perturbations' magnitude. Finally, we
establish an averaging principle for 2-player games and we show that the
empirical distribution of play converges to Nash equilibrium in zero-sum games
under any noise level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6567</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6567</id><created>2014-12-19</created><updated>2015-04-02</updated><authors><author><keyname>Trappenberg</keyname><forenames>Thomas</forenames></author><author><keyname>Hollensen</keyname><forenames>Paul</forenames></author><author><keyname>Hartono</keyname><forenames>Pitoyo</forenames></author></authors><title>Classifier with Hierarchical Topographical Maps as Internal
  Representation</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study we want to connect our previously proposed context-relevant
topographical maps with the deep learning community. Our architecture is a
classifier with hidden layers that are hierarchical two-dimensional
topographical maps. These maps differ from the conventional self-organizing
maps in that their organizations are influenced by the context of the data
labels in a top-down manner. In this way bottom-up and top-down learning are
combined in a biologically relevant representational learning setting. Compared
to our previous work, we are here specifically elaborating the model in a more
challenging setting compared to our previous experiments and to advance more
hidden representation layers to bring our discussions into the context of deep
representational learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6568</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6568</id><created>2014-12-19</created><updated>2015-04-15</updated><authors><author><keyname>Dinu</keyname><forenames>Georgiana</forenames></author><author><keyname>Lazaridou</keyname><forenames>Angeliki</forenames></author><author><keyname>Baroni</keyname><forenames>Marco</forenames></author></authors><title>Improving zero-shot learning by mitigating the hubness problem</title><categories>cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The zero-shot paradigm exploits vector-based word representations extracted
from text corpora with unsupervised methods to learn general mapping functions
from other feature spaces onto word space, where the words associated to the
nearest neighbours of the mapped vectors are used as their linguistic labels.
We show that the neighbourhoods of the mapped elements are strongly polluted by
hubs, vectors that tend to be near a high proportion of items, pushing their
correct labels down the neighbour list. After illustrating the problem
empirically, we propose a simple method to correct it by taking the proximity
distribution of potential neighbours across many mapped vectors into account.
We show that this correction leads to consistent improvements in realistic
zero-shot experiments in the cross-lingual, image labeling and image retrieval
domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6570</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6570</id><created>2014-12-19</created><authors><author><keyname>Qiu</keyname><forenames>Robert C.</forenames></author></authors><title>The Foundation of Big Data: Experiments, Formulation, and Applications</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The central theme of this talk is to promote the non-asymptotic statistical
viewpoint in the context of massive datasets. The classical viewpoint breaks
down when the data size becomes large.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6572</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6572</id><created>2014-12-19</created><updated>2015-03-20</updated><authors><author><keyname>Goodfellow</keyname><forenames>Ian J.</forenames></author><author><keyname>Shlens</keyname><forenames>Jonathon</forenames></author><author><keyname>Szegedy</keyname><forenames>Christian</forenames></author></authors><title>Explaining and Harnessing Adversarial Examples</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several machine learning models, including neural networks, consistently
misclassify adversarial examples---inputs formed by applying small but
intentionally worst-case perturbations to examples from the dataset, such that
the perturbed input results in the model outputting an incorrect answer with
high confidence. Early attempts at explaining this phenomenon focused on
nonlinearity and overfitting. We argue instead that the primary cause of neural
networks' vulnerability to adversarial perturbation is their linear nature.
This explanation is supported by new quantitative results while giving the
first explanation of the most intriguing fact about them: their generalization
across architectures and training sets. Moreover, this view yields a simple and
fast method of generating adversarial examples. Using this approach to provide
examples for adversarial training, we reduce the test set error of a maxout
network on the MNIST dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6574</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6574</id><created>2014-12-19</created><updated>2015-04-10</updated><authors><author><keyname>Razavian</keyname><forenames>Ali Sharif</forenames></author><author><keyname>Sullivan</keyname><forenames>Josephine</forenames></author><author><keyname>Maki</keyname><forenames>Atsuto</forenames></author><author><keyname>Carlsson</keyname><forenames>Stefan</forenames></author></authors><title>A Baseline for Visual Instance Retrieval with Deep Convolutional
  Networks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a simple pipeline for visual instance retrieval
exploiting image representations based on convolutional networks (ConvNets),
and demonstrates that ConvNet image representations outperform other
state-of-the-art image representations on six standard image retrieval datasets
for the first time. Unlike existing design choices, our image representation
does not require fine-tuning or learning with data similar to the test set.
Furthermore, we consider the challenge &quot;Can you construct a tiny image
representation with memory requirements less than or equal to 32 bytes that can
successfully perform retrieval?&quot; We report the promising performance of our
tiny ConvNet based representation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6575</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6575</id><created>2014-12-19</created><updated>2015-08-29</updated><authors><author><keyname>Yang</keyname><forenames>Bishan</forenames></author><author><keyname>Yih</keyname><forenames>Wen-tau</forenames></author><author><keyname>He</keyname><forenames>Xiaodong</forenames></author><author><keyname>Gao</keyname><forenames>Jianfeng</forenames></author><author><keyname>Deng</keyname><forenames>Li</forenames></author></authors><title>Embedding Entities and Relations for Learning and Inference in Knowledge
  Bases</title><categories>cs.CL</categories><comments>12 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider learning representations of entities and relations in KBs using
the neural-embedding approach. We show that most existing models, including NTN
(Socher et al., 2013) and TransE (Bordes et al., 2013b), can be generalized
under a unified learning framework, where entities are low-dimensional vectors
learned from a neural network and relations are bilinear and/or linear mapping
functions. Under this framework, we compare a variety of embedding models on
the link prediction task. We show that a simple bilinear formulation achieves
new state-of-the-art results for the task (achieving a top-10 accuracy of 73.2%
vs. 54.7% by TransE on Freebase). Furthermore, we introduce a novel approach
that utilizes the learned relation embeddings to mine logical rules such as
&quot;BornInCity(a,b) and CityInCountry(b,c) =&gt; Nationality(a,c)&quot;. We find that
embeddings learned from the bilinear objective are particularly good at
capturing relational semantics and that the composition of relations is
characterized by matrix multiplication. More interestingly, we demonstrate that
our embedding-based rule extraction approach successfully outperforms a
state-of-the-art confidence-based rule mining approach in mining Horn rules
that involve compositional reasoning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6577</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6577</id><created>2014-12-19</created><updated>2015-05-02</updated><authors><author><keyname>&#x130;rsoy</keyname><forenames>Ozan</forenames></author><author><keyname>Cardie</keyname><forenames>Claire</forenames></author></authors><title>Modeling Compositionality with Multiplicative Recurrent Neural Networks</title><categories>cs.LG cs.CL stat.ML</categories><comments>10 pages, 2 figures, published at ICLR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the multiplicative recurrent neural network as a general model for
compositional meaning in language, and evaluate it on the task of fine-grained
sentiment analysis. We establish a connection to the previously investigated
matrix-space models for compositionality, and show they are special cases of
the multiplicative recurrent net. Our experiments show that these models
perform comparably or better than Elman-type additive recurrent neural networks
and outperform matrix-space models on a standard fine-grained sentiment
analysis corpus. Furthermore, they yield comparable results to structural deep
models on the recently published Stanford Sentiment Treebank without the need
for generating parse trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6579</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6579</id><created>2014-12-19</created><updated>2015-02-08</updated><authors><author><keyname>Nakata</keyname><forenames>Keiko</forenames><affiliation>Institute of Cybernetics</affiliation></author><author><keyname>Uustalu</keyname><forenames>Tarmo</forenames><affiliation>Institute of Cybernetics</affiliation></author></authors><title>A Hoare logic for the coinductive trace-based big-step semantics of
  While</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 11, Issue 1 (February
  11, 2015) lmcs:692</journal-ref><doi>10.2168/LMCS-11(1:1)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In search for a foundational framework for reasoning about observable
behavior of programs that may not terminate, we have previously devised a
trace-based big-step semantics for While. In this semantics, both traces and
evaluation (relating initial states of program runs to traces they produce) are
defined coinductively. On terminating runs, this semantics agrees with the
standard inductive state-based semantics. Here we present a Hoare logic
counterpart of our coinductive trace-based semantics and prove it sound and
complete. Our logic subsumes the standard partial-correctness state-based Hoare
logic as well as the total-correctness variation: they are embeddable. In the
converse direction, projections can be constructed: a derivation of a Hoare
triple in our trace-based logic can be translated into a derivation in the
state-based logic of a translated, weaker Hoare triple. Since we work with a
constructive underlying logic, the range of program properties we can reason
about has a fine structure; in particular, we can distinguish between
termination and nondivergence, e.g., unbounded classically total search fails
to be terminating, but is nonetheless nondivergent. Our meta-theory is entirely
constructive as well, and we have formalized it in Coq.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6581</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6581</id><created>2014-12-19</created><updated>2015-06-15</updated><authors><author><keyname>Fabius</keyname><forenames>Otto</forenames></author><author><keyname>van Amersfoort</keyname><forenames>Joost R.</forenames></author></authors><title>Variational Recurrent Auto-Encoders</title><categories>stat.ML cs.LG cs.NE</categories><comments>Accepted at ICLR workshop track</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a model that combines the strengths of RNNs and
SGVB: the Variational Recurrent Auto-Encoder (VRAE). Such a model can be used
for efficient, large scale unsupervised learning on time series data, mapping
the time series data to a latent vector representation. The model is
generative, such that data can be generated from samples of the latent space.
An important contribution of this work is that the model can make use of
unlabeled data in order to facilitate supervised training of RNNs by
initialising the weights and network state.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6583</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6583</id><created>2014-12-19</created><updated>2015-06-17</updated><authors><author><keyname>Cheung</keyname><forenames>Brian</forenames></author><author><keyname>Livezey</keyname><forenames>Jesse A.</forenames></author><author><keyname>Bansal</keyname><forenames>Arjun K.</forenames></author><author><keyname>Olshausen</keyname><forenames>Bruno A.</forenames></author></authors><title>Discovering Hidden Factors of Variation in Deep Networks</title><categories>cs.LG cs.CV cs.NE</categories><comments>Presented at International Conference on Learning Representations
  2015 Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning has enjoyed a great deal of success because of its ability to
learn useful features for tasks such as classification. But there has been less
exploration in learning the factors of variation apart from the classification
signal. By augmenting autoencoders with simple regularization terms during
training, we demonstrate that standard deep architectures can discover and
explicitly represent factors of variation beyond those relevant for
categorization. We introduce a cross-covariance penalty (XCov) as a method to
disentangle factors like handwriting style for digits and subject identity in
faces. We demonstrate this on the MNIST handwritten digit database, the Toronto
Faces Database (TFD) and the Multi-PIE dataset by generating manipulated
instances of the data. Furthermore, we demonstrate these deep networks can
extrapolate `hidden' variation in the supervised signal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6586</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6586</id><created>2014-12-19</created><updated>2015-05-27</updated><authors><author><keyname>Wong</keyname><forenames>Alexander</forenames></author><author><keyname>Shafiee</keyname><forenames>Mohammad Javad</forenames></author><author><keyname>Siva</keyname><forenames>Parthipan</forenames></author><author><keyname>Wang</keyname><forenames>Xiao Yu</forenames></author></authors><title>A deep-structured fully-connected random field model for structured
  inference</title><categories>stat.ML cs.IT cs.LG math.IT stat.ME</categories><comments>Accepted, 13 pages</comments><journal-ref>IEEE Access Journal, vol. 3, pp. 469-477, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been significant interest in the use of fully-connected graphical
models and deep-structured graphical models for the purpose of structured
inference. However, fully-connected and deep-structured graphical models have
been largely explored independently, leaving the unification of these two
concepts ripe for exploration. A fundamental challenge with unifying these two
types of models is in dealing with computational complexity. In this study, we
investigate the feasibility of unifying fully-connected and deep-structured
models in a computationally tractable manner for the purpose of structured
inference. To accomplish this, we introduce a deep-structured fully-connected
random field (DFRF) model that integrates a series of intermediate sparse
auto-encoding layers placed between state layers to significantly reduce
computational complexity. The problem of image segmentation was used to
illustrate the feasibility of using the DFRF for structured inference in a
computationally tractable manner. Results in this study show that it is
feasible to unify fully-connected and deep-structured models in a
computationally tractable manner for solving structured inference problems such
as image segmentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6593</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6593</id><created>2014-12-19</created><authors><author><keyname>Wang</keyname><forenames>Yin</forenames></author><author><keyname>Yang</keyname><forenames>Jianjun</forenames></author><author><keyname>Shen</keyname><forenames>Ju</forenames></author><author><keyname>Guo</keyname><forenames>Juan</forenames></author><author><keyname>Hua</keyname><forenames>Kun</forenames></author></authors><title>Compression of Video Tracking and Bandwidth Balancing Routing in
  Wireless Multimedia Sensor Networks</title><categories>cs.NI cs.MM</categories><comments>6 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been a tremendous growth in multimedia applications over wireless
networks. Wireless Multimedia Sensor Networks(WMSNs) have become the premier
choice in many research communities and industry. Many state-of-art
applications, such as surveillance, traffic monitoring, and remote heath care
are essentially video tracking and transmission in WMSNs. The transmission
speed is constrained by big size of video data and fixed bandwidth allocation
in constant routing path. In this paper, we present a CamShift based algorithm
to compress the tracking of videos. Then we propose a bandwidth balancing
strategy in which each sensor node is able to dynamically select the node for
next hop with the highest potential bandwidth capacity to resume communication.
Key to the strategy is that each node merely maintains two parameters that
contains its historical bandwidth varying trend and then predicts its near
future bandwidth capacity. Then forwarding node selects the next hop with the
highest potential bandwidth capacity. Simulations demonstrate that our approach
significantly increases the data received by sink node and decreases the delay
on video transmission in Wireless Multimedia Sensor Network environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6595</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6595</id><created>2014-12-19</created><authors><author><keyname>Patterson</keyname><forenames>Stacy</forenames></author><author><keyname>McGlohon</keyname><forenames>Neil</forenames></author><author><keyname>Dyagilev</keyname><forenames>Kirill</forenames></author></authors><title>Efficient, Optimal $k$-Leader Selection for Coherent, One-Dimensional
  Formations</title><categories>math.OC cs.DS cs.SY</categories><comments>7 pages, 5 figures, submitted to ECC15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of optimal leader selection in consensus networks with
noisy relative information. The objective is to identify the set of $k$ leaders
that minimizes the formation's deviation from the desired trajectory
established by the leaders. An optimal leader set can be found by an exhaustive
search over all possible leader sets; however, this approach is not scalable to
large networks. In recent years, several works have proposed approximation
algorithms to the $k$-leader selection problem, yet the question of whether
there exists an efficient, non-combinatorial method to identify the optimal
leader set remains open. This work takes a first step towards answering this
question. We show that, in one-dimensional weighted graphs, namely path graphs
and ring graphs, the $k$-leader selection problem can be solved in polynomial
time (in both $k$ and the network size $n$). We give an $O(n^3)$ solution for
optimal $k$-leader selection in path graphs and an $O(kn^3)$ solution for
optimal $k$-leader selection in ring graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6596</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6596</id><created>2014-12-19</created><updated>2015-04-15</updated><authors><author><keyname>Reed</keyname><forenames>Scott</forenames></author><author><keyname>Lee</keyname><forenames>Honglak</forenames></author><author><keyname>Anguelov</keyname><forenames>Dragomir</forenames></author><author><keyname>Szegedy</keyname><forenames>Christian</forenames></author><author><keyname>Erhan</keyname><forenames>Dumitru</forenames></author><author><keyname>Rabinovich</keyname><forenames>Andrew</forenames></author></authors><title>Training Deep Neural Networks on Noisy Labels with Bootstrapping</title><categories>cs.CV cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current state-of-the-art deep learning systems for visual object recognition
and detection use purely supervised training with regularization such as
dropout to avoid overfitting. The performance depends critically on the amount
of labeled examples, and in current practice the labels are assumed to be
unambiguous and accurate. However, this assumption often does not hold; e.g. in
recognition, class labels may be missing; in detection, objects in the image
may not be localized; and in general, the labeling may be subjective. In this
work we propose a generic way to handle noisy and incomplete labeling by
augmenting the prediction objective with a notion of consistency. We consider a
prediction consistent if the same prediction is made given similar percepts,
where the notion of similarity is between deep network features computed from
the input data. In experiments we demonstrate that our approach yields
substantial robustness to label noise on several datasets. On MNIST handwritten
digits, we show that our model is robust to label corruption. On the Toronto
Face Database, we show that our model handles well the case of subjective
labels in emotion recognition, achieving state-of-the- art results, and can
also benefit from unlabeled face images with no modification to our method. On
the ILSVRC2014 detection challenge data, we show that our approach extends to
very deep networks, high resolution images and structured outputs, and results
in improved scalable detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6597</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6597</id><created>2014-12-19</created><updated>2015-04-10</updated><authors><author><keyname>Paine</keyname><forenames>Tom Le</forenames></author><author><keyname>Khorrami</keyname><forenames>Pooya</forenames></author><author><keyname>Han</keyname><forenames>Wei</forenames></author><author><keyname>Huang</keyname><forenames>Thomas S.</forenames></author></authors><title>An Analysis of Unsupervised Pre-training in Light of Recent Advances</title><categories>cs.CV cs.LG cs.NE</categories><comments>Accepted as a workshop contribution to ICLR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional neural networks perform well on object recognition because of a
number of recent advances: rectified linear units (ReLUs), data augmentation,
dropout, and large labelled datasets. Unsupervised data has been proposed as
another way to improve performance. Unfortunately, unsupervised pre-training is
not used by state-of-the-art methods leading to the following question: Is
unsupervised pre-training still useful given recent advances? If so, when? We
answer this in three parts: we 1) develop an unsupervised method that
incorporates ReLUs and recent unsupervised regularization techniques, 2)
analyze the benefits of unsupervised pre-training compared to data augmentation
and dropout on CIFAR-10 while varying the ratio of unsupervised to supervised
samples, 3) verify our findings on STL-10. We discover unsupervised
pre-training, as expected, helps when the ratio of unsupervised to supervised
samples is high, and surprisingly, hurts when the ratio is low. We also use
unsupervised pre-training with additional color augmentation to achieve near
state-of-the-art performance on STL-10.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6598</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6598</id><created>2014-12-19</created><updated>2015-04-11</updated><authors><author><keyname>Parizi</keyname><forenames>Sobhan Naderi</forenames></author><author><keyname>Vedaldi</keyname><forenames>Andrea</forenames></author><author><keyname>Zisserman</keyname><forenames>Andrew</forenames></author><author><keyname>Felzenszwalb</keyname><forenames>Pedro</forenames></author></authors><title>Automatic Discovery and Optimization of Parts for Image Classification</title><categories>cs.CV cs.LG</categories><comments>19 pages, template changed to camera ready version, 1 reference
  added, 1 reference fixed, Fig. 3, 4 updated (larger text)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Part-based representations have been shown to be very useful for image
classification. Learning part-based models is often viewed as a two-stage
problem. First, a collection of informative parts is discovered, using
heuristics that promote part distinctiveness and diversity, and then
classifiers are trained on the vector of part responses. In this paper we unify
the two stages and learn the image classifiers and a set of shared parts
jointly. We generate an initial pool of parts by randomly sampling part
candidates and selecting a good subset using L1/L2 regularization. All steps
are driven &quot;directly&quot; by the same objective namely the classification loss on a
training set. This lets us do away with engineered heuristics. We also
introduce the notion of &quot;negative parts&quot;, intended as parts that are negatively
correlated with one or more classes. Negative parts are complementary to the
parts discovered by other methods, which look only for positive correlations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6599</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6599</id><created>2014-12-19</created><updated>2015-04-13</updated><authors><author><keyname>Bache</keyname><forenames>Kevin</forenames></author><author><keyname>DeCoste</keyname><forenames>Dennis</forenames></author><author><keyname>Smyth</keyname><forenames>Padhraic</forenames></author></authors><title>Hot Swapping for Online Adaptation of Optimization Hyperparameters</title><categories>cs.LG</categories><comments>Submission to ICLR 2015</comments><msc-class>62L20</msc-class><acm-class>G.1.6; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a general framework for online adaptation of optimization
hyperparameters by `hot swapping' their values during learning. We investigate
this approach in the context of adaptive learning rate selection using an
explore-exploit strategy from the multi-armed bandit literature. Experiments on
a benchmark neural network show that the hot swapping approach leads to
consistently better solutions compared to well-known alternatives such as
AdaDelta and stochastic gradient with exhaustive hyperparameter search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6601</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6601</id><created>2014-12-19</created><updated>2015-09-19</updated><authors><author><keyname>Baqapuri</keyname><forenames>Afroze Ibrahim</forenames></author><author><keyname>Trofimov</keyname><forenames>Ilya</forenames></author></authors><title>Using Neural Networks for Click Prediction of Sponsored Search</title><categories>cs.LG cs.NE</categories><comments>updated typos, and removed conference header</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sponsored search is a multi-billion dollar industry and makes up a major
source of revenue for search engines (SE). click-through-rate (CTR) estimation
plays a crucial role for ads selection, and greatly affects the SE revenue,
advertiser traffic and user experience. We propose a novel architecture for
solving CTR prediction problem by combining artificial neural networks (ANN)
with decision trees. First we compare ANN with respect to other popular machine
learning models being used for this task. Then we go on to combine ANN with
MatrixNet (proprietary implementation of boosted trees) and evaluate the
performance of the system as a whole. The results show that our approach
provides significant improvement over existing models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6604</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6604</id><created>2014-12-20</created><updated>2015-04-20</updated><authors><author><keyname>Ranzato</keyname><forenames>MarcAurelio</forenames></author><author><keyname>Szlam</keyname><forenames>Arthur</forenames></author><author><keyname>Bruna</keyname><forenames>Joan</forenames></author><author><keyname>Mathieu</keyname><forenames>Michael</forenames></author><author><keyname>Collobert</keyname><forenames>Ronan</forenames></author><author><keyname>Chopra</keyname><forenames>Sumit</forenames></author></authors><title>Video (language) modeling: a baseline for generative models of natural
  videos</title><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a strong baseline model for unsupervised feature learning using
video data. By learning to predict missing frames or extrapolate future frames
from an input video sequence, the model discovers both spatial and temporal
correlations which are useful to represent complex deformations and motion
patterns. The models we propose are largely borrowed from the language modeling
literature, and adapted to the vision domain by quantizing the space of image
patches into a large dictionary. We demonstrate the approach on both a filling
and a generation task. For the first time, we show that, after training on
natural videos, such a model can predict non-trivial motions over short video
sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6605</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6605</id><created>2014-12-20</created><updated>2015-02-15</updated><authors><author><keyname>Foell</keyname><forenames>Stefan</forenames></author><author><keyname>Kortuem</keyname><forenames>Gerd</forenames></author><author><keyname>Rawassizadeh</keyname><forenames>Reza</forenames></author><author><keyname>Handte</keyname><forenames>Marcus</forenames></author><author><keyname>Iqbal</keyname><forenames>Umer</forenames></author><author><keyname>Marron</keyname><forenames>Pedro</forenames></author></authors><title>Micro-Navigation for Urban Bus Passengers: Using the Internet of Things
  to Improve the Public Transport Experience</title><categories>cs.HC cs.CY</categories><comments>Urb-IoT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Public bus services are widely deployed in cities around the world because
they provide cost-effective and economic public transportation. However, from a
passenger point of view urban bus systems can be complex and difficult to
navigate, especially for disadvantaged users, i.e. tourists, novice users,
older people, and people with impaired cognitive or physical abilities. We
present Urban Bus Navigator (UBN), a reality-aware urban navigation system for
bus passengers with the ability to recognize and track the physical public
transport infrastructure such as buses. Unlike traditional location-aware
mobile transport applications, UBN acts as a true navigation assistant for
public transport users. Insights from a six-month long trial in Madrid indicate
that UBN removes barriers for public transport usage and has a positive impact
on how people feel about public transport journeys.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6606</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6606</id><created>2014-12-20</created><updated>2015-02-25</updated><authors><author><keyname>Frostig</keyname><forenames>Roy</forenames></author><author><keyname>Ge</keyname><forenames>Rong</forenames></author><author><keyname>Kakade</keyname><forenames>Sham M.</forenames></author><author><keyname>Sidford</keyname><forenames>Aaron</forenames></author></authors><title>Competing with the Empirical Risk Minimizer in a Single Pass</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many estimation problems, e.g. linear and logistic regression, we wish to
minimize an unknown objective given only unbiased samples of the objective
function. Furthermore, we aim to achieve this using as few samples as possible.
In the absence of computational constraints, the minimizer of a sample average
of observed data -- commonly referred to as either the empirical risk minimizer
(ERM) or the $M$-estimator -- is widely regarded as the estimation strategy of
choice due to its desirable statistical convergence properties. Our goal in
this work is to perform as well as the ERM, on every problem, while minimizing
the use of computational resources such as running time and space usage.
  We provide a simple streaming algorithm which, under standard regularity
assumptions on the underlying problem, enjoys the following properties:
  * The algorithm can be implemented in linear time with a single pass of the
observed data, using space linear in the size of a single sample.
  * The algorithm achieves the same statistical rate of convergence as the
empirical risk minimizer on every problem, even considering constant factors.
  * The algorithm's performance depends on the initial error at a rate that
decreases super-polynomially.
  * The algorithm is easily parallelizable.
  Moreover, we quantify the (finite-sample) rate at which the algorithm becomes
competitive with the ERM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6607</identifier>
 <datestamp>2015-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6607</id><created>2014-12-20</created><updated>2015-04-17</updated><authors><author><keyname>Soatto</keyname><forenames>Stefano</forenames></author><author><keyname>Dong</keyname><forenames>Jingming</forenames></author><author><keyname>Karianakis</keyname><forenames>Nikolaos</forenames></author></authors><title>Visual Scene Representations: Contrast, Scaling and Occlusion</title><categories>cs.CV</categories><comments>UCLA Tech Report CSD140023, Nov. 12, 2014. Updated April 13, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the structure of representations, defined as approximations of
minimal sufficient statistics that are maximal invariants to nuisance factors,
for visual data subject to scaling and occlusion of line-of-sight. We derive
analytical expressions for such representations and show that, under certain
restrictive assumptions, they are related to features commonly in use in the
computer vision community. This link highlights the condition tacitly assumed
by these descriptors, and also suggests ways to improve and generalize them.
This new interpretation draws connections to the classical theories of
sampling, hypothesis testing and group invariance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6610</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6610</id><created>2014-12-20</created><updated>2015-06-14</updated><authors><author><keyname>Im</keyname><forenames>Daniel Jiwoong</forenames></author><author><keyname>Taylor</keyname><forenames>Graham W.</forenames></author></authors><title>Scoring and Classifying with Gated Auto-encoders</title><categories>cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Auto-encoders are perhaps the best-known non-probabilistic methods for
representation learning. They are conceptually simple and easy to train. Recent
theoretical work has shed light on their ability to capture manifold structure,
and drawn connections to density modelling. This has motivated researchers to
seek ways of auto-encoder scoring, which has furthered their use in
classification. Gated auto-encoders (GAEs) are an interesting and flexible
extension of auto-encoders which can learn transformations among different
images or pixel covariances within images. However, they have been much less
studied, theoretically or empirically. In this work, we apply a dynamical
systems view to GAEs, deriving a scoring function, and drawing connections to
Restricted Boltzmann Machines. On a set of deep learning benchmarks, we also
demonstrate their effectiveness for single and multi-label classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6614</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6614</id><created>2014-12-20</created><updated>2015-04-16</updated><authors><author><keyname>Neyshabur</keyname><forenames>Behnam</forenames></author><author><keyname>Tomioka</keyname><forenames>Ryota</forenames></author><author><keyname>Srebro</keyname><forenames>Nathan</forenames></author></authors><title>In Search of the Real Inductive Bias: On the Role of Implicit
  Regularization in Deep Learning</title><categories>cs.LG cs.AI cs.CV stat.ML</categories><comments>9 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present experiments demonstrating that some other form of capacity
control, different from network size, plays a central role in learning
multilayer feed-forward networks. We argue, partially through analogy to matrix
factorization, that this is an inductive bias that can help shed light on deep
learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6615</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6615</id><created>2014-12-20</created><updated>2015-04-06</updated><authors><author><keyname>Sagun</keyname><forenames>Levent</forenames></author><author><keyname>Guney</keyname><forenames>V. Ugur</forenames></author><author><keyname>Arous</keyname><forenames>Gerard Ben</forenames></author><author><keyname>LeCun</keyname><forenames>Yann</forenames></author></authors><title>Explorations on high dimensional landscapes</title><categories>stat.ML cs.LG</categories><comments>11 pages, 8 figures, workshop contribution at ICLR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding minima of a real valued non-convex function over a high dimensional
space is a major challenge in science. We provide evidence that some such
functions that are defined on high dimensional domains have a narrow band of
values whose pre-image contains the bulk of its critical points. This is in
contrast with the low dimensional picture in which this band is wide. Our
simulations agree with the previous theoretical work on spin glasses that
proves the existence of such a band when the dimension of the domain tends to
infinity. Furthermore our experiments on teacher-student networks with the
MNIST dataset establish a similar phenomenon in deep networks. We finally
observe that both the gradient descent and the stochastic gradient descent
methods can reach this level within the same number of steps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6616</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6616</id><created>2014-12-20</created><updated>2015-02-17</updated><authors><author><keyname>Demski</keyname><forenames>Abram</forenames></author><author><keyname>Ustun</keyname><forenames>Volkan</forenames></author><author><keyname>Rosenbloom</keyname><forenames>Paul</forenames></author><author><keyname>Kommers</keyname><forenames>Cody</forenames></author></authors><title>Outperforming Word2Vec on Analogy Tasks with Random Projections</title><categories>cs.CL cs.LG</categories><comments>This paper has been withdrawn due to problems pointed out in review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a distributed vector representation based on a simplification of
the BEAGLE system, designed in the context of the Sigma cognitive architecture.
Our method does not require gradient-based training of neural networks, matrix
decompositions as with LSA, or convolutions as with BEAGLE. All that is
involved is a sum of random vectors and their pointwise products. Despite the
simplicity of this technique, it gives state-of-the-art results on analogy
problems, in most cases better than Word2Vec. To explain this success, we
interpret it as a dimension reduction via random projection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6617</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6617</id><created>2014-12-20</created><updated>2015-04-07</updated><authors><author><keyname>Im</keyname><forenames>Daniel Jiwoong</forenames></author><author><keyname>Buchman</keyname><forenames>Ethan</forenames></author><author><keyname>Taylor</keyname><forenames>Graham W.</forenames></author></authors><title>Understanding Minimum Probability Flow for RBMs Under Various Kinds of
  Dynamics</title><categories>cs.LG</categories><comments>Nine pages including the reference page plus one page appendix.
  Appeared at ICLR2015 workshop track</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy-based models are popular in machine learning due to the elegance of
their formulation and their relationship to statistical physics. Among these,
the Restricted Boltzmann Machine (RBM), and its staple training algorithm
contrastive divergence (CD), have been the prototype for some recent
advancements in the unsupervised training of deep neural networks. However, CD
has limited theoretical motivation, and can in some cases produce undesirable
behavior. Here, we investigate the performance of Minimum Probability Flow
(MPF) learning for training RBMs. Unlike CD, with its focus on approximating an
intractable partition function via Gibbs sampling, MPF proposes a tractable,
consistent, objective function defined in terms of a Taylor expansion of the KL
divergence with respect to sampling dynamics. Here we propose a more general
form for the sampling dynamics in MPF, and explore the consequences of
different choices for these dynamics for training RBMs. Experimental results
show MPF outperforming CD for various RBM configurations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6618</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6618</id><created>2014-12-20</created><updated>2015-05-03</updated><authors><author><keyname>Kiefel</keyname><forenames>Martin</forenames></author><author><keyname>Jampani</keyname><forenames>Varun</forenames></author><author><keyname>Gehler</keyname><forenames>Peter V.</forenames></author></authors><title>Permutohedral Lattice CNNs</title><categories>cs.CV cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a convolutional layer that is able to process sparse
input features. As an example, for image recognition problems this allows an
efficient filtering of signals that do not lie on a dense grid (like pixel
position), but of more general features (such as color values). The presented
algorithm makes use of the permutohedral lattice data structure. The
permutohedral lattice was introduced to efficiently implement a bilateral
filter, a commonly used image processing operation. Its use allows for a
generalization of the convolution type found in current (spatial) convolutional
network architectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6619</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6619</id><created>2014-12-20</created><updated>2015-07-04</updated><authors><author><keyname>Lu</keyname><forenames>Daniel</forenames></author></authors><title>Planar lower envelope of monotone polygonal chains</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A simple linear search algorithm running in $O(n+mk)$ time is proposed for
constructing the lower envelope of $k$ vertices from $m$ monotone polygonal
chains in 2D with $n$ vertices in total. This can be applied to
output-sensitive construction of lower envelopes for arbitrary line segments in
optimal $O(n\log k)$ time, where $k$ is the output size. Compared to existing
output-sensitive algorithms for lower envelopes, this is simpler to implement,
does not require complex data structures, and is a constant factor faster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6621</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6621</id><created>2014-12-20</created><updated>2015-02-28</updated><authors><author><keyname>Paul</keyname><forenames>Arnab</forenames></author><author><keyname>Venkatasubramanian</keyname><forenames>Suresh</forenames></author></authors><title>Why does Deep Learning work? - A perspective from Group Theory</title><categories>cs.LG cs.NE stat.ML</categories><comments>13 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Why does Deep Learning work? What representations does it capture? How do
higher-order representations emerge? We study these questions from the
perspective of group theory, thereby opening a new approach towards a theory of
Deep learning.
  One factor behind the recent resurgence of the subject is a key algorithmic
step called pre-training: first search for a good generative model for the
input samples, and repeat the process one layer at a time. We show deeper
implications of this simple principle, by establishing a connection with the
interplay of orbits and stabilizers of group actions. Although the neural
networks themselves may not form groups, we show the existence of {\em shadow}
groups whose elements serve as close approximations.
  Over the shadow groups, the pre-training step, originally introduced as a
mechanism to better initialize a network, becomes equivalent to a search for
features with minimal orbits. Intuitively, these features are in a way the {\em
simplest}. Which explains why a deep learning network learns simple features
first. Next, we show how the same principle, when repeated in the deeper
layers, can capture higher order representations, and why representation
complexity increases as the layers get deeper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6622</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6622</id><created>2014-12-20</created><updated>2015-03-23</updated><authors><author><keyname>Hoffer</keyname><forenames>Elad</forenames></author><author><keyname>Ailon</keyname><forenames>Nir</forenames></author></authors><title>Deep metric learning using Triplet network</title><categories>cs.LG cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning has proven itself as a successful set of models for learning
useful semantic representations of data. These, however, are mostly implicitly
learned as part of a classification task. In this paper we propose the triplet
network model, which aims to learn useful representations by distance
comparisons. A similar model was defined by Wang et al. (2014), tailor made for
learning a ranking for image information retrieval. Here we demonstrate using
various datasets that our model learns a better representation than that of its
immediate competitor, the Siamese network. We also discuss future possible
usage as a framework for unsupervised learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6623</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6623</id><created>2014-12-20</created><updated>2015-05-01</updated><authors><author><keyname>Vilnis</keyname><forenames>Luke</forenames></author><author><keyname>McCallum</keyname><forenames>Andrew</forenames></author></authors><title>Word Representations via Gaussian Embedding</title><categories>cs.CL cs.LG</categories><comments>12 pages, published as conference paper at ICLR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current work in lexical distributed representations maps each word to a point
vector in low-dimensional space. Mapping instead to a density provides many
interesting advantages, including better capturing uncertainty about a
representation and its relationships, expressing asymmetries more naturally
than dot product or cosine similarity, and enabling more expressive
parameterization of decision boundaries. This paper advocates for density-based
distributed embeddings and presents a method for learning representations in
the space of Gaussian distributions. We compare performance on various word
embedding benchmarks, investigate the ability of these embeddings to model
entailment and other asymmetric relationships, and explore novel properties of
the representation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6626</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6626</id><created>2014-12-20</created><updated>2015-03-23</updated><authors><author><keyname>H&#xe9;naff</keyname><forenames>Olivier J.</forenames></author><author><keyname>Ball&#xe9;</keyname><forenames>Johannes</forenames></author><author><keyname>Rabinowitz</keyname><forenames>Neil C.</forenames></author><author><keyname>Simoncelli</keyname><forenames>Eero P.</forenames></author></authors><title>The local low-dimensionality of natural images</title><categories>cs.CV</categories><comments>Published as conference paper at ICLR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a new statistical model for photographic images, in which the
local responses of a bank of linear filters are described as jointly Gaussian,
with zero mean and a covariance that varies slowly over spatial position. We
optimize sets of filters so as to minimize the nuclear norms of matrices of
their local activations (i.e., the sum of the singular values), thus
encouraging a flexible form of sparsity that is not tied to any particular
dictionary or coordinate system. Filters optimized according to this objective
are oriented and bandpass, and their responses exhibit substantial local
correlation. We show that images can be reconstructed nearly perfectly from
estimates of the local filter response covariances alone, and with minimal
degradation (either visual or MSE) from low-rank approximations of these
covariances. As such, this representation holds much promise for use in
applications such as denoising, compression, and texture representation, and
may form a useful substrate for hierarchical decompositions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6629</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6629</id><created>2014-12-20</created><updated>2015-02-27</updated><authors><author><keyname>Palangi</keyname><forenames>H.</forenames></author><author><keyname>Deng</keyname><forenames>L.</forenames></author><author><keyname>Shen</keyname><forenames>Y.</forenames></author><author><keyname>Gao</keyname><forenames>J.</forenames></author><author><keyname>He</keyname><forenames>X.</forenames></author><author><keyname>Chen</keyname><forenames>J.</forenames></author><author><keyname>Song</keyname><forenames>X.</forenames></author><author><keyname>Ward</keyname><forenames>R.</forenames></author></authors><title>Semantic Modelling with Long-Short-Term Memory for Information Retrieval</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address the following problem in web document and
information retrieval (IR): How can we use long-term context information to
gain better IR performance? Unlike common IR methods that use bag of words
representation for queries and documents, we treat them as a sequence of words
and use long short term memory (LSTM) to capture contextual dependencies. To
the best of our knowledge, this is the first time that LSTM is applied to
information retrieval tasks. Unlike training traditional LSTMs, the training
strategy is different due to the special nature of information retrieval
problem. Experimental evaluation on an IR task derived from the Bing web search
demonstrates the ability of the proposed method in addressing both lexical
mismatch and long-term context modelling issues, thereby, significantly
outperforming existing state of the art methods for web document retrieval
task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6630</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6630</id><created>2014-12-20</created><updated>2015-01-05</updated><authors><author><keyname>Rudy</keyname><forenames>Jan</forenames></author><author><keyname>Ding</keyname><forenames>Weiguang</forenames></author><author><keyname>Im</keyname><forenames>Daniel Jiwoong</forenames></author><author><keyname>Taylor</keyname><forenames>Graham W.</forenames></author></authors><title>Neural Network Regularization via Robust Weight Factorization</title><categories>cs.LG cs.NE stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regularization is essential when training large neural networks. As deep
neural networks can be mathematically interpreted as universal function
approximators, they are effective at memorizing sampling noise in the training
data. This results in poor generalization to unseen data. Therefore, it is no
surprise that a new regularization technique, Dropout, was partially
responsible for the now-ubiquitous winning entry to ImageNet 2012 by the
University of Toronto. Currently, Dropout (and related methods such as
DropConnect) are the most effective means of regularizing large neural
networks. These amount to efficiently visiting a large number of related models
at training time, while aggregating them to a single predictor at test time.
The proposed FaMe model aims to apply a similar strategy, yet learns a
factorization of each weight matrix such that the factors are robust to noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6631</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6631</id><created>2014-12-20</created><updated>2014-12-26</updated><authors><author><keyname>Yu</keyname><forenames>Wei</forenames></author><author><keyname>Yang</keyname><forenames>Kuiyuan</forenames></author><author><keyname>Bai</keyname><forenames>Yalong</forenames></author><author><keyname>Yao</keyname><forenames>Hongxun</forenames></author><author><keyname>Rui</keyname><forenames>Yong</forenames></author></authors><title>Visualizing and Comparing Convolutional Neural Networks</title><categories>cs.CV</categories><comments>9 pages and 7 figures, submit to ICLR2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional Neural Networks (CNNs) have achieved comparable error rates to
well-trained human on ILSVRC2014 image classification task. To achieve better
performance, the complexity of CNNs is continually increasing with deeper and
bigger architectures. Though CNNs achieved promising external classification
behavior, understanding of their internal work mechanism is still limited. In
this work, we attempt to understand the internal work mechanism of CNNs by
probing the internal representations in two comprehensive aspects, i.e.,
visualizing patches in the representation spaces constructed by different
layers, and visualizing visual information kept in each layer. We further
compare CNNs with different depths and show the advantages brought by deeper
architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6632</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6632</id><created>2014-12-20</created><updated>2015-06-11</updated><authors><author><keyname>Mao</keyname><forenames>Junhua</forenames></author><author><keyname>Xu</keyname><forenames>Wei</forenames></author><author><keyname>Yang</keyname><forenames>Yi</forenames></author><author><keyname>Wang</keyname><forenames>Jiang</forenames></author><author><keyname>Huang</keyname><forenames>Zhiheng</forenames></author><author><keyname>Yuille</keyname><forenames>Alan</forenames></author></authors><title>Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)</title><categories>cs.CV cs.CL cs.LG</categories><comments>Add a simple strategy to boost the performance of image captioning
  task significantly. More details are shown in Section 8 of the paper. The
  code and related data are available at https://github.com/mjhucla/mRNN-CR ;.
  arXiv admin note: substantial text overlap with arXiv:1410.1090</comments><acm-class>I.2.6; I.2.7; I.2.10</acm-class><journal-ref>ICLR 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model
for generating novel image captions. It directly models the probability
distribution of generating a word given previous words and an image. Image
captions are generated by sampling from this distribution. The model consists
of two sub-networks: a deep recurrent neural network for sentences and a deep
convolutional network for images. These two sub-networks interact with each
other in a multimodal layer to form the whole m-RNN model. The effectiveness of
our model is validated on four benchmark datasets (IAPR TC-12, Flickr 8K,
Flickr 30K and MS COCO). Our model outperforms the state-of-the-art methods. In
addition, we apply the m-RNN model to retrieval tasks for retrieving images or
sentences, and achieves significant performance improvement over the
state-of-the-art methods which directly optimize the ranking objective function
for retrieval. The project page of this work is:
www.stat.ucla.edu/~junhua.mao/m-RNN.html .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6638</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6638</id><created>2014-12-20</created><authors><author><keyname>Baradaran-Hosseini</keyname><forenames>Mani</forenames></author></authors><title>Artifact Centric Business Process Management Logging Schema</title><categories>cs.SE</categories><comments>arXiv admin note: text overlap with arXiv:1211.5009 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the evolution of business artifacts will enable business
analyst to discover more insight from process execution data. In this context,
describing how the artifacts are wired, helps in understanding, predicting and
optimizing the behavior of dynamic processes. In many cases, however, process
artifacts evolve over time, as they pass through the business's operations.
Consequently, understanding the evolution of artifacts becomes challenging and
requires analyzing the provenance of business artifacts. In this paper our aim
is to analyze and classify existing challenges in artifact centric business
processes. We propose to extend Provenance techniques to artifact centric BPMs
in order to perform cross cutting concerns on BPMs. Provenance is
pre-requirement of addressing cross cutting concerns, which will provide
information regarding artifact instance creation and its evolution during its
life cycle. Due to dynamic nature of dynamic processes and declarative
structure of Artifact Centric BPM systems, it's vital to make sure how an
artifact instance actually executed and evolved during its processing in run
time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6641</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6641</id><created>2014-12-20</created><authors><author><keyname>Beigi</keyname><forenames>Salman</forenames></author><author><keyname>Etesami</keyname><forenames>Omid</forenames></author><author><keyname>Gohari</keyname><forenames>Amin</forenames></author></authors><title>Deterministic Randomness Extraction from Generalized and Distributed
  Santha-Vazirani Sources</title><categories>cs.CC cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Santha-Vazirani (SV) source is a sequence of random bits where the
conditional distribution of each bit, given the previous bits, can be partially
controlled by an adversary. Santha and Vazirani show that deterministic
randomness extraction from these sources is impossible. In this paper, we study
the generalization of SV sources for non-binary sequences. We show that unlike
the binary case, deterministic randomness extraction in the generalized case is
sometimes possible. We present a necessary condition and a sufficient condition
for the possibility of deterministic randomness extraction. These two
conditions coincide in &quot;non-degenerate&quot; cases.
  Next, we turn to a distributed setting. In this setting the SV source
consists of a random sequence of pairs $(a_1, b_1), (a_2, b_2), \ldots$
distributed between two parties, where the first party receives $a_i$'s and the
second one receives $b_i$'s. The goal of the two parties is to extract common
randomness without communication. Using the notion of maximal correlation, we
prove a necessary condition and a sufficient condition for the possibility of
common randomness extraction from these sources. Based on these two conditions,
the problem of common randomness extraction essentially reduces to the problem
of randomness extraction from (non-distributed) SV sources. This result
generalizes results of G\'acs and K\&quot;orner, and Witsenhausen about common
randomness extraction from i.i.d. sources to adversarial sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6645</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6645</id><created>2014-12-20</created><updated>2015-04-20</updated><authors><author><keyname>Synnaeve</keyname><forenames>Gabriel</forenames></author><author><keyname>Dupoux</keyname><forenames>Emmanuel</forenames></author></authors><title>Weakly Supervised Multi-Embeddings Learning of Acoustic Models</title><categories>cs.SD cs.CL cs.LG</categories><comments>6 pages, 3 figures</comments><acm-class>I.2.6; I.2.7; I.5.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We trained a Siamese network with multi-task same/different information on a
speech dataset, and found that it was possible to share a network for both
tasks without a loss in performance. The first task was to discriminate between
two same or different words, and the second was to discriminate between two
same or different talkers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6646</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6646</id><created>2014-12-20</created><authors><author><keyname>Bauer</keyname><forenames>Ulrich</forenames></author><author><keyname>Munch</keyname><forenames>Elizabeth</forenames></author><author><keyname>Wang</keyname><forenames>Yusu</forenames></author></authors><title>Strong Equivalence of the Interleaving and Functional Distortion Metrics
  for Reeb Graphs</title><categories>math.AT cs.CG</categories><journal-ref>31st International Symposium on Computational Geometry (SoCG
  2015), LIPIcs 34 (2015), 461-475</journal-ref><doi>10.4230/LIPIcs.SOCG.2015.461</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Reeb graph is a construction that studies a topological space through the
lens of a real valued function. It has widely been used in applications,
however its use on real data means that it is desirable and increasingly
necessary to have methods for comparison of Reeb graphs. Recently, several
methods to define metrics on the space of Reeb graphs have been presented. In
this paper, we focus on two: the functional distortion distance and the
interleaving distance. The former is based on the Gromov--Hausdorff distance,
while the latter utilizes the equivalence between Reeb graphs and a particular
class of cosheaves. However, both are defined by constructing a
near-isomorphism between the two graphs of study. In this paper, we show that
the two metrics are strongly equivalent on the space of Reeb graphs. In
particular, this gives an immediate proof of bottleneck stability for
persistence diagrams in terms of the Reeb graph interleaving distance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6649</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6649</id><created>2014-12-20</created><authors><author><keyname>Dorr</keyname><forenames>Christopher H.</forenames></author><author><keyname>Moratz</keyname><forenames>Reinhard</forenames></author></authors><title>Qualitative shape representation based on the qualitative relative
  direction and distance calculus eOPRAm</title><categories>cs.AI cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This document serves as a brief technical report, detailing the processes
used to represent and reconstruct simplified polygons using qualitative spatial
descriptions, as defined by the eOPRAm qualitative spatial calculus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6650</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6650</id><created>2014-12-20</created><updated>2015-07-07</updated><authors><author><keyname>Ter-Sarkisov</keyname><forenames>Aram</forenames></author><author><keyname>Schwenk</keyname><forenames>Holger</forenames></author><author><keyname>Barrault</keyname><forenames>Loic</forenames></author><author><keyname>Bougares</keyname><forenames>Fethi</forenames></author></authors><title>Incremental Adaptation Strategies for Neural Network Language Models</title><categories>cs.NE cs.CL cs.LG</categories><comments>accepted as workshop paper at ACL-IJCNLP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is today acknowledged that neural network language models outperform
backoff language models in applications like speech recognition or statistical
machine translation. However, training these models on large amounts of data
can take several days. We present efficient techniques to adapt a neural
network language model to new data. Instead of training a completely new model
or relying on mixture approaches, we propose two new methods: continued
training on resampled data or insertion of adaptation layers. We present
experimental results in an CAT environment where the post-edits of professional
translators are used to improve an SMT system. Both methods are very fast and
achieve significant improvements without overfitting the small adaptation data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6651</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6651</id><created>2014-12-20</created><updated>2015-10-25</updated><authors><author><keyname>Zhang</keyname><forenames>Sixin</forenames></author><author><keyname>Choromanska</keyname><forenames>Anna</forenames></author><author><keyname>LeCun</keyname><forenames>Yann</forenames></author></authors><title>Deep learning with Elastic Averaging SGD</title><categories>cs.LG stat.ML</categories><comments>NIPS2015 camera-ready version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of stochastic optimization for deep learning in the
parallel computing environment under communication constraints. A new algorithm
is proposed in this setting where the communication and coordination of work
among concurrent processes (local workers), is based on an elastic force which
links the parameters they compute with a center variable stored by the
parameter server (master). The algorithm enables the local workers to perform
more exploration, i.e. the algorithm allows the local variables to fluctuate
further from the center variable by reducing the amount of communication
between local workers and the master. We empirically demonstrate that in the
deep learning setting, due to the existence of many local optima, allowing more
exploration can lead to the improved performance. We propose synchronous and
asynchronous variants of the new algorithm. We provide the stability analysis
of the asynchronous variant in the round-robin scheme and compare it with the
more common parallelized method ADMM. We show that the stability of EASGD is
guaranteed when a simple stability condition is satisfied, which is not the
case for ADMM. We additionally propose the momentum-based version of our
algorithm that can be applied in both synchronous and asynchronous settings.
Asynchronous variant of the algorithm is applied to train convolutional neural
networks for image classification on the CIFAR and ImageNet datasets.
Experiments demonstrate that the new algorithm accelerates the training of deep
architectures compared to DOWNPOUR and other common baseline approaches and
furthermore is very communication efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6673</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6673</id><created>2014-12-20</created><authors><author><keyname>Moll</keyname><forenames>Mark</forenames></author><author><keyname>Sucan</keyname><forenames>Ioan A.</forenames></author><author><keyname>Kavraki</keyname><forenames>Lydia E.</forenames></author></authors><title>An Extensible Benchmarking Infrastructure for Motion Planning Algorithms</title><categories>cs.RO</categories><comments>Submitted to IEEE Robotics &amp; Automation Magazine (Special Issue on
  Replicable and Measurable Robotics Research), 2015</comments><doi>10.1109/MRA.2015.2448276</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sampling-based planning algorithms are the most common probabilistically
complete algorithms and are widely used on many robot platforms. Within this
class of algorithms, many variants have been proposed over the last 20 years,
yet there is still no characterization of which algorithms are well-suited for
which classes of problems. This has motivated us to develop a benchmarking
infrastructure for motion planning algorithms. It consists of three main
components. First, we have created an extensive benchmarking software framework
that is included with the Open Motion Planning Library (OMPL), a C++ library
that contains implementations of many sampling-based algorithms. Second, we
have defined extensible formats for storing benchmark results. The formats are
fairly straightforward so that other planning libraries could easily produce
compatible output. Finally, we have created an interactive, versatile
visualization tool for compact presentation of collected benchmark data. The
tool and underlying database facilitate the analysis of performance across
benchmark problems and planners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6676</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6676</id><created>2014-12-20</created><authors><author><keyname>Pach</keyname><forenames>J&#xe1;nos</forenames></author><author><keyname>Rubin</keyname><forenames>Natan</forenames></author><author><keyname>Tardos</keyname><forenames>G&#xe1;bor</forenames></author></authors><title>On the Richter-Thomassen Conjecture about Pairwise Intersecting Closed
  Curves</title><categories>math.CO cs.CG</categories><comments>To appear in SODA 2015</comments><msc-class>05C10, 05C35, 05D99, 52C30, 52C45, 52C10</msc-class><acm-class>F.2.2; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A long standing conjecture of Richter and Thomassen states that the total
number of intersection points between any $n$ simple closed Jordan curves in
the plane, so that any pair of them intersect and no three curves pass through
the same point, is at least $(1-o(1))n^2$.
  We confirm the above conjecture in several important cases, including the
case (1) when all curves are convex, and (2) when the family of curves can be
partitioned into two equal classes such that each curve from the first class is
touching every curve from the second class. (Two curves are said to be touching
if they have precisely one point in common, at which they do not properly
cross.)
  An important ingredient of our proofs is the following statement: Let $S$ be
a family of the graphs of $n$ continuous real functions defined on
$\mathbb{R}$, no three of which pass through the same point. If there are $nt$
pairs of touching curves in $S$, then the number of crossing points is
$\Omega(nt\sqrt{\log t/\log\log t})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6677</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6677</id><created>2014-12-20</created><authors><author><keyname>Peng</keyname><forenames>Mugen</forenames></author><author><keyname>Li</keyname><forenames>Yong</forenames></author><author><keyname>Zhao</keyname><forenames>Zhongyuan</forenames></author><author><keyname>Wang</keyname><forenames>Chonggang</forenames></author></authors><title>System Architecture and Key Technologies for 5G Heterogeneous Cloud
  Radio Access Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>20mpages, 6 figures, accepted by IEEE Network magazine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compared with the fourth generation (4G) cellular systems, the fifth
generation wireless communication systems (5G) are anticipated to provide
spectral and energy efficiency growth by a factor of at least 10, and the area
throughput growth by a factor of at least 25. To achieve these goals, a
heterogeneous cloud radio access network (H-CRAN) is presented in this article
as the advanced wireless access network paradigm, where cloud computing is used
to fulfill the centralized large-scale cooperative processing for suppressing
co-channel interferences. The state-of-the-art research achievements in aspects
of system architecture and key technologies for H-CRANs are surveyed.
Particularly, Node C as a new communication entity is defined to converge the
existing ancestral base stations and act as the base band unit (BBU) pool to
manage all accessed remote radio heads (RRHs), and the software-defined H-CRAN
system architecture is presented to be compatible with software-defined
networks (SDN). The principles, performance gains and open issues of key
technologies including adaptive large-scale cooperative spatial signal
processing, cooperative radio resource management, network function
virtualization, and self-organization are summarized. The major challenges in
terms of fronthaul constrained resource allocation optimization and energy
harvesting that may affect the promotion of H-CRANs are discussed as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6680</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6680</id><created>2014-12-20</created><authors><author><keyname>Peng</keyname><forenames>Mugen</forenames></author><author><keyname>Hu</keyname><forenames>Qiang</forenames></author><author><keyname>Xie</keyname><forenames>Xinqian</forenames></author><author><keyname>Zhao</keyname><forenames>Zhongyuan</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Network Coded Multi-Hop Wireless Communication Networks: Channel
  Estimation and Training Design</title><categories>cs.IT math.IT</categories><comments>14 pages, 11 figures, accepted by IEEE JSAC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  User cooperation based multi-hop wireless communication networks (MH-WCNs) as
the key communication technological component of mobile social networks (MSNs)
should be exploited to enhance the capability of accumulating data rates and
extending coverage flexibly. As one of the most promising and efficient user
cooperation techniques, network coding can increase the potential cooperation
performance gains among selfishly driven users in MSNs. To take full advantages
of network coding in MH-WCNs, a network coding transmission strategy and its
corresponding channel estimation technique are studied in this paper.
Particularly, a $4-$hop network coding transmission strategy is presented
first, followed by an extension strategy for the arbitrary $2N-$hop scenario
($N\geq 2$). The linear minimum mean square error (LMMSE) and
maximum-likelihood (ML) channel estimation methods are designed to improve the
transmission quality in MH-WCNs. Closed form expressions in terms of the mean
squared error (MSE) for the LMMSE channel estimation method are derived, which
allows the design of the optimal training sequence. Unlike the LMMSE method, it
is difficult to obtain closed-form MSE expressions for the nonlinear ML channel
estimation method. In order to accomplish optimal training sequence design for
the ML method, the Cram\'{e}r-Rao lower bound (CRLB) is employed. Numerical
results are provided to corroborate the proposed analysis, and the results
demonstrate that the analysis is accurate and the proposed methods are
effective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6682</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6682</id><created>2014-12-20</created><updated>2014-12-29</updated><authors><author><keyname>Sghaier</keyname><forenames>Mouna</forenames></author><author><keyname>Abdelkefi</keyname><forenames>Fatma</forenames></author><author><keyname>Siala</keyname><forenames>Mohamed</forenames></author></authors><title>PAPR Reduction Scheme In MIMO-OFDM Systems With Efficient Embedded
  Signaling</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn by the author due to loss of
  informations</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple Input Multiple Output (MIMO) Orthogonal Frequency Division
Multiplexing (OFDM) is a promising transmission scheme for high performance
broadband wireless communications. However, this technique suffers from a major
drawback which is the high Peak to Average Power Ratio (PAPR) of the output
signals. In order to overcome this issue, several methods that require the
transmission of explicit Side Information (SI) bits have been proposed. In
fact, the transmitted bits must be channel-encoded as they are particularly
critical to the performance of the considered OFDM system. This
channel-encoding highly increases the system complexity and also decreases the
transmission data rate. For these reasons, we propose in this paper, two robust
blind techniques that embed the (SI) implicitly into the OFDM frame. First, we
investigate a new technique referred as Blind Space Time Bloc Codes (BSTBC)
that is inspired from the conventional Selected Mapping (SLM) approach. This
technique banks on an adequate embedded signaling that mainly consist on a
specific Space Time Bloc Codes (STBC) patterns and a precoding sequences
codebook. Second, in order to improve the signal detection process and the PAPR
gain, we propose a new efficient combined Blind SLM-STBC (BSLM-STBC) method.
Both methods have the benefit of resulting in an optimized scheme during the
signal estimation process that is based on the Max-Log-Maximum A Posteriori
(MAP) algorithm. Finally, the obtained performance evaluation results show that
our proposed methods result in a spectacular PAPR reduction and furthermore
lead to a perfect signal recovery at the receiver side.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6683</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6683</id><created>2014-12-20</created><authors><author><keyname>Rafols</keyname><forenames>Ismael</forenames></author></authors><title>Knowledge Integration and Diffusion: Measures and Mapping of Diversity
  and Coherence</title><categories>physics.soc-ph cs.DL</categories><journal-ref>Measuring Scholarly Impact: Methods and Practice, Springer,
  169-190</journal-ref><doi>10.1007/978-3-319-10377-8_8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I present a framework based on the concepts of diversity and coherence for
the analysis of knowledge integration and diffusion. Visualisations that help
understand insights gained are also introduced. The key novelty offered by this
framework compared to previous approaches is the inclusion of cognitive
distance (or proximity) between the categories that characterise the body of
knowledge under study. I briefly discuss the different methods to map the
cognitive dimension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6684</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6684</id><created>2014-12-20</created><authors><author><keyname>Molas-Gallart</keyname><forenames>Jordi</forenames></author><author><keyname>Tang</keyname><forenames>Puay</forenames></author><author><keyname>Rafols</keyname><forenames>Ismael</forenames></author></authors><title>On the relationship between interdisciplinarity and impact: different
  modalities of interdisciplinarity lead to different types of impact</title><categories>cs.DL physics.soc-ph</categories><journal-ref>Journal of Science Policy and Research Management, 29(2), 69-89</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is increasing interest among funding agencies to understand how they
can best contribute to enhancing the socio-economic impact of research.
Interdisciplinarity is often presented as a research mode that can facilitate
impact but there exist a limited number of analytical studies that have
attempted to examine whether or how interdisciplinarity can affect the societal
relevance of research. We investigate fifteen Social Sciences research
investments in the UK to examine how they have achieved impact. We analyse
research drivers, cognitive distances, degree of integration, collaborative
practices, stakeholder engagement and the type of impact generated. The
analysis suggests that interdisciplinarity cannot be associated with a single
type of impact mechanism. Also, interdisciplinarity is neither a sufficient nor
a necessary condition for achieving societal relevance and impact. However, we
identify a specific modality -- &quot;long-range&quot; interdisciplinarity, which appears
more likely to be associated with societal impact because of its focused
problem-orientation and its strong interaction with stakeholders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6686</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6686</id><created>2014-12-20</created><authors><author><keyname>Banerjee</keyname><forenames>Joydeep</forenames></author><author><keyname>Das</keyname><forenames>Arun</forenames></author><author><keyname>Zhou</keyname><forenames>Chenyang</forenames></author><author><keyname>Mazumder</keyname><forenames>Anisha</forenames></author><author><keyname>Sen</keyname><forenames>Arunabha</forenames></author></authors><title>On the Entity Hardening Problem in Multi-layered Interdependent Networks</title><categories>cs.NI cs.DS</categories><comments>7 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The power grid and the communication network are highly interdependent on
each other for their well being. In recent times the research community has
shown significant interest in modeling such interdependent networks and
studying the impact of failures on these networks. Although a number of models
have been proposed, many of them are simplistic in nature and fail to capture
the complex interdependencies that exist between the entities of these
networks. To overcome the limitations, recently an Implicative Interdependency
Model that utilizes Boolean Logic, was proposed and a number of problems were
studied. In this paper we study the entity hardening problem, where by entity
hardening we imply the ability of the network operator to ensure that an
adversary (be it Nature or human) cannot take a network entity from operative
to inoperative state. Given that the network operator with a limited budget can
only harden k entities, the goal of the entity hardening problem is to identify
the set of k entities whose hardening will ensure maximum benefit for the
operator, i.e. maximally reduce the ability of the adversary to degrade the
network. We show that the problem is solvable in polynomial time for some
cases, whereas for others it is NP-complete. We provide the optimal solution
using ILP, and propose a heuristic approach to solve the problem. We evaluate
the efficacy of our heuristic using power and communication network data of
Maricopa County, Arizona. The experiments show that our heuristic almost always
produces near optimal results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6687</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6687</id><created>2014-12-20</created><authors><author><keyname>D'Oro</keyname><forenames>Salvatore</forenames></author><author><keyname>Galluccio</keyname><forenames>Laura</forenames></author><author><keyname>Morabito</keyname><forenames>Giacomo</forenames></author><author><keyname>Palazzo</keyname><forenames>Sergio</forenames></author><author><keyname>Chen</keyname><forenames>Lin</forenames></author><author><keyname>Martignon</keyname><forenames>Fabio</forenames></author></authors><title>Defeating jamming with the power of silence: a game-theoretic analysis</title><categories>cs.GT cs.NI</categories><comments>Anti-jamming, Timing Channel, Game-Theoretic Models, Nash Equilibrium</comments><doi>10.1109/TWC.2014.2385709</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The timing channel is a logical communication channel in which information is
encoded in the timing between events. Recently, the use of the timing channel
has been proposed as a countermeasure to reactive jamming attacks performed by
an energy-constrained malicious node. In fact, whilst a jammer is able to
disrupt the information contained in the attacked packets, timing information
cannot be jammed and, therefore, timing channels can be exploited to deliver
information to the receiver even on a jammed channel.
  Since the nodes under attack and the jammer have conflicting interests, their
interactions can be modeled by means of game theory. Accordingly, in this paper
a game-theoretic model of the interactions between nodes exploiting the timing
channel to achieve resilience to jamming attacks and a jammer is derived and
analyzed. More specifically, the Nash equilibrium is studied in the terms of
existence, uniqueness, and convergence under best response dynamics.
Furthermore, the case in which the communication nodes set their strategy and
the jammer reacts accordingly is modeled and analyzed as a Stackelberg game, by
considering both perfect and imperfect knowledge of the jammer's utility
function. Extensive numerical results are presented, showing the impact of
network parameters on the system performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6699</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6699</id><created>2014-12-20</created><authors><author><keyname>Tom</keyname><forenames>Anas</forenames></author><author><keyname>Sahin</keyname><forenames>Alphan</forenames></author><author><keyname>Arslan</keyname><forenames>Huseyin</forenames></author></authors><title>Suppressing Alignment: Joint PAPR and Out-of-Band Power Leakage
  Reduction for OFDM-Based Systems</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Orthogonal frequency division multiplexing (OFDM) inherently suffers from two
major drawbacks: high out-of-band (OOB) power leakage and high peak-to-average
power ratio (PAPR). This paper proposes a novel approach called suppressing
alignment for the joint reduction of the OOB power leakage and PAPR. The
proposed approach exploits the temporal degrees of freedom provided by the
cyclic prefix(CP), a necessary redundancy in OFDM systems, to generate a
suppressing signal, that when added to the OFDM symbol, results in marked
reduction in both the OOB power leakage and PAPR. Additionally, and in order to
not cause any interference to the information data carried by the OFDM symbol,
the proposed approach utilizes the wireless channel to perfectly align the
suppressing signal with the CP duration at the OFDM receiver. Essentially,
maintaining a bit error rate (BER) performance similar to legacy OFDM without
requiring any change in the receiver structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6703</identifier>
 <datestamp>2014-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6703</id><created>2014-12-20</created><updated>2014-12-23</updated><authors><author><keyname>Zenil</keyname><forenames>Hector</forenames></author></authors><title>Quantifying Natural and Artificial Intelligence in Robots and Natural
  Systems with an Algorithmic Behavioural Test</title><categories>cs.AI cs.RO</categories><comments>21 pages, Springer Cosmos Series Book on Metrics of sensory motor
  integration in robots and animals</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most important aims of the fields of robotics, artificial
intelligence and artificial life is the design and construction of systems and
machines as versatile and as reliable as living organisms at performing high
level human-like tasks. But how are we to evaluate artificial systems if we are
not certain how to measure these capacities in living systems, let alone how to
define life or intelligence? Here I survey a concrete metric towards measuring
abstract properties of natural and artificial systems, such as the ability to
react to the environment and to control one's own behaviour.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6704</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6704</id><created>2014-12-20</created><authors><author><keyname>Saglam</keyname><forenames>Cenk Oguz</forenames></author><author><keyname>Byl</keyname><forenames>Katie</forenames></author></authors><title>First Passage Value</title><categories>cs.SY cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For many stochastic dynamic systems, the Mean First Passage Time (MFPT) is a
useful concept, which gives expected time before a state of interest. This work
is an extension of MFPT in several ways. (1) We show that for some systems the
system-wide MFPT, calculated using the second largest eigenvalue only, captures
most of the fundamental dynamics, even for quite complex, high-dimensional
systems. (2) We generalize MFPT to Mean First Passage Value (MFPV), which gives
a more general value of interest, e.g., energy expenditure, distance, or time.
(3) We provide bounds on First Passage Value (FPV) for a given confidence
level. At the heart of this work, we emphasize that for our goals, many hybrid
systems can be approximated as Markov Decision Processes. So, many systems can
be controlled effectively using this framework. However, our framework is
particularly useful for metastable systems. Such systems exhibit interesting
long-living behaviors from which they are guaranteed to inevitably escape
(e.g., eventually arriving at a distinct failure or success state). Our goal is
then either minimizing or maximizing the value until escape, depending on the
application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6705</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6705</id><created>2014-12-20</created><authors><author><keyname>Dadush</keyname><forenames>Daniel</forenames></author><author><keyname>H&#xe4;hnle</keyname><forenames>Nicolai</forenames></author></authors><title>On the Shadow Simplex Method for Curved Polyhedra</title><categories>cs.DS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the simplex method over polyhedra satisfying certain &quot;discrete
curvature&quot; lower bounds, which enforce that the boundary always meets vertices
at sharp angles. Motivated by linear programs with totally unimodular
constraint matrices, recent results of Bonifas et al (SOCG 2012), Brunsch and
R\&quot;oglin (ICALP 2013), and Eisenbrand and Vempala (2014) have improved our
understanding of such polyhedra.
  We develop a new type of dual analysis of the shadow simplex method which
provides a clean and powerful tool for improving all previously mentioned
results. Our methods are inspired by the recent work of Bonifas and the first
named author (SODA 2015), who analyzed a remarkably similar process as part of
an algorithm for the Closest Vector Problem with Preprocessing.
  For our first result, we obtain a constructive diameter bound of
$O(\frac{n^2}{\delta} \ln \frac{n}{\delta})$ for $n$-dimensional polyhedra with
curvature parameter $\delta \in [0,1]$. For the class of polyhedra arising from
totally unimodular constraint matrices, this implies a bound of $O(n^3 \ln n)$.
For linear optimization, given an initial feasible vertex, we show that an
optimal vertex can be found using an expected $O(\frac{n^3}{\delta} \ln
\frac{n}{\delta})$ simplex pivots, each requiring $O(m n)$ time to compute. An
initial feasible solution can be found using $O(\frac{m n^3}{\delta} \ln
\frac{n}{\delta})$ pivot steps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6706</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6706</id><created>2014-12-20</created><authors><author><keyname>Arendt</keyname><forenames>Dustin L.</forenames></author><author><keyname>Blaha</keyname><forenames>Leslie M.</forenames></author></authors><title>SVEN: Informative Visual Representation of Complex Dynamic Structure</title><categories>cs.SI cs.GR cs.HC physics.soc-ph</categories><comments>Python, JavaScript, &amp; HTML source contained within ancillary folder
  under MPL2.0 license</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graphs change over time, and typically variations on the small multiples or
animation pattern is used to convey this dynamism visually. However, both of
these classical techniques have significant drawbacks, so a new approach,
Storyline Visualization of Events on a Network (SVEN) is proposed. SVEN builds
on storyline techniques, conveying nodes as contiguous lines over time. SVEN
encodes time in a natural manner, along the horizontal axis, and optimizes the
vertical placement of storylines to decrease clutter (line crossings,
straightness, and bends) in the drawing. This paper demonstrates SVEN on
several different flavors of real-world dynamic data, and outlines the
remaining near-term future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6720</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6720</id><created>2014-12-20</created><authors><author><keyname>Liu</keyname><forenames>Liang</forenames></author><author><keyname>Wei</keyname><forenames>Ping</forenames></author></authors><title>Off-grid DOA Estimation Based on Analysis of the Convexity of Maximum
  Likelihood Function</title><categories>cs.IT math.IT</categories><comments>10 pages, 6 figures, 1 table. arXiv admin note: text overlap with
  arXiv:1108.5838 by other authors</comments><doi>10.1587/transfun.E98.A.2705</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spatial compressive sensing (SCS) has recently been applied to
direction-of-arrival (DOA) estimation owing to advantages over conventional
ones. However the performance of compressive sensing (CS)-based estimation
methods decreases when true DOAs are not exactly on the discretized sampling
grid. We solve the off-grid DOA estimation problem using the deterministic
maximum likelihood (DML) estimation method. In this work, we analyze the
convexity of the DML function in the vicinity of the global solution.
Especially under the condition of large array, we search for an approximately
convex range around the ture DOAs to guarantee the DML function convex. Based
on the convexity of the DML function, we propose a computationally efficient
algorithm framework for off-grid DOA estimation. Numerical experiments show
that the rough convex range accords well with the exact convex range of the DML
function with large array and demonstrate the superior performance of the
proposed methods in terms of accuracy, robustness and speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6722</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6722</id><created>2014-12-20</created><authors><author><keyname>Rong</keyname><forenames>Nan</forenames></author><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author></authors><title>Cooperative Equilibrium: A solution predicting cooperative play</title><categories>cs.GT</categories><comments>34 pages</comments><msc-class>91A10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nash equilibrium (NE) assumes that players always make a best response.
However, this is not always true; sometimes people cooperate even it is not a
best response to do so. For example, in the Prisoner's Dilemma, people often
cooperate. Are there rules underlying cooperative behavior? In an effort to
answer this question, we propose a new equilibrium concept: perfect cooperative
equilibrium (PCE), and two related variants: max-PCE and cooperative
equilibrium. PCE may help explain players' behavior in games where cooperation
is observed in practice. A player's payoff in a PCE is at least as high as in
any NE. However, a PCE does not always exist. We thus consider {\alpha}-PCE,
where {\alpha} takes into account the degree of cooperation; a PCE is a 0-PCE.
Every game has a Pareto-optimal max-PCE (M-PCE); that is, an {\alpha}-PCE for a
maximum {\alpha}. We show that M-PCE does well at predicting behavior in quite
a few games of interest. We also consider cooperative equilibrium (CE), another
generalization of PCE that takes punishment into account. Interestingly, all
Pareto-optimal M-PCE are CE. We prove that, in 2-player games, a PCE (if it
exists), a M-PCE, and a CE can all be found in polynomial time using bilinear
programming. This is a contrast to Nash equilibrium, which is PPAD complete
even in 2-player games [Chen, Deng, and Teng 2009]. We compare M-PCE to the
coco value [Kalai and Kalai 2009], another solution concept that tries to
capture cooperation, both axiomatically and in terms of an algebraic
characterization, and show that the two are closely related, despite their very
different definitions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6724</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6724</id><created>2014-12-21</created><updated>2015-09-25</updated><authors><author><keyname>Mo</keyname><forenames>Dian</forenames></author><author><keyname>Duarte</keyname><forenames>Marco F.</forenames></author></authors><title>Performance of Compressive Parameter Estimation via K-Median Clustering</title><categories>cs.IT math.IT</categories><comments>29 pages, 6 figures; revision includes additional discussions and
  experiment</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, compressive sensing (CS) has attracted significant attention
in parameter estimation tasks, including frequency estimation, time delay
estimation, and localization. In order to use CS in parameter estimation,
parametric dictionaries (PDs) collect observations for a sampling of the
parameter space and yield sparse representations for signals of interest when
the sampling is sufficiently dense. While this dense sampling can lead to high
coherence in the dictionary, it is possible to leverage structured sparsity
models to prevent highly coherent dictionary elements from appearing
simultaneously in the signal representations, alleviating these coherence
issues. However, the resulting approaches depend heavily on a careful setting
of the maximum allowable coherence; furthermore, their guarantees applied on
the coefficient recovery do not translate in general to the parameter
estimation task. In this paper, we propose the use of the earth mover's
distance (EMD), as applied to a pair of true and estimated coefficient vectors,
to measure the error of the parameter estimation. We formally analyze the
connection between the aforementioned EMD and the parameter estimation error.
We theoretically show that the EMD provides a better-suited metric for the
performance of PD-based parameter estimation than the commonly used Euclidean
distance. Additionally, we leverage the previously described relationship
between K-median clustering and EMD-based sparse approximation to develop
improved PD-based parameter estimation algorithms. Finally, we present
numerical experiments that verify our theoretical results and show the
performance improvements obtained from the proposed compressive parameter
estimation algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6726</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6726</id><created>2014-12-21</created><updated>2015-05-28</updated><authors><author><keyname>Chen</keyname><forenames>Xudong</forenames></author></authors><title>Decentralized Formation Control with A Quadratic Lyapunov Function</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate a decentralized formation control algorithm for
an undirected formation control model. Unlike other formation control problems
where only the shape of a configuration counts, we emphasize here also its
Euclidean embedding. By following this decentralized formation control law, the
agents will converge to certain equilibrium of the control system. In
particular, we show that there is a quadratic Lyapunov function associated with
the formation control system whose unique local (global) minimum point is the
target configuration. In view of the fact that there exist multiple equilibria
(in fact, a continuum of equilibria) of the formation control system, and hence
there are solutions of the system which converge to some equilibria other than
the target configuration, we apply simulated annealing, as a heuristic method,
to the formation control law to fix this problem. Simulation results show that
sample paths of the modified stochastic system approach the target
configuration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6730</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6730</id><created>2014-12-21</created><updated>2015-07-14</updated><authors><author><keyname>Sanfelice</keyname><forenames>Ricardo G.</forenames></author><author><keyname>Praly</keyname><forenames>Laurent</forenames></author></authors><title>Convergence of Nonlinear Observers on R^n with a Riemannian Metric (Part
  I)</title><categories>math.OC cs.SY math.DG</categories><comments>30 pages, Version published in IEEE Transactions in Automatic
  Control, 2012</comments><doi>10.1109/TAC.2011.2179873</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study how convergence of an observer whose state lives in a copy of the
given system's space can be established using a Riemannian metric. We show that
the existence of an observer guaranteeing the property that a Riemannian
distance between system and observer solutions is nonincreasing implies that
the Lie derivative of the Riemannian metric along the system vector field is
conditionally negative. Moreover, we establish that the existence of this
metric is related to the observability of the system's linearization along its
solutions. Moreover, if the observer has an infinite gain margin then the level
sets of the output function are geodesically convex. Conversely, we establish
that, if a complete Riemannian metric has a Lie derivative along the system
vector field that is conditionally negative and is such that the output
function has a monotonicity property, then there exists an observer with an
infinite gain margin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6731</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6731</id><created>2014-12-21</created><authors><author><keyname>Chen</keyname><forenames>Xudong</forenames></author></authors><title>Adjacency Criterion For Gradient Flow With Multiple Local Maxima</title><categories>math.DS cs.SY math.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the geometry of a general class of gradient
flows with multiple local maxima. we decompose the underlying space into
disjoint regions of attraction and establish the adjacency criterion. The
criterion states a necessary and sufficient condition for two regions of
attraction of stable equilibria to be adjacent. We then apply this criterion on
a specific type of gradient flow which has as many as n! local maxima. In
particular, we characterize the set of equilibria, compute the index of each
critical manifold and moreover, find all pairs of adjacent neighbors. As an
application of the adjacency criterion, we introduce a stochastic version of
the double bracket flow and set up a Markov model to approximate the sample
path behavior. The study of this specific prototype with its special structure
provides insight into many other difficult problems involving simulated
annealing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6734</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6734</id><created>2014-12-21</created><authors><author><keyname>Tamar</keyname><forenames>Aviv</forenames></author><author><keyname>Toulis</keyname><forenames>Panos</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author><author><keyname>Airoldi</keyname><forenames>Edoardo M.</forenames></author></authors><title>Implicit Temporal Differences</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In reinforcement learning, the TD($\lambda$) algorithm is a fundamental
policy evaluation method with an efficient online implementation that is
suitable for large-scale problems. One practical drawback of TD($\lambda$) is
its sensitivity to the choice of the step-size. It is an empirically well-known
fact that a large step-size leads to fast convergence, at the cost of higher
variance and risk of instability. In this work, we introduce the implicit
TD($\lambda$) algorithm which has the same function and computational cost as
TD($\lambda$), but is significantly more stable. We provide a theoretical
explanation of this stability and an empirical evaluation of implicit
TD($\lambda$) on typical benchmark tasks. Our results show that implicit
TD($\lambda$) outperforms standard TD($\lambda$) and a state-of-the-art method
that automatically tunes the step-size, and thus shows promise for wide
applicability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6741</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6741</id><created>2014-12-21</created><authors><author><keyname>Li</keyname><forenames>Kim-Hung</forenames></author><author><keyname>Li</keyname><forenames>Cheuk Ting</forenames></author></authors><title>Locally Weighted Learning for Naive Bayes Classifier</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As a consequence of the strong and usually violated conditional independence
assumption (CIA) of naive Bayes (NB) classifier, the performance of NB becomes
less and less favorable compared to sophisticated classifiers when the sample
size increases. We learn from this phenomenon that when the size of the
training data is large, we should either relax the assumption or apply NB to a
&quot;reduced&quot; data set, say for example use NB as a local model. The latter
approach trades the ignored information for the robustness to the model
assumption. In this paper, we consider using NB as a model for locally weighted
data. A special weighting function is designed so that if CIA holds for the
unweighted data, it also holds for the weighted data. The new method is
intuitive and capable of handling class imbalance. It is theoretically more
sound than the locally weighted learners of naive Bayes that base
classification only on the $k$ nearest neighbors. Empirical study shows that
the new method with appropriate choice of parameter outperforms seven existing
classifiers of similar nature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6747</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6747</id><created>2014-12-21</created><authors><author><keyname>Liang</keyname><forenames>Ning</forenames></author><author><keyname>zhang</keyname><forenames>Wenyi</forenames></author><author><keyname>Shen</keyname><forenames>Cong</forenames></author></authors><title>An Uplink Interference Analysis for Massive MIMO Systems with MRC and ZF
  Receivers</title><categories>cs.IT math.IT</categories><comments>7 pages, 4 figures, accepted by IEEE Wireless Communications and
  Networking Conference (WCNC) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers an uplink cellular system, in which each base station
(BS) is equipped with a large number of antennas to serve multiple
single-antenna user equipments (UEs) simultaneously. Uplink training with pilot
reusing is adopted to acquire the channel state information (CSI) and maximum
ratio combining (MRC) or zero forcing (ZF) reception is used for handling
multiuser interference. Leveraging stochastic geometry to model the spatial
distribution of UEs, we analyze the statistical distributions of the
interferences experienced by a typical uplink: intra-cell interference,
inter-cell interference and interference due to pilot contamination.
  For a practical but still large number of BS antennas, a key observation for
MRC reception is that it is the intra-cell interference that accounts for the
dominant portion of the total interference. In addition, the interference due
to pilot contamination tends to have a much wider distribution range than the
inter-cell interference when shadowing is strong, although their mean powers
are roughly equal. For ZF reception, on the other hand, we observe a
significant reduction of the intra-cell interference compared to MRC reception,
while the inter-cell interference and the interference due to pilot
contamination remains almost the same, thus demonstrating a substantial
superiority over MRC reception.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6749</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6749</id><created>2014-12-21</created><authors><author><keyname>Ibraheem</keyname><forenames>Abdulrahman Oladipupo</forenames></author></authors><title>SENNS: Sparse Extraction Neural NetworkS for Feature Extraction</title><categories>cs.CV cs.AI cs.NE math.OC stat.ML</categories><comments>Eighteen pages in all, but much of the central ideas are covered in
  the first five and a half pages; most of the remaining pages are devoted to
  straightforward mathematical derivations, and the presentation of three
  algorithms. Manuscript contains no figures at this time</comments><msc-class>90-08</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By drawing on ideas from optimisation theory, artificial neural networks
(ANN), graph embeddings and sparse representations, I develop a novel
technique, termed SENNS (Sparse Extraction Neural NetworkS), aimed at
addressing the feature extraction problem. The proposed method uses (preferably
deep) ANNs for projecting input attribute vectors to an output space wherein
pairwise distances are maximized for vectors belonging to different classes,
but minimized for those belonging to the same class, while simultaneously
enforcing sparsity on the ANN outputs. The vectors that result from the
projection can then be used as features in any classifier of choice.
Mathematically, I formulate the proposed method as the minimisation of an
objective function which can be interpreted, in the ANN output space, as a
negative factor of the sum of the squares of the pair-wise distances between
output vectors belonging to different classes, added to a positive factor of
the sum of squares of the pair-wise distances between output vectors belonging
to the same classes, plus sparsity and weight decay terms. To derive an
algorithm for minimizing the objective function via gradient descent, I use the
multi-variate version of the chain rule to obtain the partial derivatives of
the function with respect to ANN weights and biases, and find that each of the
required partial derivatives can be expressed as a sum of six terms. As it
turns out, four of those six terms can be computed using the standard back
propagation algorithm; the fifth can be computed via a slight modification of
the standard backpropagation algorithm; while the sixth one can be computed via
simple arithmetic. Finally, I propose experiments on the ARABASE Arabic corpora
of digits and letters, the CMU PIE database of faces, the MNIST digits
database, and other standard machine learning databases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6752</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6752</id><created>2014-12-21</created><authors><author><keyname>Ibraheem</keyname><forenames>Abdulrahman Oladipupo</forenames></author></authors><title>Correlation of Data Reconstruction Error and Shrinkages in Pair-wise
  Distances under Principal Component Analysis (PCA)</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this on-going work, I explore certain theoretical and empirical
implications of data transformations under the PCA. In particular, I state and
prove three theorems about PCA, which I paraphrase as follows: 1). PCA without
discarding eigenvector rows is injective, but looses this injectivity when
eigenvector rows are discarded 2). PCA without discarding eigen- vector rows
preserves pair-wise distances, but tends to cause pair-wise distances to shrink
when eigenvector rows are discarded. 3). For any pair of points, the shrinkage
in pair-wise distance is bounded above by an L1 norm reconstruction error
associated with the points. Clearly, 3). suggests that there might exist some
correlation between shrinkages in pair-wise distances and mean square
reconstruction error which is defined as the sum of those eigenvalues
associated with the discarded eigenvectors. I therefore decided to perform
numerical experiments to obtain the corre- lation between the sum of those
eigenvalues and shrinkages in pair-wise distances. In addition, I have also
performed some experiments to check respectively the effect of the sum of those
eigenvalues and the effect of the shrinkages on classification accuracies under
the PCA map. So far, I have obtained the following results on some publicly
available data from the UCI Machine Learning Repository: 1). There seems to be
a strong cor- relation between the sum of those eigenvalues associated with
discarded eigenvectors and shrinkages in pair-wise distances. 2). Neither the
sum of those eigenvalues nor pair-wise distances have any strong correlations
with classification accuracies. 1
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6753</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6753</id><created>2014-12-21</created><authors><author><keyname>Zhou</keyname><forenames>Yanbo</forenames></author><author><keyname>Zeng</keyname><forenames>An</forenames></author><author><keyname>Wang</keyname><forenames>Wei-Hong</forenames></author></authors><title>Temporal effects in trend prediction: identifying the most popular nodes
  in the future</title><categories>cs.SI physics.soc-ph</categories><doi>10.1371/journal.pone.0120735</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Prediction is an important problem in different science domains. In this
paper, we focus on trend prediction in complex networks, i.e. to identify the
most popular nodes in the future. Due to the preferential attachment mechanism
in real systems, nodes' recent degree and cumulative degree have been
successfully applied to design trend prediction methods. Here we took into
account more detailed information about the network evolution and proposed a
temporal-based predictor (TBP). The TBP predicts the future trend by the node
strength in the weighted network with the link weight equal to its exponential
aging. Three data sets with time information are used to test the performance
of the new method. We find that TBP have high general accuracy in predicting
the future most popular nodes. More importantly, it can identify many potential
objects with low popularity in the past but high popularity in the future. The
effect of the decay speed in the exponential aging on the results is discussed
in detail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6755</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6755</id><created>2014-12-21</created><authors><author><keyname>M&#xf6;mke</keyname><forenames>Tobias</forenames></author></authors><title>An Improved Approximation Algorithm for the Traveling Salesman Problem
  with Relaxed Triangle Inequality</title><categories>cs.DS</categories><comments>8 pages, 1 figure</comments><msc-class>05C45, 05C85</msc-class><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a complete edge-weighted graph G, we present a polynomial time
algorithm to compute a degree-four-bounded spanning Eulerian subgraph of 2G
that has at most 1.5 times the weight of an optimal TSP solution of G. Based on
this algorithm and a novel use of orientations in graphs, we obtain a (3 beta/4
+ 3 beta^2/4)-approximation algorithm for TSP with beta-relaxed triangle
inequality (beta-TSP), where beta &gt;= 1. A graph G is an instance of beta-TSP,
if it is a complete graph with non-negative edge weights that are restricted as
follows. For each triple of vertices u,v,w in V(G), c({u,v}) &lt;= beta (c({u,w})
+ c({w,v})).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6759</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6759</id><created>2014-12-21</created><authors><author><keyname>Ibraheem</keyname><forenames>Abdulrahman Oladipupo</forenames></author></authors><title>Bi-directional Shape Correspondences (BSC): A Novel Technique for 2-d
  Shape Warping in Quadratic Time?</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose Bidirectional Shape Correspondence (BSC) as a possible improvement
on the famous shape contexts (SC) framework. Our proposals derive from the
observation that the SC framework enforces a one-to-one correspondence between
sample points, and that this leads to two possible drawbacks. First, this
denies the framework the opportunity to effect advantageous many-to-many
matching between points on the two shapes being compared. Second, this calls
for the Hungarian algorithm which unfortunately usurps cubic time. While the
dynamic-space-warping dynamic programming algorithm has provided a standard
solution to the first problem above, it demands quintic time for general
multi-contour shapes, and w times quadratic time for the special case of
single-contour shapes, even after an heuristic search window of width w has
been chosen. Therefore, in this work, we propose a simple method for computing
&quot;many-to-many&quot; correspondences for the class of all 2-d shapes in quadratic
time. Our approach is to explicitly let each point on the first shape choose a
best match on the second shape, and vice versa. Along the way, we also propose
the use of data-clustering techniques for dealing with the outliers problem,
and, from another viewpoint, it turns out that this clustering can be seen as
an autonomous, rather than pre-computed, sampling of shape boundary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6761</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6761</id><created>2014-12-21</created><authors><author><keyname>Nakanishi</keyname><forenames>Masaki</forenames></author><author><keyname>Yakary&#x131;lmaz</keyname><forenames>Abuzer</forenames></author></authors><title>Classical and quantum counter automata on promise problems</title><categories>cs.FL cs.CC quant-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we show that one-way quantum one-counter automaton with
zero-error is more powerful than its probabilistic counterpart on promise
problems. Then, we obtain a similar separation result between Las Vegas one-way
probabilistic one-counter automaton and one-way deterministic one-counter
automaton. Lastly, it was conjectured that one-way probabilistic one
blind-counter automata cannot recognize Kleene closure of equality language [A.
Yakaryilmaz: Superiority of one-way and realtime quantum machines. RAIRO -
Theor. Inf. and Applic. 46(4): 615-641 (2012)]. We show that this conjecture is
false.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6765</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6765</id><created>2014-12-21</created><authors><author><keyname>Halli</keyname><forenames>Nassim A.</forenames></author><author><keyname>Charles</keyname><forenames>Henri-Pierre</forenames></author><author><keyname>Mehaut</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author></authors><title>Performance comparison between Java and JNI for optimal implementation
  of computational micro-kernels</title><categories>cs.PF cs.DC cs.PL</categories><comments>Part of ADAPT Workshop proceedings, 2015 (arXiv:1412.2347)</comments><report-no>ADAPT/2015/06</report-no><acm-class>D.3.4; D.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  General purpose CPUs used in high performance computing (HPC) support a
vector instruction set and an out-of-order engine dedicated to increase the
instruction level parallelism. Hence, related optimizations are currently
critical to improve the performance of applications requiring numerical
computation. Moreover, the use of a Java run-time environment such as the
HotSpot Java Virtual Machine (JVM) in high performance computing is a promising
alternative. It benefits from its programming flexibility, productivity and the
performance is ensured by the Just-In-Time (JIT) compiler. Though, the JIT
compiler suffers from two main drawbacks. First, the JIT is a black box for
developers. We have no control over the generated code nor any feedback from
its optimization phases like vectorization. Secondly, the time constraint
narrows down the degree of optimization compared to static compilers like GCC
or LLVM. So, it is compelling to use statically compiled code since it benefits
from additional optimization reducing performance bottlenecks. Java enables to
call native code from dynamic libraries through the Java Native Interface
(JNI). Nevertheless, JNI methods are not inlined and require an additional cost
to be invoked compared to Java ones. Therefore, to benefit from better static
optimization, this call overhead must be leveraged by the amount of computation
performed at each JNI invocation. In this paper we tackle this problem and we
propose to do this analysis for a set of micro-kernels. Our goal is to select
the most efficient implementation considering the amount of computation defined
by the calling context. We also investigate the impact on performance of
several different optimization schemes which are vectorization, out-of-order
optimization, data alignment, method inlining and the use of native memory for
JNI methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6767</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6767</id><created>2014-12-21</created><authors><author><keyname>Maslennikova</keyname><forenames>Marina</forenames></author><author><keyname>Rodaro</keyname><forenames>Emanuele</forenames></author></authors><title>Representation of (Left) Ideal Regular Languages by Synchronizing
  Automata</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We follow language theoretic approach to synchronizing automata and
\v{C}ern\'{y}'s conjecture initiated in a series of recent papers. We find a
precise lower bound for the reset complexity of a principal ideal languages.
Also we show a strict connection between principal left ideals and
synchronizing automata. We characterize regular languages whose minimal
deterministic finite automaton is synchronizing and possesses a reset word
belonging to the recognized language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6769</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6769</id><created>2014-12-21</created><authors><author><keyname>Atar</keyname><forenames>Rami</forenames></author><author><keyname>Merhav</keyname><forenames>Neri</forenames></author></authors><title>Information-theoretic applications of the logarithmic probability
  comparison bound</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A well-known technique in estimating probabilities of rare events in general
and in information theory in particular (used, e.g., in the sphere-packing
bound), is that of finding a reference probability measure under which the
event of interest has probability of order one and estimating the probability
in question by means of the Kullback-Leibler divergence. A method has recently
been proposed in [2], that can be viewed as an extension of this idea in which
the probability under the reference measure may itself be decaying
exponentially, and the Renyi divergence is used instead. The purpose of this
paper is to demonstrate the usefulness of this approach in various
information-theoretic settings. For the problem of channel coding, we provide a
general methodology for obtaining matched, mismatched and robust error exponent
bounds, as well as new results in a variety of particular channel models. Other
applications we address include rate-distortion coding and the problem of
guessing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6781</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6781</id><created>2014-12-21</created><authors><author><keyname>Graham-Lengrand</keyname><forenames>St&#xe9;phane</forenames></author></authors><title>Polarities &amp; Focussing: a journey from Realisability to Automated
  Reasoning</title><categories>cs.LO</categories><comments>Dissertation submitted towards the degree of Habilitation \`a Diriger
  des Recherches. Universit\'e Paris-Sud. 212 pages. Thesis publicly defended
  on 17th December 2014 before a panel consisting of Laurent Regnier, Wolfgang
  Ahrendt, Hugo Herbelin, Frank Pfenning, Sylvain Conchon, David Delahaye,
  Didier Galmiche, Christine Paulin-Mohring</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This dissertation explores the roles of polarities and focussing in various
aspects of Computational Logic. These concepts play a key role in the the
interpretation of proofs as programs, a.k.a. the Curry-Howard correspondence,
in the context of classical logic. Arising from linear logic, they allow the
construction of meaningful semantics for cut-elimination in classical logic,
some of which relate to the Call-by-Name and Call-by-Value disciplines of
functional programming. The first part of this dissertation provides an
introduction to these interpretations, highlighting the roles of polarities and
focussing. For instance: proofs of positive formulae provide structured data,
while proofs of negative formulae consume such data; focussing allows the
description of the interaction between the two kinds of proofs as pure
pattern-matching. This idea is pushed further in the second part of this
dissertation, and connected to realisability semantics, where the structured
data is interpreted algebraically, and the consumption of such data is modelled
with the use of an orthogonality relation. Most of this part has been proved in
the Coq proof assistant. Polarities and focussing were also introduced with
applications to logic programming in mind, where computation is proof-search.
In the third part of this dissertation, we push this idea further by exploring
the roles that these concepts can play in other applications of proof-search,
such as theorem proving and more particularly automated reasoning. We use these
concepts to describe the main algorithm of SAT-solvers and SMT-solvers: DPLL.
We then describe the implementation of a proof-search engine called Psyche. Its
architecture, based on the concept of focussing, offers a platform where smart
techniques from automated reasoning (or a user interface) can safely and
trustworthily be implemented via the use of an API.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6782</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6782</id><created>2014-12-21</created><authors><author><keyname>Murin</keyname><forenames>Yonathan</forenames></author><author><keyname>Kaspi</keyname><forenames>Yonatan</forenames></author><author><keyname>Dabora</keyname><forenames>Ron</forenames></author></authors><title>On the Ozarow-Leung Scheme for the Gaussian Broadcast Channel with
  Feedback</title><categories>cs.IT math.IT</categories><comments>Accepted to IEEE Signal Processing Letters</comments><doi>10.1109/LSP.2014.2375893</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we consider linear-feedback schemes for the two-user Gaussian
broadcast channel with noiseless feedback. We extend the transmission scheme of
[Ozarow and Leung, 1984] by applying estimators with memory instead of the
memoryless estimators used by Ozarow and Leung (OL) in their original work. A
recursive formulation of the mean square errors achieved by the proposed
estimators is provided, along with a proof for the existence of a fixed point.
This enables characterizing the achievable rates of the extended scheme.
Finally, via numerical simulations it is shown that the extended scheme can
improve upon the original OL scheme in terms of achievable rates, as well as
achieve a low probability of error after a finite number of channel uses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6783</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6783</id><created>2014-12-21</created><updated>2015-06-07</updated><authors><author><keyname>Dosen</keyname><forenames>Kosta</forenames></author></authors><title>On Sets of Premises</title><categories>math.LO cs.LO</categories><comments>12 pages</comments><msc-class>03F03, 03F07</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conceiving of premises as collected into sets or multisets, instead of
sequences, may lead to triviality for classical and intuitionistic logic in
general proof theory, where we investigate identity of deductions. Any two
deductions with the same premises and the same conclusions become equal. In
terms of categorial proof theory, this is a consequence of a simple fact
concerning adjunction with a full and faithful functor applied to the
adjunction between the diagonal functor and the product biendofunctor, which
corresponds to the conjunction connective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6785</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6785</id><created>2014-12-21</created><updated>2015-03-11</updated><authors><author><keyname>Koyamada</keyname><forenames>Sotetsu</forenames></author><author><keyname>Koyama</keyname><forenames>Masanori</forenames></author><author><keyname>Nakae</keyname><forenames>Ken</forenames></author><author><keyname>Ishii</keyname><forenames>Shin</forenames></author></authors><title>Principal Sensitivity Analysis</title><categories>stat.ML cs.LG</categories><doi>10.1007/978-3-319-18038-0_48</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel algorithm (Principal Sensitivity Analysis; PSA) to analyze
the knowledge of the classifier obtained from supervised machine learning
techniques. In particular, we define principal sensitivity map (PSM) as the
direction on the input space to which the trained classifier is most sensitive,
and use analogously defined k-th PSM to define a basis for the input space. We
train neural networks with artificial data and real data, and apply the
algorithm to the obtained supervised classifiers. We then visualize the PSMs to
demonstrate the PSA's ability to decompose the knowledge acquired by the
trained classifiers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6787</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6787</id><created>2014-12-21</created><updated>2015-06-17</updated><authors><author><keyname>Bergstra</keyname><forenames>J. A.</forenames></author><author><keyname>Middelburg</keyname><forenames>C. A.</forenames></author></authors><title>Instruction sequence size complexity of parity</title><categories>cs.CC cs.PL</categories><comments>12 pages, the preliminaries are largely the same as the preliminaries
  in arXiv:1312.1812 [cs.PL] and some earlier papers; 13 pages, minor errors
  corrected</comments><acm-class>F.1.1; F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Each Boolean function can be computed by a single-pass instruction sequence
that contains only instructions to set and get the content of Boolean
registers, forward jump instructions, and a termination instruction. Auxiliary
Boolean registers are not necessary for this. In the current paper, we show
that, in the case of the parity functions, shorter instruction sequences are
possible with the use of an auxiliary Boolean register in the presence of
instructions to complement the content of auxiliary Boolean registers. This
result supports, in a setting where programs are instruction sequences acting
on Boolean registers, a basic intuition behind the storage of auxiliary data,
namely the intuition that this makes possible a reduction of the size of a
program.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6790</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6790</id><created>2014-12-21</created><updated>2015-09-03</updated><authors><author><keyname>Rouhling</keyname><forenames>Damien</forenames></author><author><keyname>Farooque</keyname><forenames>Mahfuza</forenames></author><author><keyname>Graham-Lengrand</keyname><forenames>St&#xe9;phane</forenames></author><author><keyname>Mahboubi</keyname><forenames>Assia</forenames></author><author><keyname>Notin</keyname><forenames>Jean-Marc</forenames></author></authors><title>Axiomatic constraint systems for proof search modulo theories</title><categories>cs.LO</categories><comments>24 pages (incl. appendix)</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Goal-directed proof search in first-order logic uses meta-variables to delay
the choice of witnesses; substitutions for such variables are produced when
closing proof-tree branches, using first-order unification or a theory-specific
background reasoner. This paper investigates a generalisation of such
mechanisms whereby theory-specific constraints are produced instead of
substitutions. In order to design modular proof-search procedures over such
mechanisms, we provide a sequent calculus with meta-variables, which
manipulates such constraints abstractly. Proving soundness and completeness of
the calculus leads to an axiomatisation that identifies the conditions under
which abstract constraints can be generated and propagated in the same way
unifiers usually are. We then extract from our abstract framework a component
interface and a specification for concrete implementations of background
reasoners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6791</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6791</id><created>2014-12-21</created><authors><author><keyname>Katti</keyname><forenames>Anoop</forenames></author><author><keyname>Mittal</keyname><forenames>Anurag</forenames></author></authors><title>Mixture of Parts Revisited: Expressive Part Interactions for Pose
  Estimation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Part-based models with restrictive tree-structured interactions for the Human
Pose Estimation problem, leaves many part interactions unhandled. Two of the
most common and strong manifestations of such unhandled interactions are
self-occlusion among the parts and the confusion in the localization of the
non-adjacent symmetric parts. By handling the self-occlusion in a data
efficient manner, we improve the performance of the basic Mixture of Parts
model by a large margin, especially on uncommon poses. Through addressing the
confusion in the symmetric limb localization using a combination of two
complementing trees, we improve the performance on all the parts by atmost
doubling the running time. Finally, we show that the combination of the two
solutions improves the results. We report results that are equivalent to the
state-of-the-art on two standard datasets. Because of maintaining the
tree-structured interactions and only part-level modeling of the base Mixture
of Parts model, this is achieved in time that is much less than the best
performing part-based model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6793</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6793</id><created>2014-12-21</created><authors><author><keyname>Venkaiah</keyname><forenames>V. Ch.</forenames></author><author><keyname>Ramanjaneyulu</keyname><forenames>K.</forenames></author><author><keyname>Jampala</keyname><forenames>Neelima</forenames></author><author><keyname>Prasad</keyname><forenames>J. Rajendra</forenames></author></authors><title>Equivalence of lower bounds on the number of perfect pairs</title><categories>cs.DM math.CO</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let c(F) be the number of perfect pairs of F and c(G) be the maximum of c(F)
over all (near-) one-factorizations F of G. Wagner showed that for odd n,
c(K_{n}) \geq n*phi(n)/2 and for m and n which are odd and co-prime to each
other, c(K_{mn}) \geq 2*c(K_{m})*c(K_{n}). In this note, we establish that both
these results are equivalent in the sense that they both give rise to the same
lower bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6803</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6803</id><created>2014-12-21</created><updated>2015-01-27</updated><authors><author><keyname>Bonamy</keyname><forenames>Marthe</forenames></author><author><keyname>Hocquard</keyname><forenames>Herv&#xe9;</forenames></author><author><keyname>Kerdjoudj</keyname><forenames>Samia</forenames></author><author><keyname>Raspaud</keyname><forenames>Andr&#xe9;</forenames></author></authors><title>Incidence coloring of graphs with high maximum average degree</title><categories>cs.DM math.CO</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  An incidence of an undirected graph G is a pair $(v,e)$ where $v$ is a vertex
of $G$ and $e$ an edge of $G$ incident with $v$. Two incidences $(v,e)$ and
$(w,f)$ are adjacent if one of the following holds: (i) $v = w$, (ii) $e = f$
or (iii) $vw = e$ or $f$. An incidence coloring of $G$ assigns a color to each
incidence of $G$ in such a way that adjacent incidences get distinct colors. In
2005, Hosseini Dolama \emph{et al.}~\citep{ds05} proved that every graph with
maximum average degree strictly less than $3$ can be incidence colored with
$\Delta+3$ colors. Recently, Bonamy \emph{et al.}~\citep{Bonamy} proved that
every graph with maximum degree at least $4$ and with maximum average degree
strictly less than $\frac{7}{3}$ admits an incidence $(\Delta+1)$-coloring. In
this paper we give bounds for the number of colors needed to color graphs
having maximum average degrees bounded by different values between $4$ and $6$.
In particular we prove that every graph with maximum degree at least $7$ and
with maximum average degree less than $4$ admits an incidence
$(\Delta+3)$-coloring. This result implies that every triangle-free planar
graph with maximum degree at least $7$ is incidence $(\Delta+3)$-colorable. We
also prove that every graph with maximum average degree less than 6 admits an
incidence $(\Delta + 7)$-coloring. More generally, we prove that $\Delta+k-1$
colors are enough when the maximum average degree is less than $k$ and the
maximum degree is sufficiently large.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6806</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6806</id><created>2014-12-21</created><updated>2015-04-13</updated><authors><author><keyname>Springenberg</keyname><forenames>Jost Tobias</forenames></author><author><keyname>Dosovitskiy</keyname><forenames>Alexey</forenames></author><author><keyname>Brox</keyname><forenames>Thomas</forenames></author><author><keyname>Riedmiller</keyname><forenames>Martin</forenames></author></authors><title>Striving for Simplicity: The All Convolutional Net</title><categories>cs.LG cs.CV cs.NE</categories><comments>accepted to ICLR-2015 workshop track; no changes other than style</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most modern convolutional neural networks (CNNs) used for object recognition
are built using the same principles: Alternating convolution and max-pooling
layers followed by a small number of fully connected layers. We re-evaluate the
state of the art for object recognition from small images with convolutional
networks, questioning the necessity of different components in the pipeline. We
find that max-pooling can simply be replaced by a convolutional layer with
increased stride without loss in accuracy on several image recognition
benchmarks. Following this finding -- and building on other recent work for
finding simple network structures -- we propose a new architecture that
consists solely of convolutional layers and yields competitive or state of the
art performance on several object recognition datasets (CIFAR-10, CIFAR-100,
ImageNet). To analyze the network we introduce a new variant of the
&quot;deconvolution approach&quot; for visualizing features learned by CNNs, which can be
applied to a broader range of network structures than existing approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6808</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6808</id><created>2014-12-21</created><updated>2015-08-09</updated><authors><author><keyname>Wu</keyname><forenames>Tong</forenames></author><author><keyname>Bajwa</keyname><forenames>Waheed U.</forenames></author></authors><title>Learning the nonlinear geometry of high-dimensional data: Models and
  algorithms</title><categories>stat.ML cs.CV cs.LG</categories><comments>Extended version of the journal paper accepted for publication in
  IEEE Trans. Signal Processing (20 pages, 7 figures, 4 tables)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern information processing relies on the axiom that high-dimensional data
lie near low-dimensional geometric structures. This paper revisits the problem
of data-driven learning of these geometric structures and puts forth two new
nonlinear geometric models for data describing &quot;related&quot; objects/phenomena. The
first one of these models straddles the two extremes of the subspace model and
the union-of-subspaces model, and is termed the metric-constrained
union-of-subspaces (MC-UoS) model. The second one of these models---suited for
data drawn from a mixture of nonlinear manifolds---generalizes the kernel
subspace model, and is termed the metric-constrained kernel union-of-subspaces
(MC-KUoS) model. The main contributions of this paper in this regard include
the following. First, it motivates and formalizes the problems of MC-UoS and
MC-KUoS learning. Second, it presents algorithms that efficiently learn an
MC-UoS or an MC-KUoS underlying data of interest. Third, it extends these
algorithms to the case when parts of the data are missing. Last, but not least,
it reports the outcomes of a series of numerical experiments involving both
synthetic and real data that demonstrate the superiority of the proposed
geometric models and learning algorithms over existing approaches in the
literature. These experiments also help clarify the connections between this
work and the literature on (subspace and kernel k-means) clustering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6815</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6815</id><created>2014-12-21</created><updated>2015-02-28</updated><authors><author><keyname>Denil</keyname><forenames>Misha</forenames></author><author><keyname>Demiraj</keyname><forenames>Alban</forenames></author><author><keyname>de Freitas</keyname><forenames>Nando</forenames></author></authors><title>Extraction of Salient Sentences from Labelled Documents</title><categories>cs.CL cs.IR cs.LG</categories><comments>arXiv admin note: substantial text overlap with arXiv:1406.3830</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a hierarchical convolutional document model with an architecture
designed to support introspection of the document structure. Using this model,
we show how to use visualisation techniques from the computer vision literature
to identify and extract topic-relevant sentences.
  We also introduce a new scalable evaluation technique for automatic sentence
extraction systems that avoids the need for time consuming human annotation of
validation data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6821</identifier>
 <datestamp>2014-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6821</id><created>2014-12-21</created><authors><author><keyname>Reininghaus</keyname><forenames>Jan</forenames></author><author><keyname>Huber</keyname><forenames>Stefan</forenames></author><author><keyname>Bauer</keyname><forenames>Ulrich</forenames></author><author><keyname>Kwitt</keyname><forenames>Roland</forenames></author></authors><title>A Stable Multi-Scale Kernel for Topological Machine Learning</title><categories>stat.ML cs.CV cs.LG math.AT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Topological data analysis offers a rich source of valuable information to
study vision problems. Yet, so far we lack a theoretically sound connection to
popular kernel-based learning techniques, such as kernel SVMs or kernel PCA. In
this work, we establish such a connection by designing a multi-scale kernel for
persistence diagrams, a stable summary representation of topological features
in data. We show that this kernel is positive definite and prove its stability
with respect to the 1-Wasserstein distance. Experiments on two benchmark
datasets for 3D shape classification/retrieval and texture recognition show
considerable performance gains of the proposed method compared to an
alternative approach that is based on the recently introduced persistence
landscapes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6830</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6830</id><created>2014-12-21</created><updated>2015-04-21</updated><authors><author><keyname>Agostinelli</keyname><forenames>Forest</forenames></author><author><keyname>Hoffman</keyname><forenames>Matthew</forenames></author><author><keyname>Sadowski</keyname><forenames>Peter</forenames></author><author><keyname>Baldi</keyname><forenames>Pierre</forenames></author></authors><title>Learning Activation Functions to Improve Deep Neural Networks</title><categories>cs.NE cs.CV cs.LG stat.ML</categories><comments>Accepted as a workshop paper contribution at the International
  Conference on Learning Representations (ICLR) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Artificial neural networks typically have a fixed, non-linear activation
function at each neuron. We have designed a novel form of piecewise linear
activation function that is learned independently for each neuron using
gradient descent. With this adaptive activation function, we are able to
improve upon deep neural network architectures composed of static rectified
linear units, achieving state-of-the-art performance on CIFAR-10 (7.51%),
CIFAR-100 (30.83%), and a benchmark from high-energy physics involving Higgs
boson decay modes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6833</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6833</id><created>2014-12-21</created><authors><author><keyname>J&#xf8;rgensen</keyname><forenames>Jakob S.</forenames></author><author><keyname>Sidky</keyname><forenames>Emil Y.</forenames></author></authors><title>How little data is enough? Phase-diagram analysis of
  sparsity-regularized X-ray CT</title><categories>math.OC cs.IT math.IT</categories><comments>24 pages, 13 figures</comments><doi>10.1098/rsta.2014.0387</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce phase-diagram analysis, a standard tool in compressed sensing,
to the X-ray CT community as a systematic method for determining how few
projections suffice for accurate sparsity-regularized reconstruction. In
compressed sensing a phase diagram is a convenient way to study and express
certain theoretical relations between sparsity and sufficient sampling. We
adapt phase-diagram analysis for empirical use in X-ray CT for which the same
theoretical results do not hold. We demonstrate in three case studies the
potential of phase-diagram analysis for providing quantitative answers to
questions of undersampling: First we demonstrate that there are cases where
X-ray CT empirically performs comparable with an optimal compressed sensing
strategy, namely taking measurements with Gaussian sensing matrices. Second, we
show that, in contrast to what might have been anticipated, taking randomized
CT measurements does not lead to improved performance compared to standard
structured sampling patterns. Finally, we show preliminary results of how well
phase-diagram analysis can predict the sufficient number of projections for
accurately reconstructing a large-scale image of a given sparsity by means of
total-variation regularization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6841</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6841</id><created>2014-12-21</created><authors><author><keyname>Liu</keyname><forenames>Shiping</forenames></author><author><keyname>Peyerimhoff</keyname><forenames>Norbert</forenames></author><author><keyname>Vdovina</keyname><forenames>Alina</forenames></author></authors><title>Signatures, lifts, and eigenvalues of graphs</title><categories>math.CO cs.DM</categories><comments>12 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the spectra of cyclic signatures of finite graphs and the
corresponding cyclic lifts. Starting from a bipartite Ramanujan graph, we prove
the existence of an infinite tower of $3$-cyclic lifts, each of which is again
Ramanujan.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6843</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6843</id><created>2014-12-21</created><authors><author><keyname>Lin</keyname><forenames>Xingqin</forenames></author><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author></authors><title>Connectivity of Millimeter Wave Networks with Multi-hop Relaying</title><categories>cs.IT cs.NI math.IT</categories><comments>10 pages; 3 figures; submitted to IEEE Wireless Communications
  Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel stochastic geometry approach to the connectivity
of milimeter wave (mmWave) networks with multi-hop relaying. The random
positions and shapes of obstacles in the radio environment are modeled as a
Boolean model, whose germs are distributed according to a Poisson point process
and grains are random rectangles. The derived analytical results shed light on
how the connectivity of mmWave networks depends on key system parameters such
as the density and size of obstacles as well as relaying route window -- the
range of distances in which routing relays are selected. We find that multi-hop
relaying can greatly improve the connectivity versus single hop mmWave
transmission. We show that to obtain near-optimal connectivity the relaying
route window should be about the size of the obstacles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6847</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6847</id><created>2014-12-21</created><authors><author><keyname>Lu</keyname><forenames>Feng</forenames></author><author><keyname>Chen</keyname><forenames>Ziqiang</forenames></author></authors><title>A New Way to Factorize Linear Cameras</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The implementation details of factorizing the 3x4 projection matrices of
linear cameras into their left matrix factors and the 4x4 homogeneous
central(also parallel for infinite center cases) projection factors are
presented in this work. Any full row rank 3x4 real matrix can be factorized
into such basic matrices which will be called LC factors.
  A further extension to multiple view midpoint triangulation, for both pinhole
and affine camera cases, is also presented based on such camera factorizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6850</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6850</id><created>2014-12-21</created><updated>2015-04-05</updated><authors><author><keyname>Mendoza-Trejo</keyname><forenames>O.</forenames></author><author><keyname>Cruz-Villar</keyname><forenames>Carlos A.</forenames></author><author><keyname>Pe&#xf3;n-Escalante</keyname><forenames>R.</forenames></author><author><keyname>Zambrano-Arjona</keyname><forenames>M. A.</forenames></author><author><keyname>Penunuri</keyname><forenames>F.</forenames></author></authors><title>Synthesis Method for the Spherical 4R Mechanism with Minimum Center of
  Mass Acceleration</title><categories>cs.CE</categories><comments>6 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the mechanisms area, minimization of the magnitude of the acceleration of
the center of mass (ACoM) implies shaking force balancing. For a mechanism
operating in cycles, the case when the ACoM is zero implies that the
gravitational potential energy (GPE) is constant. This article shows an
efficient and effective optimum synthesis method for minimum acceleration of
the center of mass of a spherical 4R mechanism by using dual functions and the
counterweights balancing method. Once the dual function for ACoM has been
written, one can minimize the shaking forces from a kinematic point of view. We
present the synthesis of a spherical 4R mechanism for the case of a path
generation task. The synthesis process involves the optimization of two
objective functions, this multiobjective problem is solved by using the
weighted sum method implemented in the evolutionary algorithm known as
Differential Evolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6853</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6853</id><created>2014-12-21</created><authors><author><keyname>Fabbri</keyname><forenames>Renato</forenames></author><author><keyname>Junior</keyname><forenames>Vilson Vieira da Silva</forenames></author><author><keyname>Pessotti</keyname><forenames>Ant&#xf4;nio Carlos Silvano</forenames></author><author><keyname>Corr&#xea;a</keyname><forenames>D&#xe9;bora Cristina</forenames></author></authors><title>Psychophysics of musical elements in the discrete-time representation of
  sound</title><categories>cs.SD physics.pop-ph</categories><comments>A software toolbox is available in which each equation is
  implemented. This article is a condensed translation of the original
  dissertation, at:
  https://github.com/ttm/dissertacao/blob/master/dissertacaoCorrigida.pdf?raw=true</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Notes, ornaments and intervals are examples of basic musical elements. Their
representation as discrete-time digital audio plays a central role in software
for music creation and design. Nevertheless, there is no systematic relation,
in analytical terms, of these musical elements to the sonic samples. Such a
compendium enables scientific experiments in precise and trustful ways, among
educational and artistic uses. This paper presents a comprehensive description
of music in digital audio, within an unified approach. Musical elements, like
pitch, duration and timbre are expressed by equations on sample level. This
quantitatively relates characteristics of the discrete-time signal to musical
qualities. Internal variations, e.g. tremolos, vibratos and spectral
fluctuations, are also considered to operate within a note. Moreover, the
generation of musical structures such as rhythmic meter, pitch intervals and
cycles, are attained canonically with notes. The availability of these
resources in scripts is provided in public domain within the \massa\ toolbox -
Music and Audio in Sequences and Samples. Authors observe that the
implementation of sample-domain analytical results as open source can encourage
concise research. As further illustrated in the paper, \massa\ has already been
employed by users for diverse purposes, including acoustics experimentation,
art and education. The efficacy of these physical descriptions was confirmed by
the synthesis of small musical pieces. As shown, it is possible to synthesize
whole albums through collage of scripts and parametrization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6856</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6856</id><created>2014-12-21</created><updated>2015-04-15</updated><authors><author><keyname>Zhou</keyname><forenames>Bolei</forenames></author><author><keyname>Khosla</keyname><forenames>Aditya</forenames></author><author><keyname>Lapedriza</keyname><forenames>Agata</forenames></author><author><keyname>Oliva</keyname><forenames>Aude</forenames></author><author><keyname>Torralba</keyname><forenames>Antonio</forenames></author></authors><title>Object Detectors Emerge in Deep Scene CNNs</title><categories>cs.CV cs.NE</categories><comments>12 pages, ICLR 2015 conference paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the success of new computational architectures for visual processing,
such as convolutional neural networks (CNN) and access to image databases with
millions of labeled examples (e.g., ImageNet, Places), the state of the art in
computer vision is advancing rapidly. One important factor for continued
progress is to understand the representations that are learned by the inner
layers of these deep architectures. Here we show that object detectors emerge
from training CNNs to perform scene classification. As scenes are composed of
objects, the CNN for scene classification automatically discovers meaningful
objects detectors, representative of the learned scene categories. With object
detectors emerging as a result of learning to recognize scenes, our work
demonstrates that the same network can perform both scene recognition and
object localization in a single forward-pass, without ever having been
explicitly taught the notion of objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6857</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6857</id><created>2014-12-21</created><updated>2015-05-12</updated><authors><author><keyname>Hwang</keyname><forenames>Jyh-Jing</forenames></author><author><keyname>Liu</keyname><forenames>Tyng-Luh</forenames></author></authors><title>Contour Detection Using Cost-Sensitive Convolutional Neural Networks</title><categories>cs.CV cs.LG cs.NE</categories><comments>9 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of contour detection via per-pixel classifications of
edge point. To facilitate the process, the proposed approach leverages with
DenseNet, an efficient implementation of multiscale convolutional neural
networks (CNNs), to extract an informative feature vector for each pixel and
uses an SVM classifier to accomplish contour detection. The main challenge lies
in adapting a pre-trained per-image CNN model for yielding per-pixel image
features. We propose to base on the DenseNet architecture to achieve pixelwise
fine-tuning and then consider a cost-sensitive strategy to further improve the
learning with a small dataset of edge and non-edge image patches. In the
experiment of contour detection, we look into the effectiveness of combining
per-pixel features from different CNN layers and obtain comparable performances
to the state-of-the-art on BSDS500.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6862</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6862</id><created>2014-12-21</created><authors><author><keyname>Islam</keyname><forenames>Shohidul</forenames></author><author><keyname>Kim</keyname><forenames>Cheol-Hong</forenames></author><author><keyname>Kim</keyname><forenames>Jong-Myon</forenames></author></authors><title>Computationally Efficient Implementation of a Hamming Code Decoder using
  a Graphics Processing Unit</title><categories>cs.DC cs.IT math.IT</categories><comments>6 pages, 8 figures, Journal of Communications and Networks</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a computationally efficient implementation of a Hamming
code decoder on a graphics processing unit (GPU) to support real-time
software-defined radio (SDR), which is a software alternative for realizing
wireless communication. The Hamming code algorithm is challenging to
parallelize effectively on a GPU because it works on sparsely located data
items with several conditional statements, leading to non-coalesced, long
latency, global memory access, and huge thread divergence. To address these
issues, we propose an optimized implementation of the Hamming code on the GPU
to exploit the higher parallelism inherent in the algorithm. Experimental
results using a compute unified device architecture (CUDA)-enabled NVIDIA
GeForce GTX 560, including 335 cores, revealed that the proposed approach
achieved a 99x speedup versus the equivalent CPU-based implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6881</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6881</id><created>2014-12-22</created><updated>2015-04-16</updated><authors><author><keyname>Nam</keyname><forenames>Jinseok</forenames></author><author><keyname>F&#xfc;rnkranz</keyname><forenames>Johannes</forenames></author></authors><title>On Learning Vector Representations in Hierarchical Label Spaces</title><categories>cs.LG cs.CL stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important problem in multi-label classification is to capture label
patterns or underlying structures that have an impact on such patterns. This
paper addresses one such problem, namely how to exploit hierarchical structures
over labels. We present a novel method to learn vector representations of a
label space given a hierarchy of labels and label co-occurrence patterns. Our
experimental results demonstrate qualitatively that the proposed method is able
to learn regularities among labels by exploiting a label hierarchy as well as
label co-occurrences. It highlights the importance of the hierarchical
information in order to obtain regularities which facilitate analogical
reasoning over a label space. We also experimentally illustrate the dependency
of the learned representations on the label hierarchy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6882</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6882</id><created>2014-12-22</created><authors><author><keyname>Yan</keyname><forenames>Shihao</forenames></author><author><keyname>Malaney</keyname><forenames>Robert</forenames></author></authors><title>Secrecy Performance Analysis of Location-Based Beamforming in Rician
  Wiretap Channels</title><categories>cs.IT cs.NI math.IT</categories><comments>10 pages, 6 figures</comments><doi>10.1109/TWC.2015.2510635</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new optimal Location-Based Beamforming (LBB) scheme for the
wiretap channel, where both the main channel and the eavesdropper's channel are
subject to Rician fading. In our LBB scheme the two key inputs are the location
of the legitimate receiver and the location of the potential eavesdropper.
Notably, our scheme does not require as direct inputs any channel state
information of the main channel or the eavesdropper's channel, making it easy
to deploy in a host of application settings in which the location inputs are
known. Our beamforming solution assumes a multiple-antenna transmitter, a
multiple-antenna eavesdropper, and a single-antenna receiver, and its aim is to
maximize the physical layer security of the channel. To obtain our solution we
first derive the secrecy outage probability of the LBB scheme in a closed-form
expression that is valid for arbitrary values of the Rician K-factors of the
main channel and the eavesdropper's channel. Using this expression we then
determine the location-based beamformer solution that minimizes the secrecy
outage probability. To assess the usefulness of our new scheme, and to quantify
the value of the location information to the beamformer, we compare our scheme
to other schemes, some of which do not utilize any location information. Our
new beamformer solution provides optimal physical layer security for a wide
range of location-based applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6883</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6883</id><created>2014-12-22</created><authors><author><keyname>Al-Ameen</keyname><forenames>Mahdi Nasrullah</forenames></author><author><keyname>Wright</keyname><forenames>Matthew</forenames></author></authors><title>iPersea : The Improved Persea with Sybil Detection Mechanism</title><categories>cs.CR</categories><comments>10 pages</comments><acm-class>C.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  P2P systems are highly susceptible to Sybil attacks, in which an attacker
creates a large number of identities and uses them to control a substantial
fraction of the system. Persea is the most recent approach towards designing a
social network based Sybil-resistant DHT. Unlike prior Sybil-resistant P2P
systems based on social networks, Persea does not rely on two key assumptions:
(i) that the social network is fast mixing, and (ii) that there is a small
ratio of attack edges to honest peers. Both assumptions have been shown to be
unreliable in real social networks. The hierarchical distribution of node IDs
in Persea confines a large attacker botnet to a considerably smaller region of
the ID space than in a normal P2P system and its replication mechanism lets a
peer to retrieve the desired results even if a given region is occupied by
attackers. However, Persea system suffers from certain limitations, since it
cannot handle the scenario, where the malicious target returns an incorrect
result instead of just ignoring the lookup request. In this paper, we address
this major limitation of Persea through a Sybil detection mechanism built on
top of Persea system, which accommodates inspection lookup, a specially
designed lookup scheme to detect the Sybil nodes based on their responses to
the lookup query. We design a scheme to filter those detected Sybils to ensure
the participation of honest nodes on the lookup path during regular DHT lookup.
Since the malicious nodes are opt-out from the lookup path in our system, they
cannot return any incorrect result during regular lookup. We evaluate our
system in simulations with social network datasets and the results show that
catster, the largest network in our simulation with 149700 nodes and 5449275
edges, gains 100% lookup success rate, even when the number of attack edges is
equal to the number of benign peers in the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6885</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6885</id><created>2014-12-22</created><authors><author><keyname>Yuan</keyname><forenames>Jun</forenames></author><author><keyname>Ni</keyname><forenames>Bingbing</forenames></author><author><keyname>Kassim</keyname><forenames>Ashraf A.</forenames></author></authors><title>Half-CNN: A General Framework for Whole-Image Regression</title><categories>cs.CV cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Convolutional Neural Network (CNN) has achieved great success in image
classification. The classification model can also be utilized at image or patch
level for many other applications, such as object detection and segmentation.
In this paper, we propose a whole-image CNN regression model, by removing the
full connection layer and training the network with continuous feature maps.
This is a generic regression framework that fits many applications. We
demonstrate this method through two tasks: simultaneous face detection &amp;
segmentation, and scene saliency prediction. The result is comparable with
other models in the respective fields, using only a small scale network. Since
the regression model is trained on corresponding image / feature map pairs,
there are no requirements on uniform input size as opposed to the
classification model. Our framework avoids classifier design, a process that
may introduce too much manual intervention in model development. Yet, it is
highly correlated to the classification network and offers some in-deep review
of CNN structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6890</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6890</id><created>2014-12-22</created><authors><author><keyname>Narasimhan</keyname><forenames>Balasubramanian</forenames></author><author><keyname>Rubin</keyname><forenames>Daniel L.</forenames></author><author><keyname>Gross</keyname><forenames>Samuel M.</forenames></author><author><keyname>Bendersky</keyname><forenames>Marina</forenames></author><author><keyname>Lavori</keyname><forenames>Philip W.</forenames></author></authors><title>Software for Distributed Computation on Medical Databases: A
  Demonstration Project</title><categories>stat.CO cs.MS cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bringing together the information latent in distributed medical databases
promises to personalize medical care by enabling reliable, stable modeling of
outcomes with rich feature sets (including patient characteristics and
treatments received). However, there are barriers to aggregation of medical
data, due to lack of standardization of ontologies, privacy concerns,
proprietary attitudes toward data, and a reluctance to give up control over end
use. Aggregation of data is not always necessary for model fitting. In models
based on maximizing a likelihood, the computations can be distributed, with
aggregation limited to the intermediate results of calculations on local data,
rather than raw data. Distributed fitting is also possible for singular value
decomposition. There has been work on the technical aspects of shared
computation for particular applications, but little has been published on the
software needed to support the &quot;social networking&quot; aspect of shared computing,
to reduce the barriers to collaboration. We describe a set of software tools
that allow the rapid assembly of a collaborative computational project, based
on the flexible and extensible R statistical software and other open source
packages, that can work across a heterogeneous collection of database
environments, with full transparency to allow local officials concerned with
privacy protections to validate the safety of the method. We describe the
principles, architecture, and successful test results for the site-stratified
Cox model and rank-k Singular Value Decomposition (SVD).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6892</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6892</id><created>2014-12-22</created><authors><author><keyname>Sun</keyname><forenames>Jian</forenames></author><author><keyname>Wu</keyname><forenames>Tianqi</forenames></author><author><keyname>Gu</keyname><forenames>Xianfeng</forenames></author><author><keyname>Luo</keyname><forenames>Feng</forenames></author></authors><title>Discrete Conformal Deformation: Algorithm and Experiments</title><categories>cs.CG cs.DM</categories><comments>34 pages, 22 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a definition of discrete conformality for
triangulated surfaces with flat cone metrics and describe an algorithm for
solving the problem of prescribing curvature, that is to deform the metric
discrete conformally so that the curvature of the resulting metric coincides
with the prescribed curvature. We explicitly construct a discrete conformal map
between the input triangulated surface and the deformed triangulated surface.
Our algorithm can handle the surface with any topology with or without
boundary, and can find a deformed metric for any prescribed curvature
satisfying the Gauss-Bonnet formula. In addition, we present the numerical
examples to show the convergence of our discrete conformality and to
demonstrate the efficiency and the robustness of our algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6912</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6912</id><created>2014-12-22</created><authors><author><keyname>Makki</keyname><forenames>Behrooz</forenames></author><author><keyname>Svensson</keyname><forenames>Tommy</forenames></author><author><keyname>Eriksson</keyname><forenames>Thomas</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Coordinated Hybrid Automatic Repeat Request; Extended Version</title><categories>cs.IT math.IT</categories><journal-ref>IEEE Communications Letters, vol. 18, no. 11, pp-1975-1978, Nov.
  2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a coordinated hybrid automatic repeat request (HARQ) approach.
With the proposed scheme, if a user message is correctly decoded in the first
HARQ rounds, its spectrum is allocated to other users, to improve the network
outage probability and the users' fairness. The results, which are obtained for
single- and multiple-antenna setups, demonstrate the efficiency of the proposed
approach in different conditions. For instance, with a maximum of M
retransmissions and single transmit/receive antennas, the diversity gain of a
user increases from M to (J+1)(M-1)+1 where J is the number of users helping
that user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6913</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6913</id><created>2014-12-22</created><authors><author><keyname>Corominas-Murtra</keyname><forenames>Bernat</forenames></author><author><keyname>Thurner</keyname><forenames>Stefan</forenames></author></authors><title>The weak core and the structure of elites in social multiplex networks</title><categories>physics.soc-ph cs.SI</categories><comments>2 figures, 1 table, 7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent approaches on elite identification highlighted the important role of
{\em intermediaries}, by means of a new definition of the core of a multiplex
network, the {\em generalised} $K$-core. This newly introduced core subgraph
crucially incorporates those individuals who, in spite of not being very
connected, maintain the cohesiveness and plasticity of the core. Interestingly,
it has been shown that the performance on elite identification of the
generalised $K$-core is sensibly better that the standard $K$-core. Here we go
further: Over a multiplex social system, we isolate the community structure of
the generalised $K$-core and we identify the weakly connected regions acting as
bridges between core communities, ensuring the cohesiveness and connectivity of
the core region. This gluing region is the {\em Weak core} of the multiplex
system. We test the suitability of our method on data from the society of
420.000 players of the Massive Multiplayer Online Game {\em Pardus}. Results
show that the generalised $K$-core displays a clearly identifiable community
structure and that the weak core gluing the core communities shows very low
connectivity and clustering. Nonetheless, despite its low connectivity, the
weak core forms a unique, cohesive structure. In addition, we find that members
populating the weak core have the best scores on social performance, when
compared to the other elements of the generalised $K$-core. The weak core
provides a new angle on understanding the social structure of elites,
highlighting those subgroups of individuals whose role is to glue different
communities in the core.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6924</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6924</id><created>2014-12-22</created><updated>2015-04-02</updated><authors><author><keyname>Jaffe</keyname><forenames>Klaus</forenames></author></authors><title>Visualizing the Invisible Hand of Markets: Simulating complex dynamic
  economic interactions</title><categories>cs.MA q-fin.GN</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In complex systems, many different parts interact in non-obvious ways.
Traditional research focuses on a few or a single aspect of the problem so as
to analyze it with the tools available. To get a better insight of phenomena
that emerge from complex interactions, we need instruments that can analyze
simultaneously complex interactions between many parts. Here, a simulator
modeling different types of economies, is used to visualize complex
quantitative aspects that affect economic dynamics. The main conclusions are:
1- Relatively simple economic settings produce complex non-linear dynamics and
therefore linear regressions are often unsuitable to capture complex economic
dynamics; 2- Flexible pricing of goods by individual agents according to their
micro-environment increases the health and wealth of the society, but
asymmetries in price sensitivity between buyers and sellers increase price
inflation; 3- Prices for goods conferring risky long term benefits are not
tracked efficiently by simple market forces. 4- Division of labor creates
synergies that improve enormously the health and wealth of the society by
increasing the efficiency of economic activity. 5- Stochastic modeling improves
our understanding of real economies, and didactic games based on them might
help policy makers and non specialists in grasping the complex dynamics
underlying even simple economic settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6925</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6925</id><created>2014-12-22</created><updated>2015-06-01</updated><authors><author><keyname>Chen</keyname><forenames>Xudong</forenames></author><author><keyname>Belabbas</keyname><forenames>M. -A.</forenames></author><author><keyname>Basar</keyname><forenames>Tamer</forenames></author></authors><title>Controllability of Formations over Time-varying Graphs</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the controllability of a class of formation
control systems. Given a directed graph, we assign an agent to each of its
vertices and let the edges of the graph describe the information flow in the
system. We relate the strongly connected components of this graph to the
reachable set of the formation control system. Moreover, we show that the
formation control model is approximately path-controllable over a
path-connected, open dense subset as long as the graph is weakly connected and
satisfies some mild assumption on the numbers of vertices of the strongly
connected components.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6935</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6935</id><created>2014-12-22</created><authors><author><keyname>Clifford</keyname><forenames>Raphael</forenames></author><author><keyname>Jalsenius</keyname><forenames>Markus</forenames></author><author><keyname>Sach</keyname><forenames>Benjamin</forenames></author></authors><title>Time Bounds for Streaming Problems</title><categories>cs.DS cs.CC</categories><comments>31 pages, 8 figures. arXiv admin note: text overlap with
  arXiv:1207.1885</comments><acm-class>F.2.2; F.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give tight cell-probe bounds for the time to compute convolution,
multiplication and Hamming distance in a stream. The cell probe model is a
particularly strong computational model and subsumes, for example, the popular
word RAM model.
  We first consider online convolution where the task is to output the inner
product between a fixed $n$-dimensional vector and a vector of the $n$ most
recent values from a stream. One symbol of the stream arrives at a time and the
each output must be computed before the next symbols arrives.
  Next we show bounds for online multiplication where the stream consists of
pairs of digits, one from each of two $n$ digit numbers that are to be
multiplied. One pair arrives at a time and the task is to output a single new
digit from the product before the next pair of digits arrives.
  Finally we look at the online Hamming distance problem where the Hamming
distance is outputted instead of the inner product.
  For each of these three problems, we give a lower bound of
$\Omega(\frac{\delta}{w}\log n)$ time on average per output, where $\delta$ is
the number of bits needed to represent an input symbol and $w$ is the cell or
word size. We argue that these bound are in fact tight within the cell probe
model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6937</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6937</id><created>2014-12-22</created><updated>2015-03-27</updated><authors><author><keyname>Chen</keyname><forenames>Xudong</forenames></author><author><keyname>Belabbas</keyname><forenames>M. -A.</forenames></author><author><keyname>Basar</keyname><forenames>Tamer</forenames></author></authors><title>Formation Control with Triangulated Laman Graphs</title><categories>cs.SY math.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Formation control deals with the design of decentralized control laws that
stabilize agents at prescribed distances from each other. We call any
configuration that satisfies the inter-agent distance conditions a target
configuration. It is well known that when the distance conditions are defined
via a rigid graph, there is a finite number of target configurations modulo
rotations and translations. We can thus recast the objective of formation
control as stabilizing one or many of the target configurations. A major issue
is that such control laws will also have equilibria corresponding to
configurations which do not meet the desired inter-agent distance conditions;
we refer to these as undesired equilibria. The undesired equilibria become
problematic if they are also stable. Designing decentralized control laws whose
stable equilibria are all target configurations in the case of a general rigid
graph is still an open problem. We propose here a partial solution to this
problem by exhibiting a class of rigid graphs and control laws for which all
stable equilibria are target configurations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6945</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6945</id><created>2014-12-22</created><authors><author><keyname>Martin</keyname><forenames>Christoph</forenames></author><author><keyname>Niemeyer</keyname><forenames>Peter</forenames></author></authors><title>Comparing the sensitivity of social networks, web graphs, and random
  graphs with respect to vertex removal</title><categories>cs.SI physics.soc-ph</categories><comments>11 pages, 4 figures</comments><doi>10.1109/SITIS.2015.22</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The sensitivity of networks regarding the removal of vertices has been
studied extensively within the last 15 years. A common approach to measure this
sensitivity is (i) removing successively vertices by following a specific
removal strategy and (ii) comparing the original and the modified network using
a specific comparison method. In this paper we apply a wide range of removal
strategies and comparison methods in order to study the sensitivity of
medium-sized networks from real world and randomly generated networks. In the
first part of our study we observe that social networks and web graphs differ
in sensitivity. When removing vertices, social networks are robust, web graphs
are not. This effect is conclusive with the work of Boldi et al. who analyzed
very large networks. For similarly generated random graphs we find that the
sensitivity highly depends on the comparison method. The choice of the removal
strategy has surprisingly marginal impact on the sensitivity as long as we
consider removal strategies implied by common centrality measures. However, it
has a strong effect when removing the vertices in random order.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6946</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6946</id><created>2014-12-22</created><authors><author><keyname>Karpuk</keyname><forenames>David</forenames></author><author><keyname>Ernvall-Hyt&#xf6;nen</keyname><forenames>Anne-Maria</forenames></author><author><keyname>Hollanti</keyname><forenames>Camilla</forenames></author><author><keyname>Viterbo</keyname><forenames>Emanuele</forenames></author></authors><title>Probability Estimates for Fading and Wiretap Channels from Ideal Class
  Zeta Functions</title><categories>cs.IT math.IT math.NT</categories><comments>24 pages. Extends our earlier arxiv submission arxiv.1303.3475</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, new probability estimates are derived for ideal lattice codes
from totally real number fields using ideal class Dedekind zeta functions. In
contrast to previous work on the subject, it is not assumed that the ideal in
question is principal. In particular, it is shown that the corresponding
inverse norm sum depends not only on the regulator and discriminant of the
number field, but also on the values of the ideal class Dedekind zeta
functions. Along the way, we derive an estimate of the number of elements in a
given ideal with a certain algebraic norm within a finite hypercube. We provide
several examples which measure the accuracy and predictive ability of our
theorems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6952</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6952</id><created>2014-12-22</created><updated>2015-10-27</updated><authors><author><keyname>Chen</keyname><forenames>Xudong</forenames></author></authors><title>Swarm Aggregation with Fading Attractions</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of gradient descent for organizing multi-agent systems is widely
appreciated in mathematics and in its real-world applications. Besides a
demonstration of the existence of a local minimum point, in the context of
formation control, gradient descent can be interpreted as providing
decentralized control laws for pairs of neighboring agents. Often, the control
laws between neighboring agents are designed so that two agents repel/attract
each other at a short/long distance. Conventional techniques for proving
convergence of a dynamical system over a Euclidean space are, for example,
constructing a radially unbounded Lyapunov function and then appealing to the
LaSalle's principle. When the attractions between neighboring agents are
nonfading; then, it is well known that the potential function associated with
the multi-agent system is radially unbounded, and hence using LaSalle's
principle is enough for establishing the system convergence. However, if the
attractions are fading; then, using only the LaSalle's arguments may not be
sufficient. In this paper, we assume that interactions between neighboring
agents have fading attractions. We develop, among other things, a new approach
for proving the convergence of the resulting gradient flow; in particular, we
introduce a class of partitions, termed dilute partitions, of frameworks. This
is a rich question relating to classic algorithms such as $k$-mean clustering
and its variants, and is useful for studying other multi-agent problems
concerning about large size formations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6953</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6953</id><created>2014-12-22</created><updated>2015-06-19</updated><authors><author><keyname>Gyori</keyname><forenames>Benjamin M.</forenames></author><author><keyname>Liu</keyname><forenames>Bing</forenames></author><author><keyname>Paul</keyname><forenames>Soumya</forenames></author><author><keyname>Ramanathan</keyname><forenames>R.</forenames></author><author><keyname>Thiagarajan</keyname><forenames>P. S.</forenames></author></authors><title>Approximate probabilistic verification of hybrid systems</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hybrid systems whose mode dynamics are governed by non-linear ordinary
differential equations (ODEs) are often a natural model for biological
processes. However such models are difficult to analyze. To address this, we
develop a probabilistic analysis method by approximating the mode transitions
as stochastic events. We assume that the probability of making a mode
transition is proportional to the measure of the set of pairs of time points
and value states at which the mode transition is enabled. To ensure a sound
mathematical basis, we impose a natural continuity property on the non-linear
ODEs. We also assume that the states of the system are observed at discrete
time points but that the mode transitions may take place at any time between
two successive discrete time points. This leads to a discrete time Markov chain
as a probabilistic approximation of the hybrid system. We then show that for
BLTL (bounded linear time temporal logic) specifications the hybrid system
meets a specification iff its Markov chain approximation meets the same
specification with probability $1$. Based on this, we formulate a sequential
hypothesis testing procedure for verifying -approximately- that the Markov
chain meets a BLTL specification with high probability. Our case studies on
cardiac cell dynamics and the circadian rhythm indicate that our scheme can be
applied in a number of realistic settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6965</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6965</id><created>2014-12-22</created><updated>2015-05-21</updated><authors><author><keyname>De Florio</keyname><forenames>Vincenzo</forenames></author></authors><title>Reflections on organization, emergence, and control in sociotechnical
  systems</title><categories>cs.CY</categories><comments>Preliminary version of a chapter to appear in a forthcoming book
  edited by Robert Macdougall and to be published by Lexington in 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human and artificial organizations may be described as networks of
interacting parts. Those parts exchange data and control information and, as a
result of these interactions, organizations produce emergent behaviors and
purposes -- traits the characterize &quot;the whole&quot; as &quot;greater than the sum of its
parts&quot;. In this chapter it is argued that, rather than a static and immutable
property, emergence should be interpreted as the result of dynamic interactions
between forces of opposite sign: centripetal (positive) forces strengthening
emergence by consolidating the whole and centrifugal (negative) forces that
weaken the social persona and as such are detrimental to emergence. The result
of this interaction is called in this chapter as &quot;quality of emergence&quot;. This
problem is discussed in the context of a particular class of organizations:
conventional hierarchies. We highlight how traditional designs produce
behaviors that may severely impact the quality of emergence. Finally we discuss
a particular class of organizations that do not suffer from the limitations
typical of strict hierarchies and result in greater quality of emergence. In
some case, however, these enhancements are counterweighted by a reduced degree
of controllability and verifiability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6973</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6973</id><created>2014-12-22</created><authors><author><keyname>Lang</keyname><forenames>Guangming</forenames></author></authors><title>Decision-theoretic rough sets-based three-way approximations of
  interval-valued fuzzy sets</title><categories>cs.AI</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In practical situations, interval-valued fuzzy sets are frequently
encountered. In this paper, firstly, we present shadowed sets for interpreting
and understanding interval fuzzy sets. We also provide an analytic solution to
computing the pair of thresholds by searching for a balance of uncertainty in
the framework of shadowed sets. Secondly, we construct errors-based three-way
approximations of interval-valued fuzzy sets. We also provide an alternative
decision-theoretic formulation for calculating the pair of thresholds by
transforming interval-valued loss functions into single-valued loss functions,
in which the required thresholds are computed by minimizing decision costs.
Thirdly, we compute errors-based three-way approximations of interval-valued
fuzzy sets by using interval-valued loss functions. Finally, we employ several
examples to illustrate that how to take an action for an object with
interval-valued membership grade by using interval-valued loss functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6980</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6980</id><created>2014-12-22</created><updated>2015-07-23</updated><authors><author><keyname>Kingma</keyname><forenames>Diederik</forenames></author><author><keyname>Ba</keyname><forenames>Jimmy</forenames></author></authors><title>Adam: A Method for Stochastic Optimization</title><categories>cs.LG</categories><comments>Published as a conference paper at the 3rd International Conference
  for Learning Representations, San Diego, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce Adam, an algorithm for first-order gradient-based optimization
of stochastic objective functions, based on adaptive estimates of lower-order
moments. The method is straightforward to implement, is computationally
efficient, has little memory requirements, is invariant to diagonal rescaling
of the gradients, and is well suited for problems that are large in terms of
data and/or parameters. The method is also appropriate for non-stationary
objectives and problems with very noisy and/or sparse gradients. The
hyper-parameters have intuitive interpretations and typically require little
tuning. Some connections to related algorithms, on which Adam was inspired, are
discussed. We also analyze the theoretical convergence properties of the
algorithm and provide a regret bound on the convergence rate that is comparable
to the best known results under the online convex optimization framework.
Empirical results demonstrate that Adam works well in practice and compares
favorably to other stochastic optimization methods. Finally, we discuss AdaMax,
a variant of Adam based on the infinity norm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6985</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6985</id><created>2014-12-22</created><authors><author><keyname>Knill</keyname><forenames>Oliver</forenames></author></authors><title>Coloring graphs using topology</title><categories>math.CO cs.CG cs.DM math.GT</categories><comments>81 pages, 48 figures</comments><msc-class>05C15, 05C10, 57M15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Higher dimensional graphs can be used to colour two-dimensional geometric
graphs. If G the boundary of a three dimensional graph H for example, we can
refine the interior until it is colourable with 4 colours. The later goal is
achieved if all interior edge degrees are even. Using a refinement process
which cuts the interior along surfaces we can adapt the degrees along the
boundary of that surface. More efficient is a self-cobordism of G with itself
with a host graph discretizing the product of G with an interval. It follows
from the fact that Euler curvature is zero everywhere for three dimensional
geometric graphs, that the odd degree edge set O is a cycle and so a boundary
if H is simply connected. A reduction to minimal colouring would imply the four
colour theorem. The method is expected to give a reason &quot;why 4 colours suffice&quot;
and suggests that every two dimensional geometric graph of arbitrary degree and
orientation can be coloured by 5 colours: since the projective plane can not be
a boundary of a 3-dimensional graph and because for higher genus surfaces, the
interior H is not simply connected, we need in general to embed a surface into
a 4-dimensional simply connected graph in order to colour it. This explains the
appearance of the chromatic number 5 for higher degree or non-orientable
situations, a number we believe to be the upper limit. For every surface type,
we construct examples with chromatic number 3,4 or 5, where the construction of
surfaces with chromatic number 5 is based on a method of Fisk. We have
implemented and illustrated all the topological aspects described in this paper
on a computer. So far we still need human guidance or simulated annealing to do
the refinements in the higher dimensional host graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6986</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6986</id><created>2014-12-22</created><authors><author><keyname>Han</keyname><forenames>Tianyi David</forenames></author><author><keyname>Abdelrahman</keyname><forenames>Tarek S.</forenames></author></authors><title>Automatic Tuning of Local Memory Use on GPGPUs</title><categories>cs.DC</categories><comments>Part of ADAPT Workshop proceedings, 2015 (arXiv:1412.2347)</comments><report-no>ADAPT/2015/04</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of local memory is important to improve the performance of OpenCL
programs. However, its use may not always benefit performance, depending on
various application characteristics, and there is no simple heuristic for
deciding when to use it. We develop a machine learning model to decide if the
optimization is beneficial or not. We train the model with millions of
synthetic benchmarks and show that it can predict if the optimization should be
applied for a single array, in both synthetic and real benchmarks, with high
accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.6988</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.6988</id><created>2014-12-22</created><authors><author><keyname>Takahashi</keyname><forenames>Hayato</forenames></author></authors><title>Universal test for Hippocratic randomness</title><categories>cs.IT math.IT</categories><comments>Submitted to Information Processing Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hippocratic randomness is defined in a similar way to Martin-Lof randomness,
however it does not assume computability of the probability and the existence
of universal test is not assured. We introduce the notion of approximation of
probability and show the existence of the universal test (Levin-Schnorr
theorem) for Hippocratic randomness when the logarithm of the probability is
approximated within additive constant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7003</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7003</id><created>2014-12-22</created><updated>2014-12-30</updated><authors><author><keyname>Maeda</keyname><forenames>Shin-ichi</forenames></author></authors><title>A Bayesian encourages dropout</title><categories>cs.LG cs.NE stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dropout is one of the key techniques to prevent the learning from
overfitting. It is explained that dropout works as a kind of modified L2
regularization. Here, we shed light on the dropout from Bayesian standpoint.
Bayesian interpretation enables us to optimize the dropout rate, which is
beneficial for learning of weight parameters and prediction after learning. The
experiment result also encourages the optimization of the dropout.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7004</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7004</id><created>2014-12-22</created><updated>2015-04-10</updated><authors><author><keyname>Madhyastha</keyname><forenames>Pranava Swaroop</forenames></author><author><keyname>Carreras</keyname><forenames>Xavier</forenames></author><author><keyname>Quattoni</keyname><forenames>Ariadna</forenames></author></authors><title>Tailoring Word Embeddings for Bilexical Predictions: An Experimental
  Comparison</title><categories>cs.CL cs.LG</categories><comments>Accepted as a workshop contribution at ICLR 2015</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We investigate the problem of inducing word embeddings that are tailored for
a particular bilexical relation. Our learning algorithm takes an existing
lexical vector space and compresses it such that the resulting word embeddings
are good predictors for a target bilexical relation. In experiments we show
that task-specific embeddings can benefit both the quality and efficiency in
lexical prediction tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7006</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7006</id><created>2014-12-22</created><updated>2015-07-07</updated><authors><author><keyname>Giering</keyname><forenames>Michael</forenames></author><author><keyname>Venugopalan</keyname><forenames>Vivek</forenames></author><author><keyname>Reddy</keyname><forenames>Kishore</forenames></author></authors><title>Multi-modal Sensor Registration for Vehicle Perception via Deep Neural
  Networks</title><categories>cs.CV cs.LG cs.NE</categories><comments>7 pages, double column, IEEE format, accepted at IEEE HPEC 2015</comments><report-no>1214_v3</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to simultaneously leverage multiple modes of sensor information
is critical for perception of an automated vehicle's physical surroundings.
Spatio-temporal alignment of registration of the incoming information is often
a prerequisite to analyzing the fused data. The persistence and reliability of
multi-modal registration is therefore the key to the stability of decision
support systems ingesting the fused information. LiDAR-video systems like on
those many driverless cars are a common example of where keeping the LiDAR and
video channels registered to common physical features is important. We develop
a deep learning method that takes multiple channels of heterogeneous data, to
detect the misalignment of the LiDAR-video inputs. A number of variations were
tested on the Ford LiDAR-video driving test data set and will be discussed. To
the best of our knowledge the use of multi-modal deep convolutional neural
networks for dynamic real-time LiDAR-video registration has not been presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7007</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7007</id><created>2014-12-22</created><updated>2015-07-07</updated><authors><author><keyname>Sarkar</keyname><forenames>Soumik</forenames></author><author><keyname>Venugopalan</keyname><forenames>Vivek</forenames></author><author><keyname>Reddy</keyname><forenames>Kishore</forenames></author><author><keyname>Giering</keyname><forenames>Michael</forenames></author><author><keyname>Ryde</keyname><forenames>Julian</forenames></author><author><keyname>Jaitly</keyname><forenames>Navdeep</forenames></author></authors><title>Occlusion Edge Detection in RGB-D Frames using Deep Convolutional
  Networks</title><categories>cs.CV cs.LG cs.NE</categories><comments>7 pages, double column, IEEE HPEC 2015 Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Occlusion edges in images which correspond to range discontinuity in the
scene from the point of view of the observer are an important prerequisite for
many vision and mobile robot tasks. Although they can be extracted from range
data however extracting them from images and videos would be extremely
beneficial. We trained a deep convolutional neural network (CNN) to identify
occlusion edges in images and videos with both RGB-D and RGB inputs. The use of
CNN avoids hand-crafting of features for automatically isolating occlusion
edges and distinguishing them from appearance edges. Other than quantitative
occlusion edge detection results, qualitative results are provided to
demonstrate the trade-off between high resolution analysis and frame-level
computation time which is critical for real-time robotics applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7009</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7009</id><created>2014-12-22</created><updated>2015-04-08</updated><authors><author><keyname>Rudy</keyname><forenames>Jan</forenames></author><author><keyname>Taylor</keyname><forenames>Graham</forenames></author></authors><title>Generative Class-conditional Autoencoders</title><categories>cs.NE cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work by Bengio et al. (2013) proposes a sampling procedure for
denoising autoencoders which involves learning the transition operator of a
Markov chain. The transition operator is typically unimodal, which limits its
capacity to model complex data. In order to perform efficient sampling from
conditional distributions, we extend this work, both theoretically and
algorithmically, to gated autoencoders (Memisevic, 2013), The proposed model is
able to generate convincing class-conditional samples when trained on both the
MNIST and TFD datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7011</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7011</id><created>2014-12-22</created><updated>2015-10-16</updated><authors><author><keyname>Shi</keyname><forenames>Guodong</forenames></author><author><keyname>Proutiere</keyname><forenames>Alexandre</forenames></author><author><keyname>Johansson</keyname><forenames>Karl Henrik</forenames></author></authors><title>Network Synchronization with Convexity</title><categories>cs.SY</categories><comments>Based on our previous manuscript arXiv:1210.6685. SIAM Journal on
  Control and Optimization, in press 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we establish a few new synchronization conditions for complex
networks with nonlinear and nonidentical self-dynamics with switching directed
communication graphs. In light of the recent works on distributed sub-gradient
methods, we impose integral convexity for the nonlinear node self-dynamics in
the sense that the self-dynamics of a given node is the gradient of some
concave function corresponding to that node. The node couplings are assumed to
be linear but with switching directed communication graphs. Several sufficient
and/or necessary conditions are established for exact or approximate
synchronization over the considered complex networks. These results show when
and how nonlinear node self-dynamics may cooperate with the linear diffusive
coupling, which eventually leads to network synchronization conditions under
relaxed connectivity requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7012</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7012</id><created>2014-12-15</created><updated>2015-06-10</updated><authors><author><keyname>Obuchi</keyname><forenames>Tomoyuki</forenames></author><author><keyname>Koma</keyname><forenames>Hirokazu</forenames></author><author><keyname>Yasuda</keyname><forenames>Muneki</forenames></author></authors><title>Boltzmann-machine learning of prior distributions of binarized natural
  images</title><categories>stat.ML cond-mat.dis-nn cs.CV</categories><comments>26 pages, 27 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Prior distributions of binarized natural images are learned by using
Boltzmann machine. We find that there emerges a structure with two sublattices
in the interactions, and the nearest-neighbor and next-nearest-neighbor
interactions correspondingly take two discriminative values, which reflects
individual characteristics of three sets of pictures we treat. On the other
hand, in a longer spacial scale, a longer-range (though still rapidly-decaying)
ferromagnetic interaction commonly appear in all the cases. The characteristic
length scale of the interactions is universally about up to four lattice
spacing $\xi \approx 4$. These results are derived by using the mean-field
method which effectively reduces the computational time required in Boltzmann
machine. An improved mean-field method called the Bethe approximation also
gives the same result, which reinforces the validity of our analysis and
findings. Relations to criticality, frustration, and simple-cell receptive
fields are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7018</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7018</id><created>2014-12-22</created><authors><author><keyname>Akbari</keyname><forenames>Hoda</forenames></author><author><keyname>Berenbrink</keyname><forenames>Petra</forenames></author><author><keyname>Els&#xe4;sser</keyname><forenames>Robert</forenames></author><author><keyname>Kaaser</keyname><forenames>Dominik</forenames></author></authors><title>Discrete Load Balancing in Heterogeneous Networks with a Focus on
  Second-Order Diffusion</title><categories>cs.DC</categories><comments>Full version of paper submitted to ICDCS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider a wide class of discrete diffusion load balancing
algorithms. The problem is defined as follows. We are given an interconnection
network and a number of load items, which are arbitrarily distributed among the
nodes of the network. The goal is to redistribute the load in iterative
discrete steps such that at the end each node has (almost) the same number of
items. In diffusion load balancing nodes are only allowed to balance their load
with their direct neighbors.
  We show three main results. Firstly, we present a general framework for
randomly rounding the flow generated by continuous diffusion schemes over the
edges of a graph in order to obtain corresponding discrete schemes. Compared to
the results of Rabani, Sinclair, and Wanka, FOCS'98, which are only valid
w.r.t. the class of homogeneous first order schemes, our framework can be used
to analyze a larger class of diffusion algorithms, such as algorithms for
heterogeneous networks and second order schemes. Secondly, we bound the
deviation between randomized second order schemes and their continuous
counterparts. Finally, we provide a bound for the minimum initial load in a
network that is sufficient to prevent the occurrence of negative load at a node
during the execution of second order diffusion schemes.
  Our theoretical results are complemented with extensive simulations on
different graph classes. We show empirically that second order schemes, which
are usually much faster than first order schemes, will not balance the load
completely on a number of networks within reasonable time. However, the maximum
load difference at the end seems to be bounded by a constant value, which can
be further decreased if first order scheme is applied once this value is
achieved by second order scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7022</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7022</id><created>2014-12-22</created><updated>2015-04-27</updated><authors><author><keyname>Sprechmann</keyname><forenames>Pablo</forenames></author><author><keyname>Bruna</keyname><forenames>Joan</forenames></author><author><keyname>LeCun</keyname><forenames>Yann</forenames></author></authors><title>Audio Source Separation with Discriminative Scattering Networks</title><categories>cs.SD cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this report we describe an ongoing line of research for solving
single-channel source separation problems. Many monaural signal decomposition
techniques proposed in the literature operate on a feature space consisting of
a time-frequency representation of the input data. A challenge faced by these
approaches is to effectively exploit the temporal dependencies of the signals
at scales larger than the duration of a time-frame. In this work we propose to
tackle this problem by modeling the signals using a time-frequency
representation with multiple temporal resolutions. The proposed representation
consists of a pyramid of wavelet scattering operators, which generalizes
Constant Q Transforms (CQT) with extra layers of convolution and complex
modulus. We first show that learning standard models with this multi-resolution
setting improves source separation results over fixed-resolution methods. As
study case, we use Non-Negative Matrix Factorizations (NMF) that has been
widely considered in many audio application. Then, we investigate the inclusion
of the proposed multi-resolution setting into a discriminative training regime.
We discuss several alternatives using different deep neural network
architectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7024</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7024</id><created>2014-12-22</created><updated>2015-09-22</updated><authors><author><keyname>Courbariaux</keyname><forenames>Matthieu</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author><author><keyname>David</keyname><forenames>Jean-Pierre</forenames></author></authors><title>Training deep neural networks with low precision multiplications</title><categories>cs.LG cs.CV cs.NE</categories><comments>10 pages, 5 figures, Accepted as a workshop contribution at ICLR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multipliers are the most space and power-hungry arithmetic operators of the
digital implementation of deep neural networks. We train a set of
state-of-the-art neural networks (Maxout networks) on three benchmark datasets:
MNIST, CIFAR-10 and SVHN. They are trained with three distinct formats:
floating point, fixed point and dynamic fixed point. For each of those datasets
and for each of those formats, we assess the impact of the precision of the
multiplications on the final error after training. We find that very low
precision is sufficient not just for running trained networks but also for
training them. For example, it is possible to train Maxout networks with 10
bits multiplications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7026</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7026</id><created>2014-12-22</created><updated>2015-02-27</updated><authors><author><keyname>Joshi</keyname><forenames>Aditya</forenames></author><author><keyname>Halseth</keyname><forenames>Johan</forenames></author><author><keyname>Kanerva</keyname><forenames>Pentti</forenames></author></authors><title>Language Recognition using Random Indexing</title><categories>cs.CL cs.LG</categories><comments>7 pages, 1 figures, 2 tables, ICLR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random Indexing is a simple implementation of Random Projections with a wide
range of applications. It can solve a variety of problems with good accuracy
without introducing much complexity. Here we use it for identifying the
language of text samples. We present a novel method of generating language
representation vectors using letter blocks. Further, we show that the method is
easily implemented and requires little computational power and space.
Experiments on a number of model parameters illustrate certain properties about
high dimensional sparse vector representations of data. Proof of statistically
relevant language vectors are shown through the extremely high success of
various language recognition tasks. On a difficult data set of 21,000 short
sentences from 21 different languages, our model performs a language
recognition task and achieves 97.8% accuracy, comparable to state-of-the-art
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7028</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7028</id><created>2014-12-22</created><updated>2015-04-10</updated><authors><author><keyname>Legrand</keyname><forenames>Jo&#xeb;l</forenames></author><author><keyname>Collobert</keyname><forenames>Ronan</forenames></author></authors><title>Joint RNN-Based Greedy Parsing and Word Composition</title><categories>cs.LG cs.CL cs.NE</categories><comments>Published as a conference paper at ICLR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a greedy parser based on neural networks, which
leverages a new compositional sub-tree representation. The greedy parser and
the compositional procedure are jointly trained, and tightly depends on
each-other. The composition procedure outputs a vector representation which
summarizes syntactically (parsing tags) and semantically (words) sub-trees.
Composition and tagging is achieved over continuous (word or tag)
representations, and recurrent neural networks. We reach F1 performance on par
with well-known existing parsers, while having the advantage of speed, thanks
to the greedy nature of the parser. We provide a fully functional
implementation of the method described in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7030</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7030</id><created>2014-12-22</created><authors><author><keyname>de Buyl</keyname><forenames>Pierre</forenames></author><author><keyname>Varoquaux</keyname><forenames>Nelle</forenames></author></authors><title>Proceedings of the 7th European Conference on Python in Science
  (EuroSciPy 2014)</title><categories>cs.CE cs.MS</categories><report-no>euroscipy-proceedings2014-00</report-no><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  These are the proceedings of the 7th European Conference on Python in
Science, EuroSciPy 2014, that was held in Cambridge, UK (27-30 August 2014).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7040</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7040</id><created>2014-12-22</created><updated>2015-12-27</updated><authors><author><keyname>Cain</keyname><forenames>Alan J.</forenames></author><author><keyname>Gray</keyname><forenames>Robert D.</forenames></author><author><keyname>Malheiro</keyname><forenames>Ant&#xf3;nio</forenames></author></authors><title>Crystal monoids \&amp; crystal bases: rewriting systems and biautomatic
  structures for plactic monoids of types $A_{n}$, $B_{n}$, $C_{n}$, $D_{n}$,
  and $G_2$</title><categories>math.GR cs.FL</categories><comments>83 pages. Major revision with expanded discussion of foundations</comments><msc-class>17B10 (Primary) 05E10, 16S15, 16T30, 20M42, 20M05, 20M35, 68Q42,
  68Q45, 68R15 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The vertices of any (combinatorial) Kashiwara crystal graph carry a natural
monoid structure given by identifying words labelling vertices that appear in
the same position of isomorphic components of the crystal. We prove some
foundational results for these crystal monoids, including the observation that
they have decidable word problem when their weight monoid is a finite rank free
abelian group. The problem of constructing finite complete rewriting systems,
and biautomatic structures, for crystal monoids is then investigated. In the
case of Kashiwara crystals of types $A_n$, $B_n$, $C_n$, $D_n$, and $G_2$
(corresponding to the $q$-analogues of the Lie algebras of these types) these
monoids are precisely the generalised plactic monoids investigated in work of
Lecouvey. We construct presentations via finite complete rewriting systems for
all of these types using a unified proof strategy that depends on Kashiwara's
crystal bases and analogies of Young tableaux, and on Lecouvey's presentations
for these monoids. As corollaries, we deduce that plactic monoids of these
types have finite derivation type and satisfy the homological finiteness
properties left and right $\mathrm{FP}_\infty$. These rewriting systems are
then applied to show that plactic monoids of these types are biautomatic and
thus have word problem soluble in quadratic time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7049</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7049</id><created>2014-12-22</created><authors><author><keyname>Kak</keyname><forenames>Subhash</forenames></author></authors><title>Friendship Paradox and Attention Economics</title><categories>cs.SI</categories><comments>8 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The friendship paradox is revisited by considering both local and global
averages of friends. How the economics of attention affects the recruitment of
friends is examined. Statistical implications of varying individual attentions
are investigated and it is argued that this is one reason why the mean of
friends is higher than the median in social networks. The distribution of
friends skews to the right for two other reasons: (i) the presence of
institutional nodes that increase the mean; and (ii) the dormancy of many of
the nodes. The difference between friends and friends of friends is a measure
of the structural information about the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7054</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7054</id><created>2014-12-22</created><updated>2015-04-10</updated><authors><author><keyname>Sermanet</keyname><forenames>Pierre</forenames></author><author><keyname>Frome</keyname><forenames>Andrea</forenames></author><author><keyname>Real</keyname><forenames>Esteban</forenames></author></authors><title>Attention for Fine-Grained Categorization</title><categories>cs.CV cs.LG cs.NE</categories><comments>ICLR 2015 Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents experiments extending the work of Ba et al. (2014) on
recurrent neural models for attention into less constrained visual
environments, specifically fine-grained categorization on the Stanford Dogs
data set. In this work we use an RNN of the same structure but substitute a
more powerful visual network and perform large-scale pre-training of the visual
network outside of the attention RNN. Most work in attention models to date
focuses on tasks with toy or more constrained visual environments, whereas we
present results for fine-grained categorization better than the
state-of-the-art GoogLeNet classification model. We show that our model learns
to direct high resolution attention to the most discriminative regions without
any spatial supervision such as bounding boxes, and it is able to discriminate
fine-grained dog breeds moderately well even when given only an initial
low-resolution context image and narrow, inexpensive glimpses at faces and fur
patterns. This and similar attention models have the major advantage of being
trained end-to-end, as opposed to other current detection and recognition
pipelines with hand-engineered components where information is lost. While our
model is state-of-the-art, further work is needed to fully leverage the
sequential input.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7056</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7056</id><created>2014-12-22</created><updated>2015-02-21</updated><authors><author><keyname>Kernfeld</keyname><forenames>Eric</forenames></author><author><keyname>Aeron</keyname><forenames>Shuchin</forenames></author><author><keyname>Kilmer</keyname><forenames>Misha</forenames></author></authors><title>Clustering multi-way data: a novel algebraic approach</title><categories>cs.LG cs.CV cs.IT math.IT stat.ML</categories><comments>20 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we develop a method for unsupervised clustering of two-way
(matrix) data by combining two recent innovations from different fields: the
Sparse Subspace Clustering (SSC) algorithm [10], which groups points coming
from a union of subspaces into their respective subspaces, and the t-product
[18], which was introduced to provide a matrix-like multiplication for third
order tensors. Our algorithm is analogous to SSC in that an &quot;affinity&quot; between
different data points is built using a sparse self-representation of the data.
Unlike SSC, we employ the t-product in the self-representation. This allows us
more flexibility in modeling; infact, SSC is a special case of our method. When
using the t-product, three-way arrays are treated as matrices whose elements
(scalars) are n-tuples or tubes. Convolutions take the place of scalar
multiplication. This framework allows us to embed the 2-D data into a
vector-space-like structure called a free module over a commutative ring. These
free modules retain many properties of complex inner-product spaces, and we
leverage that to provide theoretical guarantees on our algorithm. We show that
compared to vector-space counterparts, SSmC achieves higher accuracy and better
able to cluster data with less preprocessing in some image clustering problems.
In particular we show the performance of the proposed method on Weizmann face
database, the Extended Yale B Face database and the MNIST handwritten digits
database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7059</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7059</id><created>2014-12-22</created><updated>2015-01-03</updated><authors><author><keyname>Bi</keyname><forenames>Huibo</forenames></author><author><keyname>Gelenbe</keyname><forenames>Erol</forenames></author></authors><title>Cloud Enabled Emergency Navigation Using Faster-than-real-time
  Simulation</title><categories>cs.OH</categories><comments>Submitted to PerNEM'15 for review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  State-of-the-art emergency navigation approaches are designed to evacuate
civilians during a disaster based on real-time decisions using a pre-defined
algorithm and live sensory data. Hence, casualties caused by the poor decisions
and guidance are only apparent at the end of the evacuation process and cannot
then be remedied. Previous research shows that the performance of routing
algorithms for evacuation purposes are sensitive to the initial distribution of
evacuees, the occupancy levels, the type of disaster and its as well its
locations. Thus an algorithm that performs well in one scenario may achieve bad
results in another scenario. This problem is especially serious in
heuristic-based routing algorithms for evacuees where results are affected by
the choice of certain parameters. Therefore, this paper proposes a
simulation-based evacuee routing algorithm that optimises evacuation by making
use of the high computational power of cloud servers. Rather than guiding
evacuees with a predetermined routing algorithm, a robust Cognitive Packet
Network based algorithm is first evaluated via a cloud-based simulator in a
faster-than-real-time manner, and any &quot;simulated casualties&quot; are then re-routed
using a variant of Dijkstra's algorithm to obtain new safe paths for them to
exits. This approach can be iterated as long as corrective action is still
possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7062</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7062</id><created>2014-12-22</created><updated>2015-04-09</updated><authors><author><keyname>Chen</keyname><forenames>Liang-Chieh</forenames></author><author><keyname>Papandreou</keyname><forenames>George</forenames></author><author><keyname>Kokkinos</keyname><forenames>Iasonas</forenames></author><author><keyname>Murphy</keyname><forenames>Kevin</forenames></author><author><keyname>Yuille</keyname><forenames>Alan L.</forenames></author></authors><title>Semantic Image Segmentation with Deep Convolutional Nets and Fully
  Connected CRFs</title><categories>cs.CV cs.LG cs.NE</categories><comments>Camera ready for ICLR 2015. Best model attains 71.6% on PASCAL VOC
  2012 test set without annotations from other datasets</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep Convolutional Neural Networks (DCNNs) have recently shown state of the
art performance in high level vision tasks, such as image classification and
object detection. This work brings together methods from DCNNs and
probabilistic graphical models for addressing the task of pixel-level
classification (also called &quot;semantic image segmentation&quot;). We show that
responses at the final layer of DCNNs are not sufficiently localized for
accurate object segmentation. This is due to the very invariance properties
that make DCNNs good for high level tasks. We overcome this poor localization
property of deep networks by combining the responses at the final DCNN layer
with a fully connected Conditional Random Field (CRF). Qualitatively, our
&quot;DeepLab&quot; system is able to localize segment boundaries at a level of accuracy
which is beyond previous methods. Quantitatively, our method sets the new
state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching
71.6% IOU accuracy in the test set. We show how these results can be obtained
efficiently: Careful network re-purposing and a novel application of the 'hole'
algorithm from the wavelet community allow dense computation of neural net
responses at 8 frames per second on a modern GPU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7063</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7063</id><created>2014-12-22</created><updated>2015-04-15</updated><authors><author><keyname>Audhkhasi</keyname><forenames>Kartik</forenames></author><author><keyname>Sethy</keyname><forenames>Abhinav</forenames></author><author><keyname>Ramabhadran</keyname><forenames>Bhuvana</forenames></author></authors><title>Diverse Embedding Neural Network Language Models</title><categories>cs.CL cs.LG cs.NE</categories><comments>Under review as workshop contribution at ICLR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose Diverse Embedding Neural Network (DENN), a novel architecture for
language models (LMs). A DENNLM projects the input word history vector onto
multiple diverse low-dimensional sub-spaces instead of a single
higher-dimensional sub-space as in conventional feed-forward neural network
LMs. We encourage these sub-spaces to be diverse during network training
through an augmented loss function. Our language modeling experiments on the
Penn Treebank data set show the performance benefit of using a DENNLM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7065</identifier>
 <datestamp>2015-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7065</id><created>2014-12-22</created><updated>2015-10-26</updated><authors><author><keyname>Adamczak</keyname><forenames>Rados&#x142;aw</forenames></author><author><keyname>Lata&#x142;a</keyname><forenames>Rafa&#x142;</forenames></author><author><keyname>Pucha&#x142;a</keyname><forenames>Zbigniew</forenames></author><author><keyname>&#x17b;yczkowski</keyname><forenames>Karol</forenames></author></authors><title>Asymptotic entropic uncertainty relations</title><categories>quant-ph cs.IT math-ph math.IT math.MP math.PR</categories><comments>23 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze entropic uncertainty relations for two orthogonal measurements on
a $N$-dimensional Hilbert space, performed in two generic bases. It is assumed
that the unitary matrix $U$ relating both bases is distributed according to the
Haar measure on the unitary group. We provide lower bounds on the average
Shannon entropy of probability distributions related to both measurements. The
bounds are stronger than these obtained with use of the entropic uncertainty
relation by Maassen and Uffink, and they are optimal up to additive constants.
We also analyze the case of a large number of measurements and obtain strong
entropic uncertainty relations which hold with high probability with respect to
the random choice of bases. The lower bounds we obtain are optimal up to
additive constants and allow us to establish the conjecture by Wehner and
Winter on the asymptotic behavior of constants in entropic uncertainty
relations as the dimension tends to infinity. As a tool we develop estimates on
the maximum operator norm of a submatrix of a fixed size of a random unitary
matrix distributed according to the Haar measure, which are of an independent
interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7091</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7091</id><created>2014-12-22</created><updated>2015-07-13</updated><authors><author><keyname>Vincent</keyname><forenames>Pascal</forenames></author><author><keyname>de Br&#xe9;bisson</keyname><forenames>Alexandre</forenames></author><author><keyname>Bouthillier</keyname><forenames>Xavier</forenames></author></authors><title>Efficient Exact Gradient Update for training Deep Networks with Very
  Large Sparse Targets</title><categories>cs.NE cs.CL cs.LG</categories><comments>15 pages technical report version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important class of problems involves training deep neural networks with
sparse prediction targets of very high dimension D. These occur naturally in
e.g. neural language models or the learning of word-embeddings, often posed as
predicting the probability of next words among a vocabulary of size D (e.g. 200
000). Computing the equally large, but typically non-sparse D-dimensional
output vector from a last hidden layer of reasonable dimension d (e.g. 500)
incurs a prohibitive O(Dd) computational cost for each example, as does
updating the D x d output weight matrix and computing the gradient needed for
backpropagation to previous layers. While efficient handling of large sparse
network inputs is trivial, the case of large sparse targets is not, and has
thus so far been sidestepped with approximate alternatives such as hierarchical
softmax or sampling-based approximations during training. In this work we
develop an original algorithmic approach which, for a family of loss functions
that includes squared error and spherical softmax, can compute the exact loss,
gradient update for the output weights, and gradient for backpropagation, all
in O(d^2) per example instead of O(Dd), remarkably without ever computing the
D-dimensional output. The proposed algorithm yields a speedup of D/4d , i.e.
two orders of magnitude for typical sizes, for that critical part of the
computations that often dominates the training time in this kind of network
architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7102</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7102</id><created>2014-12-22</created><updated>2015-10-20</updated><authors><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Larsson</keyname><forenames>Erik G.</forenames></author><author><keyname>Debbah</keyname><forenames>M&#xe9;rouane</forenames></author></authors><title>Massive MIMO for Maximal Spectral Efficiency: How Many Users and Pilots
  Should Be Allocated?</title><categories>cs.IT cs.NI math.IT</categories><comments>To appear in IEEE Transactions on Wireless Communications, 16 pages,
  14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive MIMO is a promising technique to increase the spectral efficiency
(SE) of cellular networks, by deploying antenna arrays with hundreds or
thousands of active elements at the base stations and performing coherent
transceiver processing. A common rule-of-thumb is that these systems should
have an order of magnitude more antennas, $M$, than scheduled users, $K$,
because the users' channels are likely to be near-orthogonal when $M/K &gt; 10$.
However, it has not been proved that this rule-of-thumb actually maximizes the
SE. In this paper, we analyze how the optimal number of scheduled users,
$K^\star$, depends on $M$ and other system parameters. To this end, new SE
expressions are derived to enable efficient system-level analysis with power
control, arbitrary pilot reuse, and random user locations. The value of
$K^\star$ in the large-$M$ regime is derived in closed form, while simulations
are used to show what happens at finite $M$, in different interference
scenarios, with different pilot reuse factors, and for different processing
schemes. Up to half the coherence block should be dedicated to pilots and the
optimal $M/K$ is less than 10 in many cases of practical relevance.
Interestingly, $K^\star$ depends strongly on the processing scheme and hence it
is unfair to compare different schemes using the same $K$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7110</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7110</id><created>2014-12-22</created><updated>2015-04-16</updated><authors><author><keyname>Palaz</keyname><forenames>Dimitri</forenames></author><author><keyname>Doss</keyname><forenames>Mathew Magimai</forenames></author><author><keyname>Collobert</keyname><forenames>Ronan</forenames></author></authors><title>Learning linearly separable features for speech recognition using
  convolutional neural networks</title><categories>cs.LG cs.CL cs.NE</categories><comments>Final version for ICLR 2015 Workshop; Revisions according to reviews.
  Revised Section 4.5. Add references and correct typos. Submitted for ICLR
  2015 conference track</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic speech recognition systems usually rely on spectral-based features,
such as MFCC of PLP. These features are extracted based on prior knowledge such
as, speech perception or/and speech production. Recently, convolutional neural
networks have been shown to be able to estimate phoneme conditional
probabilities in a completely data-driven manner, i.e. using directly temporal
raw speech signal as input. This system was shown to yield similar or better
performance than HMM/ANN based system on phoneme recognition task and on large
scale continuous speech recognition task, using less parameters. Motivated by
these studies, we investigate the use of simple linear classifier in the
CNN-based framework. Thus, the network learns linearly separable features from
raw speech. We show that such system yields similar or better performance than
MLP based system using cepstral-based features as input.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7116</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7116</id><created>2014-12-22</created><updated>2015-10-02</updated><authors><author><keyname>Hosseini</keyname><forenames>Saghar</forenames></author><author><keyname>Chapman</keyname><forenames>Airlie</forenames></author><author><keyname>Mesbahi</keyname><forenames>Mehran</forenames></author></authors><title>Online Distributed ADMM on Networks</title><categories>math.OC cs.DC cs.DS cs.MA</categories><comments>Submitted to The IEEE Transactions on Control of Network Systems,
  2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines online distributed Alternating Direction Method of
Multipliers (ADMM). The goal is to distributively optimize a global objective
function over a network of decision makers under linear constraints. The global
objective function is composed of convex cost functions associated with each
agent. The local cost functions, on the other hand, are assumed to have been
decomposed into two distinct convex functions, one of which is revealed to the
decision makers over time and one known a priori. In addition, the agents must
achieve consensus on the global variable that relates to the private local
variables via linear constraints. In this work, we extend online ADMM to a
distributed setting based on dual-averaging and distributed gradient descent.
We then propose a performance metric for such online distributed algorithms and
explore the performance of the sequence of decisions generated by the algorithm
as compared with the best fixed decision in hindsight. This performance metric
is called the social regret. A sub-linear upper bound on the social regret of
the proposed algorithm is then obtained that underscores the role of the
underlying network topology and certain condition measures associated with the
linear constraints. The online distributed ADMM algorithm is then applied to a
formation acquisition problem demonstrating the application of the proposed
setup in distributed robotics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7119</identifier>
 <datestamp>2015-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7119</id><created>2014-12-22</created><updated>2015-03-20</updated><authors><author><keyname>Baltescu</keyname><forenames>Paul</forenames></author><author><keyname>Blunsom</keyname><forenames>Phil</forenames></author></authors><title>Pragmatic Neural Language Modelling in Machine Translation</title><categories>cs.CL</categories><comments>NAACL 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an in-depth investigation on integrating neural language
models in translation systems. Scaling neural language models is a difficult
task, but crucial for real-world applications. This paper evaluates the impact
on end-to-end MT quality of both new and existing scaling techniques. We show
when explicitly normalising neural models is necessary and what optimisation
tricks one should use in such scenarios. We also focus on scalable training
algorithms and investigate noise contrastive estimation and diagonal contexts
as sources for further speed improvements. We explore the trade-offs between
neural models and back-off n-gram models and find that neural models make
strong candidates for natural language applications in memory constrained
environments, yet still lag behind traditional models in raw translation
quality. We conclude with a set of recommendations one should follow to build a
scalable neural language model for MT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7122</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7122</id><created>2014-12-22</created><updated>2015-10-11</updated><authors><author><keyname>Peng</keyname><forenames>Xingchao</forenames></author><author><keyname>Sun</keyname><forenames>Baochen</forenames></author><author><keyname>Ali</keyname><forenames>Karim</forenames></author><author><keyname>Saenko</keyname><forenames>Kate</forenames></author></authors><title>Learning Deep Object Detectors from 3D Models</title><categories>cs.CV cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crowdsourced 3D CAD models are becoming easily accessible online, and can
potentially generate an infinite number of training images for almost any
object category.We show that augmenting the training data of contemporary Deep
Convolutional Neural Net (DCNN) models with such synthetic data can be
effective, especially when real training data is limited or not well matched to
the target domain. Most freely available CAD models capture 3D shape but are
often missing other low level cues, such as realistic object texture, pose, or
background. In a detailed analysis, we use synthetic CAD-rendered images to
probe the ability of DCNN to learn without these cues, with surprising
findings. In particular, we show that when the DCNN is fine-tuned on the target
detection task, it exhibits a large degree of invariance to missing low-level
cues, but, when pretrained on generic ImageNet classification, it learns better
when the low-level cues are simulated. We show that our synthetic DCNN training
approach significantly outperforms previous methods on the PASCAL VOC2007
dataset when learning in the few-shot scenario and improves performance in a
domain shift scenario on the Office benchmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7144</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7144</id><created>2014-12-22</created><updated>2015-04-15</updated><authors><author><keyname>Pathak</keyname><forenames>Deepak</forenames></author><author><keyname>Shelhamer</keyname><forenames>Evan</forenames></author><author><keyname>Long</keyname><forenames>Jonathan</forenames></author><author><keyname>Darrell</keyname><forenames>Trevor</forenames></author></authors><title>Fully Convolutional Multi-Class Multiple Instance Learning</title><categories>cs.CV cs.LG cs.NE</categories><comments>in ICLR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple instance learning (MIL) can reduce the need for costly annotation in
tasks such as semantic segmentation by weakening the required degree of
supervision. We propose a novel MIL formulation of multi-class semantic
segmentation learning by a fully convolutional network. In this setting, we
seek to learn a semantic segmentation model from just weak image-level labels.
The model is trained end-to-end to jointly optimize the representation while
disambiguating the pixel-image label assignment. Fully convolutional training
accepts inputs of any size, does not need object proposal pre-processing, and
offers a pixelwise loss map for selecting latent instances. Our multi-class MIL
loss exploits the further supervision given by images with multiple labels. We
evaluate this approach through preliminary experiments on the PASCAL VOC
segmentation challenge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7146</identifier>
 <datestamp>2014-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7146</id><created>2014-12-18</created><updated>2014-12-23</updated><authors><author><keyname>Cichocki</keyname><forenames>Andrzej</forenames></author><author><keyname>Cruces</keyname><forenames>Sergio</forenames></author><author><keyname>Amari</keyname><forenames>Shun-Ichi</forenames></author></authors><title>Log-Determinant Divergences Revisited: Alpha--Beta and Gamma Log-Det
  Divergences</title><categories>stat.CO cs.IT math.IT</categories><comments>35 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we review and extend a family of log-det divergences for
symmetric positive definite (SPD) matrices and discuss their fundamental
properties. We show how to generate from parameterized Alpha-Beta (AB) and
Gamma Log-det divergences many well known divergences, for example, the Stein's
loss, S-divergence, called also Jensen-Bregman LogDet (JBLD) divergence, the
Logdet Zero (Bhattacharryya) divergence, Affine Invariant Riemannian Metric
(AIRM) as well as some new divergences. Moreover, we establish links and
correspondences among many log-det divergences and display them on alpha-beta
plain for various set of parameters. Furthermore, this paper bridges these
divergences and shows also their links to divergences of multivariate and
multiway Gaussian distributions. Closed form formulas are derived for gamma
divergences of two multivariate Gaussian densities including as special cases
the Kullback-Leibler, Bhattacharryya, R\'enyi and Cauchy-Schwartz divergences.
Symmetrized versions of the log-det divergences are also discussed and
reviewed. A class of divergences is extended to multiway divergences for
separable covariance (precision) matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7148</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7148</id><created>2014-12-22</created><updated>2015-03-04</updated><authors><author><keyname>Altenkirch</keyname><forenames>Thosten</forenames><affiliation>University of Nottingham</affiliation></author><author><keyname>Chapman</keyname><forenames>James</forenames><affiliation>Institute of Cybernetics</affiliation></author><author><keyname>Uustalu</keyname><forenames>Tarmo</forenames><affiliation>Institute of Cybernetics</affiliation></author></authors><title>Monads need not be endofunctors</title><categories>cs.PL cs.LO math.CT</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 11, Issue 1 (March 6,
  2015) lmcs:928</journal-ref><doi>10.2168/LMCS-11(1:3)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a generalization of monads, called relative monads, allowing for
underlying functors between different categories. Examples include
finite-dimensional vector spaces, untyped and typed lambda-calculus syntax and
indexed containers. We show that the Kleisli and Eilenberg-Moore constructions
carry over to relative monads and are related to relative adjunctions. Under
reasonable assumptions, relative monads are monoids in the functor category
concerned and extend to monads, giving rise to a coreflection between relative
monads and monads. Arrows are also an instance of relative monads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7149</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7149</id><created>2014-12-22</created><updated>2015-07-17</updated><authors><author><keyname>Yang</keyname><forenames>Zichao</forenames></author><author><keyname>Moczulski</keyname><forenames>Marcin</forenames></author><author><keyname>Denil</keyname><forenames>Misha</forenames></author><author><keyname>de Freitas</keyname><forenames>Nando</forenames></author><author><keyname>Smola</keyname><forenames>Alex</forenames></author><author><keyname>Song</keyname><forenames>Le</forenames></author><author><keyname>Wang</keyname><forenames>Ziyu</forenames></author></authors><title>Deep Fried Convnets</title><categories>cs.LG cs.NE stat.ML</categories><comments>svd experiments included</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fully connected layers of a deep convolutional neural network typically
contain over 90% of the network parameters, and consume the majority of the
memory required to store the network parameters. Reducing the number of
parameters while preserving essentially the same predictive performance is
critically important for operating deep neural networks in memory constrained
environments such as GPUs or embedded devices.
  In this paper we show how kernel methods, in particular a single Fastfood
layer, can be used to replace all fully connected layers in a deep
convolutional neural network. This novel Fastfood layer is also end-to-end
trainable in conjunction with convolutional layers, allowing us to combine them
into a new architecture, named deep fried convolutional networks, which
substantially reduces the memory footprint of convolutional networks trained on
MNIST and ImageNet with no drop in predictive performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7155</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7155</id><created>2014-12-22</created><updated>2015-04-10</updated><authors><author><keyname>Finn</keyname><forenames>Chelsea</forenames></author><author><keyname>Hendricks</keyname><forenames>Lisa Anne</forenames></author><author><keyname>Darrell</keyname><forenames>Trevor</forenames></author></authors><title>Learning Compact Convolutional Neural Networks with Nested Dropout</title><categories>cs.CV cs.LG cs.NE</categories><comments>4 pages, 2 figures. Accepted as a workshop contribution at ICLR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, nested dropout was proposed as a method for ordering representation
units in autoencoders by their information content, without diminishing
reconstruction cost. However, it has only been applied to training
fully-connected autoencoders in an unsupervised setting. We explore the impact
of nested dropout on the convolutional layers in a CNN trained by
backpropagation, investigating whether nested dropout can provide a simple and
systematic way to determine the optimal representation size with respect to the
desired accuracy and desired task and data complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7156</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7156</id><created>2014-12-22</created><updated>2015-06-22</updated><authors><author><keyname>Contardo</keyname><forenames>Gabriella</forenames></author><author><keyname>Denoyer</keyname><forenames>Ludovic</forenames></author><author><keyname>Artieres</keyname><forenames>Thierry</forenames></author></authors><title>Representation Learning for cold-start recommendation</title><categories>cs.IR cs.LG</categories><comments>Accepted as workshop contribution at ICLR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A standard approach to Collaborative Filtering (CF), i.e. prediction of user
ratings on items, relies on Matrix Factorization techniques. Representations
for both users and items are computed from the observed ratings and used for
prediction. Unfortunatly, these transductive approaches cannot handle the case
of new users arriving in the system, with no known rating, a problem known as
user cold-start. A common approach in this context is to ask these incoming
users for a few initialization ratings. This paper presents a model to tackle
this twofold problem of (i) finding good questions to ask, (ii) building
efficient representations from this small amount of information. The model can
also be used in a more standard (warm) context. Our approach is evaluated on
the classical CF problem and on the cold-start problem on four different
datasets showing its ability to improve baseline performance in both cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7160</identifier>
 <datestamp>2014-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7160</id><created>2014-12-22</created><authors><author><keyname>Selig</keyname><forenames>Marco</forenames></author></authors><title>The NIFTY way of Bayesian signal inference</title><categories>astro-ph.IM cs.IT cs.MS math.IT physics.data-an</categories><comments>6 pages, 2 figures, refereed proceeding of the 33rd International
  Workshop on Bayesian Inference and Maximum Entropy Methods in Science and
  Engineering (MaxEnt 2013), software available at
  http://www.mpa-garching.mpg.de/ift/nifty/ and
  http://www.mpa-garching.mpg.de/ift/d3po/</comments><journal-ref>AIP Conf. Proc. 1636, 68 (2014)</journal-ref><doi>10.1063/1.4903712</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce NIFTY, &quot;Numerical Information Field Theory&quot;, a software package
for the development of Bayesian signal inference algorithms that operate
independently from any underlying spatial grid and its resolution. A large
number of Bayesian and Maximum Entropy methods for 1D signal reconstruction, 2D
imaging, as well as 3D tomography, appear formally similar, but one often finds
individualized implementations that are neither flexible nor easily
transferable. Signal inference in the framework of NIFTY can be done in an
abstract way, such that algorithms, prototyped in 1D, can be applied to real
world problems in higher-dimensional settings. NIFTY as a versatile library is
applicable and already has been applied in 1D, 2D, 3D and spherical settings. A
recent application is the D3PO algorithm targeting the non-trivial task of
denoising, deconvolving, and decomposing photon observations in high energy
astronomy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7172</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7172</id><created>2014-12-22</created><updated>2015-09-16</updated><authors><author><keyname>Harel</keyname><forenames>Matan</forenames></author><author><keyname>Mossel</keyname><forenames>Elchanan</forenames></author><author><keyname>Strack</keyname><forenames>Philipp</forenames></author><author><keyname>Tamuz</keyname><forenames>Omer</forenames></author></authors><title>When more information reduces the speed of learning</title><categories>cs.GT math.PR</categories><comments>31 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider two Bayesian agents who learn from exogenously provided private
signals, as well as the actions of the other. Our main finding is that
increased interaction between the agents can lower the speed of learning: when
both agents observe each other, learning is significantly slower than it is
when one only observes the other. This slowdown is driven by a process in which
a consensus on the wrong action causes the agents to discount new contrary
evidence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7180</identifier>
 <datestamp>2014-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7180</id><created>2014-12-22</created><authors><author><keyname>Miao</keyname><forenames>Yishu</forenames></author><author><keyname>Wang</keyname><forenames>Ziyu</forenames></author><author><keyname>Blunsom</keyname><forenames>Phil</forenames></author></authors><title>Bayesian Optimisation for Machine Translation</title><categories>cs.CL cs.LG</categories><comments>Bayesian optimisation workshop, NIPS 2014</comments><acm-class>I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents novel Bayesian optimisation algorithms for minimum error
rate training of statistical machine translation systems. We explore two
classes of algorithms for efficiently exploring the translation space, with the
first based on N-best lists and the second based on a hypergraph representation
that compactly represents an exponential number of translation options. Our
algorithms exhibit faster convergence and are capable of obtaining lower error
rates than the existing translation model specific approaches, all within a
generic Bayesian optimisation framework. Further more, we also introduce a
random embedding algorithm to scale our approach to sparse high dimensional
feature sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7184</identifier>
 <datestamp>2014-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7184</id><created>2014-12-11</created><authors><author><keyname>Cho&#x142;oniewski</keyname><forenames>Jan</forenames></author><author><keyname>Sienkiewicz</keyname><forenames>Julian</forenames></author><author><keyname>Ho&#x142;yst</keyname><forenames>Janusz A.</forenames></author><author><keyname>Thelwall</keyname><forenames>Mike</forenames></author></authors><title>The role of emotional variables in the classification and prediction of
  collective social dynamics</title><categories>cs.SI physics.soc-ph</categories><comments>16 pages, 9 figures, 2 tables and 1 appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We demonstrate the power of data mining techniques for the analysis of
collective social dynamics within British Tweets during the Olympic Games 2012.
The classification accuracy of online activities related to the successes of
British athletes significantly improved when emotional components of tweets
were taken into account, but employing emotional variables for activity
prediction decreased the classifiers' quality. The approach could be easily
adopted for any prediction or classification study with a set of
problem-specific variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7185</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7185</id><created>2014-12-22</created><updated>2015-02-03</updated><authors><author><keyname>Langerudi</keyname><forenames>Mehran Fasihozaman</forenames></author></authors><title>Parameter Selection In Particle Swarm Optimization For Transportation
  Network Design Problem</title><categories>math.OC cs.NE</categories><comments>21 pages, 11 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In transportation planning and development, transport network design problem
seeks to optimize specific objectives (e.g. total travel time) through choosing
among a given set of projects while keeping consumption of resources (e.g.
budget) within their limits. Due to the numerous cases of choosing projects,
solving such a problem is very difficult and time-consuming. Based on particle
swarm optimization (PSO) technique, a heuristic solution algorithm for the
bi-level problem is designed. This paper evaluates the algorithm performance in
the response of changing certain basic PSO parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7186</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7186</id><created>2014-12-22</created><updated>2015-03-15</updated><authors><author><keyname>Ferrer-i-Cancho</keyname><forenames>Ramon</forenames></author></authors><title>Reply to the commentary &quot;Be careful when assuming the obvious&quot;, by P.
  Alday</title><categories>cs.CL physics.data-an physics.soc-ph</categories><comments>Minor corrections (language improved)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Here we respond to some comments by Alday concerning headedness in linguistic
theory and the validity of the assumptions of a mathematical model for word
order. For brevity, we focus only on two assumptions: the unit of measurement
of dependency length and the monotonicity of the cost of a dependency as a
function of its length. We also revise the implicit psychological bias in
Alday's comments. Notwithstanding, Alday is indicating the path for linguistic
research with his unusual concerns about parsimony from multiple dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7188</identifier>
 <datestamp>2014-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7188</id><created>2014-12-22</created><authors><author><keyname>Mahboubi</keyname><forenames>Seyyed Hassan</forenames></author><author><keyname>Hussain</keyname><forenames>Mumtaz</forenames></author><author><keyname>Motahari</keyname><forenames>Abolfazl Seyed</forenames></author><author><keyname>Khandani</keyname><forenames>Amir Keyvan</forenames></author></authors><title>Layered Interference Alignment: Achieving the Total DoF of MIMO X
  Channels</title><categories>cs.IT math.IT math.NT</categories><comments>26 pages, 4 figures. Preliminary version. Any comments and
  suggestions for improvement are most welcome</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The $K\times 2$ and $2\times K$, Multiple-Input Multiple-Output (MIMO) X
channel with constant channel coefficients available at all transmitters and
receivers is considered. A new alignment scheme, named \emph{layered
interference alignment}, is proposed in which both vector and real interference
alignment are exploited, in conjunction with joint processing at receiver
sides. Data streams with fractional multiplexing gains are sent in the desired
directions to align the interfering signals at receivers. To decode the
intended messages at receivers, a joint processing/simultaneous decoding
technique, which exploits the availability of several receive antennas, is
proposed. This analysis is subsequently backed up by metrical results for
systems of linear forms. In particular, for such linear forms,
Khintchine--Groshev type theorems are proved over real and complex numbers. It
is observed that $K\times 2$ and $2\times K$, X channels with $M$ antennas at
all transmitters/receivers enjoy duality in Degrees of Freedom (DoF). It is
shown that incorporating the layered interference alignment is essential to
characterize the total DoF of $\frac{2KM}{K+1}$ in the $K\times 2$ and $2\times
K$, $M$ antenna X channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7190</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7190</id><created>2014-12-22</created><updated>2015-02-28</updated><authors><author><keyname>Massa</keyname><forenames>Francisco</forenames></author><author><keyname>Aubry</keyname><forenames>Mathieu</forenames></author><author><keyname>Marlet</keyname><forenames>Renaud</forenames></author></authors><title>Convolutional Neural Networks for joint object detection and pose
  estimation: A comparative study</title><categories>cs.CV cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the application of convolutional neural networks for
jointly detecting objects depicted in still images and estimating their 3D
pose. We identify different feature representations of oriented objects, and
energies that lead a network to learn this representations. The choice of the
representation is crucial since the pose of an object has a natural, continuous
structure while its category is a discrete variable. We evaluate the different
approaches on the joint object detection and pose estimation task of the
Pascal3D+ benchmark using Average Viewpoint Precision. We show that a
classification approach on discretized viewpoints achieves state-of-the-art
performance for joint object detection and pose estimation, and significantly
outperforms existing baselines on this benchmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7193</identifier>
 <datestamp>2014-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7193</id><created>2014-12-22</created><authors><author><keyname>Jang</keyname><forenames>Giljin</forenames></author><author><keyname>Kim</keyname><forenames>Han-Gyu</forenames></author><author><keyname>Oh</keyname><forenames>Yung-Hwan</forenames></author></authors><title>Audio Source Separation Using a Deep Autoencoder</title><categories>cs.SD cs.LG cs.NE</categories><comments>3 pages, 4 figures, ICLR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel framework for unsupervised audio source
separation using a deep autoencoder. The characteristics of unknown source
signals mixed in the mixed input is automatically by properly configured
autoencoders implemented by a network with many layers, and separated by
clustering the coefficient vectors in the code layer. By investigating the
weight vectors to the final target, representation layer, the primitive
components of the audio signals in the frequency domain are observed. By
clustering the activation coefficients in the code layer, the previously
unknown source signals are segregated. The original source sounds are then
separated and reconstructed by using code vectors which belong to different
clusters. The restored sounds are not perfect but yield promising results for
the possibility in the success of many practical applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7197</identifier>
 <datestamp>2014-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7197</id><created>2014-12-22</created><authors><author><keyname>Chazal</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Fasy</keyname><forenames>Brittany T.</forenames></author><author><keyname>Lecci</keyname><forenames>Fabrizio</forenames></author><author><keyname>Michel</keyname><forenames>Bertrand</forenames></author><author><keyname>Rinaldo</keyname><forenames>Alessandro</forenames></author><author><keyname>Wasserman</keyname><forenames>Larry</forenames></author></authors><title>Robust Topological Inference: Distance To a Measure and Kernel Distance</title><categories>math.ST cs.CG math.AT stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let P be a distribution with support S. The salient features of S can be
quantified with persistent homology, which summarizes topological features of
the sublevel sets of the distance function (the distance of any point x to S).
Given a sample from P we can infer the persistent homology using an empirical
version of the distance function. However, the empirical distance function is
highly non-robust to noise and outliers. Even one outlier is deadly. The
distance-to-a-measure (DTM), introduced by Chazal et al. (2011), and the kernel
distance, introduced by Phillips et al. (2014), are smooth functions that
provide useful topological information but are robust to noise and outliers.
Chazal et al. (2014) derived concentration bounds for DTM. Building on these
results, we derive limiting distributions and confidence sets, and we propose a
method for choosing tuning parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7210</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7210</id><created>2014-12-22</created><updated>2015-03-31</updated><authors><author><keyname>Rasmus</keyname><forenames>Antti</forenames></author><author><keyname>Raiko</keyname><forenames>Tapani</forenames></author><author><keyname>Valpola</keyname><forenames>Harri</forenames></author></authors><title>Denoising autoencoder with modulated lateral connections learns
  invariant representations of natural images</title><categories>cs.NE cs.CV cs.LG stat.ML</categories><comments>Presentation at ICLR 2015 workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suitable lateral connections between encoder and decoder are shown to allow
higher layers of a denoising autoencoder (dAE) to focus on invariant
representations. In regular autoencoders, detailed information needs to be
carried through the highest layers but lateral connections from encoder to
decoder relieve this pressure. It is shown that abstract invariant features can
be translated to detailed reconstructions when invariant features are allowed
to modulate the strength of the lateral connection. Three dAE structures with
modulated and additive lateral connections, and without lateral connections
were compared in experiments using real-world images. The experiments verify
that adding modulated lateral connections to the model 1) improves the accuracy
of the probability model for inputs, as measured by denoising performance; 2)
results in representations whose degree of invariance grows faster towards the
higher layers; and 3) supports the formation of diverse invariant poolings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7215</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7215</id><created>2014-12-22</created><authors><author><keyname>Hosseini</keyname><forenames>Saghar</forenames></author><author><keyname>Chapman</keyname><forenames>Airlie</forenames></author><author><keyname>Mesbahi</keyname><forenames>Mehran</forenames></author></authors><title>Online Distributed Optimization on Dynamic Networks</title><categories>math.OC cs.DS cs.LG cs.MA cs.SY</categories><comments>Submitted to The IEEE Transactions on Automatic Control, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a distributed optimization scheme over a network of
agents in the presence of cost uncertainties and over switching communication
topologies. Inspired by recent advances in distributed convex optimization, we
propose a distributed algorithm based on a dual sub-gradient averaging. The
objective of this algorithm is to minimize a cost function cooperatively.
Furthermore, the algorithm changes the weights on the communication links in
the network to adapt to varying reliability of neighboring agents. A
convergence rate analysis as a function of the underlying network topology is
then presented, followed by simulation results for representative classes of
sensor networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7219</identifier>
 <datestamp>2014-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7219</id><created>2014-12-22</created><authors><author><keyname>G&#xf6;&#xf6;s</keyname><forenames>Mika</forenames></author><author><keyname>Lempi&#xe4;inen</keyname><forenames>Tuomo</forenames></author><author><keyname>Czeizler</keyname><forenames>Eugen</forenames></author><author><keyname>Orponen</keyname><forenames>Pekka</forenames></author></authors><title>Search Methods for Tile Sets in Patterned DNA Self-Assembly</title><categories>cs.ET cs.DS</categories><comments>1 + 36 pages, 18 figures. arXiv admin note: text overlap with
  arXiv:0911.2924</comments><msc-class>68W05</msc-class><acm-class>F.2.2; J.2</acm-class><journal-ref>J. Comput. Syst. Sci. 80 (2014) 297-319</journal-ref><doi>10.1016/j.jcss.2013.08.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Pattern self-Assembly Tile set Synthesis (PATS) problem, which arises in
the theory of structured DNA self-assembly, is to determine a set of coloured
tiles that, starting from a bordering seed structure, self-assembles to a given
rectangular colour pattern. The task of finding minimum-size tile sets is known
to be NP-hard. We explore several complete and incomplete search techniques for
finding minimal, or at least small, tile sets and also assess the reliability
of the solutions obtained according to the kinetic Tile Assembly Model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7223</identifier>
 <datestamp>2014-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7223</id><created>2014-12-22</created><authors><author><keyname>Chen</keyname><forenames>Mo</forenames></author><author><keyname>Fisac</keyname><forenames>Jaime F.</forenames></author><author><keyname>Sastry</keyname><forenames>Shankar</forenames></author><author><keyname>Tomlin</keyname><forenames>Claire J.</forenames></author></authors><title>Safe Sequential Path Planning of Multi-Vehicle Systems via
  Double-Obstacle Hamilton-Jacobi-Isaacs Variational Inequality</title><categories>cs.MA cs.SY</categories><comments>Submitted to European Control Conference 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of planning trajectories for a group of N vehicles,
each aiming to reach its own target set while avoiding danger zones of other
vehicles. The analysis of problems like this is extremely important
practically, especially given the growing interest in utilizing unmanned
aircraft systems for civil purposes. The direct solution of this problem by
solving a single-obstacle Hamilton-Jacobi-Isaacs (HJI) variational inequality
(VI) is numerically intractable due to the exponential scaling of computation
complexity with problem dimensionality. Furthermore, the single-obstacle HJI VI
cannot directly handle situations in which vehicles do not have a common
scheduled arrival time. Instead, we perform sequential path planning by
considering vehicles in order of priority, modeling higher-priority vehicles as
time-varying obstacles for lower-priority vehicles. To do this, we solve a
double-obstacle HJI VI which allows us to obtain the reach-avoid set, defined
as the set of states from which a vehicle can reach its target while staying
within a time-varying state constraint set. From the solution of the
double-obstacle HJI VI, we can also extract the latest start time and the
optimal control for each vehicle. This is a first application of the
double-obstacle HJI VI which can handle systems with time-varying dynamics,
target sets, and state constraint sets, and results in computation complexity
that scales linearly, as opposed to exponentially, with the number of vehicles
in consideration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7224</identifier>
 <datestamp>2014-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7224</id><created>2014-12-22</created><authors><author><keyname>Zu</keyname><forenames>K.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author></authors><title>Multi-User Flexible Coordinated Beamforming using Lattice Reduction for
  Massive MIMO Systems</title><categories>cs.IT math.IT</categories><comments>5 figures, Eusipco</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The application of precoding algorithms in multi-user massive multiple-input
multiple-output (MU-Massive-MIMO) systems is restricted by the dimensionality
constraint that the number of transmit antennas has to be greater than or equal
to the total number of receive antennas. In this paper, a lattice reduction
(LR)-aided flexible coordinated beamforming (LR-FlexCoBF) algorithm is proposed
to overcome the dimensionality constraint in overloaded MU-Massive-MIMO
systems. A random user selection scheme is integrated with the proposed
LR-FlexCoBF to extend its application to MU-Massive-MIMO systems with arbitary
overloading levels. Simulation results show that significant improvements in
terms of bit error rate (BER) and sum-rate performances can be achieved by the
proposed LR-FlexCoBF precoding algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7229</identifier>
 <datestamp>2015-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7229</id><created>2014-12-22</created><updated>2015-07-22</updated><authors><author><keyname>Guly&#xe1;s</keyname><forenames>Andr&#xe1;s</forenames></author><author><keyname>B&#xed;r&#xf3;</keyname><forenames>J&#xf3;zsef</forenames></author><author><keyname>K&#x151;r&#xf6;si</keyname><forenames>Attila</forenames></author><author><keyname>R&#xe9;tv&#xe1;ri</keyname><forenames>G&#xe1;bor</forenames></author><author><keyname>Krioukov</keyname><forenames>Dmitri</forenames></author></authors><title>Navigable Networks as Nash Equilibria of Navigation Games</title><categories>physics.soc-ph cs.GT cs.SI</categories><comments>40 pages, 17 figures</comments><journal-ref>Nature Communications 6, Article number: 7651, 03 July 2015</journal-ref><doi>10.1038/ncomms8651</doi><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  The common sense suggests that networks are not random mazes of purposeless
connections, but that these connections are organised so that networks can
perform their functions well. One function common to many networks is targeted
transport or navigation. Using game theory, here we show that minimalistic
networks designed to maximise the navigation efficiency at minimal cost share
basic structural properties with real networks. These idealistic networks are
Nash equilibria of a network construction game whose purpose is to find an
optimal trade-off between the network cost and navigability. We show that these
skeletons are present in the Internet, metabolic, English word, US airport,
Hungarian road networks, and in a structural network of the human brain. The
knowledge of these skeletons allows one to identify the minimal number of edges
by altering which one can efficiently improve or paralyse navigation in the
network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7232</identifier>
 <datestamp>2014-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7232</id><created>2014-12-22</created><authors><author><keyname>Ge</keyname><forenames>Xiaohu</forenames></author><author><keyname>Cheng</keyname><forenames>H.</forenames></author><author><keyname>Guizani</keyname><forenames>M.</forenames></author><author><keyname>Han</keyname><forenames>T.</forenames></author></authors><title>5G Wireless Backhaul Networks: Challenges and Research Advance</title><categories>cs.NI</categories><journal-ref>IEEE Network, Vol. 28, No. 6, pp. 6-11, Nov. 2014</journal-ref><doi>10.1109/MNET.2014.6963798</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  5G networks are expected to achieve gigabit-level throughput in future
cellular networks. However, it is a great challenge to treat 5G wireless
backhaul traffic in an effective way. In this article, we analyze the wireless
backhaul traffic in two typical network architectures adopting small cell and
millimeter wave commmunication technologies. Furthermore, the energy efficiency
of wireless backhaul networks is compared for different network architectures
and frequency bands. Numerical comparison results provide some guidelines for
deploying future 5G wireless backhaul networks in economical and highly
energy-efficient ways.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7242</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7242</id><created>2014-12-22</created><updated>2014-12-27</updated><authors><author><keyname>Shen</keyname><forenames>Chengyao</forenames></author><author><keyname>Huang</keyname><forenames>Xun</forenames></author><author><keyname>Zhao</keyname><forenames>Qi</forenames></author></authors><title>Learning of Proto-object Representations via Fixations on Low Resolution</title><categories>cs.CV</categories><comments>This paper has been withdrawn by the author due to incompletion of
  the submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While previous researches in eye fixation prediction typically rely on
integrating low-level features (e.g. color, edge) to form a saliency map,
recently it has been found that the structural organization of these features
into a proto-object representation can play a more significant role. In this
work, we present a computational framework based on deep network to demonstrate
that proto-object representations can be learned from low-resolution image
patches from fixation regions. We advocate the use of low-resolution inputs in
this work due to the following reasons: (1) Proto-objects are computed in
parallel over an entire visual field (2) People can perceive or recognize
objects well even it is in low resolution. (3) Fixations from lower resolution
images can predict fixations on higher resolution images. In the proposed
computational model, we extract multi-scale image patches on fixation regions
from eye fixation datasets, resize them to low resolution and feed them into a
hierarchical. With layer-wise unsupervised feature learning, we find that many
proto-objects like features responsive to different shapes of object blobs are
learned out. Visualizations also show that these features are selective to
potential objects in the scene and the responses of these features work well in
predicting eye fixations on the images when combined with learned weights.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7250</identifier>
 <datestamp>2014-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7250</id><created>2014-12-22</created><authors><author><keyname>Palguna</keyname><forenames>Deepan</forenames></author><author><keyname>Love</keyname><forenames>David J.</forenames></author><author><keyname>Pollak</keyname><forenames>Ilya</forenames></author></authors><title>Secondary Spectrum Auctions for Markets with Communication Constraints</title><categories>cs.GT</categories><comments>37 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Auctions have been proposed as a way to provide economic incentives for
primary users to dynamically allocate unused spectrum to other users in need of
it. Previously proposed schemes do not take into account the fact that the
power constraints of users might prevent them from transmitting their bid
prices to the auctioneer with high precision and that transmitted bid prices
must travel through a noisy channel. These schemes also have very high
overheads which cannot be accommodated in wireless standards. We propose
auction schemes where a central clearing authority auctions spectrum to users
who bid for it, while taking into account quantization of prices, overheads in
bid revelation, and noise in the channel explicitly. Our schemes are closely
related to channel output feedback problems and, specifically, to the technique
of posterior matching. We consider several scenarios where the objective of the
clearing authority is to award spectrum to the bidders who value spectrum the
most. We prove theoretically that this objective is asymptotically attained by
our scheme when the bidders are non-strategic with constant bids. We propose
separate schemes to make strategic users reveal their private values
truthfully, to auction multiple sub-channels among strategic users, and to
track slowly time-varying bid prices. Our simulations illustrate the optimality
of our schemes for constant bid prices, and also demonstrate the effectiveness
of our tracking algorithm for slowly time-varying bids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7253</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7253</id><created>2014-12-23</created><updated>2015-01-20</updated><authors><author><keyname>Zhi</keyname><forenames>Ye</forenames></author><author><keyname>Liu</keyname><forenames>Yu</forenames></author><author><keyname>Wang</keyname><forenames>Shaowen</forenames></author><author><keyname>Deng</keyname><forenames>Min</forenames></author><author><keyname>Gao</keyname><forenames>Jing</forenames></author><author><keyname>Li</keyname><forenames>Haifeng</forenames></author></authors><title>Urban spatial-temporal activity structures: a New Approach to Inferring
  the Intra-urban Functional Regions via Social Media Check-In Data</title><categories>cs.SI physics.soc-ph</categories><comments>22 pages, 11 figures, 34 references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most existing literature focuses on the exterior temporal rhythm of human
movement to infer the functional regions in a city, but they neglects the
underlying interdependence between the functional regions and human activities
which uncovers more detailed characteristics of regions. In this research, we
proposed a novel model based on the low rank approximation (LRA) to detect the
functional regions using the data from about 15 million check-in records during
a yearlong period in Shanghai, China. We find a series of latent structures,
called urban spatial-temporal activity structure (USTAS). While interpreting
these structures, a series of outstanding underlying associations between the
spatial and temporal activity patterns can be found. Moreover, we can not only
reproduce the observed data with a lower dimensional representative but also
simultaneously project both the spatial and temporal activity patterns in the
same coordinate system. By utilizing the K-means clustering algorithm, five
significant types of clusters which are directly annotated with a corresponding
combination of temporal activities can be obtained. This provides a clear
picture of how the groups of regions are associated with different activities
at different time of day. Besides the commercial and transportation dominant
area, we also detect two kinds of residential areas, the developed residential
areas and the developing residential areas. We further verify the spatial
distribution of these clusters in the view of urban form analysis. The results
shows a high consistency with the government planning from the same periods,
indicating our model is applicable for inferring the functional regions via
social media check-in data, and can benefit a wide range of fields, such as
urban planning, public services and location-based recommender systems and
other purposes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7259</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7259</id><created>2014-12-23</created><updated>2015-05-29</updated><authors><author><keyname>Wang</keyname><forenames>Dong</forenames></author><author><keyname>Tan</keyname><forenames>Xiaoyang</forenames></author></authors><title>Unsupervised Feature Learning with C-SVDDNet</title><categories>cs.CV cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the problem of learning feature representation
from unlabeled data using a single-layer K-means network. A K-means network
maps the input data into a feature representation by finding the nearest
centroid for each input point, which has attracted researchers' great attention
recently due to its simplicity, effectiveness, and scalability. However, one
drawback of this feature mapping is that it tends to be unreliable when the
training data contains noise. To address this issue, we propose a SVDD based
feature learning algorithm that describes the density and distribution of each
cluster from K-means with an SVDD ball for more robust feature representation.
For this purpose, we present a new SVDD algorithm called C-SVDD that centers
the SVDD ball towards the mode of local density of each cluster, and we show
that the objective of C-SVDD can be solved very efficiently as a linear
programming problem. Additionally, traditional unsupervised feature learning
methods usually take an average or sum of local representations to obtain
global representation which ignore spatial relationship among them. To use
spatial information we propose a global representation with a variant of SIFT
descriptor. The architecture is also extended with multiple receptive field
scales and multiple pooling sizes. Extensive experiments on several popular
object recognition benchmarks, such as STL-10, MINST, Holiday and Copydays
shows that the proposed C-SVDDNet method yields comparable or better
performance than that of the previous state of the art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7272</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7272</id><created>2014-12-23</created><updated>2015-04-22</updated><authors><author><keyname>Al-Shedivat</keyname><forenames>Maruan</forenames></author><author><keyname>Neftci</keyname><forenames>Emre</forenames></author><author><keyname>Cauwenberghs</keyname><forenames>Gert</forenames></author></authors><title>Learning Non-deterministic Representations with Energy-based Ensembles</title><categories>cs.LG cs.NE</categories><comments>9 pages, 3 figures, ICLR-15 workshop contribution</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of a generative model is to capture the distribution underlying the
data, typically through latent variables. After training, these variables are
often used as a new representation, more effective than the original features
in a variety of learning tasks. However, the representations constructed by
contemporary generative models are usually point-wise deterministic mappings
from the original feature space. Thus, even with representations robust to
class-specific transformations, statistically driven models trained on them
would not be able to generalize when the labeled data is scarce. Inspired by
the stochasticity of the synaptic connections in the brain, we introduce
Energy-based Stochastic Ensembles. These ensembles can learn non-deterministic
representations, i.e., mappings from the feature space to a family of
distributions in the latent space. These mappings are encoded in a distribution
over a (possibly infinite) collection of models. By conditionally sampling
models from the ensemble, we obtain multiple representations for every input
example and effectively augment the data. We propose an algorithm similar to
contrastive divergence for training restricted Boltzmann stochastic ensembles.
Finally, we demonstrate the concept of the stochastic representations on a
synthetic dataset as well as test them in the one-shot learning scenario on
MNIST.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7273</identifier>
 <datestamp>2014-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7273</id><created>2014-12-23</created><authors><author><keyname>Mishra</keyname><forenames>Sumita</forenames></author><author><keyname>Mathur</keyname><forenames>Nidhi</forenames></author></authors><title>Load Balancing Optimization in LTE/LTE-A Cellular Networks: A Review</title><categories>cs.NI</categories><comments>Preprint</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During the past few decades wireless technology has seen a tremendous growth.
The recent introduction of high-end mobile devices has further increased
subscriber's demand for high bandwidth. Current cellular systems require manual
configuration and management of networks, which is now costly, time consuming
and error prone due to exponentially increasing rate of mobile users and nodes.
This leads to introduction of self organizing capabilities for network
management with minimum human involvement. It is expected to permit higher end
user Quality of Service (QoS) along with less operational and maintenance cost
for telecom service providers. Self organized cellular networks incorporate a
collection of functions for automatic configuration, optimization and
maintenance of cellular networks. As mobile end users continue to use network
resources while moving from a cell boundary to other, traffic load within a
cell does not remain constant. Thus Load balancing as a part of self organized
network solution, has become one of the most active and emerging fields of
research in Cellular Network. It involves transfer of load from overloaded
cells to the neighbouring cells with free resources for more balanced load
distribution in order to maintain appropriate end-user experience and network
performance. In this paper, review of various load balancing techniques
currently used in mobile networks is presented, with special emphasis on
techniques that are suitable for self optimization feature in future cellular
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7276</identifier>
 <datestamp>2014-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7276</id><created>2014-12-23</created><authors><author><keyname>Mostafa</keyname><forenames>Almetwally M.</forenames></author><author><keyname>Youssef</keyname><forenames>Ahmed E.</forenames></author><author><keyname>Alshorbagy</keyname><forenames>Gamal</forenames></author></authors><title>A Framework for a Smart Social Blood Donation System Based on Mobile
  Cloud Computing</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Blood Donation and Blood Transfusion Services (BTS) are crucial for saving
people lives. Recently, worldwide efforts have been undertaken to utilize
social media and smartphone applications to make the blood donation process
more convenient, offer additional services, and create communities around blood
donation centers. Blood banks suffer frequent shortage of blood; hence,
advertisements are frequently seen on social networks urging healthy
individuals to donate blood for patients who urgently require blood
transfusion. The blood donation process usually consumes a lot of time and
effort from both donors and medical staff since there is no concrete
information system that allows donors and blood donation centers communicate
efficiently and coordinate with each other to minimize time and effort required
for blood donation process. Moreover, most blood banks work in isolation and
are not integrated with other blood donation centers and health organizations
which affect the blood donation and blood transfusion services quality. This
work aims at developing a Blood Donation System (BDS) based on the cutting-edge
information technologies of cloud computing and mobile computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7277</identifier>
 <datestamp>2014-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7277</id><created>2014-12-23</created><authors><author><keyname>Dubey</keyname><forenames>Shiv Ram</forenames></author><author><keyname>Jalal</keyname><forenames>Anand Singh</forenames></author></authors><title>Fusing Color and Texture Cues to Categorize the Fruit Diseases from
  Images</title><categories>cs.CV</categories><comments>arXiv admin note: substantial text overlap with arXiv:1405.4930</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The economic and production losses in agricultural industry worldwide are due
to the presence of diseases in the several kinds of fruits. In this paper, a
method for the classification of fruit diseases is proposed and experimentally
validated. The image processing based proposed approach is composed of the
following main steps; in the first step K-Means clustering technique is used
for the defect segmentation, in the second step color and textural cues are
extracted and fused from the segmented image, and finally images are classified
into one of the classes by using a Multi-class Support Vector Machine. We have
considered diseases of apple as a test case and evaluated our approach for
three types of apple diseases namely apple scab, apple blotch and apple rot and
normal apples without diseases. Our experimentation points out that the
proposed fusion scheme can significantly support accurate detection and
automatic classification of fruit diseases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7281</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7281</id><created>2014-12-23</created><updated>2015-07-24</updated><authors><author><keyname>Zhu</keyname><forenames>Shanying</forenames></author><author><keyname>Soh</keyname><forenames>Yeng Chai</forenames></author><author><keyname>Xie</keyname><forenames>Lihua</forenames></author></authors><title>Distributed Parameter Estimation with Quantized Communication via
  Running Average</title><categories>cs.SY cs.DC cs.MA math.OC</categories><comments>13 pages, 6 figures; IEEE Transactions on Signal Processing, 2015</comments><doi>10.1109/TSP.2015.2441034</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the parameter estimation problem over sensor
networks in the presence of quantized data and directed communication links. We
propose a two-stage algorithm aiming at achieving the centralized sample mean
estimate in a distributed manner. Different from the existing algorithms, a
running average technique is utilized in the proposed algorithm to smear out
the randomness caused by the probabilistic quantization scheme. With the
running average technique, it is shown that the centralized sample mean
estimate can be achieved both in the mean square and almost sure senses, which
is not observed in the conventional consensus algorithms. In addition, the
rates of convergence are given to quantify the mean square and almost sure
performances. Finally, simulation results are presented to illustrate the
effectiveness of the proposed algorithm and highlight the improvements by using
running average technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7282</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7282</id><created>2014-12-23</created><updated>2016-01-29</updated><authors><author><keyname>Li</keyname><forenames>Jundong</forenames></author><author><keyname>Adilmagambetovm</keyname><forenames>Aibek</forenames></author><author><keyname>Jabbar</keyname><forenames>Mohomed Shazan Mohomed</forenames></author><author><keyname>Zaiane</keyname><forenames>Osmar R.</forenames></author><author><keyname>Osornio-Vargas</keyname><forenames>Alvaro</forenames></author><author><keyname>Wine</keyname><forenames>Osnat</forenames></author></authors><title>On Discovering Co-Location Patterns in Datasets: A Case Study of
  Pollutants and Child Cancers</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We intend to identify relationships between cancer cases and pollutant
emissions and attempt to understand whether cancer in children is typically
located together with some specific chemical combinations or is independent.
Co-location pattern analysis seems to be the appropriate investigation to
perform. Co-location mining is one of the tasks of spatial data mining which
focuses on the detection of co-location patterns, the sets of spatial features
frequently located in close proximity of each other. Most previous works are
based on transaction-free apriori-like algorithms which are dependent on
user-defined thresholds and are designed for boolean data points. Due to the
absence of a clear notion of transactions, it is nontrivial to use association
rule mining techniques to tackle the co-location mining problem. The approach
we propose is based on a grid &quot;transactionization&quot; of the geographic space and
is designed to mine datasets with extended spatial objects. Uncertainty of the
feature presence in transactions is taken into account in our model. The
statistical test is used instead of global thresholds to detect significant
co-location patterns and rules. We evaluate our approach on synthetic and real
datasets. This approach can be used by researchers looking for spatial
associations between environmental and health factors. In addition, we explain
the data modelling framework which is used on real datasets of pollutants
(PRTR/NPRI) and childhood cancer cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7287</identifier>
 <datestamp>2014-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7287</id><created>2014-12-23</created><authors><author><keyname>Zhang</keyname><forenames>Edin</forenames></author><author><keyname>Huang</keyname><forenames>Chiachi</forenames></author><author><keyname>Feng</keyname><forenames>Huai-Yan</forenames></author></authors><title>On Non-Integer Linear Degrees of Freedom of Constant Two-Cell MIMO
  Cellular Networks</title><categories>cs.IT math.IT</categories><comments>19 pages, 3 figures. This paper was presented in part at IEEE
  Information Theory Workshop 2014. This work has been submitted to the IEEE
  for possible publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study of degrees of freedom (DoF) of multiuser channels has led to the
development of important interference managing schemes, such as interference
alignment (IA) and interference neutralization. However, while the integer DoF
have been widely studied in literatures, non-integer DoF are much less
addressed, especially for channels with less variety. In this paper, we study
the non-integer DoF of the time-invariant multiple-input multiple-output (MIMO)
interfering multiple access channel (IMAC) in the simple setting of two cells,
$K$ users per cell, and $M$ antennas at all nodes. We provide the exact
characterization of the maximum achievable sum DoF under the constraint of
using linear interference alignment (IA) scheme with symbol extension. Our
results indicate that the integer sum DoF characterization $2MK/(K+1)$ achieved
by the Suh-Ho-Tse scheme can be extended to the non-integer case only when $K
\leq M^2$ for the circularly-symmetric-signaling systems and $K \leq 2M^2$ for
the asymmetric-complex-signaling systems. These results are further extended to
the time-invariant parallel MIMO IMAC with independent subchannels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7288</identifier>
 <datestamp>2014-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7288</id><created>2014-12-23</created><authors><author><keyname>Najdi</keyname><forenames>Hatem</forenames></author></authors><title>Observations Concerning the probability of the existence of annihilators
  for balanced boolean functions</title><categories>cs.IT math.IT</categories><comments>10 pages, 2 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  LFSR-based stream ciphers with nonlinear filters or combiners are susceptible
to algebraic attacks using linearization methods to solve an overdefined system
of nonlinear equations. And this process is greatly enhanced if the filtering
or combining function has a low degree annihilator. To prevent such an attack,
one would choose the parameters of that function so that the degree of its
annihilator becomes large enough. As computing power is continuously
increasing, a choice that seems secure today, becomes insecure tomorrow.
Therefore, a tool is needed to estimate the probability of the existence of
annihilators for balanced boolean functions with parameters that are beyond the
current computing power. Based on experimental and calculational observations,
we give in this paper an almost exact estimate of that probability, which
represent a great improvement over the upper bound previously known.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7309</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7309</id><created>2014-12-23</created><authors><author><keyname>Fabbri</keyname><forenames>Renato</forenames></author></authors><title>A connective differentiation of textual production in interaction
  networks</title><categories>cs.SI physics.data-an physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores textual production in interaction networks, with special
emphasis on its relation to topological measures. Four email lists were
selected, in which measures were taken from the texts participants wrote.
Peripheral, intermediary and hub sectors of these networks were observed to
have discrepant linguistic elaborations. For completeness of exposition,
correlation of textual and topological measures were observed for the entire
network and for each connective sector. The formation of principal components
is used for further insights of how measures are related.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7311</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7311</id><created>2014-12-23</created><authors><author><keyname>Fabbri</keyname><forenames>Renato</forenames></author></authors><title>Versinus: a visualization method for graphs in evolution</title><categories>cs.SI physics.comp-ph physics.soc-ph</categories><comments>article written by request of research colleagues that appreciated
  these visualizations. arXiv admin note: text overlap with arXiv:1310.7769</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents a novel visualization approach for dynamic graphs, the
versinus method, specially useful for real world networks exhibiting free-scale
properties. With a simple and fixed layout, and a small set of visual markups,
the method has been useful for understanding network dynamics. Local community
often suggests that it be reported, which motivated this article. Online
resources deliver videos and computer scripts for rendering new animations.
This article has a concise description of the method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7335</identifier>
 <datestamp>2014-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7335</id><created>2014-12-23</created><authors><author><keyname>Yun</keyname><forenames>Se-Young</forenames></author><author><keyname>Proutiere</keyname><forenames>Alexandre</forenames></author></authors><title>Accurate Community Detection in the Stochastic Block Model via Spectral
  Algorithms</title><categories>cs.SI cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of community detection in the Stochastic Block Model
with a finite number $K$ of communities of sizes linearly growing with the
network size $n$. This model consists in a random graph such that each pair of
vertices is connected independently with probability $p$ within communities and
$q$ across communities. One observes a realization of this random graph, and
the objective is to reconstruct the communities from this observation. We show
that under spectral algorithms, the number of misclassified vertices does not
exceed $s$ with high probability as $n$ grows large, whenever $pn=\omega(1)$,
$s=o(n)$ and \begin{equation*} \lim\inf_{n\to\infty} {n(\alpha_1 p+\alpha_2
q-(\alpha_1 + \alpha_2)p^{\frac{\alpha_1}{\alpha_1 +
\alpha_2}}q^{\frac{\alpha_2}{\alpha_1 + \alpha_2}})\over \log (\frac{n}{s})}
&gt;1,\quad\quad(1) \end{equation*} where $\alpha_1$ and $\alpha_2$ denote the
(fixed) proportions of vertices in the two smallest communities. In view of
recent work by Abbe et al. and Mossel et al., this establishes that the
proposed spectral algorithms are able to exactly recover communities whenever
this is at all possible in the case of networks with two communities with equal
sizes. We conjecture that condition (1) is actually necessary to obtain less
than $s$ misclassified vertices asymptotically, which would establish the
optimality of spectral method in more general scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7358</identifier>
 <datestamp>2014-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7358</id><created>2014-12-23</created><authors><author><keyname>Pereira</keyname><forenames>Olivier</forenames></author></authors><title>Verifiable Elections with Commitment Consistent Encryption -- A Primer</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note provides an introduction to the PPATS Commitment Consistent
Encryption (CCE) scheme proposed by Cuvelier, Pereira and Peters and its use in
the design of end-to-end verifiable elections with a perfectly private audit
trail. These elections can be verified using audit data that will never leak
any information about the vote, even if all the private keys of the elections
are compromised, or if the cryptographic assumptions are broken.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7364</identifier>
 <datestamp>2014-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7364</id><created>2014-12-23</created><authors><author><keyname>Gleich</keyname><forenames>David F.</forenames></author><author><keyname>Grama</keyname><forenames>Ananth</forenames></author><author><keyname>Zhu</keyname><forenames>Yao</forenames></author></authors><title>Erasure coding for fault oblivious linear system solvers</title><categories>cs.NA cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dealing with hardware and software faults is an important problem as parallel
and distributed systems scale to millions of processing cores and wide area
networks. Traditional methods for dealing with faults include
checkpoint-restart, active replicas, and deterministic replay. Each of these
techniques has associated resource overheads and constraints. In this paper, we
propose an alternate approach to dealing with faults, based on input
augmentation. This approach, which is an algorithmic analog of erasure coded
storage, applies a minimally modified algorithm on the augmented input to
produce an augmented output. The execution of such an algorithm proceeds
completely oblivious to faults in the system. In the event of one or more
faults, the real solution is recovered using a rapid reconstruction method from
the augmented output. We demonstrate this approach on the problem of solving
sparse linear systems using a conjugate gradient solver. We present input
augmentation and output recovery techniques. Through detailed experiments, we
show that our approach can be made oblivious to a large number of faults with
low computational overhead. Specifically, we demonstrate cases where a single
fault can be corrected with less than 10% overhead in time, and even in extreme
cases (fault rates of 20%), our approach is able to compute a solution with
reasonable overhead. These results represent a significant improvement over the
state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7366</identifier>
 <datestamp>2014-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7366</id><created>2014-12-23</created><authors><author><keyname>Brecklinghaus</keyname><forenames>Judith</forenames></author><author><keyname>Hougardy</keyname><forenames>Stefan</forenames></author></authors><title>The Approximation Ratio of the Greedy Algorithm for the Metric Traveling
  Salesman Problem</title><categories>cs.DM cs.DS math.CO</categories><msc-class>90C27 90C59 68Q25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the approximation ratio of the greedy algorithm for the metric
Traveling Salesman Problem is $\Theta(\log n)$. Moreover, we prove that the
same result also holds for graphic, Euclidean, and rectilinear instances of the
Traveling Salesman Problem. Finally we show that the approximation ratio of the
Clarke-Wright savings heuristic for the metric Traveling Salesman Problem is
$\Theta(\log n)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7367</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7367</id><created>2014-12-23</created><updated>2015-02-13</updated><authors><author><keyname>Comin</keyname><forenames>Cesar H.</forenames></author><author><keyname>Silva</keyname><forenames>Filipi N.</forenames></author><author><keyname>Costa</keyname><forenames>Luciano da F.</forenames></author></authors><title>A Framework for Evaluating Complex Networks Measurements</title><categories>physics.soc-ph cs.SI physics.data-an</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A good deal of current research in complex networks involves the
characterization and/or classification of the topological properties of given
structures, which has motivated several respective measurements. This letter
proposes a framework for evaluating the quality of complex network measurements
in terms of their effective resolution, degree of degeneracy and
discriminability. The potential of the suggested approach is illustrated with
respect to comparing the characterization of several model and real-world
networks by using concentric and symmetry measurements. The results indicate a
markedly superior performance for the latter type of mapping.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7373</identifier>
 <datestamp>2014-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7373</id><created>2014-12-20</created><authors><author><keyname>Bhowmick</keyname><forenames>Abhishek</forenames></author><author><keyname>L&#xea;</keyname><forenames>Th&#xe1;i Ho&#xe0;ng</forenames></author></authors><title>On primitive elements in finite fields of low characteristic</title><categories>math.NT cs.CC</categories><msc-class>11Lxx</msc-class><acm-class>G.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss the problem of constructing a small subset of a finite field
containing primitive elements of the field. Given a finite field,
$\mathbb{F}_{q^n}$, small $q$ and large $n$, we show that the set of all low
degree polynomials contains the expected number of primitive elements.
  The main theorem we prove is a bound for character sums over short intervals
in function fields. Our result is unconditional and slightly better than what
is known (conditionally under GRH) in the integer case and might be of
independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7384</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7384</id><created>2014-12-21</created><updated>2015-01-04</updated><authors><author><keyname>Yang</keyname><forenames>Peng</forenames></author><author><keyname>Su</keyname><forenames>Xiaoquan</forenames></author><author><keyname>Ou-Yang</keyname><forenames>Le</forenames></author><author><keyname>Chua</keyname><forenames>Hon-Nian</forenames></author><author><keyname>Li</keyname><forenames>Xiao-Li</forenames></author><author><keyname>Ning</keyname><forenames>Kang</forenames></author></authors><title>Microbial community pattern detection in human body habitats via
  ensemble clustering framework</title><categories>q-bio.QM cs.CE cs.LG q-bio.GN</categories><comments>BMC Systems Biology 2014</comments><journal-ref>BMC Systems Biology 2014, 8(Suppl 4):S7</journal-ref><doi>10.1186/1752-0509-8-S4-S7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The human habitat is a host where microbial species evolve, function, and
continue to evolve. Elucidating how microbial communities respond to human
habitats is a fundamental and critical task, as establishing baselines of human
microbiome is essential in understanding its role in human disease and health.
However, current studies usually overlook a complex and interconnected
landscape of human microbiome and limit the ability in particular body habitats
with learning models of specific criterion. Therefore, these methods could not
capture the real-world underlying microbial patterns effectively. To obtain a
comprehensive view, we propose a novel ensemble clustering framework to mine
the structure of microbial community pattern on large-scale metagenomic data.
Particularly, we first build a microbial similarity network via integrating
1920 metagenomic samples from three body habitats of healthy adults. Then a
novel symmetric Nonnegative Matrix Factorization (NMF) based ensemble model is
proposed and applied onto the network to detect clustering pattern. Extensive
experiments are conducted to evaluate the effectiveness of our model on
deriving microbial community with respect to body habitat and host gender. From
clustering results, we observed that body habitat exhibits a strong bound but
non-unique microbial structural patterns. Meanwhile, human microbiome reveals
different degree of structural variations over body habitat and host gender. In
summary, our ensemble clustering framework could efficiently explore integrated
clustering results to accurately identify microbial communities, and provide a
comprehensive view for a set of microbial communities. Such trends depict an
integrated biography of microbial communities, which offer a new insight
towards uncovering pathogenic model of human microbiome.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7386</identifier>
 <datestamp>2014-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7386</id><created>2014-12-21</created><authors><author><keyname>Cannataro</keyname><forenames>Mario</forenames></author><author><keyname>Guzzi</keyname><forenames>Pietro Hiram</forenames></author><author><keyname>Milano</keyname><forenames>Marianna</forenames></author><author><keyname>Veltri</keyname><forenames>Pierangelo</forenames></author></authors><title>A web-based tool to Analyze Semantic Similarity Networks</title><categories>cs.CE cs.AI q-bio.MN</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In computational biology, biological entities such as genes or proteins are
usually annotated with terms extracted from Gene Ontology (GO). The functional
similarity among terms of an ontology is evaluated by using Semantic Similarity
Measures (SSM). More recently, the extensive application of SSMs yielded to the
Semantic Similarity Networks (SSNs). SSNs are edge-weighted graphs where the
nodes are concepts (e.g. proteins) and each edge has an associated weight that
represents the semantic similarity among related pairs of nodes. The analysis
of SSNs may reveal biologically meaningful knowledge. For these aims, the need
for the introduction of tool able to manage and analyze SSN arises.
Consequently we developed SSN-Analyzer a web based tool able to build and
preprocess SSN. As proof of concept we demonstrate that community detection
algorithms applied to filtered (thresholded) networks, have better performances
in terms of biological relevance of the results, with respect to the use of raw
unfiltered networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7399</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7399</id><created>2014-12-23</created><updated>2015-08-11</updated><authors><author><keyname>Anand</keyname><forenames>Namit</forenames></author><author><keyname>Benjamin</keyname><forenames>Colin</forenames></author></authors><title>Do quantum strategies always win?</title><categories>quant-ph cond-mat.dis-nn cs.DS physics.data-an</categories><comments>12 pages, 3 figures, expanded with material on general quantum
  unitaries and discussion on gaming the quantum</comments><journal-ref>Quantum Information Processing, Volume 14, issue 11, pp 4027-4038
  (November 2015)</journal-ref><doi>10.1007/s11128-015-1105-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a seminal paper, Meyer [David Meyer, Phys. Rev. Lett. 82, 1052 (1999)]
described the advantages of quantum game theory by looking at the classical
penny flip game. A player using a quantum strategy can win against a classical
player almost 100\% of the time. Here we make a slight modification to the
quantum game, with the two players sharing an entangled state to begin with. We
then analyze two different scenarios, first in which quantum player makes
unitary transformations to his qubit while the classical player uses a pure
strategy of either flipping or not flipping the state of his qubit. In this
case the quantum player always wins against the classical player. In the second
scenario we have the quantum player making similar unitary transformations
while the classical player makes use of a mixed strategy wherein he either
flips or not with some probability &quot;p&quot;. We show that in the second scenario,
100\% win record of a quantum player is drastically reduced and for a
particular probability &quot;p&quot; the classical player can even win against the
quantum player. This is of possible relevance to the field of quantum
computation as we show that in this quantum game of preserving versus
destroying entanglement a particular classical algorithm can beat the quantum
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7407</identifier>
 <datestamp>2014-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7407</id><created>2014-12-23</created><authors><author><keyname>&#xd6;mer</keyname><forenames>Bernhard</forenames></author><author><keyname>Pacher</keyname><forenames>Christoph</forenames></author></authors><title>Saving fractional bits: A practical entropy efficient code for fair die
  rolls</title><categories>cs.IT math.IT</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an implementation of an algorithm that uses fair coin flips to
simulate fair rolls of an $n$-sided die. A register plays the role of an
entropy pool and holds entropy that is generated as a by-product during each
die roll and that is usually discarded. The entropy stored in this register is
completely reused during the next rolls. Consequently, we can achieve an almost
negligible loss of entropy per roll. The algorithm allows to change the number
of sides of the die in each round. We prove that the entropy loss is monotone
decreasing with increasing entropy pool size (register length).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7415</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7415</id><created>2014-12-23</created><updated>2015-09-26</updated><authors><author><keyname>Joy</keyname><forenames>Jestin</forenames></author><author><keyname>Balakrishnan</keyname><forenames>Kannan</forenames></author></authors><title>A prototype Malayalam to Sign Language Automatic Translator</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Sign language, which is a medium of communication for deaf people, uses
manual communication and body language to convey meaning, as opposed to using
sound. This paper presents a prototype Malayalam text to sign language
translation system. The proposed system takes Malayalam text as input and
generates corresponding Sign Language. Output animation is rendered using a
computer generated model. This system will help to disseminate information to
the deaf people in public utility places like railways, banks, hospitals etc.
This will also act as an educational tool in learning Sign Language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7419</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7419</id><created>2014-12-23</created><updated>2015-10-31</updated><authors><author><keyname>Gulcehre</keyname><forenames>Caglar</forenames></author><author><keyname>Moczulski</keyname><forenames>Marcin</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>ADASECANT: Robust Adaptive Secant Method for Stochastic Gradient</title><categories>cs.LG cs.NE stat.ML</categories><comments>8 pages, 3 figures, ICLR workshop submission</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Stochastic gradient algorithms have been the main focus of large-scale
learning problems and they led to important successes in machine learning. The
convergence of SGD depends on the careful choice of learning rate and the
amount of the noise in stochastic estimates of the gradients. In this paper, we
propose a new adaptive learning rate algorithm, which utilizes curvature
information for automatically tuning the learning rates. The information about
the element-wise curvature of the loss function is estimated from the local
statistics of the stochastic first order gradients. We further propose a new
variance reduction technique to speed up the convergence. In our preliminary
experiments with deep neural networks, we obtained better performance compared
to the popular stochastic gradient algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7424</identifier>
 <datestamp>2014-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7424</id><created>2014-12-23</created><authors><author><keyname>Rao</keyname><forenames>Siddharth Prakash</forenames></author></authors><title>Turning Bitcoins into the Best-coins</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we discuss Bitcoin, the leader among the existing
cryptocurrencies, to analyse its trends, success factors, current challenges
and probable solutions to make it even better. In the introduction section, we
discuss the history and working mechanism of Bitcoin. In the background
section, we develop the ideas that evolved in the process of making a stable
cryptocurrency. We also analyze the survey matrices of the present day
cryptocurrencies. This survey clearly shows that Bitcoin is the clear winner
among its kind. Section 3 is about the success factors of Bitcoin and the
proceeding sections are a discussion about current challenges which pose as
hurdles in making Bitcoin a better currency in the digital world. We finally
discuss the balance between anonymity and reduced trust in the cryptocurrency
world, before concluding the survey.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7437</identifier>
 <datestamp>2014-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7437</id><created>2014-12-23</created><authors><author><keyname>Stark</keyname><forenames>Cyril J.</forenames></author><author><keyname>Harrow</keyname><forenames>Aram W.</forenames></author></authors><title>Compressibility of positive semidefinite factorizations and quantum
  models</title><categories>quant-ph cs.IT math.IT</categories><comments>13 pages</comments><report-no>MIT-CTP 4619</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate compressibility of the dimension of positive semidefinite
matrices while approximately preserving their pairwise inner products. This can
either be regarded as compression of positive semidefinite factorizations of
nonnegative matrices or (if the matrices are subject to additional
normalization constraints) as compression of quantum models. We derive both
lower and upper bounds on compressibility. Applications are broad and range
from the statistical analysis of experimental data to bounding the one-way
quantum communication complexity of Boolean functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7448</identifier>
 <datestamp>2014-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7448</id><created>2014-12-23</created><authors><author><keyname>Khattak</keyname><forenames>Sheharbano</forenames></author><author><keyname>Simon</keyname><forenames>Laurent</forenames></author><author><keyname>Murdoch</keyname><forenames>Steven J.</forenames></author></authors><title>Systemization of Pluggable Transports for Censorship Resistance</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An increasing number of countries implement Internet censorship at different
levels and for a variety of reasons. The link between the censored client and
entry point to the uncensored communication system is a frequent target of
censorship due to the ease with which a nation-state censor can control this.
The diversity of a censor's attack landscape has led to an arms race, leading
to a dramatic speed of evolution of censorship resistance schemes (CRSs) (we
note that at least six CRSs have been written in 2014 so far). Despite the
inherent complexity of CRSs and the breadth of work in this area, there is no
principled way to evaluate individual systems and compare them against each
other.
  In this paper, we (i) sketch an attack model to comprehensively explore a
censor's capabilities, (ii) present an abstract model of a Pluggable Transport
(PT) - a system that helps a censored client communicate with a server over the
Internet while resisting censorship, (iii) describe an evaluation stack that
presents a layered approach to evaluate PT, and (iv) survey 34 existing PTs and
present a detailed evaluation of 6 of these corresponding to our attack model
and evaluation framework. We highlight the inflexibility of current PTs to lend
themselves to feature sharability for broader defense coverage. To address
this, we present Tweakable Transports-PTs built out of re-usable components
following the evaluation stack architecture with a view to flexibly combine
complementary PT features. We also list a set of challenges to guide future
work on Tweakable Transports.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7449</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7449</id><created>2014-12-23</created><updated>2015-06-09</updated><authors><author><keyname>Vinyals</keyname><forenames>Oriol</forenames></author><author><keyname>Kaiser</keyname><forenames>Lukasz</forenames></author><author><keyname>Koo</keyname><forenames>Terry</forenames></author><author><keyname>Petrov</keyname><forenames>Slav</forenames></author><author><keyname>Sutskever</keyname><forenames>Ilya</forenames></author><author><keyname>Hinton</keyname><forenames>Geoffrey</forenames></author></authors><title>Grammar as a Foreign Language</title><categories>cs.CL cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Syntactic constituency parsing is a fundamental problem in natural language
processing and has been the subject of intensive research and engineering for
decades. As a result, the most accurate parsers are domain specific, complex,
and inefficient. In this paper we show that the domain agnostic
attention-enhanced sequence-to-sequence model achieves state-of-the-art results
on the most widely used syntactic constituency parsing dataset, when trained on
a large synthetic corpus that was annotated using existing parsers. It also
matches the performance of standard parsers when trained only on a small
human-annotated dataset, which shows that this model is highly data-efficient,
in contrast to sequence-to-sequence models without the attention mechanism. Our
parser is also fast, processing over a hundred sentences per second with an
unoptimized CPU implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7479</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7479</id><created>2014-12-23</created><updated>2015-04-10</updated><authors><author><keyname>Vijayanarasimhan</keyname><forenames>Sudheendra</forenames></author><author><keyname>Shlens</keyname><forenames>Jonathon</forenames></author><author><keyname>Monga</keyname><forenames>Rajat</forenames></author><author><keyname>Yagnik</keyname><forenames>Jay</forenames></author></authors><title>Deep Networks With Large Output Spaces</title><categories>cs.NE cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep neural networks have been extremely successful at various image, speech,
video recognition tasks because of their ability to model deep structures
within the data. However, they are still prohibitively expensive to train and
apply for problems containing millions of classes in the output layer. Based on
the observation that the key computation common to most neural network layers
is a vector/matrix product, we propose a fast locality-sensitive hashing
technique to approximate the actual dot product enabling us to scale up the
training and inference to millions of output classes. We evaluate our technique
on three diverse large-scale recognition tasks and show that our approach can
train large-scale models at a faster rate (in terms of steps/total time)
compared to baseline methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7489</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7489</id><created>2014-12-23</created><updated>2015-03-26</updated><authors><author><keyname>Yang</keyname><forenames>Yongxin</forenames></author><author><keyname>Hospedales</keyname><forenames>Timothy M.</forenames></author></authors><title>A Unified Perspective on Multi-Domain and Multi-Task Learning</title><categories>stat.ML cs.LG cs.NE</categories><comments>9 pages, Accepted to ICLR 2015 Conference Track</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we provide a new neural-network based perspective on
multi-task learning (MTL) and multi-domain learning (MDL). By introducing the
concept of a semantic descriptor, this framework unifies MDL and MTL as well as
encompassing various classic and recent MTL/MDL algorithms by interpreting them
as different ways of constructing semantic descriptors. Our interpretation
provides an alternative pipeline for zero-shot learning (ZSL), where a model
for a novel class can be constructed without training data. Moreover, it leads
to a new and practically relevant problem setting of zero-shot domain
adaptation (ZSDA), which is the analogous to ZSL but for novel domains: A model
for an unseen domain can be generated by its semantic descriptor. Experiments
across this range of problems demonstrate that our framework outperforms a
variety of alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7504</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7504</id><created>2014-12-23</created><updated>2015-06-03</updated><authors><author><keyname>Jacobs</keyname><forenames>Henry O.</forenames></author><author><keyname>Sommer</keyname><forenames>Stefan</forenames></author></authors><title>Higher-order Spatial Accuracy in Diffeomorphic Image Registration</title><categories>cs.CV math.DG math.OC</categories><comments>33 pages, pages 22-33 consist of an appendix where we list coordinate
  formulas</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discretize a cost functional for image registration problems by deriving
Taylor expansions for the matching term. Minima of the discretized cost
functionals can be computed with no spatial discretization error, and the
optimal solutions are equivalent to minimal energy curves in the space of
$k$-jets. We show that the solutions convergence to optimal solutions of the
original cost functional as the number of particles increases with a
convergence rate of $O(h^{d+k})$ where $h$ is a resolution parameter. The
effect of this approach over traditional particle methods is illustrated on
synthetic examples and real images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7512</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7512</id><created>2014-12-23</created><updated>2015-12-16</updated><authors><author><keyname>Durisi</keyname><forenames>Giuseppe</forenames></author><author><keyname>Koch</keyname><forenames>Tobias</forenames></author><author><keyname>&#xd6;stman</keyname><forenames>Johan</forenames></author><author><keyname>Polyanskiy</keyname><forenames>Yury</forenames></author><author><keyname>Yang</keyname><forenames>Wei</forenames></author></authors><title>Short-Packet Communications over Multiple-Antenna Rayleigh-Fading
  Channels</title><categories>cs.IT math.IT</categories><comments>11 pages, 4 figures; to appear in IEEE Transactions on
  Communications; the numerical routines used for the simulation results are
  part of SPECTRE (short-packet communication toolbox) and can be downloaded at
  https://github.com/yp-mit/spectre</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the current interest in ultra-reliable, low-latency,
machine-type communication systems, we investigate the tradeoff between
reliability, throughput, and latency in the transmission of information over
multiple-antenna Rayleigh block-fading channels. Specifically, we obtain
finite-blocklength, finite-SNR upper and lower bounds on the maximum coding
rate achievable over such channels for a given constraint on the packet error
probability. Numerical evidence suggests that our bounds delimit tightly the
maximum coding rate already for short blocklengths (packets of about 100
symbols). Furthermore, our bounds reveal the existence of a tradeoff between
the rate gain obtainable by spreading each codeword over all available
time-frequency-spatial degrees of freedom, and the rate loss caused by the need
of estimating the fading coefficients over these degrees of freedom. In
particular, our bounds allow us to determine the optimal number of transmit
antennas and the optimal number of time-frequency diversity branches that
maximize the rate. Finally, we show that infinite-blocklength performance
metrics such as the ergodic capacity and the outage capacity yield inaccurate
throughput estimates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7513</identifier>
 <datestamp>2014-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7513</id><created>2014-12-23</created><updated>2014-12-24</updated><authors><author><keyname>Sommer</keyname><forenames>Stefan</forenames></author><author><keyname>Jacobs</keyname><forenames>Henry O.</forenames></author></authors><title>Symmetry in Image Registration and Deformation Modeling</title><categories>cs.CV math.DG</categories><comments>23 pages, survey article</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We survey the role of symmetry in diffeomorphic registration of landmarks,
curves, surfaces, images and higher-order data. The infinite dimensional
problem of finding correspondences between objects can for a range of concrete
data types be reduced resulting in compact representations of shape and spatial
structure. This reduction is possible because the available data is incomplete
in encoding the full deformation model. Using reduction by symmetry, we
describe the reduced models in a common theoretical framework that draws on
links between the registration problem and geometric mechanics. Symmetry also
arises in reduction to the Lie algebra using particle relabeling symmetry
allowing the equations of motion to be written purely in terms of Eulerian
velocity field. Reduction by symmetry has recently been applied for
jet-matching and higher-order discrete approximations of the image matching
problem. We outline these constructions and further cases where reduction by
symmetry promises new approaches to registration of complex data types.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7522</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7522</id><created>2014-12-23</created><updated>2015-01-12</updated><authors><author><keyname>Firat</keyname><forenames>Orhan</forenames></author><author><keyname>Aksan</keyname><forenames>Emre</forenames></author><author><keyname>Oztekin</keyname><forenames>Ilke</forenames></author><author><keyname>Vural</keyname><forenames>Fatos T. Yarman</forenames></author></authors><title>Learning Deep Temporal Representations for Brain Decoding</title><categories>cs.LG cs.NE</categories><comments>This paper has been withdrawn for a revision</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Functional magnetic resonance imaging produces high dimensional data, with a
less then ideal number of labelled samples for brain decoding tasks (predicting
brain states). In this study, we propose a new deep temporal convolutional
neural network architecture with spatial pooling for brain decoding which aims
to reduce dimensionality of feature space along with improved classification
performance. Temporal representations (filters) for each layer of the
convolutional model are learned by leveraging unlabelled fMRI data in an
unsupervised fashion with regularized autoencoders. Learned temporal
representations in multiple levels capture the regularities in the temporal
domain and are observed to be a rich bank of activation patterns which also
exhibit similarities to the actual hemodynamic responses. Further, spatial
pooling layers in the convolutional architecture reduce the dimensionality
without losing excessive information. By employing the proposed temporal
convolutional architecture with spatial pooling, raw input fMRI data is mapped
to a non-linear, highly-expressive and low-dimensional feature space where the
final classification is conducted. In addition, we propose a simple heuristic
approach for hyper-parameter tuning when no validation data is available.
Proposed method is tested on a ten class recognition memory experiment with
nine subjects. The results support the efficiency and potential of the proposed
model, compared to the baseline multi-voxel pattern analysis techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7525</identifier>
 <datestamp>2015-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7525</id><created>2014-12-23</created><updated>2015-11-24</updated><authors><author><keyname>Lee</keyname><forenames>Dong-Hyun</forenames></author><author><keyname>Zhang</keyname><forenames>Saizheng</forenames></author><author><keyname>Fischer</keyname><forenames>Asja</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Difference Target Propagation</title><categories>cs.LG cs.NE</categories><comments>13 pages, 8 figures, Accepted in ECML/PKDD 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Back-propagation has been the workhorse of recent successes of deep learning
but it relies on infinitesimal effects (partial derivatives) in order to
perform credit assignment. This could become a serious issue as one considers
deeper and more non-linear functions, e.g., consider the extreme case of
nonlinearity where the relation between parameters and cost is actually
discrete. Inspired by the biological implausibility of back-propagation, a few
approaches have been proposed in the past that could play a similar credit
assignment role. In this spirit, we explore a novel approach to credit
assignment in deep networks that we call target propagation. The main idea is
to compute targets rather than gradients, at each layer. Like gradients, they
are propagated backwards. In a way that is related but different from
previously proposed proxies for back-propagation which rely on a backwards
network with symmetric weights, target propagation relies on auto-encoders at
each layer. Unlike back-propagation, it can be applied even when units exchange
stochastic bits rather than real numbers. We show that a linear correction for
the imperfectness of the auto-encoders, called difference target propagation,
is very effective to make target propagation actually work, leading to results
comparable to back-propagation for deep networks with discrete and continuous
units and denoising auto-encoders and achieving state of the art for stochastic
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7528</identifier>
 <datestamp>2014-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7528</id><created>2014-12-23</created><authors><author><keyname>Masna</keyname><forenames>Akhilesh</forenames></author><author><keyname>Ganesh</keyname><forenames>Anil</forenames></author><author><keyname>Tirunampalli</keyname><forenames>Prakash</forenames></author><author><keyname>Gaddam</keyname><forenames>Sai Ganesh</forenames></author><author><keyname>Raju</keyname><forenames>Katam</forenames></author><author><keyname>Mandapaka</keyname><forenames>Avinash</forenames></author><author><keyname>Gujjula</keyname><forenames>Bharath Reddy</forenames></author><author><keyname>Iacob</keyname><forenames>Iustin-Daniel</forenames></author></authors><title>DMARF AND GIPSY High Level Architecture and Requirements Analysis</title><categories>cs.SE</categories><comments>59 pages</comments><proxy>Serguei Mokhov</proxy><acm-class>D.2; K.6; H.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the current scenario, many organizations invest on open-source systems
which are becoming popular and result in rapid growth, where in many of them
have not met the quality standards which resulted in need for assessing
quality. Initially we represent our work by analyzing the two open source case
studies which are (1) Distributed Modular Audio Recognition Framework (DMARF)
is an open-source framework which consists of Natural Language Processing (NLP)
implemented using Java which facilitates extensibility by adding new
algorithms, (2) General Intensional Programming System (GIPSY) is a platform
designed to support intensional programming languages which are built using
intensional logic and their imperative counter-parts for the intensional
execution model. During this background study we identified few metrics which
are used to assess the quality characteristics of a software product defined by
ISO standards. Among the metrics, we identified the number of the java classes
and methods using SonarQube. Followed by that, the actors and stakeholders have
been categorized and focused on the evolution of fully dressed use cases.
Besides, we analyzed the requirements and compiled the conceptual UML domain
model diagrams with the responsibilities and relationships based on the
functionalities, which leads to the creation of the class diagrams. Later the
analysis and interpretation of results has been done using the metric tools to
verify results which have been implemented and to identify the code smells
accordingly. Finally the implication is towards performing the system level
refactoring by applying appropriate refactoring methods to enhance the quality
and performance of the open source systems. Besides, the respective test cases
have been portrayed to ensure that there is not much behavioral change with the
existing architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7529</identifier>
 <datestamp>2014-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7529</id><created>2014-12-23</created><authors><author><keyname>Kaur</keyname><forenames>Manpreet</forenames></author><author><keyname>Singh</keyname><forenames>Ravjeet</forenames></author><author><keyname>Kaur</keyname><forenames>Sukhveer</forenames></author><author><keyname>Singh</keyname><forenames>Baljot</forenames></author><author><keyname>Kaur</keyname><forenames>Savpreet</forenames></author><author><keyname>Singh</keyname><forenames>Navkaran</forenames></author><author><keyname>Ohri</keyname><forenames>Aman</forenames></author><author><keyname>Sharma</keyname><forenames>Ravenna</forenames></author></authors><title>Toward Refactoring of DMARF and GIPSY Case Studies -- a Team 9
  SOEN6471-S14 Project Report</title><categories>cs.SE</categories><comments>29 pages</comments><proxy>Serguei Mokhov</proxy><acm-class>D.2; K.6; H.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software architecture consists of series of decisions taken to give a
structural solution that meets all the technical and operational requirements.
The paper involves code refactoring. Code refactoring is a process of changing
the internal structure of the code without altering its external behavior. This
paper focuses over open source systems experimental studies that are DMARF and
GIPSY. We have gone through various research papers and analyzed their
architectures. Refactoring improves understandability, maintainability,
extensibility of the code. Code smells were identified through various tools
such as JDeodorant, Logiscope, and CodePro. Reverse engineering of DMARF and
GIPSY were done for understanding the system. Tool used for this was Object Aid
UML. For better understanding use cases, domain model, design class diagram are
built.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7530</identifier>
 <datestamp>2014-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7530</id><created>2014-12-23</created><authors><author><keyname>Walia</keyname><forenames>Dipesh</forenames></author><author><keyname>Pant</keyname><forenames>Pankaj Kumar</forenames></author><author><keyname>Neela</keyname><forenames>Mahendra</forenames></author><author><keyname>Kumar</keyname><forenames>Naveen</forenames></author><author><keyname>Kunchala</keyname><forenames>Ram Babu</forenames></author></authors><title>Toward Refactoring of DMARF and GIPSY Case Studies -- a Team 12
  SOEN6471-S14 Project Report</title><categories>cs.SE</categories><comments>35 pages</comments><proxy>Serguei Mokhov</proxy><acm-class>D.2; K.6; H.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main significance of this document is two source systems namely GIPSY and
DMARF. Intensional languages are required like GIPSY for absoluteness and
forward practical investigations on the subject.DMARF mainly focuses on
software architectural design and implementation on Distributed Audio
recognition and its applications such as speaker identification which can run
distributively on web services architecture. This mainly highlights security
aspects in a distributed system, the Java data security framework (JDSF) in
DMARF. ASSL (Autonomic System Specification Language) frame work is used to
integrate a self-optimizing property for DMARF. GIPSY mainly depends on
Higher-Order Intensional Logic (HOIL) and reflects three main goals Generality,
Adaptability and Efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7531</identifier>
 <datestamp>2014-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7531</id><created>2014-12-23</created><authors><author><keyname>Yawar</keyname><forenames>Osama</forenames></author><author><keyname>Ayoub</keyname><forenames>Tahir</forenames></author><author><keyname>Beeravelli</keyname><forenames>Yashwanth</forenames></author><author><keyname>Nadir</keyname><forenames>Muhammad</forenames></author><author><keyname>Jamil</keyname><forenames>Shahroze</forenames></author><author><keyname>Darbandi</keyname><forenames>Parham</forenames></author></authors><title>Toward Refactoring of DMARF and GIPSY Case Studies -- a Team 10
  SOEN6471-S14 Project Report</title><categories>cs.SE</categories><comments>40 pages</comments><proxy>Serguei Mokhov</proxy><acm-class>D.2; K.6; H.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The intent of this report is to do a background study of the two given OSS
case study systems namely GIPSY and DMARF. It is a wide research area in which
different studies are being carried out to get the most out of it. It begins
with a formal introduction of the two systems and advance with the complex
architecture of both. GIPSY (General Intensional Programming System) is a
multi-intensional programming system that delivers as a framework for compiling
and executing programs written in Lucid Programming Languages. DMARF
(Distributed Modular Audio Recognition Framework) is a Java based research
platform that acts as a library in applications. As these systems are in their
evolving phase and a lot of research is being done upon these topics, it gives
us motivation to be a part of this research to get a deeper look into the
architectures of both the systems. For the evaluation of quality of metrics of
the two open source systems, we have used a tool namely Logiscope. It is a tool
to automate the code reviews by providing information based on software metrics
and graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7532</identifier>
 <datestamp>2014-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7532</id><created>2014-12-23</created><authors><author><keyname>Das</keyname><forenames>Zinia</forenames></author><author><keyname>Hoque</keyname><forenames>Mohammad Iftekharul</forenames></author><author><keyname>Milkoori</keyname><forenames>Renuka</forenames></author><author><keyname>Nair</keyname><forenames>Jithin</forenames></author><author><keyname>Nayak</keyname><forenames>Rohan</forenames></author><author><keyname>Reddy</keyname><forenames>Swamy Yogya</forenames></author><author><keyname>Sankini</keyname><forenames>Dhana Shree</forenames></author><author><keyname>Zaffar</keyname><forenames>Arslan</forenames></author></authors><title>Toward Refactoring of DMARF and GIPSY Case Studies -- A Team XI
  SOEN6471-S14 Project Report</title><categories>cs.SE</categories><comments>84 pages, 69 figures, 7 tables</comments><proxy>Serguei Mokhov</proxy><acm-class>D.2; K.6; H.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report focuses on improving the internal structure of the Distributed
Modular Audio recognition Framework (DMARF) and the General Intensional
Programming System (GIPSY) case studies without affecting their original
behavior. At first, the general principles, and the working of DMARF and GIPSY
are understood by mainly stressing on the architecture of the systems by
looking at their frameworks and running them in the Eclipse environment. To
improve the quality of the structure of the code, a furtherance of
understanding of the architecture of the case studies and this is achieved by
analyzing the design patterns present in the code. The improvement is done by
the identification and removal of code smells in the code of the case studies.
Code smells are identified by analyzing the source code by using Logiscope and
JDeodorant. Some refactoring techniques are suggested, out of which the best
suited ones are implemented to improve the code. Finally, Test cases are
implemented to check if the behavior of the code has changed or not.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7533</identifier>
 <datestamp>2014-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7533</id><created>2014-12-23</created><authors><author><keyname>Polu</keyname><forenames>Pavan Kumar</forenames></author><author><keyname>Najjar</keyname><forenames>Amjad Al</forenames></author><author><keyname>Banik</keyname><forenames>Biswajit</forenames></author><author><keyname>Kumar</keyname><forenames>Ajay Sujit</forenames></author><author><keyname>Pereira</keyname><forenames>Gustavo</forenames></author><author><keyname>Japhlet</keyname><forenames>Prince</forenames></author><author><keyname>R.</keyname><forenames>Bhanu Prakash</forenames></author><author><keyname>Raparla</keyname><forenames>Sabari Krishna</forenames></author></authors><title>Towards Refactoring of DMARF and GIPSY Case Studies -- A Team 5
  SOEN6471-S14 Project Report</title><categories>cs.SE</categories><comments>46 pages</comments><proxy>Serguei Mokhov</proxy><acm-class>D.2; K.6; H.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an analysis of the architectural design of two
distributed open source systems (OSS) developed in Java: Distributed Modular
Audio Recognition Framework (DMARF) and General Intensional Programming System
(GIPSY). The research starts with a background study of these frameworks to
determine their overall architectures. Afterwards, we identify the actors and
stakeholders and draft a domain model for each framework. Next, we evaluated
and proposed a fused DMARF over GIPSY Run-time Architecture (DoGRTA) as a
domain concept. Later on, the team extracted and studied the actual class
diagrams and determined classes of interest. Next, we identified design
patterns that were present within the code of each framework. Finally, code
smells in the source code were detected using popular tools and a selected
number of those identified smells were refactored using established techniques
and implemented in the final source code. Tests were written and ran prior and
after the refactoring to check for any behavioral changes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7534</identifier>
 <datestamp>2014-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7534</id><created>2014-12-23</created><authors><author><keyname>Somani</keyname><forenames>Afshin</forenames></author><author><keyname>Hassan</keyname><forenames>Ahmad Al-Sheikh</forenames></author><author><keyname>Pedditi</keyname><forenames>Anurag Reddy</forenames></author><author><keyname>Reddy</keyname><forenames>Challa Sai Sukesh</forenames></author><author><keyname>Ranga</keyname><forenames>Vijay Nag</forenames></author><author><keyname>Srinivasan</keyname><forenames>Saravanan Iyyaswamy</forenames></author><author><keyname>Lao</keyname><forenames>Hongyo</forenames></author><author><keyname>Zhili</keyname><forenames>Zhu</forenames></author></authors><title>Toward Refactoring of DMARF and GIPSY Case Studies -- a Team 4
  SOEN6471-S14 Project Report</title><categories>cs.SE</categories><comments>54 pages, 53 figures</comments><proxy>Serguei Mokhov</proxy><acm-class>D.2; K.6; H.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software Quality is a major concern in software engineering development in
order to be competitive. Such a quality can be achieved by a possible technique
called Refactoring where the systems external behavior of the system is not
changed. Initially we present our work by analyzing the case studies of ongoing
researches of DMARF and GIPSY by understanding their needs and requirements
involving the major components in their respective systems. Later sections
illustrate the conceptual architecture of these case studies, for this we have
referenced the original architecture to draw the important candidate concepts
presented in the system, and analyzing their associations with other concepts
in the system and then compared this conceptual architecture with the original
architectures. Later the document throws light on identifying the code smells
exist in the architectures to find them and resolve to minimize the deeper
problems. JDeodorant, SonarQube are the tools which we go across for
identification and analyzing the source code quality, both these tools are
available as an IDE plugin or as an open source platforms. Next is to identify
the design patterns exist in the architectures along with their importance and
need for existence in respective systems. Finally, the implication is towards
introducing refactoring methods onto the smells which have been identified and
possibly refactor them accordingly by applying appropriate refactoring methods
and showcasing the respective tests to ensure that changes in the architecture
does not change the behavior much.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7535</identifier>
 <datestamp>2014-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7535</id><created>2014-12-23</created><authors><author><keyname>Agrawal</keyname><forenames>Nitish</forenames></author><author><keyname>Naidu</keyname><forenames>Rachit</forenames></author><author><keyname>Viswanathan</keyname><forenames>Sadhana</forenames></author><author><keyname>Wankhede</keyname><forenames>Vikram</forenames></author><author><keyname>Nasereldine</keyname><forenames>Zakaria</forenames></author><author><keyname>Kiyani</keyname><forenames>Zohaib S.</forenames></author></authors><title>Towards Refactoring of DMARF and GIPSY Case Studies -- a Team 8
  SOEN6471-S14 Project Report</title><categories>cs.SE</categories><comments>53 pages</comments><proxy>Serguei Mokhov</proxy><acm-class>D.2; K.6; H.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Of the factors that determines the quality of a software system is its design
and architecture. Having a good and clear design and architecture allows the
system to evolve (plan and add new features), be easier to comprehend, easier
to develop, easier to maintain; and in conclusion increase the life time of
the, and being more competitive in its market. In the following paper we study
the architecture of two different systems: GIPSY and DMARF. This paper provides
a general overview of these two systems. What are these two systems, purpose,
architecture, and their design patterns? Classes with week architecture and
design, and code smells were also identified and some refactorings were
suggested and implemented. Several tools were used throughout the paper for
several purpose. LOGICSCOPE, JDeodoant, McCabe were used to identify classes
with weak designs and code smells. Other tools and plugins were also used to
identify class designs and relationships between classes such as ObjectAid
(Eclipse plugin).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7547</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7547</id><created>2014-12-23</created><updated>2015-12-21</updated><authors><author><keyname>Faug&#xe8;re</keyname><forenames>Jean-Charles</forenames><affiliation>PolSys</affiliation></author><author><keyname>Din</keyname><forenames>Mohab Safey El</forenames><affiliation>PolSys</affiliation></author><author><keyname>Verron</keyname><forenames>Thibaut</forenames><affiliation>LIP6, PolSys</affiliation></author></authors><title>On the complexity of computing Gr\&quot;obner bases for weighted homogeneous
  systems</title><categories>cs.SC</categories><proxy>ccsd</proxy><doi>10.1016/j.jsc.2015.12.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Solving polynomial systems arising from applications is frequently made
easier by the structure of the systems. Weighted homogeneity (or
quasi-homogeneity) is one example of such a structure: given a system of
weights $W=(w\_{1},\dots,w\_{n})$, $W$-homogeneous polynomials are polynomials
which are homogeneous w.r.t the weighted degree
$\deg\_{W}(X\_{1}^{\alpha\_{1}},\dots,X\_{n}^{\alpha\_{n}}) = \sum
w\_{i}\alpha\_{i}$. Gr\&quot;obner bases for weighted homogeneous systems can be
computed by adapting existing algorithms for homogeneous systems to the
weighted homogeneous case. We show that in this case, the complexity estimate
for Algorithm~\F5 $\left(\binom{n+\dmax-1}{\dmax}^{\omega}\right)$ can be
divided by a factor $\left(\prod w\_{i} \right)^{\omega}$. For zero-dimensional
systems, the complexity of Algorithm~\FGLM $nD^{\omega}$ (where $D$ is the
number of solutions of the system) can be divided by the same factor
$\left(\prod w\_{i} \right)^{\omega}$. Under genericity assumptions, for
zero-dimensional weighted homogeneous systems of $W$-degree
$(d\_{1},\dots,d\_{n})$, these complexity estimates are polynomial in the
weighted B\'ezout bound $\prod\_{i=1}^{n}d\_{i} / \prod\_{i=1}^{n}w\_{i}$.
Furthermore, the maximum degree reached in a run of Algorithm \F5 is bounded by
the weighted Macaulay bound $\sum (d\_{i}-w\_{i}) + w\_{n}$, and this bound is
sharp if we can order the weights so that $w\_{n}=1$. For overdetermined
semi-regular systems, estimates from the homogeneous case can be adapted to the
weighted case. We provide some experimental results based on systems arising
from a cryptography problem and from polynomial inversion problems. They show
that taking advantage of the weighted homogeneous structure yields substantial
speed-ups, and allows us to solve systems which were otherwise out of reach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7557</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7557</id><created>2014-12-23</created><updated>2015-07-14</updated><authors><author><keyname>Tanbourgi</keyname><forenames>Ralph</forenames></author><author><keyname>Dhillon</keyname><forenames>Harpreet S.</forenames></author><author><keyname>Jondral</keyname><forenames>Friedrich K.</forenames></author></authors><title>Analysis of Joint Transmit-Receive Diversity in Downlink MIMO
  Heterogeneous Cellular Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>15 pages, 10 figures. To appear in IEEE Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study multiple-input multiple-output (MIMO) based downlink heterogeneous
cellular network (HetNets) with joint transmit-receive diversity using
orthogonal space-time block codes at the base stations (BSs) and maximal-ratio
combining (MRC) at the users. MIMO diversity with MRC is especially appealing
in cellular networks due to the relatively low hardware complexity at both the
BS and user device. Using stochastic geometry, we develop a tractable
stochastic model for analyzing such HetNets taking into account the irregular
and multi-tier BS deployment. We derive the coverage probability for both
interference-blind (IB) and interference-aware (IA) MRC as a function of the
relevant tier-specific system parameters such as BS density and power, path
loss law, and number of transmit (Tx) antennas. Important insights arising from
our analysis for typical HetNets are for instance: (i) IA-MRC becomes less
favorable than IB-MRC with Tx diversity due to the smaller interference
variance and increased interference correlation across Rx antennas; (ii)
ignoring spatial interference correlation significantly overestimates the
performance of IA-MRC; (iii) for small number of Rx antennas, selection
combining may offer a better complexity-performance trade-off than MRC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7558</identifier>
 <datestamp>2014-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7558</id><created>2014-12-23</created><authors><author><keyname>Drange</keyname><forenames>P&#xe5;l Gr&#xf8;n&#xe5;s</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Micha&#x142;</forenames></author></authors><title>A Polynomial Kernel for Trivially Perfect Editing</title><categories>cs.DS</categories><comments>30 pages, 11 figures</comments><acm-class>G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a kernel with $O(k^7)$ vertices for Trivially Perfect Editing, the
problem of adding or removing at most $k$ edges in order to make a given graph
trivially perfect. This answers in affirmative an open question posed by Nastos
and Gao, and by Liu et al. Our general technique implies also the existence of
kernels of the same size for the related problems Trivially Perfect Completion
and Trivially Perfect Deletion. Whereas for the former an $O(k^3)$ kernel was
given by Guo, for the latter no polynomial kernel was known.
  We complement our study of Trivially Perfect Editing by proving that,
contrary to Trivially Perfect Completion, it cannot be solved in time $2^{o(k)}
\cdot n^{O(1)}$ unless the Exponential Time Hypothesis fails. In this manner we
complete the picture of the parameterized and kernelization complexity of the
classic edge modification problems for the class of trivially perfect graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7563</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7563</id><created>2014-12-23</created><updated>2015-06-28</updated><authors><author><keyname>Aalto</keyname><forenames>Pekka</forenames></author><author><keyname>Leskel&#xe4;</keyname><forenames>Lasse</forenames></author></authors><title>Information spreading in a large population of active transmitters and
  passive receivers</title><categories>math.PR cs.SI</categories><comments>22 pages</comments><msc-class>60K35, 91D30, 92D25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses a simple stochastic model for the spread of messages in
a large population with two types of individuals: transmitters and receivers.
Transmitters, after receiving the message, start spreading copies of the
message to their neighbors. Receivers may receive the message, but will never
spread it further. We derive approximations of the broadcast time and the first
passage times of selected individuals in populations of size tending to
infinity. These approximations explain how much the fact that only a fraction
of the individuals are transmitters slows down the propagation of information.
Our results also sharply characterize the statistical dependence structure of
first passage times using Gumbel and logistic distributions of extreme value
statistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7580</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7580</id><created>2014-12-23</created><updated>2015-04-10</updated><authors><author><keyname>Vasilache</keyname><forenames>Nicolas</forenames></author><author><keyname>Johnson</keyname><forenames>Jeff</forenames></author><author><keyname>Mathieu</keyname><forenames>Michael</forenames></author><author><keyname>Chintala</keyname><forenames>Soumith</forenames></author><author><keyname>Piantino</keyname><forenames>Serkan</forenames></author><author><keyname>LeCun</keyname><forenames>Yann</forenames></author></authors><title>Fast Convolutional Nets With fbfft: A GPU Performance Evaluation</title><categories>cs.LG cs.DC cs.NE</categories><comments>Camera ready for ICLR2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine the performance profile of Convolutional Neural Network training
on the current generation of NVIDIA Graphics Processing Units. We introduce two
new Fast Fourier Transform convolution implementations: one based on NVIDIA's
cuFFT library, and another based on a Facebook authored FFT implementation,
fbfft, that provides significant speedups over cuFFT (over 1.5x) for whole
CNNs. Both of these convolution implementations are available in open source,
and are faster than NVIDIA's cuDNN implementation for many common convolutional
layers (up to 23.5x for some synthetic kernel configurations). We discuss
different performance regimes of convolutions, comparing areas where
straightforward time domain convolutions outperform Fourier frequency domain
convolutions. Details on algorithmic applications of NVIDIA GPU hardware
specifics in the implementation of fbfft are also provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7584</identifier>
 <datestamp>2014-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7584</id><created>2014-12-23</created><authors><author><keyname>Ji</keyname><forenames>Zhanglong</forenames></author><author><keyname>Lipton</keyname><forenames>Zachary C.</forenames></author><author><keyname>Elkan</keyname><forenames>Charles</forenames></author></authors><title>Differential Privacy and Machine Learning: a Survey and Review</title><categories>cs.LG cs.CR cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The objective of machine learning is to extract useful information from data,
while privacy is preserved by concealing information. Thus it seems hard to
reconcile these competing interests. However, they frequently must be balanced
when mining sensitive data. For example, medical research represents an
important application where it is necessary both to extract useful information
and protect patient privacy. One way to resolve the conflict is to extract
general characteristics of whole populations without disclosing the private
information of individuals.
  In this paper, we consider differential privacy, one of the most popular and
powerful definitions of privacy. We explore the interplay between machine
learning and differential privacy, namely privacy-preserving machine learning
algorithms and learning-based data release mechanisms. We also describe some
theoretical results that address what can be learned differentially privately
and upper bounds of loss functions for differentially private algorithms.
  Finally, we present some open questions, including how to incorporate public
data, how to deal with missing data in private datasets, and whether, as the
number of observed samples grows arbitrarily large, differentially private
machine learning algorithms can be achieved at no cost to utility as compared
to corresponding non-differentially private algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7585</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7585</id><created>2014-12-23</created><updated>2015-02-26</updated><authors><author><keyname>Xu</keyname><forenames>Jia</forenames></author><author><keyname>Shironoshita</keyname><forenames>Patrick</forenames></author><author><keyname>Visser</keyname><forenames>Ubbo</forenames></author><author><keyname>John</keyname><forenames>Nigel</forenames></author><author><keyname>Kabuka</keyname><forenames>Mansur</forenames></author></authors><title>Converting Instance Checking to Subsumption: A Rethink for Object
  Queries over Practical Ontologies</title><categories>cs.AI</categories><journal-ref>International Journal of Intelligence Science, Vol. 5 No. 1,
  44-62, 2015</journal-ref><doi>10.4236/ijis.2015.51005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficiently querying Description Logic (DL) ontologies is becoming a vital
task in various data-intensive DL applications. Considered as a basic service
for answering object queries over DL ontologies, instance checking can be
realized by using the most specific concept (MSC) method, which converts
instance checking into subsumption problems. This method, however, loses its
simplicity and efficiency when applied to large and complex ontologies, as it
tends to generate very large MSC's that could lead to intractable reasoning. In
this paper, we propose a revision to this MSC method for DL SHI, allowing it to
generate much simpler and smaller concepts that are specific-enough to answer a
given query. With independence between computed MSC's, scalability for query
answering can also be achieved by distributing and parallelizing the
computations. An empirical evaluation shows the efficacy of our revised MSC
method and the significant efficiency achieved when using it for answering
object queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7595</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7595</id><created>2014-12-23</created><updated>2015-04-03</updated><authors><author><keyname>Zhang</keyname><forenames>Lei</forenames></author><author><keyname>Wang</keyname><forenames>Feng</forenames></author><author><keyname>Liu</keyname><forenames>Jiangchuan</forenames></author></authors><title>Test Submitting Technical Report</title><categories>cs.MM</categories><report-no>SFU-CMPT TR 2014-34-1</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides a sample of a LaTeX document which conforms, somewhat
loosely, to the formatting guidelines for ACM SIG Proceedings. It is an
alternate style which produces a tighter-looking paper and was designed in
response to concerns expressed, by authors, over page-budgets. It complements
the document Author's (Alternate) Guide to Preparing ACM SIG Proceedings Using
LaTeX and BibTeX. This source file has been written with the intention of being
compiled under LaTeX and BibTeX.
  The developers have tried to include every imaginable sort of &quot;bells and
whistles&quot;, such as a subtitle, footnotes on title, subtitle and authors, as
well as in the text, and every optional component (e.g. Acknowledgments,
Additional Authors, Appendices), not to mention examples of equations,
theorems, tables and figures.
  To make best use of this sample document, run it through LaTeX and BibTeX,
and compare this source code with the printed output produced by the dvi file.
A compiled PDF version is available on the web page to help you with the `look
and feel'.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7610</identifier>
 <datestamp>2014-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7610</id><created>2014-12-24</created><authors><author><keyname>Luo</keyname><forenames>Chen</forenames></author><author><keyname>Pang</keyname><forenames>Wei</forenames></author><author><keyname>Wang</keyname><forenames>Zhe</forenames></author></authors><title>Hete-CF: Social-Based Collaborative Filtering Recommendation using
  Heterogeneous Relations</title><categories>cs.IR</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Collaborative filtering algorithms haven been widely used in recommender
systems. However, they often suffer from the data sparsity and cold start
problems. With the increasing popularity of social media, these problems may be
solved by using social-based recommendation. Social-based recommendation, as an
emerging research area, uses social information to help mitigate the data
sparsity and cold start problems, and it has been demonstrated that the
social-based recommendation algorithms can efficiently improve the
recommendation performance. However, few of the existing algorithms have
considered using multiple types of relations within one social network. In this
paper, we investigate the social-based recommendation algorithms on
heterogeneous social networks and proposed Hete-CF, a Social Collaborative
Filtering algorithm using heterogeneous relations. Distinct from the exiting
methods, Hete-CF can effectively utilize multiple types of relations in a
heterogeneous social network. In addition, Hete-CF is a general approach and
can be used in arbitrary social networks, including event based social
networks, location based social networks, and any other types of heterogeneous
information networks associated with social information. The experimental
results on two real-world data sets, DBLP (a typical heterogeneous information
network) and Meetup (a typical event based social network) show the
effectiveness and efficiency of our algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7618</identifier>
 <datestamp>2014-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7618</id><created>2014-12-24</created><authors><author><keyname>Alamouti</keyname><forenames>Sajjad Mehri</forenames></author><author><keyname>Sharafat</keyname><forenames>Ahmad R.</forenames></author></authors><title>Resource Allocation for Energy-Efficient Device-to-Device Communication
  in 4G Networks</title><categories>cs.NI</categories><comments>2014 7th International Symposium on Telecommunications (IST'2014)</comments><report-no>IST2014-1263</report-no><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Device-to-device (D2D) communications as an underlay of a LTE-A (4G) network
can reduce the traffic load as well as power consumption in cellular networks
by way of utilizing peer-to-peer links for users in proximity of each other.
This would enable other cellular users to increment their traffic, and the
aggregate traffic for all users can be significantly increased without
requiring additional spectrum. However, D2D communications may increase
interference to cellular users (CUs) and force CUs to increase their transmit
power levels in order to maintain their required quality-of-service (QoS). This
paper proposes an energy-efficient resource allocation scheme for D2D
communications as an underlay of a fully loaded LTE-A (4G) cellular network.
Simulations show that the proposed scheme allocates cellular uplink resources
(transmit power and channel) to D2D pairs while maintaining the required QoS
for D2D and cellular users and minimizing the total uplink transmit power for
all users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7624</identifier>
 <datestamp>2014-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7624</id><created>2014-12-24</created><authors><author><keyname>Chen</keyname><forenames>Zhengchuan</forenames></author><author><keyname>Fan</keyname><forenames>Pingyi</forenames></author><author><keyname>Wu</keyname><forenames>Dapeng</forenames></author><author><keyname>Shen</keyname><forenames>Liquan</forenames></author></authors><title>On the Power Allocation for Hybrid DF and CF Protocol with Auxiliary
  Parameter in Fading Relay Channels</title><categories>cs.IT cs.NI math.IT</categories><comments>6 pages, 1 figures, accepted by IEEE WCNC'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In fading channels, power allocation over channel state may bring a rate
increment compared to the fixed constant power mode. Such a rate increment is
referred to power allocation gain. It is expected that the power allocation
gain varies for different relay protocols. In this paper, Decode-and-Forward
(DF) and Compress-and-Forward (CF) protocols are considered. We first establish
a general framework for relay power allocation of DF and CF over channel state
in half-duplex relay channels and present the optimal solution for relay power
allocation with auxiliary parameters, respectively. Then, we reconsider the
power allocation problem for one hybrid scheme which always selects the better
one between DF and CF and obtain a near optimal solution for the hybrid scheme
by introducing an auxiliary rate function as well as avoiding the non-concave
rate optimization problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7625</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7625</id><created>2014-12-24</created><updated>2015-01-06</updated><authors><author><keyname>Qiu</keyname><forenames>Teng</forenames></author><author><keyname>Li</keyname><forenames>Yongjie</forenames></author></authors><title>An Effective Semi-supervised Divisive Clustering Algorithm</title><categories>cs.LG cs.CV stat.ML</categories><comments>8 pages, 4 figures, a new (6th) member of the in-tree clustering
  family</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Nowadays, data are generated massively and rapidly from scientific fields as
bioinformatics, neuroscience and astronomy to business and engineering fields.
Cluster analysis, as one of the major data analysis tools, is therefore more
significant than ever. We propose in this work an effective Semi-supervised
Divisive Clustering algorithm (SDC). Data points are first organized by a
minimal spanning tree. Next, this tree structure is transitioned to the in-tree
structure, and then divided into sub-trees under the supervision of the labeled
data, and in the end, all points in the sub-trees are directly associated with
specific cluster centers. SDC is fully automatic, non-iterative, involving no
free parameter, insensitive to noise, able to detect irregularly shaped cluster
structures, applicable to the data sets of high dimensionality and different
attributes. The power of SDC is demonstrated on several datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7626</identifier>
 <datestamp>2014-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7626</id><created>2014-12-24</created><authors><author><keyname>Abdelaziz</keyname><forenames>Ibrahim</forenames></author><author><keyname>Abdou</keyname><forenames>Sherif</forenames></author></authors><title>AltecOnDB: A Large-Vocabulary Arabic Online Handwriting Recognition
  Database</title><categories>cs.CV</categories><comments>The preprint is in submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Arabic is a semitic language characterized by a complex and rich morphology.
The exceptional degree of ambiguity in the writing system, the rich morphology,
and the highly complex word formation process of roots and patterns all
contribute to making computational approaches to Arabic very challenging. As a
result, a practical handwriting recognition system should support large
vocabulary to provide a high coverage and use the context information for
disambiguation. Several research efforts have been devoted for building online
Arabic handwriting recognition systems. Most of these methods are either using
their small private test data sets or a standard database with limited lexicon
and coverage. A large scale handwriting database is an essential resource that
can advance the research of online handwriting recognition. Currently, there is
no online Arabic handwriting database with large lexicon, high coverage, large
number of writers and training/testing data.
  In this paper, we introduce AltecOnDB, a large scale online Arabic
handwriting database. AltecOnDB has 98% coverage of all the possible PAWS of
the Arabic language. The collected samples are complete sentences that include
digits and punctuation marks. The collected data is available on sentence, word
and character levels, hence, high-level linguistic models can be used for
performance improvements. Data is collected from more than 1000 writers with
different backgrounds, genders and ages. Annotation and verification tools are
developed to facilitate the annotation and verification phases. We built an
elementary recognition system to test our database and show the existing
difficulties when handling a large vocabulary and dealing with large amounts of
styles variations in the collected data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7633</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7633</id><created>2014-12-24</created><updated>2015-01-07</updated><authors><author><keyname>Martin-Martin</keyname><forenames>Alberto</forenames></author><author><keyname>Ordunna-Malea</keyname><forenames>Enrique</forenames></author><author><keyname>Ayllon</keyname><forenames>Juan Manuel</forenames></author><author><keyname>Lopez-Cozar</keyname><forenames>Emilio Delgado</forenames></author></authors><title>Proceedings Scholar Metrics: H Index of proceedings on Computer Science,
  Electrical &amp; Electronic Engineering, and Communications according to Google
  Scholar Metrics (2009-2013)</title><categories>cs.DL</categories><comments>29 pages</comments><report-no>EC3 Reports 12</report-no><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The objective of this report is to present a list of proceedings
(conferences, workshops, symposia, meetings) in the areas of Computer Science,
Electrical &amp; Electronic Engineering, and Communications covered by Google
Scholar Metrics and ranked according to their h-index. Google Scholar Metrics
only displays publications that have published at least 100 papers and have
received at least one citation in the last five years (2009-2013). The searches
were conducted between the 15th and 22nd of December, 2014. A total of 1208
proceedings have been identified
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7641</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7641</id><created>2014-12-24</created><updated>2015-04-10</updated><authors><author><keyname>Schr&#xf6;der</keyname><forenames>Florian</forenames></author><author><keyname>Reischuk</keyname><forenames>Raphael M.</forenames></author><author><keyname>Gehrke</keyname><forenames>Johannes</forenames></author></authors><title>Balancing Isolation and Sharing of Data for Third-Party Extensible App
  Ecosystems</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the landscape of application ecosystems, today's cloud users wish to
personalize not only their browsers with various extensions or their
smartphones with various applications, but also the various extensions and
applications themselves. The resulting personalization significantly raises the
attractiveness for typical Web 2.0 users, but gives rise to various security
risks and privacy concerns, such as unforeseen access to certain critical
components, undesired information flow of personal information to untrusted
applications, or emerging attack surfaces that were not possible before a
personalization has taken place.
  In this paper, we propose a novel extensibility mechanism which is used for
implementing personalization of existing cloud applications towards (possibly
untrusted) components in a secure and privacy-friendly manner. Our model
provides a clean component abstraction, thereby in particular ruling out
undesired component accesses and ensuring that no undesired information flow
takes place between application components -- either trusted from the base
application or untrusted from various extensions. We then instantiate our model
in the SAFE web application framework (WWW 2012), resulting in a novel
methodology that is inspired by traditional access control and specifically
designed for the newly emerging needs of extensibility in application
ecosystems. We illustrate the convenient usage of our techniques by showing how
to securely extend an existing social network application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7645</identifier>
 <datestamp>2014-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7645</id><created>2014-12-24</created><authors><author><keyname>Sasahara</keyname><forenames>Hampei</forenames></author><author><keyname>Nagahara</keyname><forenames>Masaaki</forenames></author><author><keyname>Hayashi</keyname><forenames>Kazunori</forenames></author><author><keyname>yamamoto</keyname><forenames>Yutaka</forenames></author></authors><title>Communication Performance Analysis of Sampled-Data H-infinity Optimal
  Coupling Wave Canceler</title><categories>cs.IT cs.SY math.IT</categories><comments>submitted to the SICE International Symposium on Control Systems
  2015, Dec. 2014; 2 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this manuscript, we propose a design method of digital filters which
cancel coupling waves generated in single-frequency full-duplex wireless relay
stations by using the sampled-data H-infinity control theory. Simulation
results show effectiveness of the proposed method to communication performance
from a base station to a terminal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7646</identifier>
 <datestamp>2014-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7646</id><created>2014-12-24</created><authors><author><keyname>Li</keyname><forenames>Xiao</forenames></author><author><keyname>Pawar</keyname><forenames>Sameer</forenames></author><author><keyname>Ramchandran</keyname><forenames>Kannan</forenames></author></authors><title>Sub-linear Time Support Recovery for Compressed Sensing using
  Sparse-Graph Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of robustly recovering the support of high-dimensional
sparse signals from linear measurements in a low-dimensional subspace. We
introduce a new family of sparse measurement matrices associated with
low-complexity recovery algorithms. Our measurement system is designed to
capture observations of the signal through sparse-graph codes, and to recover
the signal by using a simple peeling decoder. As a result, we can
simultaneously reduce both the measurement cost and the computational
complexity. In this paper, we formally connect general sparse recovery problems
in compressed sensing with sparse-graph decoding in packet-communication
systems, and analyze our design in terms of the measurement cost, computational
complexity and recovery performance.
  Specifically, in the noiseless setting, our scheme requires $2K$ measurements
asymptotically to recover the sparse support of any $K$-sparse signal with
${O}(K)$ arithmetic operations. In the presence of noise, both measurement and
computational costs are ${O}(K\log^{1.\dot{3}} N)$ for recovering any
$K$-sparse signal of dimension $N$. When the signal sparsity $K$ is sub-linear
in the signal dimension $N$, our design achieves {\it sub-linear time support
recovery}. Further, the measurement cost for noisy recovery can also be reduced
to ${O}(K\log N)$ by increasing the computational complexity to near-linear
time ${O}(N\log N)$. In terms of recovery performance, we show that the support
of any $K$-sparse signal can be stably recovered under finite signal-to-noise
ratios with probability one asymptotically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7650</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7650</id><created>2014-12-24</created><updated>2015-01-26</updated><authors><author><keyname>Luzzi</keyname><forenames>Laura</forenames></author><author><keyname>Vehkalahti</keyname><forenames>Roope</forenames></author></authors><title>Division algebra codes achieve MIMO block fading channel capacity within
  a constant gap</title><categories>cs.IT math.IT math.NT</categories><comments>Submitted to ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work addresses the question of achieving capacity with lattice codes in
multi-antenna block fading channels when the number of fading blocks tends to
infinity. In contrast to the standard approach in the literature which employs
random lattice ensembles, the existence results in this paper are derived from
number theory. It is shown that a multiblock construction based on division
algebras achieves rates within a constant gap from block fading capacity both
under maximum likelihood decoding and naive lattice decoding. First the gap to
capacity is shown to depend on the discriminant of the chosen division algebra;
then class field theory is applied to build families of algebras with small
discriminants. The key element in the construction is the choice of a sequence
of division algebras whose centers are number fields with small root
discriminants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7653</identifier>
 <datestamp>2014-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7653</id><created>2014-12-24</created><authors><author><keyname>Hoang</keyname><forenames>Bao-Thien</forenames></author><author><keyname>Imine</keyname><forenames>Abdessamad</forenames></author></authors><title>Efficient Polling Protocol for Decentralized Social Networks</title><categories>cs.DC cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the polling problem in social networks where individuals
collaborate to choose the most favorite choice amongst some options, without
divulging their vote and publicly exposing their potentially malicious actions.
Given this social interaction model, Guerraoui et al. recently proposed polling
protocols that do not rely on any central authority or cryptography system,
using a simple secret sharing scheme along with verification procedures to
accurately compute the poll's final result. However, these protocols can be
deployed safely and efficiently provided that, inter alia, the social graph
structure should be transformed into a ring structure-based overlay and the
number of participating users is perfect square. Consequently, designing
\emph{secure} and \emph{efficient} polling protocols regardless these
constraints remains a challenging issue.
  In this paper, we present EPol, a simple decentralized polling protocol that
relies on the current state of social graphs. More explicitly, we define one
family of social graphs that satisfy what we call the $m$-broadcasting property
(where $m$ is less than or equal to the minimum node degree) and show their
structures enable low communication cost and constitute necessary and
sufficient condition to ensure vote privacy and limit the impact of dishonest
users on the accuracy of the polling output. Our protocol is effective to
compute more precisely the final result. Furthermore, despite the use of richer
social graph structures, the communication and spatial complexities of EPol are
close to be linear.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7659</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7659</id><created>2014-12-24</created><updated>2015-04-07</updated><authors><author><keyname>Cohen</keyname><forenames>Taco S.</forenames></author><author><keyname>Welling</keyname><forenames>Max</forenames></author></authors><title>Transformation Properties of Learned Visual Representations</title><categories>cs.LG cs.CV cs.NE</categories><comments>T.S. Cohen &amp; M. Welling, Transformation Properties of Learned Visual
  Representations. In International Conference on Learning Representations
  (ICLR), 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When a three-dimensional object moves relative to an observer, a change
occurs on the observer's image plane and in the visual representation computed
by a learned model. Starting with the idea that a good visual representation is
one that transforms linearly under scene motions, we show, using the theory of
group representations, that any such representation is equivalent to a
combination of the elementary irreducible representations. We derive a striking
relationship between irreducibility and the statistical dependency structure of
the representation, by showing that under restricted conditions, irreducible
representations are decorrelated. Under partial observability, as induced by
the perspective projection of a scene onto the image plane, the motion group
does not have a linear action on the space of images, so that it becomes
necessary to perform inference over a latent representation that does transform
linearly. This idea is demonstrated in a model of rotating NORB objects that
employs a latent representation of the non-commutative 3D rotation group SO(3).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7664</identifier>
 <datestamp>2014-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7664</id><created>2014-12-24</created><authors><author><keyname>Salgado</keyname><forenames>M. G. G. C. R.</forenames></author><author><keyname>Ragel</keyname><forenames>R. G.</forenames></author></authors><title>Register Spilling for Specific Application Domains in Application
  Specific Instruction-set Processors</title><categories>cs.PL</categories><comments>The 7th International Conference on Information and Automation for
  Sustainability (ICIAfS) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An Application Specific Instruction set Processor (ASIP) is an important
component in designing embedded systems. One of the problems in designing an
instruction set for such processors is determining the number of registers is
needed in the processor that will optimize the computational time and the cost.
The performance of a processor may fall short due to register spilling, which
is caused by the lack of available registers in a processor. In the design
perspective, it will result in processors with great performance and low power
consumption if we can avoid register spilling by deciding a value for the
number of registers needed in an ASIP. However, as of now, it has not clearly
been recognized how the number of registers changes with different application
domains. In this paper, we evaluated whether different application domains have
any significant effect on register spilling and therefore the performance of a
processor so that we could use different number of registers when building
ASIPs for different application domains rather than using a constant set of
registers. Such utilization of registers will result in processors with high
performance, low cost and low power consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7677</identifier>
 <datestamp>2014-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7677</id><created>2014-12-24</created><authors><author><keyname>Bulumulla</keyname><forenames>C. B</forenames></author><author><keyname>Ragel</keyname><forenames>R. G.</forenames></author></authors><title>LineCAPTCHA Mobile: A User Friendly Replacement for Unfriendly Reverse
  Turing Tests for Mobile Devices (ICIAfS14)</title><categories>cs.HC</categories><comments>The 7th International Conference on Information and Automation for
  Sustainability (ICIAfS) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As smart phones and tablets are becoming ubiquitous and taking over as the
primary choice for accessing the Internet worldwide, ensuring a secure gateway
to the servers serving such devices become essential. CAPTCHAs play an
important role in identifying human users in internet to prevent unauthorized
bot attacks. Even though there are numerous CAPTCHA alternatives available
today, there are certain drawbacks attached with each alternative, making them
harder to find a general solution for the necessity of a CAPTCHA mechanism.
With the advancing technology and expertise in areas such as AI, cryptography
and image processing, it has come to a stage where the chase between making and
breaking CAPTCHAs are even now. This has led the humans with a hard time
deciphering the CAPTCHA mechanisms. In this paper, we adapt a novel CAPTCHA
mechanism named as LineCAPTCHA to mobile devices. LineCAPTCHA is a new reverse
Turing test based on drawing on top of Bezier curves within noisy backgrounds.
The major objective of this paper is to report the implementation and
evaluation of LineCAPTCHA on a mobile platform. At the same time we impose
certain security standards and security aspects for establishing LineCAPTCHAs
which are obtained through extensive measures. Independency from factors such
as the fluency in English language, age and easily understandable nature of it
inclines the usability of LineCAPTCHA. We believe that such independency will
favour the main target of LineCAPTCHA, user friendliness and usability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7680</identifier>
 <datestamp>2014-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7680</id><created>2014-12-24</created><authors><author><keyname>Gunarathna</keyname><forenames>G. I.</forenames></author><author><keyname>Chamikara</keyname><forenames>M. A. P.</forenames></author><author><keyname>Ragel</keyname><forenames>R. G.</forenames></author></authors><title>A Fuzzy Based Model to Identify Printed Sinhala Characters (ICIAfS14)</title><categories>cs.CV</categories><comments>The 7th International Conference on Information and Automation for
  Sustainability (ICIAfS) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Character recognition techniques for printed documents are widely used for
English language. However, the systems that are implemented to recognize Asian
languages struggle to increase the accuracy of recognition. Among other Asian
languages (such as Arabic, Tamil, Chinese), Sinhala characters are unique,
mainly because they are round in shape. This unique feature makes it a
challenge to extend the prevailing techniques to improve recognition of Sinhala
characters. Therefore, a little attention has been given to improve the
accuracy of Sinhala character recognition. A novel method, which makes use of
this unique feature, could be advantageous over other methods. This paper
describes the use of a fuzzy inference system to recognize Sinhala characters.
Feature extraction is mainly focused on distance and intersection measurements
in different directions from the center of the letter making use of the round
shape of characters. The results showed an overall accuracy of 90.7% for 140
instances of letters tested, much better than similar systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7682</identifier>
 <datestamp>2014-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7682</id><created>2014-12-24</created><authors><author><keyname>Gamaarachchi</keyname><forenames>Hasindu</forenames></author><author><keyname>Ragel</keyname><forenames>Roshan</forenames></author><author><keyname>Jayasinghe</keyname><forenames>Darshana</forenames></author></authors><title>Accelerating Correlation Power Analysis Using Graphics Processing Units</title><categories>cs.PF</categories><comments>The 7th International Conference on Information and Automation for
  Sustainability (ICIAfS) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Correlation Power Analysis (CPA) is a type of power analysis based side
channel attack that can be used to derive the secret key of encryption
algorithms including DES (Data Encryption Standard) and AES (Advanced
Encryption Standard). A typical CPA attack on unprotected AES is performed by
analysing a few thousand power traces that requires about an hour of
computational time on a general purpose CPU. Due to the severity of this
situation, a large number of researchers work on countermeasures to such
attacks. Verifying that a proposed countermeasure works well requires
performing the CPA attack on about 1.5 million power traces. Such processing,
even for a single attempt of verification on commodity hardware would run for
several days making the verification process infeasible. Modern Graphics
Processing Units (GPUs) have support for thousands of light weight threads,
making them ideal for parallelizable algorithms like CPA. While the cost of a
GPU being lesser than a high performance multicore server, still the GPU
performance for this algorithm is many folds better than that of a multicore
server. We present an algorithm and its implementation on GPU for CPA on
128-bit AES that is capable of executing 1300x faster than that on a single
threaded CPU and more than 60x faster than that on a 32 threaded multicore
server. We show that an attack that would take hours on the multicore server
would take even less than a minute on a much cost effective GPU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7684</identifier>
 <datestamp>2014-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7684</id><created>2014-12-24</created><authors><author><keyname>Satkunarajah</keyname><forenames>Suthaharan</forenames></author><author><keyname>Ratnam</keyname><forenames>Krishanthmohan</forenames></author><author><keyname>Ragel</keyname><forenames>Roshan G.</forenames></author></authors><title>Efficient Switch Architectures for Pre-configured Backup Protection with
  Sharing in Elastic Optical Networks (EON)</title><categories>cs.NI</categories><comments>The 7th International Conference on Information and Automation for
  Sustainability (ICIAfS) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the problem of providing survivability in elastic
optical networks (EONs). EONs use fine granular frequency slots or flexible
grids, when compared to the conventional fixed grid networks and therefore
utilize the frequency spectrum efficiently. For providing survivability in
EONs, we consider a recently proposed survivability method for conventional
fixed grid networks, known as pre-configured backup protection with sharing
(PBPS), because of its benefits over the traditional survivability approaches
such as dedicated and shared protection. In PBPS, backup paths can be
pre-configured and at the same time they can share resources. Therefore, both
short recovery time and efficient resource usage can be achieved. We find that
the existing switch architectures do not support both PBPS and EONs.
Specifically, we identify and illustrate that, if a switch architecture is not
carefully designed, several key problems/issues might arise in certain
scenarios. Such problems include unnecessary resource consumption, inability of
using existing free resources, and incapability of sharing backup paths. These
problems appear when PBPS is adopted in EONs and they do not arise in fixed
grid networks. In this paper, we propose new switch architectures which support
both PBPS and EONs. Particularly, we illustrate that, our switch architectures
avoid the specific problems/issues mentioned above. Therefore, our switch
architectures support using resources more efficiently and reducing blocking of
requests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7686</identifier>
 <datestamp>2014-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7686</id><created>2014-12-24</created><authors><author><keyname>Zhang</keyname><forenames>Weituo</forenames></author><author><keyname>Lim</keyname><forenames>Chjan</forenames></author></authors><title>Network Evolution by Relevance and Importance Preferential Attachment</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Relevance and importance are the main factors when humans build network
connections. We propose an evolutionary network model based on preferential
attachment(PA) considering these factors. We analyze and compute several
important features of the network class generated by this algorithm including
scale free degree distribution, high clustering coefficient, small world
property and core-periphery structure. We then compare this model with other
network models and empirical data such as inter-city road transportation and
air traffic networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7689</identifier>
 <datestamp>2014-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7689</id><created>2014-12-24</created><authors><author><keyname>Mac</keyname><forenames>Akmal Jahan</forenames></author><author><keyname>Ragel</keyname><forenames>Roshan G</forenames></author></authors><title>Locating Tables in Scanned Documents for Reconstructing and Republishing
  (ICIAfS14)</title><categories>cs.CV</categories><comments>The 7th International Conference on Information and Automation for
  Sustainability (ICIAfS) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pool of knowledge available to the mankind depends on the source of learning
resources, which can vary from ancient printed documents to present electronic
material. The rapid conversion of material available in traditional libraries
to digital form needs a significant amount of work if we are to maintain the
format and the look of the electronic documents as same as their printed
counterparts. Most of the printed documents contain not only characters and its
formatting but also some associated non text objects such as tables, charts and
graphical objects. It is challenging to detect them and to concentrate on the
format preservation of the contents while reproducing them. To address this
issue, we propose an algorithm using local thresholds for word space and line
height to locate and extract all categories of tables from scanned document
images. From the experiments performed on 298 documents, we conclude that our
algorithm has an overall accuracy of about 75% in detecting tables from the
scanned document images. Since the algorithm does not completely depend on rule
lines, it can detect all categories of tables in a range of scanned documents
with different font types, styles and sizes to extract their formatting
features. Moreover, the algorithm can be applied to locate tables in multi
column layouts with small modification in layout analysis. Treating tables with
their existing formatting features will tremendously help the reproducing of
printed documents for reprinting and updating purposes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7692</identifier>
 <datestamp>2014-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7692</id><created>2014-12-24</created><authors><author><keyname>Abeysinghe</keyname><forenames>T. M. R. L. B.</forenames></author><author><keyname>Hassan</keyname><forenames>N.</forenames></author><author><keyname>Ragel</keyname><forenames>R. G.</forenames></author></authors><title>A Feasibility Study on Programmer Specific Instruction Set Processors
  (PSISPs)</title><categories>cs.AR</categories><comments>The 7th International Conference on Information and Automation for
  Sustainability (ICIAfS) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  ASIPs are designed in order to execute instructions of a particular domain of
applications. The designing of ASIPs addresses the major challenges faced by a
system on chip such as size, cost, performance and energy consumption. The
higher the number of similar instructions within the domain to be mapped the
lesser the energy consumption, the smaller the size and the higher the
performance of the ASIP. Thus, designing processors for domains with more
similar programs would overcome these issues. This paper describes the
investigation of whether the domains of programmer specific programs have any
significance like application specific program domains and thus, whether the
approach of designing processors known as Programmer Specific Instruction Set
Processors is worthwhile. We performed the evaluation at the instruction level
by using four different measures to obtain the similarity of programs: (1) by
the existence of each instruction, (2) by the frequency of each instruction,
(3) by two consecutive instruction patterns and (4) by three consecutive
instruction patterns of application specific and programmer specific programs.
We found that although programmer specific instructions show some impact on the
similarity measures, they are much smaller and therefore insignificant compared
to the impact from application specific programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7693</identifier>
 <datestamp>2014-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7693</id><created>2014-12-24</created><authors><author><keyname>Gupta</keyname><forenames>Anupam</forenames></author><author><keyname>Kumar</keyname><forenames>Amit</forenames></author></authors><title>Greedy Algorithms for Steiner Forest</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Steiner Forest problem, we are given terminal pairs $\{s_i, t_i\}$,
and need to find the cheapest subgraph which connects each of the terminal
pairs together. In 1991, Agrawal, Klein, and Ravi, and Goemans and Williamson
gave primal-dual constant-factor approximation algorithms for this problem;
until now, the only constant-factor approximations we know are via linear
programming relaxations.
  We consider the following greedy algorithm: Given terminal pairs in a metric
space, call a terminal &quot;active&quot; if its distance to its partner is non-zero.
Pick the two closest active terminals (say $s_i, t_j$), set the distance
between them to zero, and buy a path connecting them. Recompute the metric, and
repeat. Our main result is that this algorithm is a constant-factor
approximation.
  We also use this algorithm to give new, simpler constructions of cost-sharing
schemes for Steiner forest. In particular, the first &quot;group-strict&quot; cost-shares
for this problem implies a very simple combinatorial sampling-based algorithm
for stochastic Steiner forest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7699</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7699</id><created>2014-12-24</created><authors><author><keyname>Ethier</keyname><forenames>S. N.</forenames></author><author><keyname>Lee</keyname><forenames>Jiyeon</forenames></author></authors><title>Parrondo games with spatial dependence, III</title><categories>math.PR cs.GT</categories><comments>20 pages, 4 figures</comments><msc-class>60J10 (Primary) 60J20 (Secondary)</msc-class><journal-ref>Fluctuation and Noise Letters 14 (2015) 1550039</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study Toral's Parrondo games with $N$ players and one-dimensional spatial
dependence as modified by Xie et al. Specifically, we use computer graphics to
sketch the Parrondo and anti-Parrondo regions for $3\le N\le 9$. Our work was
motivated by a recent paper of Li et al., who applied a state space reduction
method to this model, reducing the number of states from $2^N$ to $N+1$. We
show that their reduced Markov chains are inconsistent with the model of Xie et
al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7703</identifier>
 <datestamp>2014-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7703</id><created>2014-12-24</created><authors><author><keyname>Rasouli</keyname><forenames>Mohammad</forenames></author></authors><title>A Game-Theoretic Framework for Studying Dynamics of Multi Decision-maker
  Systems</title><categories>cs.GT cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  System Dynamics (SD) main aim is to study dynamic behavior of systems based
on causal relations. The other purpose of the science is to design policies,
both in initial values and causal relation, to change system behavior as we
desire. Especially we are interested in making systems behavior a convergent
one. Although now SD is mainly used in situations of single policy maker, there
are major parts of situations in which there are multi policy makers playing
role. Game Theory (GT) is an appropriate tool for studying such cases.GT is the
theory of studying multi decision-maker conditions. In this paper we will
introduce GT and explain how to apply it in SD. Also we will provide some
examples of microeconomic systems and show how to use GT for studying and
simulating dynamics of these example systems. We will also have a short discuss
on how SD can help GT studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7713</identifier>
 <datestamp>2014-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7713</id><created>2014-12-21</created><authors><author><keyname>Kang</keyname><forenames>Jinkyu</forenames><affiliation>Shitz</affiliation></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames><affiliation>Shitz</affiliation></author><author><keyname>Kang</keyname><forenames>Joonhyuk</forenames><affiliation>Shitz</affiliation></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames><affiliation>Shitz</affiliation></author></authors><title>Fronthaul Compression and Precoding Design for C-RANs over Ergodic
  Fading Channel</title><categories>cs.IT math.IT</categories><comments>25 pages, 9 figures, Submitted to IEEE Transactions on Vehicular
  Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work investigates the joint design of fronthaul compression and
precoding for the downlink of Cloud Radio Access Networks (C-RANs). In a C-RAN,
a central unit (CU) performs the baseband processing for a cluster of radio
units (RUs) that receive compressed baseband samples from the CU through
low-latency fronthaul links. Most previous works on the design of fronthaul
compression and precoding assume constant channels and instantaneous channel
state information (CSI) at the CU. This work, in contrast, concentrates on a
more practical scenario with block-ergodic channels and considers either
instantaneous or stochastic CSI at the CU. Moreover, the analysis encompasses
both the Compression-After-Precoding (CAP) and the Compression-Before-Precoding
(CBP) schemes. With the CAP approach, which is the standard C-RAN solution, the
CU performs channel coding and precoding and then the CU compresses and
forwards the resulting baseband signals on the fronthaul links to the RUs. With
the CBP scheme, instead, the CU does not perform precoding but rather forwards
separately the information messages of a subset of mobile stations (MSs) along
with the compressed precoding matrices to the each RU, which then performs
precoding. Optimization algorithms over fronthaul compression and precoding for
both CAP and CBP are proposed that are based on a stochastic successive
upper-bound minimization approach. Via numerical results, the relative merits
of the two strategies under either instantaneous or stochastic CSI are
evaluated as a function of system parameters such as fronthaul capacity and
channel coherence time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7725</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7725</id><created>2014-12-24</created><updated>2015-05-15</updated><authors><author><keyname>Yan</keyname><forenames>Zhicheng</forenames></author><author><keyname>Zhang</keyname><forenames>Hao</forenames></author><author><keyname>Wang</keyname><forenames>Baoyuan</forenames></author><author><keyname>Paris</keyname><forenames>Sylvain</forenames></author><author><keyname>Yu</keyname><forenames>Yizhou</forenames></author></authors><title>Automatic Photo Adjustment Using Deep Neural Networks</title><categories>cs.CV cs.LG</categories><comments>TOG minor revision</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Photo retouching enables photographers to invoke dramatic visual impressions
by artistically enhancing their photos through stylistic color and tone
adjustments. However, it is also a time-consuming and challenging task that
requires advanced skills beyond the abilities of casual photographers. Using an
automated algorithm is an appealing alternative to manual work but such an
algorithm faces many hurdles. Many photographic styles rely on subtle
adjustments that depend on the image content and even its semantics. Further,
these adjustments are often spatially varying. Because of these
characteristics, existing automatic algorithms are still limited and cover only
a subset of these challenges. Recently, deep machine learning has shown unique
abilities to address hard problems that resisted machine algorithms for long.
This motivated us to explore the use of deep learning in the context of photo
editing. In this paper, we explain how to formulate the automatic photo
adjustment problem in a way suitable for this approach. We also introduce an
image descriptor that accounts for the local semantics of an image. Our
experiments demonstrate that our deep learning formulation applied using these
descriptors successfully capture sophisticated photographic styles. In
particular and unlike previous techniques, it can model local adjustments that
depend on the image semantics. We show on several examples that this yields
results that are qualitatively and quantitatively better than previous work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7748</identifier>
 <datestamp>2014-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7748</id><created>2014-12-24</created><authors><author><keyname>Eydelzon</keyname><forenames>Anatoly</forenames></author></authors><title>On recoverability properties of fixed measurement matrices</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this paper is to extend a result by Donoho and Huo, Elad and
Bruckstein, Gribnoval and Nielsen on sparse representations of signals in
dictionaries to general matrices. We consider a general fixed measurement
matrix, not necessarily a dictionary, and derive sufficient condition for
having unique sparse representation of signals in this matrix. Currently, to
the best of our knowledge, no such method exists. In particular, if matrix is a
dictionary, our method is at least as good as the method proposed by Gribnoval
and Nielsen.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7753</identifier>
 <datestamp>2015-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7753</id><created>2014-12-24</created><updated>2015-04-16</updated><authors><author><keyname>Mikolov</keyname><forenames>Tomas</forenames></author><author><keyname>Joulin</keyname><forenames>Armand</forenames></author><author><keyname>Chopra</keyname><forenames>Sumit</forenames></author><author><keyname>Mathieu</keyname><forenames>Michael</forenames></author><author><keyname>Ranzato</keyname><forenames>Marc'Aurelio</forenames></author></authors><title>Learning Longer Memory in Recurrent Neural Networks</title><categories>cs.NE cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recurrent neural network is a powerful model that learns temporal patterns in
sequential data. For a long time, it was believed that recurrent networks are
difficult to train using simple optimizers, such as stochastic gradient
descent, due to the so-called vanishing gradient problem. In this paper, we
show that learning longer term patterns in real data, such as in natural
language, is perfectly possible using gradient descent. This is achieved by
using a slight structural modification of the simple recurrent neural network
architecture. We encourage some of the hidden units to change their state
slowly by making part of the recurrent weight matrix close to identity, thus
forming kind of a longer term memory. We evaluate our model in language
modeling experiments, where we obtain similar performance to the much more
complex Long Short Term Memory (LSTM) networks (Hochreiter &amp; Schmidhuber,
1997).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7755</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7755</id><created>2014-12-24</created><updated>2015-04-23</updated><authors><author><keyname>Ba</keyname><forenames>Jimmy</forenames></author><author><keyname>Mnih</keyname><forenames>Volodymyr</forenames></author><author><keyname>Kavukcuoglu</keyname><forenames>Koray</forenames></author></authors><title>Multiple Object Recognition with Visual Attention</title><categories>cs.LG cs.CV cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an attention-based model for recognizing multiple objects in
images. The proposed model is a deep recurrent neural network trained with
reinforcement learning to attend to the most relevant regions of the input
image. We show that the model learns to both localize and recognize multiple
objects despite being given only class labels during training. We evaluate the
model on the challenging task of transcribing house number sequences from
Google Street View images and show that it is both more accurate than the
state-of-the-art convolutional networks and uses fewer parameters and less
computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7760</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7760</id><created>2014-12-24</created><authors><author><keyname>Nawaz</keyname><forenames>Waqas</forenames></author><author><keyname>Khan</keyname><forenames>Kifayat Ullah</forenames></author><author><keyname>Lee</keyname><forenames>Young-Koo</forenames></author></authors><title>Shortest Path Analysis in Social Graphs</title><categories>cs.SI</categories><comments>4 pages, Accepted in The 3rd International Conference on Convergence
  and its Application (ICCA 2014), Seoul, Korea</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The shortest path problem is among the most fundamental combinatorial
optimization problems to answer reachability queries. It is hard to deter-mine
which vertices or edges are visited during shortest path traversals. In this
paper, we provide an empirical analysis on how traversal algorithms behave on
social networks. First, we compute the shortest paths between set of vertices.
Each shortest path is considered as one transaction. Second, we utilize the
pat-tern mining approach to identify the frequency of occurrence of the
vertices. We evaluate the results in terms of network properties, i.e. degree
distribution, clustering coefficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7763</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7763</id><created>2014-12-24</created><authors><author><keyname>Zhang</keyname><forenames>Chuang</forenames></author><author><keyname>Fan</keyname><forenames>Pingyi</forenames></author><author><keyname>Xiong</keyname><forenames>Ke</forenames></author></authors><title>Downlink Resource Allocation for the High-speed Train and Local Users in
  OFDMA Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider providing services for passengers in a high-speed train and local
users (quasi-static users) in a single OFDMA system. For the train, we apply a
two-hop architecture, under which, passengers communicate with base stations
(BSs) via a mobile relay (MR) installed in the train cabin. With this
architecture, all passengers in the train can be represented by the MR. Since
the channels of the MR and local users vary differently, we consider allocating
system resources (power and subcarriers) over two time-scales for them. We
formulate the problem as a capacity optimization problem for the MR subject to
the sum capacity constraint of local users. We treat the inter-carrier
interference (ICI) at the MR as additive Gaussian noise and derive an explicit
expression for the ICI using the two-path Doppler spread model. Then we discuss
the optimization problem and propose an optimal power and subcarrier allocation
(OPSA) policy. The capacity obtained using OPSA is compared with that of
constant power and subcarrier allocation (CPSA) policies. Simulation results
justify the optimality of the OPSA. Besides, by comparing the capacity bounds
achieved by OPSA with and without ICI, we find that only in specific regions,
where the gap between the capacity bounds is large, do practical ICI
cancellation methods provide meaningful rate gain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7772</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7772</id><created>2014-12-24</created><authors><author><keyname>Zu</keyname><forenames>K.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author></authors><title>Coordinate Tomlinson-Harashima Precoding Design for Overloaded
  Multi-user MIMO Systems</title><categories>cs.IT math.IT</categories><comments>3 figures, 6 pages, ISWCS 2014. arXiv admin note: text overlap with
  arXiv:1401.4753</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tomlinson-Harashima precoding (THP) is a nonlinear processing technique
employed at the transmit side to implement the concept of dirty paper coding
(DPC). The perform of THP, however, is restricted by the dimensionality
constraint that the number of transmit antennas has to be greater or equal to
the total number of receive antennas. In this paper, we propose an iterative
coordinate THP algorithm for the scenarios in which the total number of receive
antennas is larger than the number of transmit antennas. The proposed algorithm
is implemented on two types of THP structures, the decentralized THP (dTHP)
with diagonal weighted filters at the receivers of the users, and the
centralized THP (cTHP) with diagonal weighted filter at the transmitter.
Simulation results show that a much better bit error rate (BER) and sum-rate
performances can be achieved by the proposed iterative coordinate THP compared
to the previous linear art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7774</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7774</id><created>2014-12-24</created><authors><author><keyname>Ho</keyname><forenames>Chol Man</forenames></author><author><keyname>Gwak</keyname><forenames>Son Il</forenames></author><author><keyname>Pak</keyname><forenames>Song Ho</forenames></author><author><keyname>Ha</keyname><forenames>Jong Won</forenames></author></authors><title>Improved Parameter Identification Method Based on Moving Rate</title><categories>cs.NE</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  To improve the problem that the parameter identification for fuzzy neural
network has many time complexities in calculating, an improved T-S fuzzy
inference method and an parameter identification method for fuzzy neural
network are proposed. It mainly includes three parts. First, improved fuzzy
inference method based on production term for T-S Fuzzy model is explained.
Then, compared with existing Sugeno fuzzy inference based on Compositional
rules and type-distance fuzzy inference method, the proposed fuzzy inference
algorithm has a less amount of complexity in calculating and the calculating
process is simple. Next, a parameter identification method for FNN based on
production inference is proposed. Finally, the proposed method is applied for
the precipitation forecast and security situation prediction. Test results
showed that the proposed method significantly improved the effectiveness of
identification, reduced the learning order, time complexity and learning error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7780</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7780</id><created>2014-12-24</created><authors><author><keyname>Shan</keyname><forenames>Guihua</forenames></author><author><keyname>Xie</keyname><forenames>Maojin</forenames></author><author><keyname>Li</keyname><forenames>FengAn</forenames></author><author><keyname>Gao</keyname><forenames>Yang</forenames></author><author><keyname>Chi</keyname><forenames>Xuebin</forenames></author></authors><title>Interactive Visual Exploration of Halos in Large Scale Cosmology
  Simulation</title><categories>cs.GR</categories><comments>9pages, 14figures</comments><journal-ref>J. Visualization 17(3):145-156(2014)</journal-ref><doi>10.1007/s12650-014-0206-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Halo is one of the most important basic elements in cosmology simulation,
which merges from small clumps to ever larger objects. The processes of the
birth and merging of the halos play a fundamental role in studying the
evolution of large scale cosmological structures. In this paper, a visual
analysis system is developed to interactively identify and explore the
evolution histories of thousands of halos. In this system, an intelligent
structure-aware selection method in What You See Is What You Get manner is
designed to efficiently define the interesting region in 3D space with 2D
hand-drawn lasso input. Then the exact information of halos within this 3D
region is identified by data mining in the merger tree files. To avoid visual
clutter, all the halos are projected in 2D space with a MDS method. Through the
linked view of 3D View and 2D graph, Users can interactively explore these
halos, including the tracing path and evolution history tree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7782</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7782</id><created>2014-12-24</created><authors><author><keyname>Jiffriya</keyname><forenames>MAC</forenames></author><author><keyname>Jahan</keyname><forenames>MAC Akmal</forenames></author><author><keyname>Ragel</keyname><forenames>Roshan G.</forenames></author></authors><title>Plagiarism Detection on Electronic Text based Assignments using Vector
  Space Model (ICIAfS14)</title><categories>cs.IR cs.CL</categories><comments>appears in The 7th International Conference on Information and
  Automation for Sustainability (ICIAfS) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Plagiarism is known as illegal use of others' part of work or whole work as
one's own in any field such as art, poetry, literature, cinema, research and
other creative forms of study. Plagiarism is one of the important issues in
academic and research fields and giving more concern in academic systems. The
situation is even worse with the availability of ample resources on the web.
This paper focuses on an effective plagiarism detection tool on identifying
suitable intra-corpal plagiarism detection for text based assignments by
comparing unigram, bigram, trigram of vector space model with cosine similarity
measure. Manually evaluated, labelled dataset was tested using unigram, bigram
and trigram vector. Even though trigram vector consumes comparatively more
time, it shows better results with the labelled data. In addition, the selected
trigram vector space model with cosine similarity measure is compared with
tri-gram sequence matching technique with Jaccard measure. In the results,
cosine similarity score shows slightly higher values than the other. Because,
it focuses on giving more weight for terms that do not frequently exist in the
dataset and cosine similarity measure using trigram technique is more
preferable than the other. Therefore, we present our new tool and it could be
used as an effective tool to evaluate text based electronic assignments and
minimize the plagiarism among students.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7785</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7785</id><created>2014-12-24</created><authors><author><keyname>Du</keyname><forenames>Guanyao</forenames></author><author><keyname>Xiong</keyname><forenames>Ke</forenames></author><author><keyname>Zhang</keyname><forenames>Yu</forenames></author><author><keyname>Qiu</keyname><forenames>Zhengding</forenames></author></authors><title>Outage Analysis and Optimization for Time Switching-based Two-Way
  Relaying with Energy Harvesting Relay Node</title><categories>cs.IT math.IT</categories><comments>19 pages,7 figures,to appear in KSII Transactions on Internet and
  Information System</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Energy harvesting (EH) and network coding (NC) have emerged as two promising
technologies for future wireless networks. In this paper, we combine them
together in a single system and then present a time switching-based network
coding relaying (TSNCR) protocol for the two-way relay system, where an energy
constrained relay harvests energy from the transmitted radio frequency (RF)
signals from two sources, and then helps the two-way relay information exchange
between the two sources with the consumption of the harvested energy. To
evaluate the system performance, we derive an explicit expression of the outage
probability for the proposed TSNCR protocol. In order to explore the system
performance limit, we formulate an optimization problem to minimize the system
outage probability. Since the problem is non-convex and cannot be directly
solved, we design a genetic algorithm (GA)-based optimization algorithm for it.
Numerical results validate our theoretical analysis and show that in such an EH
two-way relay system, if NC is applied, the system outage probability can be
greatly decreased. Moreover, it is shown that the relay position greatly
affects the system performance of TSNCR, where relatively worse outage
performance is achieved when the relay is placed in the middle of the two
sources. This is the first time to observe such a phenomena in EH two-way relay
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7789</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7789</id><created>2014-12-25</created><authors><author><keyname>Thambawita</keyname><forenames>Vajira</forenames></author><author><keyname>Ragel</keyname><forenames>Roshan</forenames></author><author><keyname>Elkaduwe</keyname><forenames>Dhammika</forenames></author></authors><title>To Use or Not to Use: Graphics Processing Units for Pattern Matching
  Algorithms</title><categories>cs.DC cs.PF</categories><comments>appears in The 7th International Conference on Information and
  Automation for Sustainability (ICIAfS) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  String matching is an important part in today's computer applications and
Aho-Corasick algorithm is one of the main string matching algorithms used to
accomplish this. This paper discusses that when can the GPUs be used for string
matching applications using the Aho-Corasick algorithm as a benchmark. We have
to identify the best unit to run our string matching algorithm according to the
performance of our devices and the applications. Sometimes CPU gives better
performance than GPU and sometimes GPU gives better performance than CPU.
Therefore, identifying this critical point is significant task for researchers
who are using GPUs to improve the performance of their string matching
applications based on string matching algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7796</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7796</id><created>2014-12-25</created><authors><author><keyname>Xiong</keyname><forenames>Ke</forenames></author><author><keyname>Fan</keyname><forenames>Pingyi</forenames></author><author><keyname>Letaief</keyname><forenames>Khaled Ben</forenames></author></authors><title>Time-Switching Based SWIPT for Network-Coded Two-Way Relay Transmission
  with Data Rate Fairness</title><categories>cs.IT math.IT</categories><comments>5 pages,4 figures, submitted to ICASSP2015</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper investigates the simultaneous wireless power and information
transfer (SWPIT) for network-coded two-way re- lay transmission from an
information theoretical viewpoint, where two sources exchange information via
an energy har- vesting relay. By considering the time switching (TS) relay
receiver architecture, we present the TS-based two-way re- laying (TS-TWR)
protocol. In order to explore the system throughput limit with data rate
fairness, we formulate an op- timization problem under total power constraint.
To solve the problem, we first derive some explicit results and then de- sign
an efficient algorithm. Numerical results show that with the same total
available power, TS-TWR has a certain per- formance loss compared with
conventional non-EH two-way relaying due to the path loss effect on energy
transfer, where in relatively low and relatively high SNR regimes, the perfor-
mance losses are relatively small.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7811</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7811</id><created>2014-12-25</created><authors><author><keyname>Vidanagamachchi</keyname><forenames>S. M.</forenames></author><author><keyname>Dewasurendra</keyname><forenames>S. D.</forenames></author><author><keyname>Ragel</keyname><forenames>R. G.</forenames></author><author><keyname>Niranjan</keyname><forenames>M.</forenames></author></authors><title>A Structured Hardware Software Architecture for Peptide Based Diagnosis
  of Baylisascaris Procyonis Infection (ICIAfS14)</title><categories>cs.CE</categories><comments>appears in The 7th International Conference on Information and
  Automation for Sustainability (ICIAfS) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of inferring proteins from complex peptide cocktails (digestion
products of biological samples) in shotgun proteomic workflow sets extreme
demands on computational resources in respect of the required very high
processing throughputs, rapid processing rates and reliability of results. This
is exacerbated by the fact that, in general, a given protein cannot be defined
by a fixed sequence of amino acids due to the existence of splice variants and
isoforms of that protein. Therefore, the problem of protein inference could be
considered as one of identifying sequences of amino acids with some limited
tolerance. In the current paper a model-based hardware acceleration of a
structured and practical inference approach is developed and validated on a
mass spectrometry experiment of realistic size. We have achieved 10 times
maximum speed-up in the co-designed workflow compared to a similar
software-only workflow run on the processor used for co-design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7813</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7813</id><created>2014-12-25</created><authors><author><keyname>Ratnapala</keyname><forenames>I. P.</forenames></author><author><keyname>Ragel</keyname><forenames>R. G.</forenames></author><author><keyname>Deegalla</keyname><forenames>S.</forenames></author></authors><title>Students Behavioural Analysis in an Online Learning Environment Using
  Data Mining (ICIAfS)</title><categories>cs.CY</categories><comments>appears in The 7th International Conference on Information and
  Automation for Sustainability (ICIAfS) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The focus of this research was to use Educational Data Mining (EDM)
techniques to conduct a quantitative analysis of students interaction with an
e-learning system through instructor-led non-graded and graded courses. This
exercise is useful for establishing a guideline for a series of online short
courses for them. A group of 412 students' access behaviour in an e-learning
system were analysed and they were grouped into clusters using K-Means
clustering method according to their course access log records. The results
explained that more than 40% from the student group are passive online learners
in both graded and non-graded learning environments. The result showed that the
difference in the learning environments could change the online access
behaviour of a student group. Clustering divided the student population into
five access groups based on their course access behaviour. Among these groups,
the least access group (NG-41% and G-42%) and the highest access group (NG-9%
and G-5%) could be identified very clearly due to their access variation from
the rest of the groups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7818</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7818</id><created>2014-12-25</created><authors><author><keyname>Jadav</keyname><forenames>Sunil</forenames></author><author><keyname>Vashistah</keyname><forenames>Munish</forenames></author><author><keyname>Chandel</keyname><forenames>Rajeevan</forenames></author></authors><title>Carbon Nanotube Based Delay Model For High Speed Energy Efficient on
  Chip Data Transmission Using: Current Mode Technique</title><categories>cs.ET</categories><comments>12 Figures, appears in Electrical and Electronics Engineering: An
  International Journal, November 2014</comments><doi>10.14810/elelij.2014.3404</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Speed is a major concern for high density VLSI networks. In this paper the
closed form delay model for current mode signalling in VLSI interconnects has
been proposed with resistive load termination. RLC interconnect line is
modelled using characteristic impedance of transmission line and inductive
effect. The inductive effect is dominant at lower technology node is modelled
into an equivalent resistance. In this model first order transfer function is
designed using finite difference equation, and by applying the boundary
conditions at the source and load termination. It has been observed that the
dominant pole determines system response and delay in the proposed model. Using
CNIA tool (carbon nanotube interconnect analyzer) the interconnect line
parameters has been estimated at 45nm technology node. The novel proposed
current mode model superiority has been validated for CNT type of material. It
superiority factor remains to 66.66% as compared to voltage mode signalling.
And current mode dissipates 0.015pJ energy where as VM consume 0.045pJ for a
single bit transmission across the interconnect over CNT material. Secondly the
damping factor of a lumped RLC circuit is shown to be a useful figure of merit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7824</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7824</id><created>2014-12-25</created><authors><author><keyname>Sarkar</keyname><forenames>Soumic</forenames></author><author><keyname>Kar</keyname><forenames>Indra Narayan</forenames></author></authors><title>Multi Time Scale Behaviour of The Formation of Multiple Groups of
  Nonholonomic Wheeled Mobile Robots</title><categories>cs.SY cs.MA cs.RO</categories><comments>arXiv admin note: text overlap with arXiv:1412.6164</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Different geometric patterns and shapes are generated using groups of agents,
and this needs formation control. In this paper, Centroid Based Transformation
(CBT), has been applied to decompose the combined dynamics of nonholonomic
Wheeled Mobile Robots (WMRs) into three subsystems: intra and inter group shape
dynamics, and the dynamics of the centroid. The intra group shape dynamics can
further be partitioned into the shape dynamics of each group, giving the notion
of multiple group. Thus separate controllers have been designed for each
subsystem. The gains of the controllers are such chosen that the overall system
becomes singularly perturbed system, and different subsystems converge to their
desired values at different times. Then multi-time scale convergence analysis
has been carried out in this paper. Negative gradient of a potential based
function has been added to the controller to ensure collision avoidance among
the robots. Simulation results have been provided to demonstrate the
effectiveness of the proposed controller.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7828</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7828</id><created>2014-12-25</created><updated>2015-01-04</updated><authors><author><keyname>S&#xf8;nderby</keyname><forenames>S&#xf8;ren Kaae</forenames></author><author><keyname>Winther</keyname><forenames>Ole</forenames></author></authors><title>Protein Secondary Structure Prediction with Long Short Term Memory
  Networks</title><categories>q-bio.QM cs.LG cs.NE</categories><comments>v2: adds larger network with slightly better results, update author
  affiliations</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Prediction of protein secondary structure from the amino acid sequence is a
classical bioinformatics problem. Common methods use feed forward neural
networks or SVMs combined with a sliding window, as these models does not
naturally handle sequential data. Recurrent neural networks are an
generalization of the feed forward neural network that naturally handle
sequential data. We use a bidirectional recurrent neural network with long
short term memory cells for prediction of secondary structure and evaluate
using the CB513 dataset. On the secondary structure 8-class problem we report
better performance (0.674) than state of the art (0.664). Our model includes
feed forward networks between the long short term memory cells, a path that can
be further explored.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7834</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7834</id><created>2014-12-25</created><updated>2015-03-30</updated><authors><author><keyname>Karapetyan</keyname><forenames>Daniel</forenames></author><author><keyname>Gagarin</keyname><forenames>Andrei</forenames></author><author><keyname>Gutin</keyname><forenames>Gregory</forenames></author></authors><title>Pattern Backtracking Algorithm for the Workflow Satisfiability Problem</title><categories>cs.DS</categories><comments>9th International Frontiers of Algorithmics Workshop (FAW 2015), 3-5
  July 2015, Guilin, Guangxi, China</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The workflow satisfiability problem (WSP) asks whether there exists an
assignment of authorised users to the steps in a workflow specification,
subject to certain constraints on the assignment. (Such an assignment is called
valid.) The problem is NP-hard even when restricted to the large class of
user-independent constraints. Since the number of steps $k$ is relatively small
in practice, it is natural to consider a parametrisation of the WSP by $k$. We
propose a new fixed-parameter algorithm to solve the WSP with user-independent
constraints. The assignments in our method are partitioned into equivalence
classes such that the number of classes is exponential in $k$ only. We show
that one can decide, in polynomial time, whether there is a valid assignment in
an equivalence class. By exploiting this property, our algorithm reduces the
search space to the space of equivalence classes, which it browses within a
backtracking framework, hence emerging as an efficient yet relatively
simple-to-implement or generalise solution method. We empirically evaluate our
algorithm against the state-of-the-art methods and show that it clearly wins
the competition on the whole range of our test problems and significantly
extends the domain of practically solvable instances of the WSP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7839</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7839</id><created>2014-12-25</created><updated>2015-08-17</updated><authors><author><keyname>Raja</keyname><forenames>Haroon</forenames></author><author><keyname>Bajwa</keyname><forenames>Waheed U.</forenames></author></authors><title>Cloud K-SVD: A Collaborative Dictionary Learning Algorithm for Big,
  Distributed Data</title><categories>cs.LG cs.IT math.IT stat.ML</categories><comments>Accepted for Publication in IEEE Trans. Signal Processing (2015); 16
  pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the problem of data-adaptive representations for big,
distributed data. It is assumed that a number of geographically-distributed,
interconnected sites have massive local data and they are interested in
collaboratively learning a low-dimensional geometric structure underlying these
data. In contrast to previous works on subspace-based data representations,
this paper focuses on the geometric structure of a union of subspaces (UoS). In
this regard, it proposes a distributed algorithm---termed cloud K-SVD---for
collaborative learning of a UoS structure underlying distributed data of
interest. The goal of cloud K-SVD is to learn a common overcomplete dictionary
at each individual site such that every sample in the distributed data can be
represented through a small number of atoms of the learned dictionary. Cloud
K-SVD accomplishes this goal without requiring exchange of individual samples
between sites. This makes it suitable for applications where sharing of raw
data is discouraged due to either privacy concerns or large volumes of data.
This paper also provides an analysis of cloud K-SVD that gives insights into
its properties as well as deviations of the dictionaries learned at individual
sites from a centralized solution in terms of different measures of
local/global data and topology of interconnections. Finally, the paper
numerically illustrates the efficacy of cloud K-SVD on real and synthetic
distributed data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7840</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7840</id><created>2014-12-25</created><authors><author><keyname>Ikeda</keyname><forenames>Takuya</forenames></author><author><keyname>Nagahara</keyname><forenames>Masaaki</forenames></author></authors><title>Value Function in Maximum Hands-off Control</title><categories>cs.SY math.OC</categories><comments>submitted to Automatica, Dec. 2014; 6 pages with 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this brief paper, we study the value function in maximum hands-off
control. Maximum hands-off control, also known as sparse control, is the
L0-optimal control among the admissible controls. Although the L0 measure is
discontinuous and non- convex, we prove that the value function, or the minimum
L0 norm of the control, is a continuous and strictly convex function of the
initial state in the reachable set, under an assumption on the controlled plant
model. This property is important, in particular, for discussing the
sensitivity of the optimality against uncertainties in the initial state, and
also for investigating the stability by using the value function as a Lyapunov
function in model predictive control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7842</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7842</id><created>2014-12-25</created><authors><author><keyname>Mertikopoulos</keyname><forenames>Panayotis</forenames></author><author><keyname>Viossat</keyname><forenames>Yannick</forenames></author></authors><title>Imitation Dynamics with Payoff Shocks</title><categories>math.PR cs.GT math.DS</categories><comments>25 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the impact of payoff shocks on the evolution of large
populations of myopic players that employ simple strategy revision protocols
such as the &quot;imitation of success&quot;. In the noiseless case, this process is
governed by the standard (deterministic) replicator dynamics; in the presence
of noise however, the induced stochastic dynamics are different from previous
versions of the stochastic replicator dynamics (such as the aggregate-shocks
model of Fudenberg and Harris, 1992). In this context, we show that strict
equilibria are always stochastically asymptotically stable, irrespective of the
magnitude of the shocks; on the other hand, in the high-noise regime,
non-equilibrium states may also become stochastically asymptotically stable and
dominated strategies may survive in perpetuity (they become extinct if the
noise is low). Such behavior is eliminated if players are less myopic and
revise their strategies based on their cumulative payoffs. In this case, we
obtain a second order stochastic dynamical system whose attracting states
coincide with the game's strict equilibria and where dominated strategies
become extinct (a.s.), no matter the noise level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7844</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7844</id><created>2014-12-25</created><authors><author><keyname>Backes</keyname><forenames>Andr&#xe9; R.</forenames></author><author><keyname>Bruno</keyname><forenames>Odemir M.</forenames></author></authors><title>Texture analysis using volume-radius fractal dimension</title><categories>cs.CV</categories><comments>4 pages, 4 figures</comments><journal-ref>Backes, A. R and Bruno, O. M. Texture analysis using volume-radius
  fractal dimension, Applied Mathematics and Computation, Volume 219, Issue 11,
  Pages 5870 - 5875, 2013</journal-ref><doi>10.1016/j.amc.2012.11.092</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Texture plays an important role in computer vision. It is one of the most
important visual attributes used in image analysis, once it provides
information about pixel organization at different regions of the image. This
paper presents a novel approach for texture characterization, based on
complexity analysis. The proposed approach expands the idea of the Mass-radius
fractal dimension, a method originally developed for shape analysis, to a set
of coordinates in 3D-space that represents the texture under analysis in a
signature able to characterize efficiently different texture classes in terms
of complexity. An experiment using images from the Brodatz album illustrates
the method performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7849</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7849</id><created>2014-12-25</created><authors><author><keyname>Florindo</keyname><forenames>Jo&#xe3;o Batista</forenames></author><author><keyname>da Silva</keyname><forenames>N&#xfa;bia Rosa</forenames></author><author><keyname>Romualdo</keyname><forenames>Liliane Maria</forenames></author><author><keyname>da Silva</keyname><forenames>Fernanda de F&#xe1;tima</forenames></author><author><keyname>Luz</keyname><forenames>Pedro Henrique de Cerqueira</forenames></author><author><keyname>Herling</keyname><forenames>Valdo Rodrigues</forenames></author><author><keyname>Bruno</keyname><forenames>Odemir Martinez</forenames></author></authors><title>Brachiaria species identification using imaging techniques based on
  fractal descriptors</title><categories>cs.CV</categories><comments>7 pages, 5 figures</comments><journal-ref>Computers and Electronics in Agriculture, V 103, Pages 48-54, 2014</journal-ref><doi>10.1016/j.compag.2014.02.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of a rapid and accurate method in diagnosis and classification of
species and/or cultivars of forage has practical relevance, scientific and
trade in various areas of study. Thus, leaf samples of fodder plant species
\textit{Brachiaria} were previously identified, collected and scanned to be
treated by means of artificial vision to make the database and be used in
subsequent classifications. Forage crops used were: \textit{Brachiaria
decumbens} cv. IPEAN; \textit{Brachiaria ruziziensis} Germain \&amp; Evrard;
\textit{Brachiaria Brizantha} (Hochst. ex. A. Rich.) Stapf; \textit{Brachiaria
arrecta} (Hack.) Stent. and \textit{Brachiaria spp}. The images were analyzed
by the fractal descriptors method, where a set of measures are obtained from
the values of the fractal dimension at different scales. Therefore such values
are used as inputs for a state-of-the-art classifier, the Support Vector
Machine, which finally discriminates the images according to the respective
species.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7851</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7851</id><created>2014-12-25</created><authors><author><keyname>Florindo</keyname><forenames>Jo&#xe3;o Batista</forenames></author><author><keyname>Bruno</keyname><forenames>Odemir Martinez</forenames></author></authors><title>Fractal descriptors based on the probability dimension: a texture
  analysis and classification approach</title><categories>cs.CV</categories><comments>7 pages, 6 figures. arXiv admin note: text overlap with
  arXiv:1205.2821</comments><journal-ref>Pattern Recognition Letters, Volume 42, Pages 107-114, 2014</journal-ref><doi>10.1016/j.patrec.2014.01.009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose a novel technique for obtaining descriptors of
gray-level texture images. The descriptors are provided by applying a
multiscale transform to the fractal dimension of the image estimated through
the probability (Voss) method. The effectiveness of the descriptors is verified
in a classification task using benchmark over texture datasets. The results
obtained demonstrate the efficiency of the proposed method as a tool for the
description and discrimination of texture images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7854</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7854</id><created>2014-12-25</created><authors><author><keyname>Feyzabadi</keyname><forenames>Seyedshams</forenames></author></authors><title>Joint Deep Learning for Car Detection</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Traditional object recognition approaches apply feature extraction, part
deformation handling, occlusion handling and classification sequentially while
they are independent from each other. Ouyang and Wang proposed a model for
jointly learning of all of the mentioned processes using one deep neural
network. We utilized, and manipulated their toolbox in order to apply it in car
detection scenarios where it had not been tested. Creating a single deep
architecture from these components, improves the interaction between them and
can enhance the performance of the whole system. We believe that the approach
can be used as a general purpose object detection toolbox. We tested the
algorithm on UIUC car dataset, and achieved a reasonable result. The accuracy
of our method was 86 % while there are better results of accuracy with up to 91
% and will be shown later. We strongly believe that having an experiment on a
larger dataset can show the advantage of using deep models over shallow ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7856</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7856</id><created>2014-12-25</created><authors><author><keyname>Z.</keyname><forenames>&#xc1;lvaro Gomez</forenames></author><author><keyname>Florindo</keyname><forenames>Jo&#xe3;o B.</forenames></author><author><keyname>Bruno</keyname><forenames>Odemir M.</forenames></author></authors><title>Gabor wavelets combined with volumetric fractal dimension applied to
  texture analysis</title><categories>cs.CV</categories><comments>11 pages, 2 figures</comments><journal-ref>Pattern Recognition Letters, V. 36, Pages 135-143, 2014</journal-ref><doi>10.1016/j.patrec.2013.09.023</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Texture analysis and classification remain as one of the biggest challenges
for the field of computer vision and pattern recognition. On this matter, Gabor
wavelets has proven to be a useful technique to characterize distinctive
texture patterns. However, most of the approaches used to extract descriptors
of the Gabor magnitude space usually fail in representing adequately the
richness of detail present into a unique feature vector. In this paper, we
propose a new method to enhance the Gabor wavelets process extracting a fractal
signature of the magnitude spaces. Each signature is reduced using a canonical
analysis function and concatenated to form the final feature vector.
Experiments were conducted on several texture image databases to prove the
power and effectiveness of the proposed method. Results obtained shown that
this method outperforms other early proposed method, creating a more reliable
technique for texture feature extraction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7858</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7858</id><created>2014-12-25</created><authors><author><keyname>Tucker</keyname><forenames>Christopher A.</forenames></author></authors><title>Models of robotic feeding, choice, and the survival mechanism</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Diagrammatic models of feeding choices reveal fundamental robotic behaviors.
Successful choices are reinforced by positive feedback, while unsuccessful ones
by negative feedback. This paper will address robotic feeding by casually
relating consequential behavior subtended by a strong dependence upon survival.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7860</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7860</id><created>2014-12-25</created><authors><author><keyname>Tucker</keyname><forenames>Christopher A.</forenames></author></authors><title>A self-organizing geometric algorithm for autonomous data partitioning</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A model of a geometric algorithm is introduced and methodology of its
operation is presented for the dynamic partitioning of data spaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7868</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7868</id><created>2014-12-25</created><authors><author><keyname>Srijith</keyname><forenames>P. K.</forenames></author><author><keyname>Balamurugan</keyname><forenames>P.</forenames></author><author><keyname>Shevade</keyname><forenames>Shirish</forenames></author></authors><title>Gaussian Process Pseudo-Likelihood Models for Sequence Labeling</title><categories>cs.LG stat.ML</categories><comments>18 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several machine learning problems arising in natural language processing can
be modeled as a sequence labeling problem. We provide Gaussian process models
based on pseudo-likelihood approximation to perform sequence labeling. Gaussian
processes (GPs) provide a Bayesian approach to learning in a kernel based
framework. The pseudo-likelihood model enables one to capture long range
dependencies among the output components of the sequence without becoming
computationally intractable. We use an efficient variational Gaussian
approximation method to perform inference in the proposed model. We also
provide an iterative algorithm which can effectively make use of the
information from the neighboring labels to perform prediction. The ability to
capture long range dependencies makes the proposed approach useful for a wide
range of sequence labeling problems. Numerical experiments on some sequence
labeling data sets demonstrate the usefulness of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7880</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7880</id><created>2014-12-25</created><authors><author><keyname>Oliveira</keyname><forenames>Marcos W. S.</forenames></author><author><keyname>Casanova</keyname><forenames>Dalcimar</forenames></author><author><keyname>Florindo</keyname><forenames>Jo&#xe3;o B.</forenames></author><author><keyname>Bruno</keyname><forenames>Odemir Martinez</forenames></author></authors><title>Enhancing fractal descriptors on images by combining boundary and
  interior of Minkowski dilation</title><categories>physics.data-an cs.CV</categories><comments>6 pages 3 figures</comments><journal-ref>Physica A, Volume 416, Pages 41-48, 2014</journal-ref><doi>10.1016/j.physa.2014.07.074</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work proposes to obtain novel fractal descriptors from gray-level
texture images by combining information from interior and boundary measures of
the Minkowski dilation applied to the texture surface. At first, the image is
converted into a surface where the height of each point is the gray intensity
of the respective pixel in that position in the image. Thus, this surface is
morphologically dilated by spheres. The radius of such spheres is ranged within
an interval and the volume and the external area of the dilated structure are
computed for each radius. The final descriptors are given by such measures
concatenated and subject to a canonical transform to reduce the dimensionality.
The proposal is an enhancement to the classical Bouligand-Minkowski fractal
descriptors, where only the volume (interior) information is considered. As
different structures may have the same volume, but not the same area, the
proposal yields to more rich descriptors as confirmed by results on the
classification of benchmark databases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7884</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7884</id><created>2014-12-25</created><authors><author><keyname>Zhang</keyname><forenames>Zhengdong</forenames></author><author><keyname>Isola</keyname><forenames>Phillip</forenames></author><author><keyname>Adelson</keyname><forenames>Edward H.</forenames></author></authors><title>Sparkle Vision: Seeing the World through Random Specular Microfacets</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the problem of reproducing the world lighting from a
single image of an object covered with random specular microfacets on the
surface. We show that such reflectors can be interpreted as a randomized
mapping from the lighting to the image. Such specular objects have very
different optical properties from both diffuse surfaces and smooth specular
objects like metals, so we design special imaging system to robustly and
effectively photograph them. We present simple yet reliable algorithms to
calibrate the proposed system and do the inference. We conduct experiments to
verify the correctness of our model assumptions and prove the effectiveness of
our pipeline.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7888</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7888</id><created>2014-12-25</created><authors><author><keyname>Liao</keyname><forenames>Chenda</forenames></author><author><keyname>Barooah</keyname><forenames>Prabir</forenames></author></authors><title>Accurate Distributed Time Synchronization in Mobile Wireless Sensor
  Networks from Noisy Difference Measurements</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a distributed algorithm for time synchronization in mobile
wireless sensor networks. Each node can employ the algorithm to estimate the
global time based on its local clock time. The problem of time synchronization
is formulated as nodes estimating their skews and offsets from noisy difference
measurements of offsets and logarithm of skews; the measurements acquired by
time-stamped message exchanges between neighbors. A distributed stochastic
approximation based algorithm is proposed to ensure that the estimation error
is mean square convergent (variance converging to 0) under certain conditions.
A sequence of scheduled update instants is used to meet the requirement of
decreasing time-varying gains that need to be synchronized across nodes with
unsynchronized clocks. Moreover, a modification on the algorithm is also
presented to improve the initial convergence speed. Simulations indicate that
highly accurate global time estimates can be achieved with the proposed
algorithm for long time durations, while the errors in competing algorithms
increase over time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7889</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7889</id><created>2014-12-25</created><authors><author><keyname>da Silva</keyname><forenames>N&#xfa;bia Rosa</forenames></author><author><keyname>Van der Wee&#xeb;n</keyname><forenames>Pieter</forenames></author><author><keyname>De Baets</keyname><forenames>Bernard</forenames></author><author><keyname>Bruno</keyname><forenames>Odemir Martinez</forenames></author></authors><title>Improved texture image classification through the use of a
  corrosion-inspired cellular automaton</title><categories>cs.CV</categories><comments>13 pages, 14 figures</comments><journal-ref>Neurocomputing, 149, Part C, pp 1560-1572, 2015</journal-ref><doi>10.1016/j.neucom.2014.08.036</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the problem of classifying synthetic and natural texture
images is addressed. To tackle this problem, an innovative method is proposed
that combines concepts from corrosion modeling and cellular automata to
generate a texture descriptor. The core processes of metal (pitting) corrosion
are identified and applied to texture images by incorporating the basic
mechanisms of corrosion in the transition function of the cellular automaton.
The surface morphology of the image is analyzed before and during the
application of the transition function of the cellular automaton. In each
iteration the cumulative mass of corroded product is obtained to construct each
of the attributes of the texture descriptor. In a final step, this texture
descriptor is used for image classification by applying Linear Discriminant
Analysis. The method was tested on the well-known Brodatz and Vistex databases.
In addition, in order to verify the robustness of the method, its invariance to
noise and rotation were tested. To that end, different variants of the original
two databases were obtained through addition of noise to and rotation of the
images. The results showed that the method is effective for texture
classification according to the high success rates obtained in all cases. This
indicates the potential of employing methods inspired on natural phenomena in
other fields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7890</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7890</id><created>2014-12-25</created><updated>2015-09-03</updated><authors><author><keyname>Bahmani</keyname><forenames>Sohail</forenames></author><author><keyname>Romberg</keyname><forenames>Justin</forenames></author></authors><title>Compressive Deconvolution in Random Mask Imaging</title><categories>cs.IT math.FA math.IT math.NA math.OC math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of reconstructing signals from a subsampled
convolution of their modulated versions and a known filter. The problem is
studied as applies to specific imaging systems relying on spatial phase
modulation by randomly coded &quot;masks.&quot; The diversity induced by the random masks
is deemed to improve the conditioning of the deconvolution problem while
maintaining sampling efficiency.
  We analyze a linear model of the system, where the joint effect of the
spatial modulation, blurring, and spatial subsampling is represented by a
measurement matrix. We provide a bound on the conditioning of this measurement
matrix in terms of the number of masks, the dimension of the image, and certain
characteristics of the blurring kernel and subsampling operator. The derived
bound shows that stable deconvolution is possible with high probability even if
the total number of (scalar) measurements is within a logarithmic factor of the
image size. Furthermore, beyond a critical number of masks determined by the
extent of blurring and subsampling, every additional mask improves the
conditioning of the measurement matrix.
  We also consider a more interesting scenario where the target image is
sparse. We show that under mild conditions on the blurring kernel, with high
probability the measurement matrix is a restricted isometry when the number of
masks is within a logarithmic factor of the sparsity of the image. Therefore,
the image can be reconstructed using many sparse recovery algorithms such as
the basis pursuit. The bound on the required number of masks is linear in
sparsity of the image but it is logarithmic in its dimension. The bound
provides a quantitative view of the effect of the blurring and subsampling on
the required number of masks, which is critical for designing efficient imaging
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7911</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7911</id><created>2014-12-26</created><authors><author><keyname>Xu</keyname><forenames>Jiuqiang</forenames></author><author><keyname>Wang</keyname><forenames>Jinfa</forenames></author><author><keyname>Zhao</keyname><forenames>Hai</forenames></author><author><keyname>Jia</keyname><forenames>Siyuan</forenames></author></authors><title>Improving controllability of complex networks by rewiring links
  regularly</title><categories>cs.SY cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network science have constantly been in the focus of research for the last
decade, with considerable advances in the controllability of their structural.
However, much less effort has been devoted to study that how to improve the
controllability of complex networks. In this paper, a new algorithm is proposed
to improve the controllability of complex networks by rewiring links regularly
which transforms the network structure. Then it is demonstrated that our
algorithm is very effective after numerical simulation experiment on typical
network models (Erd\&quot;os-R\'enyi and scale-free network). We find that our
algorithm is mainly determined by the average degree and positive correlation
of in-degree and out-degree of network and it has nothing to do with the
network size. Furthermore, we analyze and discuss the correlation between
controllability of complex networks and degree distribution index: power-law
exponent and heterogeneity
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7912</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7912</id><created>2014-12-26</created><authors><author><keyname>Minear</keyname><forenames>Lily</forenames></author><author><keyname>Zhang</keyname><forenames>Eric</forenames></author></authors><title>Impact of Energy Consumption on Multipath TCP Enabled Mobiles</title><categories>cs.NI</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple accesses are common for most mobile devices today. This
technological advance opens up a new design space for improving the
communication performance of mobile devices. Multipath TCP is a TCP extension
that enables using multiple network paths between two end systems for a single
TCP connection, increasing performance and reliability. Meanwhile, when
operating multiple active interfaces, multipath-TCP also consumes substantial
more power and drains out bettery faster than using one interface. Thus,
enabling Multipath TCP on mobile devices brings in new challenges. In this
paper, we theoretically analyze the underlying design choices given by the
Multipath TCP. In particular, we theoretically formulate the rela- tion between
performance (throughput) and energy consumption for Multipath TCP. We find that
sometime the throughput and energy consumption can be concurrently improved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7922</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7922</id><created>2014-12-26</created><authors><author><keyname>Lenzen</keyname><forenames>Christoph</forenames></author><author><keyname>Patt-Shamir</keyname><forenames>Boaz</forenames></author></authors><title>Fast Partial Distance Estimation and Applications</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study approximate distributed solutions to the weighted {\it
all-pairs-shortest-paths} (APSP) problem in the CONGEST model. We obtain the
following results.
  $1.$ A deterministic $(1+o(1))$-approximation to APSP in $\tilde{O}(n)$
rounds. This improves over the best previously known algorithm, by both
derandomizing it and by reducing the running time by a $\Theta(\log n)$ factor.
  In many cases, routing schemes involve relabeling, i.e., assigning new names
to nodes and require that these names are used in distance and routing queries.
It is known that relabeling is necessary to achieve running times of $o(n/\log
n)$. In the relabeling model, we obtain the following results.
  $2.$ A randomized $O(k)$-approximation to APSP, for any integer $k&gt;1$,
running in $\tilde{O}(n^{1/2+1/k}+D)$ rounds, where $D$ is the hop diameter of
the network. This algorithm simplifies the best previously known result and
reduces its approximation ratio from $O(k\log k)$ to $O(k)$. Also, the new
algorithm uses uses labels of asymptotically optimal size, namely $O(\log n)$
bits.
  $3.$ A randomized $O(k)$-approximation to APSP, for any integer $k&gt;1$,
running in time $\tilde{O}((nD)^{1/2}\cdot n^{1/k}+D)$ and producing {\it
compact routing tables} of size $\tilde{O}(n^{1/k})$. The node lables consist
of $O(k\log n)$ bits. This improves on the approximation ratio of $\Theta(k^2)$
for tables of that size achieved by the best previously known algorithm, which
terminates faster, in $\tilde{O}(n^{1/2+1/k}+D)$ rounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7927</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7927</id><created>2014-12-26</created><authors><author><keyname>Goel</keyname><forenames>Kratarth</forenames></author><author><keyname>Vohra</keyname><forenames>Raunaq</forenames></author><author><keyname>Sahoo</keyname><forenames>J. K.</forenames></author></authors><title>Polyphonic Music Generation by Modeling Temporal Dependencies Using a
  RNN-DBN</title><categories>cs.LG cs.AI cs.NE</categories><comments>8 pages, A4, 1 figure, 1 table, ICANN 2014 oral presentation. arXiv
  admin note: text overlap with arXiv:1206.6392 by other authors</comments><journal-ref>Lecture Notes in Computer Science Volume 8681, 2014, pp 217-224</journal-ref><doi>10.1007/978-3-319-11179-7_28</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a generic technique to model temporal dependencies
and sequences using a combination of a recurrent neural network and a Deep
Belief Network. Our technique, RNN-DBN, is an amalgamation of the memory state
of the RNN that allows it to provide temporal information and a multi-layer DBN
that helps in high level representation of the data. This makes RNN-DBNs ideal
for sequence generation. Further, the use of a DBN in conjunction with the RNN
makes this model capable of significantly more complex data representation than
an RBM. We apply this technique to the task of polyphonic music generation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7929</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7929</id><created>2014-12-26</created><updated>2014-12-30</updated><authors><author><keyname>Kim</keyname><forenames>Kyeong Soo</forenames></author></authors><title>Designing pricing schemes based on progressive tariff and consumer
  grouping in migration to a future smart grid</title><categories>cs.CE</categories><comments>4 pages, 3 figures, to be presented at International Conference on
  Information and Convergence Technology for Smart Society (ICICTS) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the design of pricing schemes for a group of consumers with smart
meters (e.g., in a Greenfield area) who are connected through a gateway to a
traditional electricity greed with a progressive tariff. Because the
progressive tariff cannot take into account the time aspect of electricity
demands, we apply it to consumers in both an individual and a group basis over
a shorter time period, which can flatten the overall demand over time and
thereby reduce peak load. This scenario for the coexistence of traditional and
smart girds and the pricing schemes under this scenario can enable smooth
migration to a future smart grid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7932</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7932</id><created>2014-12-26</created><authors><author><keyname>Goel</keyname><forenames>Kratarth</forenames></author><author><keyname>Vohra</keyname><forenames>Raunaq</forenames></author><author><keyname>Kamath</keyname><forenames>Anant</forenames></author><author><keyname>Baths</keyname><forenames>Veeky</forenames></author></authors><title>Home Automation Using SSVEP &amp; Eye-Blink Detection Based Brain-Computer
  Interface</title><categories>cs.HC cs.SY</categories><comments>2 pages, 1 table, published at IEEE SMC 2014</comments><doi>10.1109/SMC.2014.6974563</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a novel brain computer interface based home
automation system using two responses - Steady State Visually Evoked Potential
(SSVEP) and the eye-blink artifact, which is augmented by a Bluetooth based
indoor localization system, to greatly increase the number of controllable
devices. The hardware implementation of this system to control a table lamp and
table fan using brain signals has also been discussed and state-of-the-art
results have been achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7934</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7934</id><created>2014-12-26</created><authors><author><keyname>Goel</keyname><forenames>Kratarth</forenames></author><author><keyname>Vohra</keyname><forenames>Raunaq</forenames></author><author><keyname>Bakshi</keyname><forenames>Ainesh</forenames></author></authors><title>A Novel Feature Selection and Extraction Technique for Classification</title><categories>cs.LG cs.CV</categories><comments>2 pages, 2 tables, published at IEEE SMC 2014</comments><journal-ref>IEEE Xplore, Proceedings of IEEE SMC 2014, pages 4033 - 4034</journal-ref><doi>10.1109/SMC.2014.6974562</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a versatile technique for the purpose of feature
selection and extraction - Class Dependent Features (CDFs). We use CDFs to
improve the accuracy of classification and at the same time control
computational expense by tackling the curse of dimensionality. In order to
demonstrate the generality of this technique, it is applied to handwritten
digit recognition and text categorization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7935</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7935</id><created>2014-12-26</created><authors><author><keyname>Decker</keyname><forenames>Christian</forenames></author><author><keyname>Seidel</keyname><forenames>Jochen</forenames></author><author><keyname>Wattenhofer</keyname><forenames>Roger</forenames></author></authors><title>Bitcoin Meets Strong Consistency</title><categories>cs.DC cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Bitcoin system only provides eventual consistency. For everyday life, the
time to confirm a Bitcoin transaction is prohibitively slow. In this paper we
propose a new system, built on the Bitcoin blockchain, which enables strong
consistency. Our system, PeerCensus, acts as a certification authority, manages
peer identities in a peer-to-peer network, and ultimately enhances Bitcoin and
similar systems with strong consistency. Our extensive analysis shows that
PeerCensus is in a secure state with high probability. We also show how
Discoin, a Bitcoin variant that decouples block creation and transaction
confirmation, can be built on top of PeerCensus, enabling real-time payments.
Unlike Bitcoin, once transactions in Discoin are committed, they stay
committed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7938</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7938</id><created>2014-12-26</created><updated>2015-02-10</updated><authors><author><keyname>Wang</keyname><forenames>Shusen</forenames></author><author><keyname>Zhang</keyname><forenames>Tong</forenames></author><author><keyname>Zhang</keyname><forenames>Zhihua</forenames></author></authors><title>Adjusting Leverage Scores by Row Weighting: A Practical Approach to
  Coherent Matrix Completion</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low-rank matrix completion is an important problem with extensive real-world
applications. When observations are uniformly sampled from the underlying
matrix entries, existing methods all require the matrix to be incoherent. This
paper provides the first working method for coherent matrix completion under
the standard uniform sampling model. Our approach is based on the weighted
nuclear norm minimization idea proposed in several recent work, and our key
contribution is a practical method to compute the weighting matrices so that
the leverage scores become more uniform after weighting. Under suitable
conditions, we are able to derive theoretical results, showing the
effectiveness of our approach. Experiments on synthetic data show that our
approach recovers highly coherent matrices with high precision, whereas the
standard unweighted method fails even on noise-free data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7949</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7949</id><created>2014-12-26</created><authors><author><keyname>Shatalov</keyname><forenames>Vladimir</forenames></author><author><keyname>Martynyuk</keyname><forenames>Victor</forenames></author><author><keyname>Saveliev</keyname><forenames>Maxim</forenames></author></authors><title>Through Global Monitoring to School of the Future: Smartphone as a
  Laboratory in Pocket of Each Student</title><categories>cs.CY</categories><comments>Report on the on-line conference &quot;Cloud Technologies in
  Education'2014 (December 26, 2014)&quot;
  http://tmn.ccjournals.eu/index.php/cte/CTE2014/paper/view/81</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The idea to unite smartphones used as personal environmental sensors and
health indicators into a scalable network for data collection and processing by
the internet-cloud is proposed. Access to the sensors, which are available in
every smartphone, will provide the appropriate software. Such a monitoring at
the global level would reveal the impact of the electromagnetic radiation,
environmental pollution and weather factors on human health. Participation of
students in these measurements increases their educational and social
activities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7955</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7955</id><created>2014-12-26</created><authors><author><keyname>Papadimitriou</keyname><forenames>Christos H.</forenames></author><author><keyname>Vempala</keyname><forenames>Santosh S.</forenames></author></authors><title>Unsupervised Learning through Prediction in a Model of Cortex</title><categories>cs.NE cs.DS q-bio.NC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a primitive called PJOIN, for &quot;predictive join,&quot; which combines
and extends the operations JOIN and LINK, which Valiant proposed as the basis
of a computational theory of cortex. We show that PJOIN can be implemented in
Valiant's model. We also show that, using PJOIN, certain reasonably complex
learning and pattern matching tasks can be performed, in a way that involves
phenomena which have been observed in cognition and the brain, namely
memory-based prediction and downward traffic in the cortical hierarchy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7957</identifier>
 <datestamp>2015-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7957</id><created>2014-12-26</created><authors><author><keyname>Karaoglu</keyname><forenames>Sezer</forenames></author><author><keyname>Liu</keyname><forenames>Yang</forenames></author><author><keyname>Gevers</keyname><forenames>Theo</forenames></author></authors><title>Detect2Rank : Combining Object Detectors Using Learning to Rank</title><categories>cs.CV</categories><doi>10.1109/TIP.2015.2499702</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Object detection is an important research area in the field of computer
vision. Many detection algorithms have been proposed. However, each object
detector relies on specific assumptions of the object appearance and imaging
conditions. As a consequence, no algorithm can be considered as universal. With
the large variety of object detectors, the subsequent question is how to select
and combine them.
  In this paper, we propose a framework to learn how to combine object
detectors. The proposed method uses (single) detectors like DPM, CN and EES,
and exploits their correlation by high level contextual features to yield a
combined detection list.
  Experiments on the PASCAL VOC07 and VOC10 datasets show that the proposed
method significantly outperforms single object detectors, DPM (8.4%), CN (6.8%)
and EES (17.0%) on VOC07 and DPM (6.5%), CN (5.5%) and EES (16.2%) on VOC10.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7959</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7959</id><created>2014-12-26</created><authors><author><keyname>Galinium</keyname><forenames>Maulahikmah</forenames></author><author><keyname>Shahbaz</keyname><forenames>Negar</forenames></author></authors><title>Case Studies: Business and Technical Perspectives in Migration of Legacy
  Systems to Service Oriented Architecture</title><categories>cs.CY cs.SE</categories><comments>11 pages, 4 figures</comments><journal-ref>ECTI Transactions on Computer and Information Technology, Vol 7,
  No, 2, November 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In adoption process of Service Oriented Architecture (SOA), the legacy
systems of a company can not be neglected. The reason is the legacy systems
have been deployed in the past and have been running critical business
processes within an enterprise in its current IT architecture. However not all
migration process of legacy systems to SOA has been successfull. Highlighting
the right factors to reach legacy systems migration success in a specific
company is the key value. The main adopted research method in this study has
been interviewed for different companies with different enterprises including
bank, furniture, engineering and airline companies in Europe. Through separate
interviews, critical success factors of migrating legacy systems into SOA have
been collected and identified in each case company. Finally collected results
are analyzed and presented as the recognized factors affecting successful
migration of legacy assets into SOA from business and technical perspectives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7961</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7961</id><created>2014-12-26</created><authors><author><keyname>Alirezaie</keyname><forenames>Marjan</forenames></author><author><keyname>Loutfi</keyname><forenames>Amy</forenames></author></authors><title>Reasoning for Improved Sensor Data Interpretation in a Smart Home</title><categories>cs.AI</categories><comments>ARCOE-Logic 2014 Workshop Notes, pp. 1-12</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper an ontological representation and reasoning paradigm has been
proposed for interpretation of time-series signals. The signals come from
sensors observing a smart environment. The signal chosen for the annotation
process is a set of unintuitive and complex gas sensor data. The ontology of
this paradigm is inspired form the SSN ontology (Semantic Sensor Network) and
used for representation of both the sensor data and the contextual information.
The interpretation process is mainly done by an incremental ASP solver which as
input receives a logic program that is generated from the contents of the
ontology. The contextual information together with high level domain knowledge
given in the ontology are used to infer explanations (answer sets) for changes
in the ambient air detected by the gas sensors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7963</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7963</id><created>2014-12-26</created><authors><author><keyname>Florindo</keyname><forenames>Jo&#xe3;o B.</forenames></author><author><keyname>Bruno</keyname><forenames>Odemir M.</forenames></author></authors><title>Texture analysis by multi-resolution fractal descriptors</title><categories>cs.CV</categories><comments>8 pages, 6 figures</comments><journal-ref>Expert Systems with Applications, Volume 40, Issue 10, Pages
  4022-4028, 2013</journal-ref><doi>10.1016/j.eswa.2013.01.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work proposes a texture descriptor based on fractal theory. The method
is based on the Bouligand-Minkowski descriptors. We decompose the original
image recursively into 4 equal parts. In each recursion step, we estimate the
average and the deviation of the Bouligand-Minkowski descriptors computed over
each part. Thus, we extract entropy features from both average and deviation.
The proposed descriptors are provided by the concatenation of such measures.
The method is tested in a classification experiment under well known datasets,
that is, Brodatz and Vistex. The results demonstrate that the proposed
technique achieves better results than classical and state-of-the-art texture
descriptors, such as Gabor-wavelets and co-occurrence matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7964</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7964</id><created>2014-12-26</created><authors><author><keyname>Bozzato</keyname><forenames>Loris</forenames></author><author><keyname>Serafini</keyname><forenames>Luciano</forenames></author></authors><title>Knowledge Propagation in Contextualized Knowledge Repositories: an
  Experimental Evaluation</title><categories>cs.AI</categories><comments>ARCOE-Logic 2014 Workshop Notes, pp. 13-24</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the interest in the representation of context dependent knowledge in the
Semantic Web has been recognized, a number of logic based solutions have been
proposed in this regard. In our recent works, in response to this need, we
presented the description logic-based Contextualized Knowledge Repository (CKR)
framework. CKR is not only a theoretical framework, but it has been effectively
implemented over state-of-the-art tools for the management of Semantic Web
data: inference inside and across contexts has been realized in the form of
forward SPARQL-based rules over different RDF named graphs. In this paper we
present the first evaluation results for such CKR implementation. In
particular, in first experiment we study its scalability with respect to
different reasoning regimes. In a second experiment we analyze the effects of
knowledge propagation on the computation of inferences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7965</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7965</id><created>2014-12-26</created><authors><author><keyname>Calvanese</keyname><forenames>Diego</forenames></author><author><keyname>Ceylan</keyname><forenames>&#x130;smail &#x130;lkan</forenames></author><author><keyname>Montali</keyname><forenames>Marco</forenames></author><author><keyname>Santoso</keyname><forenames>Ario</forenames></author></authors><title>Adding Context to Knowledge and Action Bases</title><categories>cs.AI</categories><comments>ARCOE-Logic 2014 Workshop Notes, pp. 25-36</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowledge and Action Bases (KABs) have been recently proposed as a formal
framework to capture the dynamics of systems which manipulate Description Logic
(DL) Knowledge Bases (KBs) through action execution. In this work, we enrich
the KAB setting with contextual information, making use of different context
dimensions. On the one hand, context is determined by the environment using
context-changing actions that make use of the current state of the KB and the
current context. On the other hand, it affects the set of TBox assertions that
are relevant at each time point, and that have to be considered when processing
queries posed over the KAB. Here we extend to our enriched setting the results
on verification of rich temporal properties expressed in mu-calculus, which had
been established for standard KABs. Specifically, we show that under a
run-boundedness condition, verification stays decidable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7967</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7967</id><created>2014-12-26</created><authors><author><keyname>Homola</keyname><forenames>Martin</forenames></author><author><keyname>Patkos</keyname><forenames>Theodore</forenames></author></authors><title>Different Types of Conflicting Knowledge in AmI Environments</title><categories>cs.AI</categories><comments>ARCOE-Logic 2014 Workshop Notes, pp. 37-43</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We characterize different types of conflicts that may occur in complex
distributed multi-agent scenarios, such as in Ambient Intelligence (AmI)
environments, and we argue that these conflicts should be resolved in a
suitable order and with the appropriate strategies for each individual conflict
type. We call for further research with the goal of turning conflict resolution
in AmI environments and similar multi-agent domains into a more coordinated and
agreed upon process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7968</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7968</id><created>2014-12-26</created><authors><author><keyname>Ringsquandl</keyname><forenames>Martin</forenames></author><author><keyname>Lamparter</keyname><forenames>Steffen</forenames></author><author><keyname>Lepratti</keyname><forenames>Raffaello</forenames></author></authors><title>Context-Aware Analytics in MOM Applications</title><categories>cs.AI</categories><comments>ARCOE-Logic 2014 Workshop Notes, pp. 44-49</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Manufacturing Operations Management (MOM) systems are complex in the sense
that they integrate data from heterogeneous systems inside the automation
pyramid. The need for context-aware analytics arises from the dynamics of these
systems that influence data generation and hamper comparability of analytics,
especially predictive models (e.g. predictive maintenance), where concept drift
affects application of these models in the future. Recently, an increasing
amount of research has been directed towards data integration using semantic
context models. Manual construction of such context models is an elaborate and
error-prone task. Therefore, we pose the challenge to apply combinations of
knowledge extraction techniques in the domain of analytics in MOM, which
comprises the scope of data integration within Product Life-cycle Management
(PLM), Enterprise Resource Planning (ERP), and Manufacturing Execution Systems
(MES). We describe motivations, technological challenges and show benefits of
context-aware analytics, which leverage from and regard the interconnectedness
of semantic context data. Our example scenario shows the need for distribution
and effective change tracking of context information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7977</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7977</id><created>2014-12-08</created><authors><author><keyname>Reiss</keyname><forenames>D. S.</forenames></author><author><keyname>Price</keyname><forenames>J. J.</forenames></author><author><keyname>Evans</keyname><forenames>T. S.</forenames></author></authors><title>Sculplexity: Sculptures of Complexity using 3D printing</title><categories>physics.ed-ph cs.HC math.HO physics.pop-ph physics.soc-ph</categories><comments>Free access to article on European Physics Letters</comments><report-no>Imperial/TP/13/TSE/1</report-no><journal-ref>European Physics Letters 2013, 104, 48001</journal-ref><doi>10.1209/0295-5075/104/48001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how to convert models of complex systems such as 2D cellular automata
into a 3D printed object. Our method takes into account the limitations
inherent to 3D printing processes and materials. Our approach automates the
greater part of this task, bypassing the use of CAD software and the need for
manual design. As a proof of concept, a physical object representing a modified
forest fire model was successfully printed. Automated conversion methods
similar to the ones developed here can be used to create objects for research,
for demonstration and teaching, for outreach, or simply for aesthetic pleasure.
As our outputs can be touched, they may be particularly useful for those with
visual disabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7978</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7978</id><created>2014-12-24</created><authors><author><keyname>Kovach</keyname><forenames>Daniel</forenames></author></authors><title>The Computational Theory of Intelligence: Information Entropy</title><categories>cs.AI cs.LG</categories><comments>Published at
  http://www.scirp.org/journal/PaperInformation.aspx?PaperID=50204</comments><msc-class>68T27</msc-class><acm-class>I.2.1</acm-class><journal-ref>International Journal of Modern Nonlinear Theory and Application,
  3, 182-190 (2014)</journal-ref><doi>10.4236/ijmnta.2014.34020</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an information theoretic approach to the concept of
intelligence in the computational sense. We introduce a probabilistic framework
from which computational intelligence is shown to be an entropy minimizing
process at the local level. Using this new scheme, we develop a simple data
driven clustering example and discuss its applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7979</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7979</id><created>2014-12-26</created><authors><author><keyname>Chung</keyname><forenames>Kai-Min</forenames></author><author><keyname>Dadush</keyname><forenames>Daniel</forenames></author><author><keyname>Liu</keyname><forenames>Feng-Hao</forenames></author><author><keyname>Peikert</keyname><forenames>Chris</forenames></author></authors><title>On the Lattice Smoothing Parameter Problem</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The smoothing parameter $\eta_{\epsilon}(\mathcal{L})$ of a Euclidean lattice
$\mathcal{L}$, introduced by Micciancio and Regev (FOCS'04; SICOMP'07), is
(informally) the smallest amount of Gaussian noise that &quot;smooths out&quot; the
discrete structure of $\mathcal{L}$ (up to error $\epsilon$). It plays a
central role in the best known worst-case/average-case reductions for lattice
problems, a wealth of lattice-based cryptographic constructions, and
(implicitly) the tightest known transference theorems for fundamental lattice
quantities.
  In this work we initiate a study of the complexity of approximating the
smoothing parameter to within a factor $\gamma$, denoted $\gamma$-${\rm
GapSPP}$. We show that (for $\epsilon = 1/{\rm poly}(n)$): $(2+o(1))$-${\rm
GapSPP} \in {\rm AM}$, via a Gaussian analogue of the classic
Goldreich-Goldwasser protocol (STOC'98); $(1+o(1))$-${\rm GapSPP} \in {\rm
coAM}$, via a careful application of the Goldwasser-Sipser (STOC'86) set size
lower bound protocol to thin spherical shells; $(2+o(1))$-${\rm GapSPP} \in
{\rm SZK} \subseteq {\rm AM} \cap {\rm coAM}$ (where ${\rm SZK}$ is the class
of problems having statistical zero-knowledge proofs), by constructing a
suitable instance-dependent commitment scheme (for a slightly worse
$o(1)$-term); $(1+o(1))$-${\rm GapSPP}$ can be solved in deterministic
$2^{O(n)} {\rm polylog}(1/\epsilon)$ time and $2^{O(n)}$ space.
  As an application, we demonstrate a tighter worst-case to average-case
reduction for basing cryptography on the worst-case hardness of the ${\rm
GapSPP}$ problem, with $\tilde{O}(\sqrt{n})$ smaller approximation factor than
the ${\rm GapSVP}$ problem.
  Central to our results are two novel, and nearly tight, characterizations of
the magnitude of discrete Gaussian sums.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7990</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7990</id><created>2014-12-26</created><authors><author><keyname>Diaz-Aviles</keyname><forenames>Ernesto</forenames></author><author><keyname>Lam</keyname><forenames>Hoang Thanh</forenames></author><author><keyname>Pinelli</keyname><forenames>Fabio</forenames></author><author><keyname>Braghin</keyname><forenames>Stefano</forenames></author><author><keyname>Gkoufas</keyname><forenames>Yiannis</forenames></author><author><keyname>Berlingerio</keyname><forenames>Michele</forenames></author><author><keyname>Calabrese</keyname><forenames>Francesco</forenames></author></authors><title>Predicting User Engagement in Twitter with Collaborative Ranking</title><categories>cs.IR cs.CY cs.LG</categories><comments>RecSysChallenge'14 at RecSys 2014, October 10, 2014, Foster City, CA,
  USA</comments><acm-class>H.3.3; I.2.6</acm-class><journal-ref>In Proceedings of the 2014 Recommender Systems Challenge
  (RecSysChallenge'14). ACM, New York, NY, USA, , Pages 41 , 6 pages</journal-ref><doi>10.1145/2668067.2668072</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collaborative Filtering (CF) is a core component of popular web-based
services such as Amazon, YouTube, Netflix, and Twitter. Most applications use
CF to recommend a small set of items to the user. For instance, YouTube
presents to a user a list of top-n videos she would likely watch next based on
her rating and viewing history. Current methods of CF evaluation have been
focused on assessing the quality of a predicted rating or the ranking
performance for top-n recommended items. However, restricting the recommender
system evaluation to these two aspects is rather limiting and neglects other
dimensions that could better characterize a well-perceived recommendation. In
this paper, instead of optimizing rating or top-n recommendation, we focus on
the task of predicting which items generate the highest user engagement. In
particular, we use Twitter as our testbed and cast the problem as a
Collaborative Ranking task where the rich features extracted from the metadata
of the tweets help to complement the transaction information limited to user
ids, item ids, ratings and timestamps. We learn a scoring function that
directly optimizes the user engagement in terms of nDCG@10 on the predicted
ranking. Experiments conducted on an extended version of the MovieTweetings
dataset, released as part of the RecSys Challenge 2014, show the effectiveness
of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7993</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7993</id><created>2014-12-26</created><authors><author><keyname>Salehi</keyname><forenames>Mostafa</forenames></author><author><keyname>Siyari</keyname><forenames>Payam</forenames></author><author><keyname>Magnani</keyname><forenames>Matteo</forenames></author><author><keyname>Montesi</keyname><forenames>Danilo</forenames></author></authors><title>Multidimensional epidemic thresholds in diffusion processes over
  interdependent networks</title><categories>cs.SI physics.soc-ph</categories><doi>10.1016/j.chaos.2014.12.018</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several systems can be modeled as sets of interdependent networks where each
network contains distinct nodes. Diffusion processes like the spreading of a
disease or the propagation of information constitute fundamental phenomena
occurring over such coupled networks. In this paper we propose a new concept of
multidimensional epidemic threshold characterizing diffusion processes over
interdependent networks, allowing different diffusion rates on the different
networks and arbitrary degree distributions. We analytically derive and
numerically illustrate the conditions for multilayer epidemics, i.e., the
appearance of a giant connected component spanning all the networks.
Furthermore, we study the evolution of infection density and diffusion dynamics
with extensive simulation experiments on synthetic and real networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7994</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7994</id><created>2014-12-26</created><updated>2015-09-15</updated><authors><author><keyname>Aggarwal</keyname><forenames>Divesh</forenames></author><author><keyname>Dadush</keyname><forenames>Daniel</forenames></author><author><keyname>Regev</keyname><forenames>Oded</forenames></author><author><keyname>Stephens-Davidowitz</keyname><forenames>Noah</forenames></author></authors><title>Solving the Shortest Vector Problem in $2^n$ Time via Discrete Gaussian
  Sampling</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a randomized $2^{n+o(n)}$-time and space algorithm for solving the
Shortest Vector Problem (SVP) on n-dimensional Euclidean lattices. This
improves on the previous fastest algorithm: the deterministic
$\widetilde{O}(4^n)$-time and $\widetilde{O}(2^n)$-space algorithm of
Micciancio and Voulgaris (STOC 2010, SIAM J. Comp. 2013).
  In fact, we give a conceptually simple algorithm that solves the (in our
opinion, even more interesting) problem of discrete Gaussian sampling (DGS).
More specifically, we show how to sample $2^{n/2}$ vectors from the discrete
Gaussian distribution at any parameter in $2^{n+o(n)}$ time and space. (Prior
work only solved DGS for very large parameters.) Our SVP result then follows
from a natural reduction from SVP to DGS. We also show that our DGS algorithm
implies a $2^{n + o(n)}$-time algorithm that approximates the Closest Vector
Problem to within a factor of $1.97$.
  In addition, we give a more refined algorithm for DGS above the so-called
smoothing parameter of the lattice, which can generate $2^{n/2}$ discrete
Gaussian samples in just $2^{n/2+o(n)}$ time and space. Among other things,
this implies a $2^{n/2+o(n)}$-time and space algorithm for $1.93$-approximate
decision SVP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.7998</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.7998</id><created>2014-12-26</created><updated>2016-03-07</updated><authors><author><keyname>Yang</keyname><forenames>Fan</forenames></author><author><keyname>V&#xe4;&#xe4;n&#xe4;nen</keyname><forenames>Jouko</forenames></author></authors><title>Propositional Logics of Dependence</title><categories>math.LO cs.LO</categories><msc-class>03B60</msc-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In this paper, we study logics of dependence on the propositional level. We
prove that several interesting propositional logics of dependence, including
propositional dependence logic, propositional intuitionistic dependence logic
as well as propositional inquisitive logic, are expressively complete and have
disjunctive or conjunctive normal forms. We provide deduction systems and prove
the completeness theorems for these logics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8003</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8003</id><created>2014-12-26</created><authors><author><keyname>Dousti</keyname><forenames>Mohammad Javad</forenames></author><author><keyname>Pedram</keyname><forenames>Massoud</forenames></author></authors><title>Minimizing the Latency of Quantum Circuits during Mapping to the
  Ion-Trap Circuit Fabric</title><categories>cs.ET quant-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum computers are exponentially faster than their classical counterparts
in terms of solving some specific, but important problems. The biggest
challenge in realizing a quantum computing system is the environmental noise.
One way to decrease the effect of noise (and hence, reduce the overhead of
building fault tolerant quantum circuits) is to reduce the latency of the
quantum circuit that runs on a quantum circuit. In this paper, a novel
algorithm is presented for scheduling, placement, and routing of a quantum
algorithm, which is to be realized on a target quantum circuit fabric
technology. This algorithm, and the accompanying software tool, advances
state-of-the-art in quantum CAD methodologies and methods while considering key
characteristics and constraints of the ion-trap quantum circuit fabric.
Experimental results show that the presented tool improves results of the
previous tool by about 41%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8004</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8004</id><created>2014-12-26</created><authors><author><keyname>Dousti</keyname><forenames>Mohammad Javad</forenames></author><author><keyname>Shafaei</keyname><forenames>Alireza</forenames></author><author><keyname>Pedram</keyname><forenames>Massoud</forenames></author></authors><title>Squash: A Scalable Quantum Mapper Considering Ancilla Sharing</title><categories>cs.ET quant-ph</categories><doi>10.1145/2591513.2591523</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum algorithms for solving problems of interesting size often result in
circuits with a very large number of qubits and quantum gates. Fortunately,
these algorithms also tend to contain a small number of repetitively-used
quantum kernels. Identifying the quantum logic blocks that implement such
quantum kernels is critical to the complexity management for realizing the
corresponding quantum circuit. Moreover, quantum computation requires some type
of quantum error correction coding to combat decoherence, which in turn results
in a large number of ancilla qubits in the circuit. Sharing the ancilla qubits
among quantum operations (even though this sharing can increase the overall
circuit latency) is important in order to curb the resource demand of the
quantum algorithm. This paper presents a multi-core reconfigurable quantum
processor architecture, called Requp, which supports a layered approach to
mapping a quantum algorithm and ancilla sharing. More precisely, a scalable
quantum mapper, called Squash, is introduced, which divides a given quantum
circuit into a number of quantum kernels- each kernel comprises k parts such
that each part will run on exactly one of k available cores. Experimental
results demonstrate that Squash can handle large-scale quantum algorithms while
providing an effective mechanism for sharing ancilla qubits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8005</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8005</id><created>2014-12-26</created><authors><author><keyname>Gavryushkin</keyname><forenames>Alex</forenames></author><author><keyname>Khoussainov</keyname><forenames>Bakhadyr</forenames></author><author><keyname>Kokho</keyname><forenames>Mikhail</forenames></author><author><keyname>Liu</keyname><forenames>Jiamou</forenames></author></authors><title>Dynamic Algorithms for Interval Scheduling on a Single Machine</title><categories>cs.DS</categories><journal-ref>Theoretical Computer Science, Volume 562, 11 January 2015, Pages
  227-242</journal-ref><doi>10.1016/j.tcs.2014.09.046</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate dynamic algorithms for the interval scheduling problem. Our
algorithm runs in amortised time $O(\log n)$ for query operation and $O(d\log^2
n)$ for insertion and removal operations, where $n$ and $d$ are the maximal
numbers of intervals and pairwise overlapping intervals respectively. We also
show that for a monotonic set, that is when no interval properly contains
another interval, the amortised complexity is $O(\log n)$ for both query and
update operations. We compare the two algorithms for the monotonic interval
sets using experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8007</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8007</id><created>2014-12-26</created><authors><author><keyname>Castiglione</keyname><forenames>Jason</forenames></author></authors><title>Combining Conventional Cryptography with Information Theoretic Security</title><categories>cs.IT math.IT</categories><comments>7 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper highlights security issues that can arise when incorrect
assumptions are made on the capabilities of an eavesdropper. In particular, we
analyze a channel model based on a split Binary Symmetric Channel (BSC).
Corresponding security parameters are chosen based on this channel model, and
assumptions on the eavesdroppers capabilities. A gradual relaxation of the
restrictions on the eavesdropper's capabilities will be made, and the resulting
loss of security will be quantified. An alternative will then be presented that
is based on stochastic encoding and creating artificially noisy channels
through the usage of private keys. The artificial channel will be constructed
through a deterministic process that will be computationally intractable to
reverse.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8010</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8010</id><created>2014-12-26</created><authors><author><keyname>Vu</keyname><forenames>Xuan-Son</forenames></author><author><keyname>Park</keyname><forenames>Seong-Bae</forenames></author></authors><title>Construction of Vietnamese SentiWordNet by using Vietnamese Dictionary</title><categories>cs.CL</categories><comments>accepted on April-9th-2014, best paper award</comments><journal-ref>The 40th Conference of the Korea Information Processing Society,
  pp. 745-748, April 2014, South Korea</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  SentiWordNet is an important lexical resource supporting sentiment analysis
in opinion mining applications. In this paper, we propose a novel approach to
construct a Vietnamese SentiWordNet (VSWN). SentiWordNet is typically generated
from WordNet in which each synset has numerical scores to indicate its opinion
polarities. Many previous studies obtained these scores by applying a machine
learning method to WordNet. However, Vietnamese WordNet is not available
unfortunately by the time of this paper. Therefore, we propose a method to
construct VSWN from a Vietnamese dictionary, not from WordNet. We show the
effectiveness of the proposed method by generating a VSWN with 39,561 synsets
automatically. The method is experimentally tested with 266 synsets with aspect
of positivity and negativity. It attains a competitive result compared with
English SentiWordNet that is 0.066 and 0.052 differences for positivity and
negativity sets respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8013</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8013</id><created>2014-12-26</created><updated>2015-01-20</updated><authors><author><keyname>Lal</keyname><forenames>Nidhi</forenames></author></authors><title>An Effective Approach for Mobile ad hoc Network via I-Watchdog Protocol</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile ad hoc network (MANET) is now days become very famous due to their
fixed infrastructure-less quality and dynamic nature. They contain a large
number of nodes which are connected and communicated to each other in wireless
nature. Mobile ad hoc network is a wireless technology that contains high
mobility of nodes and does not depend on the background administrator for
central authority, because they do not contain any infrastructure. Nodes of the
MANET use radio wave for communication and having limited resources and limited
computational power. The Topology of this network is changing very frequently
because they are distributed in nature and self-configurable. Due to its
wireless nature and lack of any central authority in the background, Mobile ad
hoc networks are always vulnerable to some security issues and performance
issues. The security imposes a huge impact on the performance of any network.
Some of the security issues are black hole attack, flooding, wormhole attack
etc. In this paper, we will discuss issues regarding low performance of
Watchdog protocol used in the MANET and proposed an improved Watchdog
mechanism, which is called by I-Watchdog protocol that overcomes the
limitations of Watchdog protocol and gives high performance in terms of
throughput, delay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8018</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8018</id><created>2014-12-27</created><authors><author><keyname>Safavi</keyname><forenames>Sam</forenames></author><author><keyname>Khan</keyname><forenames>Usman A.</forenames></author></authors><title>Asymptotic stability of stochastic LTV systems with applications to
  distributed dynamic fusion</title><categories>cs.SY</categories><comments>submitted for journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate asymptotic stability of linear time-varying
systems with (sub-) stochastic system matrices. Motivated by distributed
dynamic fusion over networks of mobile agents, we impose some mild regularity
conditions on the elements of time-varying system matrices. We provide
sufficient conditions under which the asymptotic stability of the LTV system
can be guaranteed. By introducing the notion of slices, as non-overlapping
partitions of the sequence of systems matrices, we obtain stability conditions
in terms of the slice lengths and some network parameters. In addition, we
apply the LTV stability results to the distributed leader-follower algorithm,
and show the corresponding convergence and steady-state. An illustrative
example is also included to validate the effectiveness of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8028</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8028</id><created>2014-12-27</created><authors><author><keyname>Alam</keyname><forenames>Mansaf</forenames></author><author><keyname>Shakil</keyname><forenames>Kashish Ara</forenames></author></authors><title>An NBDMMM Algorithm Based Framework for Allocation of Resources in Cloud</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing is a technological advancement in the arena of computing and
has taken the utility vision of computing a step further by providing computing
resources such as network, storage, compute capacity and servers, as a service
via an internet connection. These services are provided to the users in a pay
per use manner subjected to the amount of usage of these resources by the cloud
users. Since the usage of these resources is done in an elastic manner thus an
on demand provisioning of these resources is the driving force behind the
entire cloud computing infrastructure therefore the maintenance of these
resources is a decisive task that must be taken into account. Eventually,
infrastructure level performance monitoring and enhancement is also important.
This paper proposes a framework for allocation of resources in a cloud based
environment thereby leading to an infrastructure level enhancement of
performance in a cloud environment. The framework is divided into four stages
Stage 1: Cloud service provider monitors the infrastructure level pattern of
usage of resources and behavior of the cloud users. Stage 2: Report the
monitoring activities about the usage to cloud service providers. Stage 3:
Apply proposed Network Bandwidth Dependent DMMM algorithm .Stage 4: Allocate
resources or provide services to cloud users, thereby leading to infrastructure
level performance enhancement and efficient management of resources. Analysis
of resource usage pattern is considered as an important factor for proper
allocation of resources by the service providers, in this paper Google cluster
trace has been used for accessing the resource usage pattern in cloud.
Experiments have been conducted on cloudsim simulation framework and the
results reveal that NBDMMM algorithm improvises allocation of resources in a
virtualized cloud.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8029</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8029</id><created>2014-12-27</created><authors><author><keyname>Alam</keyname><forenames>Mansaf</forenames></author><author><keyname>Shakil</keyname><forenames>Kashish Ara</forenames></author></authors><title>A Decision Matrix and Monitoring based Framework for Infrastructure
  Performance Enhancement in A Cloud based Environment</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud environment is very different from traditional computing environment
and therefore tracking the performance of cloud leverages additional
requirements. The movement of data in cloud is very fast. Hence, it requires
that resources and infrastructure available at disposal must be equally
competent. Infrastructure level performance in cloud involves the performance
of servers, network and storage which act as the heart and soul for driving the
entire cloud business. Thus a constant improvement and enhancement of
infrastructure level performance is an important task that needs to be taken
into account. This paper proposes a framework for infrastructure performance
enhancement in a cloud based environment. The framework is broadly divided into
four steps: a) Infrastructure level monitoring of usage pattern and behaviour
of the cloud end users, b) Reporting of the monitoring activities to the cloud
service provider c) Cloud service provider assigns priority according to our
decision matrix based max-min algorithm (DMMM) d) Providing services to cloud
users leading to infrastructure performance enhancement. Our framework is based
on decision matrix and monitoring in cloud using our proposed decision matrix
based max-min algorithm, which draws its inspiration from the original min-min
algorithm. This algorithm makes use of decision matrix to make decisions
regarding distribution of resources among the cloud users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8032</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8032</id><created>2014-12-27</created><updated>2015-03-30</updated><authors><author><keyname>Sajeed</keyname><forenames>Shihan</forenames></author><author><keyname>Radchenko</keyname><forenames>Igor</forenames></author><author><keyname>Kaiser</keyname><forenames>Sarah</forenames></author><author><keyname>Bourgoin</keyname><forenames>Jean-Philippe</forenames></author><author><keyname>Pappa</keyname><forenames>Anna</forenames></author><author><keyname>Monat</keyname><forenames>Laurent</forenames></author><author><keyname>Legre</keyname><forenames>Matthieu</forenames></author><author><keyname>Makarov</keyname><forenames>Vadim</forenames></author></authors><title>Attacks exploiting deviation of mean photon number in quantum key
  distribution and coin tossing</title><categories>quant-ph cs.CR</categories><comments>15 pages, 13 figures, Improved Introduction and Conclusion, Published
  in Physical Review A, Accepted at QCrypt 2014, 4th international conference
  on quantum cryptography, September 1-5, 2014 in Paris, France
  http://2014.qcrypt.net/program/</comments><journal-ref>Phys. Rev. A 91, 032326 (2015)</journal-ref><doi>10.1103/PhysRevA.91.032326</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The security of quantum communication using a weak coherent source requires
an accurate knowledge of the source's mean photon number. Finite calibration
precision or an active manipulation by an attacker may cause the actual emitted
photon number to deviate from the known value. We model effects of this
deviation on the security of three quantum communication protocols: the
Bennett-Brassard 1984 (BB84) quantum key distribution (QKD) protocol without
decoy states, Scarani-Acin-Ribordy-Gisin 2004 (SARG04) QKD protocol, and a
coin-tossing protocol. For QKD, we model both a strong attack using technology
possible in principle, and a realistic attack bounded by today's technology. To
maintain the mean photon number in two-way systems, such as plug-and-play and
relativistic quantum cryptography schemes, bright pulse energy incoming from
the communication channel must be monitored. Implementation of a monitoring
detector has largely been ignored so far, except for ID Quantique's commercial
QKD system Clavis2. We scrutinize this implementation for security problems,
and show that designing a hack-proof pulse-energy-measuring detector is far
from trivial. Indeed the first implementation has three serious flaws confirmed
experimentally, each of which may be exploited in a cleverly constructed
Trojan-horse attack. We discuss requirements for a loophole-free implementation
of the monitoring detector.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8045</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8045</id><created>2014-12-27</created><authors><author><keyname>Gower</keyname><forenames>Robert Mansel</forenames></author><author><keyname>Gondzio</keyname><forenames>Jacek</forenames></author></authors><title>Action constrained quasi-Newton methods</title><categories>math.OC cs.NA</categories><msc-class>65K05, 49M37, 90C25, 90C53</msc-class><acm-class>G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  At the heart of Newton based optimization methods is a sequence of symmetric
linear systems. Each consecutive system in this sequence is similar to the
next, so solving them separately is a waste of computational effort. Here we
describe automatic preconditioning techniques for iterative methods for solving
such sequences of systems by maintaining an estimate of the inverse system
matrix. We update the estimate of the inverse system matrix with quasi-Newton
type formulas based on what we call an action constraint instead of the secant
equation. We implement the estimated inverses as preconditioners in a Newton-CG
method and prove quadratic termination. Our implementation is the first
parallel quasi-Newton preconditioners, in full and limited memory variants.
Tests on logistic Support Vector Machine problems reveal that our method is
very efficient, converging in wall clock time before a Newton-CG method without
preconditioning. Further tests on a set of classic test problems reveal that
the method is robust. The action constraint makes these updates flexible enough
to mesh with trust-region and active set methods, a flexibility that is not
present in classic quasi-Newton methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8054</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8054</id><created>2014-12-27</created><authors><author><keyname>Marecek</keyname><forenames>Jakub</forenames></author><author><keyname>McCoy</keyname><forenames>Timothy</forenames></author><author><keyname>Mevissen</keyname><forenames>Martin</forenames></author></authors><title>Power Flow as an Algebraic System</title><categories>math.OC cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note studies properties of feasible and optimal solutions of the power
flow problem in an alternating current model both in theory and experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8060</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8060</id><created>2014-12-27</created><updated>2015-06-15</updated><authors><author><keyname>Qu</keyname><forenames>Zheng</forenames></author><author><keyname>Richt&#xe1;rik</keyname><forenames>Peter</forenames></author></authors><title>Coordinate Descent with Arbitrary Sampling I: Algorithms and Complexity</title><categories>math.OC cs.LG cs.NA math.NA</categories><comments>32 pages, 0 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of minimizing the sum of a smooth convex function and a
convex block-separable regularizer and propose a new randomized coordinate
descent method, which we call ALPHA. Our method at every iteration updates a
random subset of coordinates, following an arbitrary distribution. No
coordinate descent methods capable to handle an arbitrary sampling have been
studied in the literature before for this problem. ALPHA is a remarkably
flexible algorithm: in special cases, it reduces to deterministic and
randomized methods such as gradient descent, coordinate descent, parallel
coordinate descent and distributed coordinate descent -- both in nonaccelerated
and accelerated variants. The variants with arbitrary (or importance) sampling
are new. We provide a complexity analysis of ALPHA, from which we deduce as a
direct corollary complexity bounds for its many variants, all matching or
improving best known bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8063</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8063</id><created>2014-12-27</created><updated>2015-05-28</updated><authors><author><keyname>Qu</keyname><forenames>Zheng</forenames></author><author><keyname>Richt&#xe1;rik</keyname><forenames>Peter</forenames></author></authors><title>Coordinate Descent with Arbitrary Sampling II: Expected Separable
  Overapproximation</title><categories>math.OC cs.LG cs.NA math.NA math.PR</categories><comments>29 pages, 0 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The design and complexity analysis of randomized coordinate descent methods,
and in particular of variants which update a random subset (sampling) of
coordinates in each iteration, depends on the notion of expected separable
overapproximation (ESO). This refers to an inequality involving the objective
function and the sampling, capturing in a compact way certain smoothness
properties of the function in a random subspace spanned by the sampled
coordinates. ESO inequalities were previously established for special classes
of samplings only, almost invariably for uniform samplings. In this paper we
develop a systematic technique for deriving these inequalities for a large
class of functions and for arbitrary samplings. We demonstrate that one can
recover existing ESO results using our general approach, which is based on the
study of eigenvalues associated with samplings and the data describing the
function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8070</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8070</id><created>2014-12-27</created><authors><author><keyname>Kovnatsky</keyname><forenames>Artiom</forenames></author><author><keyname>Bronstein</keyname><forenames>Michael M.</forenames></author><author><keyname>Bresson</keyname><forenames>Xavier</forenames></author><author><keyname>Vandergheynst</keyname><forenames>Pierre</forenames></author></authors><title>Functional correspondence by matrix completion</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of finding dense intrinsic
correspondence between manifolds using the recently introduced functional
framework. We pose the functional correspondence problem as matrix completion
with manifold geometric structure and inducing functional localization with the
$L_1$ norm. We discuss efficient numerical procedures for the solution of our
problem. Our method compares favorably to the accuracy of state-of-the-art
correspondence algorithms on non-rigid shape matching benchmarks, and is
especially advantageous in settings when only scarce data is available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8079</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8079</id><created>2014-12-27</created><authors><author><keyname>Bagheri</keyname><forenames>Ayoub</forenames></author><author><keyname>Saraee</keyname><forenames>Mohamad</forenames></author></authors><title>Persian Sentiment Analyzer: A Framework based on a Novel Feature
  Selection Method</title><categories>cs.CL cs.IR</categories><journal-ref>International Journal of Artificial Intelligence 12.2 (2014):
  115-129</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the recent decade, with the enormous growth of digital content in internet
and databases, sentiment analysis has received more and more attention between
information retrieval and natural language processing researchers. Sentiment
analysis aims to use automated tools to detect subjective information from
reviews. One of the main challenges in sentiment analysis is feature selection.
Feature selection is widely used as the first stage of analysis and
classification tasks to reduce the dimension of problem, and improve speed by
the elimination of irrelevant and redundant features. Up to now as there are
few researches conducted on feature selection in sentiment analysis, there are
very rare works for Persian sentiment analysis. This paper considers the
problem of sentiment classification using different feature selection methods
for online customer reviews in Persian language. Three of the challenges of
Persian text are using of a wide variety of declensional suffixes, different
word spacing and many informal or colloquial words. In this paper we study
these challenges by proposing a model for sentiment classification of Persian
review documents. The proposed model is based on lemmatization and feature
selection and is employed Naive Bayes algorithm for classification. We evaluate
the performance of the model on a manually gathered collection of cellphone
reviews, where the results show the effectiveness of the proposed approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8086</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8086</id><created>2014-12-27</created><authors><author><keyname>Wang</keyname><forenames>Xu</forenames></author><author><keyname>Brandt-Pearce</keyname><forenames>Maite</forenames></author><author><keyname>Subramaniam</keyname><forenames>Suresh</forenames></author></authors><title>Impact of Wavelength and Modulation Conversion on Transluscent Elastic
  Optical Networks Using MILP</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compared to legacy wavelength division multiplexing networks, elastic optical
networks (EON) have added flexibility to network deployment and management.
EONs can include previously available technology, such as signal regeneration
and wavelength conversion, as well as new features such as finer-granularity
spectrum assignment and modulation conversion. Yet each added feature adds to
the cost of the network. In order to quantify the potential benefit of each
technology, we present a link-based mixed-integer linear programming (MILP)
formulation to solve the optimal resource allocation problem. We then propose a
recursive model in order to either augment existing network deployments or
speed up the resource allocation computation time for larger networks with
higher traffic demand requirements than can be solved using an MILP. We show
through simulation that systems equipped with signal regenerators or wavelength
converters require a notably smaller total bandwidth, depending on the topology
of the network. We also show that the suboptimal recursive solution speeds up
the calculation and makes the running-time more predictable, compared to the
optimal MILP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8090</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8090</id><created>2014-12-27</created><authors><author><keyname>Kudekar</keyname><forenames>Shrinivas</forenames></author><author><keyname>Richardson</keyname><forenames>Tom</forenames></author><author><keyname>Iyengar</keyname><forenames>Aravind</forenames></author></authors><title>Analysis of Saturated Belief Propagation Decoding of Low-Density
  Parity-Check Codes</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactionson Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the effect of log-likelihood ratio saturation on belief
propagation decoder low-density parity-check codes. Saturation is commonly done
in practice and is known to have a significant effect on error floor
performance. Our focus is on threshold analysis and stability of density
evolution.
  We analyze the decoder for standard low-density parity-check code ensembles
and show that belief propagation decoding generally degrades gracefully with
saturation. Stability of density evolution is, on the other hand, rather
strongly effected by saturation and the asymptotic qualitative effect of
saturation is similar to reduction by one of variable node degree.
  We also show under what conditions the block threshold for the saturated
belief propagation corresponds with the bit threshold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8091</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8091</id><created>2014-12-27</created><updated>2015-06-18</updated><authors><author><keyname>Carneiro</keyname><forenames>Mario</forenames></author></authors><title>Conversion of HOL Light proofs into Metamath</title><categories>cs.LO math.LO</categories><comments>14 pages, 2 figures, accepted to Journal of Formalized Reasoning</comments><msc-class>03B35 (Primary), 03B15, 68T35, 03B30 (Secondary)</msc-class><acm-class>F.4.1; I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm for converting proofs from the OpenTheory interchange
format, which can be translated to and from any of the HOL family of proof
languages (HOL4, HOL Light, ProofPower, and Isabelle), into the ZFC-based
Metamath language. This task is divided into two steps: the translation of an
OpenTheory proof into a Metamath HOL formalization, $\mathtt{\text{hol.mm}}$,
followed by the embedding of the HOL formalization into the main ZFC
foundations of the main Metamath library, $\mathtt{\text{set.mm}}$. This
process provides a means to link the simplicity of the Metamath foundations to
the intense automation efforts which have borne fruit in HOL Light, allowing
the production of complete Metamath proofs of theorems in HOL Light, while also
proving that HOL Light is consistent, relative to Metamath's ZFC
axiomatization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8093</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8093</id><created>2014-12-27</created><authors><author><keyname>Roy</keyname><forenames>Kaushik</forenames></author><author><keyname>Panigrahi</keyname><forenames>Satish Ch.</forenames></author><author><keyname>Mukhopadhyay</keyname><forenames>Asish</forenames></author></authors><title>Multiple alignment of structures using center of proteins</title><categories>cs.CE q-bio.BM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we report on an algorithm for aligning multiple protein
structures. The algorithm has been tested on a variety of inputs and it
performs well in comparison to well-known algorithms for this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8097</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8097</id><created>2014-12-27</created><updated>2015-04-28</updated><authors><author><keyname>Hoza</keyname><forenames>William M.</forenames></author><author><keyname>Schulman</keyname><forenames>Leonard J.</forenames></author></authors><title>The Adversarial Noise Threshold for Distributed Protocols</title><categories>cs.DS cs.DC cs.IT math.IT</categories><comments>23 pages, 2 figures. Fixes mistake in theorem 6 and various typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of implementing distributed protocols, despite
adversarial channel errors, on synchronous-messaging networks with arbitrary
topology.
  In our first result we show that any $n$-party $T$-round protocol on an
undirected communication network $G$ can be compiled into a robust simulation
protocol on a sparse ($\mathcal{O}(n)$ edges) subnetwork so that the simulation
tolerates an adversarial error rate of $\Omega\left(\frac{1}{n}\right)$; the
simulation has a round complexity of $\mathcal{O}\left(\frac{m \log n}{n}
T\right)$, where $m$ is the number of edges in $G$. (So the simulation is
work-preserving up to a $\log$ factor.) The adversary's error rate is within a
constant factor of optimal. Given the error rate, the round complexity blowup
is within a factor of $\mathcal{O}(k \log n)$ of optimal, where $k$ is the edge
connectivity of $G$. We also determine that the maximum tolerable error rate on
directed communication networks is $\Theta(1/s)$ where $s$ is the number of
edges in a minimum equivalent digraph.
  Next we investigate adversarial per-edge error rates, where the adversary is
given an error budget on each edge of the network. We determine the exact limit
for tolerable per-edge error rates on an arbitrary directed graph. However, the
construction that approaches this limit has exponential round complexity, so we
give another compiler, which transforms $T$-round protocols into
$\mathcal{O}(mT)$-round simulations, and prove that for polynomial-query black
box compilers, the per-edge error rate tolerated by this last compiler is
within a constant factor of optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8099</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8099</id><created>2014-12-01</created><authors><author><keyname>Rathipriya</keyname><forenames>R.</forenames></author><author><keyname>Thangavel</keyname><forenames>K.</forenames></author></authors><title>Extraction of Web Usage Profiles using Simulated Annealing Based
  Biclustering Approach</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the Simulated Annealing (SA) based biclustering approach is
proposed in which SA is used as an optimization tool for biclustering of web
usage data to identify the optimal user profile from the given web usage data.
Extracted biclusters are consists of correlated users whose usage behaviors are
similar across the subset of web pages of a web site where as these users are
uncorrelated for remaining pages of a web site. These results are very useful
in web personalization so that it communicates better with its users and for
making customized prediction. Also useful for providing customized web service
too. Experiment was conducted on the real web usage dataset called CTI dataset.
Results show that proposed SA based biclustering approach can extract highly
correlated user groups from the preprocessed web usage data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8102</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8102</id><created>2014-12-27</created><authors><author><keyname>Coecke</keyname><forenames>Bob</forenames><affiliation>University of Oxford</affiliation></author><author><keyname>Hasuo</keyname><forenames>Ichiro</forenames><affiliation>The University of Tokyo</affiliation></author><author><keyname>Panangaden</keyname><forenames>Prakash</forenames><affiliation>McGill University</affiliation></author></authors><title>Proceedings of the 11th workshop on Quantum Physics and Logic</title><categories>cs.LO cs.CL cs.PL quant-ph</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 172, 2014</journal-ref><doi>10.4204/EPTCS.172</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of the 11th International Workshop on
Quantum Physics and Logic (QPL 2014), which was held from the 4th to the 6th of
June, 2014, at Kyoto University, Japan.
  The goal of the QPL workshop series is to bring together researchers working
on mathematical foundations of quantum physics, quantum computing and
spatio-temporal causal structures, and in particular those that use logical
tools, ordered algebraic and category-theoretic structures, formal languages,
semantic methods and other computer science methods for the study of physical
behavior in general. Over the past few years, there has been growing activity
in these foundational approaches, together with a renewed interest in the
foundations of quantum theory, which complement the more mainstream research in
quantum computation. Earlier workshops in this series, with the same acronym
under the name &quot;Quantum Programming Languages&quot;, were held in Ottawa (2003),
Turku (2004), Chicago (2005), and Oxford (2006). The first QPL under the new
name Quantum Physics and Logic was held in Reykjavik (2008), followed by Oxford
(2009 and 2010), Nijmegen (2011), Brussels (2012) and Barcelona (2013).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8103</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8103</id><created>2014-12-05</created><authors><author><keyname>Meghanathan</keyname><forenames>Natarajan</forenames></author><author><keyname>Milton</keyname><forenames>Leslie C.</forenames></author></authors><title>A Simulation Based Performance Comparison Study of Stability-Based
  Routing, Power-Aware Routing and Load-Balancing On-Demand Routing Protocols
  for Mobile Ad hoc Networks</title><categories>cs.NI</categories><comments>8 pages, 12 figures. arXiv admin note: substantial text overlap with
  arXiv:1007.0409</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The high-level contribution of this paper is a simulation-based detailed
performance comparison of three different classes of on-demand routing
protocols for mobile ad hoc networks: stability-based routing, power-aware
routing and load-balanced routing. We choose the Flow-Oriented Routing protocol
(FORP), Min-Max Battery Cost Routing (MMBCR) and the traffic interference based
Load Balancing Routing (LBR) protocol as representatives of the stability-based
routing, poweraware routing and load-balancing routing protocols respectively.
FORP incurs the least number of route transitions; while LBR incurs the
smallest hop count and lowest end-to-end delay per data packet. Energy consumed
per data packet is the least for LBR, closely followed by MMBCR. FORP incurs
the maximum energy consumed per data packet, both in the absence and presence
of power control. Nevertheless, in the presence of power control, the
end-to-end delay per data packet and energy consumed per data packet incurred
by FORP are significantly reduced compared to the scenario without power
control. MMBCR is the most fair in terms of node usage and incurs the largest
time for first node failure. FORP tends to repeatedly use nodes lying on the
stable path and hence is the most unfair of the three routing protocols. FORP
also incurs the smallest value for the time of first node failure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8104</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8104</id><created>2014-12-07</created><authors><author><keyname>Meghanathan</keyname><forenames>Natarajan</forenames></author></authors><title>Performance Comparison of Minimum Hop vs. Minimum Edge Based Multicast
  Routing under Different Mobility Models for Mobile Ad hoc Networks</title><categories>cs.NI</categories><comments>14 pages, 17 figures</comments><journal-ref>International Journal of Wireless and Mobile Networks, 3(3):1-14,
  June 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The high-level contribution of this paper is to establish benchmarks for the
minimum hop count per source-receiver path and the minimum number of edges per
tree for multicast routing in mobile ad hoc networks (MANETs) under different
mobility models. In this pursuit, we explore the tradeoffs between these two
routing strategies with respect to hop count, number of edges and lifetime per
multicast tree with respect to the Random Waypoint, City Section and Manhattan
mobility models. We employ the Breadth First Search algorithm and the Minimum
Steiner Tree heuristic for determining a sequence of minimum hop and minimum
edge trees respectively. While both the minimum hop and minimum edge trees
exist for a relatively longer time under the Manhattan mobility model; the
number of edges per tree and the hop count per source-receiver path are
relatively low under the Random Waypoint model. For all the three mobility
models, the minimum edge trees have a longer lifetime compared to the minimum
hop trees and the difference in lifetime increases with increase in network
density and/or the multicast group size. Multicast trees determined under the
City Section model incur fewer edges and lower hop count compared to the
Manhattan mobility model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8105</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8105</id><created>2014-12-27</created><authors><author><keyname>Wu</keyname><forenames>Junfeng</forenames></author><author><keyname>Shi</keyname><forenames>Guodong</forenames></author><author><keyname>Anderson</keyname><forenames>Brian D. O.</forenames></author><author><keyname>Johansson</keyname><forenames>Karl Henrik</forenames></author></authors><title>Kalman Filtering over Fading Channels: Zero-One Laws and Almost Sure
  Stabilities</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate probabilistic stability of Kalman filtering
over fading channels modeled by $\ast$-mixing random processes, where channel
fading is allowed to generate non-stationary packet dropouts with temporal
and/or spatial correlations. Upper/lower almost sure (a.s.) stabilities and
absolutely upper/lower a.s. stabilities are defined for characterizing the
sample-path behaviors of the Kalman filtering. We prove that both upper and
lower a.s. stabilities follow a zero-one law, i.e., these stabilities must
happen with a probability either zero or one, and when the filtering system is
one-step observable, the absolutely upper and lower a.s. stabilities can also
be interpreted using a zero-one law. We establish general stability conditions
for (absolutely) upper and lower a.s. stabilities. In particular, with one-step
observability, we show the equivalence between absolutely a.s. stabilities and
a.s. ones, and necessary and sufficient conditions in terms of packet arrival
rate are derived; for the so-called non-degenerate systems, we also manage to
give a necessary and sufficient condition for upper a.s. stability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8107</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8107</id><created>2014-12-09</created><authors><author><keyname>Cherian</keyname><forenames>Mary</forenames></author><author><keyname>Nair</keyname><forenames>T. R. Gopalakrishnan</forenames></author></authors><title>Priority based bandwidth allocation in wireless sensor networks</title><categories>cs.NI</categories><comments>10 pages, International Journal of Computer Networks &amp; Communications
  (IJCNC) Vol.6, No.6, November 2014</comments><doi>10.5121/ijcnc.2014.6609</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most of the sensor network applications need real time communication and the
need for deadline aware real time communication is becoming eminent in these
applications. These applications have different dead line requirements also.
The real time applications of wireless sensor networks are bandwidth sensitive
and need higher share of bandwidth for higher priority data to meet the dead
line requirements. In this paper we focus on the MAC layer modifications to
meet the real time requirements of different priority data. Bandwidth
partitioning among different priority transmissions is implemented through MAC
layer modifications. The MAC layer implements a queuing model that supports
lower transfer rate for lower priority packets and higher transfer rate for
real time packets with higher priority, minimizing the end to end delay. The
performance of the algorithm is evaluated with varying node distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8109</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8109</id><created>2014-12-10</created><authors><author><keyname>Charrada</keyname><forenames>Anis</forenames></author><author><keyname>Samet</keyname><forenames>Abdelaziz</forenames></author></authors><title>Complex support vector machines regression for robust channel estimation
  in LTE downlink system</title><categories>cs.IT cs.LG math.IT</categories><comments>13 pages Vol.4, IJCNC (2012) No.1, January 2012. arXiv admin note:
  substantial text overlap with arXiv:1109.0895</comments><doi>10.5121/ijcnc.2012.4115</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the problem of channel estimation for LTE Downlink system in
the environment of high mobility presenting non-Gaussian impulse noise
interfering with reference signals is faced. The estimation of the frequency
selective time varying multipath fading channel is performed by using a channel
estimator based on a nonlinear complex Support Vector Machine Regression (SVR)
which is applied to Long Term Evolution (LTE) downlink. The estimation
algorithm makes use of the pilot signals to estimate the total frequency
response of the highly selective fading multipath channel. Thus, the algorithm
maps trained data into a high dimensional feature space and uses the structural
risk minimization principle to carry out the regression estimation for the
frequency response function of the fading channel. The obtained results show
the effectiveness of the proposed method which has better performance than the
conventional Least Squares (LS) and Decision Feedback methods to track the
variations of the fading multipath channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8118</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8118</id><created>2014-12-28</created><authors><author><keyname>Zhang</keyname><forenames>Lanbo</forenames></author><author><keyname>Zhang</keyname><forenames>Yi</forenames></author></authors><title>Hierarchical Bayesian Models with Factorization for Content-Based
  Recommendation</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most existing content-based filtering approaches learn user profiles
independently without capturing the similarity among users. Bayesian
hierarchical models \cite{Zhang:Efficient} learn user profiles jointly and have
the advantage of being able to borrow discriminative information from other
users through a Bayesian prior. However, the standard Bayesian hierarchical
models assume all user profiles are generated from the same prior. Considering
the diversity of user interests, this assumption could be improved by
introducing more flexibility. Besides, most existing content-based filtering
approaches implicitly assume that each user profile corresponds to exactly one
user interest and fail to capture a user's multiple interests (information
needs).
  In this paper, we present a flexible Bayesian hierarchical modeling approach
to model both commonality and diversity among users as well as individual
users' multiple interests. We propose two models each with different
assumptions, and the proposed models are called Discriminative Factored Prior
Models (DFPM). In our models, each user profile is modeled as a discriminative
classifier with a factored model as its prior, and different factors contribute
in different levels to each user profile. Compared with existing content-based
filtering models, DFPM are interesting because they can 1) borrow
discriminative criteria of other users while learning a particular user profile
through the factored prior; 2) trade off well between diversity and commonality
among users; and 3) handle the challenging classification situation where each
class contains multiple concepts. The experimental results on a dataset
collected from real users on digg.com show that our models significantly
outperform the baseline models of L-2 regularized logistic regression and
traditional Bayesian hierarchical model with logistic regression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8120</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8120</id><created>2014-12-28</created><authors><author><keyname>Kusum</keyname><forenames>Amlan</forenames></author><author><keyname>Neamtiu</keyname><forenames>Iulian</forenames></author><author><keyname>Gupta</keyname><forenames>Rajiv</forenames></author></authors><title>Adapting Graph Application Performance via Alternate Data Structure
  Representation</title><categories>cs.PL</categories><comments>Part of ADAPT Workshop proceedings, 2015 (arXiv:1412.2347)</comments><report-no>ADAPT/2015/03</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph processing is used extensively in areas from social networking mining
to web indexing. We demonstrate that the performance and dependability of such
applications critically hinges on the graph data structure used, because a
fixed, compile-time choice of data structure can lead to poor performance or
applications unable to complete. To address this problem, we introduce an
approach that helps programmers transform regular, off-the-shelf graph
applications into adaptive, more dependable applications where adaptations are
performed via runtime selection from alternate data structure representations.
Using our approach, applications dynamically adapt to the input graph's
characteristics and changes in available memory so they continue to run when
faced with adverse conditions such as low memory. Experiments with graph
algorithms on real-world (e.g., Wikipedia metadata, Gnutella topology) and
synthetic graph datasets show that our adaptive applications run to completion
with lower execution time and/or memory utilization in comparison to their
non-adaptive versions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8125</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8125</id><created>2014-12-28</created><authors><author><keyname>Zhang</keyname><forenames>Lanbo</forenames></author><author><keyname>Zhang</keyname><forenames>Yi</forenames></author><author><keyname>Xing</keyname><forenames>Qianli</forenames></author></authors><title>Learning from Labeled Features for Document Filtering</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing document filtering systems learn user profiles based on user
relevance feedback on documents. In some cases, users may have prior knowledge
about what features are important. For example, a Spanish speaker may only want
news written in Spanish, and thus a relevant document should contain the
feature &quot;Language: Spanish&quot;; a researcher focusing on HIV knows an article with
the medical subject &quot;Subject: AIDS&quot; is very likely to be relevant to him/her.
  Semi-structured documents with rich metadata are increasingly prevalent on
the Internet. Motivated by the well-adopted faceted search interface in
e-commerce, we study the exploitation of user prior knowledge on faceted
features for semi-structured document filtering. We envision two faceted
feedback mechanisms, and propose a novel user profile learning algorithm that
can incorporate user feedback on features. To evaluate the proposed work, we
use two data sets from the TREC filtering track, and conduct a user study on
Amazon Mechanical Turk. Our experiment results show that user feedback on
faceted features is useful for filtering. The proposed user profile learning
algorithm can effectively learn from user feedback on both documents and
features, and performs better than several existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8147</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8147</id><created>2014-12-28</created><authors><author><keyname>Parseh</keyname><forenames>Saeed</forenames></author><author><keyname>Baraani</keyname><forenames>Ahmad</forenames></author></authors><title>Improving Persian Document Classification Using Semantic Relations
  between Words</title><categories>cs.IR cs.LG</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the increase of information, document classification as one of the
methods of text mining, plays vital role in many management and organizing
information. Document classification is the process of assigning a document to
one or more predefined category labels. Document classification includes
different parts such as text processing, term selection, term weighting and
final classification. The accuracy of document classification is very
important. Thus improvement in each part of classification should lead to
better results and higher precision. Term weighting has a great impact on the
accuracy of the classification. Most of the existing weighting methods exploit
the statistical information of terms in documents and do not consider semantic
relations between words. In this paper, an automated document classification
system is presented that uses a novel term weighting method based on semantic
relations between terms. To evaluate the proposed method, three standard
Persian corpuses are used. Experiment results show 2 to 4 percent improvement
in classification accuracy compared with the best previous designed system for
Persian documents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8164</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8164</id><created>2014-12-28</created><authors><author><keyname>Huang</keyname><forenames>Qin</forenames></author><author><keyname>Liu</keyname><forenames>Xingwu</forenames></author><author><keyname>Sun</keyname><forenames>Xiaoming</forenames></author><author><keyname>Zhang</keyname><forenames>Jialin</forenames></author></authors><title>How to select the largest k elements from evolving data?</title><categories>cs.DS</categories><comments>23 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate the top-$k$-selection problem, i.e. determine
the largest, second largest, ..., and the $k$-th largest elements, in the
dynamic data model. In this model the order of elements evolves dynamically
over time. In each time step the algorithm can only probe the changes of data
by comparing a pair of elements. Previously only two special cases were
studied[2]: finding the largest element and the median; and sorting all
elements. This paper systematically deals with $k\in [n]$ and solves the
problem almost completely. Specifically, we identify a critical point $k^*$
such that the top-$k$-selection problem can be solved error-free with
probability $1-o(1)$ if and only if $k=o(k^*)$. A lower bound of the error when
$k=\Omega(k^*)$ is also determined, which actually is tight under some
condition. On the other hand, it is shown that the top-$k$-set problem, which
means finding the largest $k$ elements without sorting them, can be solved
error-free for all $k\in [n]$. Additionally, we extend the dynamic data model
and show that most of these results still hold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8185</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8185</id><created>2014-12-28</created><authors><author><keyname>Kalinovsky</keyname><forenames>Yakiv O.</forenames></author><author><keyname>Boyarinova</keyname><forenames>Yuliya E.</forenames></author><author><keyname>Turenko</keyname><forenames>Alina S.</forenames></author><author><keyname>Khitsko</keyname><forenames>Yana V.</forenames></author></authors><title>Generalized quaternions and their relations with Grassmann-Clifford
  procedure of doubling</title><categories>cs.NA math.CV</categories><comments>arXiv admin note: substantial text overlap with arXiv:1409.3193</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The class of non-commutative hypercomplex number systems (HNS) of
4-dimension, constructed by using of non-commutative Grassmann-Clifford
procedure of doubling of 2-dimensional systems is investigated in the article
and established here are their relationships with the generalized quaternions.
Algorithms of performance of operations and methods of algebraic
characteristics calculation in them, such as conjugation, normalization, a type
of zero divisors are investigated. The considered arithmetic and algebraic
operations and procedures in this class HNS allow to use these HNS in
mathematical modeling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8197</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8197</id><created>2014-12-28</created><updated>2015-06-20</updated><authors><author><keyname>Bardosi</keyname><forenames>Z.</forenames></author><author><keyname>Granata</keyname><forenames>D.</forenames></author><author><keyname>Lugos</keyname><forenames>G.</forenames></author><author><keyname>Tafti</keyname><forenames>A. P.</forenames></author><author><keyname>Saxena</keyname><forenames>S.</forenames></author></authors><title>Metacarpal Bones Localization in X-ray Imagery Using Particle Filter
  Segmentation</title><categories>cs.CV</categories><comments>6 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistical methods such as sequential Monte Carlo Methods were proposed for
detection, segmentation and tracking of objects in digital images. A similar
approach, called Shape Particle Filters was introduced for the segmentation of
vertebra, lungs and hearts [1]. In this contribution, a global shape and a
local appearance model are derived from specific object annotated X-ray images
of the metacarpal bones. In the test data a unique labeling of the bone
boundary and the background points and a manual annotation is given. Using a
set of local features (Haar-like) in the neighborhood of each pixel a
probabilistic pixel classifier is built using the random forest algorithm. To
fit the shape model to a new image, a label probability map is extracted and
then the optimal shape is obtained by maximizing the probability of each
landmark with the Differential Evolution algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8222</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8222</id><created>2014-12-28</created><authors><author><keyname>Yang</keyname><forenames>Jianjun</forenames></author><author><keyname>Fei</keyname><forenames>Zongming</forenames></author><author><keyname>Shen</keyname><forenames>Ju</forenames></author></authors><title>Hole Detection and Shape-Free Representation and Double Landmarks Based
  Geographic Routing in Wireless Sensor Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In wireless sensor networks, an important issue of Geographic Routing is
local minimum problem, which is caused by hole that blocks the greedy
forwarding process. To avoid the long detour path, recent research focuses on
detecting the hole in advance, then the nodes located on the boundary of the
hole advertise the hole information to the nodes near the hole.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8225</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8225</id><created>2014-12-28</created><authors><author><keyname>Chen</keyname><forenames>Jiecao</forenames></author><author><keyname>Qin</keyname><forenames>Bo</forenames></author><author><keyname>Woodruff</keyname><forenames>David P.</forenames></author><author><keyname>Zhang</keyname><forenames>Qin</forenames></author></authors><title>A Sketching Algorithm for Spectral Graph Sparsification</title><categories>cs.DS</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of compressing a weighted graph $G$ on $n$ vertices,
building a &quot;sketch&quot; $H$ of $G$, so that given any vector $x \in \mathbb{R}^n$,
the value $x^T L_G x$ can be approximated up to a multiplicative $1+\epsilon$
factor from only $H$ and $x$, where $L_G$ denotes the Laplacian of $G$. One
solution to this problem is to build a spectral sparsifier $H$ of $G$, which,
using the result of Batson, Spielman, and Srivastava, consists of $O(n
\epsilon^{-2})$ reweighted edges of $G$ and has the property that
simultaneously for all $x \in \mathbb{R}^n$, $x^T L_H x = (1 \pm \epsilon) x^T
L_G x$. The $O(n \epsilon^{-2})$ bound is optimal for spectral sparsifiers. We
show that if one is interested in only preserving the value of $x^T L_G x$ for
a {\it fixed} $x \in \mathbb{R}^n$ (specified at query time) with high
probability, then there is a sketch $H$ using only $\tilde{O}(n
\epsilon^{-1.6})$ bits of space. This is the first data structure achieving a
sub-quadratic dependence on $\epsilon$. Our work builds upon recent work of
Andoni, Krauthgamer, and Woodruff who showed that $\tilde{O}(n \epsilon^{-1})$
bits of space is possible for preserving a fixed {\it cut query} (i.e., $x\in
\{0,1\}^n$) with high probability; here we show that even for a general query
vector $x \in \mathbb{R}^n$, a sub-quadratic dependence on $\epsilon$ is
possible. Our result for Laplacians is in sharp contrast to sketches for
general $n \times n$ positive semidefinite matrices $A$ with $O(\log n)$ bit
entries, for which even to preserve the value of $x^T A x$ for a fixed $x \in
\mathbb{R}^n$ (specified at query time) up to a $1+\epsilon$ factor with
constant probability, we show an $\Omega(n \epsilon^{-2})$ lower bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8246</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8246</id><created>2014-12-28</created><updated>2015-01-01</updated><authors><author><keyname>Chen</keyname><forenames>Shihyen</forenames></author><author><keyname>Wang</keyname><forenames>Zhuozhi</forenames></author><author><keyname>Zhang</keyname><forenames>Kaizhong</forenames></author></authors><title>Pattern Matching and Local Alignment for RNA Structures</title><categories>cs.DS</categories><comments>7 pages. V2: changed first names initials to full names in metadata.
  V3: added info of conference proceedings, updated email address</comments><journal-ref>Proceedings of the 2002 International Conference on Mathematics
  and Engineering Techniques in Medicine and Biological Sciences (METMBS),
  55-61, 2002</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The primary structure of a ribonucleic acid (RNA) molecule can be represented
as a sequence of nucleotides (bases) over the alphabet {A, C, G, U}. The
secondary or tertiary structure of an RNA is a set of base pairs which form
bonds between A-U and G-C. For secondary structures, these bonds have been
traditionally assumed to be one-to-one and non-crossing. This paper considers
pattern matching as well as local alignment between two RNA structures. For
pattern matching, we present two algorithms, one for obtaining an exact match,
the other for approximate match. We then present an algorithm for RNA local
structural alignment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8266</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8266</id><created>2014-12-29</created><authors><author><keyname>Passerat-Palmbach</keyname><forenames>Jonathan</forenames><affiliation>ISIMA, UBP, LIMOS</affiliation></author><author><keyname>Hill</keyname><forenames>David</forenames><affiliation>LIMOS, UBP, ISIMA</affiliation></author></authors><title>How to Correctly Deal With Pseudorandom Numbers in Manycore Environments
  - Application to GPU programming with Shoverand</title><categories>cs.DC</categories><proxy>ccsd</proxy><journal-ref>IEEE High Performance Computing and Simulation conference 2012,
  Jul 2012, Madrid, Spain. pp.25 - 31</journal-ref><doi>10.1109/HPCSim.2012.6266887</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic simulations are often sensitive to the source of randomness that
character-izes the statistical quality of their results. Consequently, we need
highly reliable Random Number Generators (RNGs) to feed such applications.
Recent developments try to shrink the computa-tion time by relying more and
more General Purpose Graphics Processing Units (GP-GPUs) to speed-up stochastic
simulations. Such devices bring new parallelization possibilities, but they
also introduce new programming difficulties. Since RNGs are at the base of any
stochastic simulation, they also need to be ported to GP-GPU. There is still a
lack of well-designed implementations of quality-proven RNGs on GP-GPU
platforms. In this paper, we introduce ShoveRand, a frame-work defining common
rules to generate random numbers uniformly on GP-GPU. Our framework is designed
to cope with any GPU-enabled development platform and to expose a
straightfor-ward interface to users. We also provide an existing RNG
implementation with this framework to demonstrate its efficiency in both
development and ease of use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8268</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8268</id><created>2014-12-29</created><updated>2015-03-28</updated><authors><author><keyname>Febres</keyname><forenames>Gerardo</forenames></author><author><keyname>Jaffe</keyname><forenames>Klaus</forenames></author></authors><title>A Fundamental Scale of Descriptions for Analyzing Information Content of
  Communication Systems</title><categories>cs.IT math.IT physics.soc-ph</categories><comments>29 pages, 2 Tables, 5 Figures</comments><journal-ref>Entropy 2015, 17(4), 1606-1633</journal-ref><doi>10.3390/e17041606</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The complexity of a system description is a function of the entropy of its
symbolic description. Prior to computing the entropy of the system description,
an observation scale has to be assumed. In natural language texts, typical
scales are binary, characters, and words. However, considering languages as
structures built around certain preconceived set of symbols, like words or
characters, is only a presumption. This study depicts the notion of the
Description Fundamental Scale as a set of symbols which serves to analyze the
essence a language structure. The concept of Fundamental Scale is tested using
English and MIDI music texts by means of an algorithm developed to search for a
set of symbols, which minimizes the system observed entropy, and therefore best
expresses the fundamental scale of the language employed. Test results show
that it is possible to find the Fundamental Scale of some languages. The
concept of Fundamental Scale, and the method for its determination, emerges as
an interesting tool to facilitate the study of languages and complex systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8281</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8281</id><created>2014-12-29</created><authors><author><keyname>Zhang</keyname><forenames>Lanbo</forenames></author></authors><title>Interactive Retrieval Based on Wikipedia Concepts</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new user feedback mechanism based on Wikipedia concepts
for interactive retrieval. In this mechanism, the system presents to the user a
group of Wikipedia concepts, and the user can choose those relevant to refine
his/her query. To realize this mechanism, we propose methods to address two
problems: 1) how to select a small number of possibly relevant Wikipedia
concepts to show the user, and 2) how to re-rank retrieved documents given the
user-identified Wikipedia concepts. Our methods are evaluated on three TREC
data sets. The experiment results show that our methods can dramatically
improve retrieval performances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8287</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8287</id><created>2014-12-29</created><authors><author><keyname>Wang</keyname><forenames>Junyan</forenames></author><author><keyname>Chan</keyname><forenames>Kap-Luk</forenames></author></authors><title>Rigid and Non-rigid Shape Evolutions for Shape Alignment and Recovery in
  Images</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The same type of objects in different images may vary in their shapes because
of rigid and non-rigid shape deformations, occluding foreground as well as
cluttered background. The problem concerned in this work is the shape
extraction in such challenging situations. We approach the shape extraction
through shape alignment and recovery. This paper presents a novel and general
method for shape alignment and recovery by using one example shapes based on
deterministic energy minimization. Our idea is to use general model of shape
deformation in minimizing active contour energies. Given \emph{a priori} form
of the shape deformation, we show how the curve evolution equation
corresponding to the shape deformation can be derived. The curve evolution is
called the prior variation shape evolution (PVSE). We also derive the
energy-minimizing PVSE for minimizing active contour energies. For shape
recovery, we propose to use the PVSE that deforms the shape while preserving
its shape characteristics. For choosing such shape-preserving PVSE, a theory of
shape preservability of the PVSE is established. Experimental results validate
the theory and the formulations, and they demonstrate the effectiveness of our
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8291</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8291</id><created>2014-12-29</created><authors><author><keyname>Karl</keyname><forenames>Maximilian</forenames></author><author><keyname>Osendorfer</keyname><forenames>Christian</forenames></author></authors><title>Improving approximate RPCA with a k-sparsity prior</title><categories>cs.NE cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A process centric view of robust PCA (RPCA) allows its fast approximate
implementation based on a special form o a deep neural network with weights
shared across all layers. However, empirically this fast approximation to RPCA
fails to find representations that are parsemonious. We resolve these bad local
minima by relaxing the elementwise L1 and L2 priors and instead utilize a
structure inducing k-sparsity prior. In a discriminative classification task
the newly learned representations outperform these from the original
approximate RPCA formulation significantly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8293</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8293</id><created>2014-12-29</created><updated>2015-08-09</updated><authors><author><keyname>Avron</keyname><forenames>Haim</forenames></author><author><keyname>Sindhwani</keyname><forenames>Vikas</forenames></author><author><keyname>Yang</keyname><forenames>Jiyan</forenames></author><author><keyname>Mahoney</keyname><forenames>Michael</forenames></author></authors><title>Quasi-Monte Carlo Feature Maps for Shift-Invariant Kernels</title><categories>stat.ML cs.LG math.NA stat.CO</categories><comments>A short version of this paper has been presented in ICML 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of improving the efficiency of randomized Fourier
feature maps to accelerate training and testing speed of kernel methods on
large datasets. These approximate feature maps arise as Monte Carlo
approximations to integral representations of shift-invariant kernel functions
(e.g., Gaussian kernel). In this paper, we propose to use Quasi-Monte Carlo
(QMC) approximations instead, where the relevant integrands are evaluated on a
low-discrepancy sequence of points as opposed to random point sets as in the
Monte Carlo approach. We derive a new discrepancy measure called box
discrepancy based on theoretical characterizations of the integration error
with respect to a given sequence. We then propose to learn QMC sequences
adapted to our setting based on explicit box discrepancy minimization. Our
theoretical analyses are complemented with empirical results that demonstrate
the effectiveness of classical and adaptive QMC techniques for this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8296</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8296</id><created>2014-12-29</created><authors><author><keyname>Li</keyname><forenames>Wenjun</forenames></author><author><keyname>Wang</keyname><forenames>Jianxin</forenames></author><author><keyname>Chen</keyname><forenames>Jianer</forenames></author><author><keyname>Cao</keyname><forenames>Yixin</forenames></author></authors><title>A $2k$-Vertex Kernel for Maximum Internal Spanning Tree</title><categories>cs.DS</categories><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the parameterized version of the maximum internal spanning tree
problem, which, given an $n$-vertex graph and a parameter $k$, asks for a
spanning tree with at least $k$ internal vertices. Fomin et al. [J. Comput.
System Sci., 79:1-6] crafted a very ingenious reduction rule, and showed that a
simple application of this rule is sufficient to yield a $3k$-vertex kernel.
Here we propose a novel way to use the same reduction rule, resulting in an
improved $2k$-vertex kernel. Our algorithm applies first a greedy procedure
consisting of a sequence of local exchange operations, which ends with a
local-optimal spanning tree, and then uses this special tree to find a
reducible structure. As a corollary of our kernel, we obtain a deterministic
algorithm for the problem running in time $4^k \cdot n^{O(1)}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8299</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8299</id><created>2014-12-29</created><authors><author><keyname>Langr</keyname><forenames>Daniel</forenames></author><author><keyname>&#x160;ime&#x10d;ek</keyname><forenames>Ivan</forenames></author><author><keyname>Tvrd&#xed;k</keyname><forenames>Pavel</forenames></author></authors><title>Loading Large Sparse Matrices Stored in Files in the Adaptive-Blocking
  Hierarchical Storage Format</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The parallel algorithm for loading large sparse matrices from files into
distributed memories of high performance computing (HPC) systems is presented.
This algorithm was designed specially for matrices stored in files in the
space-effcient adaptive-blocking hierarchical storage format (ABHSF). The
algorithm can be used even if matrix storing and loading procedures use a
different number of processes, different matrix-processes mapping, or different
in-memory storage format. The file format based on the utilization of the HDF5
library is described as well. Finally, the presented experimental study
evaluates the proposed algorithm empirically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8300</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8300</id><created>2014-12-29</created><updated>2014-12-29</updated><authors><author><keyname>Du</keyname><forenames>Guanyao</forenames></author><author><keyname>Xiong</keyname><forenames>Ke</forenames></author><author><keyname>Qiu</keyname><forenames>Zhengding</forenames></author></authors><title>Outage Analysis of Cooperative Transmission with Energy Harvesting
  Relay: Time Switching vs Power Splitting</title><categories>cs.IT math.IT</categories><comments>8 pages, 7 figures, with minor revision of Mathematical Problems in
  Engineering</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Recently, energy harvesting (EH) has emerged as a promising way to realize
green communications. In this paper, we investigate the multiuser transmission
network with an EH cooperative relay, where a source transmits independent
information to multiple destinations with the help of an energy constrained
relay. The relay can harvest energy from the radio frequency (RF) signals
transmitted from the source, and it helps the multiuser transmission only by
consuming the harvested energy. By adopting the time switching and the
power-splitting relay receiver architectures, we firstly propose two protocols,
the time-switching cooperative multiuser transmission (TSCMT) protocol and the
power-splitting cooperative multiuser transmission (PSCMT) protocol, to enable
the simultaneous information processing and EH at the relay for the system. To
evaluate the system performance, we theoretically analyze the system outage
probability for the two proposed protocols, and then derive explicit
expressions for each of them, respectively. Moreover, we also discuss the
effects of system configuration parameters, such as the source power and relay
location on the system performance. Numerical results are provided to
demonstrate the accuracy of our analytical results and reveal that compared
with traditional non-cooperative scheme, our proposed protocols are green
solutions to offer reliable communication and lower system outage probability
without consuming additional energy. In particular, for the same transmit power
at the source, the PSCMT protocol is superior to the TSCMT protocol to obtain
lower system outage probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8305</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8305</id><created>2014-12-29</created><authors><author><keyname>Xiong</keyname><forenames>Ke</forenames></author><author><keyname>Fan</keyname><forenames>Pingyi</forenames></author><author><keyname>Zhang</keyname><forenames>Chuang</forenames></author><author><keyname>Letaief</keyname><forenames>Khaled Ben</forenames></author></authors><title>Wireless Information and Energy Transfer for Two-Hop Non-Regenerative
  MIMO-OFDM Relay Networks</title><categories>cs.IT math.IT</categories><comments>16 pages, 12 figures, to appear in IEEE Selected Areas in
  Communications</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper investigates the simultaneous wireless information and energy
transfer for the non-regenerative multipleinput multiple-output orthogonal
frequency-division multiplexing (MIMO-OFDM) relaying system. By considering two
practical receiver architectures, we present two protocols, time switchingbased
relaying (TSR) and power splitting-based relaying (PSR). To explore the system
performance limit, we formulate two optimization problems to maximize the
end-to-end achievable information rate with the full channel state information
(CSI) assumption. Since both problems are non-convex and have no known solution
method, we firstly derive some explicit results by theoretical analysis and
then design effective algorithms for them. Numerical results show that the
performances of both protocols are greatly affected by the relay position.
Specifically, PSR and TSR show very different behaviors to the variation of
relay position. The achievable information rate of PSR monotonically decreases
when the relay moves from the source towards the destination, but for TSR, the
performance is relatively worse when the relay is placed in the middle of the
source and the destination. This is the first time to observe such a
phenomenon. In addition, it is also shown that PSR always outperforms TSR in
such a MIMO-OFDM relaying system. Moreover, the effect of the number of
antennas and the number of subcarriers are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8307</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8307</id><created>2014-12-29</created><updated>2015-07-22</updated><authors><author><keyname>McDonnell</keyname><forenames>Mark D.</forenames></author><author><keyname>Tissera</keyname><forenames>Migel D.</forenames></author><author><keyname>Vladusich</keyname><forenames>Tony</forenames></author><author><keyname>van Schaik</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Tapson</keyname><forenames>Jonathan</forenames></author></authors><title>Fast, simple and accurate handwritten digit classification by training
  shallow neural network classifiers with the 'extreme learning machine'
  algorithm</title><categories>cs.NE cs.CV cs.LG</categories><comments>Accepted for publication; 9 pages of text, 6 figures and 1 table</comments><doi>10.1371/journal.pone.0134254</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in training deep (multi-layer) architectures have inspired a
renaissance in neural network use. For example, deep convolutional networks are
becoming the default option for difficult tasks on large datasets, such as
image and speech recognition. However, here we show that error rates below 1%
on the MNIST handwritten digit benchmark can be replicated with shallow
non-convolutional neural networks. This is achieved by training such networks
using the 'Extreme Learning Machine' (ELM) approach, which also enables a very
rapid training time (~10 minutes). Adding distortions, as is common practise
for MNIST, reduces error rates even further. Our methods are also shown to be
capable of achieving less than 5.5% error rates on the NORB image database. To
achieve these results, we introduce several enhancements to the standard ELM
algorithm, which individually and in combination can significantly improve
performance. The main innovation is to ensure each hidden-unit operates only on
a randomly sized and positioned patch of each image. This form of random
`receptive field' sampling of the input ensures the input weight matrix is
sparse, with about 90% of weights equal to zero. Furthermore, combining our
methods with a small number of iterations of a single-batch backpropagation
method can significantly reduce the number of hidden-units required to achieve
a particular performance. Our close to state-of-the-art results for MNIST and
NORB suggest that the ease of use and accuracy of the ELM algorithm for
designing a single-hidden-layer neural network classifier should cause it to be
given greater consideration either as a standalone method for simpler problems,
or as the final classification stage in deep neural networks applied to more
difficult problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8313</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8313</id><created>2014-12-29</created><updated>2015-01-03</updated><authors><author><keyname>Du</keyname><forenames>Guanyao</forenames></author><author><keyname>Dong</keyname><forenames>Zhilong</forenames></author><author><keyname>Xiong</keyname><forenames>Ke</forenames></author><author><keyname>Qiu</keyname><forenames>Zhengding</forenames></author></authors><title>Wireless Information and Energy Transfer for Decode-and-Forward Relaying
  MIMO-OFDM Networks</title><categories>cs.IT math.IT</categories><comments>7 pages, 3 Figures, to appear in ICIC Express Letter</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper investigates the system achievable rate and optimization for the
multiple-input multiple-output (MIMO)-orthogonal frequency division
multiplexing (OFDM) system with an energy harvesting (EH) relay. Firstly we
propose a time switchingbased relaying (TSR) protocol to enable the
simultaneous information processing and energy harvesting at the relay. Then,
we discuss its achievable rate performance theoretically and formulated an
optimization problem to maximize the system achievable rate. As the problem is
difficult to solve, we design an Augmented Lagrangian Penalty Function (ALPF)
method for it. Extensive simulation results are provided to demonstrate the
accuracy of the analytical results and the effectiveness of the ALPF method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8319</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8319</id><created>2014-12-29</created><updated>2015-10-14</updated><authors><author><keyname>Dro&#x17c;d&#x17c;</keyname><forenames>Stanis&#x142;aw</forenames></author><author><keyname>O&#x15b;wi&#x119;cimka</keyname><forenames>Pawe&#x142;</forenames></author><author><keyname>Kulig</keyname><forenames>Andrzej</forenames></author><author><keyname>Kwapie&#x144;</keyname><forenames>Jaros&#x142;aw</forenames></author><author><keyname>Bazarnik</keyname><forenames>Katarzyna</forenames></author><author><keyname>Grabska-Gradzi&#x144;ska</keyname><forenames>Iwona</forenames></author><author><keyname>Rybicki</keyname><forenames>Jan</forenames></author><author><keyname>Stanuszek</keyname><forenames>Marek</forenames></author></authors><title>Quantifying origin and character of long-range correlations in narrative
  texts</title><categories>cs.CL physics.soc-ph</categories><comments>28 pages, 8 figures, accepted for publication in Information Sciences</comments><journal-ref>Information Sciences 331 (2016) 32-44</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In natural language using short sentences is considered efficient for
communication. However, a text composed exclusively of such sentences looks
technical and reads boring. A text composed of long ones, on the other hand,
demands significantly more effort for comprehension. Studying characteristics
of the sentence length variability (SLV) in a large corpus of world-famous
literary texts shows that an appealing and aesthetic optimum appears somewhere
in between and involves selfsimilar, cascade-like alternation of various
lengths sentences. A related quantitative observation is that the power spectra
S(f) of thus characterized SLV universally develop a convincing `1/f^beta'
scaling with the average exponent beta =~ 1/2, close to what has been
identified before in musical compositions or in the brain waves. An
overwhelming majority of the studied texts simply obeys such fractal attributes
but especially spectacular in this respect are hypertext-like, &quot;stream of
consciousness&quot; novels. In addition, they appear to develop structures
characteristic of irreducibly interwoven sets of fractals called multifractals.
Scaling of S(f) in the present context implies existence of the long-range
correlations in texts and appearance of multifractality indicates that they
carry even a nonlinear component. A distinct role of the full stops in inducing
the long-range correlations in texts is evidenced by the fact that the above
quantitative characteristics on the long-range correlations manifest themselves
in variation of the full stops recurrence times along texts, thus in SLV, but
to a much lesser degree in the recurrence times of the most frequent words. In
this latter case the nonlinear correlations, thus multifractality, disappear
even completely for all the texts considered. Treated as one extra word, the
full stops at the same time appear to obey the Zipfian rank-frequency
distribution, however.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8324</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8324</id><created>2014-12-29</created><updated>2015-03-26</updated><authors><author><keyname>Lin</keyname><forenames>Haoxiang</forenames></author></authors><title>A Constructive Proof On the Compositionality of Linearizability</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linearizability is the strongest correctness property for both shared memory
and message passing concurrent systems. One promising nature of linearizability
is the compositionality: a history(execution) is linearizable if and only if
each object subhistory is linearizable, which is instructive in that we are
able to design, implement and test a whole system from the bottom up. In this
paper, we propose a new methodology for system model that histories are defined
to be special well-ordered structures. The new methodology covers not only
finite executions as previous work does, but also infinite ones in reactive
systems that never stop. Then, we present a new constructive proof on the
compositionality of linearizability inspired by merge sort algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8338</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8338</id><created>2014-12-29</created><authors><author><keyname>S.</keyname><forenames>Neethi K.</forenames></author><author><keyname>Saxena</keyname><forenames>Sanjeev</forenames></author></authors><title>Maximum Cardinality Neighbourly Sets in Quadrilateral Free Graphs</title><categories>cs.DS math.CO</categories><doi>10.1007/s10878-015-9972-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neighbourly set of a graph is a subset of edges which either share an end
point or are joined by an edge of that graph. The maximum cardinality
neighbourly set problem is known to be NP-complete for general graphs. Mahdian
(M.Mahdian, On the computational complexity of strong edge coloring, Discrete
Applied Mathematics, 118:239-248, 2002) proved that it is in polynomial time
for quadrilateral-free graphs and proposed an O(n^{11}) algorithm for the same
(along with a note that by a straightforward but lengthy argument it can be
proved to be solvable in O(n^5) running time). In this paper we propose an
O(n^2) time algorithm for finding a maximum cardinality neighbourly set in a
quadrilateral-free graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8339</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8339</id><created>2014-12-29</created><updated>2015-06-08</updated><authors><author><keyname>Perera</keyname><forenames>Charith</forenames></author><author><keyname>Ranjan</keyname><forenames>Rajiv</forenames></author><author><keyname>Wang</keyname><forenames>Lizhe</forenames></author><author><keyname>Khan</keyname><forenames>Samee U.</forenames></author><author><keyname>Zomaya</keyname><forenames>Albert Y.</forenames></author></authors><title>Big Data Privacy in the Internet of Things Era</title><categories>cs.CY cs.DB cs.NI</categories><comments>Accepted to be published in IEEE IT Professional Magazine: Special
  Issue Internet of Anything 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the last few years, we have seen a plethora of Internet of Things (IoT)
solutions, products and services, making their way into the industry's
market-place. All such solution will capture a large amount of data pertaining
to the environment, as well as their users. The objective of the IoT is to
learn more and to serve better the system users. Some of these solutions may
store the data locally on the devices ('things'), and others may store in the
Cloud. The real value of collecting data comes through data processing and
aggregation in large-scale where new knowledge can be extracted. However, such
procedures can also lead to user privacy issues. This article discusses some of
the main challenges of privacy in IoT, and opportunities for research and
innovation. We also introduce some of the ongoing research efforts that address
IoT privacy issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8340</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8340</id><created>2014-12-29</created><authors><author><keyname>Kammoun</keyname><forenames>Abla</forenames></author><author><keyname>Alouini</keyname><forenames>M. S.</forenames></author></authors><title>On the Smallest Eigenvalue of General correlated Gaussian Matrices</title><categories>cs.IT math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the behaviour of the spectrum of generally correlated
Gaussian random matrices whose columns are zero-mean independent vectors but
have different correlations, under the specific regime where the number of
their columns and that of their rows grow at infinity with the same pace. This
work is, in particular, motivated by applications from statistical signal
processing and wireless communications, where this kind of matrices naturally
arise. Following the approach proposed in [1], we prove that under some
specific conditions, the smallest singular value of generally correlated
Gaussian matrices is almost surely away from zero.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8341</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8341</id><created>2014-12-29</created><authors><author><keyname>H&#xe1;la</keyname><forenames>Pavel</forenames></author></authors><title>Spectral classification using convolutional neural networks</title><categories>cs.CV astro-ph.IM cs.NE</categories><comments>71 pages, 50 figures, Master's thesis, Masaryk University</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  There is a great need for accurate and autonomous spectral classification
methods in astrophysics. This thesis is about training a convolutional neural
network (ConvNet) to recognize an object class (quasar, star or galaxy) from
one-dimension spectra only. Author developed several scripts and C programs for
datasets preparation, preprocessing and postprocessing of the data. EBLearn
library (developed by Pierre Sermanet and Yann LeCun) was used to create
ConvNets. Application on dataset of more than 60000 spectra yielded success
rate of nearly 95%. This thesis conclusively proved great potential of
convolutional neural networks and deep learning methods in astrophysics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8344</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8344</id><created>2014-12-29</created><authors><author><keyname>Kammoun</keyname><forenames>Abla</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>The random matrix regime of Maronna's M-estimator for observations
  corrupted by elliptical noises</title><categories>cs.IT math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article studies the behavior of the Maronna robust scatter estimator
$\hat{C}_N\in \mathbb{C}^{N\times N}$ of a sequence of observations
$y_1,...,y_n$ which is composed of a $K$ dimensional signal drown in a heavy
tailed noise, i.e $y_i=A_N s_i+x_i$ where $A_N \in \mathbb{C}^{N\times K}$ and
$x_i$ is drawn from elliptical distribution.
  In particular, we prove that as the population dimension $N$, the number of
observations $n$ and the rank of $A_N$ grow to infinity at the same pace and
under some mild assumptions, the robust scatter matrix can be characterized by
a random matrix $\hat{S}_N$ that follows a standard random model. Our analysis
can be very useful for many applications of the fields of statistical inference
and signal processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8347</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8347</id><created>2014-12-29</created><authors><author><keyname>Buchbinder</keyname><forenames>Niv</forenames><affiliation>Seffi</affiliation></author><author><keyname>Chen</keyname><forenames>Shahar</forenames><affiliation>Seffi</affiliation></author><author><keyname>Gupta</keyname><forenames>Anupam</forenames><affiliation>Seffi</affiliation></author><author><keyname>Nagarajan</keyname><forenames>Viswanath</forenames><affiliation>Seffi</affiliation></author><author><keyname>Joseph</keyname><affiliation>Seffi</affiliation></author><author><keyname>Naor</keyname></author></authors><title>Online Packing and Covering Framework with Convex Objectives</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider online fractional covering problems with a convex objective,
where the covering constraints arrive over time. Formally, we want to solve
$\min\,\{f(x) \mid Ax\ge \mathbf{1},\, x\ge 0\},$ where the objective function
$f:\mathbb{R}^n\rightarrow \mathbb{R}$ is convex, and the constraint matrix
$A_{m\times n}$ is non-negative. The rows of $A$ arrive online over time, and
we wish to maintain a feasible solution $x$ at all times while only increasing
coordinates of $x$. We also consider &quot;dual&quot; packing problems of the form
$\max\,\{c^\intercal y - g(\mu) \mid A^\intercal y \le \mu,\, y\ge 0\}$, where
$g$ is a convex function. In the online setting, variables $y$ and columns of
$A^\intercal$ arrive over time, and we wish to maintain a non-decreasing
solution $(y,\mu)$.
  We provide an online primal-dual framework for both classes of problems with
competitive ratio depending on certain &quot;monotonicity&quot; and &quot;smoothness&quot;
parameters of $f$; our results match or improve on guarantees for some special
classes of functions $f$ considered previously.
  Using this fractional solver with problem-dependent randomized rounding
procedures, we obtain competitive algorithms for the following problems: online
covering LPs minimizing $\ell_p$-norms of arbitrary packing constraints, set
cover with multiple cost functions, capacity constrained facility location,
capacitated multicast problem, set cover with set requests, and profit
maximization with non-separable production costs. Some of these results are new
and others provide a unified view of previous results, with matching or
slightly worse competitive ratios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8356</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8356</id><created>2014-12-29</created><updated>2015-09-09</updated><authors><author><keyname>Naor</keyname><forenames>Moni</forenames></author><author><keyname>Yogev</keyname><forenames>Eylon</forenames></author></authors><title>Bloom Filters in Adversarial Environments</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many efficient data structures use randomness, allowing them to improve upon
deterministic ones. Usually, their efficiency and/or correctness are analyzed
using probabilistic tools under the assumption that the inputs and queries are
independent of the internal randomness of the data structure. In this work, we
consider data structures in a more robust model, which we call the adversarial
model. Roughly speaking, this model allows an adversary to choose inputs and
queries adaptively according to previous responses. Specifically, we consider a
data structure known as &quot;Bloom filter&quot; and prove a tight connection between
Bloom filters in this model and cryptography.
  A Bloom filter represents a set $S$ of elements approximately, by using fewer
bits than a precise representation. The price for succinctness is allowing some
errors: for any $x \in S$ it should always answer 'Yes', and for any $x \notin
S$ it should answer 'Yes' only with small probability.
  In the adversarial model, we consider both efficient adversaries (that run in
polynomial time) and computationally unbounded adversaries that are only
bounded in the amount of queries they can make. For computationally bounded
adversaries, we show that non-trivial (memory-wise) Bloom filters exist if and
only if one-way functions exist. For unbounded adversaries we show that there
exists a Bloom filter for sets of size $n$ and error \varepsilon, that is
secure against t queries and uses only $O(n \log{\frac{1}{\varepsilon}} + t)$
bits of memory. In comparison, $n\log{\frac{1}{\varepsilon}}$ is the best
possible under a non-adaptive adversary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8358</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8358</id><created>2014-12-29</created><updated>2015-08-31</updated><authors><author><keyname>Wang</keyname><forenames>Tao</forenames></author><author><keyname>Zhao</keyname><forenames>Xiaodan</forenames></author></authors><title>Odd graph and its applications on the strong edge coloring</title><categories>math.CO cs.DM</categories><comments>8 pages, reorganized the section 2 and 3</comments><msc-class>05C15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A strong edge coloring of a graph is a proper edge coloring in which every
color class is an induced matching. The strong chromatic index $\chiup_{s}'(G)$
of a graph $G$ is the minimum number of colors in a strong edge coloring of
$G$. Let $\Delta \geq 4$ be an integer. In this note, we study the properties
of the odd graphs, and show that every planar graph with maximum degree at most
$\Delta$ and girth at least $10 \Delta - 4$ has a strong edge coloring with
$2\Delta - 1$ colors. In addition, we prove that if $G$ is a graph with girth
at least $2\Delta - 1$ and $\mad(G) &lt; 2 + \frac{1}{3\Delta - 2}$, where $\Delta
\geq 4$, then $\chiup_{s}'(G) \leq 2\Delta - 1$; if $G$ is a subcubic graph
with girth at least $8$ and $\mad(G) &lt; 2 + \frac{2}{23}$, then $\chiup_{s}'(G)
\leq 5$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8363</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8363</id><created>2014-12-29</created><updated>2015-12-18</updated><authors><author><keyname>Berlinkov</keyname><forenames>Mikhail</forenames></author><author><keyname>Szyku&#x142;a</keyname><forenames>Marek</forenames></author></authors><title>Algebraic synchronization criterion and computing reset words</title><categories>cs.FL</categories><comments>18 pages, 2 figures</comments><acm-class>F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We refine a uniform algebraic approach for deriving upper bounds on reset
thresholds of synchronizing automata. We express the condition that an
automaton is synchronizing in terms of linear algebra, and obtain upper bounds
for the reset thresholds of automata with a short word of a small rank. The
results are applied to make several improvements in the area.
  We improve the best general upper bound for reset thresholds of finite prefix
codes (Huffman codes): we show that an $n$-state synchronizing decoder has a
reset word of length at most $O(n \log^3 n)$. In addition to that, we prove
that the expected reset threshold of a uniformly random synchronizing binary
$n$-state decoder is at most $O(n \log n)$. We also show that for any non-unary
alphabet there exist decoders whose reset threshold is in $\varTheta(n)$.
  We prove the \v{C}ern\'{y} conjecture for $n$-state automata with a letter of
rank at most $\sqrt[3]{6n-6}$. In another corollary, based on the recent
results of Nicaud, we show that the probability that the \v{C}ern\'y conjecture
does not hold for a random synchronizing binary automaton is exponentially
small in terms of the number of states, and also that the expected value of the
reset threshold of an $n$-state random synchronizing binary automaton is at
most $n^{3/2+o(1)}$.
  Moreover, reset words of lengths within all of our bounds are computable in
polynomial time. We present suitable algorithms for this task for various
classes of automata, such as (quasi-)one-cluster and (quasi-)Eulerian automata,
for which our results can be applied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8369</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8369</id><created>2014-12-29</created><updated>2016-01-25</updated><authors><author><keyname>Jacobs</keyname><forenames>Henry O.</forenames></author><author><keyname>Vasudevan</keyname><forenames>Ram</forenames></author></authors><title>Qualitatively accurate spectral schemes for advection and transport</title><categories>cs.SY</categories><msc-class>65M70, 37A15, 58J30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The transport and continuum equations exhibit a number of conservation laws.
For example, scalar multiplication is conserved by the transport equation,
while positivity of probabilities is conserved by the continuum equation.
Certain discretization techniques, such as particle based methods, conserve
these properties, but converge slower than spectral discretization methods on
smooth data. Standard spectral discretization methods, on the other hand, do
not conserve the invariants of the transport equation and the continuum
equation. This article constructs a novel spectral discretization technique
that conserves these important invariants while simultaneously preserving
spectral convergence rates. The performance of this proposed method is
illustrated on several numerical experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8375</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8375</id><created>2014-12-29</created><authors><author><keyname>Zhu</keyname><forenames>Xingzheng</forenames></author><author><keyname>Yang</keyname><forenames>Bo</forenames></author><author><keyname>Chen</keyname><forenames>Cailian</forenames></author><author><keyname>Xue</keyname><forenames>Liang</forenames></author><author><keyname>Guan</keyname><forenames>Xinping</forenames></author><author><keyname>Wu</keyname><forenames>Fan</forenames></author></authors><title>Cross-Layer Scheduling for OFDMA-based Cognitive Radio Systems with
  Delay and Security Constraints</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the resource allocation problem in an Orthogonal
Frequency Division Multiple Access (OFDMA) based cognitive radio (CR) network,
where the CR base station adopts full overlay scheme to transmit both private
and open information to multiple users with average delay and power
constraints. A stochastic optimization problem is formulated to develop flow
control and radio resource allocation in order to maximize the long-term system
throughput of open and private information in CR system and ensure the
stability of primary system. The corresponding optimal condition for employing
full overlay is derived in the context of concurrent transmission of open and
private information. An online resource allocation scheme is designed to adapt
the transmission of open and private information based on monitoring the status
of primary system as well as the channel and queue states in the CR network.
The scheme is proven to be asymptotically optimal in solving the stochastic
optimization problem without knowing any statistical information. Simulations
are provided to verify the analytical results and efficiency of the scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8380</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8380</id><created>2014-12-29</created><updated>2015-03-29</updated><authors><author><keyname>Shimodaira</keyname><forenames>Hidetoshi</forenames></author></authors><title>A simple coding for cross-domain matching with dimension reduction via
  spectral graph embedding</title><categories>stat.ML cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data vectors are obtained from multiple domains. They are feature vectors of
images or vector representations of words. Domains may have different numbers
of data vectors with different dimensions. These data vectors from multiple
domains are projected to a common space by linear transformations in order to
search closely related vectors across domains. We would like to find projection
matrices to minimize distances between closely related data vectors. This
formulation of cross-domain matching is regarded as an extension of the
spectral graph embedding to multi-domain setting, and it includes several
multivariate analysis methods of statistics such as multiset canonical
correlation analysis, correspondence analysis, and principal component
analysis. Similar approaches are very popular recently in pattern recognition
and vision. In this paper, instead of proposing a novel method, we will
introduce an embarrassingly simple idea of coding the data vectors for
explaining all the above mentioned approaches. A data vector is concatenated
with zero vectors from all other domains to make an augmented vector. The
cross-domain matching is solved by applying the single-domain version of
spectral graph embedding to these augmented vectors of all the domains. An
interesting connection to the classical associative memory model of neural
networks is also discussed by noticing a coding for association. A
cross-validation method for choosing the dimension of the common space and a
regularization parameter will be discussed in an illustrative numerical
example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8388</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8388</id><created>2014-12-29</created><updated>2015-07-29</updated><authors><author><keyname>Kivel&#xe4;</keyname><forenames>Mikko</forenames></author><author><keyname>Porter</keyname><forenames>Mason A.</forenames></author></authors><title>Estimating inter-event time distributions from finite observation
  periods in communication networks</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>11 pages, 7 figures; Software used for the analysis:
  http://github.com/bolozna/iet</comments><doi>10.1103/PhysRevE.92.052813</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A diverse variety of processes --- including recurrent disease episodes,
neuron firing, and communication patterns among humans --- can be described
using inter-event time (IET) distributions. Many such processes are ongoing,
although event sequences are only available during a finite observation window.
Because the observation time window is more likely to begin or end during long
IETs than during short ones, the analysis of such data is susceptible to a bias
induced by the finite observation period. In this paper, we illustrate how this
length bias is born and how it can be corrected without assuming any particular
shape for the IET distribution. To do this, we model event sequences using
stationary renewal processes, and we formulate simple heuristics for
determining the severity of the bias. To illustrate our results, we focus on
the example of empirical communication networks, which are temporal networks
that are constructed from communication events. The IET distributions of such
systems guide efforts to build models of human behavior, and the variance of
IETs is very important for estimating the spreading rate of information in
networks of temporal interactions. We analyze several well-known data sets from
the literature, and we find that the resulting bias can lead to systematic
underestimates of the variance in the IET distributions and that correcting for
the bias can lead to qualitatively different results for the tails of the IET
distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8395</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8395</id><created>2014-12-29</created><authors><author><keyname>Saeid</keyname><forenames>A. Borumand</forenames></author><author><keyname>Fatemidokht</keyname><forenames>H.</forenames></author><author><keyname>Flaut</keyname><forenames>C.</forenames></author><author><keyname>Rafsanjani</keyname><forenames>M. Kuchaki</forenames></author></authors><title>On Codes based on BCK-algebras</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present some new connections between BCK- algebras and
binary block codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8412</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8412</id><created>2014-12-29</created><updated>2014-12-31</updated><authors><author><keyname>Alaggan</keyname><forenames>Mohammad</forenames></author><author><keyname>Gambs</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Matwin</keyname><forenames>Stan</forenames></author><author><keyname>Souza</keyname><forenames>Eriko</forenames></author><author><keyname>Tuhin</keyname><forenames>Mohammed</forenames></author></authors><title>Sanitization of Call Detail Records via Differentially-private Summaries</title><categories>cs.CR</categories><comments>Withdrawn due to some possible agreement issues</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we initiate the study of human mobility from sanitized call
detail records (CDRs). Such data can be extremely valuable to solve important
societal issues such as the improvement of urban transportation or the
understanding on the spread of diseases. One of the fundamental building block
for such study is the computation of mobility patterns summarizing how
individuals move during a given period from one area e.g., cellular tower or
administrative district) to another. However, such knowledge cannot be
published directly as it has been demonstrated that the access to this type of
data enable the (re-)identification of individuals. To answer this issue and to
foster the development of such applications in a privacy-preserving manner, we
propose in this paper a novel approach in which CDRs are summarized under the
form of a differentially-private Bloom filter for the purpose of privately
counting the number of mobile service users moving from one area (region) to
another in a given time frame. Our sanitization method is both time and space
efficient, and ensures differential privacy while solving the shortcomings of a
solution recently proposed to this problem. We also report on experiments
conducted with the proposed solution using a real life CDRs dataset. The
results obtained show that our method achieves - in most cases - a performance
similar to another method (linear counting sketch) that does not provide any
privacy guarantees. Thus, we conclude that our method maintains a high utility
while providing strong privacy guarantees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8415</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8415</id><created>2014-12-29</created><authors><author><keyname>Ordentlich</keyname><forenames>Or</forenames></author><author><keyname>Shayevitz</keyname><forenames>Ofer</forenames></author></authors><title>An Upper Bound on the Sizes of Multiset-Union-Free Families</title><categories>math.CO cs.DM cs.IT math.IT</categories><comments>A shorter ISIT conference version titled &quot;VC-Dimension Based Outer
  Bound on the Zero-Error Capacity of the Binary Adder Channel&quot; is available</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\mathcal{F}_1$ and $\mathcal{F}_2$ be two families of subsets of an
$n$-element set. We say that $\mathcal{F}_1$ and $\mathcal{F}_2$ are
multiset-union-free if for any $A,B\in \mathcal{F}_1$ and $C,D\in
\mathcal{F}_2$ the multisets $A\uplus C$ and $B\uplus D$ are different, unless
both $A = B$ and $C= D$. We derive a new upper bound on the maximal sizes of
multiset-union-free pairs, improving a result of Urbanke and Li.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8416</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8416</id><created>2014-12-29</created><authors><author><keyname>Sardellitti</keyname><forenames>Stefania</forenames></author><author><keyname>Scutari</keyname><forenames>Gesualdo</forenames></author><author><keyname>Barbarossa</keyname><forenames>Sergio</forenames></author></authors><title>Joint Optimization of Radio and Computational Resources for Multicell
  Mobile-Edge Computing</title><categories>cs.NI cs.IT math.IT</categories><comments>Paper submitted to IEEE Trans. on Signal and Information Processing
  over Networks</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Migrating computational intensive tasks from mobile devices to more
resourceful cloud servers is a promising technique to increase the
computational capacity of mobile devices while saving their battery energy. In
this paper, we consider a MIMO multicell system where multiple mobile users
(MUs) ask for computation offloading to a common cloud server. We formulate the
offloading problem as the joint optimization of the radio resources-the
transmit precoding matrices of the MUs-and the computational resources-the CPU
cycles/second assigned by the cloud to each MU-in order to minimize the overall
users' energy consumption, while meeting latency constraints. The resulting
optimization problem is nonconvex (in the objective function and constraints).
Nevertheless, in the single-user case, we are able to express the global
optimal solution in closed form. In the more challenging multiuser scenario, we
propose an iterative algorithm, based on a novel successive convex
approximation technique, converging to a local optimal solution of the original
nonconvex problem. Then, we reformulate the algorithm in a distributed and
parallel implementation across the radio access points, requiring only a
limited coordination/signaling with the cloud. Numerical results show that the
proposed schemes outperform disjoint optimization algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8419</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8419</id><created>2014-12-29</created><updated>2015-04-10</updated><authors><author><keyname>Lebret</keyname><forenames>Remi</forenames></author><author><keyname>Pinheiro</keyname><forenames>Pedro O.</forenames></author><author><keyname>Collobert</keyname><forenames>Ronan</forenames></author></authors><title>Simple Image Description Generator via a Linear Phrase-Based Approach</title><categories>cs.CL cs.CV cs.NE</categories><comments>Accepted as a workshop paper at ICLR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generating a novel textual description of an image is an interesting problem
that connects computer vision and natural language processing. In this paper,
we present a simple model that is able to generate descriptive sentences given
a sample image. This model has a strong focus on the syntax of the
descriptions. We train a purely bilinear model that learns a metric between an
image representation (generated from a previously trained Convolutional Neural
Network) and phrases that are used to described them. The system is then able
to infer phrases from a given image sample. Based on caption syntax statistics,
we propose a simple language model that can produce relevant descriptions for a
given test image using the phrases inferred. Our approach, which is
considerably simpler than state-of-the-art models, achieves comparable results
on the recently release Microsoft COCO dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8420</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8420</id><created>2014-12-29</created><authors><author><keyname>Li</keyname><forenames>Tiancheng</forenames></author></authors><title>Return on citation: a consistent metric to evaluate papers, journals and
  researchers</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evaluating and comparing the academic performance of a journal, a researcher
or a single paper has long remained a critical, necessary but also
controversial issue. Most of existing metrics invalidate comparison across
different fields of science or even between different types of papers in the
same field. This paper proposes a new metric, called return on citation (ROC),
which is simply a citation ratio but applies to evaluating the paper, the
journal and the researcher in a consistent way, allowing comparison across
different fields of science and between different types of papers and
discouraging unnecessary and coercive/self-citation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8439</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8439</id><created>2014-12-29</created><updated>2015-04-25</updated><authors><author><keyname>Fanti</keyname><forenames>Giulia</forenames></author><author><keyname>Kairouz</keyname><forenames>Peter</forenames></author><author><keyname>Oh</keyname><forenames>Sewoong</forenames></author><author><keyname>Viswanath</keyname><forenames>Pramod</forenames></author></authors><title>Spy vs. Spy: Rumor Source Obfuscation</title><categories>cs.SI cs.LG</categories><comments>14 pages 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Anonymous messaging platforms, such as Secret and Whisper, have emerged as
important social media for sharing one's thoughts without the fear of being
judged by friends, family, or the public. Further, such anonymous platforms are
crucial in nations with authoritarian governments; the right to free expression
and sometimes the personal safety of the author of the message depend on
anonymity. Whether for fear of judgment or personal endangerment, it is crucial
to keep anonymous the identity of the user who initially posted a sensitive
message. In this paper, we consider an adversary who observes a snapshot of the
spread of a message at a certain time. Recent advances in rumor source
detection shows that the existing messaging protocols are vulnerable against
such an adversary. We introduce a novel messaging protocol, which we call
adaptive diffusion, and show that it spreads the messages fast and achieves a
perfect obfuscation of the source when the underlying contact network is an
infinite regular tree: all users with the message are nearly equally likely to
have been the origin of the message. Experiments on a sampled Facebook network
show that it effectively hides the location of the source even when the graph
is finite, irregular and has cycles. We further consider a stronger adversarial
model where a subset of colluding users track the reception of messages. We
show that the adaptive diffusion provides a strong protection of the anonymity
of the source even under this scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8447</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8447</id><created>2014-12-29</created><updated>2015-09-16</updated><authors><author><keyname>Voronin</keyname><forenames>Sergey</forenames></author><author><keyname>Martinsson</keyname><forenames>Per-Gunnar</forenames></author></authors><title>Efficient Algorithms for CUR and Interpolative Matrix Decompositions</title><categories>math.NA cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The manuscript describes efficient algorithms for the computation of the CUR
and ID decompositions. The methods used are based on simple modifications to
the classical truncated pivoted QR decomposition, which means that highly
optimized library codes can be utilized for implementation. For certain
applications, further acceleration can be attained by incorporating techniques
based on randomized projections. Numerical experiments demonstrate advantageous
performance compared to existing techniques for computing CUR factorizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8461</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8461</id><created>2014-12-29</created><updated>2016-01-06</updated><authors><author><keyname>Liu</keyname><forenames>Yanhong A.</forenames></author><author><keyname>Stoller</keyname><forenames>Scott D.</forenames></author><author><keyname>Lin</keyname><forenames>Bo</forenames></author></authors><title>From Clarity to Efficiency for Distributed Algorithms</title><categories>cs.PL cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article describes a very high-level language for clear description of
distributed algorithms and optimizations necessary for generating efficient
implementations. The language supports high-level control flows where complex
synchronization conditions can be expressed using high-level queries,
especially logic quantifications, over message history sequences.
Unfortunately, the programs would be extremely inefficient, including consuming
unbounded memory, if executed straightforwardly.
  We present new optimizations that automatically transform complex
synchronization conditions into incremental updates of necessary auxiliary
values as messages are sent and received. The core of the optimizations is the
first general method for efficient implementation of logic quantifications. We
have developed an operational semantics of the language, implemented a
prototype of the compiler and the optimizations, and successfully used the
language and implementation on a variety of important distributed algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8467</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8467</id><created>2014-12-25</created><authors><author><keyname>Vidanagamachchi</keyname><forenames>S. M.</forenames></author><author><keyname>Dewasurendra</keyname><forenames>S. D.</forenames></author><author><keyname>Ragel</keyname><forenames>R. G.</forenames></author><author><keyname>Niranjan</keyname><forenames>M.</forenames></author></authors><title>A Structured Hardware Software Architecture for Peptide Based Diagnosis
  - Sub-string Matching Problem with Limited Tolerance (ICIAfS14)</title><categories>cs.CE cs.PF</categories><comments>appears in The 7th International Conference on Information and
  Automation for Sustainability (ICIAfS) 2014. arXiv admin note: substantial
  text overlap with arXiv:1412.7811</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of inferring proteins from complex peptide samples in shotgun
proteomic workflow sets extreme demands on computational resources. This is
exacerbated by the fact that, in general, a given protein cannot be defined by
a fixed sequence of amino acids due to the existence of splice variants and
isoforms of that protein. Therefore, the problem of protein inference could be
considered as one of identifying sequences of amino acids with some limited
tolerance. Two problems arise from this: a) due to these variations, the
applicability of exact string matching methodologies could be questioned and b)
the difficulty of defining a reference sequence for a particular set of
proteins that are functionally indistinguishable, but with some variation in
features. This paper presents a model-based inference approach that is
developed and validated to solve the inference problem. Our approach starts
from an examination of the known set of splice variants and isoforms of a
target protein to identify the Greatest Common Stable Substring (GCSS) of amino
acids and the Substrings Subjects to Limited Variation (SSLV) and their
respective locations on the GCSS. Then we define and solve the Sub-string
Matching Problem with Limited Tolerance (SMPLT). This approach is validated on
identified peptides in a labelled and clustered data set from UNIPROT.
Identification of Baylisascaris Procyonis infection was used as an application
instance that achieved up to 70 times speedup compared to a software only
system. This workflow can be generalised to any inexact multiple pattern
matching application by replacing the patterns in a clustered and distributed
environment which permits a distance between member strings to account for
permitted deviations such as substitutions, insertions and deletions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8490</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8490</id><created>2014-09-19</created><authors><author><keyname>Hashim</keyname><forenames>Hayder Raheem</forenames></author><author><keyname>Neamaa</keyname><forenames>Irtifaa Abdalkadum</forenames></author></authors><title>Image Encryption and Decryption in A Modification of ElGamal
  Cryptosystem in MATLAB</title><categories>cs.CR</categories><comments>7 pages</comments><journal-ref>International Journal of Sciences: Basic and Applied Research
  (IJSBAR)(2014) Volume 14, No 2, pp 141-147</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The need of exchanging messages and images secretly over unsecure networks
promoted the creation of cryptosystems to enable receivers to interpret the
exchanged information. In this paper, a particular public key cryptosystem
called the ElGamal Cryptosystem is presented considered with the help MATLAB
Program to be used over Images. Since the ElGamal cryptosystem over a primitive
root of a large prime is used in messages encryption in the free GNU Privacy
Guard software, recent versions of PGP, and other cryptosystems. This paper
shows a modification of the this cryptosystem by applying it over gray and
color images. That would be by transforming an image into its corresponding
matrix using MATLAB Program, then applying the encryption and decryption
algorithms over it. Actually, this modification gives one of the best image
encryptions that have been used since the encryption procedure over any image
goes smoothly and transfers the original image to completely undefined image
which makes this cryptosystem is really secure and successful over image
encryption. As well as, the decryption procedure of the encrypted image works
very well since it transfers undefined image to its original. Therefore, this
new modification can make the cryptosystem of images more immune against some
future attacks since breaking this cryptosystem depends on solving the discrete
logarithm problem which is really impossible with large prime numbers .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8493</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8493</id><created>2014-12-29</created><authors><author><keyname>Carreira-Perpi&#xf1;&#xe1;n</keyname><forenames>Miguel &#xc1;.</forenames></author></authors><title>An ADMM algorithm for solving a proximal bound-constrained quadratic
  program</title><categories>math.OC cs.LG stat.ML</categories><comments>5 pages, 1 figure. arXiv admin note: text overlap with
  arXiv:1405.5960</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a proximal operator given by a quadratic function subject to
bound constraints and give an optimization algorithm using the alternating
direction method of multipliers (ADMM). The algorithm is particularly efficient
to solve a collection of proximal operators that share the same quadratic form,
or if the quadratic program is the relaxation of a binary quadratic problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8496</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8496</id><created>2014-12-29</created><authors><author><keyname>Salarian</keyname><forenames>Mahdi</forenames></author></authors><title>Accurate Localization in Dense Urban Area Using Google Street View Image</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate information about the location and orientation of a camera in mobile
devices is central to the utilization of location-based services (LBS). Most of
such mobile devices rely on GPS data but this data is subject to inaccuracy due
to imperfections in the quality of the signal provided by satellites. This
shortcoming has spurred the research into improving the accuracy of
localization. Since mobile devices have camera, a major thrust of this research
has been seeks to acquire the local scene and apply image retrieval techniques
by querying a GPS-tagged image database to find the best match for the acquired
scene.. The techniques are however computationally demanding and unsuitable for
real-time applications such as assistive technology for navigation by the blind
and visually impaired which motivated out work. To overcome the high complexity
of those techniques, we investigated the use of inertial sensors as an aid in
image-retrieval-based approach. Armed with information of media other than
images, such as data from the GPS module along with orientation sensors such as
accelerometer and gyro, we sought to limit the size of the image set to c
search for the best match. Specifically, data from the orientation sensors
along with Dilution of precision (DOP) from GPS are used to find the angle of
view and estimation of position. We present analysis of the reduction in the
image set size for the search as well as simulations to demonstrate the
effectiveness in a fast implementation with 98% Estimated Position Error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8500</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8500</id><created>2014-12-29</created><authors><author><keyname>Zaidi</keyname><forenames>Abdallah</forenames></author><author><keyname>Rokbani</keyname><forenames>Nizar</forenames></author><author><keyname>Alimi</keyname><forenames>AdelM.</forenames></author></authors><title>Implementation of a Hierarchical fuzzy controller for a biped robot</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper the design of a control system for a biped robot is described.
Control is specified for a walk cycle of the robot. The implementation of the
control system was done on Matlab Simulink. In this paper a hierarchical fuzzy
logic controller (HFLC) is proposed to control a planar biped walk. The HFLC
design is bio-inspired from human locomotion system. The proposed method is
applied to control five links planar biped into free area and without
obstacles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8501</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8501</id><created>2014-12-29</created><authors><author><keyname>Meirom</keyname><forenames>Eli A.</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author><author><keyname>Orda</keyname><forenames>Ariel</forenames></author></authors><title>Formation Games of Reliable Networks</title><categories>cs.GT cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish a network formation game for the Internet's Autonomous System
(AS) interconnection topology. The game includes different types of players,
accounting for the heterogeneity of ASs in the Internet. We incorporate
reliability considerations in the player's utility function, and analyze static
properties of the game as well as its dynamic evolution. We provide dynamic
analysis of its topological quantities, and explain the prevalence of some
&quot;network motifs&quot; in the Internet graph. We assess our predictions with
real-world data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8504</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8504</id><created>2014-12-29</created><authors><author><keyname>Amancio</keyname><forenames>Diego R.</forenames></author></authors><title>Probing the topological properties of complex networks modeling short
  written texts</title><categories>cs.CL physics.soc-ph</categories><journal-ref>PLoS ONE 10(2): e0118394, 2015</journal-ref><doi>10.1371/journal.pone.0118394</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, graph theory has been widely employed to probe several
language properties. More specifically, the so-called word adjacency model has
been proven useful for tackling several practical problems, especially those
relying on textual stylistic analysis. The most common approach to treat texts
as networks has simply considered either large pieces of texts or entire books.
This approach has certainly worked well -- many informative discoveries have
been made this way -- but it raises an uncomfortable question: could there be
important topological patterns in small pieces of texts? To address this
problem, the topological properties of subtexts sampled from entire books was
probed. Statistical analyzes performed on a dataset comprising 50 novels
revealed that most of the traditional topological measurements are stable for
short subtexts. When the performance of the authorship recognition task was
analyzed, it was found that a proper sampling yields a discriminability similar
to the one found with full texts. Surprisingly, the support vector machine
classification based on the characterization of short texts outperformed the
one performed with entire books. These findings suggest that a local
topological analysis of large documents might improve its global
characterization. Most importantly, it was verified, as a proof of principle,
that short texts can be analyzed with the methods and concepts of complex
networks. As a consequence, the techniques described here can be extended in a
straightforward fashion to analyze texts as time-varying complex networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8518</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8518</id><created>2014-12-29</created><authors><author><keyname>Hartline</keyname><forenames>Jason D.</forenames></author><author><keyname>Roughgarden</keyname><forenames>Tim</forenames></author></authors><title>Optimal Platform Design</title><categories>cs.GT cs.DS</categories><comments>There is some overlap between this paper and the paper &quot;Optimal
  Mechanism Design and Money Burning,&quot; which appeared in the STOC 2008
  conference and as arXiv:0804.2097. However, the focus of this paper is
  different, with some of our earlier results omitted and several new results
  included</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An auction house cannot generally provide the optimal auction technology to
every client. Instead it provides one or several auction technologies, and
clients select the most appropriate one. For example, eBay provides ascending
auctions and &quot;buy-it-now&quot; pricing. For each client the offered technology may
not be optimal, but it would be too costly for clients to create their own. We
call these mechanisms, which emphasize generality rather than optimality,
platform mechanisms. A platform mechanism will be adopted by a client if its
performance exceeds that of the client's outside option, e.g., hiring (at a
cost) a consultant to design the optimal mechanism. We ask two related
questions. First, for what costs of the outside option will the platform be
universally adopted? Second, what is the structure of good platform mechanisms?
We answer these questions using a novel prior-free analysis framework in which
we seek mechanisms that are approximately optimal for every prior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8520</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8520</id><created>2014-12-29</created><authors><author><keyname>Crutchfield</keyname><forenames>James P.</forenames></author><author><keyname>James</keyname><forenames>Ryan G.</forenames></author><author><keyname>Marzen</keyname><forenames>Sarah</forenames></author><author><keyname>Varn</keyname><forenames>Dowman P.</forenames></author></authors><title>Understanding and Designing Complex Systems: Response to &quot;A framework
  for optimal high-level descriptions in science and engineering---preliminary
  report&quot;</title><categories>cond-mat.stat-mech cs.AI cs.CE cs.IT math.IT nlin.CD</categories><comments>6 pages; http://csc.ucdavis.edu/~cmg/compmech/pubs/ssc_comment.htm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We recount recent history behind building compact models of nonlinear,
complex processes and identifying their relevant macroscopic patterns or
&quot;macrostates&quot;. We give a synopsis of computational mechanics, predictive
rate-distortion theory, and the role of information measures in monitoring
model complexity and predictive performance. Computational mechanics provides a
method to extract the optimal minimal predictive model for a given process.
Rate-distortion theory provides methods for systematically approximating such
models. We end by commenting on future prospects for developing a general
framework that automatically discovers optimal compact models. As a response to
the manuscript cited in the title above, this brief commentary corrects
potentially misleading claims about its state space compression method and
places it in a broader historical setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8525</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8525</id><created>2014-12-29</created><authors><author><keyname>Marsden</keyname><forenames>Daniel</forenames><affiliation>Department of Computer Science, University of Oxford, UK</affiliation></author></authors><title>Fibred Coalgebraic Logic and Quantum Protocols</title><categories>quant-ph cs.LO</categories><comments>In Proceedings QPL 2013, arXiv:1412.7917</comments><proxy>EPTCS</proxy><acm-class>F.4.1</acm-class><journal-ref>EPTCS 171, 2014, pp. 90-99</journal-ref><doi>10.4204/EPTCS.171.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by applications in modelling quantum systems using coalgebraic
techniques, we introduce a fibred coalgebraic logic. Our approach extends the
conventional predicate lifting semantics with additional modalities relating
conditions on different fibres. As this fibred setting will typically involve
multiple signature functors, the logic incorporates a calculus of modalities
enabling the construction of new modalities using various composition
operations. We extend the semantics of coalgebraic logic to this setting, and
prove that this extension respects behavioural equivalence.
  We show how properties of the semantics of modalities are preserved under
composition operations, and then apply the calculational aspect of our logic to
produce an expressive set of modalities for reasoning about quantum systems,
building these modalities up from simpler components. We then demonstrate how
these modalities can describe some standard quantum protocols. The novel
features of our logic are shown to allow for a uniform description of unitary
evolution, and support local reasoning such as &quot;Alice's qubit satisfies
condition&quot; as is common when discussing quantum protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8526</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8526</id><created>2014-12-29</created><authors><author><keyname>Maruyama</keyname><forenames>Yoshihiro</forenames><affiliation>Quantum Group, Department of Computer Science, University of Oxford</affiliation></author></authors><title>Duality Theory and Categorical Universal Logic: With Emphasis on Quantum
  Structures</title><categories>quant-ph cs.LO</categories><comments>In Proceedings QPL 2013, arXiv:1412.7917</comments><proxy>EPTCS</proxy><acm-class>F.4.1</acm-class><journal-ref>EPTCS 171, 2014, pp. 100-112</journal-ref><doi>10.4204/EPTCS.171.10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Categorical Universal Logic is a theory of monad-relativised hyperdoctrines
(or fibred universal algebras), which in particular encompasses categorical
forms of both first-order and higher-order quantum logics as well as classical,
intuitionistic, and diverse substructural logics. Here we show there are those
dual adjunctions that have inherent hyperdoctrine structures in their predicate
functor parts. We systematically investigate into the categorical logics of
dual adjunctions by utilising Johnstone-Dimov-Tholen's duality-theoretic
framework. Our set-theoretical duality-based hyperdoctrines for quantum logic
have both universal and existential quantifiers (and higher-order structures),
giving rise to a universe of Takeuti-Ozawa's quantum sets via the
tripos-to-topos construction by Hyland-Johnstone-Pitts. The set-theoretical
hyperdoctrinal models of quantum logic, as well as all quantum hyperdoctrines
with cartesian base categories, turn out to give sound and complete semantics
for Faggian-Sambin's first-order quantum sequent calculus over cartesian type
theory; in addition, quantum hyperdoctrines with monoidal base categories are
sound and complete for the calculus over linear type theory. We finally
consider how to reconcile Birkhoff-von Neumann's quantum logic and
Abramsky-Coecke's categorical quantum mechanics (which is modernised quantum
logic as an antithesis to the traditional one) via categorical universal logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8527</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8527</id><created>2014-12-29</created><authors><author><keyname>Preller</keyname><forenames>Anne</forenames><affiliation>LIRMM, France</affiliation></author></authors><title>From Logical to Distributional Models</title><categories>cs.LO cs.CL quant-ph</categories><comments>In Proceedings QPL 2013, arXiv:1412.7917</comments><proxy>EPTCS</proxy><acm-class>F.4.0; F.4.1</acm-class><journal-ref>EPTCS 171, 2014, pp. 113-131</journal-ref><doi>10.4204/EPTCS.171.11</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper relates two variants of semantic models for natural language,
logical functional models and compositional distributional vector space models,
by transferring the logic and reasoning from the logical to the distributional
models.
  The geometrical operations of quantum logic are reformulated as algebraic
operations on vectors. A map from functional models to vector space models
makes it possible to compare the meaning of sentences word by word.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8528</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8528</id><created>2014-12-29</created><authors><author><keyname>Roumen</keyname><forenames>Frank</forenames><affiliation>Inst. for Mathematics, Astrophysics and Particle Physics</affiliation></author></authors><title>Categorical characterizations of operator-valued measures</title><categories>cs.LO math.CT</categories><comments>In Proceedings QPL 2013, arXiv:1412.7917</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 171, 2014, pp. 132-144</journal-ref><doi>10.4204/EPTCS.171.12</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The most general type of measurement in quantum physics is modeled by a
positive operator-valued measure (POVM). Mathematically, a POVM is a
generalization of a measure, whose values are not real numbers, but positive
operators on a Hilbert space. POVMs can equivalently be viewed as maps between
effect algebras or as maps between algebras for the Giry monad. We will show
that this equivalence is an instance of a duality between two categories. In
the special case of continuous POVMs, we obtain two equivalent representations
in terms of morphisms between von Neumann algebras.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8529</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8529</id><created>2014-12-29</created><updated>2015-03-25</updated><authors><author><keyname>Hernandez-Orallo</keyname><forenames>Jose</forenames></author></authors><title>A note about the generalisation of the C-tests</title><categories>cs.AI</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this exploratory note we ask the question of what a measure of performance
for all tasks is like if we use a weighting of tasks based on a difficulty
function. This difficulty function depends on the complexity of the
(acceptable) solution for the task (instead of a universal distribution over
tasks or an adaptive test). The resulting aggregations and decompositions are
(now retrospectively) seen as the natural (and trivial) interactive
generalisation of the C-tests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8530</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8530</id><created>2014-12-29</created><updated>2015-04-23</updated><authors><author><keyname>Katz</keyname><forenames>Daniel J.</forenames></author><author><keyname>Langevin</keyname><forenames>Philippe</forenames></author></authors><title>New Open Problems Related to Old Conjectures by Helleseth</title><categories>math.NT cs.IT math.IT</categories><comments>17 pages</comments><msc-class>94A55, 11T23, 11L05, 11L07, 11T71</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, very interesting results have been obtained concerning the Fourier
spectra of power permutations over a finite field. In this note we survey the
recent ideas of Aubry, Feng, Katz, and Langevin, and we pose new open problems
related to old conjectures proposed by Helleseth in the middle of the
seventies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8531</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8531</id><created>2014-12-29</created><authors><author><keyname>Fink</keyname><forenames>Michael</forenames></author><author><keyname>Homola</keyname><forenames>Martin</forenames></author><author><keyname>Mileo</keyname><forenames>Alessandra</forenames></author></authors><title>Workshop Notes of the 6th International Workshop on Acquisition,
  Representation and Reasoning about Context with Logic (ARCOE-Logic 2014)</title><categories>cs.AI</categories><comments>ARCOE-Logic 2014, 5 papers</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  ARCOE-Logic 2014, the 6th International Workshop on Acquisition,
Representation and Reasoning about Context with Logic, was held in co-location
with the 19th International Conference on Knowledge Engineering and Knowledge
Management (EKAW 2014) on November 25, 2014 in Link\&quot;oping, Sweden. These notes
contain the five papers which were accepted and presented at the workshop.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8532</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8532</id><created>2014-12-29</created><updated>2015-01-02</updated><authors><author><keyname>Tseng</keyname><forenames>Lewis</forenames></author><author><keyname>Vaidya</keyname><forenames>Nitin</forenames></author></authors><title>Crash-Tolerant Consensus in Directed Graphs</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work considers a point-to-point network of n nodes connected by directed
links, and proves tight necessary and sufficient conditions on the underlying
communication graphs for achieving consensus among these nodes under crash
faults. We identify the conditions in both synchronous and asynchronous systems
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8534</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8534</id><created>2014-12-29</created><authors><author><keyname>Sajjadi</keyname><forenames>Mehdi</forenames></author><author><keyname>Seyedhosseini</keyname><forenames>Mojtaba</forenames></author><author><keyname>Tasdizen</keyname><forenames>Tolga</forenames></author></authors><title>Disjunctive Normal Networks</title><categories>cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Artificial neural networks are powerful pattern classifiers; however, they
have been surpassed in accuracy by methods such as support vector machines and
random forests that are also easier to use and faster to train.
Backpropagation, which is used to train artificial neural networks, suffers
from the herd effect problem which leads to long training times and limit
classification accuracy. We use the disjunctive normal form and approximate the
boolean conjunction operations with products to construct a novel network
architecture. The proposed model can be trained by minimizing an error function
and it allows an effective and intuitive initialization which solves the
herd-effect problem associated with backpropagation. This leads to state-of-the
art classification accuracy and fast training times. In addition, our model can
be jointly optimized with convolutional features in an unified structure
leading to state-of-the-art results on computer vision problems with fast
convergence rates. A GPU implementation of LDNN with optional convolutional
features is also available
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8539</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8539</id><created>2014-12-29</created><authors><author><keyname>Chiribella</keyname><forenames>Giulio</forenames><affiliation>Institute for Interdisciplinary Information Sciences, Tsinghua University</affiliation></author></authors><title>Dilation of states and processes in operational-probabilistic theories</title><categories>quant-ph cs.LO</categories><comments>In Proceedings QPL 2014, arXiv:1412.8102</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 172, 2014, pp. 1-14</journal-ref><doi>10.4204/EPTCS.172.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides a concise summary of the framework of
operational-probabilistic theories, aimed at emphasizing the interaction
between category-theoretic and probabilistic structures. Within this framework,
we review an operational version of the GNS construction, expressed by the
so-called purification principle, which under mild hypotheses leads to an
operational version of Stinespring's theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8540</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8540</id><created>2014-12-29</created><authors><author><keyname>Ozawa</keyname><forenames>Masanao</forenames><affiliation>Nagoya University</affiliation></author></authors><title>Quantum Set Theory Extending the Standard Probabilistic Interpretation
  of Quantum Theory (Extended Abstract)</title><categories>quant-ph cs.LO</categories><comments>In Proceedings QPL 2014, arXiv:1412.8102</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 172, 2014, pp. 15-26</journal-ref><doi>10.4204/EPTCS.172.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The notion of equality between two observables will play many important roles
in foundations of quantum theory. However, the standard probabilistic
interpretation based on the conventional Born formula does not give the
probability of equality relation for a pair of arbitrary observables, since the
Born formula gives the probability distribution only for a commuting family of
observables. In this paper, quantum set theory developed by Takeuti and the
present author is used to systematically extend the probabilistic
interpretation of quantum theory to define the probability of equality relation
for a pair of arbitrary observables. Applications of this new interpretation to
measurement theory are discussed briefly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8541</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8541</id><created>2014-12-29</created><authors><author><keyname>Barbosa</keyname><forenames>Rui Soares</forenames><affiliation>Department of Computer Science, University of Oxford</affiliation></author></authors><title>On monogamy of non-locality and macroscopic averages: examples and
  preliminary results</title><categories>quant-ph cs.LO</categories><comments>In Proceedings QPL 2014, arXiv:1412.8102</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 172, 2014, pp. 36-55</journal-ref><doi>10.4204/EPTCS.172.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore a connection between monogamy of non-locality and a weak
macroscopic locality condition: the locality of the average behaviour. These
are revealed by our analysis as being two sides of the same coin.
  Moreover, we exhibit a structural reason for both in the case of Bell-type
multipartite scenarios, shedding light on but also generalising the results in
the literature [Ramanathan et al., Phys. Rev. Lett. 107, 060405 (2001);
Pawlowski &amp; Brukner, Phys. Rev. Lett. 102, 030403 (2009)]. More specifically,
we show that, provided the number of particles in each site is large enough
compared to the number of allowed measurement settings, and whatever the
microscopic state of the system, the macroscopic average behaviour is local
realistic, or equivalently, general multipartite monogamy relations hold.
  This result relies on a classical mathematical theorem by Vorob'ev [Theory
Probab. Appl. 7(2), 147-163 (1962)] about extending compatible families of
probability distributions defined on the faces of a simplicial complex -- in
the language of the sheaf-theoretic framework of Abramsky &amp; Brandenburger [New
J. Phys. 13, 113036 (2011)], such families correspond to no-signalling
empirical models, and the existence of an extension corresponds to locality or
non-contextuality. Since Vorob'ev's theorem depends solely on the structure of
the simplicial complex, which encodes the compatibility of the measurements,
and not on the specific probability distributions (i.e. the empirical models),
our result about monogamy relations and locality of macroscopic averages holds
not just for quantum theory, but for any empirical model satisfying the
no-signalling condition.
  In this extended abstract, we illustrate our approach by working out a couple
of examples, which convey the intuition behind our analysis while keeping the
discussion at an elementary level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8542</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8542</id><created>2014-12-29</created><authors><author><keyname>Kishida</keyname><forenames>Kohei</forenames><affiliation>University of Oxford</affiliation></author></authors><title>Stochastic Relational Presheaves and Dynamic Logic for Contextuality</title><categories>cs.LO</categories><comments>In Proceedings QPL 2014, arXiv:1412.8102</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 172, 2014, pp. 115-132</journal-ref><doi>10.4204/EPTCS.172.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Presheaf models provide a formulation of labelled transition systems that is
useful for, among other things, modelling concurrent computation. This paper
aims to extend such models further to represent stochastic dynamics such as
shown in quantum systems. After reviewing what presheaf models represent and
what certain operations on them mean in terms of notions such as internal and
external choices, composition of systems, and so on, I will show how to extend
those models and ideas by combining them with ideas from other
category-theoretic approaches to relational models and to stochastic processes.
It turns out that my extension yields a transitional formulation of
sheaf-theoretic structures that Abramsky and Brandenburger proposed to
characterize non-locality and contextuality. An alternative characterization of
contextuality will then be given in terms of a dynamic modal logic of the
models I put forward.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8543</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8543</id><created>2014-12-29</created><authors><author><keyname>Adams</keyname><forenames>Robin</forenames><affiliation>Radboud University Nijmegen</affiliation></author></authors><title>QPEL: Quantum Program and Effect Language</title><categories>cs.LO cs.ET</categories><comments>In Proceedings QPL 2014, arXiv:1412.8102</comments><proxy>EPTCS</proxy><acm-class>F.4.1;D.2.4;F.3.1;I.2.3</acm-class><journal-ref>EPTCS 172, 2014, pp. 133-153</journal-ref><doi>10.4204/EPTCS.172.10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the syntax and rules of deduction of QPEL (Quantum Program and
Effect Language), a language for describing both quantum programs, and
properties of quantum programs - effects on the appropriate Hilbert space. We
show how semantics may be given in terms of state-and-effect triangles, a
categorical setting that allows semantics in terms of Hilbert spaces,
C*-algebras, and other categories. We prove soundness and completeness results
that show the derivable judgements are exactly those provable in all
state-and-effect triangles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8544</identifier>
 <datestamp>2015-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8544</id><created>2014-12-29</created><authors><author><keyname>Uijlen</keyname><forenames>Sander</forenames><affiliation>Radboud Universiteit</affiliation></author><author><keyname>Westerbaan</keyname><forenames>Bas</forenames><affiliation>Radboud Universiteit</affiliation></author></authors><title>A Kochen-Specker system has at least 22 vectors (extended abstract)</title><categories>cs.DM</categories><comments>In Proceedings QPL 2014, arXiv:1412.8102</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 172, 2014, pp. 154-164</journal-ref><doi>10.4204/EPTCS.172.11</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  At the heart of the Conway-Kochen Free Will theorem and Kochen and Specker's
argument against non-contextual hidden variable theories is the existence of a
Kochen-Specker (KS) system: a set of points on the sphere that has no
0,1-coloring such that at most one of two orthogonal points are colored 1 and
of three pairwise orthogonal points exactly one is colored 1. In public
lectures, Conway encouraged the search for small KS systems. At the time of
writing, the smallest known KS system has 31 vectors. Arends, Ouaknine and
Wampler have shown that a KS system has at least 18 vectors, by reducing the
problem to the existence of graphs with a topological embeddability and
non-colorability property. The bottleneck in their search proved to be the
sheer number of graphs on more than 17 vertices and deciding embeddability.
  Continuing their effort, we prove a restriction on the class of graphs we
need to consider and develop a more practical decision procedure for
embeddability to improve the lower bound to 22.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8545</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8545</id><created>2014-12-29</created><authors><author><keyname>Cho</keyname><forenames>Kenta</forenames><affiliation>Radboud University Nijmegen</affiliation></author></authors><title>Semantics for a Quantum Programming Language by Operator Algebras</title><categories>cs.LO math.OA quant-ph</categories><comments>In Proceedings QPL 2014, arXiv:1412.8102</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 172, 2014, pp. 165-190</journal-ref><doi>10.4204/EPTCS.172.12</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel semantics for a quantum programming language by
operator algebras, which are known to give a formulation for quantum theory
that is alternative to the one by Hilbert spaces. We show that the opposite
category of the category of W*-algebras and normal completely positive
subunital maps is an elementary quantum flow chart category in the sense of
Selinger. As a consequence, it gives a denotational semantics for Selinger's
first-order functional quantum programming language QPL. The use of operator
algebras allows us to accommodate infinite structures and to handle classical
and quantum computations in a unified way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8546</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8546</id><created>2014-12-29</created><authors><author><keyname>Yasuda</keyname><forenames>Kazuya</forenames><affiliation>The University of Tokyo</affiliation></author><author><keyname>Kubota</keyname><forenames>Takahiro</forenames><affiliation>The University of Tokyo</affiliation></author><author><keyname>Kakutani</keyname><forenames>Yoshihiko</forenames><affiliation>The University of Tokyo</affiliation></author></authors><title>Observational Equivalence Using Schedulers for Quantum Processes</title><categories>cs.LO quant-ph</categories><comments>In Proceedings QPL 2014, arXiv:1412.8102</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 172, 2014, pp. 191-203</journal-ref><doi>10.4204/EPTCS.172.13</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the study of quantum process algebras, researchers have introduced
different notions of equivalence between quantum processes like bisimulation or
barbed congruence. However, there are intuitively equivalent quantum processes
that these notions do not regard as equivalent. In this paper, we introduce a
notion of equivalence named observational equivalence into qCCS. Since quantum
processes have both probabilistic and nondeterministic transitions, we
introduce schedulers that solve nondeterministic choices and obtain probability
distribution of quantum processes. By definition, the restrictions of
schedulers change observational equivalence. We propose some definitions of
schedulers, and investigate the relation between the restrictions of schedulers
and observational equivalence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8547</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8547</id><created>2014-12-29</created><authors><author><keyname>Takisaka</keyname><forenames>Toru</forenames><affiliation>Kyoto University</affiliation></author></authors><title>On G\'acs' quantum algorithmic entropy</title><categories>cs.IT math.IT quant-ph</categories><comments>In Proceedings QPL 2014, arXiv:1412.8102</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 172, 2014, pp. 204-216</journal-ref><doi>10.4204/EPTCS.172.14</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define an infinite dimensional modification of lower-semicomputability of
density operators by G\'acs with an attempt to fix some problem in the paper.
Our attempt is partly achieved by showing the existence of universal operator
under some additional assumption. It is left as a future task to eliminate this
assumption. We also see some properties and examples which stimulate further
research. In particular, we show that universal operator has certain nontrivial
form if it exists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8548</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8548</id><created>2014-12-29</created><authors><author><keyname>Bar</keyname><forenames>Krzysztof</forenames><affiliation>Department of Computer Science, University of Oxford</affiliation></author><author><keyname>Vicary</keyname><forenames>Jamie</forenames><affiliation>Department of Computer Science, University of Oxford</affiliation></author></authors><title>A 2-Categorical Analysis of Complementary Families, Quantum Key
  Distribution and the Mean King Problem</title><categories>cs.LO quant-ph</categories><comments>In Proceedings QPL 2014, arXiv:1412.8102</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 172, 2014, pp. 316-332</journal-ref><doi>10.4204/EPTCS.172.23</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores the use of 2-categorical technology for describing and
reasoning about complex quantum procedures. We give syntactic definitions of a
family of complementary measurements, and of quantum key distribution, and show
that they are equivalent. We then show abstractly that either structure gives a
solution to the Mean King problem, which we also formulate 2-categorically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8552</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8552</id><created>2014-12-29</created><authors><author><keyname>Kissinger</keyname><forenames>Aleks</forenames><affiliation>University of Oxford</affiliation></author><author><keyname>Quick</keyname><forenames>David</forenames><affiliation>University of Oxford</affiliation></author></authors><title>Tensors, !-graphs, and non-commutative quantum structures</title><categories>cs.LO</categories><comments>In Proceedings QPL 2014, arXiv:1412.8102</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 172, 2014, pp. 56-67</journal-ref><doi>10.4204/EPTCS.172.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Categorical quantum mechanics (CQM) and the theory of quantum groups rely
heavily on the use of structures that have both an algebraic and co-algebraic
component, making them well-suited for manipulation using diagrammatic
techniques. Diagrams allow us to easily form complex compositions of
(co)algebraic structures, and prove their equality via graph rewriting. One of
the biggest challenges in going beyond simple rewriting-based proofs is
designing a graphical language that is expressive enough to prove interesting
properties (e.g. normal form results) about not just single diagrams, but
entire families of diagrams. One candidate is the language of !-graphs, which
consist of graphs with certain subgraphs marked with boxes (called !-boxes)
that can be repeated any number of times. New !-graph equations can then be
proved using a powerful technique called !-box induction. However, previously
this technique only applied to commutative (or cocommutative) algebraic
structures, severely limiting its applications in some parts of CQM and
(especially) quantum groups. In this paper, we fix this shortcoming by offering
a new semantics for non-commutative !-graphs using an enriched version of
Penrose's abstract tensor notation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8555</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8555</id><created>2014-12-29</created><authors><author><keyname>Jabi</keyname><forenames>Mohammed</forenames></author><author><keyname>Hamss</keyname><forenames>Aata El</forenames></author><author><keyname>Szczecinski</keyname><forenames>Leszek</forenames></author><author><keyname>Piantanida</keyname><forenames>Pablo</forenames></author></authors><title>Multi-packet Hybrid ARQ: Closing gap to the ergodic capacity</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we consider incremental redundancy (IR) hybrid automatic repeat
request (HARQ), where transmission rounds are carried out over independent
block-fading channels. We propose the so-called multi-packet HARQ where the
transmitter allows different packets to share the same channel block. In this
way the resources (block) are optimally assigned throughout the transmission
rounds. This stands in contrast with the conventional HARQ, where each
transmission round occupies the entire block. We analyze superposition coding
and time-sharing transmission strategies and we optimize the parameters to
maximize the throughput. Besides the conventional one-bit feedback (ACK/NACK)
we also consider the rich, multi-bit feedback. To solve the optimization
problem we formulate it as a Markov decision process (MDP) problem where the
decisions are taken using accumulated mutual information (AMI) obtained from
the receiver via delayed feedback. When only one-bit feedback is used to inform
the transmitter about the decoding success/failure (ACK/NACK), the Partial
State Information Markov Decision Process (PSI-MDP) framework is used to obtain
the optimal policies. Numerical examples obtained in a Rayleigh-fading channel
indicate that, the proposed multi-packet HARQ outperforms the conventional one,
by more than 5 dB for high spectral efficiencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8556</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8556</id><created>2014-12-29</created><updated>2015-05-05</updated><authors><author><keyname>Dong</keyname><forenames>Jingming</forenames></author><author><keyname>Soatto</keyname><forenames>Stefano</forenames></author></authors><title>Domain-Size Pooling in Local Descriptors: DSP-SIFT</title><categories>cs.CV</categories><comments>Extended version of the CVPR 2015 paper. Technical Report UCLA CSD
  140022</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a simple modification of local image descriptors, such as SIFT,
based on pooling gradient orientations across different domain sizes, in
addition to spatial locations. The resulting descriptor, which we call
DSP-SIFT, outperforms other methods in wide-baseline matching benchmarks,
including those based on convolutional neural networks, despite having the same
dimension of SIFT and requiring no training.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8561</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8561</id><created>2014-12-29</created><authors><author><keyname>Lal</keyname><forenames>Nidhi Kumari</forenames></author><author><keyname>Singh</keyname><forenames>Ashutosh Kumar</forenames></author></authors><title>Modified Design of Microstrip Patch Antenna for WiMAX Communication
  System</title><categories>cs.OH</categories><doi>10.9781/ijimai.2014.315</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new design for U-shaped microstrip patch antenna is
proposed, which can be used in WiMAX communication systems. The aim of this
paper is to optimize the performance of microstrip patch antenna. Nowadays,
WiMAX communication applications are widely using U-shaped microstrip patch
antenna and it has become very popular. Our proposed antenna design uses 4-4.5
GHZ frequency band and it is working at narrowband within this band. RT/DUROID
5880 material is used for creating the substrate of the microstrip antenna.
This modified design of the microstrip patch antenna gives high performance in
terms of gain and return loss.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8566</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8566</id><created>2014-12-30</created><authors><author><keyname>Burda</keyname><forenames>Yuri</forenames></author><author><keyname>Grosse</keyname><forenames>Roger B.</forenames></author><author><keyname>Salakhutdinov</keyname><forenames>Ruslan</forenames></author></authors><title>Accurate and Conservative Estimates of MRF Log-likelihood using Reverse
  Annealing</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Markov random fields (MRFs) are difficult to evaluate as generative models
because computing the test log-probabilities requires the intractable partition
function. Annealed importance sampling (AIS) is widely used to estimate MRF
partition functions, and often yields quite accurate results. However, AIS is
prone to overestimate the log-likelihood with little indication that anything
is wrong. We present the Reverse AIS Estimator (RAISE), a stochastic lower
bound on the log-likelihood of an approximation to the original MRF model.
RAISE requires only the same MCMC transition operators as standard AIS.
Experimental results indicate that RAISE agrees closely with AIS
log-probability estimates for RBMs, DBMs, and DBNs, but typically errs on the
side of underestimating, rather than overestimating, the log-likelihood.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8574</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8574</id><created>2014-12-30</created><authors><author><keyname>Popic</keyname><forenames>Victoria</forenames></author><author><keyname>Salari</keyname><forenames>Raheleh</forenames></author><author><keyname>Hajirasouliha</keyname><forenames>Iman</forenames></author><author><keyname>Kashef-Haghighi</keyname><forenames>Dorna</forenames></author><author><keyname>West</keyname><forenames>Robert B.</forenames></author><author><keyname>Batzoglou</keyname><forenames>Serafim</forenames></author></authors><title>Fast and Scalable Inference of Multi-Sample Cancer Lineages</title><categories>cs.CE q-bio.GN</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Somatic variants can be used as lineage markers for the phylogenetic
reconstruction of cancer evolution. Since somatic phylogenetics is complicated
by sample heterogeneity, novel specialized tree-building methods are required
for cancer phylogeny reconstruction. We present LICHeE (Lineage Inference for
Cancer Heterogeneity and Evolution), a novel method that automates the
phylogenetic inference of cancer progression from multiple somatic samples.
LICHeE uses variant allele frequencies of SSNVs obtained by deep sequencing to
reconstruct multi-sample cell lineage trees and infer the subclonal composition
of the samples. LICHeE is open-sourced and available at
http://viq854.github.io/lichee.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8576</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8576</id><created>2014-12-30</created><updated>2015-02-13</updated><authors><author><keyname>Wang</keyname><forenames>Heng</forenames></author><author><keyname>Zheng</keyname><forenames>Da</forenames></author><author><keyname>Burns</keyname><forenames>Randal</forenames></author><author><keyname>Priebe</keyname><forenames>Carey</forenames></author></authors><title>Active Community Detection in Massive Graphs</title><categories>cs.SI physics.soc-ph</categories><comments>published in SDM-Networks 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A canonical problem in graph mining is the detection of dense communities.
This problem is exacerbated for a graph with a large order and size -- the
number of vertices and edges -- as many community detection algorithms scale
poorly. In this work we propose a novel framework for detecting active
communities that consist of the most active vertices in massive graphs. The
framework is applicable to graphs having billions of vertices and hundreds of
billions of edges. Our framework utilizes a parallelizable trimming algorithm
based on a locality statistic to filter out inactive vertices, and then
clusters the remaining active vertices via spectral decomposition on their
similarity matrix. We demonstrate the validity of our method with synthetic
Stochastic Block Model graphs, using Adjusted Rand Index as the performance
metric. We further demonstrate its practicality and efficiency on a most recent
real-world Hyperlink Web graph consisting of over 3.5 billion vertices and 128
billion edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8591</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8591</id><created>2014-12-30</created><authors><author><keyname>Nair</keyname><forenames>Aswathi</forenames></author><author><keyname>Raghunandan</keyname><forenames>Karthik</forenames></author><author><keyname>Yaswanth</keyname><forenames>Vaddi</forenames></author><author><keyname>Shridharan</keyname><forenames>Sreelal</forenames></author><author><keyname>Sambandan</keyname><forenames>Sanjiv</forenames></author></authors><title>Maze Solving Automatons for Self-Healing of Open Interconnects: Modular
  Add-on for Circuit Boards</title><categories>cs.ET cond-mat.soft</categories><doi>10.1063/1.4916513</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the circuit board integration of a self-healing mechanism to
repair open faults. The electric field driven mechanism physically restores
fractured interconnects in electronic circuits and has the ability to solve
mazes. The repair is performed by conductive particles dispersed in an
insulating fluid. We demonstrate the integration of the healing module onto
printed circuit boards and the ability of maze solving. We model and perform
experiments on the influence of the geometry of the conductive particles as
well as the terminal impedances of the route on the healing efficiency. The
typical heal rate is 10 $\mu$m/s with healed route having resistance of 100
$\Omega$ to 20 k$\Omega$ depending on the materials and concentrations used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8609</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8609</id><created>2014-12-30</created><updated>2015-01-06</updated><authors><author><keyname>Zhou</keyname><forenames>Limin</forenames></author><author><keyname>Niu</keyname><forenames>Xinxi</forenames></author><author><keyname>Yuan</keyname><forenames>Jing</forenames></author></authors><title>Noise Folding based on General Complete Perturbation in Compressed
  Sensing</title><categories>cs.IT math.IT</categories><comments>13 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper first present a new general completely perturbed compressed
sensing (CS) model y=(A+E)(x+u)+e,called \emph{noise folding based on general
completely perturbed CS system},~where $y\in R^m,~u \in R^m,~u\neq 0,~e\in
R^m$, $A\in R^{m\times n},~m\ll n$,~$E\in R^{m\times n}$ with incorporating
general nonzero perturbation E to sensing matrix A and noise u into signal x
simultaneously based on the standard CS model y=Ax+e.~Our constructions mainly
will whiten the new proposed CS model and explore into RIP,~coherence for A+E
of the new CS model after being whitened.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8615</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8615</id><created>2014-12-30</created><updated>2015-07-02</updated><authors><author><keyname>Chiplunkar</keyname><forenames>Ashish</forenames></author><author><keyname>Tirodkar</keyname><forenames>Sumedh</forenames></author><author><keyname>Vishwanathan</keyname><forenames>Sundar</forenames></author></authors><title>On Randomized Algorithms for Matching in the Online Preemptive Model</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the power of randomized algorithms for the maximum cardinality
matching (MCM) and the maximum weight matching (MWM) problems in the online
preemptive model. In this model, the edges of a graph are revealed one by one
and the algorithm is required to always maintain a valid matching. On seeing an
edge, the algorithm has to either accept or reject the edge. If accepted, then
the adjacent edges are discarded. The complexity of the problem is settled for
deterministic algorithms.
  Almost nothing is known for randomized algorithms. A lower bound of $1.693$
is known for MCM with a trivial upper bound of $2$. An upper bound of $5.356$
is known for MWM. We initiate a systematic study of the same in this paper with
an aim to isolate and understand the difficulty. We begin with a primal-dual
analysis of the deterministic algorithm due to McGregor. All deterministic
lower bounds are on instances which are trees at every step. For this class of
(unweighted) graphs we present a randomized algorithm which is
$\frac{28}{15}$-competitive. The analysis is a considerable extension of the
(simple) primal-dual analysis for the deterministic case. The key new technique
is that the distribution of primal charge to dual variables depends on the
&quot;neighborhood&quot; and needs to be done after having seen the entire input. The
assignment is asymmetric: in that edges may assign different charges to the two
end-points. Also the proof depends on a non-trivial structural statement on the
performance of the algorithm on the input tree.
  The other main result of this paper is an extension of the deterministic
lower bound of Varadaraja to a natural class of randomized algorithms which
decide whether to accept a new edge or not using independent random choices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8617</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8617</id><created>2014-12-30</created><authors><author><keyname>Liu</keyname><forenames>Zhihua</forenames></author><author><keyname>Gao</keyname><forenames>Han</forenames></author><author><keyname>Wang</keyname><forenames>Wuling</forenames></author><author><keyname>Chang</keyname><forenames>Shuai</forenames></author><author><keyname>Chen</keyname><forenames>Jiaxing</forenames></author></authors><title>Color Filtering Localization for Three-Dimensional Underwater Acoustic
  Sensor Networks</title><categories>cs.NI</categories><comments>18 pages, 11 figures, 2 tables</comments><msc-class>40B05</msc-class><acm-class>C.2.1</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Accurate localization for mobile nodes has been an important and fundamental
problem in underwater acoustic sensor networks (UASNs). The detection
information returned from a mobile node is meaningful only if its location is
known. In this paper, we propose two localization algorithms based on color
filtering technology called PCFL and ACFL. PCFL and ACFL aim at collaboratively
accomplishing accurate localization of underwater mobile nodes with minimum
energy expenditure. They both adopt the overlapping signal region of task
anchors which can communicate with the mobile node directly as the current
sampling area. PCFL employs the projected distances between each of the task
projections and the mobile node, while ACFL adopts the direct distance between
each of the task anchors and the mobile node. Also the proportion factor of
distance is proposed to weight the RGB values. By comparing the nearness
degrees of the RGB sequences between the samples and the mobile node, samples
can be filtered out. And the normalized nearness degrees are considered as the
weighted standards to calculate coordinates of the mobile nodes. The simulation
results show that the proposed methods have excellent localization performance
and can timely localize the mobile node. The average localization error of PCFL
can decline by about 30.4% than the AFLA method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8639</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8639</id><created>2014-12-30</created><authors><author><keyname>Pullicino</keyname><forenames>Kyle</forenames></author></authors><title>Jif: Language-based Information-flow Security in Java</title><categories>cs.PL cs.CR</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this report, we examine Jif, a Java extension which augments the language
with features related to security. Jif adds support for security labels to
Java's type system such that the developer can specify confidentiality and
integrity policies to the various variables used in their program. We list the
main features of Jif and discuss the information flow problem that Jif helps to
solve. We see how the information flow problem occurs in real-world systems by
looking at two examples: Civitas, a ballot/voting system where voters do not
necessarily trust voting agents, and SIF, a web application container
implemented using Jif. Finally, we implement a small program that simulates
information flow in a booking system containing sensitive data and discuss the
usefulness of Jif based on this program.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8648</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8648</id><created>2014-12-23</created><authors><author><keyname>Fan</keyname><forenames>Deliang</forenames></author><author><keyname>Shim</keyname><forenames>Yong</forenames></author><author><keyname>Raghunathan</keyname><forenames>Anand</forenames></author><author><keyname>Roy</keyname><forenames>Kaushik</forenames></author></authors><title>STT-SNN: A Spin-Transfer-Torque Based Soft-Limiting Non-Linear Neuron
  for Low-Power Artificial Neural Networks</title><categories>cs.ET cond-mat.dis-nn</categories><comments>This paper was submitted to IEEE Transactions on Nanotechnology for
  review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent years have witnessed growing interest in the use of Artificial Neural
Networks (ANNs) for vision, classification, and inference problems. An
artificial neuron sums N weighted inputs and passes the result through a
non-linear transfer function. Large-scale ANNs impose very high computing
requirements for training and classification, leading to great interest in the
use of post-CMOS devices to realize them in an energy efficient manner. In this
paper, we propose a spin-transfer-torque (STT) device based on Domain Wall
Motion (DWM) magnetic strip that can efficiently implement a Soft-limiting
Non-linear Neuron (SNN) operating at ultra-low supply voltage and current. In
contrast to previous spin-based neurons that can only realize hard-limiting
transfer functions, the proposed STT-SNN displays a continuous resistance
change with varying input current, and can therefore be employed to implement a
soft-limiting neuron transfer function. Soft-limiting neurons are greatly
preferred to hard-limiting ones due to their much improved modeling capacity,
which leads to higher network accuracy and lower network complexity. We also
present an ANN hardware design employing the proposed STT-SNNs and Memristor
Crossbar Arrays (MCA) as synapses. The ultra-low voltage operation of the
magneto metallic STT-SNN enables the programmable MCA-synapses, computing
analog-domain weighted summation of input voltages, to also operate at
ultra-low voltage. We modeled the STT-SNN using micro-magnetic simulation and
evaluated them using an ANN for character recognition. Comparisons with analog
and digital CMOS neurons show that STT-SNNs can achieve more than two orders of
magnitude lower energy consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8656</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8656</id><created>2014-12-30</created><authors><author><keyname>Aghazadeh</keyname><forenames>Nasser</forenames></author><author><keyname>Cigaroudy</keyname><forenames>Ladan Sharafyan</forenames></author></authors><title>A multistep segmentation algorithm for vessel extraction in medical
  imaging</title><categories>cs.CV math.NA</categories><comments>9 pages,4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main contribution of this paper is to propose an iterative procedure for
tubular structure segmentation of 2D images, which combines tight frame of
Curvelet transforms with a SURE technique thresholding which is based on
principle obtained by minimizing Stein Unbiased Risk Estimate for denoising.
This proposed algorithm is mainly based on the TFA proposal presented in [1,
9], which we use eigenvectors of Hessian matrix of image for improving this
iterative part in segmenting unclear and narrow vessels and filling the gap
between separate pieces of detected vessels. The experimental results are
presented to demonstrate the effectiveness of the proposed model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8657</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8657</id><created>2014-12-30</created><authors><author><keyname>Gandica</keyname><forenames>Y.</forenames></author><author><keyname>Aidos</keyname><forenames>F. Sampaio dos</forenames></author><author><keyname>Carvalho</keyname><forenames>J.</forenames></author></authors><title>Wikipedia edition dynamics</title><categories>physics.soc-ph cs.SI</categories><comments>15 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A model for the probabilistic function followed in Wikipedia edition is
presented and compared with simulations and real data. It is argued that the
probability to edit is proportional to the editor's number of previous editions
(preferential attachment), to the editor's fitness and to an ageing factor.
Using these simple ingredients, it is possible to reproduce the results
obtained for Wikipedia edition dynamics for a collection of single pages as
well as the averaged results. Using a stochastic process framework, a recursive
equation was obtained for the average of the number of editions per editor that
seems to describe the editing behaviour in Wikipedia.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8659</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8659</id><created>2014-12-30</created><updated>2015-05-30</updated><authors><author><keyname>Oyallon</keyname><forenames>Edouard</forenames></author><author><keyname>Mallat</keyname><forenames>St&#xe9;phane</forenames></author></authors><title>Deep Roto-Translation Scattering for Object Classification</title><categories>cs.CV</categories><comments>9 pages, 3 figures, CVPR 2015 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dictionary learning algorithms or supervised deep convolution networks have
considerably improved the efficiency of predefined feature representations such
as SIFT. We introduce a deep scattering convolution network, with predefined
wavelet filters over spatial and angular variables. This representation brings
an important improvement to results previously obtained with predefined
features over object image databases such as Caltech and CIFAR. The resulting
accuracy is comparable to results obtained with unsupervised deep learning and
dictionary based representations. This shows that refining image
representations by using geometric priors is a promising direction to improve
image classification and its understanding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8669</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8669</id><created>2014-12-29</created><authors><author><keyname>Trevi&#xf1;o</keyname><forenames>Santiago</forenames><suffix>III</suffix></author><author><keyname>Nyberg</keyname><forenames>Amy</forenames></author><author><keyname>Del Genio</keyname><forenames>Charo I.</forenames></author><author><keyname>Bassler</keyname><forenames>Kevin E.</forenames></author></authors><title>Fast and accurate determination of modularity and its effect size</title><categories>physics.soc-ph cs.SI</categories><comments>23 pages, 6 figures</comments><doi>10.1088/1742-5468/2015/02/P02003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a fast spectral algorithm for community detection in complex
networks. Our method searches for the partition with the maximum value of the
modularity via the interplay of several refinement steps that include both
agglomeration and division. We validate the accuracy of the algorithm by
applying it to several real-world benchmark networks. On all these, our
algorithm performs as well or better than any other known polynomial scheme.
This allows us to extensively study the modularity distribution in ensembles of
Erd\H{o}s-R\'enyi networks, producing theoretical predictions for means and
variances inclusive of finite-size corrections. Our work provides a way to
accurately estimate the effect size of modularity, providing a $z$-score
measure of it and enabling a more informative comparison of networks with
different numbers of nodes and links.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8670</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8670</id><created>2014-12-30</created><authors><author><keyname>Ordentlich</keyname><forenames>Or</forenames></author><author><keyname>Shayevitz</keyname><forenames>Ofer</forenames></author></authors><title>A VC-dimension-based Outer Bound on the Zero-Error Capacity of the
  Binary Adder Channel</title><categories>cs.IT math.IT</categories><comments>Submitted to ISIT 2015. An extended version titled &quot;An Upper Bound on
  the Sizes of Multiset-Union-Free Families&quot; is available online at
  arXiv:1412.8415</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The binary adder is a two-user multiple access channel whose inputs are
binary and whose output is the real sum of the inputs. While the Shannon
capacity region of this channel is well known, little is known regarding its
zero-error capacity region, and a large gap remains between the best inner and
outer bounds. In this paper, we provide an improved outer bound for this
problem. To that end, we introduce a soft variation of the Saur-Perles-Shelah
Lemma, that is then used in conjunction with an outer bound for the Shannon
capacity region with an additional common message.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8671</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8671</id><created>2014-12-30</created><updated>2015-09-11</updated><authors><author><keyname>Lee</keyname><forenames>Ciar&#xe1;n M.</forenames></author><author><keyname>Barrett</keyname><forenames>Jonathan</forenames></author></authors><title>Computation in generalised probabilistic theories</title><categories>quant-ph cs.CC</categories><comments>14+9 pages. Comments welcome</comments><journal-ref>New J. Phys. 17 (2015) 083001</journal-ref><doi>10.1088/1367-2630/17/8/083001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  From the existence of an efficient quantum algorithm for factoring, it is
likely that quantum computation is intrinsically more powerful than classical
computation. At present, the best upper bound known for the power of quantum
computation is that BQP is in AWPP. This work investigates limits on
computational power that are imposed by physical principles. To this end, we
define a circuit-based model of computation in a class of operationally-defined
theories more general than quantum theory, and ask: what is the minimal set of
physical assumptions under which the above inclusion still holds? We show that
given only an assumption of tomographic locality (roughly, that multipartite
states can be characterised by local measurements), efficient computations are
contained in AWPP. This inclusion still holds even without assuming a basic
notion of causality (where the notion is, roughly, that probabilities for
outcomes cannot depend on future measurement choices). Following Aaronson, we
extend the computational model by allowing post-selection on measurement
outcomes. Aaronson showed that the corresponding quantum complexity class is
equal to PP. Given only the assumption of tomographic locality, the inclusion
in PP still holds for post-selected computation in general theories. Thus in a
world with post-selection, quantum theory is optimal for computation in the
space of all general theories. We then consider if relativised complexity
results can be obtained for general theories. It is not clear how to define a
sensible notion of an oracle in the general framework that reduces to the
standard notion in the quantum case. Nevertheless, it is possible to define
computation relative to a `classical oracle'. Then, we show there exists a
classical oracle relative to which efficient computation in any theory
satisfying the causality assumption and tomographic locality does not include
NP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8690</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8690</id><created>2014-12-30</created><authors><author><keyname>Bach</keyname><forenames>Francis</forenames><affiliation>LIENS, INRIA Paris - Rocquencourt</affiliation></author></authors><title>Breaking the Curse of Dimensionality with Convex Neural Networks</title><categories>cs.LG math.OC math.ST stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider neural networks with a single hidden layer and non-decreasing
homogeneous activa-tion functions like the rectified linear units. By letting
the number of hidden units grow unbounded and using classical non-Euclidean
regularization tools on the output weights, we provide a detailed theoretical
analysis of their generalization performance, with a study of both the
approximation and the estimation errors. We show in particular that they are
adaptive to unknown underlying linear structures, such as the dependence on the
projection of the input variables onto a low-dimensional subspace. Moreover,
when using sparsity-inducing norms on the input weights, we show that
high-dimensional non-linear variable selection may be achieved, without any
strong assumption regarding the data and with a total number of variables
potentially exponential in the number of ob-servations. In addition, we provide
a simple geometric interpretation to the non-convex problem of addition of a
new unit, which is the core potentially hard computational element in the
framework of learning from continuously many basis functions. We provide simple
conditions for convex relaxations to achieve the same generalization error
bounds, even when constant-factor approxi-mations cannot be found (e.g.,
because it is NP-hard such as for the zero-homogeneous activation function). We
were not able to find strong enough convex relaxations and leave open the
existence or non-existence of polynomial-time algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8699</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8699</id><created>2014-12-30</created><authors><author><keyname>Kepner</keyname><forenames>Jeremy</forenames></author><author><keyname>Gadepally</keyname><forenames>Vijay</forenames></author><author><keyname>Michaleas</keyname><forenames>Pete</forenames></author></authors><title>Percolation Model of Insider Threats to Assess the Optimum Number of
  Rules</title><categories>cs.CY cs.CR physics.soc-ph</categories><comments>6 pages, 5 figures, submitted to IEEE HST</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rules, regulations, and policies are the basis of civilized society and are
used to coordinate the activities of individuals who have a variety of goals
and purposes. History has taught that over-regulation (too many rules) makes it
difficult to compete and under-regulation (too few rules) can lead to crisis.
This implies an optimal number of rules that avoids these two extremes. Rules
create boundaries that define the latitude an individual has to perform their
activities. This paper creates a Toy Model of a work environment and examines
it with respect to the latitude provided to a normal individual and the
latitude provided to an insider threat. Simulations with the Toy Model
illustrate four regimes with respect to an insider threat: under-regulated,
possibly optimal, tipping-point, and over-regulated. These regimes depend up
the number of rules (N) and the minimum latitude (Lmin) required by a normal
individual to carry out their activities. The Toy Model is then mapped onto the
standard 1D Percolation Model from theoretical physics and the same behavior is
observed. This allows the Toy Model to be generalized to a wide array of more
complex models that have been well studied by the theoretical physics community
and also show the same behavior. Finally, by estimating N and Lmin it should be
possible to determine the regime of any particular environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8700</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8700</id><created>2014-12-30</created><authors><author><keyname>Tagiew</keyname><forenames>Rustam</forenames></author></authors><title>Bewelcome.org -- a non-profit democratic hospex service set up for
  growth</title><categories>cs.SI</categories><comments>5 pages, 8 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an extensive data-based analysis of the non-profit
democratic hospitality exchange service bewelcome.org. We hereby pursuit the
goal of determining the factors influencing its growth. It also provides
general insights on internet-based hospitality exchange services. The other
investigated services are hospitalityclub.org and couchsurfing.org. Communities
using the three services are interconnected -- comparing their data provides
additional information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8704</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8704</id><created>2014-12-30</created><authors><author><keyname>Aerts</keyname><forenames>Diederik</forenames></author><author><keyname>Sozzo</keyname><forenames>Sandro</forenames></author><author><keyname>Veloz</keyname><forenames>Tomas</forenames></author></authors><title>Quantum Structure in Cognition and the Foundations of Human Reasoning</title><categories>cs.AI quant-ph</categories><comments>11 pages, no figures</comments><journal-ref>International Journal of Theoretical Physics, 54, pp 4557-4569,
  2015</journal-ref><doi>10.1007/s10773-015-2717-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional cognitive science rests on a foundation of classical logic and
probability theory. This foundation has been seriously challenged by several
findings in experimental psychology on human decision making. Meanwhile, the
formalism of quantum theory has provided an efficient resource for modeling
these classically problematical situations. In this paper, we start from our
successful quantum-theoretic approach to the modeling of concept combinations
to formulate a unifying explanatory hypothesis. In it, human reasoning is the
superposition of two processes -- a conceptual reasoning, whose nature is
emergence of new conceptuality, and a logical reasoning, founded on an
algebraic calculus of the logical type. In most cognitive processes however,
the former reasoning prevails over the latter. In this perspective, the
observed deviations from classical logical reasoning should not be interpreted
as biases but, rather, as natural expressions of emergence in its deepest form.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8708</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8708</id><created>2014-12-30</created><updated>2015-09-30</updated><authors><author><keyname>Goyal</keyname><forenames>Sanjay</forenames></author><author><keyname>Liu</keyname><forenames>Pei</forenames></author><author><keyname>Panwar</keyname><forenames>Shivendra</forenames></author><author><keyname>Yang</keyname><forenames>Rui</forenames></author><author><keyname>DiFazio</keyname><forenames>Robert A.</forenames></author><author><keyname>Bala</keyname><forenames>Erdem</forenames></author></authors><title>Full Duplex Operation for Small Cells</title><categories>cs.NI cs.IT math.IT</categories><comments>34 pages, 8 figures, Edited some typos in the previous version of the
  draft</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Full duplex (FD) communications has the potential to double the capacity of a
half duplex (HD) system at the link level. However, FD operation increases the
aggregate interference on each communication link, which limits the capacity
improvement. In this paper, we investigate how much of the potential doubling
can be practically achieved in the resource-managed, small multi-cellular
system, similar to the TDD variant of LTE, both in indoor and outdoor
environments, assuming FD base stations (BSs) and HD user equipment (UEs). We
focus on low-powered small cellular systems, because they are more suitable for
FD operation given practical self-interference cancellation limits. A joint UE
selection and power allocation method for a multi-cell scenario is presented,
where a hybrid scheduling policy assigns FD timeslots when it provides a
throughput advantage by pairing UEs with appropriate power levels to mitigate
the mutual interference, but otherwise defaults to HD operation. Due to the
complexity of finding the globally optimum solution of the proposed algorithm,
a sub-optimal method based on a heuristic greedy algorithm for UE selection,
and a novel solution using geometric programming for power allocation, is
proposed. With practical self-interference cancellation, antennas and circuits,
it is shown that the proposed hybrid FD system achieves as much as 94%
throughput improvement in the downlink, and 92% in the uplink, compared to a HD
system in an indoor multi-cell scenario and 54% in downlink and 61% in uplink
in an outdoor multi-cell scenario. Further, we also compare the energy
efficiency of FD operation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8712</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8712</id><created>2014-12-30</created><authors><author><keyname>Nikolopoulos</keyname><forenames>Stavros D.</forenames></author><author><keyname>Polenakis</keyname><forenames>Iosif</forenames></author></authors><title>Detecting Malicious Code by Exploiting Dependencies of System-call
  Groups</title><categories>cs.CR</categories><comments>21 pages, 4 figures</comments><acm-class>K.6.5; D.4.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present an elaborated graph-based algorithmic technique for
efficient malware detection. More precisely, we utilize the system-call
dependency graphs (or, for short ScD graphs), obtained by capturing taint
analysis traces and a set of various similarity metrics in order to detect
whether an unknown test sample is a malicious or a benign one. For the sake of
generalization, we decide to empower our model against strong mutations by
applying our detection technique on a weighted directed graph resulting from
ScD graph after grouping disjoint subsets of its vertices. Additionally, we
have developed a similarity metric, which we call NP-similarity, that combines
qualitative, quantitative, and relational characteristics that are spread among
the members of known malware families to archives a clear distinction between
graph-representations of malware and the ones of benign software. Finally, we
evaluate our detection model and compare our results against the results
achieved by a variety of techniques proving the potentials of our model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8723</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8723</id><created>2014-12-30</created><authors><author><keyname>Damci-Kurt</keyname><forenames>Pelin</forenames></author><author><keyname>Dey</keyname><forenames>Santanu S.</forenames></author><author><keyname>Kucukyavuz</keyname><forenames>Simge</forenames></author></authors><title>On a Cardinality-Constrained Transportation Problem With Market Choice</title><categories>math.OC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well-known that the intersection of the matching polytope with a
cardinality constraint is integral [8]. We prove a similar result for the
polytope corresponding to the transportation problem with market choice (TPMC)
(introduced in [4]) when the demands are in the set $\{1,2\}$. This result
generalizes the result regarding the matching polytope and also implies that
some special classes of minimum weight perfect matching problem with a
cardinality constraint on a subset of edges can be solved in polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8736</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8736</id><created>2014-12-30</created><authors><author><keyname>Neely</keyname><forenames>Michael J.</forenames></author></authors><title>Sharing Information Without Regret in Managed Stochastic Games</title><categories>math.OC cs.GT cs.MA</categories><comments>14 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers information sharing in a multi-player repeated game.
Every round, each player observes a subset of components of a random vector and
then takes a control action. The utility earned by each player depends on the
full random vector and on the actions of others. An example is a game where
different rewards are placed over multiple locations, each player only knows
the rewards in a subset of the locations, and players compete to collect the
rewards. Sharing information can help others, but can also increase competition
for desirable locations. Standard Nash equilibrium and correlated equilibrium
concepts are inadequate in this scenario. Instead, this paper develops an
algorithm where, every round, all players pass their information and intended
actions to a game manager. The manager provides suggested actions for each
player that, if taken, maximize a concave function of average utilities subject
to the constraint that each player gets an average utility no worse than it
would get without sharing. The algorithm acts online using information given at
each round and does not require a specific model of random events or player
actions. Thus, the analytical results of this paper apply in non-ergodic
situations with any sequence of actions taken by human players.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8739</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8739</id><created>2014-12-30</created><updated>2015-05-17</updated><authors><author><keyname>Drabent</keyname><forenames>W&#x142;odzimierz</forenames></author></authors><title>Correctness and completeness of logic programs</title><categories>cs.LO cs.PL</categories><comments>29 pages, 2 figures; with editorial modifications, small corrections
  and extensions. arXiv admin note: text overlap with arXiv:1411.3015. Overlaps
  explained in &quot;Related Work&quot; (p. 21)</comments><acm-class>D.1.6; F.3.1; D.2.4; D.2.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss proving correctness and completeness of definite clause logic
programs. We propose a method for proving completeness, while for proving
correctness we employ a method which should be well known but is often
neglected. Also, we show how to prove completeness and correctness in the
presence of SLD-tree pruning, and point out that approximate specifications
simplify specifications and proofs.
  We compare the proof methods to declarative diagnosis (algorithmic
debugging), showing that approximate specifications eliminate a major drawback
of the latter. We argue that our proof methods reflect natural declarative
thinking about programs, and that they can be used, formally or informally, in
every-day programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1412.8746</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1412.8746</id><created>2014-12-30</created><updated>2015-09-11</updated><authors><author><keyname>Li</keyname><forenames>Fu</forenames></author><author><keyname>Tzameret</keyname><forenames>Iddo</forenames></author><author><keyname>Wang</keyname><forenames>Zhengyu</forenames></author></authors><title>Characterizing Propositional Proofs as Non-Commutative Formulas</title><categories>cs.CC math.LO</categories><comments>Extended abstract appeared in Proc. of CCC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Does every Boolean tautology have a short propositional-calculus proof? Here,
a propositional calculus (i.e. Frege) proof is a proof starting from a set of
axioms and deriving new Boolean formulas using a set of fixed sound derivation
rules. Establishing any super-polynomial size lower bound on Frege proofs (in
terms of the size of the formula proved) is a major open problem in proof
complexity, and among a handful of fundamental hardness questions in complexity
theory by and large. Non-commutative arithmetic formulas, on the other hand,
constitute a quite weak computational model, for which exponential-size lower
bounds were shown already back in 1991 by Nisan [Nis91] who used a particularly
transparent argument.
  In this work we show that Frege lower bounds in fact follow from
corresponding size lower bounds on non-commutative formulas computing certain
polynomials (and that such lower bounds on non-commutative formulas must exist,
unless NP=coNP). More precisely, we demonstrate a natural association between
tautologies $T$ to non-commutative polynomials $p$, such that: if $T$ has a
polynomial-size Frege proof then $p$ has a polynomial-size non-commutative
arithmetic formula; and conversely, when $T$ is a DNF, if $p$ has a
polynomial-size non-commutative arithmetic formula over $GF(2)$ then $T$ has a
Frege proof of quasi-polynomial size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00001</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00001</id><created>2014-12-29</created><authors><author><keyname>Gorcin</keyname><forenames>Ali</forenames></author><author><keyname>Arslan</keyname><forenames>Huseyin</forenames></author></authors><title>An OFDM Signal Identification Method for Wireless Communications Systems</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distinction of OFDM signals from single carrier signals is highly important
for adaptive receiver algorithms and signal identification applications. OFDM
signals exhibit Gaussian characteristics in time domain and fourth order
cumulants of Gaussian distributed signals vanish in contrary to the cumulants
of other signals. Thus fourth order cumulants can be utilized for OFDM signal
identification. In this paper, first, formulations of the estimates of the
fourth order cumulants for OFDM signals are provided. Then it is shown these
estimates are affected significantly from the wireless channel impairments,
frequency offset, phase offset and sampling mismatch. To overcome these
problems, a general chi-square constant false alarm rate Gaussianity test which
employs estimates of cumulants and their covariances is adapted to the specific
case of wireless OFDM signals. Estimation of the covariance matrix of the
fourth order cumulants are greatly simplified peculiar to the OFDM signals. A
measurement setup is developed to analyze the performance of the identification
method and for comparison purposes. A parametric measurement analysis is
provided depending on modulation order, signal to noise ratio, number of
symbols, and degree of freedom of the underlying test. The proposed method
outperforms statistical tests which are based on fixed thresholds or empirical
values, while a priori information requirement and complexity of the proposed
method are lower than the coherent identification techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00008</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00008</id><created>2014-12-30</created><authors><author><keyname>Harrow</keyname><forenames>Aram W.</forenames></author></authors><title>Review of Quantum Algorithms for Systems of Linear Equations</title><categories>quant-ph cs.DS</categories><comments>3 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article reviews the 2008 quantum algorithm for linear systems of
equations due to Harrow, Hassidim and Lloyd, as well as some of the followup
and related work. It was submitted to the Springer Encyclopedia of Algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00011</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00011</id><created>2014-12-30</created><authors><author><keyname>Harrow</keyname><forenames>Aram W.</forenames></author></authors><title>Why now is the right time to study quantum computing</title><categories>quant-ph cs.CC</categories><comments>6 pages, written to explain quantum computing to computer science
  undergrads</comments><journal-ref>ACM XRDS: vol. 18, no. 3, pp. 32-37, Spring 2012</journal-ref><doi>10.1145/2090276.2090288</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum computing is a good way to justify difficult physics experiments. But
until quantum computers are built, do computer scientists need to know anything
about quantum information? In fact, quantum computing is not merely a recipe
for new computing devices, but a new way of looking at the world that has been
astonishingly intellectually productive. In this article, I'll talk about where
quantum computing came from, what it is, and what we can learn from it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00014</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00014</id><created>2014-12-30</created><authors><author><keyname>Cont</keyname><forenames>Rama</forenames></author><author><keyname>Heidari</keyname><forenames>Massoud</forenames></author></authors><title>Optimal rounding under integer constraints</title><categories>cs.DS math.OC</categories><msc-class>90C10, 90C27</msc-class><acm-class>G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given real numbers whose sum is an integer, we study the problem of finding
integers which match these real numbers as closely as possible, in the sense of
L^p norm, while preserving the sum. We describe the structure of solutions for
this integer optimization problem and propose an algorithm with complexity O(N
log N) for solving it. In contrast to fractional rounding and randomized
rounding, which yield biased estimators of the solution when applied to this
problem, our method yields an exact solution which minimizes the relative
rounding error across the set of all solutions for any value of p greater than
1, while avoiding the complexity of exhaustive search. The proposed algorithm
also solves a class of integer optimization problems with integer constraints
and may be used as the rounding step of relaxed integer programming problems,
for rounding real-valued solutions. We give several examples of applications
for the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00027</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00027</id><created>2014-12-23</created><authors><author><keyname>Werbos</keyname><forenames>Paul J.</forenames></author></authors><title>Time-Symmetric Physics: A Radical Approach to the Decoherence Problem</title><categories>cs.OH quant-ph</categories><comments>5 pages, 2 figures. Invited keynote talk for Quantum Computing
  workshop at Pacific Rim AI (PRICAI) conference, December 2014. Simple summary
  and link to the slides and audio of the talk posted at
  http://www.werbos.com/quantum.htm</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  The most powerful form of quantum learning system possible would somehow
learn the parameters W of a quantum system f(X, W), for f representing the
largest, most powerful set of possible input-output relations. This paper
addresses the issue of how to enlarge the set represented by f, by using a new
formulation of time-symmetric physics to model analog quantum computers based
on spin and by exploring possible sources of backwards-time free energy so as
to address problems of decoherence and dissipation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00029</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00029</id><created>2014-12-30</created><updated>2015-04-15</updated><authors><author><keyname>Gabora</keyname><forenames>Liane</forenames></author></authors><title>LIVEIA: A Light-based Immersive Visualization Environment for
  Imaginative Actualization</title><categories>cs.CY</categories><comments>6 pages in Gabora, L. (2015). Proc 12th International Conf on
  Information Technology: New Generations (pp. 686-691). Washington DC: IEEE
  Conf Publishing Services. arXiv admin note: substantial text overlap with
  arXiv:1409.1064</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes an immersive and interactive visualization environment
that uses light as a metaphor for psychological phenomena. Creative life force
is portrayed as ambient light, and peoples' psyches are represented by spheres
that amplify and transform light. Personality characteristics, situations, and
relationships are systematically depicted using a systematic visual language
based on the properties of light and how it interacts with physical objects.
The technology enables users to visualize and creatively experiment with
light-based representations of themselves and others, including patterns of
interaction and how they have come about, and how they could change and unfold
in the future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00033</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00033</id><created>2014-12-26</created><updated>2015-04-06</updated><authors><author><keyname>Chung</keyname><forenames>Kai-Min</forenames></author><author><keyname>Wu</keyname><forenames>Xiaodi</forenames></author><author><keyname>Yuen</keyname><forenames>Henry</forenames></author></authors><title>Parallel repetition for entangled k-player games via fast quantum search</title><categories>quant-ph cs.CC</categories><comments>This paper is a significantly revised version of arXiv:1411.1397,
  which erroneously claimed strong parallel repetition for free entangled
  games. Fixed author order to alphabetical</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present two parallel repetition theorems for the entangled value of
multi-player, one-round free games (games where the inputs come from a product
distribution). Our first theorem shows that for a $k$-player free game $G$ with
entangled value $\mathrm{val}^*(G) = 1 - \epsilon$, the $n$-fold repetition of
$G$ has entangled value $\mathrm{val}^*(G^{\otimes n})$ at most $(1 -
\epsilon^{3/2})^{\Omega(n/sk^4)}$, where $s$ is the answer length of any
player. In contrast, the best known parallel repetition theorem for the
classical value of two-player free games is $\mathrm{val}(G^{\otimes n}) \leq
(1 - \epsilon^2)^{\Omega(n/s)}$, due to Barak, et al. (RANDOM 2009). This
suggests the possibility of a separation between the behavior of entangled and
classical free games under parallel repetition.
  Our second theorem handles the broader class of free games $G$ where the
players can output (possibly entangled) quantum states. For such games, the
repeated entangled value is upper bounded by $(1 -
\epsilon^2)^{\Omega(n/sk^2)}$. We also show that the dependence of the exponent
on $k$ is necessary: we exhibit a $k$-player free game $G$ and $n \geq 1$ such
that $\mathrm{val}^*(G^{\otimes n}) \geq \mathrm{val}^*(G)^{n/k}$.
  Our analysis exploits the novel connection between communication protocols
and quantum parallel repetition, first explored by Chailloux and Scarpa (ICALP
2014). We demonstrate that better communication protocols yield better parallel
repetition theorems: our first theorem crucially uses a quantum search protocol
by Aaronson and Ambainis, which gives a quadratic speed-up for distributed
search problems. Finally, our results apply to a broader class of games than
were previously considered before; in particular, we obtain the first parallel
repetition theorem for entangled games involving more than two players, and for
games involving quantum outputs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00035</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00035</id><created>2014-12-30</created><authors><author><keyname>Zhang</keyname><forenames>Changchun</forenames></author><author><keyname>Qiu</keyname><forenames>Robert C.</forenames></author></authors><title>Massive MIMO testbed - Implementation and Initial Results in System
  Model Validation</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the design and implementation of a novel SDR based
massive MIMO testbed with up to 70 nodes built at Tennessee Technological
University. The deployment can reach a $30 \times 30$ antenna MIMO scheme. With
this testbed, we are able to measure the channel matrix and compute the
achievable rate of the massive MIMO system using experimental data. The
measured channel capacity is linearly increasing with the number of antennas of
the base station. We also demonstrate the channel reciprocity including the
circuits impact from the transmitter and receiver. We show that the Vandermonde
channel model is more realistic to describe the massive MIMO architecture than
the widely used Gaussian channel model, in terms of capacity. By adjusting the
range for angle of arrival $\alpha$ and the base station antenna distance $d$
during the simulation, we find out the Vandermonde model agrees with our
measured capacity at a certain $\alpha$ for each selected $d$ and the $\alpha$
is very close to that of the experiment deployment. It is the first time that
the feasibility of Vandermonde channel model is demonstrated by the experiment
for massive MIMO.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00037</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00037</id><created>2014-12-30</created><authors><author><keyname>Pei</keyname><forenames>Yuanli</forenames></author><author><keyname>Fern</keyname><forenames>Xiaoli Z.</forenames></author><author><keyname>Rosales</keyname><forenames>R&#xf3;mer</forenames></author><author><keyname>Tjahja</keyname><forenames>Teresa Vania</forenames></author></authors><title>Discriminative Clustering with Relative Constraints</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of clustering with relative constraints, where each
constraint specifies relative similarities among instances. In particular, each
constraint $(x_i, x_j, x_k)$ is acquired by posing a query: is instance $x_i$
more similar to $x_j$ than to $x_k$? We consider the scenario where answers to
such queries are based on an underlying (but unknown) class concept, which we
aim to discover via clustering. Different from most existing methods that only
consider constraints derived from yes and no answers, we also incorporate don't
know responses. We introduce a Discriminative Clustering method with Relative
Constraints (DCRC) which assumes a natural probabilistic relationship between
instances, their underlying cluster memberships, and the observed constraints.
The objective is to maximize the model likelihood given the constraints, and in
the meantime enforce cluster separation and cluster balance by also making use
of the unlabeled instances. We evaluated the proposed method using constraints
generated from ground-truth class labels, and from (noisy) human judgments from
a user study. Experimental results demonstrate: 1) the usefulness of relative
constraints, in particular when don't know answers are considered; 2) the
improved performance of the proposed method over state-of-the-art methods that
utilize either relative or pairwise constraints; and 3) the robustness of our
method in the presence of noisy constraints, such as those provided by human
judgement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00039</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00039</id><created>2014-12-30</created><updated>2015-01-05</updated><authors><author><keyname>Cusick</keyname><forenames>James J.</forenames></author><author><keyname>Miller</keyname><forenames>William</forenames></author><author><keyname>Laurita</keyname><forenames>Nicholas</forenames></author><author><keyname>Pitt</keyname><forenames>Tasha</forenames></author></authors><title>Design, Construction, and Use of a Single Board Computer Beowulf
  Cluster: Application of the Small-Footprint, Low-Cost, InSignal 5420 Octa
  Board</title><categories>cs.DC</categories><comments>9 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years development in the area of Single Board Computing has been
advancing rapidly. At Wolters Kluwer's Corporate Legal Services Division a
prototyping effort was undertaken to establish the utility of such devices for
practical and general computing needs. This paper presents the background of
this work, the design and construction of a 64 core 96 GHz cluster, and their
possibility of yielding approximately 400 GFLOPs from a set of small footprint
InSignal boards created for just over $2,300. Additionally this paper discusses
the software environment on the cluster, the use of a standard Beowulf library
and its operation, as well as other software application uses including Elastic
Search and ownCloud. Finally, consideration will be given to the future use of
such technologies in a business setting in order to introduce new Open Source
technologies, reduce computing costs, and improve Time to Market.
  Index Terms: Single Board Computing, Raspberry Pi, InSignal Exynos 5420,
Linaro Ubuntu Linux, High Performance Computing, Beowulf clustering, Open
Source, MySQL, MongoDB, ownCloud, Computing Architectures, Parallel Computing,
Cluster Computing
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00040</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00040</id><created>2014-12-30</created><updated>2015-02-14</updated><authors><author><keyname>Bazzi</keyname><forenames>Marya</forenames></author><author><keyname>Porter</keyname><forenames>Mason A.</forenames></author><author><keyname>Williams</keyname><forenames>Stacy</forenames></author><author><keyname>McDonald</keyname><forenames>Mark</forenames></author><author><keyname>Fenn</keyname><forenames>Daniel J.</forenames></author><author><keyname>Howison</keyname><forenames>Sam D.</forenames></author></authors><title>Community detection in temporal multilayer networks, and its application
  to correlation networks</title><categories>physics.soc-ph cs.SI nlin.AO q-fin.ST</categories><comments>39 pages, 10 figures (many with multiple parts)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Networks are a convenient way to represent complex systems of interacting
entities. Many networks contain &quot;communities&quot; of nodes that are more densely
connected to each other than to nodes in the rest of the network. In this
paper, we investigate the detection of communities in temporal networks
represented as multilayer networks. As a focal example, we study time-dependent
financial-asset correlation networks. We first argue that the use of the
&quot;modularity&quot; quality function---which is defined by comparing edge weights in
an observed network to expected edge weights in a &quot;null network&quot;---is
application-dependent. We differentiate between &quot;null networks&quot; and &quot;null
models&quot; in our discussion of modularity maximization, and we highlight that the
same null network can correspond to different null models. We then investigate
a multilayer modularity-maximization problem to identify communities in
temporal networks. Our multilayer analysis only depends on the form of the
maximization problem and not on the specific quality function that one chooses.
We introduce a diagnostic to measure \emph{persistence} of community structure
in a multilayer network partition. We prove several results that describe how
the multilayer maximization problem measures a trade-off between static
community structure within layers and higher values of persistence across
layers. We also discuss some implementation issues that the popular &quot;Louvain&quot;
heuristic faces with temporal multilayer networks and suggest ways to mitigate
them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00046</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00046</id><created>2014-12-30</created><updated>2015-04-06</updated><authors><author><keyname>Bahmani</keyname><forenames>Sohail</forenames></author><author><keyname>Romberg</keyname><forenames>Justin</forenames></author></authors><title>Lifting for Blind Deconvolution in Random Mask Imaging: Identifiability
  and Convex Relaxation</title><categories>cs.IT math.FA math.IT math.NA math.OC math.ST stat.TH</categories><journal-ref>SIAM J. Imaging Sci. 8(4):2203--2238, 2015</journal-ref><doi>10.1137/141002165</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we analyze the blind deconvolution of an image and an unknown
blur in a coded imaging system. The measurements consist of subsampled
convolution of an unknown blurring kernel with multiple random binary
modulations (coded masks) of the image. To perform the deconvolution, we
consider a standard lifting of the image and the blurring kernel that
transforms the measurements into a set of linear equations of the matrix formed
by their outer product. Any rank-one solution to this system of equation
provides a valid pair of an image and a blur.
  We first express the necessary and sufficient conditions for the uniqueness
of a rank-one solution under some additional assumptions (uniform subsampling
and no limit on the number of coded masks). These conditions are special case
of a previously established result regarding identifiability in the matrix
completion problem. We also characterize a low-dimensional subspace model for
the blur kernel that is sufficient to guarantee identifiability, including the
interesting instance of &quot;bandpass&quot;` blur kernels.
  Next, assuming the bandpass model for the blur kernel, we show that the image
and the blur kernel can be found using nuclear norm minimization. Our main
results show that recovery is achieved (with high probability) when the number
of masks is on the order of
$\mu\log^{2}L\,\log\frac{Le}{\mu}\,\log\log\left(N+1\right)$ where $\mu$ is the
\emph{coherence} of the blur, $L$ is the dimension of the image, and $N$ is the
number of measured samples per mask.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00048</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00048</id><created>2014-12-30</created><authors><author><keyname>Georgakoudis</keyname><forenames>Giorgis</forenames></author><author><keyname>Gillan</keyname><forenames>Charles J.</forenames></author><author><keyname>Sayed</keyname><forenames>Ahmed</forenames></author><author><keyname>Spence</keyname><forenames>Ivor</forenames></author><author><keyname>Faloon</keyname><forenames>Richard</forenames></author><author><keyname>Nikolopoulos</keyname><forenames>Dimitrios S.</forenames></author></authors><title>Methods and Metrics for Fair Server Assessment under Real-Time Financial
  Workloads</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy efficiency has been a daunting challenge for datacenters. The
financial industry operates some of the largest datacenters in the world. With
increasing energy costs and the financial services sector growth, emerging
financial analytics workloads may incur extremely high operational costs, to
meet their latency targets. Microservers have recently emerged as an
alternative to high-end servers, promising scalable performance and low energy
consumption in datacenters via scale-out. Unfortunately, stark differences in
architectural features, form factor and design considerations make a fair
comparison between servers and microservers exceptionally challenging. In this
paper we present a rigorous methodology and new metrics for fair comparison of
server and microserver platforms. We deploy our methodology and metrics to
compare a microserver with ARM cores against two servers with x86 cores,
running the same real-time financial analytics workload. We define
workload-specific but platform-independent performance metrics for platform
comparison, targeting both datacenter operators and end users. Our methodology
establishes that a server based the Xeon Phi processor delivers the highest
performance and energy-efficiency. However, by scaling out energy-efficient
microservers, we achieve competitive or better energy-efficiency than a
power-equivalent server with two Sandy Bridge sockets despite the microserver's
slower cores. Using a new iso-QoS (iso-Quality of Service) metric, we find that
the ARM microserver scales enough to meet market throughput demand, i.e. a 100%
QoS in terms of timely option pricing, with as little as 55% of the energy
consumed by the Sandy Bridge server.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00052</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00052</id><created>2014-12-30</created><authors><author><keyname>Huggins</keyname><forenames>Jonathan H.</forenames></author><author><keyname>Saeedi</keyname><forenames>Ardavan</forenames></author><author><keyname>Johnson</keyname><forenames>Matthew J.</forenames></author></authors><title>Detailed Derivations of Small-Variance Asymptotics for some Hierarchical
  Bayesian Nonparametric Models</title><categories>stat.ML cs.LG</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note we provide detailed derivations of two versions of
small-variance asymptotics for hierarchical Dirichlet process (HDP) mixture
models and the HDP hidden Markov model (HDP-HMM, a.k.a. the infinite HMM). We
include derivations for the probabilities of certain CRP and CRF partitions,
which are of more general interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00067</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00067</id><created>2014-12-30</created><authors><author><keyname>Liu</keyname><forenames>Xiaoming</forenames></author><author><keyname>Zhou</keyname><forenames>Yadong</forenames></author><author><keyname>Guan</keyname><forenames>Xiaohong</forenames></author></authors><title>A Feasible Graph Partition Framework for Random Walks Implemented by
  Parallel Computing in Big Graph</title><categories>cs.SI cs.DC physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph partition is a fundamental problem of parallel computing for big graph
data. Many graph partition algorithms have been proposed to solve the problem
in various applications, such as matrix computations and PageRank, etc., but
none has pay attention to random walks. Random walks is a widely used method to
explore graph structure in lots of fields. The challenges of graph partition
for random walks include the large number of times of communication between
partitions, lots of replications of the vertices, unbalanced partition, etc. In
this paper, we propose a feasible graph partition framework for random walks
implemented by parallel computing in big graph. The framework is based on two
optimization functions to reduce the bandwidth, memory and storage cost in the
condition that the load balance is guaranteed. In this framework, several
greedy graph partition algorithms are proposed. We also propose five metrics
from different perspectives to evaluate the performance of these algorithms. By
running the algorithms on the big graph data set of real world, the
experimental results show that these algorithms in the framework are capable of
solving the problem of graph partition for random walks for different needs,
e.g. the best result is improved more than 70 times in reducing the times of
communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00076</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00076</id><created>2014-12-30</created><authors><author><keyname>Abrego</keyname><forenames>Bernardo</forenames></author><author><keyname>Fernandez-Merchant</keyname><forenames>Silvia</forenames></author><author><keyname>Katz</keyname><forenames>Daniel J.</forenames></author><author><keyname>Kolesnikov</keyname><forenames>Levon</forenames></author></authors><title>On The Number of Similar Instances of a Pattern in a Finite Set</title><categories>math.CO cs.CG</categories><comments>19 pages</comments><msc-class>52C10, 05A15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  New bounds on the number of similar or directly similar copies of a pattern
within a finite subset of the line or the plane are proved. The number of
equilateral triangles whose vertices all lie within an $n$-point subset of the
plane is shown to be no more than $\lfloor{(4 n-1)(n-1)/18}\rfloor$. The number
of $k$-term arithmetic progressions that lie within an $n$-point subset of the
line is shown to be at most $(n-r)(n+r-k+1)/(2 k-2)$, where $r$ is the
remainder when $n$ is divided by $k-1$. This upper bound is achieved when the
$n$ points themselves form an arithmetic progression, but for some values of
$k$ and $n$, it can also be achieved for other configurations of the $n$
points, and a full classification of such optimal configurations is given.
These results are achieved using a new general method based on ordering
relations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00077</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00077</id><created>2014-12-31</created><authors><author><keyname>Lee</keyname><forenames>Namyoon</forenames></author><author><keyname>Dimakis</keyname><forenames>Alexandros G.</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>Index Coding with Coded Side-Information</title><categories>cs.IT math.IT</categories><comments>A short version will be appeared in IEEE Communications Letter</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter investigates a new class of index coding problems. One sender
broadcasts packets to multiple users, each desiring a subset, by exploiting
prior knowledge of linear combinations of packets. We refer to this class of
problems as index coding with coded side-information. Our aim is to
characterize the minimum index code length that the sender needs to transmit to
simultaneously satisfy all user requests. We show that the optimal binary
vector index code length is equal to the minimum rank (minrank) of a matrix
whose elements consist of the sets of desired packet indices and side-
information encoding matrices. This is the natural extension of matrix minrank
in the presence of coded side information. Using the derived expression, we
propose a greedy randomized algorithm to minimize the rank of the derived
matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00078</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00078</id><created>2014-12-31</created><authors><author><keyname>Wang</keyname><forenames>Ning</forenames></author><author><keyname>Hossain</keyname><forenames>Ekram</forenames></author><author><keyname>Bhargava</keyname><forenames>Vijay K.</forenames></author></authors><title>Joint Downlink Cell Association and Bandwidth Allocation for Wireless
  Backhauling in Two-Tier HetNets with Large-Scale Antenna Arrays</title><categories>cs.NI cs.IT math.IT</categories><comments>IEEE Transactions on Wireless Communications, to appear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of joint downlink cell association (CA) and wireless backhaul
bandwidth allocation (WBBA) in two-tier cellular heterogeneous networks
(HetNets) is considered. Large-scale antenna array is implemented at the macro
base station (BS), while the small cells within the macro cell range are
single-antenna BSs and they rely on over-the-air links to the macro BS for
backhauling. A sum logarithmic user rate maximization problem is investigated
considering wireless backhauling constraints. A duplex and spectrum sharing
scheme based on co-channel reverse time-division duplex (TDD) and dynamic soft
frequency reuse (SFR) is proposed for interference management in two-tier
HetNets with large-scale antenna arrays at the macro BS and wireless
backhauling for small cells. Two in-band WBBA scenarios, namely, unified
bandwidth allocation and per-small-cell bandwidth allocation scenarios, are
investigated for joint CA-WBBA in the HetNet. A two-level hierarchical
decomposition method for relaxed optimization is employed to solve the
mixed-integer nonlinear program (MINLP). Solutions based on the General
Algorithm Modeling System (GAMS) optimization solver and fast heuristics are
also proposed for cell association in the per-small-cell WBBA scenario. It is
shown that when all small cells have to use in-band wireless backhaul, the
system load has more impact on both the sum log-rate and per-user rate
performance than the number of small cells deployed within the macro cell
range. The proposed joint CA-WBBA algorithms have an optimal load approximately
equal to the size of the large-scale antenna array at the macro BS. The cell
range expansion (CRE) strategy, which is an efficient cell association scheme
for HetNets with perfect backhauling, is shown to be inefficient when in-band
wireless backhauling for small cells comes into play.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00080</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00080</id><created>2014-12-31</created><updated>2015-09-10</updated><authors><author><keyname>Lee</keyname><forenames>Namyoon</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>Advanced Interference Management Technique: Potentials and Limitations</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Wireless Communications Magazine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interference management has the potential to improve spectrum efficiency in
current and next generation wireless systems (e.g. 3GPP LTE and IEEE 802.11).
Recently, new paradigms for interference management have emerged to tackle
interference in a general class of wireless networks: interference shaping and
interference exploitation. Both approaches offer better performance in
interference-limited communication regimes than traditionally thought possible.
This article provides a high-level overview of several different interference
shaping and exploitation techniques for single-hop, multi-hop, and multi-way
network architectures. Graphical illustrations that explain the intuition
behind each strategy are provided. The article concludes with a discussion of
practical challenges associated with adopting sophisticated interference
management strategies in the future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00092</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00092</id><created>2014-12-31</created><updated>2015-07-31</updated><authors><author><keyname>Dong</keyname><forenames>Chao</forenames></author><author><keyname>Loy</keyname><forenames>Chen Change</forenames></author><author><keyname>He</keyname><forenames>Kaiming</forenames></author><author><keyname>Tang</keyname><forenames>Xiaoou</forenames></author></authors><title>Image Super-Resolution Using Deep Convolutional Networks</title><categories>cs.CV cs.NE</categories><comments>14 pages, 14 figures, journal</comments><acm-class>I.4.5; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a deep learning method for single image super-resolution (SR). Our
method directly learns an end-to-end mapping between the low/high-resolution
images. The mapping is represented as a deep convolutional neural network (CNN)
that takes the low-resolution image as the input and outputs the
high-resolution one. We further show that traditional sparse-coding-based SR
methods can also be viewed as a deep convolutional network. But unlike
traditional methods that handle each component separately, our method jointly
optimizes all layers. Our deep CNN has a lightweight structure, yet
demonstrates state-of-the-art restoration quality, and achieves fast speed for
practical on-line usage. We explore different network structures and parameter
settings to achieve trade-offs between performance and speed. Moreover, we
extend our network to cope with three color channels simultaneously, and show
better overall reconstruction quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00100</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00100</id><created>2014-12-31</created><updated>2015-04-14</updated><authors><author><keyname>Gramaglia</keyname><forenames>Marco</forenames></author><author><keyname>Fiore</keyname><forenames>Marco</forenames></author></authors><title>On the anonymizability of mobile traffic datasets</title><categories>cs.CY cs.CR</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Preserving user privacy is paramount when it comes to publicly disclosed
datasets that contain fine-grained data about large populations. The problem is
especially critical in the case of mobile traffic datasets collected by
cellular operators, as they feature elevate subscriber trajectory uniqueness
and they are resistant to anonymization through spatiotemporal generalization.
In this work, we investigate the $k$-anonymizability of trajectories in two
large-scale mobile traffic datasets, by means of a novel dedicated measure. Our
results are in agreement with those of previous analyses, however they also
provide additional insights on the reasons behind the poor anonimizability of
mobile traffic datasets. As such, our study is a step forward in the direction
of a more robust dataset anonymization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00102</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00102</id><created>2014-12-31</created><updated>2015-06-06</updated><authors><author><keyname>Neverova</keyname><forenames>Natalia</forenames></author><author><keyname>Wolf</keyname><forenames>Christian</forenames></author><author><keyname>Taylor</keyname><forenames>Graham W.</forenames></author><author><keyname>Nebout</keyname><forenames>Florian</forenames></author></authors><title>ModDrop: adaptive multi-modal gesture recognition</title><categories>cs.CV cs.HC cs.LG</categories><comments>14 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method for gesture detection and localisation based on
multi-scale and multi-modal deep learning. Each visual modality captures
spatial information at a particular spatial scale (such as motion of the upper
body or a hand), and the whole system operates at three temporal scales. Key to
our technique is a training strategy which exploits: i) careful initialization
of individual modalities; and ii) gradual fusion involving random dropping of
separate channels (dubbed ModDrop) for learning cross-modality correlations
while preserving uniqueness of each modality-specific representation. We
present experiments on the ChaLearn 2014 Looking at People Challenge gesture
recognition track, in which we placed first out of 17 teams. Fusing multiple
modalities at several spatial and temporal scales leads to a significant
increase in recognition rates, allowing the model to compensate for errors of
the individual classifiers as well as noise in the separate channels.
Futhermore, the proposed ModDrop training technique ensures robustness of the
classifier to missing signals in one or several channels to produce meaningful
predictions from any number of available modalities. In addition, we
demonstrate the applicability of the proposed fusion scheme to modalities of
arbitrary nature by experiments on the same dataset augmented with audio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00105</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00105</id><created>2014-12-31</created><authors><author><keyname>Anbarjafari</keyname><forenames>Gholamreza</forenames></author></authors><title>Face recognition using color local binary pattern from mutually
  independent color channels</title><categories>cs.CV</categories><comments>11 pages in EURASIP Journal on Image and Video Processing, 2013</comments><doi>10.1186/1687-5281-2013-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a high performance face recognition system based on local
binary pattern (LBP) using the probability distribution functions (PDF) of
pixels in different mutually independent color channels which are robust to
frontal homogenous illumination and planer rotation is proposed. The
illumination of faces is enhanced by using the state-of-the-art technique which
is using discrete wavelet transform (DWT) and singular value decomposition
(SVD). After equalization, face images are segmented by use of local Successive
Mean Quantization Transform (SMQT) followed by skin color based face detection
system. Kullback-Leibler Distance (KLD) between the concatenated PDFs of a
given face obtained by LBP and the concatenated PDFs of each face in the
database is used as a metric in the recognition process. Various decision
fusion techniques have been used in order to improve the recognition rate. The
proposed system has been tested on the FERET, HP, and Bosphorus face databases.
The proposed system is compared with conventional and thestate-of-the-art
techniques. The recognition rates obtained using FVF approach for FERET
database is 99.78% compared with 79.60% and 68.80% for conventional gray scale
LBP and Principle Component Analysis (PCA) based face recognition techniques
respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00108</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00108</id><created>2014-12-31</created><authors><author><keyname>Anbarjafari</keyname><forenames>Gholamreza</forenames></author></authors><title>HSI based colour image equalization using iterative nth root and nth
  power</title><categories>cs.CV cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper an equalization technique for colour images is introduced. The
method is based on nth root and nth power equalization approach but with
optimization of the mean of the image in different colour channels such as RGB
and HSI. The performance of the proposed method has been measured by the means
of peak signal to noise ratio. The proposed algorithm has been compared with
conventional histogram equalization and the visual and quantitative
experimental results are showing that the proposed method over perform the
histogram equalization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00125</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00125</id><created>2014-12-31</created><authors><author><keyname>Wu</keyname><forenames>Hao</forenames></author></authors><title>Maximum Margin Clustering for State Decomposition of Metastable Systems</title><categories>cs.LG cs.NA cs.SY math.NA physics.data-an</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When studying a metastable dynamical system, a prime concern is how to
decompose the phase space into a set of metastable states. Unfortunately, the
metastable state decomposition based on simulation or experimental data is
still a challenge. The most popular and simplest approach is geometric
clustering which is developed based on the classical clustering technique.
However, the prerequisites of this approach are: (1) data are obtained from
simulations or experiments which are in global equilibrium and (2) the
coordinate system is appropriately selected. Recently, the kinetic clustering
approach based on phase space discretization and transition probability
estimation has drawn much attention due to its applicability to more general
cases, but the choice of discretization policy is a difficult task. In this
paper, a new decomposition method designated as maximum margin metastable
clustering is proposed, which converts the problem of metastable state
decomposition to a semi-supervised learning problem so that the large margin
technique can be utilized to search for the optimal decomposition without phase
space discretization. Moreover, several simulation examples are given to
illustrate the effectiveness of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00144</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00144</id><created>2014-12-31</created><authors><author><keyname>Mas</keyname><forenames>Massimiliano Dal</forenames></author></authors><title>Liquidity on Web Dynamic Network</title><categories>cs.SI</categories><comments>6 pages, 2 figures; for details see: http://www.maxdalmas.com</comments><msc-class>03B65, 03G10, 68M11, 68P05, 68Q55, 68T30, 68U35</msc-class><acm-class>D.2.2; G.1.10; G.2.2; H.1.1; H.1.2; H.3.1; H.3.3; H.3.5; H.5.2;
  H.5.3; H.5.4; I.2.1; I.2.4; I.2.7; I.3.6; K.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, the exponentially growing of the Web renders the problem of
correlation among different topics of paramount importance. The proposed model
can be used to study the evolution of network depicted by different topics on
the web correlated by a dynamic &quot;fluid&quot; of tags among them. The fluid-dynamic
model depicted is completely evolutive, thus it is able to describe the dynamic
situation of a network at every instant of time. This overcomes the
difficulties encountered by many static models. The theory permits the
development of efficient numerical schemes also for very large networks. This
is possible since dynamic flow at junctions is modeled in a simple and
computationally convenient way (resorting to a linear programming problem). The
obtained model consists of a single conservation law and is on one side simple
enough to permit a complete understanding, on the other side reach enough to
detect the evolution of the dynamic network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00149</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00149</id><created>2014-12-31</created><authors><author><keyname>Lopes</keyname><forenames>Edson</forenames></author><author><keyname>Caetano</keyname><forenames>Jos&#xe9;</forenames></author><author><keyname>Abreu</keyname><forenames>Ant&#xf3;nio</forenames></author><author><keyname>Grilo</keyname><forenames>Frederico</forenames></author></authors><title>iReclass - An automatic system for recording classes</title><categories>cs.CY cs.HC</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the details of a system capable of recording on video a
traditional class. By traditional class it is meant a teacher, a blackboard and
a white canvas where course notes are projected. The system is able to track
the movements of the lecturer, while recording it on video at the required
frame rate (e.g., 25 fps). The system is also capable of understanding five arm
gestures made by the lecturer with the intent of controlling which scenario is
recorded: himself, the blackboard or the white canvas. The remaining two
gestures are for start/stop the recorder. The system is composed by a Kinect
sensor, a video camera, a microphone, one pan-tilt system and one pan system,
using a total of three step motors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00151</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00151</id><created>2014-12-31</created><authors><author><keyname>Xing</keyname><forenames>Zhengli</forenames></author><author><keyname>Zhou</keyname><forenames>Jie</forenames></author><author><keyname>Ye</keyname><forenames>Jiangfeng</forenames></author><author><keyname>Yan</keyname><forenames>Jun</forenames></author><author><keyname>Zou</keyname><forenames>Lin</forenames></author><author><keyname>Wan</keyname><forenames>Qun</forenames></author></authors><title>A Novel Compressed Sensing Based Model for Reconstructing Sparse Signals
  Using Phase Sparse Character</title><categories>cs.IT math.IT</categories><comments>8 pages, 39 figures, subjected to &quot;Elektronika ir Elektrotechnika&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Phase modulation is a commonly used modulation mode in digital communication,
which usually brings phase sparsity to digital signals. It is naturally to
connect the sparsity with the newly emerged theory of compressed sensing (CS),
which enables sub-Nyquist sampling of high-bandwidth to sparse signals. For the
present, applications of CS theory in communication field mainly focus on
spectrum sensing, sparse channel estimation etc. Few of current researches take
the phase sparse character into consideration. In this paper, we establish the
novel model of phase modulation signals based on phase sparsity, and introduce
CS theory to the phase domain. According to CS theory, rather than the
bandwidth, the sampling rate required here is scaling with the symbol rate,
which is usually much lower than the Nyquist rate. In this paper, we provide
analytical support for the model, and simulations verify its validity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00153</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00153</id><created>2014-12-31</created><updated>2016-01-21</updated><authors><author><keyname>Westerb&#xe4;ck</keyname><forenames>Thomas</forenames></author><author><keyname>Freij</keyname><forenames>Ragnar</forenames></author><author><keyname>Ernvall</keyname><forenames>Toni</forenames></author><author><keyname>Hollanti</keyname><forenames>Camilla</forenames></author></authors><title>On the Combinatorics of Locally Repairable Codes via Matroid Theory</title><categories>cs.IT math.CO math.IT</categories><comments>48 pages. Submitted for publication. In this version: The text has
  been edited to improve the readability. Parameter d for matroids is now
  defined by the use of the rank function instead of the dual matroid. Typos
  are corrected. Section III is divided into two parts, and some numberings of
  theorems etc. have been changed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides a link between matroid theory and locally repairable
codes (LRCs) that are either linear or more generally almost affine. Using this
link, new results on both LRCs and matroid theory are derived. The parameters
$(n,k,d,r,\delta)$ of LRCs are generalized to matroids, and the matroid
analogue of the generalized Singleton bound in [P. Gopalan et al., &quot;On the
locality of codeword symbols,&quot; IEEE Trans. Inf. Theory] for linear LRCs is
given for matroids. It is shown that the given bound is not tight for certain
classes of parameters, implying a nonexistence result for the corresponding
locally repairable almost affine codes, that are coined perfect in this paper.
  Constructions of classes of matroids with a large span of the parameters
$(n,k,d,r,\delta)$ and the corresponding local repair sets are given. Using
these matroid constructions, new LRCs are constructed with prescribed
parameters. The existence results on linear LRCs and the nonexistence results
on almost affine LRCs given in this paper strengthen the nonexistence and
existence results on perfect linear LRCs given in [W. Song et al., &quot;Optimal
locally repairable codes,&quot; IEEE J. Sel. Areas Comm.].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00154</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00154</id><created>2014-12-31</created><authors><author><keyname>Xing</keyname><forenames>Zhengli</forenames></author><author><keyname>Zhou</keyname><forenames>Jie</forenames></author><author><keyname>Ye</keyname><forenames>Jiangfeng</forenames></author><author><keyname>Yan</keyname><forenames>Jun</forenames></author><author><keyname>Zou</keyname><forenames>Lin</forenames></author><author><keyname>Wan</keyname><forenames>Qun</forenames></author></authors><title>Automatic Modulation Recognition of PSK Signals Using Nonuniform
  Compressive Samples Based on High Order Statistics</title><categories>cs.IT math.IT</categories><comments>4 pages, 6 figures, submitted to the International Conference on
  Communications Problem -Solving (ICCP) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Phase modulation is a commonly used modulation mode in digital communication,
which usually brings phase sparsity to digital signals. It is naturally to
connect the sparsity with the newly emerged theory of compressed sensing (CS),
which enables sub-Nyquist sampling of high-bandwidth to sparse signals. For the
present, applications of CS theory in communication field mainly focus on
spectrum sensing, sparse channel estimation etc. Few of current researches take
the phase sparse character into consideration. In this paper, we establish the
novel model of phase modulation signals based on phase sparsity, and introduce
CS theory to the phase domain. According to CS theory, rather than the
bandwidth, the sampling rate required here is scaling with the symbol rate,
which is usually much lower than the Nyquist rate. In this paper, we provide
analytical support for the model, and simulations verify its validity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00158</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00158</id><created>2014-12-31</created><authors><author><keyname>Xing</keyname><forenames>Zhengli</forenames></author><author><keyname>Zhou</keyname><forenames>Jie</forenames></author><author><keyname>Ye</keyname><forenames>Jiangfeng</forenames></author><author><keyname>Yan</keyname><forenames>Jun</forenames></author><author><keyname>Zou</keyname><forenames>Jifeng</forenames></author><author><keyname>Zou</keyname><forenames>Lin</forenames></author><author><keyname>Wan</keyname><forenames>Qun</forenames></author></authors><title>Automatic Modulation Recognition of PSK Signals with Sub-Nyquist
  Sampling Based on High Order Statistics</title><categories>cs.IT math.IT</categories><comments>7 pages, 8 figures, submitted to IEEE International Symposium on
  Signal Processing and Information Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sampling rate required in the Nth Power Nonlinear Transformation (NPT) method
is typically much greater than Nyquist rate, which causes heavy burden for the
Analog to Digital Converter (ADC). Taking advantage of the sparse property of
PSK signals' spectrum under NPT, we develop the NPT method for PSK signals with
Sub-Nyquist rate samples. In this paper, combined the NPT method with
Compressive Sensing (CS) theory, frequency spectrum reconstruction of the Nth
power nonlinear transformation of PSK signals is presented, which can be
further used for AMR and rough estimations of unknown carrier frequency and
symbol rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00160</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00160</id><created>2014-12-31</created><updated>2015-01-05</updated><authors><author><keyname>Batenkov</keyname><forenames>Dmitry</forenames></author></authors><title>Accurate solution of near-colliding Prony systems via decimation and
  homotopy continuation</title><categories>cs.NA cs.SC math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider polynomial systems of Prony type, appearing in many areas of
mathematics. Their robust numerical solution is considered to be difficult,
especially in &quot;near-colliding&quot; situations. We transform the nonlinear part of
the Prony system into a Hankel-type polynomial system. Combining this
representation with a recently discovered &quot;decimation&quot; technique, we present an
algorithm which applies homotopy continuation to an appropriately chosen
Hankel-type system as above. In this way, we are able to solve for the
nonlinear variables of the original system with high accuracy when the data is
perturbed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00162</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00162</id><created>2014-12-31</created><authors><author><keyname>Babka</keyname><forenames>Martin</forenames></author></authors><title>Expected number of uniformly distributed balls in a most loaded bin
  using placement with simple linear functions</title><categories>cs.DS</categories><acm-class>E.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We estimate the size of a most loaded bin in the setting when the balls are
placed into the bins using a random linear function in a finite field. The
balls are chosen from a transformed interval. We show that in this setting the
expected load of the most loaded bins is constant.
  This is an interesting fact because using fully random hash functions with
the same class of input sets leads to an expectation of $\Theta\left(\frac{\log
m}{\log \log m}\right)$ balls in most loaded bins where $m$ is the number of
balls and bins.
  Although the family of the functions is quite common the size of largest bins
was not known even in this simple case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00163</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00163</id><created>2014-12-31</created><authors><author><keyname>Xiao</keyname><forenames>Li</forenames></author><author><keyname>Xia</keyname><forenames>Xiang-Gen</forenames></author></authors><title>Error Correction in Polynomial Remainder Codes with Non-Pairwise Coprime
  Moduli and Robust Chinese Remainder Theorem for Polynomials</title><categories>cs.IT math.IT</categories><comments>12 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates polynomial remainder codes with non-pairwise coprime
moduli. We first consider a robust reconstruction problem for polynomials from
erroneous residues when the degrees of all residue errors are assumed small,
namely robust Chinese Remainder Theorem (CRT) for polynomials. It basically
says that a polynomial can be reconstructed from erroneous residues such that
the degree of the reconstruction error is upper bounded by $\tau$ whenever the
degrees of all residue errors are upper bounded by $\tau$, where a sufficient
condition for $\tau$ and a reconstruction algorithm are obtained. By releasing
the constraint that all residue errors have small degrees, another robust
reconstruction is then presented when there are multiple unrestricted errors
and an arbitrary number of errors with small degrees in the residues. By making
full use of redundancy in moduli, we obtain a stronger residue error correction
capability in the sense that apart from the number of errors that can be
corrected in the previous existing result, some errors with small degrees can
be also corrected in the residues. With this newly obtained result,
improvements in uncorrected error probability and burst error correction
capability in a data transmission are illustrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00166</identifier>
 <datestamp>2015-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00166</id><created>2014-12-31</created><updated>2015-07-03</updated><authors><author><keyname>Ahadpour</keyname><forenames>Sodeif</forenames></author><author><keyname>Sadra</keyname><forenames>Yaser</forenames></author></authors><title>Chaotic trigonometric haar wavelet with focus on image encryption</title><categories>cs.CR nlin.CD</categories><comments>10pages, 9 figures,2 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, after reviewing the main points of Haar wavelet transform and
chaotic trigonometric maps, we introduce a new perspective of Haar wavelet
transform. The essential idea of the paper is given linearity properties of the
scaling function of the Haar wavelet. With regard to applications of Haar
wavelet transform in image processing, we introduce chaotic trigonometric Haar
wavelet transform to encrypt the plain images. In addition, the encrypted
images based on a proposed algorithm were made. To evaluate the security of the
encrypted images, the key space analysis, the correlation coefficient analysis
and differential attack were performed. Here, the chaotic trigonometric Haar
wavelet transform tries to improve the problem of failure of encryption such as
small key space and level of security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00178</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00178</id><created>2014-12-31</created><updated>2015-09-13</updated><authors><author><keyname>Lenstra</keyname><forenames>H. W.</forenames><suffix>Jr.</suffix></author><author><keyname>Silverberg</keyname><forenames>A.</forenames></author></authors><title>Lattices with Symmetry</title><categories>math.NT cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For large ranks, there is no good algorithm that decides whether a given
lattice has an orthonormal basis. But when the lattice is given with enough
symmetry, we can construct a provably deterministic polynomial-time algorithm
to accomplish this, based on the work of Gentry and Szydlo. The techniques
involve algorithmic algebraic number theory, analytic number theory,
commutative algebra, and lattice basis reduction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00179</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00179</id><created>2014-12-31</created><updated>2015-08-28</updated><authors><author><keyname>Bubenik</keyname><forenames>Peter</forenames></author><author><keyname>Dlotko</keyname><forenames>Pawel</forenames></author></authors><title>A persistence landscapes toolbox for topological statistics</title><categories>cs.CG cs.MS math.AT stat.CO</categories><comments>24 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Topological data analysis provides a multiscale description of the geometry
and topology of quantitative data. The persistence landscape is a topological
summary that can be easily combined with tools from statistics and machine
learning. We give efficient algorithms for calculating persistence landscapes,
their averages, and distances between such averages. We discuss an
implementation of these algorithms and some related procedures. These are
intended to facilitate the combination of statistics and machine learning with
topological data analysis. We present an experiment showing that the
low-dimensional persistence landscapes of points sampled from spheres (and
boxes) of varying dimensions differ.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00180</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00180</id><created>2014-12-31</created><authors><author><keyname>Borgohain</keyname><forenames>Tuhin</forenames></author><author><keyname>Borgohain</keyname><forenames>Amardeep</forenames></author><author><keyname>Borgohain</keyname><forenames>Rajdeep</forenames></author><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author></authors><title>Multipath Routing of Fragmented Data Transfer in a Smart Grid
  Environment</title><categories>cs.CR</categories><comments>5 pages, 2 figures</comments><doi>10.5120/19528-1165</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this paper is to do a general survey on the existing
communication modes inside a smart grid, the existing security loopholes and
their countermeasures. Then we suggest a detailed countermeasure, building upon
the Jigsaw based secure data transfer [8] for enhanced security of the data
flow inside the communication system of a smart grid. The paper has been
written without the consideration of any factor of inoperability between the
various security techniques inside a smart grid
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00182</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00182</id><created>2014-12-31</created><updated>2015-01-07</updated><authors><author><keyname>Portnoi</keyname><forenames>Marcos</forenames></author><author><keyname>Zurawsky</keyname><forenames>Jason</forenames></author><author><keyname>Swany</keyname><forenames>Martin</forenames></author></authors><title>An information services algorithm to heuristically summarize IP
  addresses for a distributed, hierarchical directory service</title><categories>cs.DC</categories><comments>Grid Computing (GRID), 2010 11th IEEE/ACM International Conference
  on, 25-28 Oct. 2010</comments><doi>10.1109/GRID.2010.5697950</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A distributed, hierarchical information service for computer networks might
rely in several instances, located in different layers. A distributed directory
service, for example, might be comprised of upper level listings, and local
directories. The upper level listings contain a compact version of the local
directories. Clients desiring to access the information contained in local
directories might first access the high-level listings, in order to locate the
appropriate local instance. One of the keys for the competent operation of such
service is the ability of properly summarizing the information, which will be
maintained in the upper level directories. We analyze the case of the Lookup
Service in the Information Services plane of perfSONAR performance monitoring
distributed architecture, which implements IPv4 summarization in its functions.
We propose an empirical method, or heuristic, to achieve the summarizations,
based on the PATRICIA tree. We further apply the heuristic on a simulated
distributed test bed and contemplate the results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00198</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00198</id><created>2014-12-31</created><authors><author><keyname>Smailhodvic</keyname><forenames>Armin</forenames></author><author><keyname>Andrew</keyname><forenames>Keith</forenames></author><author><keyname>Hahn</keyname><forenames>Lance</forenames></author><author><keyname>Womble</keyname><forenames>Phillip C.</forenames></author><author><keyname>Webb</keyname><forenames>Cathleen</forenames></author></authors><title>Sample NLPDE and NLODE Social-Media Modeling of Information Transmission
  for Infectious Diseases:Case Study Ebola</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>14 pages, 4 tables, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the spreading of information through Twitter messaging related
to the spread of Ebola in western Africa using epidemic based dynamic models.
Diffusive spreading leads to NLPDE models and fixed point analysis yields
systems of NLODE models. When tweets are mapped as connected nodes in a graph
and are treated as a time sequenced Markov chain, TSMC, then by the Kurtz
theorem these specific paths can be identified as being near solutions to
systems of ordinary differential equations that in the large N limit retain
many of the features of the original Tweet dynamics. Constraints on the model
related to Tweet and re-Tweet rates lead to different versions of the system of
equations. We use Ebola Twitter meme based data to investigate a modified four
parameter model and apply the resulting fit to an accuracy metric for a set of
Ebola memes. In principle the temporal and spatial evolution equations
describing the propagation of the Twitter based memes can help ascertain and
inform decision makers on the nature of the spreading and containment of an
epidemic of this type.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00199</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00199</id><created>2014-12-31</created><authors><author><keyname>Beutel</keyname><forenames>Alex</forenames></author><author><keyname>Ahmed</keyname><forenames>Amr</forenames></author><author><keyname>Smola</keyname><forenames>Alexander J.</forenames></author></authors><title>ACCAMS: Additive Co-Clustering to Approximate Matrices Succinctly</title><categories>cs.LG stat.ML</categories><comments>22 pages, under review for conference publication</comments><acm-class>H.2.8; H.3.3; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Matrix completion and approximation are popular tools to capture a user's
preferences for recommendation and to approximate missing data. Instead of
using low-rank factorization we take a drastically different approach, based on
the simple insight that an additive model of co-clusterings allows one to
approximate matrices efficiently. This allows us to build a concise model that,
per bit of model learned, significantly beats all factorization approaches to
matrix approximation. Even more surprisingly, we find that summing over small
co-clusterings is more effective in modeling matrices than classic
co-clustering, which uses just one large partitioning of the matrix.
  Following Occam's razor principle suggests that the simple structure induced
by our model better captures the latent preferences and decision making
processes present in the real world than classic co-clustering or matrix
factorization. We provide an iterative minimization algorithm, a collapsed
Gibbs sampler, theoretical guarantees for matrix approximation, and excellent
empirical evidence for the efficacy of our approach. We achieve
state-of-the-art results on the Netflix problem with a fraction of the model
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00203</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00203</id><created>2014-12-31</created><authors><author><keyname>Liu</keyname><forenames>Feilu</forenames></author><author><keyname>Bala</keyname><forenames>Erdem</forenames></author><author><keyname>Erkip</keyname><forenames>Elza</forenames></author><author><keyname>Beluri</keyname><forenames>Mihaela C.</forenames></author><author><keyname>Yang</keyname><forenames>Rui</forenames></author></authors><title>Small Cell Traffic Balancing Over Licensed and Unlicensed Bands</title><categories>cs.NI</categories><comments>Accepted by IEEE Transaction on Vehicular Technology</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The 3rd Generation Partnership Project (3GPP) recently started standardizing
the &quot;Licensed-Assisted Access using LTE&quot; for small cells, referred to as Dual
Band Femtocell (DBF) in this paper, which uses LTE air interface in both
licensed and unlicensed bands based on the Long Term Evolution (LTE) carrier
aggregation feature. Alternatively, the Small Cell Forum introduced the
Integrated Femto-WiFi (IFW) small cell which simultaneously accesses both the
licensed band (via cellular interface) and the unlicensed band (via WiFi
interface). In this paper, a practical algorithm for IFW and DBF to
automatically balance their traffic in licensed and unlicensed bands, based on
the real-time channel, interference and traffic conditions of both bands is
described. The algorithm considers the fact that some &quot;smart&quot; devices
(sDevices) have both cellular and WiFi radios while some WiFi-only devices
(wDevices) may only have WiFi radio. In addition, the algorithm considers a
realistic scenario where a single small cell user may simultaneously use
multiple sDevices and wDevices via either the IFW, or the DBF in conjunction
with a Wireless Local Area Network (WLAN). The goal is to maximize the total
user satisfaction/utility of the small cell user, while keeping the
interference from small cell to macrocell below predefined thresholds. The
algorithm can be implemented at the Radio Link Control (RLC) or the network
layer of the IFW and DBF small cell base stations. Results demonstrate that the
proposed traffic-balancing algorithm applied to either IFW or DBF significantly
increases sum utility of all macrocell and small cell users, compared with the
current practices. Finally, various implementation issues of IFW and DBF are
addressed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00212</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00212</id><created>2014-12-31</created><authors><author><keyname>Gabow</keyname><forenames>Harold N.</forenames></author></authors><title>Set-merging for the Matching Algorithm of Micali and Vazirani</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The algorithm of Micali and Vazirani \cite{MV} finds a maximum cardinality
matching in time $O(\sqrt n m)$ if an efficient set-merging algorithm is used.
The latter is provided by the incremental-tree set-merging algorithm of
\cite{GabTar}. Details of this application to matching were omitted from
\cite{GabTar} and are presented in this note.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00216</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00216</id><created>2014-12-31</created><authors><author><keyname>Dehghan</keyname><forenames>Mostafa</forenames></author><author><keyname>Seetharam</keyname><forenames>Anand</forenames></author><author><keyname>Jiang</keyname><forenames>Bo</forenames></author><author><keyname>He</keyname><forenames>Ting</forenames></author><author><keyname>Salonidis</keyname><forenames>Theodoros</forenames></author><author><keyname>Kurose</keyname><forenames>Jim</forenames></author><author><keyname>Towsley</keyname><forenames>Don</forenames></author><author><keyname>Sitaraman</keyname><forenames>Ramesh</forenames></author></authors><title>On the Complexity of Optimal Routing and Content Caching in
  Heterogeneous Networks</title><categories>cs.NI</categories><comments>Infocom</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of optimal request routing and content caching in
a heterogeneous network supporting in-network content caching with the goal of
minimizing average content access delay. Here, content can either be accessed
directly from a back-end server (where content resides permanently) or be
obtained from one of multiple in-network caches. To access a piece of content,
a user must decide whether to route its request to a cache or to the back-end
server. Additionally, caches must decide which content to cache. We investigate
the problem complexity of two problem formulations, where the direct path to
the back-end server is modeled as i) a congestion-sensitive or ii) a
congestion-insensitive path, reflecting whether or not the delay of the
uncached path to the back-end server depends on the user request load,
respectively. We show that the problem is NP-complete in both cases. We prove
that under the congestion-insensitive model the problem can be solved optimally
in polynomial time if each piece of content is requested by only one user, or
when there are at most two caches in the network. We also identify a structural
property of the user-cache graph that potentially makes the problem
NP-complete. For the congestion-sensitive model, we prove that the problem
remains NP-complete even if there is only one cache in the network and each
content is requested by only one user. We show that approximate solutions can
be found for both models within a (1-1/e) factor of the optimal solution, and
demonstrate a greedy algorithm that is found to be within 1% of optimal for
small problem sizes. Through trace-driven simulations we evaluate the
performance of our greedy algorithms, which show up to a 50% reduction in
average delay over solutions based on LRU content caching.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00218</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00218</id><created>2014-12-31</created><updated>2015-06-17</updated><authors><author><keyname>Guo</keyname><forenames>Shenghan</forenames></author></authors><title>Comparison of Accuracy for Methods to Approximate Fisher Information in
  the Scalar Case</title><categories>cs.IT math.IT</categories><comments>26 pages, 6 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Fisher information matrix (FIM) has long been of interest in statistics
and other areas. It is widely used to measure the amount of information and
calculate the lower bound for the variance for maximum likelihood estimation
(MLE). In practice, we do not always know the actual FIM. This is often because
obtaining the first or second-order derivatives of the log-likelihood function
is difficult, or simply because the calculation of FIM is too formidable. In
such cases, we need to utilize the approximation of FIM. In general, there are
two ways to estimate FIM. One is to use the product of gradient and the
transpose of itself, and the other is to calculate the Hessian matrix and then
take negative sign. Mostly people use the latter method in practice. However,
this is not necessarily the optimal way. To find out which of the two methods
is better, we need to conduct a theoretical study to compare their efficiency.
In this paper we mainly focus on the case where the unknown parameter that
needs to be estimated by MLE is scalar and the random variables we have are
independent. In this scenario, Fisher information matrix is virtually Fisher
information number (FIN). Using the Central Limit Theorem (CLT), we get
asymptotic variances for the two methods, by which we compare their accuracy.
Taylor expansion assists in estimating the two asymptotic variances. A
numerical study is provided as an illustration of the conclusion. The next is a
summary of limitations of this paper. We also enumerate several fields of
interest for future study in the end of this paper.
  Key words: Fisher information matrix, Fisher information number, the Central
Limit Theorem, Taylor expansion, asymptotic variance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00255</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00255</id><created>2015-01-01</created><authors><author><keyname>Qin</keyname><forenames>Chengjie</forenames></author><author><keyname>Rusu</keyname><forenames>Florin</forenames></author></authors><title>Speculative Approximations for Terascale Analytics</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model calibration is a major challenge faced by the plethora of statistical
analytics packages that are increasingly used in Big Data applications.
Identifying the optimal model parameters is a time-consuming process that has
to be executed from scratch for every dataset/model combination even by
experienced data scientists. We argue that the incapacity to evaluate multiple
parameter configurations simultaneously and the lack of support to quickly
identify sub-optimal configurations are the principal causes. In this paper, we
develop two database-inspired techniques for efficient model calibration.
Speculative parameter testing applies advanced parallel multi-query processing
methods to evaluate several configurations concurrently. The number of
configurations is determined adaptively at runtime, while the configurations
themselves are extracted from a distribution that is continuously learned
following a Bayesian process. Online aggregation is applied to identify
sub-optimal configurations early in the processing by incrementally sampling
the training dataset and estimating the objective function corresponding to
each configuration. We design concurrent online aggregation estimators and
define halting conditions to accurately and timely stop the execution. We apply
the proposed techniques to distributed gradient descent optimization -- batch
and incremental -- for support vector machines and logistic regression models.
We implement the resulting solutions in GLADE PF-OLA -- a state-of-the-art Big
Data analytics system -- and evaluate their performance over terascale-size
synthetic and real datasets. The results confirm that as many as 32
configurations can be evaluated concurrently almost as fast as one, while
sub-optimal configurations are detected accurately in as little as a
$1/20^{\text{th}}$ fraction of the time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00263</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00263</id><created>2015-01-01</created><authors><author><keyname>Zhang</keyname><forenames>Yuchen</forenames></author><author><keyname>Xiao</keyname><forenames>Lin</forenames></author></authors><title>Communication-Efficient Distributed Optimization of Self-Concordant
  Empirical Loss</title><categories>math.OC cs.LG stat.ML</categories><report-no>MSR-TR-2015-1</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider distributed convex optimization problems originated from sample
average approximation of stochastic optimization, or empirical risk
minimization in machine learning. We assume that each machine in the
distributed computing system has access to a local empirical loss function,
constructed with i.i.d. data sampled from a common distribution. We propose a
communication-efficient distributed algorithm to minimize the overall empirical
loss, which is the average of the local empirical losses. The algorithm is
based on an inexact damped Newton method, where the inexact Newton steps are
computed by a distributed preconditioned conjugate gradient method. We analyze
its iteration complexity and communication efficiency for minimizing
self-concordant empirical loss functions, and discuss the results for
distributed ridge regression, logistic regression and binary classification
with a smoothed hinge loss. In a standard setting for supervised learning, the
required number of communication rounds of the algorithm does not increase with
the sample size, and only grows slowly with the number of machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00265</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00265</id><created>2015-01-01</created><authors><author><keyname>Shtrakov</keyname><forenames>Sl.</forenames></author><author><keyname>Damyanov</keyname><forenames>I.</forenames></author></authors><title>On the complexity of finite valued functions</title><categories>cs.CC</categories><comments>23 pages, 4 figures, 6 tables, Preprint of the article is submitted
  for consideration in [WSPC (2015)]
  [http://www.worldscientific.com/worldscinet/ijfcs]</comments><msc-class>03D15</msc-class><acm-class>F.1.3</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The essential variables in a finite function $f$ are defined as variables
which occur in $f$ and weigh with the values of that function.
  The number of essential variables is an important measure of complexity for
discrete functions.
  When replacing some variables in a function with constants the resulting
functions are called subfunctions, and when replacing all essential variables
in a function with constants we obtain an implementation of this function.
  Such an implementation corresponds with a path in an ordered decision diagram
(ODD) of the function which connects the root with a leaf of the diagram. The
sets of essential variables in subfunctions of $f$ are called separable in $f$.
In this paper we study several properties of separable sets of variables in
functions which directly impact on the number of implementations and
subfunctions in these functions.
  We define equivalence relations which classify the functions of $k$-valued
logic into classes with same number of implementations, subfunctions or
separable sets. These relations induce three transformation groups which are
compared with the lattice of all subgroups of restricted affine group (RAG).
This allows us to solve several important computational and combinatorial
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00267</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00267</id><created>2015-01-01</created><authors><author><keyname>Madry</keyname><forenames>Aleksander</forenames></author><author><keyname>Straszak</keyname><forenames>Damian</forenames></author><author><keyname>Tarnawski</keyname><forenames>Jakub</forenames></author></authors><title>Fast Generation of Random Spanning Trees and the Effective Resistance
  Metric</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new algorithm for generating a uniformly random spanning tree in
an undirected graph. Our algorithm samples such a tree in expected
$\tilde{O}(m^{4/3})$ time. This improves over the best previously known bound
of $\min(\tilde{O}(m\sqrt{n}),O(n^{\omega}))$ -- that follows from the work of
Kelner and M\k{a}dry [FOCS'09] and of Colbourn et al. [J. Algorithms'96] --
whenever the input graph is sufficiently sparse.
  At a high level, our result stems from carefully exploiting the interplay of
random spanning trees, random walks, and the notion of effective resistance, as
well as from devising a way to algorithmically relate these concepts to the
combinatorial structure of the graph. This involves, in particular,
establishing a new connection between the effective resistance metric and the
cut structure of the underlying graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00287</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00287</id><created>2015-01-01</created><authors><author><keyname>Ramaswamy</keyname><forenames>Harish G.</forenames></author><author><keyname>Narasimhan</keyname><forenames>Harikrishna</forenames></author><author><keyname>Agarwal</keyname><forenames>Shivani</forenames></author></authors><title>Consistent Classification Algorithms for Multi-class Non-Decomposable
  Performance Metrics</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study consistency of learning algorithms for a multi-class performance
metric that is a non-decomposable function of the confusion matrix of a
classifier and cannot be expressed as a sum of losses on individual data
points; examples of such performance metrics include the macro F-measure
popular in information retrieval and the G-mean metric used in class-imbalanced
problems. While there has been much work in recent years in understanding the
consistency properties of learning algorithms for `binary' non-decomposable
metrics, little is known either about the form of the optimal classifier for a
general multi-class non-decomposable metric, or about how these learning
algorithms generalize to the multi-class case. In this paper, we provide a
unified framework for analysing a multi-class non-decomposable performance
metric, where the problem of finding the optimal classifier for the performance
metric is viewed as an optimization problem over the space of all confusion
matrices achievable under the given distribution. Using this framework, we show
that (under a continuous distribution) the optimal classifier for a multi-class
performance metric can be obtained as the solution of a cost-sensitive
classification problem, thus generalizing several previous results on specific
binary non-decomposable metrics. We then design a consistent learning algorithm
for concave multi-class performance metrics that proceeds via a sequence of
cost-sensitive classification problems, and can be seen as applying the
conditional gradient (CG) optimization method over the space of feasible
confusion matrices. To our knowledge, this is the first efficient learning
algorithm (whose running time is polynomial in the number of classes) that is
consistent for a large family of multi-class non-decomposable metrics. Our
consistency proof uses a novel technique based on the convergence analysis of
the CG method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00288</identifier>
 <datestamp>2015-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00288</id><created>2015-01-01</created><updated>2015-07-03</updated><authors><author><keyname>Bienstock</keyname><forenames>Daniel</forenames></author><author><keyname>Munoz</keyname><forenames>Gonzalo</forenames></author></authors><title>LP approximations to mixed-integer polynomial optimization problems</title><categories>math.OC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a class of linear programming approximations for constrained
optimization problems. In the case of mixed-integer polynomial optimization
problems, if the intersection graph of the constraints has bounded tree-width
our construction yields a class of linear size formulations that attain any
desired tolerance. As a result, we obtain an approximation scheme for the
&quot;AC-OPF&quot; problem on graphs with bounded tree-width. We also describe a more
general construction for pure binary optimization problems where individual
constraints are available through a membership oracle; if the intersection
graph for the constraints has bounded tree-width our construction is of linear
size and exact. This improves on a number of results in the literature, both
from the perspective of formulation size and generality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00299</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00299</id><created>2015-01-01</created><authors><author><keyname>Pezeshki</keyname><forenames>Mohammad</forenames></author></authors><title>Sequence Modeling using Gated Recurrent Neural Networks</title><categories>cs.NE cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we have used Recurrent Neural Networks to capture and model
human motion data and generate motions by prediction of the next immediate data
point at each time-step. Our RNN is armed with recently proposed Gated
Recurrent Units which has shown promising results in some sequence modeling
problems such as Machine Translation and Speech Synthesis. We demonstrate that
this model is able to capture long-term dependencies in data and generate
realistic motions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00304</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00304</id><created>2015-01-01</created><updated>2015-05-03</updated><authors><author><keyname>Alam</keyname><forenames>Md. Jawaherul</forenames></author><author><keyname>Evans</keyname><forenames>William</forenames></author><author><keyname>Kobourov</keyname><forenames>Stephen G.</forenames></author><author><keyname>Pupyrev</keyname><forenames>Sergey</forenames></author><author><keyname>Toeniskoetter</keyname><forenames>Jackson</forenames></author><author><keyname>Ueckerdt</keyname><forenames>Torsten</forenames></author></authors><title>Contact Representations of Graphs in 3D</title><categories>cs.CG cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study contact representations of graphs in which vertices are represented
by axis-aligned polyhedra in 3D and edges are realized by non-zero area common
boundaries between corresponding polyhedra. We show that for every 3-connected
planar graph, there exists a simultaneous representation of the graph and its
dual with 3D boxes. We give a linear-time algorithm for constructing such a
representation. This result extends the existing primal-dual contact
representations of planar graphs in 2D using circles and triangles. While
contact graphs in 2D directly correspond to planar graphs, we next study
representations of non-planar graphs in 3D. In particular we consider
representations of optimal 1-planar graphs. A graph is 1-planar if there exists
a drawing in the plane where each edge is crossed at most once, and an optimal
n-vertex 1-planar graph has the maximum (4n - 8) number of edges. We describe a
linear-time algorithm for representing optimal 1-planar graphs without
separating 4-cycles with 3D boxes. However, not every optimal 1-planar graph
admits a representation with boxes. Hence, we consider contact representations
with the next simplest axis-aligned 3D object, L-shaped polyhedra. We provide a
quadratic-time algorithm for representing optimal 1-planar graph with L-shaped
polyhedra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00305</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00305</id><created>2015-01-01</created><authors><author><keyname>Farhang</keyname><forenames>Arman</forenames></author><author><keyname>Marchetti</keyname><forenames>Nicola</forenames></author><author><keyname>Figueiredo</keyname><forenames>Fabricio</forenames></author><author><keyname>Miranda</keyname><forenames>Joao Paulo</forenames></author></authors><title>Massive MIMO and Waveform Design for 5th Generation Wireless
  Communication Systems</title><categories>cs.IT math.IT</categories><comments>6 pages, 2 figures, 1st International Conference on 5G for Ubiquitous
  Connectivity</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article reviews existing related work and identifies the main challenges
in the key 5G area at the intersection of waveform design and large-scale
multiple antenna systems, also known as Massive MIMO. The property of
self-equalization is introduced for Filter Bank Multicarrier (FBMC)-based
Massive MIMO, which can reduce the number of subcarriers required by the
system. It is also shown that the blind channel tracking property of FBMC can
be used to address pilot contamination -- one of the main limiting factors of
Massive MIMO systems. Our findings shed light into and motivate for an entirely
new research line towards a better understanding of waveform design with
emphasis on FBMC-based Massive MIMO networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00311</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00311</id><created>2015-01-01</created><authors><author><keyname>Ng</keyname><forenames>Jun-Ping</forenames></author><author><keyname>Kan</keyname><forenames>Min-Yen</forenames></author></authors><title>QANUS: An Open-source Question-Answering Platform</title><categories>cs.IR cs.CL</categories><comments>6 pages, 3 figures, demo paper describing QANUS</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we motivate the need for a publicly available, generic
software framework for question-answering (QA) systems. We present an
open-source QA framework QANUS which researchers can leverage on to build new
QA systems easily and rapidly. The framework implements much of the code that
will otherwise have been repeated across different QA systems. To demonstrate
the utility and practicality of the framework, we further present a fully
functioning factoid QA system QA-SYS built on top of QANUS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00312</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00312</id><created>2015-01-01</created><authors><author><keyname>Loh</keyname><forenames>Po-Ling</forenames></author></authors><title>Statistical consistency and asymptotic normality for high-dimensional
  robust M-estimators</title><categories>math.ST cs.IT math.IT stat.ML stat.TH</categories><comments>56 pages, 8 figures</comments><msc-class>62F12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study theoretical properties of regularized robust M-estimators,
applicable when data are drawn from a sparse high-dimensional linear model and
contaminated by heavy-tailed distributions and/or outliers in the additive
errors and covariates. We first establish a form of local statistical
consistency for the penalized regression estimators under fairly mild
conditions on the error distribution: When the derivative of the loss function
is bounded and satisfies a local restricted curvature condition, all stationary
points within a constant radius of the true regression vector converge at the
minimax rate enjoyed by the Lasso with sub-Gaussian errors. When an appropriate
nonconvex regularizer is used in place of an l_1-penalty, we show that such
stationary points are in fact unique and equal to the local oracle solution
with the correct support---hence, results on asymptotic normality in the
low-dimensional case carry over immediately to the high-dimensional setting.
This has important implications for the efficiency of regularized nonconvex
M-estimators when the errors are heavy-tailed. Our analysis of the local
curvature of the loss function also has useful consequences for optimization
when the robust regression function and/or regularizer is nonconvex and the
objective function possesses stationary points outside the local region. We
show that as long as a composite gradient descent algorithm is initialized
within a constant radius of the true regression vector, successive iterates
will converge at a linear rate to a stationary point within the local region.
Furthermore, the global optimum of a convex regularized robust regression
function may be used to obtain a suitable initialization. The result is a novel
two-step procedure that uses a convex M-estimator to achieve consistency and a
nonconvex M-estimator to increase efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00316</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00316</id><created>2015-01-01</created><authors><author><keyname>Wu</keyname><forenames>Wei</forenames></author></authors><title>Phenomenological modelling for Time-Resolved Electron Paramagnetic
  Resonance in radical-triplet system</title><categories>quant-ph cs.IT math.IT</categories><comments>10 pages, 7 figures</comments><report-no>WW_RTS1_2015</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The spin dynamics of radical-triplet system (RTS) has been calculated by
using the Lindblad formalism within the theory of open quantum system. The
single-radical-triplet system (SRTS) is considered here for single-qubit
quantum gate operations while double-radical-triplet system (DRTS) for
two-qubit operations. The environment effects taken into account include the
spin-lattice relaxation of the triplet exciton and radical spin-$\frac{1}{2}$,
the inter-system crossing process that induces the transition from singlet
excited state to the triplet ground state, and the rather slow relaxation
process from the triplet ground state back down to the singlet ground state.
These calculations shown that the line shape broadening is strongly related to
the exchange interaction between triplet and exciton, which can be understood
as a spontaneous magnetic field created by the triplet renormalises the
original spin-$\frac{1}{2}$ electron spin resonance spectra. This work will
provide key information about the spin dynamics for building
optically-controlled molecular quantum gate out of radical-bearing molecules.
Moreover, this has generated the further theoretical question on how the
mixture of fermion and boson behaves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00318</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00318</id><created>2015-01-01</created><authors><author><keyname>Alam</keyname><forenames>Md. Jawaherul</forenames></author><author><keyname>Eppstein</keyname><forenames>David</forenames></author><author><keyname>Kaufmann</keyname><forenames>Michael</forenames></author><author><keyname>Kobourov</keyname><forenames>Stephen G.</forenames></author><author><keyname>Pupyrev</keyname><forenames>Sergey</forenames></author><author><keyname>Schulz</keyname><forenames>Andre</forenames></author><author><keyname>Ueckerdt</keyname><forenames>Torsten</forenames></author></authors><title>Contact Representations of Sparse Planar Graphs</title><categories>cs.CG cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study representations of graphs by contacts of circular arcs,
CCA-representations for short, where the vertices are interior-disjoint
circular arcs in the plane and each edge is realized by an endpoint of one arc
touching the interior of another. A graph is (2,k)-sparse if every s-vertex
subgraph has at most 2s - k edges, and (2, k)-tight if in addition it has
exactly 2n - k edges, where n is the number of vertices. Every graph with a
CCA- representation is planar and (2, 0)-sparse, and it follows from known
results on contacts of line segments that for k &gt;= 3 every (2, k)-sparse graph
has a CCA-representation. Hence the question of CCA-representability is open
for (2, k)-sparse graphs with 0 &lt;= k &lt;= 2. We partially answer this question by
computing CCA-representations for several subclasses of planar (2,0)-sparse
graphs. In particular, we show that every plane (2, 2)-sparse graph has a
CCA-representation, and that any plane (2, 1)-tight graph or (2, 0)-tight graph
dual to a (2, 3)-tight graph or (2, 4)-tight graph has a CCA-representation.
Next, we study CCA-representations in which each arc has an empty convex hull.
We characterize the plane graphs that have such a representation, based on the
existence of a special orientation of the graph edges. Using this
characterization, we show that every plane graph of maximum degree 4 has such a
representation, but that finding such a representation for a plane (2, 0)-tight
graph with maximum degree 5 is an NP-complete problem. Finally, we describe a
simple algorithm for representing plane (2, 0)-sparse graphs with wedges, where
each vertex is represented with a sequence of two circular arcs (straight-line
segments).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00320</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00320</id><created>2015-01-01</created><authors><author><keyname>Pawar</keyname><forenames>Sameer</forenames></author><author><keyname>Ramchandran</keyname><forenames>Kannan</forenames></author></authors><title>A robust sub-linear time R-FFAST algorithm for computing a sparse DFT</title><categories>cs.IT cs.LG math.IT</categories><comments>35 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Fast Fourier Transform (FFT) is the most efficiently known way to compute
the Discrete Fourier Transform (DFT) of an arbitrary n-length signal, and has a
computational complexity of O(n log n). If the DFT X of the signal x has only k
non-zero coefficients (where k &lt; n), can we do better? In [1], we addressed
this question and presented a novel FFAST (Fast Fourier Aliasing-based Sparse
Transform) algorithm that cleverly induces sparse graph alias codes in the DFT
domain, via a Chinese-Remainder-Theorem (CRT)-guided sub-sampling operation of
the time-domain samples. The resulting sparse graph alias codes are then
exploited to devise a fast and iterative onion-peeling style decoder that
computes an n length DFT of a signal using only O(k) time-domain samples and
O(klog k) computations. The FFAST algorithm is applicable whenever k is
sub-linear in n (i.e. k = o(n)), but is obviously most attractive when k is
much smaller than n.
  In this paper, we adapt the FFAST framework of [1] to the case where the
time-domain samples are corrupted by a white Gaussian noise. In particular, we
show that the extended noise robust algorithm R-FFAST computes an n-length
k-sparse DFT X using O(klog ^3 n) noise-corrupted time-domain samples, in
O(klog^4n) computations, i.e., sub-linear time complexity. While our
theoretical results are for signals with a uniformly random support of the
non-zero DFT coefficients and additive white Gaussian noise, we provide
simulation results which demonstrates that the R-FFAST algorithm performs well
even for signals like MR images, that have an approximately sparse Fourier
spectrum with a non-uniform support for the dominant DFT coefficients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00324</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00324</id><created>2015-01-01</created><authors><author><keyname>Wong</keyname><forenames>Jonathan</forenames></author><author><keyname>Kuhl</keyname><forenames>Ellen</forenames></author><author><keyname>Darve</keyname><forenames>Eric</forenames></author></authors><title>A New Sparse Matrix Vector Multiplication GPU Algorithm Designed for
  Finite Element Problems</title><categories>cs.MS cs.CE</categories><comments>35 pages, 22 figures</comments><msc-class>65Y10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, graphics processors (GPUs) have been increasingly leveraged in a
variety of scientific computing applications. However, architectural
differences between CPUs and GPUs necessitate the development of algorithms
that take advantage of GPU hardware. As sparse matrix vector multiplication
(SPMV) operations are commonly used in finite element analysis, a new SPMV
algorithm and several variations are developed for unstructured finite element
meshes on GPUs. The effective bandwidth of current GPU algorithms and the newly
proposed algorithms are measured and analyzed for 15 sparse matrices of varying
sizes and varying sparsity structures. The effects of optimization and
differences between the new GPU algorithm and its variants are then
subsequently studied. Lastly, both new and current SPMV GPU algorithms are
utilized in the GPU CG Solver in GPU finite element simulations of the heart.
These results are then compared against parallel PETSc finite element
implementation results. The effective bandwidth tests indicate that the new
algorithms compare very favorably with current algorithms for a wide variety of
sparse matrices and can yield very notable benefits. GPU finite element
simulation results demonstrate the benefit of using GPUs for finite element
analysis, and also show that the proposed algorithms can yield speedup factors
up to 12-fold for real finite element applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00329</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00329</id><created>2015-01-01</created><authors><author><keyname>Blasco</keyname><forenames>Pol</forenames></author><author><keyname>Gunduz</keyname><forenames>Deniz</forenames></author></authors><title>Multi-Access Communications with Energy Harvesting: A Multi-Armed Bandit
  Model and the Optimality of the Myopic Policy</title><categories>cs.IT cs.LG math.IT</categories><comments>accepted for publication in IEEE Journal of Selected Areas in
  Communications (IEEE JSAC) Special Issue on Wireless Communications Powered
  by Energy Harvesting and Wireless Energy Transfer</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A multi-access wireless network with N transmitting nodes, each equipped with
an energy harvesting (EH) device and a rechargeable battery of finite capacity,
is studied. At each time slot (TS) a node is operative with a certain
probability, which may depend on the availability of data, or the state of its
channel. The energy arrival process at each node is modelled as an independent
two-state Markov process, such that, at each TS, a node either harvests one
unit of energy, or none. At each TS a subset of the nodes is scheduled by the
access point (AP). The scheduling policy that maximises the total throughput is
studied assuming that the AP does not know the states of either the EH
processes or the batteries. The problem is identified as a restless multiarmed
bandit (RMAB) problem, and an upper bound on the optimal scheduling policy is
found. Under certain assumptions regarding the EH processes and the battery
sizes, the optimality of the myopic policy (MP) is proven. For the general
case, the performance of MP is compared numerically to the upper bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00334</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00334</id><created>2015-01-01</created><updated>2015-05-05</updated><authors><author><keyname>Rodriguez</keyname><forenames>Jose Israel</forenames></author><author><keyname>Tang</keyname><forenames>Xiaoxian</forenames></author></authors><title>Data-Discriminants of Likelihood Equations</title><categories>cs.SC</categories><comments>2 tables</comments><acm-class>G.3; I.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Maximum likelihood estimation (MLE) is a fundamental computational problem in
statistics. The problem is to maximize the likelihood function with respect to
given data on a statistical model. An algebraic approach to this problem is to
solve a very structured parameterized polynomial system called likelihood
equations. For general choices of data, the number of complex solutions to the
likelihood equations is finite and called the ML-degree of the model. The only
solutions to the likelihood equations that are statistically meaningful are the
real/positive solutions. However, the number of real/positive solutions is not
characterized by the ML-degree. We use discriminants to classify data according
to the number of real/positive solutions of the likelihood equations. We call
these discriminants data-discriminants (DD). We develop a probabilistic
algorithm for computing DDs. Experimental results show that, for the benchmarks
we have tried, the probabilistic algorithm is more efficient than the standard
elimination algorithm. Based on the computational results, we discuss the real
root classification problem for the 3 by 3 symmetric matrix~model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00335</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00335</id><created>2015-01-01</created><authors><author><keyname>Isley</keyname><forenames>Steven C.</forenames></author></authors><title>A Data Transparency Framework for Mobile Applications</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In today's mobile application marketplace, the ability of consumers to make
informed choices regarding their privacy is extremely limited. Consumers
largely rely on privacy policies and app permission mechanisms, but these do an
inadequate job of conveying how information will be collected, used, stored,
and shared. Mobile application developers go largely unrewarded for making apps
more privacy conscious as it is difficult to communicate these features to
consumers while they are searching for a new app. This paper provides an
overview of a framework designed to help consumers make informed choices, and
an incentive mechanism to encourage app developers to implement it. This
framework includes machine readable privacy policies encouraged by mobile app
stores and enhanced by user software agents. Such a framework would provide the
foundation required for more advanced forms of privacy management to develop.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00340</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00340</id><created>2015-01-01</created><updated>2015-04-17</updated><authors><author><keyname>Inkulu</keyname><forenames>Rajasekhar</forenames></author><author><keyname>Kapoor</keyname><forenames>Sanjiv</forenames></author></authors><title>A polynomial time algorithm for finding an approximate shortest path
  amid weighted regions</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We devise a polynomial-time approximation scheme for the classical geometric
problem of finding an approximate short path amid weighted regions. In this
problem, a triangulated region P comprising of n vertices, a positive weight
associated with each triangle, and two points s and t that belong to P are
given as the input. The objective is to find a path whose cost is at most
(1+\epsilon)OPT where OPT is the cost of an optimal path between s and t. Our
algorithm initiates a discretized-Dijkstra wavefront from source s and
progresses the wavefront till it strikes t. This result is about a cubic factor
(in $n$) improvement over the Mitchell and Papadimitriou '91 result, which is
the only known polynomial time algorithm for this problem to date.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00343</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00343</id><created>2015-01-01</created><updated>2016-03-05</updated><authors><author><keyname>Mishra</keyname><forenames>Tapas Kumar</forenames></author><author><keyname>Pal</keyname><forenames>Sudebkumar Prasant</forenames></author></authors><title>Bicoloring covers for graphs and hypergraphs</title><categories>cs.DM math.CO</categories><comments>20 pages, 3 figures</comments><msc-class>05C15, 05C65</msc-class><acm-class>G.2.2; F.2.2</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Let the {\it bicoloring cover number $\chi^c(G)$} for a hypergraph $G(V,E)$
be the minimum number of bicolorings of vertices of $G$ such that every
hyperedge $e\in E$ of $G$ is properly bicolored in at least one of the
$\chi^c(G)$ bicolorings. We investigate the relationship between $\chi^c(G)$,
matchings, hitting sets, $\alpha(G)$(independence number) and $\chi(G)$
(chromatic number). We design a factor $O(\frac{\log n}{\log \log n-\log \log
\log n})$ approximation algorithm for computing a bicoloring cover. We define a
new parameter for hypergraphs - &quot;cover independence number $\gamma(G)$&quot; and
prove that $\log \frac{|V|}{\gamma(G)}$ and $\frac{|V|}{2\gamma(G)}$ are lower
bounds for $\chi^c(G)$ and $\chi(G)$, respectively. We show that $\chi^c(G)$
can be approximated by a polynomial time algorithm achieving approximation
ratio $\frac{1}{1-t}$, if $\gamma(G)=n^t$, where $t&lt;1$. We also construct a
particular class of hypergraphs $G(V,E)$ called {\it cover friendly}
hypergraphs where the ratio of $\alpha(G)$ to $\gamma(G)$ can be arbitrarily
large.We prove that for any $t\geq 1$, there exists a $k$-uniform hypergraph
$G$ such that the {\it clique number} $\omega(G)=k$ and $\chi^c(G) &gt; t$. Let
$m(k,x)$ denote the minimum number of hyperedges %in a $k$-uniform hypergraph
$G$ such that some $k$-uniform hypergraph $G$ with $m(k,x)$ hyperedges does not
have a bicoloring cover of size $x$. We show that $ 2^{(k-1)x-1} &lt; m(k,x) \leq
x \cdot k^2 \cdot 2^{(k+1)x+2}$. Let the {\it dependency $d(G)$} of $G$ be the
maximum number of hyperedge neighbors of any hyperedge in $G$. We propose an
algorithm for computing a bicoloring cover of size $x$ for $G$ if $d(G)
\leq(\frac{2^{x(k-1)}}{e}-1)$ using $nx+kx\frac{m}{d}$ random bits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00349</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00349</id><created>2015-01-02</created><authors><author><keyname>Altowayan</keyname><forenames>A. Aziz</forenames></author></authors><title>Static Analysis for Biological Systems (BioAmbients)</title><categories>cs.CE cs.PL</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, I present a summary on some works that utilized static
analysis techniques for understanding biological systems. Control flow
analysis, context dependent analysis, and other techniques were employed to
investigate the properties of BioAmbients. In this summary report, I tried to
introduce the ideas and explain the techniques used in the subject papers. This
summary will highlight the biological concepts of BioAmbients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00354</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00354</id><created>2015-01-02</created><authors><author><keyname>Kim</keyname><forenames>Sang-Pil</forenames></author><author><keyname>Gil</keyname><forenames>Myeong-Sun</forenames></author><author><keyname>Moon</keyname><forenames>Yang-Sae</forenames></author><author><keyname>Won</keyname><forenames>Hee-Sun</forenames></author></authors><title>Efficient 2-Step Protocol and Its Discriminative Feature Selections in
  Secure Similar Document Detection</title><categories>cs.CR cs.DB</categories><comments>25 pages, 12 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Secure similar document detection (SSDD) identifies similar documents of two
parties while each party does not disclose its own sensitive documents to
another party. In this paper, we propose an efficient 2-step protocol that
exploits a feature selection as the lower-dimensional transformation and
presents discriminative feature selections to maximize the performance of the
protocol. For this, we first analyze that the existing 1-step protocol causes
serious computation and communication overhead for high dimensional document
vectors. To alleviate the overhead, we next present the feature selection-based
2-step protocol and formally prove its correctness. The proposed 2-step
protocol works as follows: (1) in the filtering step, it uses low dimensional
vectors obtained by the feature selection to filter out non-similar documents;
(2) in the post-processing step, it identifies similar documents only from the
non-filtered documents by using the 1-step protocol. As the feature selection,
we first consider the simplest one, random projection (RP), and propose its
2-step solution SSDD-RP. We then present two discriminative feature selections
and their solutions: SSDD-LF (local frequency) which selects a few dimensions
locally frequent in the current querying vector and SSDD-GF (global frequency)
which selects ones globally frequent in the set of all document vectors. We
finally propose a hybrid one, SSDD-HF (hybrid frequency), that takes advantage
of both SSDD-LF and SSDD-GF. We empirically show that the proposed 2-step
protocol outperforms the 1-step protocol by three or four orders of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00358</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00358</id><created>2015-01-02</created><authors><author><keyname>Yang</keyname><forenames>Cheng</forenames></author><author><keyname>Liu</keyname><forenames>Zhiyuan</forenames></author></authors><title>Comprehend DeepWalk as Matrix Factorization</title><categories>cs.LG</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Word2vec, as an efficient tool for learning vector representation of words
has shown its effectiveness in many natural language processing tasks. Mikolov
et al. issued Skip-Gram and Negative Sampling model for developing this
toolbox. Perozzi et al. introduced the Skip-Gram model into the study of social
network for the first time, and designed an algorithm named DeepWalk for
learning node embedding on a graph. We prove that the DeepWalk algorithm is
actually factoring a matrix M where each entry M_{ij} is logarithm of the
average probability that node i randomly walks to node j in fix steps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00375</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00375</id><created>2015-01-02</created><authors><author><keyname>Jitkrittum</keyname><forenames>Wittawat</forenames></author><author><keyname>Gretton</keyname><forenames>Arthur</forenames></author><author><keyname>Heess</keyname><forenames>Nicolas</forenames></author></authors><title>Passing Expectation Propagation Messages with Kernel Methods</title><categories>stat.ML cs.LG</categories><comments>Accepted to Advances in Variational Inference, NIPS 2014 Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose to learn a kernel-based message operator which takes as input all
expectation propagation (EP) incoming messages to a factor node and produces an
outgoing message. In ordinary EP, computing an outgoing message involves
estimating a multivariate integral which may not have an analytic expression.
Learning such an operator allows one to bypass the expensive computation of the
integral during inference by directly mapping all incoming messages into an
outgoing message. The operator can be learned from training data (examples of
input and output messages) which allows automated inference to be made on any
kind of factor that can be sampled.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00379</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00379</id><created>2015-01-02</created><updated>2015-04-11</updated><authors><author><keyname>Raz</keyname><forenames>Orit E.</forenames></author><author><keyname>Sharir</keyname><forenames>Micha</forenames></author></authors><title>The number of unit-area triangles in the plane: Theme and variations</title><categories>math.CO cs.CG cs.DM math.MG</categories><msc-class>52C10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the number of unit-area triangles determined by a set $S$ of $n$
points in the plane is $O(n^{20/9})$, improving the earlier bound $O(n^{9/4})$
of Apfelbaum and Sharir [Discrete Comput. Geom., 2010]. We also consider two
special cases of this problem: (i) We show, using a somewhat subtle
construction, that if $S$ consists of points on three lines, the number of
unit-area triangles that $S$ spans can be $\Omega(n^2)$, for any triple of
lines (it is always $O(n^2)$ in this case). (ii) We show that if $S$ is a {\em
convex grid} of the form $A\times B$, where $A$, $B$ are {\em convex} sets of
$n^{1/2}$ real numbers each (i.e., the sequences of differences of consecutive
elements of $A$ and of $B$ are both strictly increasing), then $S$ determines
$O(n^{31/14})$ unit-area triangles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00381</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00381</id><created>2015-01-02</created><updated>2015-11-30</updated><authors><author><keyname>Iyer</keyname><forenames>Srikanth K.</forenames></author><author><keyname>Vaze</keyname><forenames>Rahul</forenames></author></authors><title>Achieving Non-Zero Information Velocity in Wireless Networks</title><categories>cs.IT math.IT math.PR</categories><comments>to appear in Annals of Applied Probability</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In wireless networks, where each node transmits independently of other nodes
in the network (the ALOHA protocol), the expected delay experienced by a packet
until it is successfully received at any other node is known to be infinite for
signal-to-interference-plus-noise-ratio (SINR) model with node locations
distributed according to a Poisson point process. Consequently, the information
velocity, defined as the limit of the ratio of the distance to the destination
and the time taken for a packet to successfully reach the destination over
multiple hops, is zero, as the distance tends to infinity. A nearest neighbor
distance based power control policy is proposed to show that the expected delay
required for a packet to be successfully received at the nearest neighbor can
be made finite. Moreover, the information velocity is also shown to be non-zero
with the proposed power control policy. The condition under which these results
hold does not depend on the intensity of the underlying Poisson point process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00386</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00386</id><created>2015-01-02</created><authors><author><keyname>Pauly</keyname><forenames>Arno</forenames></author></authors><title>Computability on the countable ordinals and the Hausdorff-Kuratowski
  theorem</title><categories>cs.LO math.GN</categories><msc-class>03E15, 54H05, 03D60, 03F15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, we explore various potential representations of the set of
countable ordinals. An equivalence class of representations is then suggested
as a standard, as it offers the desired closure properties. With a decent
notion of computability on the space of countable ordinals in place, we can
then state and prove a computable uniform version of the Hausdorff-Kuratowski
theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00387</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00387</id><created>2015-01-02</created><authors><author><keyname>Schlotter</keyname><forenames>Ildik&#xf3;</forenames></author><author><keyname>Faliszewski</keyname><forenames>Piotr</forenames></author><author><keyname>Elkind</keyname><forenames>Edith</forenames></author></authors><title>Campaign Management under Approval-Driven Voting Rules</title><categories>cs.GT</categories><comments>34 pages, 1 figure</comments><msc-class>68Q17</msc-class><acm-class>I.2.11; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Approval-like voting rules, such as Sincere-Strategy Preference-Based
Approval voting (SP-AV), the Bucklin rule (an adaptive variant of $k$-Approval
voting), and the Fallback rule (an adaptive variant of SP-AV) have many
desirable properties: for example, they are easy to understand and encourage
the candidates to choose electoral platforms that have a broad appeal. In this
paper, we investigate both classic and parameterized computational complexity
of electoral campaign management under such rules. We focus on two methods that
can be used to promote a given candidate: asking voters to move this candidate
upwards in their preference order or asking them to change the number of
candidates they approve of. We show that finding an optimal campaign management
strategy of the first type is easy for both Bucklin and Fallback. In contrast,
the second method is computationally hard even if the degree to which we need
to affect the votes is small. Nevertheless, we identify a large class of
scenarios that admit fixed-parameter tractable algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00405</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00405</id><created>2015-01-02</created><authors><author><keyname>Agarwal</keyname><forenames>Puneet</forenames></author><author><keyname>Shroff</keyname><forenames>Gautam</forenames></author><author><keyname>Saikia</keyname><forenames>Sarmimala</forenames></author><author><keyname>Khan</keyname><forenames>Zaigham</forenames></author></authors><title>Efficiently Discovering Frequent Motifs in Large-scale Sensor Data</title><categories>cs.DB cs.LG</categories><comments>13 pages, 8 figures, Technical Report</comments><report-no>TR-DAIF-2015-1</report-no><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  While analyzing vehicular sensor data, we found that frequently occurring
waveforms could serve as features for further analysis, such as rule mining,
classification, and anomaly detection. The discovery of waveform patterns, also
known as time-series motifs, has been studied extensively; however, available
techniques for discovering frequently occurring time-series motifs were found
lacking in either efficiency or quality: Standard subsequence clustering
results in poor quality, to the extent that it has even been termed
'meaningless'. Variants of hierarchical clustering using techniques for
efficient discovery of 'exact pair motifs' find high-quality frequent motifs,
but at the cost of high computational complexity, making such techniques
unusable for our voluminous vehicular sensor data. We show that good quality
frequent motifs can be discovered using bounded spherical clustering of
time-series subsequences, which we refer to as COIN clustering, with near
linear complexity in time-series size. COIN clustering addresses many of the
challenges that previously led to subsequence clustering being viewed as
meaningless. We describe an end-to-end motif-discovery procedure using a
sequence of pre and post-processing techniques that remove trivial-matches and
shifted-motifs, which also plagued previous subsequence-clustering approaches.
We demonstrate that our technique efficiently discovers frequent motifs in
voluminous vehicular sensor data as well as in publicly available data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00406</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00406</id><created>2015-01-02</created><authors><author><keyname>Yajnanarayana</keyname><forenames>Vijaya</forenames></author><author><keyname>Dwivedi</keyname><forenames>Satyam</forenames></author><author><keyname>H&#xe4;ndel</keyname><forenames>Peter</forenames></author></authors><title>Multi Detector Fusion of Dynamic TOA Estimation using Kalman Filter</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose fusion of dynamic TOA (time of arrival) from
multiple non-coherent detectors like energy detectors operating at sub-Nyquist
rate through Kalman filtering. We also show that by using multiple of these
energy detectors, we can achieve the performance of a digital matched filter
implementation in the AWGN (additive white Gaussian noise) setting. We derive
analytical expression for number of energy detectors needed to achieve the
matched filter performance. We demonstrate in simulation the validity of our
analytical approach. Results indicate that number of energy detectors needed
will be high at low SNRs and converge to a constant number as the SNR
increases. We also study the performance of the strategy proposed using IEEE
802.15.4a CM1 channel model and show in simulation that two sub-Nyquist
detectors are sufficient to match the performance of digital matched filter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00409</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00409</id><created>2015-01-02</created><updated>2016-03-01</updated><authors><author><keyname>Yajnanarayana</keyname><forenames>Vijaya</forenames></author><author><keyname>Dwivedi</keyname><forenames>Satyam</forenames></author><author><keyname>H&#xe4;ndel</keyname><forenames>Peter</forenames></author></authors><title>IR-UWB Detection and Fusion Strategies using Multiple Detector Types</title><categories>cs.IT math.IT</categories><comments>Accepted for publishing in IEEE WCNC 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimal detection of ultra wideband (UWB) pulses in a UWB transceiver
employing multiple detector types is proposed and analyzed in this paper. We
propose several fusion techniques for fusing decisions made by individual
IR-UWB detectors. We assess the performance of these fusion techniques for
commonly used detector types like matched filter, energy detector and amplitude
detector. In order to perform this, we derive the detection performance
equation for each of the detectors in terms of false alarm rate, shape of the
pulse and number of UWB pulses used in the detection and apply these in the
fusion algorithms. We show that the performance can be improved approximately
by 4 dB in terms of signal to noise ratio (SNR) for perfect detectability of a
UWB signal in a practical scenario by fusing the decisions from individual
detectors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00419</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00419</id><created>2015-01-02</created><authors><author><keyname>Rook</keyname><forenames>Christopher J.</forenames></author></authors><title>Minimizing the Probability of Ruin in Retirement</title><categories>q-fin.GN cs.CE math.OC</categories><comments>Proofs appendix with full C++ implementation is included</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Retirees who exhaust their savings while still alive are said to experience
financial ruin. These savings are typically grown during the accumulation phase
then spent during the retirement decumulation phase. Extensive research into
invest-and-harvest decumulation strategies has been conducted, but
recommendations differ markedly. This has likely been a source of concern and
confusion for the retiree. Our goal is to find what has heretofore been
elusive, namely an optimal decumulation strategy. Optimality implies that no
alternate strategy exists or can be constructed that delivers a lower
probability of ruin, given a fixed inflation-adjusted withdrawal rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00432</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00432</id><created>2015-01-02</created><updated>2015-03-31</updated><authors><author><keyname>Li</keyname><forenames>Zhenping</forenames></author><author><keyname>Wang</keyname><forenames>Rui-Sheng</forenames></author><author><keyname>Zhang</keyname><forenames>Shihua</forenames></author><author><keyname>Zhang</keyname><forenames>Xiang-Sun</forenames></author></authors><title>Quantitative Function and Algorithm for Community Detection in Bipartite
  Networks</title><categories>cs.SI physics.soc-ph</categories><comments>18 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community detection in complex networks is a topic of high interest in many
fields. Bipartite networks are a special type of complex networks in which
nodes are decomposed into two disjoint sets, and only nodes between the two
sets can be connected. Bipartite networks represent diverse interaction
patterns in many real-world systems, such as predator-prey networks,
plant-pollinator networks, and drug-target networks. While community detection
in unipartite networks has been extensively studied in the past decade,
identification of modules or communities in bipartite networks is still in its
early stage. Several quantitative functions proposed for evaluating the quality
of bipartite network divisions are based on null models and have distinct
resolution limits. In this paper, we propose a new quantitative function for
community detection in bipartite networks, and demonstrate that this
quantitative function is superior to the widely used Barber's bipartite
modularity and other functions. Based on the new quantitative function, the
bipartite network community detection problem is formulated into an integer
programming model. Bipartite networks can be partitioned into reasonable
overlapping communities by maximizing the quantitative function. We further
develop a heuristic and adapted label propagation algorithm (BiLPA) to optimize
the quantitative function in large-scale bipartite networks. BiLPA does not
require any prior knowledge about the number of communities in the networks. We
apply BiLPA to both artificial networks and real-world networks and demonstrate
that this method can successfully identify the community structures of
bipartite networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00433</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00433</id><created>2015-01-02</created><updated>2015-12-21</updated><authors><author><keyname>Brattka</keyname><forenames>Vasco</forenames></author><author><keyname>Hendtlass</keyname><forenames>Matthew</forenames></author><author><keyname>Kreuzer</keyname><forenames>Alexander P.</forenames></author></authors><title>On the Uniform Computational Content of Computability Theory</title><categories>math.LO cs.LO</categories><comments>41 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We demonstrate that the Weihrauch lattice can be used to study the uniform
computational content of computability theoretic properties and theorems in one
common setting. The properties that we study include diagonal
non-computability, hyperimmunity, complete extensions of Peano arithmetic,
1-genericity, Martin-L\&quot;of randomness and cohesiveness. The theorems that we
include in our case study are the Low Basis Theorem of Jockusch and Soare, the
Kleene-Post Theorem and Friedman's Jump Inversion Theorem. It turns out that
all the aforementioned properties and many theorems in computability theory,
including all theorems that claim the existence of some Turing degree, have
very little uniform computational content. They are all located outside of the
upper cone of binary choice (also known as LLPO) and we call problems with this
property indiscriminative. Since practically all theorems from classical
analysis whose computational content has been classified are discriminative,
our observation could yield an explanation for why theorems and results in
computability theory typically have very little direct consequences in other
disciplines such as analysis. A notable exception in our case study is the Low
Basis Theorem which is discriminative, this is perhaps why it is considered to
be one of the most applicable theorems in computability theory. In some cases a
bridge between the indiscriminative world and the discriminative world of
classical mathematics can be established via a suitable residual operation and
we demonstrate this in case of the cohesiveness problem, which turns out to be
the quotient of two discriminative problems, namely the limit operation and the
jump of Weak K\H{o}nig's Lemma.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00436</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00436</id><created>2015-01-02</created><authors><author><keyname>Basterrech</keyname><forenames>Sebasti&#xe1;n</forenames></author><author><keyname>Alba</keyname><forenames>Enrique</forenames></author><author><keyname>Sn&#xe1;&#x161;el</keyname><forenames>V&#xe1;clav</forenames></author></authors><title>An Experimental Analysis of the Echo State Network Initialization Using
  the Particle Swarm Optimization</title><categories>cs.NE</categories><comments>IEEE Sixth World Congress on Nature and Biologically Inspired
  Computing (NaBIC2014), Porto, Portugal, July 30-August 1, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article introduces a robust hybrid method for solving supervised
learning tasks, which uses the Echo State Network (ESN) model and the Particle
Swarm Optimization (PSO) algorithm. An ESN is a Recurrent Neural Network with
the hidden-hidden weights fixed in the learning process. The recurrent part of
the network stores the input information in internal states of the network.
Another structure forms a free-memory method used as supervised learning tool.
The setting procedure for initializing the recurrent structure of the ESN model
can impact on the model performance. On the other hand, the PSO has been shown
to be a successful technique for finding optimal points in complex spaces.
Here, we present an approach to use the PSO for finding some initial
hidden-hidden weights of the ESN model. We present empirical results that
compare the canonical ESN model with this hybrid method on a wide range of
benchmark problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00437</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00437</id><created>2015-01-02</created><authors><author><keyname>Ben-David</keyname><forenames>Shai</forenames></author></authors><title>Computational Feasibility of Clustering under Clusterability Assumptions</title><categories>cs.CC cs.LG</categories><msc-class>68Q25, 68Q32</msc-class><acm-class>F.1.3; F.2.2; H.3.3; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that most of the common clustering objectives are NP-hard to
optimize. In practice, however, clustering is being routinely carried out. One
approach for providing theoretical understanding of this seeming discrepancy is
to come up with notions of clusterability that distinguish realistically
interesting input data from worst-case data sets. The hope is that there will
be clustering algorithms that are provably efficient on such 'clusterable'
instances. In other words, hope that &quot;Clustering is difficult only when it does
not matter&quot; (CDNM thesis, for short).
  We believe that to some extent this may indeed be the case. This paper
provides a survey of recent papers along this line of research and a critical
evaluation their results. Our bottom line conclusion is that that CDNM thesis
is still far from being formally substantiated. We start by discussing which
requirements should be met in order to provide formal support the validity of
the CDNM thesis. In particular, we list some implied requirements for notions
of clusterability. We then examine existing results in view of those
requirements and outline some research challenges and open questions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00440</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00440</id><created>2015-01-02</created><authors><author><keyname>Beica</keyname><forenames>Andreea</forenames></author><author><keyname>Guet</keyname><forenames>Calin</forenames></author><author><keyname>Petrov</keyname><forenames>Tatjana</forenames></author></authors><title>Efficient reduction of Kappa models by static inspection of the rule-set</title><categories>cs.CE cs.LO cs.PL q-bio.MN</categories><comments>11 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When designing genetic circuits, the typical primitives used in major
existing modelling formalisms are gene interaction graphs, where edges between
genes denote either an activation or inhibition relation. However, when
designing experiments, it is important to be precise about the low-level
mechanistic details as to how each such relation is implemented. The rule-based
modelling language Kappa allows to unambiguously specify mechanistic details
such as DNA binding sites, dimerisation of transcription factors, or
co-operative interactions. However, such a detailed description comes with
complexity and computationally costly execution. We propose a general method
for automatically transforming a rule-based program, by eliminating
intermediate species and adjusting the rate constants accordingly. Our method
consists of searching for those interaction patterns known to be amenable to
equilibrium approximations (e.g. Michaelis-Menten scheme). The reduced model is
efficiently obtained by static inspection over the rule-set, and it represents
a particular theoretical limit of the original model. The Bhattacharyya
distance is proposed as a metric to estimate the reduction error for a given
observable. The tool is tested on a detailed rule-based model of a
$\lambda$-phage switch, which lists $96$ rules and $16$ agents. The reduced
model has $11$ rules and $5$ agents, and provides a dramatic reduction in
simulation time of several orders of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00447</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00447</id><created>2015-01-02</created><authors><author><keyname>Verb&#xfc;cheln</keyname><forenames>Stephan</forenames></author></authors><title>How Perfect Offline Wallets Can Still Leak Bitcoin Private Keys</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  ECDSA has become a popular choice as lightweight alternative to RSA and
classic DL based signature algorithms in recent years. As standardized, the
signature produced by ECDSA for a pair of a message and a key is not
deterministic. This work shows how this non-deterministic choice can be
exploited by an attacker to leak private information through the signature
without any side channels, an attack first discovered by Young and Yung for
classic DL-based cryptosystems in 1997, and how this attack affects the
application of ECDSA in the Bitcoin protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00491</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00491</id><created>2015-01-02</created><authors><author><keyname>D'Alberto</keyname><forenames>Paolo</forenames></author><author><keyname>Milenkly</keyname><forenames>Veronica</forenames></author></authors><title>Mapping and Matching Algorithms: Data Mining by Adaptive Graphs</title><categories>cs.OH</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Assume we have two bijective functions $U(x)$ and $M(x)$ with $M(x)\neq U(x)$
for all $x$ and $M,N: \N \rightarrow \N$ . Every day and in different
locations, we see the different results of $U$ and $M$ without seeing $x$. We
are not assured about the time stamp nor the order within the day but at least
the location is fully defined. We want to find the matching between $U(x)$ and
$M(x)$ (i.e., we will not know $x$). We formulate this problem as an adaptive
graph mining: we develop the theory, the solution, and the implementation. This
work stems from a practical problem thus our definitions. The solution is
simple, clear, and the implementation parallel and efficient. In our
experience, the problem and the solution are novel and we want to share our
finding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00503</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00503</id><created>2015-01-02</created><authors><author><keyname>Basterrech</keyname><forenames>Sebasti&#xe1;n</forenames></author></authors><title>An Empirical Study of the L2-Boost technique with Echo State Networks</title><categories>cs.LG cs.NE</categories><comments>To appear in Journal of Network and Innovative Computing, Volume 2,
  Issue 1, pp. 120 - 127, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A particular case of Recurrent Neural Network (RNN) was introduced at the
beginning of the 2000s under the name of Echo State Networks (ESNs). The ESN
model overcomes the limitations during the training of the RNNs while
introducing no significant disadvantages. Although the model presents some
well-identified drawbacks when the parameters are not well initialised. The
performance of an ESN is highly dependent on its internal parameters and
pattern of connectivity of the hidden-hidden weights Often, the tuning of the
network parameters can be hard and can impact in the accuracy of the models.
  In this work, we investigate the performance of a specific boosting technique
(called L2-Boost) with ESNs as single predictors. The L2-Boost technique has
been shown to be an effective tool to combine &quot;weak&quot; predictors in regression
problems. In this study, we use an ensemble of random initialized ESNs (without
control their parameters) as &quot;weak&quot; predictors of the boosting procedure. We
evaluate our approach on five well-know time-series benchmark problems.
Additionally, we compare this technique with a baseline approach that consists
of averaging the prediction of an ensemble of ESNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00505</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00505</id><created>2015-01-02</created><authors><author><keyname>Mironchyk</keyname><forenames>P.</forenames></author></authors><title>Adaptive Control of 4-DoF Robot manipulator</title><categories>cs.SY cs.RO</categories><comments>7 pages, 4(5) figures</comments><msc-class>93C40</msc-class><acm-class>J.7; I.2.9</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In experimental robotics, researchers may face uncertainties in parameters of
a robot manipulator that they are working with. This uncertainty may be caused
by deviations in the manufacturing process of a manipulator, or changes applied
to manipulator in the lab for sake of experiments. Another situation when
dynamical and inertial parameters of a robot are uncertain arises, is the
grasping of objects by a manipulator. In all these situations there is a need
for adaptive control strategies that would identify changes in dynamical
properties of manipulator and adjust for them. This article presents a work on
designing of an adaptive control strategy for 4-DoF manipulator with uncertain
dynamical properties, and outcomes of testing of this strategy applied to
control of simulator of robot.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00507</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00507</id><created>2015-01-02</created><authors><author><keyname>Yodaiken</keyname><forenames>Victor</forenames></author></authors><title>Defining and composing big state machines</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A sequence function alternative representation of state machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00512</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00512</id><created>2015-01-02</created><authors><author><keyname>Mas</keyname><forenames>Massimiliano Dal</forenames></author></authors><title>Function of Forgetfulness for the Tedium of Oblivion on Liquidity of
  Ontology Matching</title><categories>cs.CY</categories><comments>4 pages, 1 figure; for details see: http://www.maxdalmas.com</comments><msc-class>03B65, 03G10, 68M11, 68P05, 68Q55, 68T30, 68U35</msc-class><acm-class>D.2.2; G.1.10; G.2.2; H.1.1; H.1.2; H.3.1; H.3.3; H.3.5; H.5.2;
  H.5.3; H.5.4; I.2.1; I.2.4; I.2.7; I.3.6; K.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The shallow and fragile knowledge on the Web does not examine in depth the
things: it behaves lightly. The conditions created by the Web makes our
attention labile and especially fickle, it's unable to concentrate for long as
we are trained to &quot;surf&quot; without going though never in depth. The Web also
brings with it the added advantage of a nearly availability infinite knowledge
but leads to a loss of the ability to retain and evaluate that knowledge within
us increasing forgetfulness of knowledge. In this paper we show how the
&quot;function of forgetfulness&quot; appears linked to tedium and oblivion of knowledge
through the liquidity of ontology matching.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00513</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00513</id><created>2015-01-02</created><authors><author><keyname>P&#xe2;ris</keyname><forenames>Jehan-Fran&#xe7;ois</forenames></author><author><keyname>Amer</keyname><forenames>Ahmed</forenames></author><author><keyname>Long</keyname><forenames>Darrell D. E.</forenames></author><author><keyname>Schwarz</keyname><forenames>Thomas J. E.</forenames></author></authors><title>Self-Repairing Disk Arrays</title><categories>cs.DC</categories><comments>Part of ADAPT Workshop proceedings, 2015 (arXiv:1412.2347)</comments><report-no>ADAPT/2015/02</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the prices of magnetic storage continue to decrease, the cost of replacing
failed disks becomes increasingly dominated by the cost of the service call
itself. We propose to eliminate these calls by building disk arrays that
contain enough spare disks to operate without any human intervention during
their whole lifetime. To evaluate the feasibility of this approach, we have
simulated the behavior of two-dimensional disk arrays with n parity disks and
n(n-1)/2 data disks under realistic failure and repair assumptions. Our
conclusion is that having n(n+1)/2 spare disks is more than enough to achieve a
99.999 percent probability of not losing data over four years. We observe that
the same objectives cannot be reached with RAID level 6 organizations and would
require RAID stripes that could tolerate triple disk failures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00526</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00526</id><created>2015-01-02</created><authors><author><keyname>Thuseethan</keyname><forenames>S.</forenames></author></authors><title>Department Management System for Departments of Sri Lankan Universities</title><categories>cs.CY</categories><journal-ref>International Journal Of Scientific &amp; Technology Research, 3(6),
  173-175 (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new method of data handling as well as implementation
of management system for an academic department. Management system is a proven
framework for managing and continually improving the organizations policies,
procedures and processes. Department of Sri Lankan Universities is a division
within a faculty comprising one subject area or a number of related subject
areas. The time consumption and error rate of producing information is
extremely high. This paper describes how to do the data handling in a
department of Sri Lankan Universities using the management system. The
efficiency of the management system measured with the amount of data handled by
the department. Experiment results shows that this method of data handling
increase the efficiency of the department in terms of processing information
and the reduction of risks involved with the department activities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00528</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00528</id><created>2015-01-02</created><authors><author><keyname>Thuseethan</keyname><forenames>S.</forenames></author><author><keyname>Kuhanesan</keyname><forenames>S.</forenames></author></authors><title>Influence of Facebook in Academic Performance of Sri Lankan University
  Students</title><categories>cs.CY</categories><journal-ref>Global Journal of Computer Science and Technology, 14(4) (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Facebook is only an electronic communication between human but unfortunately
it has become an addiction for all. This paper examines the usage of Facebook
among university students and its influence in their academic performance. The
impact of Facebook can either be good or bad on university students and in
their academic activities. Even though a closer look on the real impact of
Facebook reveals that it leads to several problems in university students
academic performances. Today Facebook is somehow destroying the future and
academic carrier of university students. At the same time also intended to find
the significance of use of Facebook by University students in their academic
success with the help of a survey conducted to collect the data among more than
250 students of different Universities in Sri Lanka.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00529</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00529</id><created>2015-01-02</created><authors><author><keyname>Thuseethan</keyname><forenames>S.</forenames></author><author><keyname>Kuhanesan</keyname><forenames>S.</forenames></author></authors><title>Effective Use of Human Computer Interaction in Digital Academic
  Supportive Devices</title><categories>cs.HC cs.CY</categories><journal-ref>International Journal of Science and Research, 3(6), 388-392
  (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this research, a literature in human-computer interaction is reviewed and
the technology aspect of human computer interaction related with digital
academic supportive devices is also analyzed. According to all these concerns,
recommendations to design good human-computer digital academic supportive
devices are analyzed and proposed. Due to improvements in both hardware and
software, digital devices have unveiled continuous advances in efficiency and
processing capacity. However, many of these systems are also becoming larger
and increasingly more complex. Although such complexity usually poses no
difficulties for many users, it often creates barriers for academic users while
using digital devices. Usually, in designing those digital devices, the
human-computer interaction is left behind without consideration. To achieve
dependable, usable, and well-engineered interactive digital academic supportive
devices requires applied human computer interaction research and awareness of
its issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00539</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00539</id><created>2015-01-03</created><authors><author><keyname>Bunte</keyname><forenames>Christoph</forenames></author><author><keyname>Lapidoth</keyname><forenames>Amos</forenames></author></authors><title>Maximum R\'enyi Entropy Rate</title><categories>cs.IT math.IT</categories><comments>Submitted to the IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two maximization problems of R\'enyi entropy rate are investigated: the
maximization over all stochastic processes whose marginals satisfy a linear
constraint, and the Burg-like maximization over all stochastic processes whose
autocovariance function begins with some given values. The solutions are
related to the solutions to the analogous maximization problems of Shannon
entropy rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00549</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00549</id><created>2015-01-03</created><authors><author><keyname>Pastor-Escuredo</keyname><forenames>David</forenames></author><author><keyname>Savy</keyname><forenames>Thierry</forenames></author><author><keyname>Luengo-Oroz</keyname><forenames>Miguel A.</forenames></author></authors><title>Can Fires, Night Lights, and Mobile Phones reveal behavioral
  fingerprints useful for Development?</title><categories>cs.CY</categories><comments>Published in D4D Challenge. NetMob, May 1-3, 2013, MIT</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fires, lights at night and mobile phone activity have been separately used as
proxy indicators of human activity with high potential for measuring human
development. In this preliminary report, we develop some tools and
methodologies to identify and visualize relations among remote sensing datasets
containing fires and night lights information with mobile phone activity in
Cote D'Ivoire from December 2011 to April 2012.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00559</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00559</id><created>2015-01-03</created><authors><author><keyname>Cheng</keyname><forenames>Hao-Chung</forenames></author><author><keyname>Hsieh</keyname><forenames>Min-Hsiu</forenames></author><author><keyname>Yeh</keyname><forenames>Ping-Cheng</forenames></author></authors><title>The Learnability of Unknown Quantum Measurements</title><categories>quant-ph cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum machine learning has received significant attention in recent years,
and promising progress has been made in the development of quantum algorithms
to speed up traditional machine learning tasks. In this work, however, we focus
on investigating the information-theoretic upper bounds of sample complexity -
how many training samples are sufficient to predict the future behaviour of an
unknown target function. This kind of problem is, arguably, one of the most
fundamental problems in statistical learning theory and the bounds for
practical settings can be completely characterised by a simple measure of
complexity.
  Our main result in the paper is that, for learning an unknown quantum
measurement, the upper bound, given by the fat-shattering dimension, is
linearly proportional to the dimension of the underlying Hilbert space.
Learning an unknown quantum state becomes a dual problem to ours, and as a
byproduct, we can recover Aaronson's famous result [Proc. R. Soc. A
463:3089-3144 (2007)] solely using a classical machine learning technique. In
addition, other famous complexity measures like covering numbers and Rademacher
complexities are derived explicitly. We are able to connect measures of sample
complexity with various areas in quantum information science, e.g. quantum
state/measurement tomography, quantum state discrimination and quantum random
access codes, which may be of independent interest. Lastly, with the assistance
of general Bloch-sphere representation, we show that learning quantum
measurements/states can be mathematically formulated as a neural network.
Consequently, classical ML algorithms can be applied to efficiently accomplish
the two quantum learning tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00561</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00561</id><created>2015-01-03</created><authors><author><keyname>Ahn</keyname><forenames>Hee-Kap</forenames></author><author><keyname>Barba</keyname><forenames>Luis</forenames></author><author><keyname>Bose</keyname><forenames>Prosenjit</forenames></author><author><keyname>de Carufel</keyname><forenames>Jean-Lou</forenames></author><author><keyname>Korman</keyname><forenames>Matias</forenames></author><author><keyname>Oh</keyname><forenames>Eunjin</forenames></author></authors><title>A linear-time algorithm for the geodesic center of a simple polygon</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given two points in a simple polygon $P$ of $n$ vertices, its geodesic
distance is the length of the shortest path that connects them among all paths
that stay within $P$. The geodesic center of $P$ is the unique point in $P$
that minimizes the largest geodesic distance to all other points of $P$. In
1989, Pollack, Sharir and Rote [Disc. \&amp; Comput. Geom. 89] showed an $O(n\log
n)$-time algorithm that computes the geodesic center of $P$. Since then, a
longstanding question has been whether this running time can be improved
(explicitly posed by Mitchell [Handbook of Computational Geometry, 2000]). In
this paper we affirmatively answer this question and present a linear time
algorithm to solve this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00563</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00563</id><created>2015-01-03</created><updated>2015-02-22</updated><authors><author><keyname>Bj&#xf6;rklund</keyname><forenames>Andreas</forenames></author><author><keyname>Kamat</keyname><forenames>Vikram</forenames></author><author><keyname>Kowalik</keyname><forenames>&#x141;ukasz</forenames></author><author><keyname>Zehavi</keyname><forenames>Meirav</forenames></author></authors><title>Spotting Trees with Few Leaves</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show two results related to the Hamiltonicity and $k$-Path algorithms in
undirected graphs by Bj\&quot;orklund [FOCS'10], and Bj\&quot;orklund et al., [arXiv'10].
First, we demonstrate that the technique used can be generalized to finding
some $k$-vertex tree with $l$ leaves in an $n$-vertex undirected graph in
$O^*(1.657^k2^{l/2})$ time. It can be applied as a subroutine to solve the
$k$-Internal Spanning Tree ($k$-IST) problem in $O^*(\min(3.455^k, 1.946^n))$
time using polynomial space, improving upon previous algorithms for this
problem. In particular, for the first time we break the natural barrier of
$O^*(2^n)$. Second, we show that the iterated random bipartition employed by
the algorithm can be improved whenever the host graph admits a vertex coloring
with few colors; it can be an ordinary proper vertex coloring, a fractional
vertex coloring, or a vector coloring. In effect, we show improved bounds for
$k$-Path and Hamiltonicity in any graph of maximum degree $\Delta=4,\ldots,12$
or with vector chromatic number at most 8.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00567</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00567</id><created>2015-01-03</created><authors><author><keyname>Wang</keyname><forenames>Lan</forenames></author><author><keyname>Gelenbe</keyname><forenames>Erol</forenames></author></authors><title>Adaptive Dispatching of Tasks in the Cloud</title><categories>cs.DC</categories><comments>10 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasingly wide application of Cloud Computing enables the
consolidation of tens of thousands of applications in shared infrastructures.
Thus, meeting the quality of service requirements of so many diverse
applications in such shared resource environments has become a real challenge,
especially since the characteristics and workload of applications differ widely
and may change over time. This paper presents an experimental system that can
exploit a variety of online quality of service aware adaptive task allocation
schemes, and three such schemes are designed and compared. These are a
measurement driven algorithm that uses reinforcement learning, secondly a
&quot;sensible&quot; allocation algorithm that assigns jobs to sub-systems that are
observed to provide a lower response time, and then an algorithm that splits
the job arrival stream into sub-streams at rates computed from the hosts'
processing capabilities. All of these schemes are compared via measurements
among themselves and with a simple round-robin scheduler, on two experimental
test-beds with homogeneous and heterogeneous hosts having different processing
capacities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00569</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00569</id><created>2015-01-03</created><authors><author><keyname>Place</keyname><forenames>Thomas</forenames></author><author><keyname>Zeitoun</keyname><forenames>Marc</forenames></author></authors><title>A Transfer Theorem for the Separation Problem</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate two problems for a class C of regular word languages. The
C-membership problem asks for an algorithm to decide whether an input language
belongs to C. The C-separation problem asks for an algorithm that, given as
input two regular languages, decides whether there exists a third language in C
containing the first language, while being disjoint from the second. These
problems are considered as means to obtain a deep understanding of the class C.
  It is usual for such classes to be defined by logical formalisms. Logics are
often built on top of each other, by adding new predicates. A natural
construction is to enrich a logic with the successor relation. In this paper,
we obtain simple self-contained proofs of two transfer results: we show that
for suitable logically defined classes, the membership, resp. the separation
problem for a class enriched with the successor relation reduces to the same
problem for the original class.
  Our reductions work both for languages of finite words and infinite words.
The proofs are mostly self-contained, and only require a basic background on
regular languages. This paper therefore gives new, simple proofs of results
that were considered as difficult, such as the decid- ability of the membership
problem for the levels 1, 3/2, 2 and 5/2 of the dot-depth hierarchy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00579</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00579</id><created>2015-01-03</created><authors><author><keyname>Nasir</keyname><forenames>Saad Bin</forenames></author><author><keyname>Raychowdhury</keyname><forenames>Arijit</forenames></author></authors><title>A Model Study of an All-Digital, Discrete-Time and Embedded Linear
  Regulator</title><categories>cs.AR</categories><comments>Submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With an increasing number of power-states, finer- grained power management
and larger dynamic ranges of digital circuits, the integration of compact,
scalable linear-regulators embedded deep within logic blocks has become
important. While analog linear-regulators have traditionally been used in
digital ICs, the need for digitally implementable designs that can be
synthesized and embedded in digital functional units for ultra fine- grained
power management has emerged. This paper presents the circuit design and
control models of an all-digital, discrete-time linear regulator and explores
the parametric design space for transient response time and loop stability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00587</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00587</id><created>2015-01-03</created><authors><author><keyname>Toni</keyname><forenames>Laura</forenames></author><author><keyname>Frossard</keyname><forenames>Pascal</forenames></author></authors><title>Prioritized Random MAC Optimization via Graph-based Analysis</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the analogy between successive interference cancellation and
iterative belief-propagation on erasure channels, irregular repetition slotted
ALOHA (IRSA) strategies have received a lot of attention in the design of
medium access control protocols. The IRSA schemes have been mostly analyzed for
theoretical scenarios for homogenous sources, where they are shown to
substantially improve the system performance compared to classical slotted
ALOHA protocols. In this work, we consider generic systems where sources in
different importance classes compete for a common channel. We propose a new
prioritized IRSA algorithm and derive the probability to correctly resolve
collisions for data from each source class. We then make use of our theoretical
analysis to formulate a new optimization problem for selecting the transmission
strategies of heterogenous sources. We optimize both the replication
probability per class and the source rate per class, in such a way that the
overall system utility is maximized. We then propose a heuristic-based
algorithm for the selection of the transmission strategy, which is built on
intrinsic characteristics of the iterative decoding methods adopted for
recovering from collisions. Experimental results validate the accuracy of the
theoretical study and show the gain of well-chosen prioritized transmission
strategies for transmission of data from heterogenous classes over shared
wireless channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00594</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00594</id><created>2015-01-03</created><updated>2015-06-16</updated><authors><author><keyname>Jiang</keyname><forenames>Jonathan Q.</forenames></author></authors><title>Stochastic block model and exploratory analysis in signed networks</title><categories>cs.SI physics.soc-ph</categories><comments>12 pages, 8 figures, 2 tables. arXiv admin note: text overlap with
  arXiv:1110.1976 by other authors</comments><journal-ref>Phys. Rev. E 91, 062805 (2015)</journal-ref><doi>10.1103/PhysRevE.91.062805</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a generalized stochastic block model to explore the mesoscopic
structures in signed networks by grouping vertices that exhibit similar
positive and negative connection profiles into the same cluster. In this model,
the group memberships are viewed as hidden or unobserved quantities, and the
connection patterns between groups are explicitly characterized by two block
matrices, one for positive links and the other for negative links. By fitting
the model to the observed network, we can not only extract various structural
patterns existing in the network without prior knowledge, but also recognize
what specific structures we obtained. Furthermore, the model parameters provide
vital clues about the probabilities that each vertex belongs to different
groups and the centrality of each vertex in its corresponding group. This
information sheds light on the discovery of the networks' overlapping
structures and the identification of two types of important vertices, which
serve as the cores of each group and the bridges between different groups,
respectively. Experiments on a series of synthetic and real-life networks show
the effectiveness as well as the superiority of our model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00601</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00601</id><created>2015-01-03</created><updated>2015-04-09</updated><authors><author><keyname>&#xd6;zkural</keyname><forenames>Eray</forenames></author></authors><title>Ultimate Intelligence Part I: Physical Completeness and Objectivity of
  Induction</title><categories>cs.AI</categories><comments>Under review at AGI-2015 conference. An early draft was submitted to
  ALT-2014. This paper is now being split into two papers, one philosophical,
  and one more technical. We intend that all installments of the paper series
  will be on the arxiv</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose that Solomonoff induction is complete in the physical sense via
several strong physical arguments. We also argue that Solomonoff induction is
fully applicable to quantum mechanics. We show how to choose an objective
reference machine for universal induction by defining a physical message
complexity and physical message probability, and argue that this choice
dissolves some well-known objections to universal induction. We also introduce
many more variants of physical message complexity based on energy and action,
and discuss the ramifications of our proposals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00602</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00602</id><created>2015-01-03</created><authors><author><keyname>Bachoc</keyname><forenames>Christine</forenames></author><author><keyname>Serra</keyname><forenames>Oriol</forenames></author><author><keyname>Zemor</keyname><forenames>Gilles</forenames></author></authors><title>An analogue of Vosper's Theorem for Extension Fields</title><categories>math.NT cs.IT math.CO math.IT</categories><msc-class>11P70 (Primary) 94B65 12F99 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are interested in characterising pairs $S,T$ of $F$-linear subspaces in a
field extension $L/F$ such that the linear span $ST$ of the set of products of
elements of $S$ and of elements of $T$ has small dimension. Our central result
is a linear analogue of Vosper's Theorem, which gives the structure of vector
spaces $S,T$ in a prime extension $L$ of a finite field $F$ for which
$\dim_F(ST) =\dim_F(S)+\dim_F(T)-1$, when $\dim_F(S), \dim_F(T) &gt;1$ and
$\dim_F(ST) &lt; [L:F]-1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00606</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00606</id><created>2015-01-03</created><authors><author><keyname>Teimoory</keyname><forenames>Mehri</forenames></author><author><keyname>Amirsoleimani</keyname><forenames>Amirali</forenames></author><author><keyname>Shamsi</keyname><forenames>Jafar</forenames></author><author><keyname>Ahmadi</keyname><forenames>Arash</forenames></author><author><keyname>Alirezaee</keyname><forenames>Shahpour</forenames></author><author><keyname>Ahmadi</keyname><forenames>Majid</forenames></author></authors><title>Optimized Implementation of Memristor-Based Full Adder by Material
  Implication Logic</title><categories>cs.ET</categories><comments>International Conference on Electronics Circuits and Systems (ICECS),
  2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently memristor-based applications and circuits are receiving an increased
attention. Furthermore, memristors are also applied in logic circuit design.
Material implication logic is one of the main areas with memristors. In this
paper an optimized memristor-based full adder design by material implication
logic is presented. This design needs 27 memristors and less area in comparison
with typical CMOS-based 8-bit full adders. Also the presented full adder needs
only 184 computational steps which enhance former full adder design speed by 20
percent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00607</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00607</id><created>2015-01-03</created><authors><author><keyname>Danjuma</keyname><forenames>Kwetishe</forenames></author><author><keyname>Osofisan</keyname><forenames>Adenike O.</forenames></author></authors><title>Evaluation of Predictive Data Mining Algorithms in Erythemato-Squamous
  Disease Diagnosis</title><categories>cs.LG cs.CE</categories><comments>10 pages, 3 figures 2 tables</comments><journal-ref>IJCSI International Journal of Computer Science Issues, 11(6),
  85-94 (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A lot of time is spent searching for the most performing data mining
algorithms applied in clinical diagnosis. The study set out to identify the
most performing predictive data mining algorithms applied in the diagnosis of
Erythemato-squamous diseases. The study used Naive Bayes, Multilayer Perceptron
and J48 decision tree induction to build predictive data mining models on 366
instances of Erythemato-squamous diseases datasets. Also, 10-fold
cross-validation and sets of performance metrics were used to evaluate the
baseline predictive performance of the classifiers. The comparative analysis
shows that the Naive Bayes performed best with accuracy of 97.4%, Multilayer
Perceptron came out second with accuracy of 96.6%, and J48 came out the worst
with accuracy of 93.5%. The evaluation of these classifiers on clinical
datasets, gave an insight into the predictive ability of different data mining
algorithms applicable in clinical diagnosis especially in the diagnosis of
Erythemato-squamous diseases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00611</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00611</id><created>2015-01-03</created><updated>2015-02-08</updated><authors><author><keyname>Chen</keyname><forenames>Shihyen</forenames></author></authors><title>A Review on the Tree Edit Distance Problem and Related
  Path-Decomposition Algorithms</title><categories>cs.DS cs.CC cs.DM</categories><comments>23 pages. 13 figures. Revisions: minor changes</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An ordered labeled tree is a tree in which the nodes are labeled and the
left-to-right order among siblings is relevant. The edit distance between two
ordered labeled trees is the minimum cost of changing one tree into the other
through a sequence of edit steps. In the literature, there are a class of
algorithms based on different yet closely related path-decomposition schemes.
This article reviews the principles of these algorithms, and studies the
concepts related to the algorithmic complexities as a consequence of the
decomposition schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00614</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00614</id><created>2015-01-03</created><authors><author><keyname>Kalayeh</keyname><forenames>Mahdi M.</forenames></author><author><keyname>Mussmann</keyname><forenames>Stephen</forenames></author><author><keyname>Petrakova</keyname><forenames>Alla</forenames></author><author><keyname>Lobo</keyname><forenames>Niels da Vitoria</forenames></author><author><keyname>Shah</keyname><forenames>Mubarak</forenames></author></authors><title>Understanding Trajectory Behavior: A Motion Pattern Approach</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mining the underlying patterns in gigantic and complex data is of great
importance to data analysts. In this paper, we propose a motion pattern
approach to mine frequent behaviors in trajectory data. Motion patterns,
defined by a set of highly similar flow vector groups in a spatial locality,
have been shown to be very effective in extracting dominant motion behaviors in
video sequences. Inspired by applications and properties of motion patterns, we
have designed a framework that successfully solves the general task of
trajectory clustering. Our proposed algorithm consists of four phases: flow
vector computation, motion component extraction, motion component's
reachability set creation, and motion pattern formation. For the first phase,
we break down trajectories into flow vectors that indicate instantaneous
movements. In the second phase, via a Kmeans clustering approach, we create
motion components by clustering the flow vectors with respect to their location
and velocity. Next, we create motion components' reachability set in terms of
spatial proximity and motion similarity. Finally, for the fourth phase, we
cluster motion components using agglomerative clustering with the weighted
Jaccard distance between the motion components' signatures, a set created using
path reachability. We have evaluated the effectiveness of our proposed method
in an extensive set of experiments on diverse datasets. Further, we have shown
how our proposed method handles difficulties in the general task of trajectory
clustering that challenge the existing state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00619</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00619</id><created>2015-01-03</created><authors><author><keyname>Zhang</keyname><forenames>Yu</forenames></author><author><keyname>Xiong</keyname><forenames>Ke</forenames></author></authors><title>Outage Probability of Overhearing Amplify-and-Forward Cooperative
  Relaying</title><categories>cs.IT math.IT</categories><comments>4 pages, 3 figures, submitted to ELECTRONICS 2015</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper investigates the outage performance of overhearing
amplify-and-forward (AF) cooperative relaying, where a source transmits
information to its destination through multiple helping overhearing AF relays
with space-time network coding (STNC) employed. Firstly, the transmission
protocol of such a relaying system, i.e., cooperative relaying with overhearing
AF relays based on STNC (STNC-OHAF) is presented. Then, the instantaneous
end-to-end SNR expression of STNC-OHAF is analysed. Based on this, an explicit
expression of the outage probability for STNC-OHAF over independent but not
necessarily identically distributed (i.n.i.d) Rayleigh fading channels is
theoretically derived. Numerical results validate our theoretical analysis and
show that by introducing overhearing among relays, the outage performance of
the system can be greatly improved. It also shows that there is a trade-off
between system sum outage capacity and the transmitted number of symbols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00620</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00620</id><created>2015-01-03</created><authors><author><keyname>Lu</keyname><forenames>Xiao</forenames></author><author><keyname>Wang</keyname><forenames>Ping</forenames></author><author><keyname>Niyato</keyname><forenames>Dusit</forenames></author></authors><title>Hierarchical Cooperation for Operator-Controlled Device-to-Device
  Communications: A Layered Coalitional Game Approach</title><categories>cs.NI cs.GT</categories><comments>IEEE Wireless Communications and Networking Conference 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Device-to-Device (D2D) communications, which allow direct communication among
mobile devices, have been proposed as an enabler of local services in 3GPP
LTE-Advanced (LTE-A) cellular networks. This work investigates a hierarchical
LTE-A network framework consisting of multiple D2D operators at the upper layer
and a group of devices at the lower layer. We propose a cooperative model that
allows the operators to improve their utility in terms of revenue by sharing
their devices, and the devices to improve their payoff in terms of end-to-end
throughput by collaboratively performing multi-path routing. To help
understanding the interaction among operators and devices, we present a
game-theoretic framework to model the cooperation behavior, and further, we
propose a layered coalitional game (LCG) to address the decision making
problems among them. Specifically, the cooperation of operators is modeled as
an overlapping coalition formation game (CFG) in a partition form, in which
operators should form a stable coalitional structure. Moreover, the cooperation
of devices is modeled as a coalitional graphical game (CGG), in which devices
establish links among each other to form a stable network structure for
multi-path routing.We adopt the extended recursive core, and Nash network, as
the stability concept for the proposed CFG and CGG, respectively. Numerical
results demonstrate that the proposed LCG yields notable gains compared to both
the non-cooperative case and a LCG variant and achieves good convergence speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00622</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00622</id><created>2015-01-03</created><updated>2015-01-24</updated><authors><author><keyname>Ge</keyname><forenames>Dongdong</forenames></author><author><keyname>Wang</keyname><forenames>Zizhuo</forenames></author><author><keyname>Ye</keyname><forenames>Yinyu</forenames></author><author><keyname>Yin</keyname><forenames>Hao</forenames></author></authors><title>Strong NP-Hardness Result for Regularized $L_q$-Minimization Problems
  with Concave Penalty Functions</title><categories>math.OC cs.CC math.ST stat.CO stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, we consider the regularize $L_q$-minimization problem ($q\ge
1$) with a general penalty function. We show that if the penalty function is
concave but not linear in a neighborhood of zero, then the optimization problem
is strongly NP-hard. This result answers the complexity of many regularized
optimization problems studied in the literature. It implies that it is
impossible to have a fully polynomial-time approximation scheme (FPTAS) for a
large class of regularization problems unless P = NP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00624</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00624</id><created>2015-01-03</created><authors><author><keyname>Efremenko</keyname><forenames>Klim</forenames></author><author><keyname>Gelles</keyname><forenames>Ran</forenames></author><author><keyname>Haeupler</keyname><forenames>Bernhard</forenames></author></authors><title>Maximal Noise in Interactive Communication over Erasure Channels and
  Channels with Feedback</title><categories>cs.DS</categories><comments>A preliminary version of this work appears at ITCS'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide tight upper and lower bounds on the noise resilience of
interactive communication over noisy channels with feedback. In this setting,
we show that the maximal fraction of noise that any robust protocol can resist
is 1/3. Additionally, we provide a simple and efficient robust protocol that
succeeds as long as the fraction of noise is at most 1/3 - \epsilon.
Surprisingly, both bounds hold regardless of whether the parties send bits or
symbols from an arbitrarily large alphabet. We also consider interactive
communication over erasure channels. We provide a protocol that matches the
optimal tolerable erasure rate of 1/2 - \epsilon of previous protocols
(Franklin et al., CRYPTO '13) but operates in a much simpler and more efficient
way. Our protocol works with an alphabet of size 4, in contrast to prior
protocols in which the alphabet size grows as epsilon goes to zero. Building on
the above algorithm with a fixed alphabet size, we are able to devise a
protocol for binary erasure channels that tolerates erasure rates of up to 1/3
- \epsilon.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00630</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00630</id><created>2015-01-03</created><updated>2015-02-21</updated><authors><author><keyname>Khoo</keyname><forenames>Yuehaw</forenames></author><author><keyname>Kapoor</keyname><forenames>Ankur</forenames></author></authors><title>Towards non-iterative closest point: Exact recovery of pose for rigid
  2D/3D registration using semidefinite programming</title><categories>cs.CV math.OC</categories><comments>13 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a convex programming framework for pose estimation in 2D/3D
point-set registration with unknown point correspondences. We give two
mixed-integer nonlinear program (MINP) formulations of the 2D/3D registration
problem when there are multiple 2D images, and propose convex relaxations for
both of the MINPs to semidefinite programs (SDP) that can be solved efficiently
by interior point methods. Our approach to the 2D/3D registration problem is
non-iterative in nature as we jointly solve for pose and correspondence.
Furthermore, these convex programs can readily incorporate feature descriptors
of points to enhance registration results. We prove that the convex programs
exactly recover the solution to the original nonconvex 2D/3D registration
problem under noiseless condition. We apply these formulations to the
registration of 3D models of coronary vessels to their 2D projections obtained
from multiple intra-operative fluoroscopic images. For this application, we
experimentally corroborate the exact recovery property in the absence of noise
and further demonstrate robustness of the convex programs in the presence of
noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00637</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00637</id><created>2015-01-04</created><authors><author><keyname>Amini</keyname><forenames>Rashied</forenames></author></authors><title>&quot;Should I break up with my girlfriend? Will I find another?&quot; Or: An
  Algorithm for the Forecasting of Romantic Options</title><categories>cs.SI</categories><comments>Nanaya Romance White Paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The prospect of finding love may be scary but the prospect of committing to a
relationship for the rest of your life is almost certainly scary. The secretary
problem is a parallel to romantic decision making where an individual decides
when to be satisfied with a selection choice in the face of uncertain future
options. However, the secretary problem and its variations still do not provide
a practical solution in a world where individual preference, goals, and
societal context create a highly complex space of values that factor into
decision making. In light of these complexities, we offer a general process
that can determine the value of romantic options in a highly personal context.
This algorithm is currently being developed into a service that will be
available in 2015 for the general public.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00638</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00638</id><created>2015-01-04</created><authors><author><keyname>Mackowski</keyname><forenames>Daniel</forenames></author><author><keyname>Bai</keyname><forenames>Yun</forenames></author><author><keyname>Ouyang</keyname><forenames>Yanfeng</forenames></author></authors><title>Parking Space Management via Dynamic Performance-Based Pricing</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In congested urban areas, it remains a pressing challenge to reduce
unnecessary vehicle circling for parking while at the same time maximize
parking space utilization. In observance of new information technologies that
have become readily accessible to drivers and parking agencies, we develop a
dynamic non-cooperative bi-level model (i.e. Stackelberg leader-follower game)
to set parking prices in real-time for effective parking access and space
utilization. The model is expected to fit into an integrated parking pricing
and management system, where parking reservations and transactions are
facilitated by sensing and informatics infrastructures, that ensures the
availability of convenient spaces at equilibrium market prices. It is shown
with numerical examples that the proposed dynamic parking pricing model has the
potential to virtually eliminate vehicle circling for parking, which results in
significant reduction in adverse socioeconomic externalities such as traffic
congestion and emissions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.00642</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.00642</id><created>2015-01-04</created><updated>2015-04-23</updated><authors><author><keyname>Zhang</keyname><forenames>Chao</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Shen</keyname><forenames>Tingzhi</forenames></author></authors><title>Unsupervised Feature Learning for Dense Correspondences across Scenes</title><categories>cs.CV</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a fast, accurate matching method for estimating dense pixel
correspondences across scenes. It is a challenging problem to estimate dense
pixel correspondences between images depicting different scenes or instances of
the same object category. While most such matching methods rely on hand-crafted
features such as SIFT, we learn features from a large amount of unlabeled image
patches using unsupervised learning. Pixel-layer features are obtained by
encoding over the dictionary, followed by spatial pooling to obtain patch-layer
features. The learned features are then seamlessly embedded into a multi-layer
match- ing framework. We experimentally demonstrate that the learned features,
together with our matching model, outperforms state-of-the-art methods such as
the SIFT flow, coherency sensitive hashing and the recent deformable spatial
pyramid matching methods both in terms of accuracy and computation efficiency.
Furthermore, we evaluate the performance of a few different dictionary learning
and feature encoding methods in the proposed pixel correspondences estimation
framework, and analyse the impact of dictionary learning and feature encoding
with respect to the final matching performance.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="70000" completeListSize="102538">1122234|71001</resumptionToken>
</ListRecords>
</OAI-PMH>
