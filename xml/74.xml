<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T03:43:55Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|73001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04301</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04301</id><created>2015-02-15</created><updated>2015-03-15</updated><authors><author><keyname>Kaibel</keyname><forenames>Volker</forenames></author><author><keyname>Onn</keyname><forenames>Shmuel</forenames></author><author><keyname>Sarrabezolles</keyname><forenames>Pauline</forenames></author></authors><title>The Unimodular Intersection Problem</title><categories>math.OC cs.DM cs.DS math.CO</categories><msc-class>05A, 15A, 51M, 52A, 52B, 52C, 62H, 68Q, 68R, 68U, 68W, 90B, 90C</msc-class><journal-ref>Operations Research Letters, 43:592-594, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that finding minimally intersecting $n$ paths from $s$ to $t$ in a
directed graph or $n$ perfect matchings in a bipartite graph can be done in
polynomial time. This holds more generally for unimodular set systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04312</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04312</id><created>2015-02-15</created><authors><author><keyname>Nemoto</keyname><forenames>Keiichi</forenames></author><author><keyname>Okada</keyname><forenames>Ken-ichi</forenames></author></authors><title>WIKI THANKS: Cultural Differences in Thanks Network of
  Different-Language Wikipedias</title><categories>cs.SI</categories><comments>Proceedings of the 5th International Conference on Collaborative
  Innovation Networks COINs15, Tokyo, Japan March 12-14, 2015
  (arXiv:1502.01142)</comments><report-no>coins15/2015/07</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wikipedia introduced a new social function &quot;wiki-thanks&quot;. &quot;Wiki-thanks&quot;
enable editors to send thanks to other editors' contributions. In this paper,
we aim to investigate this new social tool from different cultural
perspectives. To achieve this goal, we analyze &quot;wiki-thanks&quot; log events and
compared the English, German, Spanish, Chinese, Japanese, Korean, and Finish
language Wikipedias.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04316</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04316</id><created>2015-02-15</created><authors><author><keyname>Fei</keyname><forenames>Zongming</forenames></author><author><keyname>Yang</keyname><forenames>Jianjun</forenames></author><author><keyname>Lu</keyname><forenames>Hui</forenames></author></authors><title>Improving Routing Efficiency through Intermediate Target Based
  Geographic Routing</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The greedy strategy of geographical routing may cause the local minimum
problem when there is a hole in the routing area. It depends on other
strategies such as perimeter routing to find a detour path, which can be long
and result in inefficiency of the routing protocol. In this paper, we propose a
new approach called Intermediate Target based Geographic Routing (ITGR) to
solve the long detour path problem. The basic idea is to use previous
experience to determine the destination areas that are shaded by the holes. The
novelty of the approach is that a single forwarding path can be used to
determine a shaded area that may cover many destination nodes. We design an
efficient method for the source to find out whether a destination node belongs
to a shaded area. The source then selects an intermediate node as the tentative
target and greedily forwards packets to it, which in turn forwards the packet
to the final destination by greedy routing. ITGR can combine multiple shaded
areas to improve the efficiency of representation and routing. We perform
simulations and demonstrate that ITGR significantly reduces the routing path
length, compared with existing geographic routing protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04321</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04321</id><created>2015-02-15</created><updated>2015-02-17</updated><authors><author><keyname>Schi&#xf6;berg</keyname><forenames>Doris</forenames></author><author><keyname>Schneider</keyname><forenames>Fabian</forenames></author><author><keyname>Schmid</keyname><forenames>Stefan</forenames></author><author><keyname>Uhlig</keyname><forenames>Steve</forenames></author><author><keyname>Feldmann</keyname><forenames>Anja</forenames></author></authors><title>Evolution of Directed Triangle Motifs in the Google+ OSN</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motifs are a fundamental building block and distinguishing feature of
networks. While characteristic motif distribution have been found in many
networks, very little is known today about the evolution of network motifs.
This paper studies the most important motifs in social networks, triangles, and
how directed triangle motifs change over time. Our chosen subject is one of the
largest Online Social Networks, Google+. Google+ has two distinguishing
features that make it particularly interesting: (1) it is a directed network,
which yields a rich set of triangle motifs, and (2) it is a young and fast
evolving network, whose role in the OSN space is still not fully understood.
For the purpose of this study, we crawled the network over a time period of six
weeks, collecting several snapshots. We find that some triangle types display
significant dynamics, e.g., for some specific initial types, up to 20% of the
instances evolve to other types. Due to the fast growth of the OSN in the
observed time period, many new triangles emerge. We also observe that many
triangles evolve into less-connected motifs (with less edges), suggesting that
growth also comes with pruning. We complement the topological study by also
considering publicly available user profile data (mostly geographic locations).
The corresponding results shed some light on the semantics of the triangle
motifs. Indeed, we find that users in more symmetric triangle motifs live
closer together, indicating more personal relationships. In contrast,
asymmetric links in motifs often point to faraway users with a high in-degree
(celebrities).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04326</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04326</id><created>2015-02-15</created><authors><author><keyname>Andreyev</keyname><forenames>Sergey</forenames></author></authors><title>Interface Transformation from Ruling to Obedience</title><categories>cs.HC</categories><comments>5 pages, 5 figures</comments><acm-class>H.1.2; H.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article is about one feature which was partly introduced 30 years ago
with the development of multi windows operating systems. It is about the
movability of screen objects not according to some predetermined algorithm but
by the direct user action. Many years ago it was introduced on a very limited
basis and nothing was improved since then. Smartphones and tablets give direct
access to screen elements but on a very limited set of commands (scroll and
zoom). There is an easy to use algorithm which turns any screen object into
movable / resizable. This algorithm uses only mouse to turn screens of normal
PCs into touchscreens, but this simple change means a revolution in our work
with computers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04328</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04328</id><created>2015-02-15</created><authors><author><keyname>Joeris</keyname><forenames>Benson</forenames></author><author><keyname>Urrutia</keyname><forenames>Isabel</forenames></author><author><keyname>Urrutia</keyname><forenames>Jorge</forenames></author></authors><title>Geometric Spanning Cycles in Bichromatic Point Sets</title><categories>cs.CG</categories><doi>10.1007/s00373-015-1545-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set of points in the plane each colored either red or blue, we find
non-self-intersecting geometric spanning cycles of the red points and of the
blue points such that each edge of the red spanning cycle is crossed at most
three times by the blue spanning cycle and vice-versa.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04331</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04331</id><created>2015-02-15</created><authors><author><keyname>Na</keyname><forenames>Seung-Hoon</forenames></author></authors><title>Two-Stage Document Length Normalization for Information Retrieval</title><categories>cs.IR</categories><comments>40 pages (to appear in ACM TOIS)</comments><acm-class>H.3.3</acm-class><journal-ref>ACM Transactions on Information Systems (TOIS), vol. 33, no. 2,
  2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The standard approach for term frequency normalization is based only on the
document length. However, it does not distinguish the verbosity from the scope,
these being the two main factors determining the document length. Because the
verbosity and scope have largely different effects on the increase in term
frequency, the standard approach can easily suffer from insufficient or
excessive penalization depending on the specific type of long document. To
overcome these problems, this paper proposes two-stage normalization by
performing verbosity and scope normalization separately, and by employing
different penalization functions. In verbosity normalization, each document is
pre-normalized by dividing the term frequency by the verbosity of the document.
In scope normalization, an existing retrieval model is applied in a
straightforward manner to the pre-normalized document, finally leading us to
formulate our proposed verbosity normalized (VN) retrieval model. Experimental
results carried out on standard TREC collections demonstrate that the VN model
leads to marginal but statistically significant improvements over standard
retrieval models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04336</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04336</id><created>2015-02-15</created><authors><author><keyname>Harremo&#xeb;s</keyname><forenames>Peter</forenames></author></authors><title>Lattices with non-Shannon Inequalities</title><categories>cs.IT math.IT</categories><comments>Ten pages. Submitted to ISIT 2015. The appendix will not appear in
  the proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the existence or absence of non-Shannon inequalities for variables
that are related by functional dependencies. Although the power-set on four
variables is the smallest Boolean lattice with non-Shannon inequalities there
exist lattices with many more variables without non-Shannon inequalities. We
search for conditions that ensures that no non-Shannon inequalities exist. It
is demonstrated that 3-dimensional distributive lattices cannot have
non-Shannon inequalities and planar modular lattices cannot have non-Shannon
inequalities. The existence of non-Shannon inequalities is related to the
question of whether a lattice is isomorphic to a lattice of subgroups of a
group.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04341</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04341</id><created>2015-02-15</created><updated>2015-08-16</updated><authors><author><keyname>Vorobjov</keyname><forenames>Nicolai</forenames></author><author><keyname>Gabrielov</keyname><forenames>Andrei</forenames></author></authors><title>On topological lower bounds for algebraic computation trees</title><categories>cs.CC math.AT</categories><comments>10 pages, minor editorial corrections</comments><msc-class>68Q17, 14P25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the height of any algebraic computation tree for deciding
membership in a semialgebraic set is bounded from below (up to a multiplicative
constant) by the logarithm of m-th Betti number (with respect to singular
homology) of the set, divided by m+1. This result complements the well known
lower bound by Yao for locally closed semialgebraic sets in terms of the total
Borel-Moore Betti number. We also prove that the height is bounded from below
by the logarithm of m-th Betti number of a projection of the set onto a
coordinate subspace, divided by (m+1)^2. We illustrate these general results by
examples of lower complexity bounds for some specific computational problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04344</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04344</id><created>2015-02-15</created><updated>2015-03-25</updated><authors><author><keyname>Lei</keyname><forenames>Lei</forenames></author><author><keyname>Yuan</keyname><forenames>Di</forenames></author><author><keyname>Ho</keyname><forenames>Chin Keong</forenames></author><author><keyname>Sun</keyname><forenames>Sumei</forenames></author></authors><title>Optimal Cell Clustering and Activation for Energy Saving in Load-Coupled
  Wireless Networks</title><categories>cs.IT math.IT</categories><comments>Revision, IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimizing activation and deactivation of base station transmissions provides
an instrument for improving energy efficiency in cellular networks. In this
paper, we study optimal cell clustering and scheduling of activation duration
for each cluster, with the objective of minimizing the sum energy, subject to a
time constraint of delivering the users' traffic demand. The cells within a
cluster are simultaneously in transmission and napping modes, with cluster
activation and deactivation, respectively. Our optimization framework accounts
for the coupling relation among cells due to the mutual interference. Thus, the
users' achievable rates in a cell depend on the cluster composition. On the
theoretical side, we provide mathematical formulation and structural
characterization for the energy-efficient cell clustering and scheduling
optimization problem, and prove its NP hardness. On the algorithmic side, we
first show how column generation facilitates problem solving, and then present
our notion of local enumeration as a flexible and effective means for dealing
with the trade-off between optimality and the combinatorial nature of cluster
formation, as well as for the purpose of gauging the deviation from optimality.
Numerical results demonstrate that our solutions achieve more than 60% energy
saving over existing schemes, and that the solutions we obtain are within a few
percent of deviation from global optimum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04348</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04348</id><created>2015-02-15</created><authors><author><keyname>Ahmed</keyname><forenames>Norman</forenames></author><author><keyname>Bryant</keyname><forenames>Jason</forenames></author><author><keyname>Hasseler</keyname><forenames>Gregory</forenames></author><author><keyname>Paulini</keyname><forenames>Matthew</forenames></author></authors><title>Semantic Modeling of Analytic-based Relationships with Direct
  Qualification</title><categories>cs.IR</categories><comments>Proceedings of the 2015 IEEE 9th International Conference on Semantic
  Computing (IEEE ICSC 2015)</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Successfully modeling state and analytics-based semantic relationships of
documents enhances representation, importance, relevancy, provenience, and
priority of the document. These attributes are the core elements that form the
machine-based knowledge representation for documents. However, modeling
document relationships that can change over time can be inelegant, limited,
complex or overly burdensome for semantic technologies. In this paper, we
present Direct Qualification (DQ), an approach for modeling any semantically
referenced document, concept, or named graph with results from associated
applied analytics. The proposed approach supplements the traditional
subject-object relationships by providing a third leg to the relationship; the
qualification of how and why the relationship exists. To illustrate, we show a
prototype of an event-based system with a realistic use case for applying DQ to
relevancy analytics of PageRank and Hyperlink-Induced Topic Search (HITS).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04354</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04354</id><created>2015-02-15</created><updated>2015-02-20</updated><authors><author><keyname>Bhattacharyya</keyname><forenames>Arnab</forenames></author><author><keyname>Dey</keyname><forenames>Palash</forenames></author></authors><title>Sample Complexity for Winner Prediction in Elections</title><categories>cs.DS cs.MA</categories><comments>Accepted in AAMAS 2015</comments><acm-class>F.2; I.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Predicting the winner of an election is a favorite problem both for news
media pundits and computational social choice theorists. Since it is often
infeasible to elicit the preferences of all the voters in a typical prediction
scenario, a common algorithm used for winner prediction is to run the election
on a small sample of randomly chosen votes and output the winner as the
prediction. We analyze the performance of this algorithm for many common voting
rules.
  More formally, we introduce the $(\epsilon, \delta)$-winner determination
problem, where given an election on $n$ voters and $m$ candidates in which the
margin of victory is at least $\epsilon n$ votes, the goal is to determine the
winner with probability at least $1-\delta$. The margin of victory of an
election is the smallest number of votes that need to be modified in order to
change the election winner. We show interesting lower and upper bounds on the
number of samples needed to solve the $(\epsilon, \delta)$-winner determination
problem for many common voting rules, including scoring rules, approval,
maximin, Copeland, Bucklin, plurality with runoff, and single transferable
vote. Moreover, the lower and upper bounds match for many common voting rules
in a wide range of practically appealing scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04364</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04364</id><created>2015-02-15</created><authors><author><keyname>Zareh</keyname><forenames>Mehran</forenames></author><author><keyname>Seatzu</keyname><forenames>Carla</forenames></author><author><keyname>Franceschelli</keyname><forenames>Mauro</forenames></author></authors><title>Consensus on the average in arbitrary directed network topologies with
  time-delays</title><categories>math.OC cs.DM cs.RO math.DS</categories><comments>4th IFAC Workshop on Distributed Estimation and Control in Networked
  Systems (NecSys)</comments><doi>10.3182/20130925-2-DE-4044.00022</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this preliminary paper we study the stability property of a consensus on
the average algorithm in arbitrary directed graphs with respect to
communication/sensing time-delays. The proposed algorithm adds a storage
variable to the agents' states so that the information about the average of the
states is preserved despite the algorithm iterations are performed in an
arbitrary strongly connected directed graph. We prove that for any network
topology and choice of design parameters the consensus on the average algorithm
is stable for sufficiently small delays. We provide simulations and numerical
results to estimate the maximum delay allowed by an arbitrary unbalanced
directed network topology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04367</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04367</id><created>2015-02-15</created><authors><author><keyname>Padakandla</keyname><forenames>Arun</forenames></author><author><keyname>Pradhan</keyname><forenames>S. Sandeep</forenames></author></authors><title>Coset codes for communicating over non-additive channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a case for the use of codes possessing algebraic closure
properties - coset codes - in developing coding techniques and characterizing
achievable rate regions for generic multi-terminal channels. In particular, we
consider three diverse communication scenarios - $3-$user interference channel
(many-to-many), $3-$user broadcast channel (one-to-many), and multiple access
with distributed states (many-to-one) - and identify non-additive examples for
which coset codes are analytically proven to yield strictly larger achievable
rate regions than those achievable using iid codes. On the one hand, our
findings motivate the need for multi-terminal information theory to step beyond
iid codes. On the other, it encourages current research of linear code-based
techniques to go beyond particular additive communication channels. Detailed
proofs of our results are available in [1]-[3].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04368</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04368</id><created>2015-02-15</created><authors><author><keyname>Arrighi</keyname><forenames>Pablo</forenames></author><author><keyname>Martiel</keyname><forenames>Simon</forenames></author><author><keyname>Perdrix</keyname><forenames>Simon</forenames></author></authors><title>Reversible Causal Graph Dynamics</title><categories>cs.DM cs.FL</categories><comments>21 pages. arXiv admin note: substantial text overlap with
  arXiv:1212.0027</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Causal Graph Dynamics extend Cellular Automata to arbitrary, bounded-degree,
time-varying graphs. The whole graph evolves in discrete time steps, and this
global evolution is required to have a number of physics-like symmetries:
shift-invariance (it acts everywhere the same) and causality (information has a
bounded speed of propagation). We add a further physics-like symmetry, namely
reversibility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04380</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04380</id><created>2015-02-15</created><authors><author><keyname>Jiang</keyname><forenames>Maosheng</forenames></author><author><keyname>Chen</keyname><forenames>Yonxiang</forenames></author><author><keyname>Chen</keyname><forenames>Ling</forenames></author></authors><title>Link Prediction in Networks with Nodes Attributes by Similarity
  Propagation</title><categories>cs.SI physics.soc-ph</categories><comments>13 pages, 2 tables. arXiv admin note: text overlap with
  arXiv:1112.3265 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of link prediction has attracted considerable recent attention
from various domains such as sociology, anthropology, information science, and
computer sciences. A link prediction algorithm is proposed based on link
similarity score propagation by a random walk in networks with nodes
attributes. In the algorithm, each link in the network is assigned a
transmission probability according to the similarity of the attributes on the
nodes connected by the link. The link similarity score between the nodes are
then propagated via the links according to their transmission probability. Our
experimental results show that it can obtain higher quality results on the
networks with node attributes than other algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04381</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04381</id><created>2015-02-15</created><authors><author><keyname>Lambiotte</keyname><forenames>Renaud</forenames></author><author><keyname>Delvenne</keyname><forenames>Jean-Charles</forenames></author><author><keyname>Barahona</keyname><forenames>Mauricio</forenames></author></authors><title>Random Walks, Markov Processes and the Multiscale Modular Organization
  of Complex Networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>Extended version of arXiv:0812.1770, 'Laplacian Dynamics and
  Multiscale Modular Structure in Networks', by the same authors, with new
  content; published in Transactions on Network Science and Engineering; 16
  pages; 11 figs</comments><journal-ref>IEEE Transactions on Network Science and Engineering (Volume:1 ,
  Issue: 2 ) pp 76-90, 2015</journal-ref><doi>10.1109/TNSE.2015.2391998</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most methods proposed to uncover communities in complex networks rely on
combinatorial graph properties. Usually an edge-counting quality function, such
as modularity, is optimized over all partitions of the graph compared against a
null random graph model. Here we introduce a systematic dynamical framework to
design and analyze a wide variety of quality functions for community detection.
The quality of a partition is measured by its Markov Stability, a
time-parametrized function defined in terms of the statistical properties of a
Markov process taking place on the graph. The Markov process provides a
dynamical sweeping across all scales in the graph, and the time scale is an
intrinsic parameter that uncovers communities at different resolutions.
  This dynamic-based community detection leads to a compound optimization,
which favours communities of comparable centrality (as defined by the
stationary distribution), and provides a unifying framework for spectral
algorithms, as well as different heuristics for community detection, including
versions of modularity and Potts model. Our dynamic framework creates a
systematic link between different stochastic dynamics and their corresponding
notions of optimal communities under distinct (node and edge) centralities. We
show that the Markov Stability can be computed efficiently to find multi-scale
community structure in large networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04382</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04382</id><created>2015-02-15</created><authors><author><keyname>Mertzios</keyname><forenames>George B.</forenames></author><author><keyname>Michail</keyname><forenames>Othon</forenames></author><author><keyname>Spirakis</keyname><forenames>Paul G.</forenames></author></authors><title>Temporal Network Optimization Subject to Connectivity Constraints</title><categories>cs.DM</categories><msc-class>68R10, 68Q17, 68Q25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we consider \emph{temporal networks}, i.e. networks defined by a
\emph{labeling} $\lambda$ assigning to each edge of an \emph{underlying graph}
$G$ a set of \emph{discrete} time-labels. The labels of an edge, which are
natural numbers, indicate the discrete time moments at which the edge is
available. We focus on \emph{path problems} of temporal networks. In
particular, we consider \emph{time-respecting} paths, i.e. paths whose edges
are assigned by $\lambda$ a strictly increasing sequence of labels. We begin by
giving two efficient algorithms for computing shortest time-respecting paths on
a temporal network. We then prove that there is a \emph{natural analogue of
Menger's theorem} holding for arbitrary temporal networks. Finally, we propose
two \emph{cost minimization parameters} for temporal network design. One is the
\emph{temporality} of $G$, in which the goal is to minimize the maximum number
of labels of an edge, and the other is the \emph{temporal cost} of $G$, in
which the goal is to minimize the total number of labels used. Optimization of
these parameters is performed subject to some \emph{connectivity constraint}.
We prove several lower and upper bounds for the temporality and the temporal
cost of some very basic graph families such as rings, directed acyclic graphs,
and trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04383</identifier>
 <datestamp>2015-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04383</id><created>2015-02-15</created><updated>2015-07-23</updated><authors><author><keyname>Ding</keyname><forenames>Changxing</forenames></author><author><keyname>Tao</keyname><forenames>Dacheng</forenames></author></authors><title>A Comprehensive Survey on Pose-Invariant Face Recognition</title><categories>cs.CV</categories><comments>2nd version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The capacity to recognize faces under varied poses is a fundamental human
ability that presents a unique challenge for computer vision systems. Compared
to frontal face recognition, which has been intensively studied and has
gradually matured in the past few decades, pose-invariant face recognition
(PIFR) remains a largely unsolved problem. However, PIFR is crucial to
realizing the full potential of face recognition for real-world applications,
since face recognition is intrinsically a passive biometric technology for
recognizing uncooperative subjects. In this paper, we discuss the inherent
difficulties in PIFR and present a comprehensive review of established
techniques. Existing PIFR methods can be grouped into four categories, i.e.,
pose-robust feature extraction approaches, multi-view subspace learning
approaches, face synthesis approaches, and hybrid approaches. The motivations,
strategies, pros/cons, and performance of representative approaches are
described and compared. Moreover, promising directions for future research are
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04390</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04390</id><created>2015-02-15</created><updated>2015-08-29</updated><authors><author><keyname>Dauphin</keyname><forenames>Yann N.</forenames></author><author><keyname>de Vries</keyname><forenames>Harm</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Equilibrated adaptive learning rates for non-convex optimization</title><categories>cs.LG cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parameter-specific adaptive learning rate methods are computationally
efficient ways to reduce the ill-conditioning problems encountered when
training large deep networks. Following recent work that strongly suggests that
most of the critical points encountered when training such networks are saddle
points, we find how considering the presence of negative eigenvalues of the
Hessian could help us design better suited adaptive learning rate schemes. We
show that the popular Jacobi preconditioner has undesirable behavior in the
presence of both positive and negative curvature, and present theoretical and
empirical evidence that the so-called equilibration preconditioner is
comparatively better suited to non-convex problems. We introduce a novel
adaptive learning rate scheme, called ESGD, based on the equilibration
preconditioner. Our experiments show that ESGD performs as well or better than
RMSProp in terms of convergence speed, always clearly improving over plain
stochastic gradient descent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04395</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04395</id><created>2015-02-15</created><updated>2015-10-06</updated><authors><author><keyname>Tseng</keyname><forenames>Lewis</forenames></author><author><keyname>Benzer</keyname><forenames>Alec</forenames></author><author><keyname>Vaidya</keyname><forenames>Nitin H.</forenames></author></authors><title>Application-Aware Consistency: An Application to Social Network</title><categories>cs.DC cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work weakens well-known consistency models using graphs that capture
applications' characteristics. The weakened models not only respect application
semantic, but also yield a performance benefit. We introduce a notion of
dependency graphs, which specify relations between events that are important
with respect to application semantic, and then weaken traditional consistency
models (e.g., causal consistency) using these graphs. Particularly, we consider
two types of graphs: intra-process and inter-process dependency graphs, where
intra-process dependency graphs specify how events in a single process are
related, and inter-process dependency graphs specify how events across multiple
processes are related. Then, based on these two types of graphs, we define new
consistency model, namely {\em application-aware} consistency. We also discuss
why such application-aware consistency can be useful in social network
applications.
  This is a work in progress, and we present early ideas regarding
application-aware consistency here.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04419</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04419</id><created>2015-02-15</created><authors><author><keyname>Alves</keyname><forenames>Sandra</forenames><affiliation>University of Porto</affiliation></author><author><keyname>Cervesato</keyname><forenames>Iliano</forenames><affiliation>Carnegie Mellon University</affiliation></author></authors><title>Proceedings Third International Workshop on Linearity</title><categories>cs.LO</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 176, 2015</journal-ref><doi>10.4204/EPTCS.176</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the papers presented at LINEARITY 2014, the Third
International Workshop on Linearity, held on July 13, 2014 in Vienna, Austria.
The workshop was a one-day satellite event of FLoC 2014, the sixth Federated
Logic Conference. It was held as part of the 2014 Vienna Summer of Logic.
  The aim of this workshop was to bring together researchers who are exploring
theory and applications of linear calculi, to foster their interaction and
provide a forum for presenting new ideas and work in progress, and enable
newcomers to learn about current activities in this area. Of interest were new
results that made a central use of linearity, ranging from foundational work to
applications in any field. This included: sub-linear logics, linear term
calculi, linear type systems, linear proof-theory, linear programming
languages, applications to concurrency, interaction-based systems, verification
of linear systems, quantum models of computation, and biological and chemical
models of computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04423</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04423</id><created>2015-02-16</created><updated>2015-04-25</updated><authors><author><keyname>Goudarzi</keyname><forenames>Alireza</forenames></author><author><keyname>Shabani</keyname><forenames>Alireza</forenames></author><author><keyname>Stefanovic</keyname><forenames>Darko</forenames></author></authors><title>Exploring Transfer Function Nonlinearity in Echo State Networks</title><categories>cs.NE</categories><comments>arXiv admin note: text overlap with arXiv:1502.00718</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Supralinear and sublinear pre-synaptic and dendritic integration is
considered to be responsible for nonlinear computation power of biological
neurons, emphasizing the role of nonlinear integration as opposed to nonlinear
output thresholding. How, why, and to what degree the transfer function
nonlinearity helps biologically inspired neural network models is not fully
understood. Here, we study these questions in the context of echo state
networks (ESN). ESN is a simple neural network architecture in which a fixed
recurrent network is driven with an input signal, and the output is generated
by a readout layer from the measurements of the network states. ESN
architecture enjoys efficient training and good performance on certain
signal-processing tasks, such as system identification and time series
prediction. ESN performance has been analyzed with respect to the connectivity
pattern in the network structure and the input bias. However, the effects of
the transfer function in the network have not been studied systematically.
Here, we use an approach tanh on the Taylor expansion of a frequently used
transfer function, the hyperbolic tangent function, to systematically study the
effect of increasing nonlinearity of the transfer function on the memory,
nonlinear capacity, and signal processing performance of ESN. Interestingly, we
find that a quadratic approximation is enough to capture the computational
power of ESN with tanh function. The results of this study apply to both
software and hardware implementation of ESN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04428</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04428</id><created>2015-02-16</created><authors><author><keyname>Zhang</keyname><forenames>Zhong-Yuan</forenames></author><author><keyname>Ahn</keyname><forenames>Yong-Yeol</forenames></author></authors><title>Community detection in bipartite networks using weighted symmetric
  binary matrix factorization</title><categories>cs.SI physics.soc-ph</categories><comments>International Journal of Modern Physics C (2014)</comments><doi>10.1142/S0129183115500965</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose weighted symmetric binary matrix factorization
(wSBMF) framework to detect overlapping communities in bipartite networks,
which describe relationships between two types of nodes. Our method improves
performance by recognizing the distinction between two types of missing
edges---ones among the nodes in each node type and the others between two node
types. Our method can also explicitly assign community membership and
distinguish outliers from overlapping nodes, as well as incorporating existing
knowledge on the network. We propose a generalized partition density for
bipartite networks as a quality function, which identifies the most appropriate
number of communities. The experimental results on both synthetic and
real-world networks demonstrate the effectiveness of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04430</identifier>
 <datestamp>2015-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04430</id><created>2015-02-16</created><authors><author><keyname>Chitambar</keyname><forenames>Eric</forenames></author><author><keyname>Fortescue</keyname><forenames>Ben</forenames></author><author><keyname>Hsieh</keyname><forenames>Min-Hsiu</forenames></author></authors><title>Distributions Attaining Secret Key at a Rate of the Conditional Mutual
  Information</title><categories>quant-ph cs.IT math.IT</categories><doi>10.1007/978-3-662-48000-7_22</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the problem of extracting secret key from an
eavesdropped source $p_{XYZ}$ at a rate given by the conditional mutual
information. We investigate this question under three different scenarios: (i)
Alice ($X$) and Bob ($Y$) are unable to communicate but share common randomness
with the eavesdropper Eve ($Z$), (ii) Alice and Bob are allowed one-way public
communication, and (iii) Alice and Bob are allowed two-way public
communication. Distributions having a key rate of the conditional mutual
information are precisely those in which a &quot;helping&quot; Eve offers Alice and Bob
no greater advantage for obtaining secret key than a fully adversarial one. For
each of the above scenarios, strong necessary conditions are derived on the
structure of distributions attaining a secret key rate of $I(X:Y|Z)$. In
obtaining our results, we completely solve the problem of secret key
distillation under scenario (i) and identify $H(S|Z)$ to be the optimal key
rate using shared randomness, where $S$ is the G\'acs-K\&quot;orner Common
Information. We thus provide an operational interpretation of the conditional
G\'acs-K\&quot;orner Common Information. Additionally, we introduce simple example
distributions in which the rate $I(X:Y|Z)$ is achievable if and only if two-way
communication is allowed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04433</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04433</id><created>2015-02-16</created><authors><author><keyname>Chitambar</keyname><forenames>Eric</forenames></author><author><keyname>Fortescue</keyname><forenames>Ben</forenames></author><author><keyname>Hsieh</keyname><forenames>Min-Hsiu</forenames></author></authors><title>A Classical Analog to Entanglement Reversibility</title><categories>quant-ph cs.IT math.IT</categories><journal-ref>Phys. Rev. Lett. 115, 090501 (2015)</journal-ref><doi>10.1103/PhysRevLett.115.090501</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter we introduce the problem of secrecy reversibility. This asks
when two honest parties can distill secret bits from some tripartite
distribution $p_{XYZ}$ and transform secret bits back into $p_{XYZ}$ at equal
rates using local operation and public communication (LOPC). This is the
classical analog to the well-studied problem of reversibly concentrating and
diluting entanglement in a quantum state. We identify the structure of
distributions possessing reversible secrecy when one of the honest parties
holds a binary distribution, and it is possible that all reversible
distributions have this form. These distributions are more general than what is
obtained by simply constructing a classical analog to the family of quantum
states known to have reversible entanglement. An indispensable tool used in our
analysis is a conditional form of the G\'{a}cs-K\&quot;{o}rner Common Information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04434</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04434</id><created>2015-02-16</created><updated>2016-01-14</updated><authors><author><keyname>Demyanov</keyname><forenames>Sergey</forenames></author><author><keyname>Bailey</keyname><forenames>James</forenames></author><author><keyname>Kotagiri</keyname><forenames>Ramamohanarao</forenames></author><author><keyname>Leckie</keyname><forenames>Christopher</forenames></author></authors><title>Invariant backpropagation: how to train a transformation-invariant
  neural network</title><categories>stat.ML cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many classification problems a classifier should be robust to small
variations in the input vector. This is a desired property not only for
particular transformations, such as translation and rotation in image
classification problems, but also for all others for which the change is small
enough to retain the object perceptually indistinguishable. We propose two
extensions of the backpropagation algorithm that train a neural network to be
robust to variations in the feature vector. While the first of them enforces
robustness of the loss function to all variations, the second method trains the
predictions to be robust to a particular variation which changes the loss
function the most. The second methods demonstrates better results, but is
slightly slower. We analytically compare the proposed algorithm with two the
most similar approaches (Tangent BP and Adversarial Training), and propose
their fast versions. In the experimental part we perform comparison of all
algorithms in terms of classification accuracy and robustness to noise on MNIST
and CIFAR-10 datasets. Additionally we analyze how the performance of the
proposed algorithm depends on the dataset size and data augmentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04464</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04464</id><created>2015-02-16</created><updated>2015-06-23</updated><authors><author><keyname>Reynolds</keyname><forenames>Andrew</forenames></author><author><keyname>Deters</keyname><forenames>Morgan</forenames></author><author><keyname>Kuncak</keyname><forenames>Viktor</forenames></author><author><keyname>Tinelli</keyname><forenames>Cesare</forenames></author><author><keyname>Barrett</keyname><forenames>Clark</forenames></author></authors><title>On Counterexample Guided Quantifier Instantiation for Synthesis in CVC4</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the first program synthesis engine implemented inside an SMT
solver. We present an approach that extracts solution functions from
unsatisfiability proofs of the negated form of synthesis conjectures. We also
discuss novel counterexample-guided techniques for quantifier instantiation
that we use to make finding such proofs practically feasible. A particularly
important class of specifications are single-invocation properties, for which
we present a dedicated algorithm. To support syntax restrictions on generated
solutions, our approach can transform a solution found without restrictions
into the desired syntactic form. As an alternative, we show how to use
evaluation function axioms to embed syntactic restrictions into constraints
over algebraic datatypes, and then use an algebraic datatype decision procedure
to drive synthesis. Our experimental evaluation on syntax-guided synthesis
benchmarks shows that our implementation in the CVC4 SMT solver is competitive
with state-of-the-art tools for synthesis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04469</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04469</id><created>2015-02-16</created><updated>2015-03-11</updated><authors><author><keyname>Mei</keyname><forenames>Jian-Ping</forenames></author><author><keyname>Kwoh</keyname><forenames>Chee-Keong</forenames></author><author><keyname>Yang</keyname><forenames>Peng</forenames></author><author><keyname>Li</keyname><forenames>Xiao-Li</forenames></author></authors><title>Classification and its applications for drug-target interaction
  identification</title><categories>cs.LG q-bio.MN q-bio.QM</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Classification is one of the most popular and widely used supervised learning
tasks, which categorizes objects into predefined classes based on known
knowledge. Classification has been an important research topic in machine
learning and data mining. Different classification methods have been proposed
and applied to deal with various real-world problems. Unlike unsupervised
learning such as clustering, a classifier is typically trained with labeled
data before being used to make prediction, and usually achieves higher accuracy
than unsupervised one.
  In this paper, we first define classification and then review several
representative methods. After that, we study in details the application of
classification to a critical problem in drug discovery, i.e., drug-target
prediction, due to the challenges in predicting possible interactions between
drugs and targets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04485</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04485</id><created>2015-02-16</created><updated>2015-02-23</updated><authors><author><keyname>Casagrande</keyname><forenames>Alberto</forenames></author><author><keyname>Jarmolowska</keyname><forenames>Joanna</forenames></author><author><keyname>Turconi</keyname><forenames>Marcello</forenames></author><author><keyname>Fabris</keyname><forenames>Francesco</forenames></author><author><keyname>Busan</keyname><forenames>Pierpaolo</forenames></author><author><keyname>Battaglini</keyname><forenames>Piero Paolo</forenames></author></authors><title>PolyMorph: Increasing P300 Spelling Efficiency by Selection Matrix
  Polymorphism and Sentence-Based Predictions</title><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  P300 is an electric signal emitted by brain about 300 milliseconds after a
rare, but relevant-for-the-user event. One of the applications of this signal
is sentence spelling that enables subjects who lost the control of their motor
pathways to communicate by selecting characters in a matrix containing all the
alphabet symbols. Although this technology has made considerable progress in
the last years, it still suffers from both low communication rate and high
error rate. This article presents a P300 speller, named PolyMorph, that
introduces two major novelties in the field: the selection matrix polymorphism,
that reduces the size of the selection matrix itself by removing useless
symbols, and sentence-based predictions, that exploit all the spelt characters
of a sentence to determine the probability of a word. In order to measure the
effectiveness of the presented speller, we describe two sets of tests: the
first one in vivo and the second one in silico. The results of these
experiments suggest that the use of PolyMorph in place of the naive
character-by-character speller both increases the number of spelt characters
per time unit and reduces the error rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04488</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04488</id><created>2015-02-16</created><updated>2015-02-17</updated><authors><author><keyname>Law</keyname><forenames>Ka Lung</forenames></author><author><keyname>Wen</keyname><forenames>Xin</forenames></author><author><keyname>Vu</keyname><forenames>Minh Thanh</forenames></author><author><keyname>Pesavento</keyname><forenames>Marius</forenames></author></authors><title>General Rank Multiuser Downlink Beamforming With Shaping Constraints
  Using Real-valued OSTBC</title><categories>cs.IT math.IT</categories><doi>10.1109/TSP.2015.2455516</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider optimal multiuser downlink beamforming in the
presence of a massive number of arbitrary quadratic shaping constraints. We
combine beamforming with full-rate high dimensional real-valued orthogonal
space time block coding (OSTBC) to increase the number of beamforming weight
vectors and associated degrees of freedom in the beamformer design. The
original multi-constraint beamforming problem is converted into a convex
optimization problem using semidefinite relaxation (SDR) which can be solved
efficiently. In contrast to conventional (rank-one) beamforming approaches in
which an optimal beamforming solution can be obtained only when the SDR
solution (after rank reduction) exhibits the rank-one property, in our approach
optimality is guaranteed when a rank of eight is not exceeded. We show that our
approach can incorporate up to 79 additional shaping constraints for which an
optimal beamforming solution is guaranteed as compared to a maximum of two
additional constraints that bound the conventional rank-one downlink
beamforming designs. Simulation results demonstrate the flexibility of our
proposed beamformer design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04492</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04492</id><created>2015-02-16</created><authors><author><keyname>Buonanno</keyname><forenames>Amedeo</forenames></author><author><keyname>Palmieri</keyname><forenames>Francesco A. N.</forenames></author></authors><title>Towards Building Deep Networks with Bayesian Factor Graphs</title><categories>cs.CV cs.LG</categories><comments>Submitted for journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a Multi-Layer Network based on the Bayesian framework of the
Factor Graphs in Reduced Normal Form (FGrn) applied to a two-dimensional
lattice. The Latent Variable Model (LVM) is the basic building block of a
quadtree hierarchy built on top of a bottom layer of random variables that
represent pixels of an image, a feature map, or more generally a collection of
spatially distributed discrete variables. The multi-layer architecture
implements a hierarchical data representation that, via belief propagation, can
be used for learning and inference. Typical uses are pattern completion,
correction and classification. The FGrn paradigm provides great flexibility and
modularity and appears as a promising candidate for building deep networks: the
system can be easily extended by introducing new and different (in cardinality
and in type) variables. Prior knowledge, or supervised information, can be
introduced at different scales. The FGrn paradigm provides a handy way for
building all kinds of architectures by interconnecting only three types of
units: Single Input Single Output (SISO) blocks, Sources and Replicators. The
network is designed like a circuit diagram and the belief messages flow
bidirectionally in the whole system. The learning algorithms operate only
locally within each block. The framework is demonstrated in this paper in a
three-layer structure applied to images extracted from a standard data set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04495</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04495</id><created>2015-02-16</created><authors><author><keyname>Patrascu</keyname><forenames>Vasile</forenames></author></authors><title>A Generalization of Gustafson-Kessel Algorithm Using a New Constraint
  Parameter</title><categories>cs.AI</categories><comments>Proceedings of the Joint 4th Conference of the European Society for
  Fuzzy Logic and Technology and the 11th Rencontres Francophones sur la
  Logique Floue et ses Applications, pp. 1250-1255, Barcelona, Spain, September
  7-9, 2005</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper one presents a new fuzzy clustering algorithm based on a
dissimilarity function determined by three parameters. This algorithm can be
considered a generalization of the Gustafson-Kessel algorithm for fuzzy
clustering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04496</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04496</id><created>2015-02-16</created><authors><author><keyname>Brandenburger</keyname><forenames>Marcus</forenames></author><author><keyname>Cachin</keyname><forenames>Christian</forenames></author><author><keyname>Kne&#x17e;evi&#x107;</keyname><forenames>Nikola</forenames></author></authors><title>Don't Trust the Cloud, Verify: Integrity and Consistency for Cloud
  Object Stores</title><categories>cs.DC cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud services have turned remote computation into a commodity and enable
convenient online collaboration. However, they require that clients fully trust
the service provider in terms of confidentiality, integrity, and availability.
Towards reducing this dependency, this paper introduces a protocol for
verification of integrity and consistency for cloud object storage (VICOS),
which enables a group of mutually trusting clients to detect data-integrity and
consistency violations for cloud storage. It aims at services where multiple
clients cooperate on data stored remotely on a potentially misbehaving service.
VICOS enforces the consistency notion of fork-linearizability, supports
wait-free client semantics for most operations, and reduces the computation and
communication overhead compared to previous protocols. VICOS is based in a
generic way on any authenticated data structure. A prototype of VICOS that
works with the key-value store interface of commodity cloud storage has been
implemented, and an evaluation demonstrates its advantage compared to existing
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04499</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04499</id><created>2015-02-16</created><authors><author><keyname>Patrascu</keyname><forenames>Vasile</forenames></author></authors><title>Color Image Enhancement Using the lrgb Coordinates in the Context of
  Support Fuzzification</title><categories>cs.CV</categories><journal-ref>Romanian Academy Journal, Fuzzy systems and A. I., Reports and
  Letters, F.S.A.I., Vol.10, Nos. 1-2, pp. 29-40, 2004</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image enhancement is an important stage in the image-processing domain. The
most known image enhancement method is the histogram equalization. This method
is an automated one, and realizes a simultaneous modification for brightness
and contrast in the case of monochrome images and for brightness, contrast,
saturation and hue in the case of color images. Simple and efficient methods
can be obtained if affine transforms within logarithmic models are used. A very
important thing in the affine transform determination for color images is the
coordinate system that is used for color space representation. Thus, the using
of the RGB coordinates leads to a simultaneous modification of luminosity and
saturation. In this paper using the lrgb perceptual coordinates one can define
affine transforms, which allow a separated modification of luminosity l and
saturation s (saturation being calculated with the component rgb in the
chromatic plane). Better results can be obtained if partitions are defined on
the image support and then the pixels are separately processed in each window
belonging to the defined partition. Classical partitions frequently lead to the
appearance of some discontinuities at the boundaries between these windows. In
order to avoid all these drawbacks the classical partitions may be replaced by
fuzzy partitions. Their elements will be fuzzy windows and in each of them
there will be defined an affine transform induced by parameters using the fuzzy
mean, fuzzy variance and fuzzy saturation computed for the pixels that belong
to the analyzed window. The final image is obtained by summing up in a weight
way the images of every fuzzy window.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04500</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04500</id><created>2015-02-16</created><updated>2015-02-22</updated><authors><author><keyname>Sparavigna</keyname><forenames>Amelia Carolina</forenames></author></authors><title>Bi-Level Image Thresholding obtained by means of Kaniadakis Entropy</title><categories>cs.CV</categories><comments>Kaniadakis Entropy, Image Processing, Image Segmentation, Image
  Thresholding, Texture Transitions, added reference and revised typos This
  paper has been withdrawn by the author due to a crucial sign error in Figures
  3 and 4</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we are proposing the use of Kaniadakis entropy in the bi-level
thresholding of images, in the framework of a maximum entropy principle. We
discuss the role of its entropic index in determining the threshold and in
driving an &quot;image transition&quot;, that is, an abrupt transition in the appearance
of the corresponding bi-level image. Some examples are proposed to illustrate
the method and for comparing it to the approach which is using the Tsallis
entropy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04502</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04502</id><created>2015-02-16</created><authors><author><keyname>Qiu</keyname><forenames>Teng</forenames></author><author><keyname>Li</keyname><forenames>Yongjie</forenames></author></authors><title>Clustering by Descending to the Nearest Neighbor in the Delaunay Graph
  Space</title><categories>stat.ML cs.CV cs.LG</categories><comments>7 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In our previous works, we proposed a physically-inspired rule to organize the
data points into an in-tree (IT) structure, in which some undesired edges are
allowed to occur. By removing those undesired or redundant edges, this IT
structure is divided into several separate parts, each representing one
cluster. In this work, we seek to prevent the undesired edges from arising at
the source. Before using the physically-inspired rule, data points are at first
organized into a proximity graph which restricts each point to select the
optimal directed neighbor just among its neighbors. Consequently, separated
in-trees or clusters automatically arise, without redundant edges requiring to
be removed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04511</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04511</id><created>2015-02-16</created><authors><author><keyname>Feuilloley</keyname><forenames>Laurent</forenames></author><author><keyname>Hirvonen</keyname><forenames>Juho</forenames></author><author><keyname>Suomela</keyname><forenames>Jukka</forenames></author></authors><title>Locally Optimal Load Balancing</title><categories>cs.DC</categories><comments>19 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work studies distributed algorithms for locally optimal load-balancing:
We are given a graph of maximum degree $\Delta$, and each node has up to $L$
units of load. The task is to distribute the load more evenly so that the loads
of adjacent nodes differ by at most $1$.
  If the graph is a path ($\Delta = 2$), it is easy to solve the fractional
version of the problem in $O(L)$ communication rounds, independently of the
number of nodes. We show that this is tight, and we show that it is possible to
solve also the discrete version of the problem in $O(L)$ rounds in paths.
  For the general case ($\Delta &gt; 2$), we show that fractional load balancing
can be solved in $\operatorname{poly}(L,\Delta)$ rounds and discrete load
balancing in $f(L,\Delta)$ rounds for some function $f$, independently of the
number of nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04514</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04514</id><created>2015-02-16</created><authors><author><keyname>S.</keyname><forenames>Neethi K.</forenames></author><author><keyname>Saxena</keyname><forenames>Sanjeev</forenames></author></authors><title>Maximal Independent Sets in Generalised Caterpillar Graphs</title><categories>cs.DS</categories><doi>10.1007/s10878-015-9960-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A caterpillar graph is a tree which on removal of all its pendant vertices
leaves a chordless path. The chordless path is called the backbone of the
graph. The edges from the backbone to the pendant vertices are called the hairs
of the caterpillar graph. Ortiz and Villanueva (C.Ortiz and M.Villanueva,
Discrete Applied Mathematics, 160(3): 259-266, 2012) describe an algorithm,
linear in the size of the output, for finding a family of maximal independent
sets in a caterpillar graph.
  In this paper, we propose an algorithm, again linear in the output size, for
a generalised caterpillar graph, where at each vertex of the backbone, there
can be any number of hairs of length one and at most one hair of length two.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04533</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04533</id><created>2015-02-16</created><authors><author><keyname>Carmi</keyname><forenames>Paz</forenames></author><author><keyname>Chaitman-Yerushalmi</keyname><forenames>Lilach</forenames></author></authors><title>On the Minimum Cost Range Assignment Problem</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of assigning transmission ranges to radio stations
placed arbitrarily in a $d$-dimensional ($d$-D) Euclidean space in order to
achieve a strongly connected communication network with minimum total power
consumption. The power required for transmitting in range $r$ is proportional
to $r^\alpha$, where $\alpha$ is typically between $1$ and $6$, depending on
various environmental factors. While this problem can be solved optimally in
$1$D, in higher dimensions it is known to be $NP$-hard for any $\alpha \geq 1$.
  For the $1$D version of the problem, i.e., radio stations located on a line
and $\alpha \geq 1$, we propose an optimal $O(n^2)$-time algorithm. This
improves the running time of the best known algorithm by a factor of $n$.
Moreover, we show a polynomial-time algorithm for finding the minimum cost
range assignment in $1$D whose induced communication graph is a $t$-spanner,
for any $t \geq 1$.
  In higher dimensions, finding the optimal range assignment is $NP$-hard;
however, it can be approximated within a constant factor. The best known
approximation ratio is for the case $\alpha=1$, where the approximation ratio
is $1.5$. We show a new approximation algorithm with improved approximation
ratio of $1.5-\epsilon$, where $\epsilon&gt;0$ is a small constant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04538</identifier>
 <datestamp>2015-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04538</id><created>2015-02-16</created><updated>2015-07-03</updated><authors><author><keyname>Lee</keyname><forenames>Dongeun</forenames></author><author><keyname>Choi</keyname><forenames>Jaesik</forenames></author></authors><title>Learning Dynamic Compressive Sensing Models</title><categories>cs.IT math.IT</categories><acm-class>E.4; I.5.4; G.1.3; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random sampling in compressive sensing (CS) enables the compression of large
amounts of input signals in an efficient manner, which is useful for many
applications. CS reconstructs the compressed signals exactly with overwhelming
probability when incoming data can be sparsely represented with a fixed number
of components, which is one of the drawbacks of CS frameworks because the
signal sparsity in many dynamic systems changes over time. We present a new CS
framework that handles signals without the fixed sparsity assumption by
incorporating the distribution of signal sparsity. We show that the signal
recovery success in our beta distribution modeling is more accurate than the
success probability analysis in the CS framework. Alternatively, the success or
failure of signal recovery can be relaxed, and the numbers of components
included in signal recoveries can be represented with a probability
distribution. We show this distribution is skewed to the right and naturally
represented by the gamma distribution. Experimental results confirm the
accuracy of our modeling in the context of dynamic signal sparsity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04539</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04539</id><created>2015-02-16</created><authors><author><keyname>Maghsudi</keyname><forenames>Setareh</forenames></author><author><keyname>Stanczak</keyname><forenames>Slawomir</forenames></author></authors><title>Hybrid Centralized-Distributed Resource Allocation for Device-to-Device
  Communication Underlaying Cellular Networks</title><categories>cs.GT cs.IT cs.NI math.IT</categories><comments>35 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The basic idea of device-to-device (D2D) communication is that pairs of
suitably selected wireless devices reuse the cellular spectrum to establish
direct communication links, provided that the adverse effects of D2D
communication on cellular users is minimized and cellular users are given a
higher priority in using limited wireless resources. Despite its great
potential in terms of coverage and capacity performance, implementing this new
concept poses some challenges, in particular with respect to radio resource
management. The main challenges arise from a strong need for distributed D2D
solutions that operate in the absence of precise channel and network knowledge.
In order to address this challenge, this paper studies a resource allocation
problem in a single-cell wireless network with multiple D2D users sharing the
available radio frequency channels with cellular users. We consider a realistic
scenario where the base station (BS) is provided with strictly limited channel
knowledge while D2D and cellular users have no information. We prove a
lower-bound for the cellular aggregate utility in the downlink with fixed BS
power, which allows for decoupling the channel allocation and D2D power control
problems. An efficient graph-theoretical approach is proposed to perform the
channel allocation, which offers flexibility with respect to allocation
criterion (aggregate utility maximization, fairness, quality of service
guarantee). We model the power control problem as a multi-agent learning game.
We show that the game is an exact potential game with noisy rewards, defined on
a discrete strategy set, and characterize the set of Nash equilibria.
Q-learning better-reply dynamics is then used to achieve equilibrium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04544</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04544</id><created>2015-02-16</created><authors><author><keyname>Cao</keyname><forenames>Zhengjun</forenames></author><author><keyname>Liu</keyname><forenames>Lihua</forenames></author></authors><title>A Note On Boneh-Gentry-Waters Broadcast Encryption Scheme and Its Like</title><categories>cs.CR</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Key establishment is any process whereby a shared secret key becomes
available to two or more parties, for subsequent cryptographic use such as
symmetric-key encryption. Though it is widely known that the primitive of
encryption is different from key establishment, we find some researchers have
confused the two primitives. In this note, we shall clarify the fundamental
difference between the two primitives, and point out that the
Boneh-Gentry-Waters broadcast encryption scheme and its like are key
establishment schemes, not encryption schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04545</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04545</id><created>2015-02-16</created><authors><author><keyname>K&#xf6;nig</keyname><forenames>Daniel</forenames></author><author><keyname>Lohrey</keyname><forenames>Markus</forenames></author></authors><title>Parallel Identity Testing for Skew Circuits with Big Powers and
  Applications</title><categories>cs.CC</categories><msc-class>20F10, 68Q17, 68W30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Powerful skew arithmetic circuits are introduced. These are skew arithmetic
circuits with variables, where input gates can be labelled with powers $x^n$
for binary encoded numbers $n$. It is shown that polynomial identity testing
for powerful skew arithmetic circuits belongs to $\mathsf{coRNC}^2$, which
generalizes a corresponding result for (standard) skew circuits. Two
applications of this result are presented: (i) Equivalence of
higher-dimensional straight-line programs can be tested in $\mathsf{coRNC}^2$;
this result is even new in the one-dimensional case, where the straight-line
programs produce strings. (ii) The compressed word problem (or circuit
evaluation problem) for certain wreath products of finitely generated abelian
groups belongs to $\mathsf{coRNC}^2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04548</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04548</id><created>2015-02-16</created><updated>2016-01-05</updated><authors><author><keyname>G&#xf3;mez</keyname><forenames>Vicen&#xe7;</forenames></author><author><keyname>Thijssen</keyname><forenames>Sep</forenames></author><author><keyname>Symington</keyname><forenames>Andrew</forenames></author><author><keyname>Hailes</keyname><forenames>Stephen</forenames></author><author><keyname>Kappen</keyname><forenames>Hilbert J.</forenames></author></authors><title>Real-Time Stochastic Optimal Control for Multi-agent Quadrotor Swarms</title><categories>cs.SY cs.MA cs.RO</categories><comments>17 pages, 8 figures, supplementary video at
  http://www.mbfys.ru.nl/staff/v.gomez/uav.html. submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel method for controlling teams of unmanned aerial
vehicles using Stochastic Optimal Control (SOC) theory. The approach consists
of a centralized high-level controller that computes optimal state trajectories
as velocity sequences, and a platform-specific low-level controller which
ensures that these velocity sequences are met. The high-level control task is
expressed as a centralized path integral control problem, for which optimal
control computation corresponds to a probabilistic inference problem that can
be solved by efficient sampling methods. Through simulation we show that our
SOC approach (a) has significant benefits compared to deterministic control and
other SOC methods in multimodal problems with noise-dependent optimal
solutions, (b) is capable of controlling a large number of platforms in
real-time, and (c) yields collective emergent behavior in the form of flight
formations. Finally, we show that our approach works for real platforms, by
controlling a team of three quadrotors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04551</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04551</id><created>2015-02-16</created><authors><author><keyname>Karpi&#x144;ski</keyname><forenames>Micha&#x142;</forenames></author><author><keyname>Piotr&#xf3;w</keyname><forenames>Marek</forenames></author></authors><title>Smaller Selection Networks for Cardinality Constraints Encoding</title><categories>cs.DS cs.DC</categories><comments>Extended version of the paper sent to CP2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Selection comparator networks have been studied for many years. Recently,
they have been successfully applied to encode cardinality constraints for
SAT-solvers. To decrease the size of generated formula there is a need for
constructions of selection networks that can be efficiently generated and
produce networks of small sizes for the practical range of their two
parameters: n - the number of inputs (boolean variables) and k - the number of
selected items (a cardinality bound). In this paper we give and analyze a new
construction of smaller selection networks that are based on the pairwise
selection networks introduced by Codish and Zanon-Ivry. We prove also that
standard encodings of cardinality constraints with selection networks preserve
arc-consistency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04569</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04569</id><created>2015-02-16</created><updated>2015-04-16</updated><authors><author><keyname>Jas</keyname><forenames>Mainak</forenames></author><author><keyname>Parikh</keyname><forenames>Devi</forenames></author></authors><title>Image Specificity</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For some images, descriptions written by multiple people are consistent with
each other. But for other images, descriptions across people vary considerably.
In other words, some images are specific $-$ they elicit consistent
descriptions from different people $-$ while other images are ambiguous.
Applications involving images and text can benefit from an understanding of
which images are specific and which ones are ambiguous. For instance, consider
text-based image retrieval. If a query description is moderately similar to the
caption (or reference description) of an ambiguous image, that query may be
considered a decent match to the image. But if the image is very specific, a
moderate similarity between the query and the reference description may not be
sufficient to retrieve the image.
  In this paper, we introduce the notion of image specificity. We present two
mechanisms to measure specificity given multiple descriptions of an image: an
automated measure and a measure that relies on human judgement. We analyze
image specificity with respect to image content and properties to better
understand what makes an image specific. We then train models to automatically
predict the specificity of an image from image features alone without requiring
textual descriptions of the image. Finally, we show that modeling image
specificity leads to improvements in a text-based image retrieval application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04578</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04578</id><created>2015-02-16</created><updated>2015-02-17</updated><authors><author><keyname>Boja&#x144;czyk</keyname><forenames>Miko&#x142;aj</forenames></author><author><keyname>Parys</keyname><forenames>Pawe&#x142;</forenames></author><author><keyname>Toru&#x144;czyk</keyname><forenames>Szymon</forenames></author></authors><title>The MSO+U theory of (N, &lt;) is undecidable</title><categories>cs.LO</categories><comments>9 pages, with 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the logic MSO+U, which is monadic second-order logic extended
with the unbounding quantifier. The unbounding quantifier is used to say that a
property of finite sets holds for sets of arbitrarily large size. We prove that
the logic is undecidable on infinite words, i.e. the MSO+U theory of (N,&lt;) is
undecidable. This settles an open problem about the logic, and improves a
previous undecidability result, which used infinite trees and additional axioms
from set theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04579</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04579</id><created>2015-02-16</created><updated>2015-06-09</updated><authors><author><keyname>Akrida</keyname><forenames>Eleni C.</forenames></author><author><keyname>Gasieniec</keyname><forenames>Leszek</forenames></author><author><keyname>Mertzios</keyname><forenames>George B.</forenames></author><author><keyname>Spirakis</keyname><forenames>Paul G.</forenames></author></authors><title>On Temporally Connected Graphs of small cost</title><categories>cs.DM</categories><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the design and verification of temporal graphs that are temporally
connected. We mainly consider undirected graphs of $n$ vertices and follow the
model of \cite{kempe}, where each edge has an associated set of discrete
availability instances (labels). A journey from vertex $u$ to vertex $v$ is a
path from $u$ to $v$ where successive path edges have strictly increasing
labels. A graph is temporally connected iff there is a $(u,v)$-journey for any
pair of vertices $u,v$. We first give a simple polynomial-time algorithm to
check whether a given temporal graph is temporally connected. We then consider
the case in which a designer can \emph{freely choose} availability instances
for all edges and aims for temporal connectivity with a very small \emph{cost};
the cost here is the total number of availability instances used. We achieve
this via a simple polynomial-time procedure which derives designs of cost
linear in $n$, and at most the optimal cost plus $2$. To show this, we prove a
lower bound on the cost for any undirected graph. Next, we consider the case in
which a designer could only choose among a pre-specified set of availability
instances. She comes to us and says &quot;with those availability instances I
deliver a temporally connected graph.&quot; Here, we first have to verify the
correctness of the design. Then our aim is to decrease the cost by removing
some labels without destroying temporal connectivity (redundant labels). Our
main technical result is that computing the maximum number of labels that are
redundant is APX-hard, i.e., there is no PTAS unless $P=NP$. On the other hand,
a temporal design may be &quot;minimal&quot; i.e. all its labels may be needed for
temporal connectivity. We partially characterise minimal temporal designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04585</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04585</id><created>2015-02-16</created><authors><author><keyname>Blum</keyname><forenames>Avrim</forenames></author><author><keyname>Hardt</keyname><forenames>Moritz</forenames></author></authors><title>The Ladder: A Reliable Leaderboard for Machine Learning Competitions</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The organizer of a machine learning competition faces the problem of
maintaining an accurate leaderboard that faithfully represents the quality of
the best submission of each competing team. What makes this estimation problem
particularly challenging is its sequential and adaptive nature. As participants
are allowed to repeatedly evaluate their submissions on the leaderboard, they
may begin to overfit to the holdout data that supports the leaderboard. Few
theoretical results give actionable advice on how to design a reliable
leaderboard. Existing approaches therefore often resort to poorly understood
heuristics such as limiting the bit precision of answers and the rate of
re-submission.
  In this work, we introduce a notion of &quot;leaderboard accuracy&quot; tailored to the
format of a competition. We introduce a natural algorithm called &quot;the Ladder&quot;
and demonstrate that it simultaneously supports strong theoretical guarantees
in a fully adaptive model of estimation, withstands practical adversarial
attacks, and achieves high utility on real submission files from an actual
competition hosted by Kaggle.
  Notably, we are able to sidestep a powerful recent hardness result for
adaptive risk estimation that rules out algorithms such as ours under a
seemingly very similar notion of accuracy. On a practical note, we provide a
completely parameter-free variant of our algorithm that can be deployed in a
real competition with no tuning required whatsoever.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04588</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04588</id><created>2015-02-16</created><updated>2015-04-27</updated><authors><author><keyname>Feldmann</keyname><forenames>Andreas Emil</forenames></author><author><keyname>Fung</keyname><forenames>Wai Shing</forenames></author><author><keyname>K&#xf6;nemann</keyname><forenames>Jochen</forenames></author><author><keyname>Post</keyname><forenames>Ian</forenames></author></authors><title>A $(1 + {\varepsilon})$-Embedding of Low Highway Dimension Graphs into
  Bounded Treewidth Graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graphs with bounded highway dimension were introduced in [Abraham et al.,
SODA 2010] as a model of transportation networks. We show that any such graph
can be embedded into a distribution over bounded treewidth graphs with
arbitrarily small distortion. More concretely, if the highway dimension of $G$
is constant we show how to randomly compute a subgraph of the shortest path
metric of the input graph $G$ with the following two properties: it distorts
the distances of $G$ by a factor of $1+\varepsilon$ in expectation and has a
treewidth that is polylogarithmic in the aspect ratio of $G$. In particular,
this result implies quasi-polynomial time approximation schemes for a number of
optimization problems that naturally arise on transportation networks,
including Travelling Salesman, Steiner Tree, and Facility Location.
  To construct our embedding for low highway dimension graphs we extend
Talwar's [STOC 2004] embedding of low doubling dimension metrics into bounded
treewidth graphs, which generalizes known results for Euclidean metrics. We add
several non-trivial ingredients to Talwar's techniques, and in particular
thoroughly analyze the structure of low highway dimension graphs. Thus we
demonstrate that the geometric toolkit used for Euclidean metrics extends
beyond the class of low doubling metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04593</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04593</id><created>2015-02-16</created><authors><author><keyname>Belahcene</keyname><forenames>K.</forenames></author><author><keyname>Labreuche</keyname><forenames>C.</forenames></author><author><keyname>Maudet</keyname><forenames>N.</forenames></author><author><keyname>Mousseau</keyname><forenames>V.</forenames></author><author><keyname>Ouerdane</keyname><forenames>W.</forenames></author></authors><title>Explaining robust additive utility models by sequences of preference
  swaps</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multicriteria decision analysis aims at supporting a person facing a decision
problem involving conflicting criteria. We consider an additive utility model
which provides robust conclusions based on preferences elicited from the
decision maker. The recommendations based on these robust conclusions are even
more convincing if they are complemented by explanations. We propose a general
scheme, based on sequence of preference swaps, in which explanations can be
computed. We show first that the length of explanations can be unbounded in the
general case. However, in the case of binary reference scales, this length is
bounded and we provide an algorithm to compute the corresponding explanation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04600</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04600</id><created>2015-02-16</created><authors><author><keyname>Chen</keyname><forenames>Bo</forenames></author><author><keyname>Coffman</keyname><forenames>Ed</forenames></author><author><keyname>Dereniowski</keyname><forenames>Dariusz</forenames></author><author><keyname>Kubiak</keyname><forenames>Wieslaw</forenames></author></authors><title>Structural Properties of an Open Problem in Preemptive Scheduling</title><categories>cs.DM cs.DS</categories><msc-class>90B35, 68R05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Structural properties of optimal preemptive schedules have been studied in a
number of recent papers with a primary focus on two structural parameters: the
minimum number of preemptions necessary, and a tight lower bound on `shifts',
i.e., the sizes of intervals bounded by the times created by preemptions, job
starts, or completions. So far only rough bounds for these parameters have been
derived for specific problems. This paper sharpens the bounds on these
structural parameters for a well-known open problem in the theory of preemptive
scheduling: Instances consist of in-trees of $n$ unit-execution-time jobs with
release dates, and the objective is to minimize the total completion time on
two processors. This is among the current, tantalizing `threshold' problems of
scheduling theory: Our literature survey reveals that any significant
generalization leads to an NP-hard problem, but that any significant
simplification leads to tractable problem.
  For the above problem, we show that the number of preemptions necessary for
optimality need not exceed $2n-1$; that the number must be of order
$\Omega(\log n)$ for some instances; and that the minimum shift need not be
less than $2^{-2n+1}$. These bounds are obtained by combinatorial analysis of
optimal schedules rather than by the analysis of polytope corners for
linear-program formulations, an approach to be found in earlier papers. The
bounds immediately follow from a fundamental structural property called
`normality', by which minimal shifts of a job are exponentially decreasing
functions. In particular, the first interval between a preempted job's start
and its preemption is a multiple of 1/2, the second such interval is a multiple
of 1/4, and in general, the $i$-th preemption occurs at a multiple of $2^{-i}$.
We expect the new structural properties to play a prominent role in finally
settling a vexing, still-open question of complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04609</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04609</id><created>2014-11-03</created><authors><author><keyname>Greene</keyname><forenames>Derek</forenames></author><author><keyname>Archambault</keyname><forenames>Daniel</forenames></author><author><keyname>Bel&#xe1;k</keyname><forenames>V&#xe1;clav</forenames></author><author><keyname>Cunningham</keyname><forenames>P&#xe1;draig</forenames></author></authors><title>TextLuas: Tracking and Visualizing Document and Term Clusters in Dynamic
  Text Data</title><categories>cs.IR cs.HC</categories><comments>21 page version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For large volumes of text data collected over time, a key knowledge discovery
task is identifying and tracking clusters. These clusters may correspond to
emerging themes, popular topics, or breaking news stories in a corpus.
Therefore, recently there has been increased interest in the problem of
clustering dynamic data. However, there exists little support for the
interactive exploration of the output of these analysis techniques,
particularly in cases where researchers wish to simultaneously explore both the
change in cluster structure over time and the change in the textual content
associated with clusters. In this paper, we propose a model for tracking
dynamic clusters characterized by the evolutionary events of each cluster.
Motivated by this model, the TextLuas system provides an implementation for
tracking these dynamic clusters and visualizing their evolution using a metro
map metaphor. To provide overviews of cluster content, we adapt the tag cloud
representation to the dynamic clustering scenario. We demonstrate the TextLuas
system on two different text corpora, where they are shown to elucidate the
evolution of key themes. We also describe how TextLuas was applied to a problem
in bibliographic network research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04617</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04617</id><created>2015-02-16</created><authors><author><keyname>Simpson</keyname><forenames>Andrew J. R.</forenames></author></authors><title>Deep Transform: Error Correction via Probabilistic Re-Synthesis</title><categories>cs.LG</categories><msc-class>68Txx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Errors in data are usually unwelcome and so some means to correct them is
useful. However, it is difficult to define, detect or correct errors in an
unsupervised way. Here, we train a deep neural network to re-synthesize its
inputs at its output layer for a given class of data. We then exploit the fact
that this abstract transformation, which we call a deep transform (DT),
inherently rejects information (errors) existing outside of the abstract
feature space. Using the DT to perform probabilistic re-synthesis, we
demonstrate the recovery of data that has been subject to extreme degradation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04622</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04622</id><created>2015-02-16</created><authors><author><keyname>Lakshminarayanan</keyname><forenames>Balaji</forenames></author><author><keyname>Roy</keyname><forenames>Daniel M.</forenames></author><author><keyname>Teh</keyname><forenames>Yee Whye</forenames></author></authors><title>Particle Gibbs for Bayesian Additive Regression Trees</title><categories>stat.ML cs.LG stat.CO</categories><journal-ref>Proceedings of the 18th International Conference on Artificial
  Intelligence and Statistics (AISTATS) 2015, San Diego, CA, USA. JMLR: W&amp;CP
  volume 38</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Additive regression trees are flexible non-parametric models and popular
off-the-shelf tools for real-world non-linear regression. In application
domains, such as bioinformatics, where there is also demand for probabilistic
predictions with measures of uncertainty, the Bayesian additive regression
trees (BART) model, introduced by Chipman et al. (2010), is increasingly
popular. As data sets have grown in size, however, the standard
Metropolis-Hastings algorithms used to perform inference in BART are proving
inadequate. In particular, these Markov chains make local changes to the trees
and suffer from slow mixing when the data are high-dimensional or the best
fitting trees are more than a few layers deep. We present a novel sampler for
BART based on the Particle Gibbs (PG) algorithm (Andrieu et al., 2010) and a
top-down particle filtering algorithm for Bayesian decision trees
(Lakshminarayanan et al., 2013). Rather than making local changes to individual
trees, the PG sampler proposes a complete tree to fit the residual. Experiments
show that the PG sampler outperforms existing samplers in many settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04623</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04623</id><created>2015-02-16</created><updated>2015-05-20</updated><authors><author><keyname>Gregor</keyname><forenames>Karol</forenames></author><author><keyname>Danihelka</keyname><forenames>Ivo</forenames></author><author><keyname>Graves</keyname><forenames>Alex</forenames></author><author><keyname>Rezende</keyname><forenames>Danilo Jimenez</forenames></author><author><keyname>Wierstra</keyname><forenames>Daan</forenames></author></authors><title>DRAW: A Recurrent Neural Network For Image Generation</title><categories>cs.CV cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural
network architecture for image generation. DRAW networks combine a novel
spatial attention mechanism that mimics the foveation of the human eye, with a
sequential variational auto-encoding framework that allows for the iterative
construction of complex images. The system substantially improves on the state
of the art for generative models on MNIST, and, when trained on the Street View
House Numbers dataset, it generates images that cannot be distinguished from
real data with the naked eye.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04625</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04625</id><created>2015-02-16</created><authors><author><keyname>Lohrey</keyname><forenames>Markus</forenames></author><author><keyname>Maneth</keyname><forenames>Sebastian</forenames></author><author><keyname>Peternek</keyname><forenames>Fabian</forenames></author></authors><title>Compressed Tree Canonization</title><categories>cs.DS cs.FL</categories><msc-class>68Q17, 68Q42</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Straight-line (linear) context-free tree (SLT) grammars have been used to
compactly represent ordered trees. It is well known that equivalence of SLT
grammars is decidable in polynomial time. Here we extend this result and show
that isomorphism of unordered trees given as SLT grammars is decidable in
polynomial time. The proof constructs a compressed version of the canonical
form of the tree represented by the input SLT grammar. The result is
generalized to unrooted trees by &quot;re-rooting&quot; the compressed trees in
polynomial time. We further show that bisimulation equivalence of unrooted
unordered trees represented by SLT grammars is decidable in polynomial time.
For non-linear SLT grammars which can have double-exponential compression
ratios, we prove that unordered isomorphism is PSPACE-hard and in EXPTIME. The
same complexity bounds are shown for bisimulation equivalence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04634</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04634</id><created>2015-02-16</created><updated>2015-06-23</updated><authors><author><keyname>Ilik</keyname><forenames>Danko</forenames></author></authors><title>On the exp-log normal form of types</title><categories>cs.LO cs.PL math.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a type pseudo-normal form that any type built from arrows,
products and sums can be isomorphically mapped to and that systematically
minimizes the number of premises of sum type used. Inspired from a
representation of exponential polynomials, the normal form presents an
extension of the notion of disjunctive normal form that handles arrows. We also
show how to apply it for simplifying the axioms of the theory of
$\beta\eta$-equality of terms of the lambda calculus with sums.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04635</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04635</id><created>2015-02-16</created><updated>2015-08-29</updated><authors><author><keyname>Reverdy</keyname><forenames>Paul</forenames></author><author><keyname>Leonard</keyname><forenames>Naomi E.</forenames></author></authors><title>Parameter estimation in softmax decision-making models with linear
  objective functions</title><categories>math.OC cs.LG stat.ML</categories><comments>In press</comments><msc-class>93E10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With an eye towards human-centered automation, we contribute to the
development of a systematic means to infer features of human decision-making
from behavioral data. Motivated by the common use of softmax selection in
models of human decision-making, we study the maximum likelihood parameter
estimation problem for softmax decision-making models with linear objective
functions. We present conditions under which the likelihood function is convex.
These allow us to provide sufficient conditions for convergence of the
resulting maximum likelihood estimator and to construct its asymptotic
distribution. In the case of models with nonlinear objective functions, we show
how the estimator can be applied by linearizing about a nominal parameter
value. We apply the estimator to fit the stochastic UCL (Upper Credible Limit)
model of human decision-making to human subject data. We show statistically
significant differences in behavior across related, but distinct, tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04638</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04638</id><created>2015-02-16</created><authors><author><keyname>Newton</keyname><forenames>Nigel J.</forenames></author></authors><title>Information Geometric Nonlinear Filtering</title><categories>math.PR cs.IT math.IT math.OC</categories><comments>30 pages. To be published in: Infinite Dimensional Analysis, Quantum
  Probability and Related Topics</comments><msc-class>60G35 93E11 (Primary) 60J25 60J60 94A17 (Secondary)</msc-class><journal-ref>Infinite Dimensional Analysis, Quantum Probability and Related
  Topics, 18 (2015), 1550014 (24 pages), World Scientific Publishing Company</journal-ref><doi>10.1142/S0219025715500149</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops information geometric representations for nonlinear
filters in continuous time. The posterior distribution associated with an
abstract nonlinear filtering problem is shown to satisfy a stochastic
differential equation on a Hilbert information manifold. This supports the
Fisher metric as a pseudo-Riemannian metric. Flows of Shannon information are
shown to be connected with the quadratic variation of the process of posterior
distributions in this metric. Apart from providing a suitable setting in which
to study such information-theoretic properties, the Hilbert manifold has an
appropriate topology from the point of view of multi-objective filter
approximations. A general class of finite-dimensional exponential filters is
shown to fit within this framework, and an intrinsic evolution equation,
involving Amari's $-1$-covariant derivative, is developed for such filters.
Three example systems, one of infinite dimension, are developed in detail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04643</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04643</id><created>2015-02-16</created><updated>2015-09-02</updated><authors><author><keyname>Mecklenbr&#xe4;uker</keyname><forenames>Christoph F.</forenames></author><author><keyname>Gerstoft</keyname><forenames>Peter</forenames></author><author><keyname>Z&#xf6;chmann</keyname><forenames>Erich</forenames></author></authors><title>Using the LASSO's Dual for Regularization in Sparse Signal
  Reconstruction from Array Data</title><categories>math.ST cs.IT math.IT stat.TH</categories><comments>submitted to IEEE Transactions on Signal Processing, 09-Aug-2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Waves from a sparse set of source hidden in additive noise are observed by a
sensor array. We treat the estimation of the sparse set of sources as a
generalized complex-valued LASSO problem. The corresponding dual problem is
formulated and it is shown that the dual solution is useful for selecting the
regularization parameter of the LASSO when the number of sources is given. The
solution path of the complex-valued LASSO is analyzed. For a given number of
sources, the corresponding regularization parameter is determined by an
order-recursive algorithm and two iterative algorithms that are based on a
further approximation. Using this regularization parameter, the DOAs of all
sources are estimated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04644</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04644</id><created>2015-02-16</created><updated>2015-04-30</updated><authors><author><keyname>Fischer</keyname><forenames>Johannes</forenames></author><author><keyname>Holub</keyname><forenames>&#x160;t&#x11b;p&#xe1;n</forenames></author><author><keyname>I</keyname><forenames>Tomohiro</forenames></author><author><keyname>Lewenstein</keyname><forenames>Moshe</forenames></author></authors><title>Beyond the Runs Theorem</title><categories>cs.FL cs.DM</categories><comments>New version with substantially improved bound and coauthors who
  carried out a similar research independently</comments><msc-class>68R15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, a short and elegant proof was presented showing that a binary word
of length $n$ contains at most $n-3$ runs. Here we show, using the same
technique and a computer search, that the number of runs in a binary word of
length $n$ is at most $\frac{22}{23}n&lt;0.957n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04645</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04645</id><created>2015-02-16</created><authors><author><keyname>B&#xe9;can</keyname><forenames>Guillaume</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Behjati</keyname><forenames>Razieh</forenames><affiliation>SRL</affiliation></author><author><keyname>Gotlieb</keyname><forenames>Arnaud</forenames><affiliation>SRL</affiliation></author><author><keyname>Acher</keyname><forenames>Mathieu</forenames><affiliation>INRIA - IRISA</affiliation></author></authors><title>Synthesis of Attributed Feature Models From Product Descriptions:
  Foundations</title><categories>cs.SE</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature modeling is a widely used formalism to characterize a set of products
(also called configurations). As a manual elaboration is a long and arduous
task, numerous techniques have been proposed to reverse engineer feature models
from various kinds of artefacts. But none of them synthesize feature attributes
(or constraints over attributes) despite the practical relevance of attributes
for documenting the different values across a range of products. In this
report, we develop an algorithm for synthesizing attributed feature models
given a set of product descriptions. We present sound, complete, and
parametrizable techniques for computing all possible hierarchies, feature
groups, placements of feature attributes, domain values, and constraints. We
perform a complexity analysis w.r.t. number of features, attributes,
configurations, and domain size. We also evaluate the scalability of our
synthesis procedure using randomized configuration matrices. This report is a
first step that aims to describe the foundations for synthesizing attributed
feature models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04649</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04649</id><created>2015-02-16</created><updated>2016-01-27</updated><authors><author><keyname>Quintero</keyname><forenames>Victor</forenames></author><author><keyname>Perlaza</keyname><forenames>Samir M.</forenames></author><author><keyname>Gorce</keyname><forenames>Jean-Marie</forenames></author></authors><title>Noisy Channel-Output Feedback Capacity of the Linear Deterministic
  Interference Channel</title><categories>cs.IT math.IT</categories><comments>5 pages, 9 figures, see proofs in V. Quintero, S. M. Perlaza, and
  J.-M. Gorce, &quot;Noisy channel-output feedback capacity of the linear
  deterministic interference channel,&quot; INRIA, Tech. Rep. 456, Jan. 2015. This
  was submitted and accepted in IEEE ITW 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the capacity region of the two-user linear deterministic (LD)
interference channel with noisy output feedback (IC-NOF) is fully
characterized. This result allows the identification of several asymmetric
scenarios in which imple- menting channel-output feedback in only one of the
transmitter- receiver pairs is as beneficial as implementing it in both links,
in terms of achievable individual rate and sum-rate improvements w.r.t. the
case without feedback. In other scenarios, the use of channel-output feedback
in any of the transmitter-receiver pairs benefits only one of the two pairs in
terms of achievable individual rate improvements or simply, it turns out to be
useless, i.e., the capacity regions with and without feedback turn out to be
identical even in the full absence of noise in the feedback links.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04650</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04650</id><created>2015-02-16</created><updated>2015-02-19</updated><authors><author><keyname>Chen</keyname><forenames>Weimin</forenames></author></authors><title>Lower Bound for General Circuits Computing Clique Function</title><categories>cs.CC</categories><comments>This paper has been withdrawn by the author due to a crucial error in
  Section 3</comments><msc-class>68Q17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove an exponential lower bound for general circuits computing the clique
function and hereby confirm that NP != P.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04652</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04652</id><created>2015-02-16</created><authors><author><keyname>Gupta</keyname><forenames>Saurabh</forenames></author><author><keyname>Arbel&#xe1;ez</keyname><forenames>Pablo</forenames></author><author><keyname>Girshick</keyname><forenames>Ross</forenames></author><author><keyname>Malik</keyname><forenames>Jitendra</forenames></author></authors><title>Inferring 3D Object Pose in RGB-D Images</title><categories>cs.CV</categories><comments>13 pages, 8 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this work is to replace objects in an RGB-D scene with
corresponding 3D models from a library. We approach this problem by first
detecting and segmenting object instances in the scene using the approach from
Gupta et al. [13]. We use a convolutional neural network (CNN) to predict the
pose of the object. This CNN is trained using pixel normals in images
containing rendered synthetic objects. When tested on real data, it outperforms
alternative algorithms trained on real data. We then use this coarse pose
estimate along with the inferred pixel support to align a small number of
prototypical models to the data, and place the model that fits the best into
the scene. We observe a 48% relative improvement in performance at the task of
3D detection over the current state-of-the-art [33], while being an order of
magnitude faster at the same time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04653</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04653</id><created>2015-02-16</created><authors><author><keyname>Penelle</keyname><forenames>Vincent</forenames></author></authors><title>Rewriting Higher-Order Stack Trees</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Higher-order pushdown systems and ground tree rewriting systems can be seen
as extensions of suffix word rewriting systems. Both classes generate infinite
graphs with interesting logical properties. Indeed, the model-checking problem
for monadic second order logic (respectively first order logic with a
reachability predicate) is decidable on such graphs. We unify both models by
introducing the notion of stack trees, trees whose nodes are labelled by
higher-order stacks, and define the corresponding class of higher-order ground
tree rewriting systems. We show that these graphs retain the decidability
properties of ground tree rewriting graphs while generalising the pushdown
hierarchy of graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04656</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04656</id><created>2015-02-16</created><authors><author><keyname>Vinzant</keyname><forenames>Cynthia</forenames></author></authors><title>A small frame and a certificate of its injectivity</title><categories>math.FA cs.IT math.AG math.IT</categories><comments>4 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a complex frame of eleven vectors in 4-space and prove that it
defines injective measurements. That is, any rank-one $4\times 4$ Hermitian
matrix is uniquely determined by its values as a Hermitian form on this
collection of eleven vectors. This disproves a recent conjecture of Bandeira,
Cahill, Mixon, and Nelson. We use algebraic computations and certificates in
order to prove injectivity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04658</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04658</id><created>2015-02-16</created><authors><author><keyname>Qi</keyname><forenames>Xianbiao</forenames></author><author><keyname>Zhao</keyname><forenames>Guoying</forenames></author><author><keyname>Li</keyname><forenames>Chun-Guang</forenames></author><author><keyname>Guo</keyname><forenames>Jun</forenames></author><author><keyname>Pietik&#xe4;inen</keyname><forenames>Matti</forenames></author></authors><title>HEp-2 Cell Classification via Fusing Texture and Shape Information</title><categories>cs.CV</categories><comments>11 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Indirect Immunofluorescence (IIF) HEp-2 cell image is an effective evidence
for diagnosis of autoimmune diseases. Recently computer-aided diagnosis of
autoimmune diseases by IIF HEp-2 cell classification has attracted great
attention. However the HEp-2 cell classification task is quite challenging due
to large intra-class variation and small between-class variation. In this paper
we propose an effective and efficient approach for the automatic classification
of IIF HEp-2 cell image by fusing multi-resolution texture information and
richer shape information. To be specific, we propose to: a) capture the
multi-resolution texture information by a novel Pairwise Rotation Invariant
Co-occurrence of Local Gabor Binary Pattern (PRICoLGBP) descriptor, b) depict
the richer shape information by using an Improved Fisher Vector (IFV) model
with RootSIFT features which are sampled from large image patches in multiple
scales, and c) combine them properly. We evaluate systematically the proposed
approach on the IEEE International Conference on Pattern Recognition (ICPR)
2012, IEEE International Conference on Image Processing (ICIP) 2013 and ICPR
2014 contest data sets. The experimental results for the proposed methods
significantly outperform the winners of ICPR 2012 and ICIP 2013 contest, and
achieve comparable performance with the winner of the newly released ICPR 2014
contest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04661</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04661</id><created>2015-02-16</created><authors><author><keyname>Dames</keyname><forenames>Philip</forenames></author><author><keyname>Kumar</keyname><forenames>Vijay</forenames></author></authors><title>Experimental Characterization of a Bearing-only Sensor for Use With the
  PHD Filter</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report outlines the procedure and results of an experiment to
characterize a bearing-only sensor for use with PHD filter. The resulting
detection, measurement, and clutter models are used for hardware and simulated
experiments with a team of mobile robots autonomously seeking an unknown number
of objects of interest in an office environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04662</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04662</id><created>2015-02-16</created><updated>2015-06-08</updated><authors><author><keyname>Althoff</keyname><forenames>Tim</forenames></author><author><keyname>Dong</keyname><forenames>Xin Luna</forenames></author><author><keyname>Murphy</keyname><forenames>Kevin</forenames></author><author><keyname>Alai</keyname><forenames>Safa</forenames></author><author><keyname>Dang</keyname><forenames>Van</forenames></author><author><keyname>Zhang</keyname><forenames>Wei</forenames></author></authors><title>TimeMachine: Timeline Generation for Knowledge-Base Entities</title><categories>cs.DB cs.IR</categories><comments>To appear at ACM SIGKDD KDD'15. 12pp, 7 fig. With appendix. Demo and
  other info available at http://cs.stanford.edu/~althoff/timemachine/</comments><acm-class>H.2.8</acm-class><doi>10.1145/2783258.2783325</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method called TIMEMACHINE to generate a timeline of events and
relations for entities in a knowledge base. For example for an actor, such a
timeline should show the most important professional and personal milestones
and relationships such as works, awards, collaborations, and family
relationships. We develop three orthogonal timeline quality criteria that an
ideal timeline should satisfy: (1) it shows events that are relevant to the
entity; (2) it shows events that are temporally diverse, so they distribute
along the time axis, avoiding visual crowding and allowing for easy user
interaction, such as zooming in and out; and (3) it shows events that are
content diverse, so they contain many different types of events (e.g., for an
actor, it should show movies and marriages and awards, not just movies). We
present an algorithm to generate such timelines for a given time period and
screen size, based on submodular optimization and web-co-occurrence statistics
with provable performance guarantees. A series of user studies using Mechanical
Turk shows that all three quality criteria are crucial to produce quality
timelines and that our algorithm significantly outperforms various baseline and
state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04665</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04665</id><created>2015-02-16</created><updated>2015-05-04</updated><authors><author><keyname>Stawowy</keyname><forenames>Michele</forenames></author></authors><title>Optimizations for Decision Making and Planning in Description Logic
  Dynamic Knowledge Bases</title><categories>cs.AI</categories><comments>16 pages, extended version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Artifact-centric models for business processes recently raised a lot of
attention, as they manage to combine structural (i.e. data related) with
dynamical (i.e. process related) aspects in a seamless way. Many frameworks
developed under this approach, although, are not built explicitly for planning,
one of the most prominent operations related to business processes. In this
paper, we try to overcome this by proposing a framework named Dynamic Knowledge
Bases, aimed at describing rich business domains through Description
Logic-based ontologies, and where a set of actions allows the system to evolve
by modifying such ontologies. This framework, by offering action rewriting and
knowledge partialization, represents a viable and formal environment to develop
decision making and planning techniques for DL-based artifact-centric business
domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04666</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04666</id><created>2015-02-16</created><updated>2015-02-23</updated><authors><author><keyname>Zhang</keyname><forenames>Cong</forenames></author><author><keyname>Liu</keyname><forenames>Jiangchuan</forenames></author></authors><title>On Crowdsourced Interactive Live Streaming: A Twitch.TV-Based
  Measurement Study</title><categories>cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Empowered by today's rich tools for media generation and collaborative
production, the multimedia service paradigm is shifting from the conventional
single source, to multi-source, to many sources, and now toward {\em
crowdsource}. Such crowdsourced live streaming platforms as Twitch.tv allow
general users to broadcast their content to massive viewers, thereby greatly
expanding the content and user bases. The resources available for these
non-professional broadcasters however are limited and unstable, which
potentially impair the streaming quality and viewers' experience. The diverse
live interactions among the broadcasters and viewers can further aggravate the
problem.
  In this paper, we present an initial investigation on the modern crowdsourced
live streaming systems. Taking Twitch as a representative, we outline their
inside architecture using both crawled data and captured traffic of local
broadcasters/viewers. Closely examining the access data collected in a
two-month period, we reveal that the view patterns are determined by both
events and broadcasters' sources. Our measurements explore the unique source-
and event-driven views, showing that the current delay strategy on the viewer's
side substantially impacts the viewers' interactive experience, and there is
significant disparity between the long broadcast latency and the short live
messaging latency. On the broadcaster's side, the dynamic uploading capacity is
a critical challenge, which noticeably affects the smoothness of live streaming
for viewers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04670</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04670</id><created>2015-02-16</created><authors><author><keyname>de Souza</keyname><forenames>R. M. Campello</forenames></author><author><keyname>de Oliveira</keyname><forenames>H. M.</forenames></author><author><keyname>Kauffman</keyname><forenames>A. N.</forenames></author></authors><title>The Hartley Transform in a Finite Field</title><categories>math.NT cs.DM</categories><comments>7 pages, IEEE/SBT International Telecommunication Symposium, ITS,
  1998, Sao Paulo, Brazil</comments><journal-ref>Journal of Communication and Information Systems, vol.14, N.1,
  1999</journal-ref><doi>10.14209/jcis.1999.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The k-trigonometric functions over the Galois Field GF(q) are introduced and
their main properties derived. This leads to the definition of the cask(.)
function over GF(q), which in turn leads to a finite field Hartley Transform.
The main properties of this new discrete transform are presented and areas for
possible applications are mentioned.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04676</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04676</id><created>2015-02-16</created><updated>2015-04-25</updated><authors><author><keyname>Garnaev</keyname><forenames>Andrey</forenames></author><author><keyname>Trappe</keyname><forenames>Wade</forenames></author></authors><title>Optimal Scanning Bandwidth Strategy Incorporating Uncertainty about
  Adversary's Characteristics</title><categories>cs.GT</categories><comments>This is the last draft version of the paper. Revised version of the
  paper was published in EAI Endorsed Transactions on Mobile Communications and
  Applications, Vol. 14, Issue 5, 2014, doi=10.4108/mca.2.5.e6. arXiv admin
  note: substantial text overlap with arXiv:1310.7247</comments><msc-class>91A05</msc-class><journal-ref>EAI Endorsed Transactions on Mobile Communications and
  Applications, volume 14, number 5, 2014</journal-ref><doi>10.4108/mca.2.5.e6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate the problem of designing a spectrum scanning
strategy to detect an intelligent Invader who wants to utilize spectrum
undetected for his/her unapproved purposes. To deal with this problem we model
the situation as two games, between a Scanner and an Invader, and solve them
sequentially. The first game is formulated to design the optimal (in maxmin
sense) scanning algorithm, while the second one allows one to find the optimal
values of the parameters for the algorithm depending on parameters of the
network. These games provide solutions for two dilemmas that the rivals face.
The Invader's dilemma consists of the following: the more bandwidth the Invader
attempts to use leads to a larger payoff if he is not detected, but at the same
time also increases the probability of being detected and thus fined.
Similarly, the Scanner faces a dilemma: the wider the bandwidth scanned, the
higher the probability of detecting the Invader, but at the expense of
increasing the cost of building the scanning system. The equilibrium strategies
are found explicitly and reveal interesting properties. In particular, we have
found a discontinuous dependence of the equilibrium strategies on the network
parameters, fine and the type of the Invader's award. This discontinuity of the
fine means that the network provider has to take into account a human/social
factor since some threshold values of fine could be very sensible for the
Invader, while in other situations simply increasing the fine has minimal
deterrence impact. Also we show how incomplete information about the Invader's
technical characteristics and reward (e.g. motivated by using different type of
application, say, video-streaming or downloading files) can be incorporated
into scanning strategy to increase its efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04681</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04681</id><created>2015-02-16</created><updated>2016-01-03</updated><authors><author><keyname>Srivastava</keyname><forenames>Nitish</forenames></author><author><keyname>Mansimov</keyname><forenames>Elman</forenames></author><author><keyname>Salakhutdinov</keyname><forenames>Ruslan</forenames></author></authors><title>Unsupervised Learning of Video Representations using LSTMs</title><categories>cs.LG cs.CV cs.NE</categories><comments>Added link to code on github</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use multilayer Long Short Term Memory (LSTM) networks to learn
representations of video sequences. Our model uses an encoder LSTM to map an
input sequence into a fixed length representation. This representation is
decoded using single or multiple decoder LSTMs to perform different tasks, such
as reconstructing the input sequence, or predicting the future sequence. We
experiment with two kinds of input sequences - patches of image pixels and
high-level representations (&quot;percepts&quot;) of video frames extracted using a
pretrained convolutional net. We explore different design choices such as
whether the decoder LSTMs should condition on the generated output. We analyze
the outputs of the model qualitatively to see how well the model can
extrapolate the learned video representation into the future and into the past.
We try to visualize and interpret the learned features. We stress test the
model by running it on longer time scales and on out-of-domain data. We further
evaluate the representations by finetuning them for a supervised learning
problem - human action recognition on the UCF-101 and HMDB-51 datasets. We show
that the representations help improve classification accuracy, especially when
there are only a few training examples. Even models pretrained on unrelated
datasets (300 hours of YouTube videos) can help action recognition performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04687</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04687</id><created>2015-02-16</created><updated>2015-07-12</updated><authors><author><keyname>Chakraborty</keyname><forenames>Promita</forenames></author></authors><title>Physical Biomodeling: a new field enabled by 3-D printing in biomodeling</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate physical modeling with 3D-printing techniques could lead to new
approaches to study structure and dynamics of biological systems complementing
computational methods. Computational biology has become an important part of
research over the last couple of decades. Now 3D printing technology opens the
door for a new field, Physical Biomodeling, at the intersection of experimental
data, computational biology and physical modeling for study of biological
systems, such as protein folding at nano-scale. Here I explore this new domain
of precision physical modeling and correlate it with existing visualization and
computational systems and future possibilities. Dynamic physical models can be
designed to-scale that can serve as research tools in future along with
existing biocomputational tools and databases, adding a third angle to tackle
unsolved scientific problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04689</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04689</id><created>2015-02-16</created><updated>2015-02-27</updated><authors><author><keyname>Zhang</keyname><forenames>Zemin</forenames></author><author><keyname>Aeron</keyname><forenames>Shuchin</forenames></author></authors><title>Exact tensor completion using t-SVD</title><categories>cs.LG cs.NA stat.ML</categories><comments>16 pages, 5 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we focus on the problem of completion of multidimensional
arrays (also referred to as tensors) from limited sampling. Our approach is
based on a recently proposed tensor-Singular Value Decomposition (t-SVD) [1].
Using this factorization one can derive notion of tensor rank, referred to as
the tensor tubal rank, which has optimality properties similar to that of
matrix rank derived from SVD. As shown in [2] some multidimensional data, such
as panning video sequences exhibit low tensor tubal rank and we look at the
problem of completing such data under random sampling of the data cube. We show
that by solving a convex optimization problem, which minimizes the tensor
nuclear norm obtained as the convex relaxation of tensor tubal rank, one can
guarantee recovery with overwhelming probability as long as samples in
proportion to the degrees of freedom in t-SVD are observed. In this sense our
results are order-wise optimal. The conditions under which this result holds
are very similar to the incoherency conditions for the matrix completion,
albeit we define incoherency under the algebraic set-up of t-SVD. We show the
performance of the algorithm on some real data sets and compare it with other
existing approaches based on tensor flattening and Tucker decomposition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04696</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04696</id><created>2015-02-15</created><authors><author><keyname>Bryant</keyname><forenames>Jason</forenames></author><author><keyname>Hasseler</keyname><forenames>Gregory</forenames></author><author><keyname>Lebo</keyname><forenames>Timothy</forenames></author><author><keyname>Paulini</keyname><forenames>Matthew</forenames></author></authors><title>Enhancing Information Awareness Through Directed Qualification of
  Semantic Relevancy Scoring Operations</title><categories>cs.IR</categories><comments>Proceedings of the 19th International Command and Control Research
  and Technology Symposium. arXiv admin note: substantial text overlap with
  arXiv:1502.04348</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Successfully managing analytics-based semantic relationships and their
provenance enables determinations of document importance and priority,
furthering capabilities for machine-based relevancy scoring operations.
Semantic technologies are well suited for modeling explicit and fully qualified
relationships but struggle with modeling relationships that are qualified in
nature, or resultant from applied analytics. Our work seeks to implement the
autonomous Directed Qualification of analytic-based relationships by pairing
the Prov-O Ontology (W3C Recommendation) with a relevancy ontology supporting
analytics terminology. This work results in the capability for any semantically
referenced document, concept, or named graph to be associated with the results
of applied analytics as Direct Qualification (DQ) modeled relational nodes.
This new capability will enable role, identity, or any other content-based
measures of relevancy and analytics-based metrics for semantically described
documents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04697</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04697</id><created>2015-02-16</created><updated>2015-02-23</updated><authors><author><keyname>Hamon</keyname><forenames>Ronan</forenames></author><author><keyname>Borgnat</keyname><forenames>Pierre</forenames></author><author><keyname>Flandrin</keyname><forenames>Patrick</forenames></author><author><keyname>Robardet</keyname><forenames>C&#xe9;line</forenames></author></authors><title>From graphs to signals and back: Identification of network structures
  using spectral analysis</title><categories>physics.data-an cs.DM cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many systems comprising entities in interactions can be represented as
graphs, whose structure gives significant insights about how these systems
work. Network theory has undergone further developments, in particular in
relation to detection of communities in graphs, to catch this structure.
Recently, an approach has been proposed to transform a graph into a collection
of signals: Using a multidimensional scaling technique on a distance matrix
representing relations between vertices of the graph, points in a Euclidean
space are obtained and interpreted as signals, indexed by the vertices. In this
article, we propose several extensions to this approach, developing a framework
to study graph structures using signal processing tools. We first extend the
current methodology, enabling us to highlight connections between properties of
signals and graph structures, such as communities, regularity or randomness, as
well as combinations of those. A robust inverse transformation method is next
described, taking into account possible changes in the signals compared to
original ones. This technique uses, in addition to the relationships between
the points in the Euclidean space, the energy of each signal, coding the
different scales of the graph structure. These contributions open up new
perspectives in the study of graphs, by enabling processing of graphs through
the processing of the corresponding collection of signals, using reliable tools
from signal processing. A technique of denoising of a graph by filtering of the
corresponding signals is then described, suggesting considerable potential of
the approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04700</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04700</id><created>2015-02-16</created><updated>2015-10-01</updated><authors><author><keyname>Potirniche</keyname><forenames>Ionut-Dragos</forenames></author><author><keyname>Laumann</keyname><forenames>C. R.</forenames></author><author><keyname>Sondhi</keyname><forenames>S. L.</forenames></author></authors><title>Classical-Quantum Mixing in the Random 2-Satisfiability Problem</title><categories>quant-ph cond-mat.stat-mech cs.CC</categories><comments>Updated references</comments><journal-ref>Phys. Rev. A 92, 040301 (2015)</journal-ref><doi>10.1103/PhysRevA.92.040301</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classical satisfiability (SAT) and quantum satisfiability (QSAT) are complete
problems for the complexity classes NP and QMA which are believed to be
intractable for classical and quantum computers, respectively. Statistical
ensembles of instances of these problems have been studied previously in an
attempt to elucidate their typical, as opposed to worst case, behavior. In this
paper we introduce a new statistical ensemble that interpolates between
classical and quantum. For the simplest 2-SAT/2-QSAT ensemble we find the exact
boundary that separates SAT and UNSAT instances. We do so by establishing
coincident lower and upper bounds, in the limit of large instances, on the
extent of the UNSAT and SAT regions, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04726</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04726</id><created>2015-02-16</created><authors><author><keyname>Mousavi</keyname><forenames>Hojjat S.</forenames></author><author><keyname>Monga</keyname><forenames>Vishal</forenames></author><author><keyname>Tran</keyname><forenames>Trac D.</forenames></author></authors><title>ICR: Iterative Convex Refinement for Sparse Signal Recovery Using Spike
  and Slab Priors</title><categories>stat.ML cs.CV math.OC</categories><comments>Submitted to IEEE Signal Processing Letters, Feb 2015</comments><doi>10.1109/LSP.2015.2438255</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we address sparse signal recovery using spike and slab
priors. In particular, we focus on a Bayesian framework where sparsity is
enforced on reconstruction coefficients via probabilistic priors. The
optimization resulting from spike and slab prior maximization is known to be a
hard non-convex problem, and existing solutions involve simplifying assumptions
and/or relaxations. We propose an approach called Iterative Convex Refinement
(ICR) that aims to solve the aforementioned optimization problem directly
allowing for greater generality in the sparse structure. Essentially, ICR
solves a sequence of convex optimization problems such that sequence of
solutions converges to a sub-optimal solution of the original hard optimization
problem. We propose two versions of our algorithm: a.) an unconstrained
version, and b.) with a non-negativity constraint on sparse coefficients, which
may be required in some real-world problems. Experimental validation is
performed on both synthetic data and for a real-world image recovery problem,
which illustrates merits of ICR over state of the art alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04727</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04727</id><created>2015-02-16</created><authors><author><keyname>Mou</keyname><forenames>Xiaolin</forenames></author><author><keyname>Sun</keyname><forenames>Hongjian</forenames></author></authors><title>Wireless Power Transfer: Survey and Roadmap</title><categories>cs.IT math.IT</categories><comments>To appear in Proceedings of IEEE VTC 2015 Spring, First International
  Workshop on Integrating Communications, Control, Computing Technologies for
  Smart Grid (ICT4SG)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless power transfer (WPT) technologies have been widely used in many
areas, e.g., the charging of electric toothbrush, mobile phones, and electric
vehicles. This paper introduces fundamental principles of three WPT
technologies, i.e., inductive coupling-based WPT, magnetic resonant
coupling-based WPT, and electromagnetic radiation-based WPT, together with
discussions of their strengths and weaknesses. Main research themes are then
presented, i.e., improving the transmission efficiency and distance, and
designing multiple transmitters/receivers. The state-of-the-art techniques are
reviewed and categorised. Several WPT applications are described. Open research
challenges are then presented with a brief discussion of potential roadmap.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04744</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04744</id><created>2015-02-16</created><authors><author><keyname>Budhiraja</keyname><forenames>Pulkit</forenames></author><author><keyname>Sodhi</keyname><forenames>Rajinder</forenames></author><author><keyname>Jones</keyname><forenames>Brett</forenames></author><author><keyname>Karsch</keyname><forenames>Kevin</forenames></author><author><keyname>Bailey</keyname><forenames>Brian</forenames></author><author><keyname>Forsyth</keyname><forenames>David</forenames></author></authors><title>Where's My Drink? Enabling Peripheral Real World Interactions While
  Using HMDs</title><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Head Mounted Displays (HMDs) allow users to experience virtual reality with a
great level of immersion. However, even simple physical tasks like drinking a
beverage can be difficult and awkward while in a virtual reality experience. We
explore mixed reality renderings that selectively incorporate the physical
world into the virtual world for interactions with physical objects. We
conducted a user study comparing four rendering techniques that balances
immersion in a virtual world with ease of interaction with the physical world.
Finally, we discuss the pros and cons of each approach, suggesting guidelines
for future rendering techniques that bring physical objects into virtual
reality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04748</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04748</id><created>2015-02-16</created><updated>2015-03-12</updated><authors><author><keyname>Marinov</keyname><forenames>Martin</forenames></author><author><keyname>Gregg</keyname><forenames>David</forenames></author></authors><title>The Takeoff Towards Optimal Sorting Networks</title><categories>cs.DS</categories><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A complete set of filters $F_n$ for the optimal-depth $n$-input sorting
network problem is such that if there exists an $n$-input sorting network of
depth $d$ then there exists one of the form $C \oplus C'$ for some $C \in F_n$.
Previous work on the topic presents a method for finding complete set of
filters $R_{n, 1}$ and $R_{n, 2}$ that consists only of networks of depths one
and two respectively, whose outputs are minimal and representative up to
permutation and reflection. Our main contribution is a practical approach for
finding a complete set of filters $R_{n, 3}$ containing only networks of depth
three whose outputs are minimal and representative up to permutation and
reflection. In previous work, we have developed a highly efficient algorithm
for finding extremal sets ( i.e. outputs of comparator networks; itemsets; ) up
to permutation. In this paper we present a modification to this algorithm that
identifies the representative itemsets up to permutation and reflection. Hence,
the presented practical approach is the successful combination of known theory
and practice that we apply to the domain of sorting networks. For $n &lt; 17$, we
empirically compute the complete set of filters $R_{n, 2}$, $R_{n, 3}$, $R_{n,
2} \upharpoonright w $ and $R_{n, 3}^w$ of the representative minimal up to
permutation and reflection $n$-input networks, where all but $R_{n, 2}$ are
novel to this work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04749</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04749</id><created>2015-02-16</created><authors><author><keyname>Wilson</keyname><forenames>S. C.</forenames></author><author><keyname>Slaybaugh</keyname><forenames>R. N.</forenames></author></authors><title>Improved Monte Carlo Variance Reduction for Space and Energy
  Self-Shielding</title><categories>cs.NA</categories><comments>20 pages, 22 figures, 7 tables</comments><journal-ref>Nuclear Science and Engineering, 179, 22--41 (2015)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Continued demand for accurate and computationally efficient transport methods
to solve optically thick, fixed-source transport problems has inspired research
on variance-reduction (VR) techniques for Monte Carlo (MC). Methods that use
deterministic results to create VR maps for MC constitute a dominant branch of
this research, with Forward Weighted-Consistent Adjoint Driven Importance
Sampling (FW-CADIS) being a particularly successful example. However, locations
in which energy and spatial self-shielding are combined, such as thin plates
embedded in concrete, challenge FW-CADIS. In these cases the deterministic flux
cannot appropriately capture transport behavior, and the associated VR
parameters result in high variance in and following the plate.
  This work presents a new method that improves performance in transport
calculations that contain regions of combined space and energy self-shielding
without significant impact on the solution quality in other parts of the
problem. This method is based on FW-CADIS and applies a Resonance Factor
correction to the adjoint source. The impact of the Resonance Factor method is
investigated in this work through an example problem. It is clear that this new
method dramatically improves performance in terms of lowering the maximum 95%
confidence interval relative error and reducing the compute time. Based on this
work, we recommend that the Resonance Factor method be used when the accuracy
of the solution in the presence of combined space and energy self-shielding is
important.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04754</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04754</id><created>2015-02-16</created><updated>2015-07-20</updated><authors><author><keyname>Rubino</keyname><forenames>Cosimo</forenames></author><author><keyname>Crocco</keyname><forenames>Marco</forenames></author><author><keyname>Perina</keyname><forenames>Alessandro</forenames></author><author><keyname>Murino</keyname><forenames>Vittorio</forenames></author><author><keyname>Del Bue</keyname><forenames>Alessio</forenames></author></authors><title>3D Pose from Detections</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel method to infer, in closed-form, a general 3D spatial
occupancy and orientation of a collection of rigid objects given 2D image
detections from a sequence of images. In particular, starting from 2D ellipses
fitted to bounding boxes, this novel multi-view problem can be reformulated as
the estimation of a quadric (ellipsoid) in 3D. We show that an efficient
solution exists in the dual-space using a minimum of three views while a
solution with two views is possible through the use of regularization. However,
this algebraic solution can be negatively affected in the presence of gross
inaccuracies in the bounding boxes estimation. To this end, we also propose a
robust ellipse fitting algorithm able to improve performance in the presence of
errors in the detected objects. Results on synthetic tests and on different
real datasets, involving real challenging scenarios, demonstrate the
applicability and potential of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04769</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04769</id><created>2015-02-16</created><authors><author><keyname>Chaudhuri</keyname><forenames>Kaustuv</forenames><affiliation>INRIA</affiliation></author></authors><title>Undecidability of Multiplicative Subexponential Logic</title><categories>cs.LO</categories><comments>In Proceedings LINEARITY 2014, arXiv:1502.04419</comments><proxy>EPTCS</proxy><acm-class>F.4.2</acm-class><journal-ref>EPTCS 176, 2015, pp. 1-8</journal-ref><doi>10.4204/EPTCS.176.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Subexponential logic is a variant of linear logic with a family of
exponential connectives--called subexponentials--that are indexed and arranged
in a pre-order. Each subexponential has or lacks associated structural
properties of weakening and contraction. We show that classical propositional
multiplicative linear logic extended with one unrestricted and two incomparable
linear subexponentials can encode the halting problem for two register Minsky
machines, and is hence undecidable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04770</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04770</id><created>2015-02-16</created><authors><author><keyname>Paykin</keyname><forenames>Jennifer</forenames><affiliation>University of Pennsylvania</affiliation></author><author><keyname>Zdancewic</keyname><forenames>Steve</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>A Linear/Producer/Consumer Model of Classical Linear Logic</title><categories>cs.LO cs.PL</categories><comments>In Proceedings LINEARITY 2014, arXiv:1502.04419</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 176, 2015, pp. 9-23</journal-ref><doi>10.4204/EPTCS.176.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper defines a new proof- and category-theoretic framework for
classical linear logic that separates reasoning into one linear regime and two
persistent regimes corresponding to ! and ?. The resulting
linear/producer/consumer (LPC) logic puts the three classes of propositions on
the same semantic footing, following Benton's linear/non-linear formulation of
intuitionistic linear logic. Semantically, LPC corresponds to a system of three
categories connected by adjunctions reflecting the linear/producer/consumer
structure. The paper's metatheoretic results include admissibility theorems for
the cut and duality rules, and a translation of the LPC logic into category
theory. The work also presents several concrete instances of the LPC model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04771</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04771</id><created>2015-02-16</created><authors><author><keyname>Brock-Nannestad</keyname><forenames>Taus</forenames><affiliation>INRIA &amp; LIX, &#xc9;cole Polytechnique</affiliation></author><author><keyname>Guenot</keyname><forenames>Nicolas</forenames><affiliation>IT University of Copenhagen</affiliation></author></authors><title>Cut Elimination in Multifocused Linear Logic</title><categories>cs.LO</categories><comments>In Proceedings LINEARITY 2014, arXiv:1502.04419</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 176, 2015, pp. 24-33</journal-ref><doi>10.4204/EPTCS.176.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study cut elimination for a multifocused variant of full linear logic in
the sequent calculus. The multifocused normal form of proofs yields problems
that do not appear in a standard focused system, related to the constraints in
grouping rule instances in focusing phases. We show that cut elimination can be
performed in a sensible way even though the proof requires some specific lemmas
to deal with multifocusing phases, and discuss the difficulties arising with
cut elimination when considering normal forms of proofs in linear logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04772</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04772</id><created>2015-02-16</created><authors><author><keyname>Gan</keyname><forenames>Edward</forenames><affiliation>Facebook</affiliation></author><author><keyname>Tov</keyname><forenames>Jesse A.</forenames><affiliation>Northeastern University</affiliation></author><author><keyname>Morrisett</keyname><forenames>Greg</forenames><affiliation>Harvard University</affiliation></author></authors><title>Type Classes for Lightweight Substructural Types</title><categories>cs.PL</categories><comments>In Proceedings LINEARITY 2014, arXiv:1502.04419</comments><proxy>EPTCS</proxy><acm-class>D.3.3</acm-class><journal-ref>EPTCS 176, 2015, pp. 34-48</journal-ref><doi>10.4204/EPTCS.176.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear and substructural types are powerful tools, but adding them to
standard functional programming languages often means introducing extra
annotations and typing machinery. We propose a lightweight substructural type
system design that recasts the structural rules of weakening and contraction as
type classes; we demonstrate this design in a prototype language, Clamp.
  Clamp supports polymorphic substructural types as well as an expressive
system of mutable references. At the same time, it adds little additional
overhead to a standard Damas-Hindley-Milner type system enriched with type
classes. We have established type safety for the core model and implemented a
type checker with type inference in Haskell.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04773</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04773</id><created>2015-02-16</created><authors><author><keyname>Basaldella</keyname><forenames>Michele</forenames><affiliation>Universit&#xe9; d'Aix-Marseille, CNRS, I2M, Marseille, France</affiliation></author></authors><title>Ludics without Designs I: Triads</title><categories>cs.LO</categories><comments>In Proceedings LINEARITY 2014, arXiv:1502.04419</comments><proxy>EPTCS</proxy><acm-class>F.4.1</acm-class><journal-ref>EPTCS 176, 2015, pp. 49-63</journal-ref><doi>10.4204/EPTCS.176.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce the concept of triad. Using this notion, we
study, revisit, discover and rediscover some basic properties of ludics from a
very general point of view.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04774</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04774</id><created>2015-02-16</created><authors><author><keyname>Lago</keyname><forenames>Ugo Dal</forenames><affiliation>Universit&#xe0; di Bologna &amp; INRIA</affiliation></author><author><keyname>Zorzi</keyname><forenames>Margherita</forenames><affiliation>Universit&#xe0; di Verona</affiliation></author></authors><title>Wave-Style Token Machines and Quantum Lambda Calculi</title><categories>cs.LO cs.PL</categories><comments>In Proceedings LINEARITY 2014, arXiv:1502.04419</comments><proxy>EPTCS</proxy><acm-class>F.3.2; F.1.1</acm-class><journal-ref>EPTCS 176, 2015, pp. 64-78</journal-ref><doi>10.4204/EPTCS.176.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Particle-style token machines are a way to interpret proofs and programs,
when the latter are written following the principles of linear logic. In this
paper, we show that token machines also make sense when the programs at hand
are those of a simple quantum lambda-calculus with implicit qubits. This,
however, requires generalising the concept of a token machine to one in which
more than one particle travel around the term at the same time. The presence of
multiple tokens is intimately related to entanglement and allows us to give a
simple operational semantics to the calculus, coherently with the principles of
quantum computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04775</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04775</id><created>2015-02-16</created><authors><author><keyname>Solieri</keyname><forenames>Marco</forenames><affiliation>LIPN, Paris 13, Sorbonne Paris Cit&#xe9;, CNRS -- DISI, Bologna, INRIA</affiliation></author></authors><title>Geometry of Resource Interaction - A Minimalist Approach</title><categories>cs.LO cs.PL</categories><comments>In Proceedings LINEARITY 2014, arXiv:1502.04419</comments><proxy>EPTCS</proxy><acm-class>F.3.2; F.4.1</acm-class><journal-ref>EPTCS 176, 2015, pp. 79-94</journal-ref><doi>10.4204/EPTCS.176.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Resource $\lambda$-calculus is a variation of the $\lambda$-calculus
where arguments can be superposed and must be linearly used. Hence it is a
model for linear and non-deterministic programming languages, and the target
language of Ehrhard-Taylor expansion of $\lambda$-terms. In a strictly typed
restriction of the Resource $\lambda$-calculus, we study the notion of path
persistence, and we define a Geometry of Interaction that characterises it, is
invariant under reduction, and counts addends in normal forms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04780</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04780</id><created>2015-02-16</created><authors><author><keyname>Wu</keyname><forenames>Qiong</forenames></author></authors><title>Computational Curiosity (A Book Draft)</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This book discusses computational curiosity, from the psychology of curiosity
to the computational models of curiosity, and then showcases several
interesting applications of computational curiosity. A brief overview of the
book is given as follows. Chapter 1 discusses the underpinnings of curiosity in
human beings, including the major categories of curiosity, curiosity-related
emotions and behaviors, and the benefits of curiosity. Chapter 2 reviews the
arousal theories of curiosity in psychology and summarizes a general two-step
process model for computational curiosity. Base on the perspective of the
two-step process model, Chapter 3 reviews and analyzes some of the traditional
computational models of curiosity. Chapter 4 introduces a novel generic
computational model of curiosity, which is developed based on the arousal
theories of curiosity. After the discussion of computational models of
curiosity, we outline the important applications where computational curiosity
may bring significant impacts in Chapter 5. Chapter 6 discusses the application
of the generic computational model of curiosity in a machine learning
framework. Chapter 7 discusses the application of the generic computational
model of curiosity in a recommender system. In Chapter 8 and Chapter 9, the
generic computational model of curiosity is studied in two types of pedagogical
agents. In Chapter 8, a curious peer learner is studied. It is a non-player
character that aims to provide a believable virtual learning environment for
users. In Chapter 9, a curious learning companion is studied. It aims to
enhance users' learning experience through providing meaningful interactions
with them. Chapter 10 discusses open questions in the research field of
computation curiosity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04791</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04791</id><created>2015-02-16</created><authors><author><keyname>Diamant</keyname><forenames>Emanuel</forenames></author></authors><title>Advances in Artificial Intelligence: Are you sure, we are on the right
  track?</title><categories>cs.AI q-bio.NC</categories><comments>The paper was submitted to IJCAI-15 conference, but was prudently
  rejected. Thus, replenishing the collection of my repudiated papers</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past decade, AI has made a remarkable progress. It is agreed that
this is due to the recently revived Deep Learning technology. Deep Learning
enables to process large amounts of data using simplified neuron networks that
simulate the way in which the brain works. However, there is a different point
of view, which posits that the brain is processing information, not data. This
unresolved duality hampered AI progress for years. In this paper, I propose a
notion of Integrated information that hopefully will resolve the problem. I
consider integrated information as a coupling between two separate entities -
physical information (that implies data processing) and semantic information
(that provides physical information interpretation). In this regard,
intelligence becomes a product of information processing. Extending further
this line of thinking, it can be said that information processing does not
require more a human brain for its implementation. Indeed, bacteria and amoebas
exhibit intelligent behavior without any sign of a brain. That dramatically
removes the need for AI systems to emulate the human brain complexity! The
paper tries to explore this shift in AI systems design philosophy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04797</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04797</id><created>2015-02-16</created><authors><author><keyname>Khalili</keyname><forenames>Azam</forenames></author><author><keyname>Rastegarnia</keyname><forenames>Amir</forenames></author></authors><title>Impact of network size on the performance of incremental LMS adaptive
  networks</title><categories>cs.IT cs.SY math.IT</categories><comments>4 pages; 4 figures; Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the impact of network size on the performance of
incremental least mean square (ILMS) adaptive networks. Specifically, we
consider two ILMS networks with different number of nodes and compare their
performance in two different cases including (i) ideal links and (ii) noisy
links. We show that when the links between nodes are ideal, increasing the
network size improves the steady-state error. On the other hand, in the
presence of noisy links, we see different behavior and the ILMS adaptive
network with more nodes necessarily has not better steady-state performance.
Simulation results are also provided to illustrate the discussions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04798</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04798</id><created>2015-02-16</created><authors><author><keyname>Nguyen</keyname><forenames>Ha Huy Cuong</forenames></author><author><keyname>Dang</keyname><forenames>Van Thuan</forenames></author><author><keyname>Le</keyname><forenames>Van Son</forenames></author></authors><title>Technical solutions to resources allocation for distributed virtual
  machine systems</title><categories>cs.DC</categories><comments>International Journal of Computer Science and Telecommunications
  www.ijcst.org, International Journal of Computer Science and Information
  Security - 2015</comments><report-no>ID: 30011511</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Virtual machine is built on group of real servers which are scattered
globally and connect together through the telecommunications systems, it has an
increasingly important role in the operation, providing the ability to exploit
virtual resources. The latest technique helps to use computing resources more
effectively and has many benefits, such as cost reduction of power, cooling
and, hence, contributes to the Green Computing. To ensure the supply of these
resources to demand processes correctly and promptly, avoiding any duplication
or conflict, especially remote resources, it is necessary to study and propose
a reliable solution appropriate to be the foundation for internal control
systems in the cloud. In the scope of this paper, we find a way to produce
efficient distributed resources which emphasizes solutions preventing deadlock
and proposing methods to avoid resource shortage issue. With this approach, the
outcome result is the checklist of re-sources state which has the possibility
of deadlock and lack of resources, by sending messages to the servers, the
server would know the situation and have corresponding reaction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04801</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04801</id><created>2015-02-14</created><authors><author><keyname>Shrivastava</keyname><forenames>Sonal</forenames></author><author><keyname>Agrawal</keyname><forenames>Chetan</forenames></author><author><keyname>Jain</keyname><forenames>Anurag</forenames></author></authors><title>An IDS scheme against Black hole Attack to Secure AOMDV Routing in MANET</title><categories>cs.NI</categories><comments>14 pages,6 figures, International Journal on AdHoc Networking Systems
  (IJANS) Vol. 5, No. 1, January 2015. arXiv admin note: text overlap with
  arXiv:1203.3729 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Mobile Ad hoc Network (MANET) all the nodes are freely moves in the
absence of without ant centralized coordination system. Due to that the
attackers or malicious nodes are easily affected that kind of network and
responsible for the routing misbehavior. The routing is network is mandatory to
deliver data in between source and destination. In this research we work on
security field in MANET and proposed a novel security scheme against routing
misbehavior through Black hole attack. The Ad hoc On demand Multipath Routing
(AOMDV) protocol is consider for routing and also to improves the routing
quality as compare to single path routing protocol. The attacker is affected
all the possible paths that is selected by sender for sending data in network.
The malicious nodes are forward optimistic reply at the time of routing by that
their identification is also a complex procedure. The proposed Intrusion
Detection System (IDS) scheme is identified the attacker information through
hop count mechanism. The routing information of actual data is reached to which
intermediate node and the next hop information is exist at that node is confirm
by IDS scheme. The black hole attacker node Identification (ID) is forward in
network by that in future attacker is not participating in routing procedure.
The proposed security scheme detects and provides the deterrence against
routing misbehavior through malicious attack. Here we compare the routing
performance of AOMDV, Attack and IDS scheme. The performance of normal
multipath routing and proposed IDS scheme is almost equal. The attacker has
degrades the whole routing performance but observed that in presence of
attacker, routing misbehavior is completely block by the proposed IDS scheme
and recovers 95 % of data as compare to normal routing. Keywords- -AOMDV,
MANET, IDS, Black hole attack, Routing misbehavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04803</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04803</id><created>2015-02-17</created><authors><author><keyname>Lokshtanov</keyname><forenames>Daniel</forenames></author><author><keyname>Mouawad</keyname><forenames>Amer E.</forenames></author><author><keyname>Panolan</keyname><forenames>Fahad</forenames></author><author><keyname>Ramanujan</keyname><forenames>M. S.</forenames></author><author><keyname>Saurabh</keyname><forenames>Saket</forenames></author></authors><title>Reconfiguration on sparse graphs</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A vertex-subset graph problem Q defines which subsets of the vertices of an
input graph are feasible solutions. A reconfiguration variant of a
vertex-subset problem asks, given two feasible solutions S and T of size k,
whether it is possible to transform S into T by a sequence of vertex additions
and deletions such that each intermediate set is also a feasible solution of
size bounded by k. We study reconfiguration variants of two classical
vertex-subset problems, namely Independent Set and Dominating Set. We denote
the former by ISR and the latter by DSR. Both ISR and DSR are PSPACE-complete
on graphs of bounded bandwidth and W[1]-hard parameterized by k on general
graphs. We show that ISR is fixed-parameter tractable parameterized by k when
the input graph is of bounded degeneracy or nowhere-dense. As a corollary, we
answer positively an open question concerning the parameterized complexity of
the problem on graphs of bounded treewidth. Moreover, our techniques generalize
recent results showing that ISR is fixed-parameter tractable on planar graphs
and graphs of bounded degree. For DSR, we show the problem fixed-parameter
tractable parameterized by k when the input graph does not contain large
bicliques, a class of graphs which includes graphs of bounded degeneracy and
nowhere-dense graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04806</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04806</id><created>2015-02-17</created><authors><author><keyname>Pillai</keyname><forenames>Sibi Raj B.</forenames></author><author><keyname>Prabhakaran</keyname><forenames>Vinod M.</forenames></author></authors><title>On the Noisy Feedback Capacity of Gaussian Broadcast Channels</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 figures, to appear in IEEE Information Theory Workshop
  2015, Jerusalem</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that, in general, feedback may enlarge the capacity region
of Gaussian broadcast channels. This has been demonstrated even when the
feedback is noisy (or partial-but-perfect) and only from one of the receivers.
The only case known where feedback has been shown not to enlarge the capacity
region is when the channel is physically degraded (El Gamal 1978, 1981). In
this paper, we show that for a class of two-user Gaussian broadcast channels
(not necessarily physically degraded), passively feeding back the stronger
user's signal over a link corrupted by Gaussian noise does not enlarge the
capacity region if the variance of feedback noise is above a certain threshold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04820</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04820</id><created>2015-02-17</created><authors><author><keyname>Maitra</keyname><forenames>Tanmoy</forenames></author></authors><title>Cryptanalysis of A Secure Remote User Authentication Scheme Using Smart
  Cards</title><categories>cs.CR</categories><comments>4 pages, no figure, fresh submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Smart card based authentication schemes are used in various fields like
e-banking, e-commerce, wireless sensor networks, medical system and so on to
authenticate the both remote user and the application server during the
communication via internet. Recently, Karuppiah and Saravanan proposed an
authentication scheme which is based on password and one-way cryptographic hash
function. They have used a secure identity mechanism i.e., users' and server's
identity are not public. Thus, the user and the server do not send their
identity directly to each other during communications. In this paper, we have
found out that their scheme does not overcome the reply attack and also there
is a fault in the login phase, which makes their scheme is not perfect for
practical use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04823</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04823</id><created>2015-02-17</created><authors><author><keyname>Zhang</keyname><forenames>Hui</forenames></author><author><keyname>Yang</keyname><forenames>Kiduk</forenames></author><author><keyname>Jacob</keyname><forenames>Elin</forenames></author></authors><title>Topic Level Disambiguation for Weak Queries</title><categories>cs.IR</categories><journal-ref>Journal of Information Science Theory and Practice, 1(3), 33-46</journal-ref><doi>10.1633/JISTaP.2013.1.3.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite limited success, information retrieval (IR) systems today are not
intelligent or reliable. IR systems return poor search results when users
formulate their information needs into incomplete or ambiguous queries (i.e.,
weak queries). Therefore, one of the main challenges in modern IR research is
to provide consistent results across all queries by improving the performance
on weak queries. However, existing IR approaches such as query expansion are
not overly effective because they make little effort to analyze and exploit the
meanings of the queries. Furthermore, word sense disambiguation approaches,
which rely on textual context, are ineffective against weak queries that are
typically short. Motivated by the demand for a robust IR system that can
consistently provide highly accurate results, the proposed study implemented a
novel topic detection that leveraged both the language model and structural
knowledge of Wikipedia and systematically evaluated the effect of query
disambiguation and topic-based retrieval approaches on TREC collections. The
results not only confirm the effectiveness of the proposed topic detection and
topic-based retrieval approaches but also demonstrate that query disambiguation
does not improve IR as expected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04824</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04824</id><created>2015-02-17</created><authors><author><keyname>Rotbart</keyname><forenames>Aviv</forenames></author><author><keyname>Shabat</keyname><forenames>Gil</forenames></author><author><keyname>Shmueli</keyname><forenames>Yaniv</forenames></author><author><keyname>Averbuch</keyname><forenames>Amir</forenames></author></authors><title>Randomized LU decomposition: An Algorithm for Dictionaries Construction</title><categories>cs.CV</categories><acm-class>I.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, distinctive-dictionary construction has gained importance
due to his usefulness in data processing. Usually, one or more dictionaries are
constructed from a training data and then they are used to classify signals
that did not participate in the training process. A new dictionary construction
algorithm is introduced. It is based on a low-rank matrix factorization being
achieved by the application of the randomized LU decomposition to a training
data. This method is fast, scalable, parallelizable, consumes low memory,
outperforms SVD in these categories and works also extremely well on large
sparse matrices. In contrast to existing methods, the randomized LU
decomposition constructs an under-complete dictionary, which simplifies both
the construction and the classification processes of newly arrived signals. The
dictionary construction is generic and general that fits different
applications. We demonstrate the capabilities of this algorithm for file type
identification, which is a fundamental task in digital security arena,
performed nowadays for example by sandboxing mechanism, deep packet inspection,
firewalls and anti-virus systems. We propose a content-based method that
detects file types that neither depend on file extension nor on metadata. Such
approach is harder to deceive and we show that only a few file fragments from a
whole file are needed for a successful classification. Based on the constructed
dictionaries, we show that the proposed method can effectively identify
execution code fragments in PDF files.
  $\textbf{Keywords.}$ Dictionary construction, classification, LU
decomposition, randomized LU decomposition, content-based file detection,
computer security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04827</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04827</id><created>2015-02-17</created><authors><author><keyname>Bora</keyname><forenames>Saurabh</forenames><affiliation>PDPM Indian Institute of Information Technology, Design and Manufacturing Jabalpur</affiliation></author><author><keyname>Ojha</keyname><forenames>Aparajita</forenames><affiliation>PDPM Indian Institute of Information Technology, Design and Manufacturing Jabalpur</affiliation></author></authors><title>Corrigendum to &quot;Random grid-based visual secret sharing with abilities
  of OR and XOR decryptions&quot;[Journal of Visual Communication and Image
  Representation. 24(2013) 48-62]</title><categories>cs.CR</categories><comments>16 pages, 1 table, Appendix showing computations</comments><acm-class>D.4.6; E.3; H.5.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been observed that the contrast values for (2, 3) VSS scheme, (2, 4)
VSS scheme, (3, 5) VSS scheme and (4, 5) VSS scheme claimed by Wu and Sun
(2013) are found to be incorrect. Since the same values are cited and compared
by many other researchers in their works, we have calculated and presented the
correct values of contrast in this note.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04837</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04837</id><created>2015-02-17</created><updated>2015-03-18</updated><authors><author><keyname>Qiu</keyname><forenames>Teng</forenames></author><author><keyname>Li</keyname><forenames>Yongjie</forenames></author></authors><title>Nonparametric Nearest Neighbor Descent Clustering based on Delaunay
  Triangulation</title><categories>stat.ML cs.CV cs.LG</categories><comments>7 pages; 6 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In our physically inspired in-tree (IT) based clustering algorithm and the
series after it, there is only one free parameter involved in computing the
potential value of each point. In this work, based on the Delaunay
Triangulation or its dual Voronoi tessellation, we propose a nonparametric
process to compute potential values by the local information. This computation,
though nonparametric, is relatively very rough, and consequently, many local
extreme points will be generated. However, unlike those gradient-based methods,
our IT-based methods are generally insensitive to those local extremes. This
positively demonstrates the superiority of these parametric (previous) and
nonparametric (in this work) IT-based methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04843</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04843</id><created>2015-02-17</created><updated>2015-06-09</updated><authors><author><keyname>Jain</keyname><forenames>Brijnesh</forenames></author></authors><title>Generalized Gradient Learning on Time Series under Elastic
  Transformations</title><categories>cs.LG</categories><comments>accepted for publication in Machine Learning</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The majority of machine learning algorithms assumes that objects are
represented as vectors. But often the objects we want to learn on are more
naturally represented by other data structures such as sequences and time
series. For these representations many standard learning algorithms are
unavailable. We generalize gradient-based learning algorithms to time series
under dynamic time warping. To this end, we introduce elastic functions, which
extend functions on time series to matrix spaces. Necessary conditions are
presented under which generalized gradient learning on time series is
consistent. We indicate how results carry over to arbitrary elastic distance
functions and to sequences consisting of symbolic elements. Specifically, four
linear classifiers are extended to time series under dynamic time warping and
applied to benchmark datasets. Results indicate that generalized gradient
learning via elastic functions have the potential to complement the
state-of-the-art in statistical pattern recognition on time series.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04844</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04844</id><created>2015-02-17</created><authors><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author><author><keyname>Doyen</keyname><forenames>Laurent</forenames></author><author><keyname>Vardi</keyname><forenames>Moshe Y.</forenames></author></authors><title>The Complexity of Synthesis from Probabilistic Components</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The synthesis problem asks for the automatic construction of a system from
its specification. In the traditional setting, the system is &quot;constructed from
scratch&quot; rather than composed from reusable components. However, this is rare
in practice, and almost every non-trivial software system relies heavily on the
use of libraries of reusable components. Recently, Lustig and Vardi introduced
dataflow and controlflow synthesis from libraries of reusable components. They
proved that dataflow synthesis is undecidable, while controlflow synthesis is
decidable. The problem of controlflow synthesis from libraries of probabilistic
components was considered by Nain, Lustig and Vardi, and was shown to be
decidable for qualitative analysis (that asks that the specification be
satisfied with probability 1). Our main contributions for controlflow synthesis
from probabilistic components are to establish better complexity bounds for the
qualitative analysis problem, and to show that the more general quantitative
problem is undecidable. For the qualitative analysis, we show that the problem
(i) is EXPTIME-complete when the specification is given as a deterministic
parity word automaton, improving the previously known 2EXPTIME upper bound; and
(ii) belongs to UP $\cap$ coUP and is parity-games hard, when the specification
is given directly as a parity condition on the components, improving the
previously known EXPTIME upper bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04860</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04860</id><created>2015-02-17</created><authors><author><keyname>Duan</keyname><forenames>Wei</forenames></author><author><keyname>Song</keyname><forenames>Wei</forenames></author><author><keyname>Lee</keyname><forenames>Moon Ho</forenames></author></authors><title>Low-Complexity QL-QR Decomposition Based Beamforming Design for Two-Way
  MIMO Relay Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the optimization problem of joint source and
relay beamforming matrices for a twoway amplify-and-forward (AF) multi-input
multi-output (MIMO) relay system. The system consisting of two source nodes and
two relay nodes is considered and the linear minimum meansquare- error (MMSE)
is employed at both receivers. We assume individual relay power constraints and
study an important design problem, a so-called determinant maximization (DM)
problem. Since this DM problem is nonconvex, we consider an efficient iterative
algorithm by using an MSE balancing result to obtain at least a locally optimal
solution. The proposed algorithm is developed based on QL, QR and Choleskey
decompositions which differ in the complexity and performance. Analytical and
simulation results show that the proposed algorithm can significantly reduce
computational complexity compared with their existing two-way relay systems and
have equivalent bit-error-rate (BER) performance to the singular value
decomposition (SVD) based on a regular block diagonal (RBD) scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04861</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04861</id><created>2015-02-17</created><authors><author><keyname>Schad</keyname><forenames>Adrian</forenames></author><author><keyname>Law</keyname><forenames>Ka L.</forenames></author><author><keyname>Pesavento</keyname><forenames>Marius</forenames></author></authors><title>Rank-Two Beamforming and Power Allocation in Multicasting Relay Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel single-group multicasting relay beamforming
scheme. We assume a source that transmits common messages via multiple
amplify-and-forward relays to multiple destinations. To increase the number of
degrees of freedom in the beamforming design, the relays process two received
signals jointly and transmit the Alamouti space-time block code over two
different beams. Furthermore, in contrast to the existing relay multicasting
scheme of the literature, we take into account the direct links from the source
to the destinations. We aim to maximize the lowest received quality-of-service
by choosing the proper relay weights and the ideal distribution of the power
resources in the network. To solve the corresponding optimization problem, we
propose an iterative algorithm which solves sequences of convex approximations
of the original non-convex optimization problem. Simulation results demonstrate
significant performance improvements of the proposed methods as compared with
the existing relay multicasting scheme of the literature and an algorithm based
on the popular semidefinite relaxation technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04868</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04868</id><created>2015-02-17</created><updated>2015-02-18</updated><authors><author><keyname>Boloix-Tortosa</keyname><forenames>Rafael</forenames></author><author><keyname>Pay&#xe1;n-Somet</keyname><forenames>F. Javier</forenames></author><author><keyname>Arias-de-Reyna</keyname><forenames>Eva</forenames></author><author><keyname>Murillo-Fuentes</keyname><forenames>Juan Jos&#xe9;</forenames></author></authors><title>Proper Complex Gaussian Processes for Regression</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex-valued signals are used in the modeling of many systems in
engineering and science, hence being of fundamental interest. Often, random
complex-valued signals are considered to be proper. A proper complex random
variable or process is uncorrelated with its complex conjugate. This assumption
is a good model of the underlying physics in many problems, and simplifies the
computations. While linear processing and neural networks have been widely
studied for these signals, the development of complex-valued nonlinear kernel
approaches remains an open problem. In this paper we propose Gaussian processes
for regression as a framework to develop 1) a solution for proper
complex-valued kernel regression and 2) the design of the reproducing kernel
for complex-valued inputs, using the convolutional approach for
cross-covariances. In this design we pay attention to preserve, in the complex
domain, the measure of similarity between near inputs. The hyperparameters of
the kernel are learned maximizing the marginal likelihood using Wirtinger
derivatives. Besides, the approach is connected to the multiple output learning
scenario. In the experiments included, we first solve a proper complex Gaussian
process where the cross-covariance does not cancel, a challenging scenario when
dealing with proper complex signals. Then we successfully use these novel
results to solve some problems previously proposed in the literature as
benchmarks, reporting a remarkable improvement in the estimation error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04870</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04870</id><created>2015-02-17</created><authors><author><keyname>Shabtai</keyname><forenames>Asaf</forenames></author><author><keyname>Mimran</keyname><forenames>Dudu</forenames></author><author><keyname>Elovici</keyname><forenames>Yuval</forenames></author></authors><title>Evaluation of Security Solutions for Android Systems</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the increasing usage of smartphones a plethora of security solutions are
being designed and developed. Many of the security solutions fail to cope with
advanced attacks and are not aways properly designed for smartphone platforms.
Therefore, there is a need for a methodology to evaluate their effectiveness.
Since the Android operating system has the highest market share today, we
decided to focus on it in this study in which we review some of the
state-of-the-art security solutions for Android-based smartphones. In addition,
we present a set of evaluation criteria aiming at evaluating security
mechanisms that are specifically designed for Android-based smartphones. We
believe that the proposed framework will help security solution designers
develop more effective solutions and assist security experts evaluate the
effectiveness of security solutions for Android-based smartphones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04873</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04873</id><created>2015-02-17</created><authors><author><keyname>Vaccarino</keyname><forenames>Francesco</forenames></author><author><keyname>Patania</keyname><forenames>Alice</forenames></author><author><keyname>Petri</keyname><forenames>Giovanni</forenames></author></authors><title>$P$-persistent homology of finite topological spaces</title><categories>math.AT cs.CG math.CO math.CT</categories><msc-class>55N99, 18A25, 05C22</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $P$ be a finite poset. We will show that for any reasonable
$P$-persistent object $X$ in the category of finite topological spaces, there
is a $P-$ weighted graph, whose clique complex has the same $P$-persistent
homology as $X$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04885</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04885</id><created>2015-02-17</created><authors><author><keyname>Pitel</keyname><forenames>Guillaume</forenames></author><author><keyname>Fouquier</keyname><forenames>Geoffroy</forenames></author></authors><title>Count-Min-Log sketch: Approximately counting with approximate counters</title><categories>cs.IR cs.DS</categories><comments>7 pages, 3 figures. Some preliminary notes can be found on
  http://blog.guillaume-pitel.fr/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Count-Min Sketch is a widely adopted algorithm for approximate event counting
in large scale processing. However, the original version of the
Count-Min-Sketch (CMS) suffers of some deficiences, especially if one is
interested by the low-frequency items, such as in text-mining related tasks.
Several variants of CMS have been proposed to compensate for the high relative
error for low-frequency events, but the proposed solutions tend to correct the
errors instead of preventing them. In this paper, we propose the Count-Min-Log
sketch, which uses logarithm-based, approximate counters instead of linear
counters to improve the average relative error of CMS at constant memory
footprint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04888</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04888</id><created>2015-02-17</created><updated>2015-03-30</updated><authors><author><keyname>Aziz</keyname><forenames>Haris</forenames></author><author><keyname>Gaspers</keyname><forenames>Serge</forenames></author><author><keyname>Mackenzie</keyname><forenames>Simon</forenames></author><author><keyname>Mattei</keyname><forenames>Nicholas</forenames></author><author><keyname>Narodytska</keyname><forenames>Nina</forenames></author><author><keyname>Walsh</keyname><forenames>Toby</forenames></author></authors><title>Equilibria Under the Probabilistic Serial Rule</title><categories>cs.GT</categories><comments>arXiv admin note: text overlap with arXiv:1401.6523, this paper
  supersedes the equilibria section in our previous report arXiv:1401.6523</comments><msc-class>91A12, 68Q15</msc-class><acm-class>F.2; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The probabilistic serial (PS) rule is a prominent randomized rule for
assigning indivisible goods to agents. Although it is well known for its good
fairness and welfare properties, it is not strategyproof. In view of this, we
address several fundamental questions regarding equilibria under PS. Firstly,
we show that Nash deviations under the PS rule can cycle. Despite the
possibilities of cycles, we prove that a pure Nash equilibrium is guaranteed to
exist under the PS rule. We then show that verifying whether a given profile is
a pure Nash equilibrium is coNP-complete, and computing a pure Nash equilibrium
is NP-hard. For two agents, we present a linear-time algorithm to compute a
pure Nash equilibrium which yields the same assignment as the truthful profile.
Finally, we conduct experiments to evaluate the quality of the equilibria that
exist under the PS rule, finding that the vast majority of pure Nash equilibria
yield social welfare that is at least that of the truthful profile.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04896</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04896</id><created>2015-02-17</created><authors><author><keyname>Nguyen</keyname><forenames>C. Thach</forenames></author></authors><title>Optimal non-adaptive solutions for the counterfeit coin problem</title><categories>cs.DM math.HO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give optimal solutions to all versions of the popular counterfeit coin
problem obtained by varying whether (i) we know if the counterfeit coin is
heavier or lighter than the genuine ones, (ii) we know if the counterfeit coin
exists, (iii) we have access to additional genuine coins, and (iv) we need to
determine if the counterfeit coin is heavier or lighter than the genuine ones.
Moreover, our solutions are non-adaptive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04898</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04898</id><created>2015-02-17</created><authors><author><keyname>Boja&#x144;czyk</keyname><forenames>Miko&#x142;aj</forenames></author></authors><title>Recognisable languages over monads</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The principle behind algebraic language theory for various kinds of
structures, such as words or trees, is to use a compositional function from the
structures into a finite set. To talk about compositionality, one needs some
way of composing structures into bigger structures. It so happens that category
theory has an abstract concept for this, namely a monad. The goal of this paper
is to propose monads as a unifying framework for discussing existing algebras
and designing new algebras.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04908</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04908</id><created>2015-02-17</created><updated>2015-11-12</updated><authors><author><keyname>Kuznetsov</keyname><forenames>Petr</forenames></author><author><keyname>Ravi</keyname><forenames>Srivatsan</forenames></author></authors><title>Progressive Transactional Memory in Time and Space</title><categories>cs.DC</categories><comments>Model of Transactional Memory identical with arXiv:1407.6876,
  arXiv:1502.02725</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transactional memory (TM) allows concurrent processes to organize sequences
of operations on shared \emph{data items} into atomic transactions. A
transaction may commit, in which case it appears to have executed sequentially
or it may \emph{abort}, in which case no data item is updated.
  The TM programming paradigm emerged as an alternative to conventional
fine-grained locking techniques, offering ease of programming and
compositionality. Though typically themselves implemented using locks, TMs hide
the inherent issues of lock-based synchronization behind a nice transactional
programming interface.
  In this paper, we explore inherent time and space complexity of lock-based
TMs, with a focus of the most popular class of \emph{progressive} lock-based
TMs. We derive that a progressive TM might enforce a read-only transaction to
perform a quadratic (in the number of the data items it reads) number of steps
and access a linear number of distinct memory locations, closing the question
of inherent cost of \emph{read validation} in TMs. We then show that the total
number of \emph{remote memory references} (RMRs) that take place in an
execution of a progressive TM in which $n$ concurrent processes perform
transactions on a single data item might reach $\Omega(n \log n)$, which
appears to be the first RMR complexity lower bound for transactional memory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04918</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04918</id><created>2015-02-17</created><updated>2016-01-12</updated><authors><author><keyname>Li</keyname><forenames>Jian</forenames></author><author><keyname>Jin</keyname><forenames>Yifei</forenames></author></authors><title>A PTAS for the Weighted Unit Disk Cover Problem</title><categories>cs.CG</categories><comments>We fixed several typos in this version. 37 pages. 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are given a set of weighted unit disks and a set of points in Euclidean
plane. The minimum weight unit disk cover (\UDC) problem asks for a subset of
disks of minimum total weight that covers all given points. \UDC\ is one of the
geometric set cover problems, which have been studied extensively for the past
two decades (for many different geometric range spaces, such as (unit) disks,
halfspaces, rectangles, triangles). It is known that the unweighted \UDC\
problem is NP-hard and admits a polynomial-time approximation scheme (PTAS).
For the weighted \UDC\ problem, several constant approximations have been
developed. However, whether the problem admits a PTAS has been an open
question. In this paper, we answer this question affirmatively by presenting
the first PTAS for \UDC. Our result implies the first PTAS for the minimum
weight dominating set problem in unit disk graphs. Combining with existing
ideas, our result can also be used to obtain the first PTAS for the maxmimum
lifetime coverage problem and an improved constant approximation ratio for the
connected dominating set problem in unit disk graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04925</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04925</id><created>2015-02-17</created><authors><author><keyname>Asinowski</keyname><forenames>Andrei</forenames></author><author><keyname>Rote</keyname><forenames>G&#xfc;nter</forenames></author></authors><title>Point sets with many non-crossing matchings</title><categories>cs.CG cs.DM math.CO</categories><comments>33 pages, 19 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The maximum number of non-crossing straight-line perfect matchings that a set
of $n$ points in the plane can have is known to be $O(10.0438^n)$ and
$\Omega^*(3^n)$. The lower bound, due to Garc\'ia, Noy, and Tejel (2000) is
attained by the double chain, which has $\Theta(3^n n^{O(1)})$ such matchings.
We reprove this bound in a simplified way that uses the novel notion of
down-free matching, and apply this approach on several other constructions. As
a result, we improve the lower bound. First we show that double zigzag chain
with $n$ points has $\Theta^*(\lambda^n)$ such matchings with $\lambda \approx
3.0532$. Next we analyze further generalizations of double zigzag chains -
double $r$-chains. The best choice of parameters leads to a construction with
$\Theta^*(\nu^n)$ matchings, with $\nu \approx 3.0930$. The derivation of this
bound requires an analysis of a coupled dynamic-programming recursion between
two infinite vectors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04933</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04933</id><created>2015-02-17</created><authors><author><keyname>He</keyname><forenames>Chen</forenames></author><author><keyname>Wang</keyname><forenames>Z. Jane</forenames></author><author><keyname>Leung</keyname><forenames>Victor C. M.</forenames></author></authors><title>Block-Level Unitary Query: Incorporating Orthogonal-like Space-time Code
  with Query Diversity for MIMO Backscatter RFID</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Because of the emerging field of Internet of Things (IoT), future backscatter
RFID is required to be more reliable and data intensive. Motivated by this,
orthogonal space-time block code (OSTBC), which is very successful in mobile
communications for its low complexity and high performance, has already been
investigated for backscatter RFID. On the other hand, a recently proposed
scheme called unitary query was shown to be able to considerably improve the
reliability of backscatter radio by exploiting query diversity. Therefore
incorporating the classical OSTBC (at the tag end) with the recently proposed
unitary query (at the query end) seems to be promising. However, in this paper,
we show that simple, direct employment of OSTBC together with unitary query
incurs a linear decoding problem and eventually leads to a severe performance
degradation. As a re-design of the recently proposed unitary query and the
classical OSTBC specifically for MIMO backscatter RFID, we present a
BUTQ-mOSTBC design pair idea by proposing the block-level unitary query (BUTQ)
at the query end and the corresponding modified OSTBC (mOSTBC) at the tag end.
The proposed BUTQ-mOSTBC can resolve the linear decoding problem, keep the
simplicity and high performance properties of the classical OSTBC, and achieve
the query diversity for the $M \times L \times N$ MIMO backscatter RFID
channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04938</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04938</id><created>2015-02-17</created><authors><author><keyname>Bisazza</keyname><forenames>Arianna</forenames></author><author><keyname>Federico</keyname><forenames>Marcello</forenames></author></authors><title>A Survey of Word Reordering in Statistical Machine Translation:
  Computational Models and Language Phenomena</title><categories>cs.CL</categories><comments>46 pages, submitted to Computational Linguistics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Word reordering is one of the most difficult aspects of statistical machine
translation (SMT), and an important factor of its quality and efficiency.
Despite the vast amount of research published to date, the interest of the
community in this problem has not decreased, and no single method appears to be
strongly dominant across language pairs. Instead, the choice of the optimal
approach for a new translation task still seems to be mostly driven by
empirical trials. To orientate the reader in this vast and complex research
area, we present a comprehensive survey of word reordering viewed as a
statistical modeling challenge and as a natural language phenomenon. The survey
describes in detail how word reordering is modeled within different
string-based and tree-based SMT frameworks and as a stand-alone task, including
systematic overviews of the literature in advanced reordering modeling. We then
question why some approaches are more successful than others in different
language pairs. We argue that, besides measuring the amount of reordering, it
is important to understand which kinds of reordering occur in a given language
pair. To this end, we conduct a qualitative analysis of word reordering
phenomena in a diverse sample of language pairs, based on a large collection of
linguistic knowledge. Empirical results in the SMT literature are shown to
support the hypothesis that a few linguistic facts can be very useful to
anticipate the reordering characteristics of a language pair and to select the
SMT framework that best suits them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04956</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04956</id><created>2015-02-17</created><authors><author><keyname>Gatterbauer</keyname><forenames>Wolfgang</forenames></author></authors><title>The Linearization of Pairwise Markov Networks</title><categories>cs.AI cs.LG cs.SI</categories><comments>14 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Belief Propagation (BP) allows to approximate exact probabilistic inference
in graphical models, such as Markov networks (also called Markov random fields,
or undirected graphical models). However, no exact convergence guarantees for
BP are known, in general. Recent work has proposed to approximate BP by
linearizing the update equations around default values for the special case
when all edges in the Markov network carry the same symmetric, doubly
stochastic potential. This linearization has led to exact convergence
guarantees, considerable speed-up, while maintaining high quality results in
network-based classification (i.e. when we only care about the most likely
label or class for each node and not the exact probabilities). The present
paper generalizes our prior work on Linearized Belief Propagation (LinBP) with
an approach that approximates Loopy Belief Propagation on any pairwise Markov
network with the problem of solving a linear equation system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04963</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04963</id><created>2015-02-17</created><updated>2015-02-26</updated><authors><author><keyname>Rybicki</keyname><forenames>Joel</forenames></author><author><keyname>Suomela</keyname><forenames>Jukka</forenames></author></authors><title>Exact bounds for distributed graph colouring</title><categories>cs.DC cs.DS</categories><comments>16 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove exact bounds on the time complexity of distributed graph colouring.
If we are given a directed path that is properly coloured with $n$ colours, by
prior work it is known that we can find a proper 3-colouring in $\frac{1}{2}
\log^*(n) \pm O(1)$ communication rounds. We close the gap between upper and
lower bounds: we show that for infinitely many $n$ the time complexity is
precisely $\frac{1}{2} \log^* n$ communication rounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04972</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04972</id><created>2015-02-17</created><authors><author><keyname>Tsai</keyname><forenames>Chuan-Yung</forenames></author><author><keyname>Cox</keyname><forenames>David D.</forenames></author></authors><title>Measuring and Understanding Sensory Representations within Deep Networks
  Using a Numerical Optimization Framework</title><categories>cs.NE q-bio.NC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A central challenge in sensory neuroscience is describing how the activity of
populations of neurons can represent useful features of the external
environment. However, while neurophysiologists have long been able to record
the responses of neurons in awake, behaving animals, it is another matter
entirely to say what a given neuron does. A key problem is that in many sensory
domains, the space of all possible stimuli that one might encounter is
effectively infinite; in vision, for instance, natural scenes are
combinatorially complex, and an organism will only encounter a tiny fraction of
possible stimuli. As a result, even describing the response properties of
sensory neurons is difficult, and investigations of neuronal functions are
almost always critically limited by the number of stimuli that can be
considered. In this paper, we propose a closed-loop, optimization-based
experimental framework for characterizing the response properties of sensory
neurons, building on past efforts in closed-loop experimental methods, and
leveraging recent advances in artificial neural networks to serve as as a
proving ground for our techniques. Specifically, using deep convolutional
neural networks, we asked whether modern black-box optimization techniques can
be used to interrogate the &quot;tuning landscape&quot; of an artificial neuron in a
deep, nonlinear system, without imposing significant constraints on the space
of stimuli under consideration. We introduce a series of measures to quantify
the tuning landscapes, and show how these relate to the performances of the
networks in an object recognition task. To the extent that deep convolutional
neural networks increasingly serve as de facto working hypotheses for
biological vision, we argue that developing a unified approach for studying
both artificial and biological systems holds great potential to advance both
fields together.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04980</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04980</id><created>2015-02-17</created><updated>2015-04-09</updated><authors><author><keyname>Fearnley</keyname><forenames>John</forenames></author><author><keyname>Igwe</keyname><forenames>Tobenna Peter</forenames></author><author><keyname>Savani</keyname><forenames>Rahul</forenames></author></authors><title>An Empirical Study of Finding Approximate Equilibria in Bimatrix Games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While there have been a number of studies about the efficacy of methods to
find exact Nash equilibria in bimatrix games, there has been little empirical
work on finding approximate Nash equilibria. Here we provide such a study that
compares a number of approximation methods and exact methods. In particular, we
explore the trade-off between the quality of approximate equilibrium and the
required running time to find one. We found that the existing library GAMUT,
which has been the de facto standard that has been used to test exact methods,
is insufficient as a test bed for approximation methods since many of its games
have pure equilibria or other easy-to-find good approximate equilibria. We
extend the breadth and depth of our study by including new interesting families
of bimatrix games, and studying bimatrix games upto size $2000 \times 2000$.
Finally, we provide new close-to-worst-case examples for the best-performing
algorithms for finding approximate Nash equilibria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04981</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04981</id><created>2015-02-17</created><updated>2015-02-25</updated><authors><author><keyname>Ozay</keyname><forenames>Mete</forenames></author></authors><title>Semi-supervised Segmentation Fusion of Multi-spectral and Aerial Images</title><categories>cs.CV</categories><comments>A version of the manuscript was published in ICPR 2014</comments><journal-ref>Proc. 22nd International Conference on Pattern Recognition, pp.
  3839-3844, Stockholm, 2014</journal-ref><doi>10.1109/ICPR.2014.659</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Semi-supervised Segmentation Fusion algorithm is proposed using consensus
and distributed learning. The aim of Unsupervised Segmentation Fusion (USF) is
to achieve a consensus among different segmentation outputs obtained from
different segmentation algorithms by computing an approximate solution to the
NP problem with less computational complexity. Semi-supervision is incorporated
in USF using a new algorithm called Semi-supervised Segmentation Fusion (SSSF).
In SSSF, side information about the co-occurrence of pixels in the same or
different segments is formulated as the constraints of a convex optimization
problem. The results of the experiments employed on artificial and real-world
benchmark multi-spectral and aerial images show that the proposed algorithms
perform better than the individual state-of-the art segmentation algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04983</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04983</id><created>2015-02-17</created><authors><author><keyname>Intharah</keyname><forenames>Thanapong</forenames></author><author><keyname>Brostow</keyname><forenames>Gabriel J.</forenames></author></authors><title>Context Tricks for Cheap Semantic Segmentation</title><categories>cs.CV</categories><comments>Supplementary material can be found at
  http://www0.cs.ucl.ac.uk/staff/T.Intharah/research.html</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate semantic labeling of image pixels is difficult because intra-class
variability is often greater than inter-class variability. In turn, fast
semantic segmentation is hard because accurate models are usually too
complicated to also run quickly at test-time. Our experience with building and
running semantic segmentation systems has also shown a reasonably obvious
bottleneck on model complexity, imposed by small training datasets. We
therefore propose two simple complementary strategies that leverage context to
give better semantic segmentation, while scaling up or down to train on
different-sized datasets.
  As easy modifications for existing semantic segmentation algorithms, we
introduce Decorrelated Semantic Texton Forests, and the Context Sensitive Image
Level Prior. The proposed modifications are tested using a Semantic Texton
Forest (STF) system, and the modifications are validated on two standard
benchmark datasets, MSRC-21 and PascalVOC-2010. In Python based comparisons,
our system is insignificantly slower than STF at test-time, yet produces
superior semantic segmentations overall, with just push-button training.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04997</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04997</id><created>2015-02-17</created><authors><author><keyname>Gloor</keyname><forenames>Peter A.</forenames></author><author><keyname>Colladon</keyname><forenames>Andrea Fronzetti</forenames></author></authors><title>Measuring Organizational Consciousness Through E-Mail Based Social
  Network Analysis</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>Proceedings of the 5th International Conference on Collaborative
  Innovation Networks COINs15, Tokyo, Japan March 12-14, 2015
  (arXiv:1502.01142)</comments><report-no>coins15/2015/05</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes first experiments measuring organizational consciousness
by comparing six &quot;honest signals&quot; of interpersonal communication within
organizations with organizational metrics of performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05021</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05021</id><created>2015-02-17</created><authors><author><keyname>Verhodubs</keyname><forenames>Olegs</forenames></author></authors><title>Inductive Learning for Rule Generation from Ontology</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an idea of inductive learning use for rule generation
from ontologies. The main purpose of the paper is to evaluate the possibility
of inductive learning use in rule generation from ontologies and to develop the
way how this can be done. Generated rules are necessary to supplement or even
to develop the Semantic Web Expert System (SWES) knowledge base. The SWES
emerges as the result of evolution of expert system concept toward the Web, and
the SWES is based on the Semantic Web technologies. Available publications show
that the problem of rule generation from ontologies based on inductive learning
is not investigated deeply enough.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05023</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05023</id><created>2015-02-17</created><updated>2015-02-19</updated><authors><author><keyname>Bhojanapalli</keyname><forenames>Srinadh</forenames></author><author><keyname>Sanghavi</keyname><forenames>Sujay</forenames></author></authors><title>A New Sampling Technique for Tensors</title><categories>stat.ML cs.DS cs.IT cs.LG math.IT</categories><comments>29 pages,3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose new techniques to sample arbitrary third-order
tensors, with an objective of speeding up tensor algorithms that have recently
gained popularity in machine learning. Our main contribution is a new way to
select, in a biased random way, only $O(n^{1.5}/\epsilon^2)$ of the possible
$n^3$ elements while still achieving each of the three goals: \\ {\em (a)
tensor sparsification}: for a tensor that has to be formed from arbitrary
samples, compute very few elements to get a good spectral approximation, and
for arbitrary orthogonal tensors {\em (b) tensor completion:} recover an
exactly low-rank tensor from a small number of samples via alternating least
squares, or {\em (c) tensor factorization:} approximating factors of a low-rank
tensor corrupted by noise. \\ Our sampling can be used along with existing
tensor-based algorithms to speed them up, removing the computational bottleneck
in these methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05040</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05040</id><created>2015-02-17</created><updated>2015-02-21</updated><authors><author><keyname>Neama</keyname><forenames>Tamer M. Abo</forenames></author><author><keyname>Ismail</keyname><forenames>Ismail A.</forenames></author><author><keyname>Sobh</keyname><forenames>Tarek S.</forenames></author><author><keyname>Zaki</keyname><forenames>M.</forenames></author></authors><title>Design of a Framework to Facilitate Decisions Using Information Fusion</title><categories>cs.AI</categories><comments>17 pages, 5 figures, Journal of Al Azhar University Engineering
  Sector, Vol. 8, No. 28, July 2013, 1237-1250. arXiv admin note: text overlap
  with arXiv:cs/0409007 by other authors</comments><journal-ref>Journal of Al Azhar University Engineering Sector, Vol. 8, No. 28,
  July 2013, 1237-1250</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information fusion is an advanced research area which can assist decision
makers in enhancing their decisions. This paper aims at designing a new
multi-layer framework that can support the process of performing decisions from
the obtained beliefs using information fusion. Since it is not an easy task to
cross the gap between computed beliefs of certain hypothesis and decisions, the
proposed framework consists of the following layers in order to provide a
suitable architecture (ordered bottom up): 1. A layer for combination of basic
belief assignments using an information fusion approach. Such approach exploits
Dezert-Smarandache Theory, DSmT, and proportional conflict redistribution to
provide more realistic final beliefs. 2. A layer for computation of pignistic
probability of the underlying propositions from the corresponding final
beliefs. 3. A layer for performing probabilistic reasoning using a Bayesian
network that can obtain the probable reason of a proposition from its pignistic
probability. 4. Ranking the system decisions is ultimately used to support
decision making. A case study has been accomplished at various operational
conditions in order to prove the concept, in addition it pointed out that: 1.
The use of DSmT for information fusion yields not only more realistic beliefs
but also reliable pignistic probabilities for the underlying propositions. 2.
Exploiting the pignistic probability for the integration of the information
fusion with the Bayesian network provides probabilistic inference and enable
decision making on the basis of both belief based probabilities for the
underlying propositions and Bayesian based probabilities for the corresponding
reasons. A comparative study of the proposed framework with respect to other
information fusion systems confirms its superiority to support decision making.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05041</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05041</id><created>2015-02-17</created><authors><author><keyname>Tran</keyname><forenames>Ngoc Hieu</forenames></author><author><keyname>Chen</keyname><forenames>Xin</forenames></author></authors><title>AMAS: optimizing the partition and filtration of adaptive seeds to speed
  up read mapping</title><categories>q-bio.GN cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background: Identifying all possible mapping locations of next-generation
sequencing (NGS) reads is highly essential in several applications such as
prediction of genomic variants or protein binding motifs located in repeat
regions, isoform expression quantification, metagenomics analysis, etc.
However, this task is very time-consuming and majority of mapping tools only
focus on one or a few best mapping locations. Results: We propose AMAS, an
alignment tool specialized in identifying all possible mapping locations of NGS
reads in a reference sequence. AMAS features an effective use of adaptive seeds
to speed up read mapping while preserving sensitivity. Specifically, an index
is designed to pre-store the locations of adaptive seeds in the reference
sequence, efficiently reducing the time for seed matching and partitioning. An
accurate filtration of adaptive seeds is further applied to substantially
tighten the candidate alignment space. As a result, AMAS runs several times
faster than other state-of-the-art read mappers while achieving similar
accuracy. Conclusions: AMAS provides a valuable resource to speed up the
important yet time-consuming task of identifying all mapping locations of NGS
reads. AMAS is implemented in C++ based on the SeqAn library and is freely
available at https://sourceforge.net/projects/ngsamas/. Keywords:
next-generation sequencing, read mapping, sequence alignment, adaptive seeds,
seed partition, filtration
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05056</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05056</id><created>2015-02-17</created><authors><author><keyname>Meir</keyname><forenames>Reshef</forenames></author><author><keyname>Parkes</keyname><forenames>David</forenames></author></authors><title>On Sex, Evolution, and the Multiplicative Weights Update Algorithm</title><categories>cs.LG cs.GT</categories><comments>full version of a paper accepted to AAMAS-2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a recent innovative theory by Chastain et al. on the role of sex
in evolution [PNAS'14]. In short, the theory suggests that the evolutionary
process of gene recombination implements the celebrated multiplicative weights
updates algorithm (MWUA). They prove that the population dynamics induced by
sexual reproduction can be precisely modeled by genes that use MWUA as their
learning strategy in a particular coordination game. The result holds in the
environments of \emph{weak selection}, under the assumption that the population
frequencies remain a product distribution.
  We revisit the theory, eliminating both the requirement of weak selection and
any assumption on the distribution of the population. Removing the assumption
of product distributions is crucial, since as we show, this assumption is
inconsistent with the population dynamics. We show that the marginal allele
distributions induced by the population dynamics precisely match the marginals
induced by a multiplicative weights update algorithm in this general setting,
thereby affirming and substantially generalizing these earlier results.
  We further revise the implications for convergence and utility or fitness
guarantees in coordination games. In contrast to the claim of Chastain et
al.[PNAS'14], we conclude that the sexual evolutionary dynamics does not entail
any property of the population distribution, beyond those already implied by
convergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05058</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05058</id><created>2015-02-17</created><authors><author><keyname>Benson</keyname><forenames>Austin R.</forenames></author><author><keyname>Gleich</keyname><forenames>David F.</forenames></author><author><keyname>Leskovec</keyname><forenames>Jure</forenames></author></authors><title>Tensor Spectral Clustering for Partitioning Higher-order Network
  Structures</title><categories>cs.SI physics.soc-ph</categories><comments>SDM 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectral graph theory-based methods represent an important class of tools for
studying the structure of networks. Spectral methods are based on a first-order
Markov chain derived from a random walk on the graph and thus they cannot take
advantage of important higher-order network substructures such as triangles,
cycles, and feed-forward loops. Here we propose a Tensor Spectral Clustering
(TSC) algorithm that allows for modeling higher-order network structures in a
graph partitioning framework. Our TSC algorithm allows the user to specify
which higher-order network structures (cycles, feed-forward loops, etc.) should
be preserved by the network clustering. Higher-order network structures of
interest are represented using a tensor, which we then partition by developing
a multilinear spectral method. Our framework can be applied to discovering
layered flows in networks as well as graph anomaly detection, which we
illustrate on synthetic networks. In directed networks, a higher-order
structure of particular interest is the directed 3-cycle, which captures
feedback loops in networks. We demonstrate that our TSC algorithm produces
large partitions that cut fewer directed 3-cycles than standard spectral
clustering algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05061</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05061</id><created>2015-02-17</created><authors><author><keyname>&#x160;ubelj</keyname><forenames>Lovro</forenames></author><author><keyname>Fiala</keyname><forenames>Dalibor</forenames></author><author><keyname>Bajec</keyname><forenames>Marko</forenames></author></authors><title>Network-based statistical comparison of citation topology of
  bibliographic databases</title><categories>cs.DL cs.SI physics.data-an physics.soc-ph</categories><comments>16 pages, 3 figures, 3 tables</comments><journal-ref>Sci. Rep. 4, 6496 (2014)</journal-ref><doi>10.1038/srep06496</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern bibliographic databases provide the basis for scientific research and
its evaluation. While their content and structure differ substantially, there
exist only informal notions on their reliability. Here we compare the
topological consistency of citation networks extracted from six popular
bibliographic databases including Web of Science, CiteSeer and arXiv.org. The
networks are assessed through a rich set of local and global graph statistics.
We first reveal statistically significant inconsistencies between some of the
databases with respect to individual statistics. For example, the introduced
field bow-tie decomposition of DBLP Computer Science Bibliography substantially
differs from the rest due to the coverage of the database, while the citation
information within arXiv.org is the most exhaustive. Finally, we compare the
databases over multiple graph statistics using the critical difference diagram.
The citation topology of DBLP Computer Science Bibliography is the least
consistent with the rest, while, not surprisingly, Web of Science is
significantly more reliable from the perspective of consistency. This work can
serve either as a reference for scholars in bibliometrics and scientometrics or
a scientific evaluation guideline for governments and research agencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05067</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05067</id><created>2015-02-17</created><authors><author><keyname>&#x160;ubelj</keyname><forenames>Lovro</forenames></author><author><keyname>&#x17d;itnik</keyname><forenames>Slavko</forenames></author><author><keyname>Blagus</keyname><forenames>Neli</forenames></author><author><keyname>Bajec</keyname><forenames>Marko</forenames></author></authors><title>Node mixing and group structure of complex software networks</title><categories>cs.SI cs.SE physics.soc-ph</categories><comments>26 pages, 14 figures, 11 tables</comments><journal-ref>Advs. Complex Syst. 17(7-8), 1450022 (2014)</journal-ref><doi>10.1142/S0219525914500222</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large software projects are among most sophisticated human-made systems
consisting of a network of interdependent parts. Past studies of software
systems from the perspective of complex networks have already led to notable
discoveries with different applications. Nevertheless, our comprehension of the
structure of software networks remains to be only partial. We here investigate
correlations or mixing between linked nodes and show that software networks
reveal dichotomous node degree mixing similar to that recently observed in
biological networks. We further show that software networks also reveal
characteristic clustering profiles and mixing. Hence, node mixing in software
networks significantly differs from that in, e.g., the Internet or social
networks. We explain the observed mixing through the presence of groups of
nodes with common linking pattern. More precisely, besides densely linked
groups known as communities, software networks also consist of disconnected
groups denoted modules, core/periphery structures and other. Moreover, groups
coincide with the intrinsic properties of the underlying software projects,
which promotes practical applications in software engineering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05082</identifier>
 <datestamp>2015-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05082</id><created>2015-02-17</created><updated>2015-08-01</updated><authors><author><keyname>Hosang</keyname><forenames>Jan</forenames></author><author><keyname>Benenson</keyname><forenames>Rodrigo</forenames></author><author><keyname>Doll&#xe1;r</keyname><forenames>Piotr</forenames></author><author><keyname>Schiele</keyname><forenames>Bernt</forenames></author></authors><title>What makes for effective detection proposals?</title><categories>cs.CV</categories><comments>TPAMI final version, duplicate proposals removed in experiments</comments><doi>10.1109/TPAMI.2015.2465908</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current top performing object detectors employ detection proposals to guide
the search for objects, thereby avoiding exhaustive sliding window search
across images. Despite the popularity and widespread use of detection
proposals, it is unclear which trade-offs are made when using them during
object detection. We provide an in-depth analysis of twelve proposal methods
along with four baselines regarding proposal repeatability, ground truth
annotation recall on PASCAL, ImageNet, and MS COCO, and their impact on DPM,
R-CNN, and Fast R-CNN detection performance. Our analysis shows that for object
detection improving proposal localisation accuracy is as important as improving
recall. We introduce a novel metric, the average recall (AR), which rewards
both high recall and good localisation and correlates surprisingly well with
detection performance. Our findings show common strengths and weaknesses of
existing methods, and provide insights and metrics for selecting and tuning
proposal methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05086</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05086</id><created>2015-02-17</created><updated>2015-10-15</updated><authors><author><keyname>Fulla</keyname><forenames>Peter</forenames></author><author><keyname>Zivny</keyname><forenames>Stanislav</forenames></author></authors><title>A Galois Connection for Weighted (Relational) Clones of Infinite Size</title><categories>cs.CC cs.DM</categories><acm-class>F.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Galois connection between clones and relational clones on a fixed finite
domain is one of the cornerstones of the so-called algebraic approach to the
computational complexity of non-uniform Constraint Satisfaction Problems
(CSPs). Cohen et al. established a Galois connection between finitely-generated
weighted clones and finitely-generated weighted relational clones [SICOMP'13],
and asked whether this connection holds in general. We answer this question in
the affirmative for weighted (relational) clones with real weights and show
that the complexity of the corresponding valued CSPs is preserved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05090</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05090</id><created>2015-02-17</created><authors><author><keyname>Pacchiano</keyname><forenames>Aldo</forenames></author><author><keyname>Williams</keyname><forenames>Oliver</forenames></author></authors><title>Real time clustering of time series using triangular potentials</title><categories>cs.LG</categories><comments>AIFU15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the problem of computing investment portfolio weightings we
investigate various methods of clustering as alternatives to traditional
mean-variance approaches. Such methods can have significant benefits from a
practical point of view since they remove the need to invert a sample
covariance matrix, which can suffer from estimation error and will almost
certainly be non-stationary. The general idea is to find groups of assets which
share similar return characteristics over time and treat each group as a single
composite asset. We then apply inverse volatility weightings to these new
composite assets. In the course of our investigation we devise a method of
clustering based on triangular potentials and we present associated theoretical
results as well as various examples based on synthetic data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05094</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05094</id><created>2015-02-17</created><authors><author><keyname>Stone</keyname><forenames>Christopher A.</forenames></author><author><keyname>O'Neill</keyname><forenames>Melissa E.</forenames></author><author><keyname>Bohr</keyname><forenames>Sonja A.</forenames></author><author><keyname>Cozzette</keyname><forenames>Adam M.</forenames></author><author><keyname>DeBlasio</keyname><forenames>M. Joe</forenames></author><author><keyname>Matsieva</keyname><forenames>Julia</forenames></author><author><keyname>Pernsteiner</keyname><forenames>Stuart A.</forenames></author><author><keyname>Schumer</keyname><forenames>Ari D.</forenames></author></authors><title>Observationally Cooperative Multithreading</title><categories>cs.PL</categories><acm-class>D.1.3; D.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite widespread interest in multicore computing, concur- rency models in
mainstream languages often lead to subtle, error-prone code.
  Observationally Cooperative Multithreading (OCM) is a new approach to
shared-memory parallelism. Programmers write code using the well-understood
cooperative (i.e., nonpreemptive) multithreading model for uniprocessors. OCM
then allows threads to run in parallel, so long as results remain consistent
with the cooperative model.
  Programmers benefit because they can reason largely sequentially. Remaining
interthread interactions are far less chaotic than in other models, permitting
easier reasoning and debugging. Programmers can also defer the choice of
concurrency-control mechanism (e.g., locks or transactions) until after they
have written their programs, at which point they can compare
concurrency-control strategies and choose the one that offers the best
performance. Implementers and researchers also benefit from the agnostic nature
of OCM -- it provides a level of abstraction to investigate, compare, and
combine a variety of interesting concurrency-control techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05096</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05096</id><created>2015-02-17</created><authors><author><keyname>Krishnasamy</keyname><forenames>Subhashini</forenames></author><author><keyname>Banerjee</keyname><forenames>Siddhartha</forenames></author><author><keyname>Shakkottai</keyname><forenames>Sanjay</forenames></author></authors><title>The Behavior of Epidemics under Bounded Susceptibility</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the sensitivity of epidemic behavior to a bounded
susceptibility constraint -- susceptible nodes are infected by their neighbors
via the regular SI/SIS dynamics, but subject to a cap on the infection rate.
Such a constraint is motivated by modern social networks, wherein messages are
broadcast to all neighbors, but attention spans are limited. Bounded
susceptibility also arises in distributed computing applications with download
bandwidth constraints, and in human epidemics under quarantine policies.
  Network epidemics have been extensively studied in literature; prior work
characterizes the graph structures required to ensure fast spreading under the
SI dynamics, and long lifetime under the SIS dynamics. In particular, these
conditions turn out to be meaningful for two classes of networks of practical
relevance -- dense, uniform (i.e., clique-like) graphs, and sparse, structured
(i.e., star-like) graphs. We show that bounded susceptibility has a surprising
impact on epidemic behavior in these graph families. For the SI dynamics,
bounded susceptibility has no effect on star-like networks, but dramatically
alters the spreading time in clique-like networks. In contrast, for the SIS
dynamics, clique-like networks are unaffected, but star-like networks exhibit a
sharp change in extinction times under bounded susceptibility.
  Our findings are useful for the design of disease-resistant networks and
infrastructure networks. More generally, they show that results for existing
epidemic models are sensitive to modeling assumptions in non-intuitive ways,
and suggest caution in directly using these as guidelines for real systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05100</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05100</id><created>2015-02-17</created><authors><author><keyname>Xu</keyname><forenames>Shouhuai</forenames></author></authors><title>Cybersecurity Dynamics</title><categories>cs.CR</categories><comments>2 pages</comments><journal-ref>HotSoS'2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the emerging field of {\em Cybersecurity Dynamics}, a candidate
foundation for the Science of Cybersecurity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05102</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05102</id><created>2015-02-17</created><authors><author><keyname>Xu</keyname><forenames>Shouhuai</forenames></author></authors><title>Emergent Behavior in Cybersecurity</title><categories>cs.CR</categories><comments>2 pages, HotSoS'2014 (2014 Symposium and Bootcamp on the Science of
  Security)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We argue that emergent behavior is inherent to cybersecurity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05106</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05106</id><created>2015-02-17</created><updated>2015-04-12</updated><authors><author><keyname>Rahman</keyname><forenames>Habibur</forenames></author><author><keyname>Roy</keyname><forenames>Senjuti Basu</forenames></author><author><keyname>Thirumuruganathan</keyname><forenames>Saravanan</forenames></author><author><keyname>Amer-Yahia</keyname><forenames>Sihem</forenames></author><author><keyname>Das</keyname><forenames>Gautam</forenames></author></authors><title>&quot;The Whole Is Greater Than the Sum of Its Parts&quot;: Optimization in
  Collaborative Crowdsourcing</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we initiate the investigation of optimization opportunities in
collaborative crowdsourcing. Many popular applications, such as collaborative
document editing, sentence translation, or citizen science resort to this
special form of human-based computing, where, crowd workers with appropriate
skills and expertise are required to form groups to solve complex tasks.
Central to any collaborative crowdsourcing process is the aspect of successful
collaboration among the workers, which, for the first time, is formalized and
then optimized in this work. Our formalism considers two main
collaboration-related human factors, affinity and upper critical mass,
appropriately adapted from organizational science and social theories. Our
contributions are (a) proposing a comprehensive model for collaborative
crowdsourcing optimization, (b) rigorous theoretical analyses to understand the
hardness of the proposed problems, (c) an array of efficient exact and
approximation algorithms with provable theoretical guarantees. Finally, we
present a detailed set of experimental results stemming from two real-world
collaborative crowdsourcing application us- ing Amazon Mechanical Turk, as well
as conduct synthetic data analyses on scalability and qualitative aspects of
our proposed algorithms. Our experimental results successfully demonstrate the
efficacy of our proposed solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05110</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05110</id><created>2015-02-17</created><updated>2015-05-29</updated><authors><author><keyname>Li</keyname><forenames>Mingqiang</forenames></author><author><keyname>Qin</keyname><forenames>Chuan</forenames></author><author><keyname>Lee</keyname><forenames>Patrick P. C.</forenames></author></authors><title>CDStore: Toward Reliable, Secure, and Cost-Efficient Cloud Storage via
  Convergent Dispersal</title><categories>cs.CR cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present CDStore, which disperses users' backup data across multiple clouds
and provides a unified multi-cloud storage solution with reliability, security,
and cost-efficiency guarantees. CDStore builds on an augmented secret sharing
scheme called convergent dispersal, which supports deduplication by using
deterministic content-derived hashes as inputs to secret sharing. We present
the design of CDStore, and in particular, describe how it combines convergent
dispersal with two-stage deduplication to achieve both bandwidth and storage
savings and be robust against side-channel attacks. We evaluate the performance
of our CDStore prototype using real-world workloads on LAN and commercial cloud
testbeds. Our cost analysis also demonstrates that CDStore achieves a monetary
cost saving of 70% over a baseline cloud storage solution using
state-of-the-art secret sharing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05111</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05111</id><created>2015-02-17</created><authors><author><keyname>Li</keyname><forenames>Fangfang</forenames></author><author><keyname>Xu</keyname><forenames>Guandong</forenames></author><author><keyname>Cao</keyname><forenames>Longbing</forenames></author></authors><title>CSAL: Self-adaptive Labeling based Clustering Integrating Supervised
  Learning on Unlabeled Data</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Supervised classification approaches can predict labels for unknown data
because of the supervised training process. The success of classification is
heavily dependent on the labeled training data. Differently, clustering is
effective in revealing the aggregation property of unlabeled data, but the
performance of most clustering methods is limited by the absence of labeled
data. In real applications, however, it is time-consuming and sometimes
impossible to obtain labeled data. The combination of clustering and
classification is a promising and active approach which can largely improve the
performance. In this paper, we propose an innovative and effective clustering
framework based on self-adaptive labeling (CSAL) which integrates clustering
and classification on unlabeled data. Clustering is first employed to partition
data and a certain proportion of clustered data are selected by our proposed
labeling approach for training classifiers. In order to refine the trained
classifiers, an iterative process of Expectation-Maximization algorithm is
devised into the proposed clustering framework CSAL. Experiments are conducted
on publicly data sets to test different combinations of clustering algorithms
and classification models as well as various training data labeling methods.
The experimental results show that our approach along with the self-adaptive
method outperforms other methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05113</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05113</id><created>2015-02-17</created><authors><author><keyname>Liu</keyname><forenames>Jiajun</forenames></author><author><keyname>Zhao</keyname><forenames>Kun</forenames></author><author><keyname>Kusy</keyname><forenames>Brano</forenames></author><author><keyname>Wen</keyname><forenames>Ji-rong</forenames></author><author><keyname>Jurdak</keyname><forenames>Raja</forenames></author></authors><title>Temporal Embedding in Convolutional Neural Networks for Robust Learning
  of Abstract Snippets</title><categories>cs.LG cs.NE</categories><comments>a submission to kdd 15'</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The prediction of periodical time-series remains challenging due to various
types of data distortions and misalignments. Here, we propose a novel model
called Temporal embedding-enhanced convolutional neural Network (TeNet) to
learn repeatedly-occurring-yet-hidden structural elements in periodical
time-series, called abstract snippets, for predicting future changes. Our model
uses convolutional neural networks and embeds a time-series with its potential
neighbors in the temporal domain for aligning it to the dominant patterns in
the dataset. The model is robust to distortions and misalignments in the
temporal domain and demonstrates strong prediction power for periodical
time-series.
  We conduct extensive experiments and discover that the proposed model shows
significant and consistent advantages over existing methods on a variety of
data modalities ranging from human mobility to household power consumption
records. Empirical results indicate that the model is robust to various factors
such as number of samples, variance of data, numerical ranges of data etc. The
experiments also verify that the intuition behind the model can be generalized
to multiple data types and applications and promises significant improvement in
prediction performances across the datasets studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05131</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05131</id><created>2015-02-18</created><authors><author><keyname>Wang</keyname><forenames>Ju-Chiang</forenames></author><author><keyname>Yang</keyname><forenames>Yi-Hsuan</forenames></author><author><keyname>Wang</keyname><forenames>Hsin-Min</forenames></author></authors><title>Affective Music Information Retrieval</title><categories>cs.IR</categories><comments>40 pages, 18 figures, 5 tables, author version</comments><acm-class>H.3.3; H.5.5</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Much of the appeal of music lies in its power to convey emotions/moods and to
evoke them in listeners. In consequence, the past decade witnessed a growing
interest in modeling emotions from musical signals in the music information
retrieval (MIR) community. In this article, we present a novel generative
approach to music emotion modeling, with a specific focus on the
valence-arousal (VA) dimension model of emotion. The presented generative
model, called \emph{acoustic emotion Gaussians} (AEG), better accounts for the
subjectivity of emotion perception by the use of probability distributions.
Specifically, it learns from the emotion annotations of multiple subjects a
Gaussian mixture model in the VA space with prior constraints on the
corresponding acoustic features of the training music pieces. Such a
computational framework is technically sound, capable of learning in an online
fashion, and thus applicable to a variety of applications, including
user-independent (general) and user-dependent (personalized) emotion
recognition and emotion-based music retrieval. We report evaluations of the
aforementioned applications of AEG on a larger-scale emotion-annotated corpora,
AMG1608, to demonstrate the effectiveness of AEG and to showcase how
evaluations are conducted for research on emotion-based MIR. Directions of
future work are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05134</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05134</id><created>2015-02-18</created><updated>2015-08-18</updated><authors><author><keyname>Wang</keyname><forenames>Jingbin</forenames></author><author><keyname>Zhou</keyname><forenames>Yihua</forenames></author><author><keyname>Duan</keyname><forenames>Kanghong</forenames></author><author><keyname>Wang</keyname><forenames>Jim Jing-Yan</forenames></author><author><keyname>Bensmail</keyname><forenames>Halima</forenames></author></authors><title>Supervised cross-modal factor analysis for multiple modal data
  classification</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the problem of learning from multiple modal data for
purpose of document classification. In this problem, each document is composed
two different modals of data, i.e., an image and a text. Cross-modal factor
analysis (CFA) has been proposed to project the two different modals of data to
a shared data space, so that the classification of a image or a text can be
performed directly in this space. A disadvantage of CFA is that it has ignored
the supervision information. In this paper, we improve CFA by incorporating the
supervision information to represent and classify both image and text modals of
documents. We project both image and text data to a shared data space by factor
analysis, and then train a class label predictor in the shared space to use the
class label information. The factor analysis parameter and the predictor
parameter are learned jointly by solving one single objective function. With
this objective function, we minimize the distance between the projections of
image and text of the same document, and the classification error of the
projection measured by hinge loss function. The objective function is optimized
by an alternate optimization strategy in an iterative algorithm. Experiments in
two different multiple modal document data sets show the advantage of the
proposed algorithm over other CFA methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05137</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05137</id><created>2015-02-18</created><updated>2015-04-11</updated><authors><author><keyname>Sattar</keyname><forenames>Hosnieh</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Sabine</forenames></author><author><keyname>Fritz</keyname><forenames>Mario</forenames></author><author><keyname>Bulling</keyname><forenames>Andreas</forenames></author></authors><title>Prediction of Search Targets From Fixations in Open-World Settings</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Previous work on predicting the target of visual search from human fixations
only considered closed-world settings in which training labels are available
and predictions are performed for a known set of potential targets. In this
work we go beyond the state of the art by studying search target prediction in
an open-world setting in which we no longer assume that we have fixation data
to train for the search targets. We present a dataset containing fixation data
of 18 users searching for natural images from three image categories within
synthesised image collages of about 80 images. In a closed-world baseline
experiment we show that we can predict the correct target image out of a
candidate set of five images. We then present a new problem formulation for
search target prediction in the open-world setting that is based on learning
compatibilities between fixations and potential targets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05142</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05142</id><created>2015-02-18</created><authors><author><keyname>Martalo'</keyname><forenames>Marco</forenames></author><author><keyname>Raheli</keyname><forenames>Riccardo</forenames></author></authors><title>Models, Statistics, and Rates of Binary Correlated Sources</title><categories>cs.IT math.IT</categories><comments>submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses and analyzes various models of binary correlated
sources, which may be relevant in several distributed communication scenarios.
These models are statistically characterized in terms of joint Probability Mass
Function (PMF) and covariance. Closed-form expressions for the joint entropy of
the sources are also presented. The asymptotic entropy rate for very large
number of sources is shown to converge to a common limit for all the considered
models. This fact generalizes recent results on the information-theoretic
performance limit of communication schemes which exploit the correlation among
sources at the receiver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05145</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05145</id><created>2015-02-18</created><authors><author><keyname>Ivanova</keyname><forenames>Inga</forenames></author><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>Knowledge-generating Efficiency in Innovation Systems: The relation
  between structural and temporal effects</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using time series of US patents per million inhabitants, knowledge-generating
cycles can be distinguished. These cycles partly coincide with Kondratieff long
waves. The changes in the slopes between them indicate discontinuities in the
knowledge-generating paradigms. The knowledge-generating paradigms can be
modeled in terms of interacting dimensions (for example, in
university-industry-government relations) that set limits to the maximal
efficiency of innovation systems. The maximum values of the parameters in the
model are of the same order as the regression coefficients of the empirical
waves. The mechanism of the increase in the dimensionality is specified as
self-organization which leads to the breaking of existing relations into the
more diversified structure of a fractal-like network. This breaking can be
modeled in analogy to 2D and 3D (Koch) snowflakes. The boost of knowledge
generation leads to newly emerging technologies that can be expected to be more
diversified and show shorter life cycles than before. Time spans of the
knowledge-generating cycles can also be analyzed in terms of Fibonacci numbers.
This perspective allows for forecasting expected dates of future possible
paradigm changes. In terms of policy implications, this suggests a shift in
focus from the manufacturing technologies to developing new organizational
technologies and formats of human interactions
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05147</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05147</id><created>2015-02-18</created><updated>2015-05-01</updated><authors><author><keyname>Grellois</keyname><forenames>Charles</forenames></author><author><keyname>Melli&#xe8;s</keyname><forenames>Paul-Andr&#xe9;</forenames></author></authors><title>Finitary semantics of linear logic and higher-order model-checking</title><categories>cs.LO</categories><comments>12 pages, submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we explain how the connection between higher-order
model-checking and linear logic recently exhibited by the authors leads to a
new and conceptually enlightening proof of the selection problem originally
established by Carayol and Serre using collapsible pushdown automata. The main
idea is to start from an infinitary and colored relational semantics of the
lambdaY-calculus already formulated, and to replace it by its finitary
counterpart based on finite prime-algebraic lattices. Given a higher-order
recursion scheme G, the finiteness of its interpretation in the model enables
us to associate to any MSO formula phi a new higher-order recursion scheme
G_phi resolving the selection problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05149</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05149</id><created>2015-02-18</created><authors><author><keyname>Cazabet</keyname><forenames>Remy</forenames></author><author><keyname>Chawuthai</keyname><forenames>Rathachai</forenames></author><author><keyname>Takeda</keyname><forenames>Hideaki</forenames></author></authors><title>Using multiple-criteria methods to evaluate community partitions</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community detection is one of the most studied problems on complex networks.
Although hundreds of methods have been proposed so far, there is still no
universally accepted formal definition of what is a good community. As a
consequence, the problem of the evaluation and the comparison of the quality of
the solutions produced by these algorithms is still an open question, despite
constant progress on the topic. In this article, we investigate how using a
multi-criteria evaluation can solve some of the existing problems of community
evaluation, in particular the question of multiple equally-relevant solutions
of different granularity. After exploring several approaches, we introduce a
new quality function, called MDensity, and propose a method that can be related
both to a widely used community detection metric, the Modularity, and to the
Precision/Recall approach, ubiquitous in information retrieval.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05153</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05153</id><created>2015-02-18</created><authors><author><keyname>Chakraborty</keyname><forenames>Sandip</forenames></author><author><keyname>Dalal</keyname><forenames>Jiban</forenames></author><author><keyname>Sarkar</keyname><forenames>Bikramjit</forenames></author><author><keyname>Mukherjee</keyname><forenames>Debaprasad</forenames></author></authors><title>Neural Synchronization based Secret Key Exchange over Public Channels: A
  survey</title><categories>cs.CR</categories><comments>8 pages, 2 figures, Review article</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exchange of secret keys over public channels based on neural synchronization
using a variety of learning rules offer an appealing alternative to number
theory based cryptography algorithms. Though several forms of attacks are
possible on this neural protocol e.g. geometric, genetic and majority attacks,
our survey finds that deterministic algorithms that synchronize with the
end-point networks have high time complexity, while probabilistic and
population-based algorithms have demonstrated ability to decode the key during
its exchange over the public channels. Our survey also discusses queries,
heuristics, erroneous information, group key exchange, synaptic depths, etc,
that have been proposed to increase the time complexity of algorithmic
interception or decoding of the key during exchange. The Tree Parity Machine
and its variants, neural networks with tree topologies incorporating parity
checking of state bits, appear to be one of the most secure and stable models
of the end-point networks. Our survey also mentions some noteworthy studies on
neural networks applied to other necessary aspects of cryptography. We conclude
that discovery of neural architectures with very high synchronization speed,
and designing the encoding and entropy of the information exchanged during
mutual learning, and design of extremely sensitive chaotic maps for
transformation of synchronized states of the networks to chaotic encryption
keys, are the primary issues in this field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05156</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05156</id><created>2015-02-18</created><authors><author><keyname>Blagus</keyname><forenames>Neli</forenames></author><author><keyname>&#x160;ubelj</keyname><forenames>Lovro</forenames></author><author><keyname>Bajec</keyname><forenames>Marko</forenames></author></authors><title>Assessing the effectiveness of real-world network simplification</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many real-world networks are large, complex and thus hard to understand,
analyze or visualize. The data about networks is not always complete, their
structure may be hidden or they change quickly over time. Therefore,
understanding how incomplete system differs from complete one is crucial. In
this paper, we study the changes in networks under simplification (i.e.,
reduction in size). We simplify 30 real-world networks with six simplification
methods and analyze the similarity between original and simplified networks
based on preservation of several properties, for example degree distribution,
clustering coefficient, betweenness centrality, density and degree mixing. We
propose an approach for assessing the effectiveness of simplification process
to define the most appropriate size of simplified networks and to determine the
method, which preserves the most properties of original networks. The results
reveal the type and size of original networks do not influence the changes of
networks under simplification process, while the size of simplified networks
does. Moreover, we investigate the performance of simplification methods when
the size of simplified networks is 10% of the original networks. The findings
show that sampling methods outperform merging ones, particularly random node
selection based on degree and breadth-first sampling perform the best.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05167</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05167</id><created>2015-02-18</created><authors><author><keyname>Shakil</keyname><forenames>Kashish Ara</forenames></author><author><keyname>Anis</keyname><forenames>Shadma</forenames></author><author><keyname>Alam</keyname><forenames>Mansaf</forenames></author></authors><title>Dengue disease prediction using weka data mining tool</title><categories>cs.CY cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dengue is a life threatening disease prevalent in several developed as well
as developing countries like India.In this paper we discuss various algorithm
approaches of data mining that have been utilized for dengue disease
prediction. Data mining is a well known technique used by health organizations
for classification of diseases such as dengue, diabetes and cancer in
bioinformatics research. In the proposed approach we have used WEKA with 10
cross validation to evaluate data and compare results. Weka has an extensive
collection of different machine learning and data mining algorithms. In this
paper we have firstly classified the dengue data set and then compared the
different data mining techniques in weka through Explorer, knowledge flow and
Experimenter interfaces. Furthermore in order to validate our approach we have
used a dengue dataset with 108 instances but weka used 99 rows and 18
attributes to determine the prediction of disease and their accuracy using
classifications of different algorithms to find out the best performance. The
main objective of this paper is to classify data and assist the users in
extracting useful information from data and easily identify a suitable
algorithm for accurate predictive model from it. From the findings of this
paper it can be concluded that Na\&quot;ive Bayes and J48 are the best performance
algorithms for classified accuracy because they achieved maximum accuracy= 100%
with 99 correctly classified instances, maximum ROC = 1, had least mean
absolute error and it took minimum time for building this model through
Explorer and Knowledge flow results
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05168</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05168</id><created>2015-02-18</created><authors><author><keyname>Vaidyanathan</keyname><forenames>Rekha</forenames></author><author><keyname>Das</keyname><forenames>Sujoy</forenames></author><author><keyname>Srivastava</keyname><forenames>Namita</forenames></author></authors><title>Query Expansion Strategy based on Pseudo Relevance Feedback and Term
  Weight Scheme for Monolingual Retrieval</title><categories>cs.IR</categories><msc-class>68</msc-class><acm-class>H.3.3</acm-class><journal-ref>International Journal of Computer Applications 105(8):1-6,
  November 2014</journal-ref><doi>10.5120/18394-9646</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Query Expansion using Pseudo Relevance Feedback is a useful and a popular
technique for reformulating the query. In our proposed query expansion method,
we assume that relevant information can be found within a document near the
central idea. The document is normally divided into sections, paragraphs and
lines. The proposed method tries to extract keywords that are closer to the
central theme of the document. The expansion terms are obtained by
equi-frequency partition of the documents obtained from pseudo relevance
feedback and by using tf-idf scores. The idf factor is calculated for number of
partitions in documents. The group of words for query expansion is selected
using the following approaches: the highest score, average score and a group of
words that has maximum number of keywords. As each query behaved differently
for different methods, the effect of these methods in selecting the words for
query expansion is investigated. From this initial study, we extend the
experiment to develop a rule-based statistical model that automatically selects
the best group of words incorporating the tf-idf scoring and the 3 approaches
explained here, in the future. The experiments were performed on FIRE 2011
Adhoc Hindi and English test collections on 50 queries each, using Terrier as
retrieval engine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05179</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05179</id><created>2015-02-18</created><authors><author><keyname>Shchurov</keyname><forenames>Andrey A.</forenames></author><author><keyname>Marik</keyname><forenames>Radek</forenames></author></authors><title>Dependability Tests Selection Based on the Concept of Layered Networks</title><categories>cs.DC cs.SE</categories><comments>10 pages, 3 figures, 5 tables</comments><journal-ref>International Journal of Scientific and Engineering Research
  (IJSER) V6(1):1165-1174, January 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, the consequences of failure and downtime of distributed systems
have become more and more severe. As an obvious solution, these systems
incorporate protection mechanisms to tolerate faults that could cause systems
failures and system dependability must be validated to ensure that protection
mechanisms have been implemented correctly and the system will provide the
desired level of reliable service. This paper presents a systematic approach
for identifying (1) characteristic sets of critical system elements for
dependability testing (single points of failure and recovery groups) based on
the concept of layered networks; and (2) the most important combinations of
components from each recovery group based on a combinatorial technique. Based
on these combinations, we determine a set of test templates to be performed to
demonstrate system dependability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05183</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05183</id><created>2015-02-18</created><authors><author><keyname>Dolev</keyname><forenames>Shlomi</forenames></author><author><keyname>Georgiou</keyname><forenames>Chryssis</forenames></author><author><keyname>Marcoullis</keyname><forenames>Ioannis</forenames></author><author><keyname>Schiller</keyname><forenames>Elad Michael</forenames></author></authors><title>Practically Stabilizing Virtual Synchrony</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Virtual synchrony is an important abstraction that is proven to be extremely
useful when implemented over asynchronous, typically large, message-passing
distributed systems. Fault tolerant design is a key criterion for the success
of such implementations. This is because large distributed systems can be
highly available as long as they do not depend on the full operational status
of every system participant. That is, when using redundancy in numbers to
overcome non-optimal behavior of participants and to gain global robustness and
high availability.
  Self-stabilizing systems can tolerate transient faults that drive the system
to an arbitrary unpredicted configuration. Such systems automatically regain
consistency from any such arbitrary configuration, and then produce the desired
system behavior. Practically self-stabilizing systems ensure the desired system
behavior for practically infinite number of successive steps e.g., $2^{64}$
steps.
  We present the first practically self-stabilizing virtual synchrony
algorithm. The algorithm is a combination of several new techniques that may be
of independent interest. In particular, we present a new counter algorithm that
establishes an efficient practically unbounded counter, that in turn can be
directly used to implement a self-stabilizing Multiple-Writer Multiple-Reader
(MWMR) register emulation. Other components include self-stabilizing group
membership, self-stabilizing multicast, and self-stabilizing emulation of
replicated state machine. As we base the replicated state machine
implementation on virtual synchrony, rather than consensus, the system
progresses in more extreme asynchronous executions with relation to
consensus-based replicated state machine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05186</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05186</id><created>2015-02-18</created><authors><author><keyname>James</keyname><forenames>Joshua I.</forenames></author><author><keyname>Lopez-Fernandez</keyname><forenames>Alejandra</forenames></author><author><keyname>Gladyshev</keyname><forenames>Pavel</forenames></author></authors><title>Measuring Accuracy of Automated Parsing and Categorization Tools and
  Processes in Digital Investigations</title><categories>cs.CY</categories><comments>17 pages, 2 appendices, 1 figure, 5th International Conference on
  Digital Forensics and Cyber Crime; Digital Forensics and Cyber Crime, pp.
  147-169, 2014</comments><doi>10.1007/978-3-319-14289-0_11</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This work presents a method for the measurement of the accuracy of evidential
artifact extraction and categorization tasks in digital forensic
investigations. Instead of focusing on the measurement of accuracy and errors
in the functions of digital forensic tools, this work proposes the application
of information retrieval measurement techniques that allow the incorporation of
errors introduced by tools and analysis processes. This method uses a `gold
standard' that is the collection of evidential objects determined by a digital
investigator from suspect data with an unknown ground truth. This work proposes
that the accuracy of tools and investigation processes can be evaluated
compared to the derived gold standard using common precision and recall values.
Two example case studies are presented showing the measurement of the accuracy
of automated analysis tools as compared to an in-depth analysis by an expert.
It is shown that such measurement can allow investigators to determine changes
in accuracy of their processes over time, and determine if such a change is
caused by their tools or knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05191</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05191</id><created>2015-02-18</created><authors><author><keyname>James</keyname><forenames>Joshua I.</forenames></author><author><keyname>Shosha</keyname><forenames>Ahmed F.</forenames></author><author><keyname>Gladyshev</keyname><forenames>Pavel</forenames></author></authors><title>Determining Training Needs for Cloud Infrastructure Investigations using
  I-STRIDE</title><categories>cs.CY</categories><comments>13 pages, 3 figures, 3 tables, 5th International Conference on
  Digital Forensics and Cyber Crime; Digital Forensics and Cyber Crime, pp.
  223-236, 2014</comments><doi>10.1007/978-3-319-14289-0_15</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  As more businesses and users adopt cloud computing services, security
vulnerabilities will be increasingly found and exploited. There are many
technological and political challenges where investigation of potentially
criminal incidents in the cloud are concerned. Security experts, however, must
still be able to acquire and analyze data in a methodical, rigorous and
forensically sound manner. This work applies the STRIDE asset-based risk
assessment method to cloud computing infrastructure for the purpose of
identifying and assessing an organization's ability to respond to and
investigate breaches in cloud computing environments. An extension to the
STRIDE risk assessment model is proposed to help organizations quickly respond
to incidents while ensuring acquisition and integrity of the largest amount of
digital evidence possible. Further, the proposed model allows organizations to
assess the needs and capacity of their incident responders before an incident
occurs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05197</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05197</id><created>2015-02-18</created><updated>2016-01-27</updated><authors><author><keyname>Tozza</keyname><forenames>Silvia</forenames></author><author><keyname>Falcone</keyname><forenames>Maurizio</forenames></author></authors><title>Analysis and approximation of some Shape-from-Shading models for
  non-Lambertian surfaces</title><categories>math.NA cs.CV cs.NA math.AP</categories><comments>Accepted version to Journal of Mathematical Imaging and Vision, 57
  pages</comments><report-no>Roma01.Math.NA</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The reconstruction of a 3D object or a scene is a classical inverse problem
in Computer Vision. In the case of a single image this is called the
Shape-from-Shading (SfS) problem and it is known to be ill-posed even in a
simplified version like the vertical light source case. A huge number of works
deals with the orthographic SfS problem based on the Lambertian reflectance
model, the most common and simplest model which leads to an eikonal type
equation when the light source is on the vertical axis. In this paper we want
to study non-Lambertian models since they are more realistic and suitable
whenever one has to deal with different kind of surfaces, rough or specular. We
will present a unified mathematical formulation of some popular orthographic
non-Lambertian models, considering vertical and oblique light directions as
well as different viewer positions. These models lead to more complex
stationary nonlinear partial differential equations of Hamilton-Jacobi type
which can be regarded as the generalization of the classical eikonal equation
corresponding to the Lambertian case. However, all the equations corresponding
to the models considered here (Oren-Nayar and Phong) have a similar structure
so we can look for weak solutions to this class in the viscosity solution
framework. Via this unified approach, we are able to develop a semi-Lagrangian
approximation scheme for the Oren-Nayar and the Phong model and to prove a
general convergence result. Numerical simulations on synthetic and real images
will illustrate the effectiveness of this approach and the main features of the
scheme, also comparing the results with previous results in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05204</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05204</id><created>2015-02-18</created><authors><author><keyname>Chan</keyname><forenames>Timothy M.</forenames></author><author><keyname>Lewenstein</keyname><forenames>Moshe</forenames></author></authors><title>Clustered Integer 3SUM via Additive Combinatorics</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a collection of new results on problems related to 3SUM,
including:
  1. The first truly subquadratic algorithm for
  $\ \ \ \ \ $ 1a. computing the (min,+) convolution for monotone increasing
sequences with integer values bounded by $O(n)$,
  $\ \ \ \ \ $1b. solving 3SUM for monotone sets in 2D with integer coordinates
bounded by $O(n)$, and
  $\ \ \ \ \ $1c. preprocessing a binary string for histogram indexing (also
called jumbled indexing).
  The running time is:
$O(n^{(9+\sqrt{177})/12}\,\textrm{polylog}\,n)=O(n^{1.859})$ with
randomization, or $O(n^{1.864})$ deterministically. This greatly improves the
previous $n^2/2^{\Omega(\sqrt{\log n})}$ time bound obtained from Williams'
recent result on all-pairs shortest paths [STOC'14], and answers an open
question raised by several researchers studying the histogram indexing problem.
  2. The first algorithm for histogram indexing for any constant alphabet size
that achieves truly subquadratic preprocessing time and truly sublinear query
time.
  3. A truly subquadratic algorithm for integer 3SUM in the case when the given
set can be partitioned into $n^{1-\delta}$ clusters each covered by an interval
of length $n$, for any constant $\delta&gt;0$.
  4. An algorithm to preprocess any set of $n$ integers so that subsequently
3SUM on any given subset can be solved in $O(n^{13/7}\,\textrm{polylog}\,n)$
time.
  All these results are obtained by a surprising new technique, based on the
Balog--Szemer\'edi--Gowers Theorem from additive combinatorics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05209</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05209</id><created>2015-02-18</created><updated>2015-03-02</updated><authors><author><keyname>Cruz-Filipe</keyname><forenames>Lu&#xed;s</forenames></author><author><keyname>Schneider-Kamp</keyname><forenames>Peter</forenames></author></authors><title>Formalizing Size-Optimal Sorting Networks: Extracting a Certified Proof
  Checker</title><categories>cs.LO</categories><comments>IMADA-preprint-cs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since the proof of the four color theorem in 1976, computer-generated proofs
have become a reality in mathematics and computer science. During the last
decade, we have seen formal proofs using verified proof assistants being used
to verify the validity of such proofs.
  In this paper, we describe a formalized theory of size-optimal sorting
networks. From this formalization we extract a certified checker that
successfully verifies computer-generated proofs of optimality on up to 8
inputs. The checker relies on an untrusted oracle to shortcut the search for
witnesses on more than 1.6 million NP-complete subproblems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05212</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05212</id><created>2015-02-18</created><authors><author><keyname>Ciocca</keyname><forenames>Gianluigi</forenames></author><author><keyname>Napoletano</keyname><forenames>Paolo</forenames></author><author><keyname>Schettini</keyname><forenames>Raimondo</forenames></author></authors><title>IAT - Image Annotation Tool: Manual</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The annotation of image and video data of large datasets is a fundamental
task in multimedia information retrieval and computer vision applications. In
order to support the users during the image and video annotation process,
several software tools have been developed to provide them with a graphical
environment which helps drawing object contours, handling tracking information
and specifying object metadata. Here we introduce a preliminary version of the
image annotation tools developed at the Imaging and Vision Laboratory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05213</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05213</id><created>2015-02-18</created><authors><author><keyname>Mukherjee</keyname><forenames>Sankar</forenames></author><author><keyname>Mandal</keyname><forenames>Shyamal Kumar Das</forenames></author></authors><title>F0 Modeling In Hmm-Based Speech Synthesis System Using Deep Belief
  Network</title><categories>cs.LG cs.NE</categories><comments>OCOCOSDA 2014</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In recent years multilayer perceptrons (MLPs) with many hid- den layers Deep
Neural Network (DNN) has performed sur- prisingly well in many speech tasks,
i.e. speech recognition, speaker verification, speech synthesis etc. Although
in the context of F0 modeling these techniques has not been ex- ploited
properly. In this paper, Deep Belief Network (DBN), a class of DNN family has
been employed and applied to model the F0 contour of synthesized speech which
was generated by HMM-based speech synthesis system. The experiment was done on
Bengali language. Several DBN-DNN architectures ranging from four to seven
hidden layers and up to 200 hid- den units per hidden layer was presented and
evaluated. The results were compared against clustering tree techniques pop-
ularly found in statistical parametric speech synthesis. We show that from
textual inputs DBN-DNN learns a high level structure which in turn improves F0
contour in terms of ob- jective and subjective tests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05216</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05216</id><created>2015-02-16</created><authors><author><keyname>Latkin</keyname><forenames>Evgeny</forenames></author></authors><title>Twofold exp and log</title><categories>cs.MS</categories><comments>Experimental code and tests at &quot;twofolds&quot; project Web site:
  https://sites.google.com/site/yevgenylatkin/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article is about twofold arithmetic. Here I introduce algorithms and
experimental code for twofold variant of C/C++ standard functions exp() and
log(), and expm1() and log1p(). Twofold function $y_0+y_1 \approx f(x_0+x_1)$
is nearly 2x-precise so can assess accuracy of standard one. Performance allows
assessing on-fly: twofold texp() over double is ~10x times faster than expq()
by GNU quadmath.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05222</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05222</id><created>2015-02-18</created><updated>2015-07-24</updated><authors><author><keyname>Kontogiannis</keyname><forenames>Spyros</forenames></author><author><keyname>Wagner</keyname><forenames>Dorothea</forenames></author><author><keyname>Zaroliagis</keyname><forenames>Christos</forenames></author></authors><title>Hierarchical Oracles for Time-Dependent Networks</title><categories>cs.DS</categories><msc-class>05C85, 05C12, 68Q25, 68W40, 68W25</msc-class><acm-class>F.2.2; I.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computing min-cost paths in time-dependent networks whose arcs obey
continuous, piecewise-linear, periodic, FIFO-abiding, arc-cost functions of the
actual time of usage of each arc is hard, since it has space-complexity of
$(1+K^*)\cdot n^{\Theta(\log n)}$, where $n$ is the number of vertices and
$K^*$ is the number of concavity-spoiling breakpoints in the arc-cost
functions. A main challenge is to provide oracles with subquadratic
preprocessing space and time independent of $K^*$, which in general may be
$\Theta(n)$, and query-response times sublinear, not only in the worst-case
(i.e., in $n$), but also in the Dijkstra Rank $\Gamma$ (number of settled
vertices using Dijkstra's algorithm until the destination is settled). We
address positively the aforementioned challenge, by providing: (i) A novel
efficient algorithm (TRAP) for constructing $(1+\epsilon)$-summaries of the
min-cost functions, for an arbitrary origin. TRAP assures that the constructed
functions have a succinct representation independent of $K^*$. (ii) The FLAT
oracle, which combines TRAP along with another one-to-all approximation
algorithm, to construct summaries from randomly selected landmarks towards all
reachable destinations. The preprocessing space is $O(n^{2-\delta}
polylog(n))$, for some $\delta\in(0,1)$ and independent of $K^*$, while the
query-time is $O(n^{b})$ for some $b\in(0,1)$. (iii) The HORN oracle, which
organises a geometrically decreasing hierarchy of landmarks, from local
landmarks possessing summaries only for small neighborhoods of destinations
around them, up to global landmarks possessing summaries for all reachable
vertices. The time and space requirements of HORN are subquadratic. We propose
a novel query algorithm guaranteeing query-time $O(\Gamma^{b})$, for some
$b\in(0,1)$. Preliminary experiments on real-world networks demonstrate an
excellent performance of the new oracles in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05224</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05224</id><created>2015-02-18</created><updated>2015-02-25</updated><authors><author><keyname>Gu</keyname><forenames>Yun</forenames></author><author><keyname>Xue</keyname><forenames>Haoyang</forenames></author><author><keyname>Yang</keyname><forenames>Jie</forenames></author></authors><title>Cross-Modality Hashing with Partial Correspondence</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning a hashing function for cross-media search is very desirable due to
its low storage cost and fast query speed. However, the data crawled from
Internet cannot always guarantee good correspondence among different modalities
which affects the learning for hashing function. In this paper, we focus on
cross-modal hashing with partially corresponded data. The data without full
correspondence are made in use to enhance the hashing performance. The
experiments on Wiki and NUS-WIDE datasets demonstrates that the proposed method
outperforms some state-of-the-art hashing approaches with fewer correspondence
information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05241</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05241</id><created>2015-02-18</created><authors><author><keyname>Dirnberger</keyname><forenames>Michael</forenames></author><author><keyname>Neumann</keyname><forenames>Adrian</forenames></author><author><keyname>Kehl</keyname><forenames>Tim</forenames></author></authors><title>NEFI: Network Extraction From Images</title><categories>cs.CV cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Networks and network-like structures are amongst the central building blocks
of many technological and biological systems. Given a mathematical graph
representation of a network, methods from graph theory enable a precise
investigation of its properties. Software for the analysis of graphs is widely
available and has been applied to graphs describing large scale networks such
as social networks, protein-interaction networks, etc. In these applications,
graph acquisition, i.e., the extraction of a mathematical graph from a network,
is relatively simple. However, for many network-like structures, e.g. leaf
venations, slime molds and mud cracks, data collection relies on images where
graph extraction requires domain-specific solutions or even manual. Here we
introduce Network Extraction From Images, NEFI, a software tool that
automatically extracts accurate graphs from images of a wide range of networks
originating in various domains. While there is previous work on graph
extraction from images, theoretical results are fully accessible only to an
expert audience and ready-to-use implementations for non-experts are rarely
available or insufficiently documented. NEFI provides a novel platform allowing
practitioners from many disciplines to easily extract graph representations
from images by supplying flexible tools from image processing, computer vision
and graph theory bundled in a convenient package. Thus, NEFI constitutes a
scalable alternative to tedious and error-prone manual graph extraction and
special purpose tools. We anticipate NEFI to enable the collection of larger
datasets by reducing the time spent on graph extraction. The analysis of these
new datasets may open up the possibility to gain new insights into the
structure and function of various types of networks. NEFI is open source and
available http://nefi.mpi-inf.mpg.de.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05243</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05243</id><created>2015-02-17</created><updated>2015-08-29</updated><authors><author><keyname>Gangopadhyay</keyname><forenames>Aalok</forenames></author><author><keyname>Tripathi</keyname><forenames>Shivam Mani</forenames></author><author><keyname>Jindal</keyname><forenames>Ishan</forenames></author><author><keyname>Raman</keyname><forenames>Shanmuganathan</forenames></author></authors><title>SA-CNN: Dynamic Scene Classification using Convolutional Neural Networks</title><categories>cs.CV</categories><acm-class>I.5.4; I.4.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The task of classifying videos of natural dynamic scenes into appropriate
classes has gained lot of attention in recent years. The problem especially
becomes challenging when the camera used to capture the video is dynamic. In
this paper, we analyse the performance of statistical aggregation (SA)
techniques on various pre-trained convolutional neural network(CNN) models to
address this problem. The proposed approach works by extracting CNN activation
features for a number of frames in a video and then uses an aggregation scheme
in order to obtain a robust feature descriptor for the video. We show through
results that the proposed approach performs better than the-state-of-the arts
for the Maryland and YUPenn dataset. The final descriptor obtained is powerful
enough to distinguish among dynamic scenes and is even capable of addressing
the scenario where the camera motion is dominant and the scene dynamics are
complex. Further, this paper shows an extensive study on the performance of
various aggregation methods and their combinations. We compare the proposed
approach with other dynamic scene classification algorithms on two publicly
available datasets - Maryland and YUPenn to demonstrate the superior
performance of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05256</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05256</id><created>2015-02-18</created><authors><author><keyname>Gloor</keyname><forenames>Peter</forenames></author><author><keyname>De Boer</keyname><forenames>Patrick</forenames></author><author><keyname>Lo</keyname><forenames>Wei</forenames></author><author><keyname>Wagner</keyname><forenames>Stefan</forenames></author><author><keyname>Nemoto</keyname><forenames>Keiichi</forenames></author><author><keyname>Fuehres</keyname><forenames>Hauke</forenames></author></authors><title>Cultural Anthropology Through the Lens of Wikipedia - A Comparison of
  Historical Leadership Networks in the English, Chinese, Japanese and German
  Wikipedia</title><categories>cs.SI</categories><comments>Proceedings of the 5th International Conference on Collaborative
  Innovation Networks COINs15, Tokyo, Japan March 12-14, 2015
  (arXiv:1502.01142)</comments><report-no>coins15/2015/04</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the differences in historical worldview between
Western and Eastern cultures, represented through the English, Chinese,
Japanese, and German Wikipedia. In particular, we analyze the historical
networks of the World's leaders since the beginning of written history,
comparing them in the four different Wikipedias.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05260</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05260</id><created>2015-02-18</created><authors><author><keyname>Gloor</keyname><forenames>Peter</forenames></author><author><keyname>Paasivaara</keyname><forenames>Maria</forenames></author><author><keyname>Miller</keyname><forenames>Christine</forenames></author></authors><title>Lessons from the Coinseminar</title><categories>cs.SI cs.CY</categories><comments>Proceedings of the 5th International Conference on Collaborative
  Innovation Networks COINs15, Tokyo, Japan March 12-14, 2015
  (arXiv:1502.01142)</comments><report-no>coins15/2015/06</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes lessons learned from teaching a distributed virtual
course on COINs (Collaborative Innovation Networks) over the last 12 years at
five different sites located in four different time zones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05263</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05263</id><created>2015-02-18</created><authors><author><keyname>Maddali</keyname><forenames>Hanuma Teja</forenames></author><author><keyname>Gloor</keyname><forenames>Peter A.</forenames></author><author><keyname>Margolis</keyname><forenames>Peter</forenames></author></authors><title>Comparing Online Community Structure of Patients of Chronic Diseases</title><categories>cs.SI physics.soc-ph</categories><comments>Proceedings of the 5th International Conference on Collaborative
  Innovation Networks COINs15, Tokyo, Japan March 12-14, 2015 (arXiv:1502.01142</comments><report-no>coins15/2015/09</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we compare the social network structure of people talking about
Crohn's disease, Cystic Fibrosis, and Type 1 diabetes on Facebook and Twitter.
We find that the Crohn's community's contributors are most emotional on
Facebook and Twitter and most negative on Twitter, while the T1D community's
communication network structure is most cohesive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05264</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05264</id><created>2015-02-18</created><authors><author><keyname>Launonen</keyname><forenames>Pentti</forenames></author><author><keyname>Kern</keyname><forenames>KC</forenames></author><author><keyname>Tiilikainen</keyname><forenames>Sanna</forenames></author></authors><title>Measuring Creativity of Wikipedia Editors</title><categories>cs.SI physics.soc-ph</categories><comments>Proceedings of the 5th International Conference on Collaborative
  Innovation Networks COINs15, Tokyo, Japan March 12-14, 2015
  (arXiv:1502.01142)</comments><report-no>coins15/2015/27</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our paper explores contribution patterns of creativity and collaboration of
Wikipedia editors as manifestations of social dynamics between the editors. We
find support for existence of four socially constructed personas among the
editors and difference in distribution of personas in articles of different
qualities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05267</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05267</id><created>2015-02-18</created><authors><author><keyname>Grassl</keyname><forenames>Markus</forenames></author><author><keyname>Roetteler</keyname><forenames>Martin</forenames></author></authors><title>Quantum MDS Codes over Small Fields</title><categories>quant-ph cs.IT math.IT</categories><comments>6 pages, 3 figures</comments><journal-ref>Proceedings 2015 IEEE International Symposium on Information
  Theory (ISIT 2015), Hong Kong, 14-19 June 2015 , pp. 1104-1108</journal-ref><doi>10.1109/ISIT.2015.7282626</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider quantum MDS (QMDS) codes for quantum systems of dimension $q$
with lengths up to $q^2+2$ and minimum distances up to $q+1$. We show how
starting from QMDS codes of length $q^2+1$ based on cyclic and constacyclic
codes, new QMDS codes can be obtained by shortening. We provide numerical
evidence for our conjecture that almost all admissible lengths, from a lower
bound $n_0(q,d)$ on, are achievable by shortening. Some additional codes that
fill gaps in the list of achievable lengths are presented as well along with a
construction of a family of QMDS codes of length $q^2+2$, where $q=2^m$, that
appears to be new.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05273</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05273</id><created>2015-02-18</created><authors><author><keyname>Jakobsen</keyname><forenames>Sune K.</forenames></author><author><keyname>Orlandi</keyname><forenames>Claudio</forenames></author></authors><title>How to Bootstrap Anonymous Communication</title><categories>cs.CR</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We ask whether it is possible to anonymously communicate a large amount of
data using only public (non-anonymous) communication together with a small
anonymous channel. We think this is a central question in the theory of
anonymous communication and to the best of our knowledge this is the first
formal study in this direction. To solve this problem, we introduce the concept
of anonymous steganography: think of a leaker Lea who wants to leak a large
document to Joe the journalist. Using anonymous steganography Lea can embed
this document in innocent looking communication on some popular website (such
as cat videos on YouTube or funny memes on 9GAG). Then Lea provides Joe with a
short key $k$ which, when applied to the entire website, recovers the document
while hiding the identity of Lea among the large number of users of the
website. Our contributions include:
  - Introducing and formally defining anonymous steganography,
  - A construction showing that anonymous steganography is possible (which uses
recent results in circuits obfuscation),
  - A lower bound on the number of bits which are needed to bootstrap anonymous
communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05275</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05275</id><created>2015-02-18</created><authors><author><keyname>Barcucci</keyname><forenames>Elena</forenames></author><author><keyname>Bernini</keyname><forenames>Antonio</forenames></author><author><keyname>Bilotta</keyname><forenames>Stefano</forenames></author><author><keyname>Pinzani</keyname><forenames>Renzo</forenames></author></authors><title>Cross-bifix-free sets in two dimensions</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A bidimensional bifix (in short bibifix) of a square matrix T is a square
submatrix of T which occurs in the top-left and bottom-right corners of T. This
allows us to extend the definition of bifix-free words and cross-bifix-free set
of words to bidimensional structures. In this paper we exhaustively generate
all the bibifix-free square matrices and we construct a particular
non-expandable cross-bibifix-free set of square matrices. Moreover, we provide
a Gray code for listing this set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05279</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05279</id><created>2015-02-18</created><authors><author><keyname>Halldorsson</keyname><forenames>Magnus M.</forenames></author><author><keyname>Tonoyan</keyname><forenames>Tigran</forenames></author></authors><title>The Price of Local Power Control in Wireless Scheduling</title><categories>cs.NI cs.DS</categories><comments>23 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of scheduling wireless links in the physical model,
where we seek an assignment of power levels and a partition of the given set of
links into the minimum number of subsets satisfying the
signal-to-interference-and-noise-ratio (SINR) constraints. Specifically, we are
interested in the efficiency of local power assignment schemes, or oblivious
power schemes, in approximating wireless scheduling. Oblivious power schemes
are motivated by networking scenarios when power levels must be decided in
advance, and not as part of the scheduling computation.
  We first show that the known algorithms fail to obtain sub-logarithmic
bounds; that is, their approximation ratio are $\tilde\Omega(\log
\max(\Delta,n))$, where $n$ is the number of links, $\Delta$ is the ratio of
the maximum and minimum link lengths, and $\tilde\Omega$ hides
doubly-logarithmic factors. We then present the first
$O(\log{\log\Delta})$-approximation algorithm, which is known to be best
possible (in terms of $\Delta$) for oblivious power schemes. We achieve this by
representing interference by a conflict graph, which allows the application of
graph-theoretic results for a variety of related problems, including the
weighted capacity problem. We explore further the contours of approximability,
and find the choice of power assignment matters; that not all metric spaces are
equal; and that the presence of weak links makes the problem harder. Combined,
our result resolve the price of oblivious power for wireless scheduling, or the
value of allowing unfettered power control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05292</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05292</id><created>2015-02-18</created><updated>2015-09-22</updated><authors><author><keyname>Farina</keyname><forenames>Gabriele</forenames></author><author><keyname>Laura</keyname><forenames>Luigi</forenames></author></authors><title>Dynamic subtrees queries revisited: the Depth First Tour Tree</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the dynamic tree problem the goal is the maintenance of an arbitrary
n-vertex forest, where the trees are subject to joining and splitting by,
respectively, adding and removing edges. Depending on the application,
information can be associated to nodes or edges (or both), and queries might
require to combine values in path or (sub)trees.
  In this paper we present a novel data structure, called the Depth First Tour
Tree, based on a linearization of a DFS visit of the tree. Despite the
simplicity of the approach, similar to the ET-Trees (based on a Euler Tour),
our data structure is able to answer queries related to both paths and
(sub)trees. In particular, focusing on subtree computations, we show how to
customize the data structure in order to answer queries for three distinct
applications: impact of the removal of an articulation point from a graph,
betweenness centrality and closeness centrality of a dynamic tree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05301</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05301</id><created>2015-02-18</created><authors><author><keyname>Thapper</keyname><forenames>Johan</forenames></author><author><keyname>Zivny</keyname><forenames>Stanislav</forenames></author></authors><title>Sherali-Adams relaxations for valued CSPs</title><categories>cs.CC cs.DM</categories><acm-class>F.2.0</acm-class><journal-ref>Proc. of ICALP'15 1058-1069 (2015)</journal-ref><doi>10.1007/978-3-662-47672-7_86</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider Sherali-Adams linear programming relaxations for solving valued
constraint satisfaction problems to optimality. The utility of linear
programming relaxations in this context have previously been demonstrated using
the lowest possible level of this hierarchy under the name of the basic linear
programming relaxation (BLP). It has been shown that valued constraint
languages containing only finite-valued weighted relations are tractable if,
and only if, the integrality gap of the BLP is 1. In this paper, we demonstrate
that almost all of the known tractable languages with arbitrary weighted
relations have an integrality gap 1 for the Sherali-Adams relaxation with
parameters (2,3). The result is closely connected to the notion of bounded
relational width for the ordinary constraint satisfaction problem and its
recent characterisation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05321</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05321</id><created>2015-02-18</created><authors><author><keyname>Namiot</keyname><forenames>Dmitry</forenames></author><author><keyname>Sneps-Sneppe</keyname><forenames>Manfred</forenames></author></authors><title>On Mobile Bluetooth Tags</title><categories>cs.NI</categories><comments>submitted to FRUCT-17 conference (http://fruct.org)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new approach for hyper-local data sharing and delivery
on the base of discoverable Bluetooth nodes. Our approach allows customers to
associate user-defined data with network nodes and use a special mobile
application (context-aware browser) for presenting this information to mobile
users in proximity. Alternatively, mobile services can request and share local
data in M2M applications rely on network proximity. Bluetooth nodes in cars are
among the best candidates for the role of the bearing nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05326</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05326</id><created>2015-02-18</created><authors><author><keyname>Elkouss</keyname><forenames>David</forenames></author><author><keyname>Strelchuk</keyname><forenames>Sergii</forenames></author></authors><title>Superadditivity of private information for any number of uses of the
  channel</title><categories>quant-ph cs.IT math.IT</categories><comments>6 pages, 1 figure</comments><journal-ref>Phys. Rev. Lett. 115, 040501 (2015)</journal-ref><doi>10.1103/PhysRevLett.115.040501</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The quantum capacity of a quantum channel is always smaller than the capacity
of the channel for private communication. However, both quantities are given by
the infinite regularization of respectively the coherent and the private
information. Here, we construct a family of channels for which the private and
coherent information can remain strictly superadditive for unbounded number of
uses. We prove this by showing that the coherent information is strictly larger
than the private information of a smaller number of uses of the channel. This
implies that even though the quantum capacity is upper bounded by the private
capacity, the non-regularized quantities can be interleaved. From an
operational point of view, the private capacity can be used for gauging the
practical value of quantum channels for secure communication and, consequently,
for key distribution. We thus show that in order to evaluate the interest a
channel for this task it is necessary to optimize the private information over
an unlimited number of uses of the channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05332</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05332</id><created>2015-02-18</created><authors><author><keyname>Asinowski</keyname><forenames>Andrei</forenames></author></authors><title>The number of non-crossing perfect plane matchings is minimized (almost)
  only by point sets in convex position</title><categories>cs.CG</categories><comments>Short note, 6 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well-known that the number of non-crossing perfect matchings of $2k$
points in convex position in the plane is $C_k$, the $k$th Catalan number.
Garc\'ia, Noy, and Tejel proved in 2000 that for any set of $2k$ points in
general position, the number of such matchings is at least $C_k$. We show that
the equality holds only for sets of points in convex position, and for one
exceptional configuration of $6$ points.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05334</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05334</id><created>2015-02-18</created><authors><author><keyname>Eppstein</keyname><forenames>David</forenames></author></authors><title>D3-Reducible Graphs</title><categories>cs.DS cs.DM</categories><comments>14 pages, 5 figures</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe two local reduction rules that can be used to recognize Halin
graphs in linear time, avoiding the general-purpose planarity testing step of
previous linear time Halin graph recognition algorithms. The same two rules can
also be used to recognize a broader class of polyhedral graphs, which we call
D3-reducible graphs. These graphs are the dual graphs of the polyhedra formed
by gluing pyramids together on their triangular faces; their treewidth is
bounded, and they necessarily have Lombardi drawings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05337</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05337</id><created>2015-02-18</created><updated>2015-04-16</updated><authors><author><keyname>Freudiger</keyname><forenames>Julien</forenames></author><author><keyname>De Cristofaro</keyname><forenames>Emiliano</forenames></author><author><keyname>Brito</keyname><forenames>Alex</forenames></author></authors><title>Controlled Data Sharing for Collaborative Predictive Blacklisting</title><categories>cs.CR cs.NI</categories><comments>A preliminary version of this paper appears in DIMVA 2015. This is
  the full version. arXiv admin note: substantial text overlap with
  arXiv:1403.2123</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although sharing data across organizations is often advocated as a promising
way to enhance cybersecurity, collaborative initiatives are rarely put into
practice owing to confidentiality, trust, and liability challenges. In this
paper, we investigate whether collaborative threat mitigation can be realized
via a controlled data sharing approach, whereby organizations make informed
decisions as to whether or not, and how much, to share. Using appropriate
cryptographic tools, entities can estimate the benefits of collaboration and
agree on what to share in a privacy-preserving way, without having to disclose
their datasets. We focus on collaborative predictive blacklisting, i.e.,
forecasting attack sources based on one's logs and those contributed by other
organizations. We study the impact of different sharing strategies by
experimenting on a real-world dataset of two billion suspicious IP addresses
collected from Dshield over two months. We find that controlled data sharing
yields up to 105% accuracy improvement on average, while also reducing the
false positive rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05347</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05347</id><created>2015-02-18</created><authors><author><keyname>De</keyname><forenames>Avik</forenames></author><author><keyname>Koditschek</keyname><forenames>Daniel E.</forenames></author></authors><title>The Penn Jerboa: A Platform for Exploring Parallel Composition of
  Templates</title><categories>cs.RO</categories><comments>Technical Report to Accompany: A. De and D. Koditschek, &quot;Parallel
  composition of templates for tail-energized planar hopping,&quot; in 2015 IEEE
  International Conference on Robotics and Automation (ICRA), May 2015, (to
  appear)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have built a 12DOF, passive-compliant legged, tailed biped actuated by
four brushless DC motors. We anticipate that this machine will achieve varied
modes of quasistatic and dynamic balance, enabling a broad range of locomotion
tasks including sitting, standing, walking, hopping, running, turning, leaping,
and more. Achieving this diversity of behavior with a single under-actuated
body, requires a correspondingly diverse array of controllers, motivating our
interest in compositional techniques that promote mixing and reuse of a
relatively few base constituents to achieve a combinatorially growing array of
available choices. Here we report on the development of one important example
of such a behavioral programming method, the construction of a novel monopedal
sagittal plane hopping gait through parallel composition of four decoupled 1DOF
base controllers.
  For this example behavior, the legs are locked in phase and the body is
fastened to a boom to restrict motion to the sagittal plane. The platform's
locomotion is powered by the hip motor that adjusts leg touchdown angle in
flight and balance in stance, along with a tail motor that adjusts body shape
in flight and drives energy into the passive leg shank spring during stance.
The motor control signals arise from the application in parallel of four
simple, completely decoupled 1DOF feedback laws that provably stabilize in
isolation four corresponding 1DOF abstract reference plants. Each of these
abstract 1DOF closed loop dynamics represents some simple but crucial specific
component of the locomotion task at hand. We present a partial proof of
correctness for this parallel composition of template reference systems along
with data from the physical platform suggesting these templates are anchored as
evidenced by the correspondence of their characteristic motions with a suitably
transformed image of traces from the physical platform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05361</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05361</id><created>2015-02-18</created><updated>2015-04-28</updated><authors><author><keyname>Kolman</keyname><forenames>Petr</forenames></author><author><keyname>Kouteck&#xfd;</keyname><forenames>Martin</forenames></author></authors><title>Extended Formulation for CSP that is Compact for Instances of Bounded
  Treewidth</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we provide an extended formulation for the class of constraint
satisfaction problems and prove that its size is polynomial for instances whose
constraint graph has bounded treewidth. This implies new upper bounds on
extension complexity of several important NP-hard problems on graphs of bounded
treewidth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05366</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05366</id><created>2015-02-18</created><authors><author><keyname>Voronin</keyname><forenames>Sergey</forenames></author><author><keyname>Martinsson</keyname><forenames>Per-Gunnar</forenames></author></authors><title>RSVDPACK: Subroutines for computing partial singular value
  decompositions via randomized sampling on single core, multi core, and GPU
  architectures</title><categories>math.NA cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This document describes an implementation in C of a set of randomized
algorithms for computing partial Singular Value Decompositions (SVDs). The
techniques largely follow the prescriptions in the article &quot;Finding structure
with randomness: Probabilistic algorithms for constructing approximate matrix
decompositions,&quot; N. Halko, P.G. Martinsson, J. Tropp, SIAM Review, 53(2), 2011,
pp. 217-288, but with some modifications to improve performance. The codes
implement a number of low rank SVD computing routines for three different sets
of hardware: (1) single core CPU, (2) multi core CPU, and (3) massively
multicore GPU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05370</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05370</id><created>2015-02-18</created><authors><author><keyname>Kailkhura</keyname><forenames>Bhavya</forenames></author><author><keyname>Wimalajeewa</keyname><forenames>Thakshila</forenames></author><author><keyname>Varshney</keyname><forenames>Pramod K.</forenames></author></authors><title>Collaborative Compressive Detection with Physical Layer Secrecy
  Constraints</title><categories>stat.AP cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of detecting a high dimensional signal (not
necessarily sparse) based on compressed measurements with physical layer
secrecy guarantees. First, we propose a collaborative compressive detection
(CCD) framework to compensate for the performance loss due to compression with
a single sensor. We characterize the trade-off between dimensionality reduction
achieved by a universal compressive sensing (CS) based measurement scheme and
the achievable performance of CCD analytically. Next, we consider a scenario
where the network operates in the presence of an eavesdropper who wants to
discover the state of the nature being monitored by the system. To keep the
data secret from the eavesdropper, we propose to use cooperating trustworthy
nodes that assist the fusion center (FC) by injecting artificial noise to
deceive the eavesdropper. We seek the answers to the questions: Does CS help
improve the security performance in such a framework? What are the optimal
values of parameters which maximize the CS based collaborative detection
performance at the FC while ensuring perfect secrecy at the eavesdropper?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05375</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05375</id><created>2015-02-18</created><authors><author><keyname>Bhattacharyya</keyname><forenames>Arnab</forenames></author><author><keyname>Gadekar</keyname><forenames>Ameet</forenames></author><author><keyname>Rajgopal</keyname><forenames>Ninad</forenames></author></authors><title>On learning k-parities with and without noise</title><categories>cs.DS cs.DM cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We first consider the problem of learning $k$-parities in the on-line
mistake-bound model: given a hidden vector $x \in \{0,1\}^n$ with $|x|=k$ and a
sequence of &quot;questions&quot; $a_1, a_2, ...\in \{0,1\}^n$, where the algorithm must
reply to each question with $&lt; a_i, x&gt; \pmod 2$, what is the best tradeoff
between the number of mistakes made by the algorithm and its time complexity?
We improve the previous best result of Buhrman et al. by an $\exp(k)$ factor in
the time complexity.
  Second, we consider the problem of learning $k$-parities in the presence of
classification noise of rate $\eta \in (0,1/2)$. A polynomial time algorithm
for this problem (when $\eta &gt; 0$ and $k = \omega(1)$) is a longstanding
challenge in learning theory. Grigorescu et al. showed an algorithm running in
time ${n \choose k/2}^{1 + 4\eta^2 +o(1)}$. Note that this algorithm inherently
requires time ${n \choose k/2}$ even when the noise rate $\eta$ is polynomially
small. We observe that for sufficiently small noise rate, it is possible to
break the $n \choose k/2$ barrier. In particular, if for some function $f(n) =
\omega(1)$ and $\alpha \in [1/2, 1)$, $k = n/f(n)$ and $\eta = o(f(n)^{-
\alpha}/\log n)$, then there is an algorithm for the problem with running time
$poly(n)\cdot {n \choose k}^{1-\alpha} \cdot e^{-k/4.01}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05394</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05394</id><created>2015-02-18</created><authors><author><keyname>Pu</keyname><forenames>Cunlai</forenames></author><author><keyname>Li</keyname><forenames>Siyuan</forenames></author><author><keyname>Yang</keyname><forenames>Jian</forenames></author></authors><title>Traffic-driven SIR epidemic model on networks</title><categories>physics.soc-ph cs.SI</categories><comments>12 pages, 11 figures</comments><doi>10.1016/j.physa.2015.11.028</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel SIR epidemic model which is driven by the transmission of
infection packets in networks. Specifically, infected nodes generate and
deliver infection packets causing the spread of the epidemic, while recovered
nodes block the delivery of infection packets, and this inhibits the epidemic
spreading. The efficient routing protocol governed by a control parameter
$\alpha$ is used in the packet transmission. We obtain the maximum
instantaneous population of infected nodes, the maximum population of ever
infected nodes, as well as the corresponding optimal $\alpha$ through
simulation. We find that generally more balanced load distribution leads to
more intense and wide spread of an epidemic in networks. Increasing either
average node degree or homogeneity of degree distribution will facilitate
epidemic spreading. When packet generation rate $\rho$ is small, increasing
$\rho$ favors epidemic spreading. However, when $\rho$ is large enough, traffic
congestion appears which inhibits epidemic spreading.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05417</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05417</id><created>2015-02-17</created><authors><author><keyname>Mauch</keyname><forenames>Matthias</forenames></author><author><keyname>MacCallum</keyname><forenames>Robert M.</forenames></author><author><keyname>Levy</keyname><forenames>Mark</forenames></author><author><keyname>Leroi</keyname><forenames>Armand M.</forenames></author></authors><title>The Evolution of Popular Music: USA 1960-2010</title><categories>physics.soc-ph cs.SD</categories><comments>MS: 13 pages, 6 figures; SI: 15 pages, 7 figures</comments><journal-ref>R. Soc. open sci. 2015 2 150081</journal-ref><doi>10.1098/rsos.150081</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In modern societies, cultural change seems ceaseless. The flux of fashion is
especially obvious for popular music. While much has been written about the
origin and evolution of pop, most claims about its history are anecdotal rather
than scientific in nature. To rectify this we investigate the US Billboard Hot
100 between 1960 and 2010. Using Music Information Retrieval (MIR) and
text-mining tools we analyse the musical properties of ~17,000 recordings that
appeared in the charts and demonstrate quantitative trends in their harmonic
and timbral properties. We then use these properties to produce an audio-based
classification of musical styles and study the evolution of musical diversity
and disparity, testing, and rejecting, several classical theories of cultural
change. Finally, we investigate whether pop musical evolution has been gradual
or punctuated. We show that, although pop music has evolved continuously, it
did so with particular rapidity during three stylistic &quot;revolutions&quot; around
1964, 1983 and 1991. We conclude by discussing how our study points the way to
a quantitative science of cultural change.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05428</identifier>
 <datestamp>2015-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05428</id><created>2015-02-18</created><authors><author><keyname>Tian</keyname><forenames>Chao</forenames></author><author><keyname>Chen</keyname><forenames>Jun</forenames></author><author><keyname>Diggavi</keyname><forenames>Suhas</forenames></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames></author></authors><title>Matched Multiuser Gaussian Source-Channel Communications via Uncoded
  Schemes</title><categories>cs.IT math.IT</categories><comments>25 pages, 6 figures. A short version was submitted to ISIT-2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate whether uncoded schemes are optimal for Gaussian sources on
multiuser Gaussian channels. Particularly, we consider two problems: the first
is to send correlated Gaussian sources on a Gaussian broadcast channel where
each receiver is interested in reconstructing only one source component (or one
specific linear function of the sources) under the mean squared error
distortion measure; the second is to send vector Gaussian sources on a Gaussian
multiple-access channel, where each transmitter observes a noisy combination of
the source, and the receiver wishes to reconstruct the individual source
components (or individual linear functions) under the mean squared error
distortion measure. It is shown that when the channel parameters match certain
general conditions, the induced distortion tuples are on the boundary of the
achievable distortion region, and thus optimal. Instead of following the
conventional approach of attempting to characterize the achievable distortion
region, we ask the question whether and how a match can be effectively
determined. This decision problem formulation helps to circumvent the difficult
optimization problem often embedded in region characterization problems, and
also leads us to focus on the critical conditions in the outer bounds that make
the inequalities become equalities, which effectively decouples the overall
problem into several simpler sub-problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05433</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05433</id><created>2015-02-18</created><authors><author><keyname>You</keyname><forenames>Li</forenames></author><author><keyname>Gao</keyname><forenames>Xiqi</forenames></author><author><keyname>Xia</keyname><forenames>Xiang-Gen</forenames></author><author><keyname>Ma</keyname><forenames>Ni</forenames></author><author><keyname>Peng</keyname><forenames>Yan</forenames></author></authors><title>Pilot Reuse for Massive MIMO Transmission over Spatially Correlated
  Rayleigh Fading Channels</title><categories>cs.IT math.IT</categories><comments>14 pages, 7 figures. Accepted for publication in IEEE Transactions on
  Wireless Communications</comments><journal-ref>IEEE Trans. Wireless Commun., vol. 14, no. 6, pp. 3352--3366, Jun.
  2015</journal-ref><doi>10.1109/TWC.2015.2404839</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose pilot reuse (PR) in single cell for massive multiuser
multiple-input multiple-output (MIMO) transmission to reduce the pilot
overhead. For spatially correlated Rayleigh fading channels, we establish a
relationship between channel spatial correlations and channel power angle
spectrum when the base station antenna number tends to infinity. With this
channel model, we show that sum mean square error (MSE) of channel estimation
can be minimized provided that channel angle of arrival intervals of the user
terminals reusing the pilots are non-overlapping, which shows feasibility of PR
over spatially correlated massive MIMO channels with constrained channel
angular spreads. Regarding that channel estimation performance might degrade
due to PR, we also develop the closed-form robust multiuser uplink receiver and
downlink precoder that minimize sum MSE of signal detection, and reveal a
duality between them. Subsequently, we investigate pilot scheduling, which
determines the PR pattern, under two minimum MSE related criteria, and propose
a low complexity pilot scheduling algorithm which relies on the channel
statistics only. Simulation results show that the proposed PR scheme provides
significant performance gains over the conventional orthogonal training scheme
in terms of net spectral efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05435</identifier>
 <datestamp>2015-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05435</id><created>2015-02-18</created><authors><author><keyname>Ozay</keyname><forenames>Mete</forenames></author><author><keyname>Vural</keyname><forenames>Fatos T. Yarman</forenames></author><author><keyname>Kulkarni</keyname><forenames>Sanjeev R.</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Fusion of Image Segmentation Algorithms using Consensus Clustering</title><categories>cs.CV</categories><comments>A version of the manuscript was published in ICIP 2013</comments><journal-ref>20th IEEE International Conference on Image Processing (ICIP), pp.
  4049-4053, Melbourne, VIC, 15-18 Sept. 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new segmentation fusion method is proposed that ensembles the output of
several segmentation algorithms applied on a remotely sensed image. The
candidate segmentation sets are processed to achieve a consensus segmentation
using a stochastic optimization algorithm based on the Filtered Stochastic BOEM
(Best One Element Move) method. For this purpose, Filtered Stochastic BOEM is
reformulated as a segmentation fusion problem by designing a new distance
learning approach. The proposed algorithm also embeds the computation of the
optimum number of clusters into the segmentation fusion problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05441</identifier>
 <datestamp>2015-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05441</id><created>2015-02-18</created><authors><author><keyname>Hassanat</keyname><forenames>Ahmad B. A.</forenames></author><author><keyname>Altarawneh</keyname><forenames>Ghada Awad</forenames></author></authors><title>Rule-and Dictionary-based Solution for Variations in Written Arabic
  Names in Social Networks, Big Data, Accounting Systems and Large Databases</title><categories>cs.DB cs.CL cs.IR</categories><journal-ref>Research Journal of Applied Sciences, Engineering and Technology,
  2014, 8(14): 1630-1638</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the problem that some Arabic names can be written in
multiple ways. When someone searches for only one form of a name, neither exact
nor approximate matching is appropriate for returning the multiple variants of
the name. Exact matching requires the user to enter all forms of the name for
the search, and approximate matching yields names not among the variations of
the one being sought. In this paper, we attempt to solve the problem with a
dictionary of all Arabic names mapped to their different (alternative) writing
forms. We generated alternatives based on rules we derived from reviewing the
first names of 9.9 million citizens and former citizens of Jordan. This
dictionary can be used for both standardizing the written form when inserting a
new name into a database and for searching for the name and all its alternative
written forms. Creating the dictionary automatically based on rules resulted in
at least 7% erroneous acceptance errors and 7.9% erroneous rejection errors. We
addressed the errors by manually editing the dictionary. The dictionary can be
of help to real world-databases, with the qualification that manual editing
does not guarantee 100% correctness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05443</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05443</id><created>2015-02-18</created><updated>2015-07-20</updated><authors><author><keyname>Oliehoek</keyname><forenames>Frans A.</forenames></author><author><keyname>Spaan</keyname><forenames>Matthijs T. J.</forenames></author><author><keyname>Witwicki</keyname><forenames>Stefan</forenames></author></authors><title>Influence-Optimistic Local Values for Multiagent Planning --- Extended
  Version</title><categories>cs.AI cs.SY</categories><comments>Long version of IJCAI 2015 paper (and extended abstract at AAMAS
  2015)</comments><acm-class>I.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent years have seen the development of methods for multiagent planning
under uncertainty that scale to tens or even hundreds of agents. However, most
of these methods either make restrictive assumptions on the problem domain, or
provide approximate solutions without any guarantees on quality. Methods in the
former category typically build on heuristic search using upper bounds on the
value function. Unfortunately, no techniques exist to compute such upper bounds
for problems with non-factored value functions. To allow for meaningful
benchmarking through measurable quality guarantees on a very general class of
problems, this paper introduces a family of influence-optimistic upper bounds
for factored decentralized partially observable Markov decision processes
(Dec-POMDPs) that do not have factored value functions. Intuitively, we derive
bounds on very large multiagent planning problems by subdividing them in
sub-problems, and at each of these sub-problems making optimistic assumptions
with respect to the influence that will be exerted by the rest of the system.
We numerically compare the different upper bounds and demonstrate how we can
achieve a non-trivial guarantee that a heuristic solution for problems with
hundreds of agents is close to optimal. Furthermore, we provide evidence that
the upper bounds may improve the effectiveness of heuristic influence search,
and discuss further potential applications to multiagent planning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05447</identifier>
 <datestamp>2015-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05447</id><created>2015-02-18</created><authors><author><keyname>Fomin</keyname><forenames>Fedor V.</forenames></author><author><keyname>Golovnev</keyname><forenames>Alexander</forenames></author><author><keyname>Kulikov</keyname><forenames>Alexander S.</forenames></author><author><keyname>Mihajlin</keyname><forenames>Ivan</forenames></author></authors><title>Lower Bounds for the Graph Homomorphism Problem</title><categories>cs.DS</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The graph homomorphism problem (HOM) asks whether the vertices of a given
$n$-vertex graph $G$ can be mapped to the vertices of a given $h$-vertex graph
$H$ such that each edge of $G$ is mapped to an edge of $H$. The problem
generalizes the graph coloring problem and at the same time can be viewed as a
special case of the $2$-CSP problem. In this paper, we prove several lower
bound for HOM under the Exponential Time Hypothesis (ETH) assumption. The main
result is a lower bound $2^{\Omega\left( \frac{n \log h}{\log \log h}\right)}$.
This rules out the existence of a single-exponential algorithm and shows that
the trivial upper bound $2^{{\mathcal O}(n\log{h})}$ is almost asymptotically
tight.
  We also investigate what properties of graphs $G$ and $H$ make it difficult
to solve HOM$(G,H)$. An easy observation is that an ${\mathcal O}(h^n)$ upper
bound can be improved to ${\mathcal O}(h^{\operatorname{vc}(G)})$ where
$\operatorname{vc}(G)$ is the minimum size of a vertex cover of $G$. The second
lower bound $h^{\Omega(\operatorname{vc}(G))}$ shows that the upper bound is
asymptotically tight. As to the properties of the &quot;right-hand side&quot; graph $H$,
it is known that HOM$(G,H)$ can be solved in time $(f(\Delta(H)))^n$ and
$(f(\operatorname{tw}(H)))^n$ where $\Delta(H)$ is the maximum degree of $H$
and $\operatorname{tw}(H)$ is the treewidth of $H$. This gives
single-exponential algorithms for graphs of bounded maximum degree or bounded
treewidth. Since the chromatic number $\chi(H)$ does not exceed
$\operatorname{tw}(H)$ and $\Delta(H)+1$, it is natural to ask whether similar
upper bounds with respect to $\chi(H)$ can be obtained. We provide a negative
answer to this question by establishing a lower bound $(f(\chi(H)))^n$ for any
function $f$. We also observe that similar lower bounds can be obtained for
locally injective homomorphisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05448</identifier>
 <datestamp>2015-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05448</id><created>2015-02-18</created><authors><author><keyname>Kailkhura</keyname><forenames>Bhavya</forenames></author><author><keyname>Nadendla</keyname><forenames>V. Sriram Siddhardh</forenames></author><author><keyname>Varshney</keyname><forenames>Pramod K.</forenames></author></authors><title>Distributed Inference in the Presence of Eavesdroppers: A Survey</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The distributed inference framework comprises of a group of spatially
distributed nodes which acquire observations about a phenomenon of interest.
Due to bandwidth and energy constraints, the nodes often quantize their
observations into a finite-bit local message before sending it to the fusion
center (FC). Based on the local summary statistics transmitted by nodes, the FC
makes a global decision about the presence of the phenomenon of interest. The
distributed and broadcast nature of such systems makes them quite vulnerable to
different types of attacks. This paper addresses the problem of secure
communication in the presence of eavesdroppers. In particular, we focus on
efficient mitigation schemes to mitigate the impact of eavesdropping. We
present an overview of the distributed inference schemes under secrecy
constraints and describe the currently available approaches in the context of
distributed detection and estimation followed by a discussion on avenues for
future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05450</identifier>
 <datestamp>2015-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05450</id><created>2015-02-18</created><authors><author><keyname>Alliot</keyname><forenames>Jean-Marc</forenames></author></authors><title>The (Final) countdown</title><categories>cs.AI</categories><msc-class>Computer Games</msc-class><acm-class>I.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Countdown game is one of the oldest TV show running in the world. It
started broadcasting in 1972 on the french television and in 1982 on British
channel 4, and it has been running since in both countries. The game, while
extremely popular, never received any serious scientific attention, probably
because it seems too simple at first sight. We present in this article an
in-depth analysis of the numbers round of the countdown game. This includes a
complexity analysis of the game, an analysis of existing algorithms, the
presentation of a new algorithm that increases resolution speed by a factor of
20. It also includes some leads on how to turn the game into a more difficult
one, both for a human player and for a computer, and even to transform it into
a probably undecidable problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05451</identifier>
 <datestamp>2015-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05451</id><created>2015-02-18</created><authors><author><keyname>Tochimani</keyname><forenames>Azucena</forenames></author><author><keyname>Villarreal</keyname><forenames>Rafael H.</forenames></author></authors><title>Vanishing ideals over rational parameterizations</title><categories>math.AC cs.IT math.AG math.CO math.IT</categories><msc-class>13P25, 14G50, 14G15, 11T71, 94B27, 94B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $K$ be a field and let $\mathbb{X}$ (resp. $\mathbb{X}^*$) be a subset of
a projective space ${\mathbb P}^{s-1}$ (resp. affine space $\mathbb{A}^s$),
over the field $K$, parameterized by rational functions. Let $I(\mathbb{X})$
(resp. $I(\mathbb{X}^*)$) be the vanishing ideal of $\mathbb{X}$ (resp.
$\mathbb{X}^*$). Some of the main contributions of this paper are in
determining formulas for $I(\mathbb{X})$ (resp. $I(\mathbb{X}^*)$) to compute
their algebraic invariants using elimination theory and Gr\&quot;obner bases. The
formulas for vanishing ideals over finite fields that we give in this paper
were discovered by making experiments with Macaulay$2$, we are specially
interested in this case because of its relation to algebraic coding theory.
Then we use our results to study: the degree and structure of vanishing ideals,
the projective closure of $\mathbb{X}^*$, and the basic parameters of affine
and projective Reed-Muller-type codes. We recover some results for vanishing
ideals over monomial parameterizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05461</identifier>
 <datestamp>2015-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05461</id><created>2015-02-18</created><authors><author><keyname>Vondrick</keyname><forenames>Carl</forenames></author><author><keyname>Khosla</keyname><forenames>Aditya</forenames></author><author><keyname>Pirsiavash</keyname><forenames>Hamed</forenames></author><author><keyname>Malisiewicz</keyname><forenames>Tomasz</forenames></author><author><keyname>Torralba</keyname><forenames>Antonio</forenames></author></authors><title>Visualizing Object Detection Features</title><categories>cs.CV</categories><comments>In submission to IJCV</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce algorithms to visualize feature spaces used by object detectors.
Our method works by inverting a visual feature back to multiple natural images.
We found that these visualizations allow us to analyze object detection systems
in new ways and gain new insight into the detector's failures. For example,
when we visualize the features for high scoring false alarms, we discovered
that, although they are clearly wrong in image space, they do look deceptively
similar to true positives in feature space. This result suggests that many of
these false alarms are caused by our choice of feature space, and supports that
creating a better learning algorithm or building bigger datasets is unlikely to
correct these errors. By visualizing feature spaces, we can gain a more
intuitive understanding of recognition systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05472</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05472</id><created>2015-02-19</created><updated>2015-03-04</updated><authors><author><keyname>Marcheggiani</keyname><forenames>Diego</forenames></author><author><keyname>Sebastiani</keyname><forenames>Fabrizio</forenames></author></authors><title>On the Effects of Low-Quality Training Data on Information Extraction
  from Clinical Reports</title><categories>cs.LG cs.CL cs.IR</categories><comments>Submitted for publication</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In the last five years there has been a flurry of work on information
extraction from clinical documents, i.e., on algorithms capable of extracting,
from the informal and unstructured texts that are generated during everyday
clinical practice, mentions of concepts relevant to such practice. Most of this
literature is about methods based on supervised learning, i.e., methods for
training an information extraction system from manually annotated examples.
While a lot of work has been devoted to devising learning methods that generate
more and more accurate information extractors, no work has been devoted to
investigating the effect of the quality of training data on the learning
process. Low quality in training data often derives from the fact that the
person who has annotated the data is different from the one against whose
judgment the automatically annotated data must be evaluated. In this paper we
test the impact of such data quality issues on the accuracy of information
extraction systems as applied to the clinical domain. We do this by comparing
the accuracy deriving from training data annotated by the authoritative coder
(i.e., the one who has also annotated the test data, and by whose judgment we
must abide), with the accuracy deriving from training data annotated by a
different coder. The results indicate that, although the disagreement between
the two coders (as measured on the training set) is substantial, the difference
is (surprisingly enough) not always statistically significant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05477</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05477</id><created>2015-02-19</created><updated>2015-06-08</updated><authors><author><keyname>Schulman</keyname><forenames>John</forenames></author><author><keyname>Levine</keyname><forenames>Sergey</forenames></author><author><keyname>Moritz</keyname><forenames>Philipp</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author><author><keyname>Abbeel</keyname><forenames>Pieter</forenames></author></authors><title>Trust Region Policy Optimization</title><categories>cs.LG</categories><comments>16 pages, ICML 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we describe a method for optimizing control policies, with
guaranteed monotonic improvement. By making several approximations to the
theoretically-justified scheme, we develop a practical algorithm, called Trust
Region Policy Optimization (TRPO). This algorithm is effective for optimizing
large nonlinear policies such as neural networks. Our experiments demonstrate
its robust performance on a wide variety of tasks: learning simulated robotic
swimming, hopping, and walking gaits; and playing Atari games using images of
the screen as input. Despite its approximations that deviate from the theory,
TRPO tends to give monotonic improvement, with little tuning of
hyperparameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05484</identifier>
 <datestamp>2015-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05484</id><created>2015-02-19</created><authors><author><keyname>Gui</keyname><forenames>Guan</forenames></author><author><keyname>Xu</keyname><forenames>Li</forenames></author><author><keyname>Ma</keyname><forenames>Wentao</forenames></author><author><keyname>Chen</keyname><forenames>Badong</forenames></author></authors><title>Robust Adaptive Sparse Channel Estimation in the Presence of Impulsive
  Noises</title><categories>cs.IT cs.SY math.IT</categories><comments>5 pages, 4 figures, submitted for DSP2015 conference paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Broadband wireless channels usually have the sparse nature. Based on the
assumption of Gaussian noise model, adaptive filtering algorithms for
reconstruction sparse channels were proposed to take advantage of channel
sparsity. However, impulsive noises are often existed in many advance broadband
communications systems. These conventional algorithms are vulnerable to
deteriorate due to interference of impulsive noise. In this paper, sign least
mean square algorithm (SLMS) based robust sparse adaptive filtering algorithms
are proposed for estimating channels as well as for mitigating impulsive noise.
By using different sparsity-inducing penalty functions, i.e., zero-attracting
(ZA), reweighted ZA (RZA), reweighted L1-norm (RL1) and Lp-norm (LP), the
proposed SLMS algorithms are termed as SLMS-ZA, SLMS-RZA, LSMS-RL1 and SLMS-LP.
Simulation results are given to validate the proposed algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05487</identifier>
 <datestamp>2015-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05487</id><created>2015-02-19</created><authors><author><keyname>Niemann</keyname><forenames>Raik</forenames></author><author><keyname>Pfingst</keyname><forenames>Udo</forenames></author><author><keyname>G&#xf6;bel</keyname><forenames>Richard</forenames></author></authors><title>Performance Evaluation of netfilter: A Study on the Performance Loss
  When Using netfilter as a Firewall</title><categories>cs.NI</categories><comments>7 pages, 7 figures, 1 table</comments><acm-class>C.2.6; C.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since GNU/Linux became a popular operating system on computer network
routers, its packet routing mechanisms attracted more interest. This does not
only concern 'big' Linux servers acting as a router but more and more small and
medium network access devices, such as DSL or cable access devices.
  Although there are a lot of documents dealing with high performance routing
with GNU/Linux, only a few offer experimental results to prove the given
advices. This study evaluates the throughput performance of Linux' routing
subsystem netfilter under various conditions like different data transport
protocols in combination with different IP address families and transmission
strategies. Those conditions were evaluated with two different types of
netfilter rules for a high number in the rule tables. In addition to this, our
experiments allowed us to evaluate two prominent client connection handling
techniques (threads and the epoll() facility).
  The evaluation of the 1.260 different combinations of our test parameters
shows a nearly linear but small throughput loss with the number of rules which
is independant from the transport protocol and framesize. However, this
evaluation identifies another issue concerning the throughput loss when it
comes to the address family, i.e. IPv4 and IPv6.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05491</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05491</id><created>2015-02-19</created><updated>2015-04-15</updated><authors><author><keyname>Esuli</keyname><forenames>Andrea</forenames></author><author><keyname>Sebastiani</keyname><forenames>Fabrizio</forenames></author></authors><title>Optimizing Text Quantifiers for Multivariate Loss Functions</title><categories>cs.LG cs.IR</categories><comments>In press in ACM Transactions on Knowledge Discovery from Data, 2015</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We address the problem of \emph{quantification}, a supervised learning task
whose goal is, given a class, to estimate the relative frequency (or
\emph{prevalence}) of the class in a dataset of unlabelled items.
Quantification has several applications in data and text mining, such as
estimating the prevalence of positive reviews in a set of reviews of a given
product, or estimating the prevalence of a given support issue in a dataset of
transcripts of phone calls to tech support. So far, quantification has been
addressed by learning a general-purpose classifier, counting the unlabelled
items which have been assigned the class, and tuning the obtained counts
according to some heuristics. In this paper we depart from the tradition of
using general-purpose classifiers, and use instead a supervised learning model
for \emph{structured prediction}, capable of generating classifiers directly
optimized for the (multivariate and non-linear) function used for evaluating
quantification accuracy. The experiments that we have run on 5500 binary
high-dimensional datasets (averaging more than 14,000 documents each) show that
this method is more accurate, more stable, and more efficient than existing,
state-of-the-art quantification methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05501</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05501</id><created>2015-02-19</created><updated>2015-07-19</updated><authors><author><keyname>Alagi</keyname><forenames>G&#xe1;bor</forenames></author><author><keyname>Weidenbach</keyname><forenames>Christoph</forenames></author></authors><title>NRCL - A Model Building Approach to the Bernays-Sch\&quot;onfinkel Fragment
  (Full Paper)</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We combine constrained literals for model representation with key concepts
from first-order superposition and propositional conflict-driven clause
learning (CDCL) to create the new calculus Non-Redundant Clause Learning (NRCL)
deciding the Bernays-Sch\&quot;onfinkel fragment. Our calculus uses first-order
literals constrained by disequations between tuples of terms for compact model
representation. From superposition, NRCL inherits the abstract redundancy
criterion and the monotone model operator. CDCL adds the dynamic,
conflict-driven search for an atom ordering inducing a model. As a result, in
NRCL a false clause can be found effectively modulo the current model
candidate. It guides the derivation of a first-order ordered resolvent that is
never redundant. Similar to 1UIP-learning in CDCL, the learned resolvent
induces backtracking and, by blocking the previous conflict state via
propagation, it enforces progress towards finding a model or a refutation. The
non-redundancy result also implies that only finitely many clauses can be
generated by NRCL on the Bernays-Sch\&quot;onfinkel fragment, which serves as an
argument for termination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05507</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05507</id><created>2015-02-19</created><updated>2015-11-02</updated><authors><author><keyname>Geil</keyname><forenames>Olav</forenames></author><author><keyname>Martin</keyname><forenames>Stefano</forenames></author><author><keyname>Mart&#xed;nez-Pe&#xf1;as</keyname><forenames>Umberto</forenames></author><author><keyname>Matsumoto</keyname><forenames>Ryutaroh</forenames></author><author><keyname>Ruano</keyname><forenames>Diego</forenames></author></authors><title>On asymptotically good ramp secret sharing schemes</title><categories>cs.IT math.IT</categories><comments>Flawful and insufficient citation of the work by R. Cramer et al. has
  been corrected</comments><msc-class>94A62, 94B27, 94B65</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Asymptotically good sequences of ramp secret sharing schemes have been
intensively studied by Cramer et al. in [6, 7, 8, 9, 10, 11, 12, 13]. In those
works the focus is on full privacy and full reconstruction. We propose an
alternative definition of asymptotically good sequences of ramp secret sharing
schemes where a small amount of information leakage is allowed (and possibly
also non full recovery). By a non-constructive proof we demonstrate the
existence of sequences that - following our definition of goodness - have
parameters arbitrary close to the optimal ones. Moreover - still using our
definition - we demonstrate how to concretely construct asymptotically good
sequences of schemes from sequences of algebraic geometric codes related to a
tower of function fields. Our study involves a detailed treatment of the
(relative) generalized Hamming weights of the involved codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05511</identifier>
 <datestamp>2015-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05511</id><created>2015-02-19</created><authors><author><keyname>Dunjko</keyname><forenames>Vedran</forenames></author><author><keyname>Briegel</keyname><forenames>Hans J.</forenames></author></authors><title>Quantum mixing of Markov chains for special distributions</title><categories>quant-ph cs.DS</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The preparation of the stationary distribution of irreducible,
time-reversible Markov chains is a fundamental building block in many heuristic
approaches to algorithmically hard problems. It has been conjectured that
quantum analogs of classical mixing processes may offer a generic quadratic
speed-up in realizing such stationary distributions. Such a speed-up would also
imply a speed-up of a broad family of heuristic algorithms.
  However, a true quadratic speed up has thus far only been demonstrated for
special classes of Markov chains. These results often presuppose a regular
structure of the underlying graph of the Markov chain, and also a regularity in
the transition probabilities.
  In this work, we demonstrate a true quadratic speed-up for a class of Markov
chains where the restriction is only on the form of the stationary
distribution, rather than directly on the Markov chain structure itself. In
particular, we show efficient mixing can be achieved when it is beforehand
known that the distribution is monotonically decreasing relative to a known
order on the state space. Following this, we show that our approach extends to
a wider class of distributions, where only a fraction of the shape of the
distribution is known to be monotonic. Our approach is built on the
Szegedy-type quantization of transition operators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05516</identifier>
 <datestamp>2015-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05516</id><created>2015-02-19</created><authors><author><keyname>Zheng</keyname><forenames>Zhong</forenames></author><author><keyname>Wei</keyname><forenames>Lu</forenames></author><author><keyname>Speicher</keyname><forenames>Roland</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Ralf</forenames></author><author><keyname>H&#xe4;m&#xe4;l&#xe4;inen</keyname><forenames>Jyri</forenames></author><author><keyname>Corander</keyname><forenames>Jukka</forenames></author></authors><title>Outage Capacity of Rayleigh Product Channels: a Free Probability
  Approach</title><categories>cs.IT math.IT</categories><comments>32 pages, 6 figures, submitted to IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Rayleigh product channel model is useful in capturing the performance
degradation due to rank deficiency of MIMO channels. In this paper, such a
performance degradation is investigated via the channel outage probability
assuming slowly varying channel with delay-constrained decoding. Using
techniques of free probability theory, the asymptotic variance of channel
capacity is derived when the dimensions of the channel matrices approach
infinity. In this asymptotic regime, the channel capacity is rigorously proven
to be Gaussian distributed. Using the obtained results, a fundamental tradeoff
between multiplexing gain and diversity gain of Rayleigh product channels can
be characterized by closed-form expression at any finite signal-to-noise ratio.
Numerical results are provided to compare the relative outage performance
between Rayleigh product channels and conventional Rayleigh MIMO channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05532</identifier>
 <datestamp>2015-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05532</id><created>2015-02-19</created><authors><author><keyname>Fielder</keyname><forenames>Andrew</forenames></author><author><keyname>Panaousis</keyname><forenames>Emmanouil</forenames></author><author><keyname>Malacaria</keyname><forenames>Pasquale</forenames></author><author><keyname>Hankin</keyname><forenames>Chris</forenames></author><author><keyname>Smeraldi</keyname><forenames>Fabrizio</forenames></author></authors><title>Comparing Decision Support Approaches for Cyber Security Investment</title><categories>cs.GT cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When investing in cyber security resources, information security managers
have to follow effective decision-making strategies. We refer to this as the
cyber security investment challenge. In this paper, we consider three possible
decision-support methodologies for security managers to tackle this challenge.
We consider methods based on game theory, combinatorial optimisation and a
hybrid of the two. Our modelling starts by building a framework where we can
investigate the effectiveness of a cyber security control regarding the
protection of different assets seen as targets in presence of commodity
threats. In terms of game theory we consider a 2-person control game between
the security manager who has to choose among different implementation levels of
a cyber security control, and a commodity attacker who chooses among different
targets to attack. The pure game theoretical methodology consists of a large
game including all controls and all threats. In the hybrid methodology the game
solutions of individual control-games along with their direct costs (e.g.
financial) are combined with a knapsack algorithm to derive an optimal
investment strategy. The combinatorial optimisation technique consists of a
multi-objective multiple choice knapsack based strategy. We compare these
approaches on a case study that was built on SANS top critical controls. The
main achievements of this work is to highlight the weaknesses and strengths of
different investment methodologies for cyber security, the benefit of their
interaction, and the impact that indirect costs have on cyber security
investment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05533</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05533</id><created>2015-02-19</created><updated>2015-08-20</updated><authors><author><keyname>Etessami</keyname><forenames>Kousha</forenames></author><author><keyname>Stewart</keyname><forenames>Alistair</forenames></author><author><keyname>Yannakakis</keyname><forenames>Mihalis</forenames></author></authors><title>Greatest Fixed Points of Probabilistic Min/Max Polynomial Equations, and
  Reachability for Branching Markov Decision Processes</title><categories>cs.CC cs.GT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give polynomial time algorithms for quantitative (and qualitative)
reachability analysis for Branching Markov Decision Processes (BMDPs).
Specifically, given a BMDP, and given an initial population, where the
objective of the controller is to maximize (or minimize) the probability of
eventually reaching a population that contains an object of a desired (or
undesired) type, we give algorithms for approximating the supremum (infimum)
reachability probability, within desired precision epsilon &gt; 0, in time
polynomial in the encoding size of the BMDP and in log(1/epsilon). We
furthermore give P-time algorithms for computing epsilon-optimal strategies for
both maximization and minimization of reachability probabilities. We also give
P-time algorithms for all associated qualitative analysis problems, namely:
deciding whether the optimal (supremum or infimum) reachability probabilities
are 0 or 1. Prior to this paper, approximation of optimal reachability
probabilities for BMDPs was not even known to be decidable.
  Our algorithms exploit the following basic fact: we show that for any BMDP,
its maximum (minimum) non-reachability probabilities are given by the greatest
fixed point (GFP) solution g* in [0,1]^n of a corresponding monotone max (min)
Probabilistic Polynomial System of equations (max/min-PPS), x=P(x), which are
the Bellman optimality equations for a BMDP with non-reachability objectives.
We show how to compute the GFP of max/min PPSs to desired precision in P-time.
  We also study more general Branching Simple Stochastic Games (BSSGs) with
(non-)reachability objectives. We show that: (1) the value of these games is
captured by the GFP of a corresponding max-minPPS; (2) the quantitative problem
of approximating the value is in TFNP; and (3) the qualitative problems
associated with the value are all solvable in P-time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05534</identifier>
 <datestamp>2015-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05534</id><created>2015-02-19</created><authors><author><keyname>Nagaraj</keyname><forenames>Kalyan</forenames></author><author><keyname>Sridhar</keyname><forenames>Amulyashree</forenames></author></authors><title>NeuroSVM: A Graphical User Interface for Identification of Liver
  Patients</title><categories>cs.LG cs.HC</categories><comments>9 pages, 6 figures</comments><journal-ref>IJCSIT. 5(6): 8280-8284 (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Diagnosis of liver infection at preliminary stage is important for better
treatment. In todays scenario devices like sensors are used for detection of
infections. Accurate classification techniques are required for automatic
identification of disease samples. In this context, this study utilizes data
mining approaches for classification of liver patients from healthy
individuals. Four algorithms (Naive Bayes, Bagging, Random forest and SVM) were
implemented for classification using R platform. Further to improve the
accuracy of classification a hybrid NeuroSVM model was developed using SVM and
feed-forward artificial neural network (ANN). The hybrid model was tested for
its performance using statistical parameters like root mean square error (RMSE)
and mean absolute percentage error (MAPE). The model resulted in a prediction
accuracy of 98.83%. The results suggested that development of hybrid model
improved the accuracy of prediction. To serve the medicinal community for
prediction of liver disease among patients, a graphical user interface (GUI)
has been developed using R. The GUI is deployed as a package in local
repository of R platform for users to perform prediction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05535</identifier>
 <datestamp>2015-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05535</id><created>2015-02-19</created><authors><author><keyname>Filatov</keyname><forenames>Dmytro</forenames></author><author><keyname>Filatov</keyname><forenames>Taras</forenames></author></authors><title>Evolutionary algorithm based adaptive navigation in information
  retrieval interfaces</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In computer interfaces in general, especially in information retrieval tasks,
it is important to be able to quickly find and retrieve information. State of
the art approach, used, for example, in search engines, is not effective as it
introduces losses of meanings due to context to keywords back and forth
translation. Authors argue it increases the time and reduces the accuracy of
information retrieval compared to what it could be in the system that employs
modern information retrieval and text mining methods while presenting results
in an adaptive human- computer interface where system effectively learns what
operator needs through iterative interaction. In current work, a combination of
adaptive navigational interface and real time collaborative feedback analysis
for documents relevance weighting is proposed as an viable alternative to
prevailing &quot;telegraphic&quot; approach in information retrieval systems. Adaptive
navigation is provided through a dynamic links panel controlled by an
evolutionary algorithm. Documents relevance is initially established with
standard information retrieval techniques and is further refined in real time
through interaction of users with the system. Introduced concepts of
multidimensional Knowledge Map and Weighted Point of Interest allow finding
relevant documents and users with common interests through a trivial
calculation. Browsing search approach, the ability of the algorithm to adapt
navigation to users interests, collaborative refinement and the self-organising
features of the system are the main factors making such architecture effective
in various fields where non-structured knowledge shall be represented to the
users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05543</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05543</id><created>2015-02-19</created><updated>2015-04-07</updated><authors><author><keyname>Elkin</keyname><forenames>Michael</forenames></author><author><keyname>Filtser</keyname><forenames>Arnold</forenames></author><author><keyname>Neiman</keyname><forenames>Ofer</forenames></author></authors><title>Prioritized Metric Structures and Embedding</title><categories>cs.DS</categories><comments>To appear at STOC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Metric data structures (distance oracles, distance labeling schemes, routing
schemes) and low-distortion embeddings provide a powerful algorithmic
methodology, which has been successfully applied for approximation algorithms
\cite{llr}, online algorithms \cite{BBMN11}, distributed algorithms
\cite{KKMPT12} and for computing sparsifiers \cite{ST04}. However, this
methodology appears to have a limitation: the worst-case performance inherently
depends on the cardinality of the metric, and one could not specify in advance
which vertices/points should enjoy a better service (i.e., stretch/distortion,
label size/dimension) than that given by the worst-case guarantee.
  In this paper we alleviate this limitation by devising a suit of {\em
prioritized} metric data structures and embeddings. We show that given a
priority ranking $(x_1,x_2,\ldots,x_n)$ of the graph vertices (respectively,
metric points) one can devise a metric data structure (respectively, embedding)
in which the stretch (resp., distortion) incurred by any pair containing a
vertex $x_j$ will depend on the rank $j$ of the vertex. We also show that other
important parameters, such as the label size and (in some sense) the dimension,
may depend only on $j$. In some of our metric data structures (resp.,
embeddings) we achieve both prioritized stretch (resp., distortion) and label
size (resp., dimension) {\em simultaneously}. The worst-case performance of our
metric data structures and embeddings is typically asymptotically no worse than
of their non-prioritized counterparts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05545</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05545</id><created>2015-02-19</created><updated>2015-11-29</updated><authors><author><keyname>Menc</keyname><forenames>Artur</forenames></author><author><keyname>Paj&#x105;k</keyname><forenames>Dominik</forenames></author><author><keyname>Uzna&#x144;ski</keyname><forenames>Przemys&#x142;aw</forenames></author></authors><title>Time and space optimality of rotor-router graph exploration</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of exploration of an anonymous, port-labeled,
undirected graph with $n$ nodes and $m$ edges and diameter $D$, by a single
mobile agent. Initially the agent does not know the graph topology nor any of
the global parameters. Moreover, the agent does not know the incoming port when
entering to a vertex. Each vertex is endowed with memory that can be read and
modified by the agent upon its visit to that node. However the agent has no
operational memory i.e., it cannot carry any state while traversing an edge. In
such a model at least $\log_2 d$ bits are needed at each vertex of degree $d$
for the agent to be able to traverse each graph edge. This number of bits is
always sufficient to explore any graph in time $O(mD)$ using algorithm
Rotor-Router. We show that even if the available node memory is unlimited then
time $\Omega(n^3)$ is sometimes required for any algorithm. This shows that
Rotor-Router is asymptotically optimal in the worst-case graphs. Secondly we
show that for the case of the path the Rotor-Router attains exactly optimal
time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05548</identifier>
 <datestamp>2015-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05548</id><created>2015-02-19</created><authors><author><keyname>Filos-Ratsikas</keyname><forenames>Aris</forenames></author><author><keyname>Li</keyname><forenames>Minming</forenames></author><author><keyname>Zhang</keyname><forenames>Jie</forenames></author><author><keyname>Zhang</keyname><forenames>Qiang</forenames></author></authors><title>Facility location with double-peaked preference</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of locating a single facility on a real line based on
the reports of self-interested agents, when agents have double-peaked
preferences, with the peaks being on opposite sides of their locations. We
observe that double-peaked preferences capture real-life scenarios and thus
complement the well-studied notion of single-peaked preferences. We mainly
focus on the case where peaks are equidistant from the agents' locations and
discuss how our results extend to more general settings. We show that most of
the results for single-peaked preferences do not directly apply to this
setting; this makes the problem essentially more challenging. As our main
contribution, we present a simple truthful-in-expectation mechanism that
achieves an approximation ratio of 1+b/c for both the social and the maximum
cost, where b is the distance of the agent from the peak and c is the minimum
cost of an agent. For the latter case, we provide a 3/2 lower bound on the
approximation ratio of any truthful-in-expectation mechanism. We also study
deterministic mechanisms under some natural conditions, proving lower bounds
and approximation guarantees. We prove that among a large class of reasonable
mechanisms, there is no deterministic mechanism that outperforms our
truthful-in-expectation mechanism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05551</identifier>
 <datestamp>2015-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05551</id><created>2015-02-19</created><authors><author><keyname>Li</keyname><forenames>Zhong</forenames></author><author><keyname>Wang</keyname><forenames>Cheng</forenames></author><author><keyname>Shao</keyname><forenames>Lu</forenames></author><author><keyname>Jiang</keyname><forenames>Changjun</forenames></author></authors><title>Privately Information Sharing with Delusive Paths for Data Forwarding in
  Vehicular Networks</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss how to efficiently forward data in vehicular networks. Existing
solutions do not make full use of trajectory planning of nearby vehicles, or
social attributes. The development of onboard navigation system provides
drivers some traveling route information. The main novelty of our approach is
to envision sharing partial traveling information to the encountered vehicles
for better service. Our data forwarding algorithm utilizes this lightweight
information under the delusive paths privacy preservation together with the
social community structure in vehicular networks. We assume that data
transmission is carried by vehicles and road side units (RSUs), while cellular
network manages and coordinates relevant global information. The approximate
destination set is the set of RSUs that are often passed by the destination
vehicle. RSU importance is raised by summing encounter ratios of RSUs in the
same connected component. We first define a concept of space-time
approachability which is derived from shared partial traveling route and
encounter information. It describes the capability of a vehicle to advance
messages toward destination. Then, we design a novel data forwarding algorithm,
called approachability based algorithm, which combines the space-time
approachability with the social community attribute in vehicular networks. We
evaluate our approachability based algorithm on data sets from San Francisco
Cabspotting and Shanghai Taxi Movement. Results show that the partially shared
traveling information plays a positive role in data forwarding in vehicular
networks. Approachability based data forwarding algorithm achieves a better
performance than existing social based algorithms in vehicular networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05552</identifier>
 <datestamp>2015-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05552</id><created>2015-02-19</created><authors><author><keyname>Shah</keyname><forenames>Shalin</forenames></author><author><keyname>Dave</keyname><forenames>Parth</forenames></author><author><keyname>Gupta</keyname><forenames>Manish K</forenames></author></authors><title>Computing Real Numbers using DNA Self-Assembly</title><categories>cs.ET</categories><comments>21 pages, draft</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  DNA Self-Assembly has emerged as an interdisciplinary field with many
intriguing applications such DNA bio-sensor, DNA circuits, DNA storage, drug
delivery etc. Tile assembly model of DNA has been studied for various
computational primitives such as addition, subtraction, multiplication, and
division. Xuncai et. al. gave computational DNA tiles to perform division of a
number but the output had integer quotient. In this work, we simply modify
their method of division to improve its compatibility with further computation
and this modification has found its application in computing rational numbers,
both recurring and terminating, with computational tile complexity of
$\mathcal{O} (1)$ and $\mathcal{O} (h)$ respectively. Additionally, we also
propose a method to compute square-root of a number with computational tile
complexity of $\mathcal{O} (n)$ for an n bit number. Finally, after combining
tiles of division and square-root, we propose a simple way to compute the
ubiquitously used irrational number, $\pi$, using its infinite series.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05556</identifier>
 <datestamp>2015-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05556</id><created>2015-02-19</created><authors><author><keyname>Maystre</keyname><forenames>Lucas</forenames></author><author><keyname>Grossglauser</keyname><forenames>Matthias</forenames></author></authors><title>Robust Active Ranking from Sparse Noisy Comparisons</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  From sporting events to sociological surveys, ranking from pairwise
comparisons is a tool of choice for many applications. When certain pairs of
items are difficult to compare, outcomes can be noisy, and it is necessary to
develop robust strategies. In this work, we show how a simple active sampling
scheme that uses a standard black box sorting algorithm enables the efficient
recovery of the ranking, achieving low error with sparse samples. Both in
theory and practice, this active strategy performs systematically better than
selecting comparisons at random. As a detour, we show a link between Rank
Centrality, a recently proposed algorithm for rank aggregation, and the ML
estimator for the Bradley-Terry model. This enables us to develop a new,
provably convergent iterative algorithm for computing the ML estimate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05558</identifier>
 <datestamp>2015-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05558</id><created>2015-02-19</created><authors><author><keyname>Becker</keyname><forenames>Florent</forenames></author><author><keyname>Meunier</keyname><forenames>Pierre-&#xc9;tienne</forenames></author></authors><title>It's a Tough Nanoworld: in Tile Assembly, Cooperation is not (strictly)
  more Powerful than Competition</title><categories>cs.CG cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a strict separation between the class of &quot;mismatch free&quot;
self-assembly systems and general aTAM systems. Mismatch free systems are those
systems in which concurrently grown parts must always agree with each other.
  Tile self-assembly is a model of the formation of crystal growth, in which a
large number of particles concurrently and selectively stick to each other,
forming complex shapes and structures. It is useful in nanotechnologies, and
more generally in the understanding of these processes, ubiquitous in natural
systems.
  The other property of the local assembly process known to change the power of
the model is cooperation between two tiles to attach another. We show that
disagreement (mismatches) and cooperation are incomparable: neither can be used
to simulate the other one.
  The fact that mismatches are a hard property is especially surprising, since
no known, explicit construction of a computational device in tile assembly uses
mismatches, except for the recent construction of an intrinsically universal
tileset, i.e. a tileset capable of simulating any other tileset up to
rescaling. This work shows how to use intrinsic universality in a systematic
way to highlight the essence of different features of tile assembly.
  Moreover, even the most recent experimental realizations do not use
competition, which, in view of our results, suggests that a large part of the
natural phenomena behind DNA self-assembly remains to be understood
experimentally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05561</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05561</id><created>2015-02-19</created><updated>2015-03-26</updated><authors><author><keyname>Ghani</keyname><forenames>Neil</forenames><affiliation>University of Strathclyde</affiliation></author><author><keyname>Forsberg</keyname><forenames>Fredrik Nordvall</forenames><affiliation>University of Strathclyde</affiliation></author><author><keyname>Malatesta</keyname><forenames>Lorenzo</forenames><affiliation>University of Strathclyde</affiliation></author></authors><title>Positive Inductive-Recursive Definitions</title><categories>cs.LO</categories><comments>21 pages</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 11, Issue 1 (March 27,
  2015) lmcs:1154</journal-ref><doi>10.2168/LMCS-11(1:13)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new theory of data types which allows for the definition of types as
initial algebras of certain functors Fam(C) -&gt; Fam(C) is presented. This
theory, which we call positive inductive-recursive definitions, is a
generalisation of Dybjer and Setzer's theory of inductive-recursive definitions
within which C had to be discrete -- our work can therefore be seen as lifting
this restriction. This is a substantial endeavour as we need to not only
introduce a type of codes for such data types (as in Dybjer and Setzer's work),
but also a type of morphisms between such codes (which was not needed in Dybjer
and Setzer's development). We show how these codes are interpreted as functors
on Fam(C) and how these morphisms of codes are interpreted as natural
transformations between such functors. We then give an application of positive
inductive-recursive definitions to the theory of nested data types and we give
concrete examples of recursive functions defined on universes by using their
elimination principle. Finally we justify the existence of positive
inductive-recursive definitions by adapting Dybjer and Setzer's set-theoretic
model to our setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05562</identifier>
 <datestamp>2015-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05562</id><created>2015-02-19</created><authors><author><keyname>Patrascu</keyname><forenames>Vasile</forenames></author></authors><title>A New Penta-valued Logic Based Knowledge Representation</title><categories>cs.AI</categories><comments>The 12th International Conference Information Processing and
  Management of Uncertainty in Knowledge-Based Systems, June 22-27, 2008,
  Malaga, Spain</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a knowledge representation model are proposed, FP5, which
combine the ideas from fuzzy sets and penta-valued logic. FP5 represents
imprecise properties whose accomplished degree is undefined, contradictory or
indeterminate for some objects. Basic operations of conjunction, disjunction
and negation are introduced. Relations to other representation models like
fuzzy sets, intuitionistic, paraconsistent and bipolar fuzzy sets are
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05565</identifier>
 <datestamp>2015-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05565</id><created>2015-02-19</created><authors><author><keyname>Patrascu</keyname><forenames>Vasile</forenames></author></authors><title>Multi-valued Color Representation Based on Frank t-norm Properties</title><categories>cs.CV</categories><comments>12th International Conference Information Processing and Management
  of Uncertainty for Knowledge-Based Systems, IPMU'2008, pp. 1215-1222, June
  22-27, 2008, Malaga, Spain</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper two knowledge representation models are proposed, FP4 and FP6.
Both combine ideas from fuzzy sets and four-valued and hexa-valued logics. Both
represent imprecise properties whose accomplished degree is unknown or
contradictory for some objects. A possible application in the color analysis
and color image processing is discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05577</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05577</id><created>2015-02-19</created><updated>2015-08-08</updated><authors><author><keyname>A.</keyname><forenames>Prashanth L.</forenames></author><author><keyname>Bhatnagar</keyname><forenames>Shalabh</forenames></author><author><keyname>Fu</keyname><forenames>Michael</forenames></author><author><keyname>Marcus</keyname><forenames>Steve</forenames></author></authors><title>Adaptive system optimization using random directions stochastic
  approximation</title><categories>math.OC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present novel algorithms for simulation optimization using random
directions stochastic approximation (RDSA). These include first-order
(gradient) as well as second-order (Newton) schemes. We incorporate both
continuous-valued as well as discrete-valued perturbations into both our
algorithms. The former are chosen to be independent and identically distributed
(i.i.d.) symmetric, uniformly distributed random variables (r.v.), while the
latter are i.i.d., asymmetric, Bernoulli r.v.s. Our Newton algorithm, with a
novel Hessian estimation scheme, requires N-dimensional perturbations and three
loss measurements per iteration, whereas the simultaneous perturbation Newton
search algorithm of [1] requires 2N-dimensional perturbations and four loss
measurements per iteration. We prove the unbiasedness of both gradient and
Hessian estimates and asymptotic (strong) convergence for both first-order and
second-order schemes. We also provide asymptotic normality results, which in
particular establish that the asymmetric Bernoulli variant of Newton RDSA
method is better than 2SPSA of [1]. Numerical experiments are used to validate
the theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05578</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05578</id><created>2015-02-19</created><updated>2015-08-17</updated><authors><author><keyname>Papadopoulos</keyname><forenames>Fragkiskos</forenames></author><author><keyname>Aldecoa</keyname><forenames>Rodrigo</forenames></author><author><keyname>Krioukov</keyname><forenames>Dmitri</forenames></author></authors><title>Network Geometry Inference using Common Neighbors</title><categories>cs.SI cs.NI physics.soc-ph</categories><journal-ref>Phys. Rev. E 92, 022807 (2015)</journal-ref><doi>10.1103/PhysRevE.92.022807</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce and explore a new method for inferring hidden geometric
coordinates of nodes in complex networks based on the number of common
neighbors between the nodes. We compare this approach to the HyperMap method,
which is based only on the connections (and disconnections) between the nodes,
i.e., on the links that the nodes have (or do not have). We find that for high
degree nodes the common-neighbors approach yields a more accurate inference
than the link-based method, unless heuristic periodic adjustments (or
&quot;correction steps&quot;) are used in the latter. The common-neighbors approach is
computationally intensive, requiring $O(t^4)$ running time to map a network of
$t$ nodes, versus $O(t^3)$ in the link-based method. But we also develop a
hybrid method with $O(t^3)$ running time, which combines the common-neighbors
and link-based approaches, and explore a heuristic that reduces its running
time further to $O(t^2)$, without significant reduction in the mapping
accuracy. We apply this method to the Autonomous Systems (AS) Internet, and
reveal how soft communities of ASes evolve over time in the similarity space.
We further demonstrate the method's predictive power by forecasting future
links between ASes. Taken altogether, our results advance our understanding of
how to efficiently and accurately map real networks to their latent geometric
spaces, which is an important necessary step towards understanding the laws
that govern the dynamics of nodes in these spaces, and the fine-grained
dynamics of network connections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05591</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05591</id><created>2015-02-19</created><updated>2016-02-17</updated><authors><author><keyname>Coufal</keyname><forenames>David</forenames></author></authors><title>Radial Fuzzy Systems</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The class of radial fuzzy systems is introduced. The fuzzy systems in this
class use radial functions to implement membership functions of fuzzy sets and
exhibit a shape preservation property in antecedents of their rules. The
property is called the radial property. It enables the radial fuzzy systems to
have their computational model mathematically tractable under both conjunctive
and implicative representations of their rule bases. Coherence of radial
implicative fuzzy systems is discussed and a sufficient condition for coherence
is stated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05597</identifier>
 <datestamp>2015-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05597</id><created>2015-02-19</created><authors><author><keyname>Torres</keyname><forenames>Paulo</forenames></author><author><keyname>Charrua</keyname><forenames>Luis</forenames></author><author><keyname>Gusmao</keyname><forenames>Antonio</forenames></author></authors><title>On Detection Issues in the SC-based Uplink of a MU-MIMO System with a
  Large Number of BS Antennas</title><categories>cs.IT math.IT</categories><comments>7 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with SC/FDE within a MU-MIMO system where a large number of
BS antennas is adopted. In this context, either linear or reduced-complexity
iterative DF detection techniques are considered. Regarding performance
evaluation by simulation, appropriate semi-analytical methods are proposed.
This paper includes a detailed evaluation of BER performances for uncoded
4-Quadrature Amplitude Modulation (4-QAM) schemes and a MU-MIMO channel with
uncorrelated Rayleigh fading. The accuracy of performance results obtained
through the semi-analytical simulation methods is assessed by means of parallel
conventional Monte Carlo simulations, under the assumptions of perfect power
control and perfect channel estimation. The performance results are discussed
in detail, with the help of selected performance bounds. We emphasize that a
moderately large number of BS antennas is enough to closely approximate the
SIMO MFB performance, especially when using the suggested low-complexity
iterative DF technique, which does not require matrix inversion operations. We
also emphasize the achievable &quot;massive MIMO&quot; effects, even for strongly
reduced-complexity linear detection techniques, provided that the number of BS
antennas is much higher than the number of antennas which are jointly employed
in the terminals of the multiple autonomous users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05599</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05599</id><created>2015-02-15</created><updated>2015-02-20</updated><authors><author><keyname>Cicalese</keyname><forenames>Ferdinando</forenames></author><author><keyname>Cordasco</keyname><forenames>Gennaro</forenames></author><author><keyname>Gargano</keyname><forenames>Luisa</forenames></author><author><keyname>Milanic</keyname><forenames>Martin</forenames></author><author><keyname>Peters</keyname><forenames>Joseph</forenames></author><author><keyname>Vaccaro</keyname><forenames>Ugo</forenames></author></authors><title>Spread of Influence in Weighted Networks under Time and Budget
  Constraints</title><categories>cs.SI cs.DS math.CO</categories><comments>This paper will appear in the special issue of Theoretical Computer
  Science devoted to selected papers presented at Fun 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a network represented by a weighted directed graph G, we consider the
problem of finding a bounded cost set of nodes S such that the influence
spreading from S in G, within a given time bound, is as large as possible. The
dynamic that governs the spread of influence is the following: initially only
elements in S are influenced; subsequently at each round, the set of influenced
elements is augmented by all nodes in the network that have a sufficiently
large number of already influenced neighbors. We prove that the problem is
NP-hard, even in simple networks like complete graphs and trees. We also derive
a series of positive results. We present exact pseudo-polynomial time
algorithms for general trees, that become polynomial time in case the trees are
unweighted. This last result improves on previously published results. We also
design polynomial time algorithms for general weighted paths and cycles, and
for unweighted complete graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05614</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05614</id><created>2015-02-19</created><updated>2015-02-20</updated><authors><author><keyname>Abramovskaya</keyname><forenames>Tatjana V.</forenames></author><author><keyname>Fomin</keyname><forenames>Fedor V.</forenames></author><author><keyname>Golovach</keyname><forenames>Petr A.</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Micha&#x142;</forenames></author></authors><title>How to Hunt an Invisible Rabbit on a Graph</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate Hunters &amp; Rabbit game, where a set of hunters tries to catch
an invisible rabbit that slides along the edges of a graph. We show that the
minimum number of hunters required to win on an (n\times m)-grid is \lfloor
min{n,m}/2\rfloor+1. We also show that the extremal value of this number on
n-vertex trees is between \Omega(log n/log log n) and O(log n).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05615</identifier>
 <datestamp>2015-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05615</id><created>2015-02-19</created><authors><author><keyname>Mart&#xed;nez-Plumed</keyname><forenames>Fernando</forenames></author><author><keyname>Ferri</keyname><forenames>C&#xe8;sar</forenames></author><author><keyname>Hern&#xe1;ndez-Orallo</keyname><forenames>Jos&#xe9;</forenames></author><author><keyname>Ram&#xed;rez-Quintana</keyname><forenames>Mar&#xed;a Jos&#xe9;</forenames></author></authors><title>Forgetting and consolidation for incremental and cumulative knowledge
  acquisition systems</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The application of cognitive mechanisms to support knowledge acquisition is,
from our point of view, crucial for making the resulting models coherent,
efficient, credible, easy to use and understandable. In particular, there are
two characteristic features of intelligence that are essential for knowledge
development: forgetting and consolidation. Both plays an important role in
knowledge bases and learning systems to avoid possible information overflow and
redundancy, and in order to preserve and strengthen important or frequently
used rules and remove (or forget) useless ones. We present an incremental,
long-life view of knowledge acquisition which tries to improve task after task
by determining what to keep, what to consolidate and what to forget, overcoming
The Stability-Plasticity dilemma. In order to do that, we rate rules by
introducing several metrics through the first adaptation, to our knowledge, of
the Minimum Message Length (MML) principle to a coverage graph, a hierarchical
assessment structure which treats evidence and rules in a unified way. The
metrics are not only used to forget some of the worst rules, but also to set a
consolidation process to promote those selected rules to the knowledge base,
which is also mirrored by a demotion system. We evaluate the framework with a
series of tasks in a chess rule learning domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05618</identifier>
 <datestamp>2015-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05618</id><created>2015-02-19</created><authors><author><keyname>Elwes</keyname><forenames>Richard</forenames></author></authors><title>Preferential Attachment Processes Approaching The Rado Multigraph</title><categories>math.CO cs.SI math.PR</categories><comments>21 pages</comments><msc-class>05C80</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a preferential attachment process in which the number of edges
added at stage $t$ is given by some prescribed function $f(t)$, generalising a
model considered by Kleinberg and Kleinberg ([8]). We show that if $f(t)$ is
asymptotically bounded above and below by linear functions in $t$, then with
probability $1$ the infinite limit of the process is isomorphic to the Rado
multigraph. This structure is the natural multigraph analogue of the famous
Rado graph, which we introduce here.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05623</identifier>
 <datestamp>2015-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05623</id><created>2015-02-19</created><authors><author><keyname>Gallet</keyname><forenames>Matteo</forenames></author><author><keyname>Koutschan</keyname><forenames>Christoph</forenames></author><author><keyname>Li</keyname><forenames>Zijia</forenames></author><author><keyname>Regensburger</keyname><forenames>Georg</forenames></author><author><keyname>Schicho</keyname><forenames>Josef</forenames></author><author><keyname>Villamizar</keyname><forenames>Nelly</forenames></author></authors><title>Planar Linkages Following a Prescribed Motion</title><categories>cs.SC cs.CG cs.RO math.AG math.RA</categories><comments>32 pages, 13 figures</comments><msc-class>70B15, 68W30, 70G55, 20G20, 16Z05, 14P05, 12Y05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing mechanical devices, called linkages, that draw a given plane curve
has been a topic that interested engineers and mathematicians for hundreds of
years, and recently also computer scientists. Already in 1876, Kempe proposed a
procedure for solving the problem in full generality, but his constructions
tend to be extremely complicated. We provide a novel algorithm that produces
much simpler linkages, but works only for parametric curves. Our approach is to
transform the problem into a factorization task over some noncommutative
algebra. We show how to compute such a factorization, and how to use it to
construct a linkage tracing a given curve.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05632</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05632</id><created>2015-02-19</created><updated>2015-04-27</updated><authors><author><keyname>Ronnholm</keyname><forenames>Raine</forenames></author></authors><title>Capturing k-ary Existential Second Order Logic with k-ary
  Inclusion-Exclusion Logic</title><categories>math.LO cs.LO</categories><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we analyze k-ary inclusion-exclusion logic, INEX[k], which is
obtained by extending first order logic with k-ary inclusion and exclusion
atoms. We show that every formula of INEX[k] can be expressed with a formula of
k-ary existential second order logic, ESO[k]. Conversely, every formula of
ESO[k] with at most k-ary free relation variables can be expressed with a
formula of INEX[k]. From this it follows that, on the level of sentences,
INEX[k] captures the expressive power of ESO[k].
  We also introduce several useful operators that can be expressed in INEX[k].
In particular, we define inclusion and exclusion quantifiers and so-called term
value preserving disjunction. The latter one is needed in the proofs of the
main results in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05675</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05675</id><created>2015-02-19</created><updated>2015-02-20</updated><authors><author><keyname>Magdon-Ismail</keyname><forenames>Malik</forenames></author></authors><title>NP-Hardness and Inapproximability of Sparse PCA</title><categories>cs.LG cs.CC cs.DS math.CO stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a reduction from {\sc clique} to establish that sparse PCA is
NP-hard. The reduction has a gap which we use to exclude an FPTAS for sparse
PCA (unless P=NP). Under weaker complexity assumptions, we also exclude
polynomial constant-factor approximation algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05676</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05676</id><created>2015-02-19</created><updated>2015-04-24</updated><authors><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author><author><keyname>Heimeriks</keyname><forenames>Gaston</forenames></author><author><keyname>Rotolo</keyname><forenames>Daniele</forenames></author></authors><title>Journal Portfolio Analysis for Countries, Cities, and Organizations:
  Maps and Comparisons</title><categories>cs.DL</categories><comments>accepted for publication by the Journal of the Association for
  Information Science and Technology (March 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using Web-of-Science data, portfolio analysis in terms of journal coverage
can be projected on a base map for units of analysis such as countries, cities,
universities, and firms. The units of analysis under study can be compared
statistically across the 10,000+ journals. The interdisciplinarity of the
portfolios is measured using Rao-Stirling diversity or Zhang et al.'s (in
press) improved measure 2D3. At the country level we find regional
differentiation (e.g., Latin-American or Asian countries), but also a major
divide between advanced and less-developed countries. Israel and Israeli cities
outperform other nations and cities in terms of diversity. Universities appear
to be specifically related to firms when a number of these units are
exploratively compared. The instrument is relatively simple and
straightforward, and one can generalize the application to any document set
retrieved from WoS. Further instruction is provided online at
http://www.leydesdorff.net/portfolio .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05678</identifier>
 <datestamp>2015-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05678</id><created>2015-02-19</created><updated>2015-04-16</updated><authors><author><keyname>Mathialagan</keyname><forenames>Clint Solomon</forenames></author><author><keyname>Gallagher</keyname><forenames>Andrew C.</forenames></author><author><keyname>Batra</keyname><forenames>Dhruv</forenames></author></authors><title>VIP: Finding Important People in Images</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  People preserve memories of events such as birthdays, weddings, or vacations
by capturing photos, often depicting groups of people. Invariably, some
individuals in the image are more important than others given the context of
the event. This paper analyzes the concept of the importance of individuals in
group photographs. We address two specific questions -- Given an image, who are
the most important individuals in it? Given multiple images of a person, which
image depicts the person in the most important role? We introduce a measure of
importance of people in images and investigate the correlation between
importance and visual saliency. We find that not only can we automatically
predict the importance of people from purely visual cues, incorporating this
predicted importance results in significant improvement in applications such as
im2text (generating sentences that describe images of groups of people).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05680</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05680</id><created>2015-02-19</created><updated>2015-07-30</updated><authors><author><keyname>Montanari</keyname><forenames>Andrea</forenames></author></authors><title>Finding One Community in a Sparse Graph</title><categories>stat.ML cond-mat.stat-mech cs.SI</categories><comments>30 pages, 8 pdf figures</comments><doi>10.1007/s10955-015-1338-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a random sparse graph with bounded average degree, in which a
subset of vertices has higher connectivity than the background. In particular,
the average degree inside this subset of vertices is larger than outside (but
still bounded). Given a realization of such graph, we aim at identifying the
hidden subset of vertices. This can be regarded as a model for the problem of
finding a tightly knitted community in a social network, or a cluster in a
relational dataset.
  In this paper we present two sets of contributions: $(i)$ We use the cavity
method from spin glass theory to derive an exact phase diagram for the
reconstruction problem. In particular, as the difference in edge probability
increases, the problem undergoes two phase transitions, a static phase
transition and a dynamic one. $(ii)$ We establish rigorous bounds on the
dynamic phase transition and prove that, above a certain threshold, a local
algorithm (belief propagation) correctly identify most of the hidden set. Below
the same threshold \emph{no local algorithm} can achieve this goal. However, in
this regime the subset can be identified by exhaustive search.
  For small hidden sets and large average degree, the phase transition for
local algorithms takes an intriguingly simple form. Local algorithms succeed
with high probability for ${\rm deg}_{\rm in} - {\rm deg}_{\rm out} &gt;
\sqrt{{\rm deg}_{\rm out}/e}$ and fail for ${\rm deg}_{\rm in} - {\rm deg}_{\rm
out} &lt; \sqrt{{\rm deg}_{\rm out}/e}$ (with ${\rm deg}_{\rm in}$, ${\rm
deg}_{\rm out}$ the average degrees inside and outside the community). We argue
that spectral algorithms are also ineffective in the latter regime.
  It is an open problem whether any polynomial time algorithms might succeed
for ${\rm deg}_{\rm in} - {\rm deg}_{\rm out} &lt; \sqrt{{\rm deg}_{\rm out}/e}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05689</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05689</id><created>2015-02-19</created><updated>2016-01-22</updated><authors><author><keyname>Liu</keyname><forenames>Ming-Yu</forenames></author><author><keyname>Mallya</keyname><forenames>Arun</forenames></author><author><keyname>Tuzel</keyname><forenames>Oncel C.</forenames></author><author><keyname>Chen</keyname><forenames>Xi</forenames></author></authors><title>Unsupervised Network Pretraining via Encoding Human Design</title><categories>cs.CV</categories><comments>9 pages, 11 figures, WACV 2016: IEEE Conference on Applications of
  Computer Vision</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the years, computer vision researchers have spent an immense amount of
effort on designing image features for the visual object recognition task. We
propose to incorporate this valuable experience to guide the task of training
deep neural networks. Our idea is to pretrain the network through the task of
replicating the process of hand-designed feature extraction. By learning to
replicate the process, the neural network integrates previous research
knowledge and learns to model visual objects in a way similar to the
hand-designed features. In the succeeding finetuning step, it further learns
object-specific representations from labeled data and this boosts its
classification power. We pretrain two convolutional neural networks where one
replicates the process of histogram of oriented gradients feature extraction,
and the other replicates the process of region covariance feature extraction.
After finetuning, we achieve substantially better performance than the baseline
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05696</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05696</id><created>2015-02-19</created><updated>2015-09-07</updated><authors><author><keyname>Shah</keyname><forenames>Nihar B.</forenames></author><author><keyname>Zhou</keyname><forenames>Dengyong</forenames></author><author><keyname>Peres</keyname><forenames>Yuval</forenames></author></authors><title>Approval Voting and Incentives in Crowdsourcing</title><categories>cs.GT cs.AI cs.LG cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The growing need for labeled training data has made crowdsourcing an
important part of machine learning. The quality of crowdsourced labels is,
however, adversely affected by three factors: (1) the workers are not experts;
(2) the incentives of the workers are not aligned with those of the requesters;
and (3) the interface does not allow workers to convey their knowledge
accurately, by forcing them to make a single choice among a set of options. In
this paper, we address these issues by introducing approval voting to utilize
the expertise of workers who have partial knowledge of the true answer, and
coupling it with a (&quot;strictly proper&quot;) incentive-compatible compensation
mechanism. We show rigorous theoretical guarantees of optimality of our
mechanism together with a simple axiomatic characterization. We also conduct
preliminary empirical studies on Amazon Mechanical Turk which validate our
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05698</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05698</id><created>2015-02-19</created><updated>2015-12-31</updated><authors><author><keyname>Weston</keyname><forenames>Jason</forenames></author><author><keyname>Bordes</keyname><forenames>Antoine</forenames></author><author><keyname>Chopra</keyname><forenames>Sumit</forenames></author><author><keyname>Rush</keyname><forenames>Alexander M.</forenames></author><author><keyname>van Merri&#xeb;nboer</keyname><forenames>Bart</forenames></author><author><keyname>Joulin</keyname><forenames>Armand</forenames></author><author><keyname>Mikolov</keyname><forenames>Tomas</forenames></author></authors><title>Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks</title><categories>cs.AI cs.CL stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One long-term goal of machine learning research is to produce methods that
are applicable to reasoning and natural language, in particular building an
intelligent dialogue agent. To measure progress towards that goal, we argue for
the usefulness of a set of proxy tasks that evaluate reading comprehension via
question answering. Our tasks measure understanding in several ways: whether a
system is able to answer questions via chaining facts, simple induction,
deduction and many more. The tasks are designed to be prerequisites for any
system that aims to be capable of conversing with a human. We believe many
existing learning systems can currently not solve them, and hence our aim is to
classify these tasks into skill sets, so that researchers can identify (and
then rectify) the failings of their systems. We also extend and improve the
recently introduced Memory Networks model, and show it is able to solve some,
but not all, of the tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05701</identifier>
 <datestamp>2015-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05701</id><created>2015-02-19</created><authors><author><keyname>Haustein</keyname><forenames>Stefanie</forenames></author><author><keyname>Bowman</keyname><forenames>Timothy D.</forenames></author><author><keyname>Costas</keyname><forenames>Rodrigo</forenames></author></authors><title>Interpreting &quot;altmetrics&quot;: viewing acts on social media through the lens
  of citation and social theories</title><categories>cs.DL</categories><comments>to be published in: Cassidy R. Sugimoto (Ed.). Theories of
  Informetrics: A Festschrift in Honor of Blaise Cronin</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  More than 30 years after Cronin's seminal paper on &quot;the need for a theory of
citing&quot; (Cronin, 1981), the metrics community is once again in need of a new
theory, this time one for so-called &quot;altmetrics&quot;. Altmetrics, short for
alternative (to citation) metrics -- and as such a misnomer -- refers to a new
group of metrics based (largely) on social media events relating to scholarly
communication. As current definitions of altmetrics are shaped and limited by
active platforms, technical possibilities, and business models of aggregators
such as Altmetric.com, ImpactStory, PLOS, and Plum Analytics, and as such
constantly changing, this work refrains from defining an umbrella term for
these very heterogeneous new metrics. Instead a framework is presented that
describes acts leading to (online) events on which the metrics are based. These
activities occur in the context of social media, such as discussing on Twitter
or saving to Mendeley, as well as downloading and citing. The framework groups
various types of acts into three categories -- accessing, appraising, and
applying -- and provides examples of actions that lead to visibility and
traceability online. To improve the understanding of the acts, which result in
online events from which metrics are collected, select citation and social
theories are used to interpret the phenomena being measured. Citation theories
are used because the new metrics based on these events are supposed to replace
or complement citations as indicators of impact. Social theories, on the other
hand, are discussed because there is an inherent social aspect to the
measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05729</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05729</id><created>2015-02-19</created><authors><author><keyname>Knudsen</keyname><forenames>Mathias B&#xe6;k Tejs</forenames></author><author><keyname>St&#xf6;ckel</keyname><forenames>Morten</forenames></author></authors><title>Quicksort, Largest Bucket, and Min-Wise Hashing with Limited
  Independence</title><categories>cs.DS</categories><comments>Submitted to ICALP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Randomized algorithms and data structures are often analyzed under the
assumption of access to a perfect source of randomness. The most fundamental
metric used to measure how &quot;random&quot; a hash function or a random number
generator is, is its independence: a sequence of random variables is said to be
$k$-independent if every variable is uniform and every size $k$ subset is
independent. In this paper we consider three classic algorithms under limited
independence. We provide new bounds for randomized quicksort, min-wise hashing
and largest bucket size under limited independence. Our results can be
summarized as follows.
  -Randomized quicksort. When pivot elements are computed using a
$5$-independent hash function, Karloff and Raghavan, J.ACM'93 showed $O ( n
\log n)$ expected worst-case running time for a special version of quicksort.
We improve upon this, showing that the same running time is achieved with only
$4$-independence.
  -Min-wise hashing. For a set $A$, consider the probability of a particular
element being mapped to the smallest hash value. It is known that
$5$-independence implies the optimal probability $O (1 /n)$. Broder et al.,
STOC'98 showed that $2$-independence implies it is $O(1 / \sqrt{|A|})$. We show
a matching lower bound as well as new tight bounds for $3$- and $4$-independent
hash functions.
  -Largest bucket. We consider the case where $n$ balls are distributed to $n$
buckets using a $k$-independent hash function and analyze the largest bucket
size. Alon et. al, STOC'97 showed that there exists a $2$-independent hash
function implying a bucket of size $\Omega ( n^{1/2})$. We generalize the
bound, providing a $k$-independent family of functions that imply size $\Omega
( n^{1/k})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05730</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05730</id><created>2015-02-19</created><authors><author><keyname>Pluzhnik</keyname><forenames>Evgeniy</forenames></author><author><keyname>Lukyanchikov</keyname><forenames>Oleg</forenames></author><author><keyname>Nikulchev</keyname><forenames>Evgeny</forenames></author><author><keyname>Payain</keyname><forenames>Simon</forenames></author></authors><title>Designing Applications with Distributed Databases in a Hybrid Cloud</title><categories>cs.DC cs.DB</categories><comments>in WIT Transactions of Information and Communication Technologies,
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing applications for use in a hybrid cloud has many features. These
include dynamic virtualization management and an unknown route switching
customers. This makes it impossible to evaluate the query and hence the optimal
distribution of data. In this paper, we formulate the main challenges of
designing and simulation offer installation for processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05742</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05742</id><created>2015-02-19</created><updated>2015-07-28</updated><authors><author><keyname>Baghaie</keyname><forenames>Ahmadreza</forenames></author><author><keyname>D'souza</keyname><forenames>Roshan M.</forenames></author><author><keyname>Yu</keyname><forenames>Zeyun</forenames></author></authors><title>Application of Independent Component Analysis Techniques in Speckle
  Noise Reduction of Retinal OCT Images</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optical Coherence Tomography (OCT) is an emerging technique in the field of
biomedical imaging, with applications in ophthalmology, dermatology, coronary
imaging etc. OCT images usually suffer from a granular pattern, called speckle
noise, which restricts the process of interpretation. Therefore the need for
speckle noise reduction techniques is of high importance. To the best of our
knowledge, use of Independent Component Analysis (ICA) techniques has never
been explored for speckle reduction of OCT images. Here, a comparative study of
several ICA techniques (InfoMax, JADE, FastICA and SOBI) is provided for noise
reduction of retinal OCT images. Having multiple B-scans of the same location,
the eye movements are compensated using a rigid registration technique. Then,
different ICA techniques are applied to the aggregated set of B-scans for
extracting the noise-free image. Signal-to-Noise-Ratio (SNR),
Contrast-to-Noise-Ratio (CNR) and Equivalent-Number-of-Looks (ENL), as well as
analysis on the computational complexity of the methods, are considered as
metrics for comparison. The results show that use of ICA can be beneficial,
especially in case of having fewer number of B-scans.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05744</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05744</id><created>2015-02-19</created><updated>2015-07-01</updated><authors><author><keyname>Orabona</keyname><forenames>Francesco</forenames></author><author><keyname>Pal</keyname><forenames>David</forenames></author></authors><title>Scale-Free Algorithms for Online Linear Optimization</title><categories>cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We design algorithms for online linear optimization that have optimal regret
and at the same time do not need to know any upper or lower bounds on the norm
of the loss vectors. We achieve adaptiveness to norms of loss vectors by scale
invariance, i.e., our algorithms make exactly the same decisions if the
sequence of loss vectors is multiplied by any positive constant. Our algorithms
work for any decision set, bounded or unbounded. For unbounded decisions sets,
these are the first truly adaptive algorithms for online linear optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05745</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05745</id><created>2015-02-19</created><authors><author><keyname>Alistarh</keyname><forenames>Dan</forenames></author><author><keyname>Gelashvili</keyname><forenames>Rati</forenames></author></authors><title>Polylogarithmic-Time Leader Election in Population Protocols Using
  Polylogarithmic States</title><categories>cs.DC</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Population protocols are networks of finite-state agents, interacting
randomly, and updating their states using simple rules. Despite their extreme
simplicity, these systems have been shown to cooperatively perform complex
computational tasks, such as simulating register machines to compute standard
arithmetic functions. The election of a unique leader agent is a key
requirement in such computational constructions. Yet, the fastest currently
known population protocol for electing a leader only has linear convergence
time, and, it has recently been shown that no population protocol using a
constant number of states per node may overcome this linear bound.
  In this paper, we give the first population protocol for leader election with
polylogarithmic convergence time, using polylogarithmic memory states per node.
The protocol structure is quite simple: each node has an associated value, and
is either a leader (still in contention) or a minion (following some leader). A
leader keeps incrementing its value and &quot;defeats&quot; other leaders in one-to-one
interactions, and will drop from contention and become a minion if it meets a
leader with higher value. Importantly, a leader also drops out if it meets a
minion with higher absolute value. While these rules are quite simple, the
proof that this algorithm achieves polylogarithmic convergence time is
non-trivial. In particular, the argument combines careful use of concentration
inequalities with anti-concentration bounds, showing that the leaders' values
become spread apart as the execution progresses, which in turn implies that
straggling leaders get quickly eliminated. We complement our analysis with
empirical results, showing that our protocol converges extremely fast, even for
large network sizes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05746</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05746</id><created>2015-02-19</created><authors><author><keyname>Yi</keyname><forenames>Xinyang</forenames></author><author><keyname>Caramanis</keyname><forenames>Constantine</forenames></author><author><keyname>Price</keyname><forenames>Eric</forenames></author></authors><title>Binary Embedding: Fundamental Limits and Fast Algorithm</title><categories>cs.DS cs.IT math.IT</categories><comments>30 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Binary embedding is a nonlinear dimension reduction methodology where high
dimensional data are embedded into the Hamming cube while preserving the
structure of the original space. Specifically, for an arbitrary $N$ distinct
points in $\mathbb{S}^{p-1}$, our goal is to encode each point using
$m$-dimensional binary strings such that we can reconstruct their geodesic
distance up to $\delta$ uniform distortion. Existing binary embedding
algorithms either lack theoretical guarantees or suffer from running time
$O\big(mp\big)$. We make three contributions: (1) we establish a lower bound
that shows any binary embedding oblivious to the set of points requires $m =
\Omega(\frac{1}{\delta^2}\log{N})$ bits and a similar lower bound for
non-oblivious embeddings into Hamming distance; (2) we propose a novel fast
binary embedding algorithm with provably optimal bit complexity $m =
O\big(\frac{1}{\delta^2}\log{N}\big)$ and near linear running time $O(p \log
p)$ whenever $\log N \ll \delta \sqrt{p}$, with a slightly worse running time
for larger $\log N$; (3) we also provide an analytic result about embedding a
general set of points $K \subseteq \mathbb{S}^{p-1}$ with even infinite size.
Our theoretical findings are supported through experiments on both synthetic
and real data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05748</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05748</id><created>2015-02-19</created><updated>2015-08-13</updated><authors><author><keyname>Rosenmann</keyname><forenames>Amnon</forenames></author></authors><title>A Multiple-Valued Logic Approach to the Design and Verification of
  Hardware Circuits</title><categories>cs.LO</categories><comments>34 pages, 4 figures</comments><acm-class>B.5.2; B.7.2; I.6.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel approach, which is based on multiple-valued logic (MVL),
to the verification and analysis of digital hardware designs, which extends the
common ternary or quaternary approaches for simulations. The simulations which
are performed in the more informative MVL setting reveal details which are
either invisible or harder to detect through binary or ternary simulations. In
equivalence verification, detecting different behavior under MVL simulations
may lead to the discovery of a genuine binary nonequivalence or to a
qualitative gap between two designs. The value of a variable in a simulation
may hold information about its degree of truth and its &quot;place of birth&quot; and
&quot;date of birth.&quot; Applications include equivalence verification, initialization,
assertions generation and verification, partial control on the flow of data by
prioritizing and block-oriented simulations. Much of the paper is devoted to
theoretical aspects behind the MVL approach, including the reason for choosing
a specific algebra for computations, and the introduction of the verification
complexity of a Boolean expression. Two basic algorithms are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05751</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05751</id><created>2015-02-19</created><updated>2015-07-09</updated><authors><author><keyname>De Sena</keyname><forenames>Enzo</forenames></author><author><keyname>Hacihabiboglu</keyname><forenames>Huseyin</forenames></author><author><keyname>Cvetkovic</keyname><forenames>Zoran</forenames></author><author><keyname>Smith</keyname><forenames>Julius O.</forenames><suffix>III</suffix></author></authors><title>Efficient Synthesis of Room Acoustics via Scattering Delay Networks</title><categories>cs.SD cs.MM</categories><journal-ref>IEEE/ACM Transactions on Audio, Speech, and Language Processing,
  Vol. 23, No. 9, September 2015</journal-ref><doi>10.1109/TASLP.2015.2438547</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An acoustic reverberator consisting of a network of delay lines connected via
scattering junctions is proposed. All parameters of the reverberator are
derived from physical properties of the enclosure it simulates. It allows for
simulation of unequal and frequency-dependent wall absorption, as well as
directional sources and microphones. The reverberator renders the first-order
reflections exactly, while making progressively coarser approximations of
higher-order reflections. The rate of energy decay is close to that obtained
with the image method (IM) and consistent with the predictions of Sabine and
Eyring equations. The time evolution of the normalized echo density, which was
previously shown to be correlated with the perceived texture of reverberation,
is also close to that of IM. However, its computational complexity is one to
two orders of magnitude lower, comparable to the computational complexity of a
feedback delay network (FDN), and its memory requirements are negligible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05752</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05752</id><created>2015-02-19</created><authors><author><keyname>Fu</keyname><forenames>Zhenyong</forenames></author><author><keyname>Lu</keyname><forenames>Zhiwu</forenames></author></authors><title>Pairwise Constraint Propagation: A Survey</title><categories>cs.CV cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As one of the most important types of (weaker) supervised information in
machine learning and pattern recognition, pairwise constraint, which specifies
whether a pair of data points occur together, has recently received significant
attention, especially the problem of pairwise constraint propagation. At least
two reasons account for this trend: the first is that compared to the data
label, pairwise constraints are more general and easily to collect, and the
second is that since the available pairwise constraints are usually limited,
the constraint propagation problem is thus important.
  This paper provides an up-to-date critical survey of pairwise constraint
propagation research. There are two underlying motivations for us to write this
survey paper: the first is to provide an up-to-date review of the existing
literature, and the second is to offer some insights into the studies of
pairwise constraint propagation. To provide a comprehensive survey, we not only
categorize existing propagation techniques but also present detailed
descriptions of representative methods within each category.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05760</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05760</id><created>2015-02-19</created><authors><author><keyname>Zhou</keyname><forenames>Bin</forenames></author><author><keyname>He</keyname><forenames>Zhe</forenames></author><author><keyname>Jiang</keyname><forenames>Luo-Luo</forenames></author><author><keyname>Wang</keyname><forenames>Nian-Xin</forenames></author><author><keyname>Wang</keyname><forenames>Bing-Hong</forenames></author></authors><title>Bidirectional selection between two classes in complex social networks</title><categories>physics.soc-ph cs.SI</categories><doi>10.1038/srep07577</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The bidirectional selection between two classes widely emerges in various
social lives, such as commercial trading and mate choosing. Until now, the
discussions on bidirectional selection in structured human society are quite
limited. We demonstrated theoretically that the rate of successfully matching
is affected greatly by individuals neighborhoods in social networks, regardless
of the type of networks. Furthermore, it is found that the high average degree
of networks contributes to increasing rates of successful matches. The matching
performance in different types of networks has been quantitatively
investigated, revealing that the small-world networks reinforces the matching
rate more than scale-free networks at given average degree. In addition, our
analysis is consistent with the modeling result, which provides the theoretical
understanding of underlying mechanisms of matching in complex networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05767</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05767</id><created>2015-02-19</created><updated>2015-04-19</updated><authors><author><keyname>Baydin</keyname><forenames>Atilim Gunes</forenames></author><author><keyname>Pearlmutter</keyname><forenames>Barak A.</forenames></author><author><keyname>Radul</keyname><forenames>Alexey Andreyevich</forenames></author><author><keyname>Siskind</keyname><forenames>Jeffrey Mark</forenames></author></authors><title>Automatic differentiation in machine learning: a survey</title><categories>cs.SC cs.LG</categories><comments>29 pages, 5 figures</comments><msc-class>68W30, 65D25, 68T05</msc-class><acm-class>G.1.4; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in
machine learning. Automatic differentiation (AD) is a technique for calculating
derivatives of numeric functions expressed as computer programs efficiently and
accurately, used in fields such as computational fluid dynamics, nuclear
engineering, and atmospheric sciences. Despite its advantages and use in other
fields, machine learning practitioners have been little influenced by AD and
make scant use of available tools. We survey the intersection of AD and machine
learning, cover applications where AD has the potential to make a big impact,
and report on some recent developments in the adoption of this technique. We
aim to dispel some misconceptions that we contend have impeded the use of AD
within the machine learning community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05773</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05773</id><created>2015-02-19</created><updated>2015-04-20</updated><authors><author><keyname>Banerjee</keyname><forenames>Pradeep Kr.</forenames></author></authors><title>Multipartite Monotones for Secure Sampling by Public Discussion From
  Noisy Correlations</title><categories>cs.IT math.IT</categories><comments>6 pages, 2 figures, v2: Improved readability</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of quantifying the cryptographic content of
probability distributions, in relation to an application to secure multi-party
sampling against a passive t-adversary. We generalize a recently introduced
notion of assisted common information of a pair of correlated sources to that
of K sources and define a family of monotone rate regions indexed by K. This
allows for a simple characterization of all t-private distributions that can be
statistically securely sampled without any auxiliary setup of pre-shared noisy
correlations. We also give a new monotone called the residual total correlation
that admits a simple operational interpretation. Interestingly, for sampling
with non-trivial setups (K &gt; 2) in the public discussion model, our definition
of a monotone region differs from the one by Prabhakaran and Prabhakaran (ITW
2012).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05774</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05774</id><created>2015-02-20</created><updated>2015-06-05</updated><authors><author><keyname>Abernethy</keyname><forenames>Jacob</forenames></author><author><keyname>Chen</keyname><forenames>Yiling</forenames></author><author><keyname>Ho</keyname><forenames>Chien-Ju</forenames></author><author><keyname>Waggoner</keyname><forenames>Bo</forenames></author></authors><title>Low-Cost Learning via Active Data Procurement</title><categories>cs.GT cs.AI cs.LG stat.ML</categories><comments>Full version of EC 2015 paper. Color recommended for figures but
  nonessential. 36 pages, of which 12 appendix</comments><acm-class>J.4; I.2.6</acm-class><doi>10.1145/2764468.2764519</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We design mechanisms for online procurement of data held by strategic agents
for machine learning tasks. The challenge is to use past data to actively price
future data and give learning guarantees even when an agent's cost for
revealing her data may depend arbitrarily on the data itself. We achieve this
goal by showing how to convert a large class of no-regret algorithms into
online posted-price and learning mechanisms. Our results in a sense parallel
classic sample complexity guarantees, but with the key resource being money
rather than quantity of data: With a budget constraint $B$, we give robust risk
(predictive error) bounds on the order of $1/\sqrt{B}$. Because we use an
active approach, we can often guarantee to do significantly better by
leveraging correlations between costs and data.
  Our algorithms and analysis go through a model of no-regret learning with $T$
arriving pairs (cost, data) and a budget constraint of $B$. Our regret bounds
for this model are on the order of $T/\sqrt{B}$ and we give lower bounds on the
same order.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05775</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05775</id><created>2015-02-20</created><updated>2015-05-30</updated><authors><author><keyname>Banerjee</keyname><forenames>Pradeep Kr.</forenames></author></authors><title>A Secret Common Information Duality for Tripartite Noisy Correlations</title><categories>cs.IT math.IT</categories><comments>12 pages, 1 figure, v3: Improved readability</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the duality between the simulation and extraction of secret
correlations in light of a similar well-known operational duality between the
two notions of common information due to Wyner, and G\'acs and K\&quot;orner. For
the inverse problem of simulating a tripartite noisy correlation from noiseless
secret key and unlimited public communication, we show that Winter's (2005)
result for the key cost in terms of a conditional version of Wyner's common
information can be simply reexpressed in terms of the existence of a bipartite
protocol monotone. For the forward problem of key distillation from noisy
correlations, we construct simple distributions for which the conditional
G\'acs and K\&quot;orner common information achieves a tight bound on the secret key
rate. We conjecture that this holds in general for non-communicative key
agreement models. We also comment on the interconvertibility of secret
correlations under local operations and public communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05777</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05777</id><created>2015-02-20</created><authors><author><keyname>Henderson</keyname><forenames>James A.</forenames></author><author><keyname>Gibson</keyname><forenames>TingTing A.</forenames></author><author><keyname>Wiles</keyname><forenames>Janet</forenames></author></authors><title>Spike Event Based Learning in Neural Networks</title><categories>cs.NE cs.LG</categories><comments>Figure 4 can be viewed as a movie in a separate file</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A scheme is derived for learning connectivity in spiking neural networks. The
scheme learns instantaneous firing rates that are conditional on the activity
in other parts of the network. The scheme is independent of the choice of
neuron dynamics or activation function, and network architecture. It involves
two simple, online, local learning rules that are applied only in response to
occurrences of spike events. This scheme provides a direct method for
transferring ideas between the fields of deep learning and computational
neuroscience. This learning scheme is demonstrated using a layered feedforward
spiking neural network trained self-supervised on a prediction and
classification task for moving MNIST images collected using a Dynamic Vision
Sensor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05782</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05782</id><created>2015-02-20</created><authors><author><keyname>Li</keyname><forenames>Xiao</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr.</suffix></author><author><keyname>Linehan</keyname><forenames>Kevin</forenames></author><author><keyname>Butler</keyname><forenames>Ray</forenames></author></authors><title>Impact of Metro Cell Antenna Pattern and Downtilt in Heterogeneous
  Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>7 pages, 9 figues, submitted to Vehicular Technology Magazine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article discusses the positive impact metro cell antennas with narrow
vertical beamwidth and electrical downtilt can have on heterogeneous cellular
networks. Using a model of random cell placement based on Poisson distribution,
along with an innovative 3D building model that quantifies blockage due to
shadowing, it is demonstrated that network spectral efficiency and average user
throughput both increase as vertical beamwidth is decreased and downtilt is
applied to metro cell transmission. Moreover, the network becomes more energy
efficient. Importantly, these additional gains in network performance can be
achieved without any cooperation or exchange of information between macro cell
base stations and metro cells.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05784</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05784</id><created>2015-02-20</created><authors><author><keyname>Ferrett</keyname><forenames>Terry</forenames></author><author><keyname>Valenti</keyname><forenames>Matthew C.</forenames></author></authors><title>LDPC Code Design for Noncoherent Physical Layer Network Coding</title><categories>cs.IT math.IT</categories><comments>Six pages, submitted to 2015 IEEE International Conference on
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work considers optimizing LDPC codes in the physical-layer network coded
two-way relay channel using noncoherent FSK modulation. The error-rate
performance of channel decoding at the relay node during the multiple-access
phase was improved through EXIT-based optimization of Tanner graph variable
node degree distributions. Codes drawn from the DVB-S2 and WiMAX standards were
used as a basis for design and performance comparison. The computational
complexity characteristics of the standard codes were preserved in the
optimized codes by maintaining the extended irregular repeat-accumulate (eIRA).
The relay receiver performance was optimized considering two modulation orders
M = {4, 8} using iterative decoding in which the decoder and demodulator refine
channel estimates by exchanging information. The code optimization procedure
yielded unique optimized codes for each case of modulation order and available
channel state information. Performance of the standard and optimized codes were
measured using Monte Carlo simulation in the flat Rayleigh fading channel, and
error rate improvements up to 1.2 dB are demonstrated depending on system
parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05786</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05786</id><created>2015-02-20</created><authors><author><keyname>Mukhopadhyay</keyname><forenames>Arpan</forenames></author><author><keyname>Karthik</keyname><forenames>A.</forenames></author><author><keyname>Mazumdar</keyname><forenames>Ravi R.</forenames></author></authors><title>Randomized Assignment of Jobs to Servers in Heterogeneous Clusters of
  Shared Servers for Low Delay</title><categories>cs.DC cs.PF cs.SY math.PR stat.AP</categories><msc-class>60K35, 60K25, 90B15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the job assignment problem in a multi-server system consisting of
$N$ parallel processor sharing servers, categorized into $M$ ($\ll N$)
different types according to their processing capacity or speed. Jobs of random
sizes arrive at the system according to a Poisson process with rate $N
\lambda$. Upon each arrival, a small number of servers from each type is
sampled uniformly at random. The job is then assigned to one of the sampled
servers based on a selection rule. We propose two schemes, each corresponding
to a specific selection rule that aims at reducing the mean sojourn time of
jobs in the system.
  We first show that both methods achieve the maximal stability region. We then
analyze the system operating under the proposed schemes as $N \to \infty$ which
corresponds to the mean field. Our results show that asymptotic independence
among servers holds even when $M$ is finite and exchangeability holds only
within servers of the same type. We further establish the existence and
uniqueness of stationary solution of the mean field and show that the tail
distribution of server occupancy decays doubly exponentially for each server
type. When the estimates of arrival rates are not available, the proposed
schemes offer simpler alternatives to achieving lower mean sojourn time of
jobs, as shown by our numerical studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05789</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05789</id><created>2015-02-20</created><authors><author><keyname>Li</keyname><forenames>Wen-Tai</forenames></author><author><keyname>Wen</keyname><forenames>Chao-Kai</forenames></author><author><keyname>Chen</keyname><forenames>Jung-Chieh</forenames></author><author><keyname>Wong</keyname><forenames>Kai-Kit</forenames></author><author><keyname>Teng</keyname><forenames>Jen-Hao</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author></authors><title>Location Identification of Power Line Outages Using PMU Measurements
  with Bad Data</title><categories>cs.SY cs.IT math.IT</categories><comments>10 pages, 3 figures, 5 tables, submitted to IEEE Transactions on
  Power Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of phasor angle measurements provided by phasor measurement units
(PMUs) in fault detection is regarded as a promising method in identifying
locations of power line outages. However, communication errors or system
malfunctions may introduce errors to the measurements and thus yield bad data.
Most of the existing methods on line outage identification fail to consider
such error. This paper develops a framework for identifying multiple power line
outages based on the PMUs' measurements in the presence of bad data. In
particular, we design an algorithm to identify locations of line outage and
recover the faulty measurements simultaneously. The proposed algorithm does not
require any prior information on the number of line outages and the noise
variance. Case studies carried out on test systems of different sizes validate
the effectiveness and efficiency of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05803</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05803</id><created>2015-02-20</created><updated>2016-03-07</updated><authors><author><keyname>&#x10c;ehovin</keyname><forenames>Luka</forenames></author><author><keyname>Leonardis</keyname><forenames>Ale&#x161;</forenames></author><author><keyname>Kristan</keyname><forenames>Matej</forenames></author></authors><title>Visual object tracking performance measures revisited</title><categories>cs.CV</categories><journal-ref>IEEE Transactions on Image Processing (March 2016), 1261 - 1274</journal-ref><doi>10.1109/TIP.2016.2520370</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of visual tracking evaluation is sporting a large variety of
performance measures, and largely suffers from lack of consensus about which
measures should be used in experiments. This makes the cross-paper tracker
comparison difficult. Furthermore, as some measures may be less effective than
others, the tracking results may be skewed or biased towards particular
tracking aspects. In this paper we revisit the popular performance measures and
tracker performance visualizations and analyze them theoretically and
experimentally. We show that several measures are equivalent from the point of
information they provide for tracker comparison and, crucially, that some are
more brittle than the others. Based on our analysis we narrow down the set of
potential measures to only two complementary ones, describing accuracy and
robustness, thus pushing towards homogenization of the tracker evaluation
methodology. These two measures can be intuitively interpreted and visualized
and have been employed by the recent Visual Object Tracking (VOT) challenges as
the foundation for the evaluation methodology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05807</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05807</id><created>2015-02-20</created><authors><author><keyname>Chou</keyname><forenames>Evan</forenames></author><author><keyname>G&#xfc;nt&#xfc;rk</keyname><forenames>C. Sinan</forenames></author><author><keyname>Krahmer</keyname><forenames>Felix</forenames></author><author><keyname>Saab</keyname><forenames>Rayan</forenames></author><author><keyname>Y&#x131;lmaz</keyname><forenames>&#xd6;zg&#xfc;r</forenames></author></authors><title>Noise-shaping Quantization Methods for Frame-based and Compressive
  Sampling Systems</title><categories>cs.IT math.FA math.IT</categories><comments>18 pages, 2 figures. chapter in the SAMPTA 2013 book</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Noise shaping refers to an analog-to-digital conversion methodology in which
quantization error is arranged to lie mostly outside the signal spectrum by
means of oversampling and feedback. Recently it has been successfully applied
to more general redundant linear sampling and reconstruction systems associated
with frames as well as non-linear systems associated with compressive sampling.
This chapter reviews some of the recent progress in this subject.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05808</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05808</id><created>2015-02-20</created><authors><author><keyname>Hernandez</keyname><forenames>Bryan</forenames></author><author><keyname>Sison</keyname><forenames>Virgilio</forenames></author></authors><title>Matrix Codes as Ideals for Grassmannian Codes and their Weight
  Properties</title><categories>cs.IT math.IT</categories><msc-class>94B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A systematic way of constructing Grassmannian codes endowed with the subspace
distance as lifts of matrix codes over the prime field $GF(p)$ is introduced.
The matrix codes are $GF(p)$-subspaces of the ring $M_2(GF(p))$ of $2 \times 2$
matrices over $GF(p)$ on which the rank metric is applied, and are generated as
one-sided proper principal ideals by idempotent elements of $M_2(GF(p))$.
Furthermore a weight function on the non-commutative matrix ring $M_2(GF(p))$,
$q$ a power of $p$, is studied in terms of the egalitarian and homogeneous
conditions. The rank weight distribution of $M_2(GF(q))$ is completely
determined by the general linear group $GL(2,q)$. Finally a weight function on
subspace codes is analogously defined and its egalitarian property is examined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05811</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05811</id><created>2015-02-20</created><updated>2015-03-08</updated><authors><author><keyname>T&#xf3;thm&#xe9;r&#xe9;sz</keyname><forenames>Lilla</forenames></author></authors><title>Rotor-routing orbits in directed graphs and the Picard group</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In [5], Holroyd, Levine, M\'esz\'aros, Peres, Propp and Wilson characterize
recurrent chip-and-rotor configurations for strongly connected digraphs.
However, the number of steps needed to recur, and the number of orbits is left
open for general digraphs. Recently, these questions were answered by Pham [6],
using linear algebraic methods. We give new, purely combinatorial proofs for
these formulas. We also relate rotor-router orbits to the chip-firing game: The
number of recurrent rotor-router unicycle-orbits equals the order of the Picard
group of the graph, defined in the sense of [1], and during a period, the same
chip-moves happen, as during firing the period vector in the chip-firing game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05817</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05817</id><created>2015-02-20</created><authors><author><keyname>Abd-Elrahman</keyname><forenames>Emad</forenames></author><author><keyname>Said</keyname><forenames>Adel Mounir</forenames></author><author><keyname>Toukabri</keyname><forenames>Thouraya</forenames></author><author><keyname>Afifi</keyname><forenames>Hossam</forenames></author><author><keyname>Marot</keyname><forenames>Michel</forenames></author></authors><title>A Hybrid Model to Extend Vehicular Intercommunication V2V through D2D
  Architecture</title><categories>cs.NI</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the recent years, many solutions for Vehicle to Vehicle (V2V)
communication were proposed to overcome failure problems (also known as dead
ends). This paper proposes a novel framework for V2V failure recovery using
Device-to-Device (D2D) communications. Based on the unified Intelligent
Transportation Systems (ITS) architecture, LTE-based D2D mechanisms can improve
V2V dead ends failure recovery delays. This new paradigm of hybrid V2V-D2D
communications overcomes the limitations of traditional V2V routing techniques.
According to NS2 simulation results, the proposed hybrid model decreases the
end to end delay (E2E) of messages delivery. A complete comparison of different
D2D use cases (best &amp; worst scenarios) is presented to show the enhancements
brought by our solution compared to traditional V2V techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05818</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05818</id><created>2015-02-20</created><authors><author><keyname>Luoto</keyname><forenames>Petri</forenames></author><author><keyname>Pirinen</keyname><forenames>Pekka</forenames></author><author><keyname>Bennis</keyname><forenames>Mehdi</forenames></author><author><keyname>Samarakoon</keyname><forenames>Sumudu</forenames></author><author><keyname>Scott</keyname><forenames>Simon</forenames></author><author><keyname>Latva-aho</keyname><forenames>Matti</forenames></author></authors><title>Co-Primary Multi-Operator Resource Sharing for Small Cell Networks</title><categories>cs.NI cs.IT math.IT</categories><doi>10.1109/TWC.2015.2402671</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To tackle the challenge of providing higher data rates within limited
spectral resources we consider the case of multiple operators sharing a common
pool of radio resources. Four algorithms are proposed to address co-primary
multi-operator radio resource sharing under heterogeneous traffic in both
centralized and distributed scenarios. The performance of these algorithms is
assessed through extensive system-level simulations for two indoor small cell
layouts. It is assumed that the spectral allocations of the small cells are
orthogonal to the macro network layer and thus, only the small cell traffic is
modeled. The main performance metrics are user throughput and the relative
amount of shared spectral resources. The numerical results demonstrate the
importance of coordination among co-primary operators for an optimal resource
sharing. Also, maximizing the spectrum sharing percentage generally improves
the achievable throughput gains over non-sharing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05825</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05825</id><created>2015-02-20</created><authors><author><keyname>Soeken</keyname><forenames>Mathias</forenames></author><author><keyname>Thomsen</keyname><forenames>Michael Kirkedal</forenames></author><author><keyname>Dueck</keyname><forenames>Gerhard W.</forenames></author><author><keyname>Miller</keyname><forenames>D. Michael</forenames></author></authors><title>Self-Inverse Functions and Palindromic Circuits</title><categories>cs.ET math.GR quant-ph</categories><comments>6 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the subclass of reversible functions that are self-inverse and
relate them to reversible circuits that are equal to their reverse circuit,
which are called palindromic circuits. We precisely determine which
self-inverse functions can be realized as a palindromic circuit. For those
functions that cannot be realized as a palindromic circuit, we find alternative
palindromic representations that require an extra circuit line or quantum gates
in their construction. Our analyses make use of involutions in the symmetric
group $S_{2^n}$ which are isomorphic to self-inverse reversible function on $n$
variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05828</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05828</id><created>2015-02-20</created><authors><author><keyname>Bonnet</keyname><forenames>&#xc9;douard</forenames></author><author><keyname>Lampis</keyname><forenames>Michael</forenames></author><author><keyname>Paschos</keyname><forenames>Vangelis Th.</forenames></author></authors><title>Time-Approximation Trade-offs for Inapproximable Problems</title><categories>cs.DS cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we focus on problems which do not admit a constant-factor
approximation in polynomial time and explore how quickly their approximability
improves as the allowed running time is gradually increased from polynomial to
(sub-)exponential.
  We tackle a number of problems: For Min Independent Dominating Set, Max
Induced Path, Forest and Tree, for any $r(n)$, a simple, known scheme gives an
approximation ratio of $r$ in time roughly $r^{n/r}$. We show that, for most
values of $r$, if this running time could be significantly improved the ETH
would fail. For Max Minimal Vertex Cover we give a non-trivial
$\sqrt{r}$-approximation in time $2^{n/r}$. We match this with a similarly
tight result. We also give a $\log r$-approximation for Min ATSP in time
$2^{n/r}$ and an $r$-approximation for Max Grundy Coloring in time $r^{n/r}$.
  Furthermore, we show that Min Set Cover exhibits a curious behavior in this
super-polynomial setting: for any $\delta &gt; 0$ it admits an
$m^\delta$-approximation, where $m$ is the number of sets, in just
quasi-polynomial time. We observe that if such ratios could be achieved in
polynomial time, the ETH or the Projection Games Conjecture would fail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05831</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05831</id><created>2015-02-20</created><updated>2015-12-12</updated><authors><author><keyname>Liu</keyname><forenames>Shengyun</forenames></author><author><keyname>Viotti</keyname><forenames>Paolo</forenames></author><author><keyname>Cachin</keyname><forenames>Christian</forenames></author><author><keyname>Qu&#xe9;ma</keyname><forenames>Vivien</forenames></author><author><keyname>Vukoli&#x107;</keyname><forenames>Marko</forenames></author></authors><title>XFT: Practical Fault Tolerance Beyond Crashes</title><categories>cs.DC</categories><comments>40 pages (21 + 19-page appendix)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite years of intensive research, Byzantine fault-tolerance (BFT) has not
been practically adopted due to its resource and operation overhead with
respect to crash fault-tolerance (CFT).
  We introduce XFT (&quot;cross fault tolerance&quot;), a novel approach to building
reliable and secure distributed systems. XFT allows for both Byzantine machines
and network faults (communication asynchrony), yet considers them independent.
This allows the development of XFT systems at the cost of CFT (already paid for
in practice), yet with many more nines of reliability than CFT and sometimes
even better reliability than BFT itself. In short, beyond the guarantees of
CFT, XFT tolerates Byzantine faults so long as a majority of replicas
comprising the system are correct and communicate synchronously.
  We demonstrate our approach with XPaxos, the first XFT state machine
replication protocol. Despite its much stronger reliability guarantees at no
extra resource cost, XPaxos performance matches that of the state-of-the-art
CFT protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05832</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05832</id><created>2015-02-20</created><authors><author><keyname>Baqu&#xe9;</keyname><forenames>Pierre</forenames></author><author><keyname>Hours</keyname><forenames>Jean-Hubert</forenames></author><author><keyname>Fleuret</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Fua</keyname><forenames>Pascal</forenames></author></authors><title>A provably convergent alternating minimization method for mean field
  inference</title><categories>cs.LG math.OC</categories><comments>Submitted to Colt 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mean-Field is an efficient way to approximate a posterior distribution in
complex graphical models and constitutes the most popular class of Bayesian
variational approximation methods. In most applications, the mean field
distribution parameters are computed using an alternate coordinate
minimization. However, the convergence properties of this algorithm remain
unclear. In this paper, we show how, by adding an appropriate penalization
term, we can guarantee convergence to a critical point, while keeping a closed
form update at each step. A convergence rate estimate can also be derived based
on recent results in non-convex optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05834</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05834</id><created>2015-02-20</created><authors><author><keyname>Kurucz</keyname><forenames>Agi</forenames></author></authors><title>Bimodal logics with a `weakly connected' component without the finite
  model property</title><categories>cs.LO math.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are two known general results on the finite model property (fmp) of
commutators [L,L'] (bimodal logics with commuting and confluent modalities). If
L is finitely axiomatisable by modal formulas having universal Horn first-order
correspondents, then both [L,K] and [L,S5] are determined by classes of frames
that admit filtration, and so have the fmp. On the negative side, if both L and
L' are determined by transitive frames and have frames of arbitrarily large
depth, then [L,L'] does not have the fmp. In this paper we show that
commutators with a `weakly connected' component often lack the fmp. Our results
imply that the above positive result does not generalise to universally
axiomatisable component logics, and even commutators without `transitive'
components such as [K.3,K] can lack the fmp. We also generalise the above
negative result to cases where one of the component logics has frames of depth
one only, such as [S4.3,S5] and the decidable product logic S4.3xS5. We also
show cases when already half of commutativity is enough to force infinite
frames.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05838</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05838</id><created>2015-02-20</created><authors><author><keyname>Furbach</keyname><forenames>Ulrich</forenames></author><author><keyname>Schon</keyname><forenames>Claudia</forenames></author><author><keyname>Stolzenburg</keyname><forenames>Frieder</forenames></author></authors><title>Automated Reasoning for Robot Ethics</title><categories>cs.AI cs.LO</categories><comments>arXiv admin note: substantial text overlap with arXiv:1411.4823</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deontic logic is a very well researched branch of mathematical logic and
philosophy. Various kinds of deontic logics are considered for different
application domains like argumentation theory, legal reasoning, and acts in
multi-agent systems. In this paper, we show how standard deontic logic can be
used to model ethical codes for multi-agent systems. Furthermore we show how
Hyper, a high performance theorem prover, can be used to prove properties of
these ethical codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05840</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05840</id><created>2015-02-20</created><authors><author><keyname>Yan</keyname><forenames>Junchi</forenames></author><author><keyname>Cho</keyname><forenames>Minsu</forenames></author><author><keyname>Zha</keyname><forenames>Hongyuan</forenames></author><author><keyname>Yang</keyname><forenames>Xiaokang</forenames></author><author><keyname>Chu</keyname><forenames>Stephen</forenames></author></authors><title>A General Multi-Graph Matching Approach via Graduated
  Consistency-regularized Boosting</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of matching $N$ weighted graphs referring to
an identical object or category. More specifically, matching the common node
correspondences among graphs. This multi-graph matching problem involves two
ingredients affecting the overall accuracy: i) the local pairwise matching
affinity score among graphs; ii) the global matching consistency that measures
the uniqueness of the pairwise matching results by different chaining orders.
Previous studies typically either enforce the matching consistency constraints
in the beginning of iterative optimization, which may propagate matching error
both over iterations and across graph pairs; or separate affinity optimizing
and consistency regularization in two steps. This paper is motivated by the
observation that matching consistency can serve as a regularizer in the
affinity objective function when the function is biased due to noises or
inappropriate modeling. We propose multi-graph matching methods to incorporate
the two aspects by boosting the affinity score, meanwhile gradually infusing
the consistency as a regularizer. Furthermore, we propose a node-wise
consistency/affinity-driven mechanism to elicit the common inlier nodes out of
the irrelevant outliers. Extensive results on both synthetic and public image
datasets demonstrate the competency of the proposed algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05844</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05844</id><created>2015-02-20</created><authors><author><keyname>Dadjoo</keyname><forenames>Mona</forenames></author><author><keyname>Kheirkhah</keyname><forenames>Esmaeil</forenames></author></authors><title>An Approach For Transforming of Relational Databases to OWL Ontology</title><categories>cs.DB cs.AI cs.SE</categories><comments>10 pages in International Journal of Web &amp; Semantic Technology
  (IJWesT) Vol.6, No.1, January 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rapid growth of documents, web pages, and other types of text content is a
huge challenge for the modern content management systems. One of the problems
in the areas of information storage and retrieval is the lacking of semantic
data. Ontologies can present knowledge in sharable and repeatedly usable manner
and provide an effective way to reduce the data volume overhead by encoding the
structure of a particular domain. Metadata in relational databases can be used
to extract ontology from database in a special domain. According to solve the
problem of sharing and reusing of data, approaches based on transforming
relational database to ontology are proposed. In this paper we propose a method
for automatic ontology construction based on relational database. Mining and
obtaining further components from relational database leads to obtain knowledge
with high semantic power and more expressiveness. Triggers are one of the
database components which could be transformed to the ontology model and
increase the amount of power and expressiveness of knowledge by presenting part
of the knowledge dynamically
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05860</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05860</id><created>2015-02-20</created><updated>2015-03-05</updated><authors><author><keyname>Das</keyname><forenames>Anupam</forenames><affiliation>&#xc9;cole Normale Sup&#xe9;rieure de Lyon</affiliation></author></authors><title>On the relative proof complexity of deep inference via atomic flows</title><categories>cs.LO math.LO</categories><comments>27 pages, 2 figures, full version of conference paper</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 11, Issue 1 (March 6,
  2015) lmcs:735</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the proof complexity of the minimal complete fragment, KS, of
standard deep inference systems for propositional logic. To examine the size of
proofs we employ atomic flows, diagrams that trace structural changes through a
proof but ignore logical information. As results we obtain a polynomial
simulation of versions of Resolution, along with some extensions. We also show
that these systems, as well as bounded-depth Frege systems, cannot polynomially
simulate KS, by giving polynomial-size proofs of certain variants of the
propositional pigeonhole principle in KS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05864</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05864</id><created>2015-02-20</created><authors><author><keyname>Nayak</keyname><forenames>Sukanta</forenames></author><author><keyname>Chakraverty</keyname><forenames>Snehashish</forenames></author></authors><title>Pseudo Fuzzy Set</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Here a novel idea to handle imprecise or vague set viz. Pseudo fuzzy set has
been proposed. Pseudo fuzzy set is a triplet of element and its two membership
functions. Both the membership functions may or may not be dependent. The
hypothesis is that every positive sense has some negative sense. So, one
membership function has been considered as positive and another as negative.
Considering this concept, here the development of Pseudo fuzzy set and its
property along with Pseudo fuzzy numbers has been discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05871</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05871</id><created>2015-02-20</created><authors><author><keyname>Lakicevic</keyname><forenames>Maja</forenames></author><author><keyname>Moracanin</keyname><forenames>Mitar</forenames></author><author><keyname>Djerkovic</keyname><forenames>Nadja</forenames></author></authors><title>Robust CS reconstruction based on appropriate minimization norm</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Noise robust compressive sensing algorithm is considered. This algorithm
allows an efficient signal reconstruction in the presence of different types of
noise due to the possibility to change minimization norm. For instance, the
commonly used l1 and l2 norms, provide good results in case of Laplace and
Gaussian noise. However, when the signal is corrupted by Cauchy or Cubic
Gaussian noise, these norms fail to provide accurate reconstruction. Therefore,
in order to achieve accurate reconstruction, the application of l3 minimization
norm is analyzed. The efficiency of algorithm will be demonstrated on examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05873</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05873</id><created>2015-02-20</created><updated>2015-02-25</updated><authors><author><keyname>Hague</keyname><forenames>Matthew</forenames></author><author><keyname>Penelle</keyname><forenames>Vincent</forenames></author></authors><title>Annotated Stack Trees</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Annotated pushdown automata provide an automaton model of higher-order
recursion schemes, which may in turn be used to model higher-order programs for
the purposes of verification. We study Ground Annotated Stack Tree Rewrite
Systems -- a tree rewrite system where each node is labelled by the
configuration of an annotated pushdown automaton. This allows the modelling of
fork and join constructs in higher-order programs and is a generalisation of
higher-order stack trees recently introduced by Penelle.
  We show that, given a regular set of annotated stack trees, the set of trees
that can reach this set is also regular, and constructible in n-EXPTIME for an
order-n system, which is optimal. We also show that our construction can be
extended to allow a global state through which unrelated nodes of the tree may
communicate, provided the number of communications is subject to a fixed bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05879</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05879</id><created>2015-02-20</created><authors><author><keyname>de Oliveira</keyname><forenames>H. M.</forenames></author><author><keyname>de Souza</keyname><forenames>D. F.</forenames></author></authors><title>Wavelet Analysis as an Information Processing Technique</title><categories>cs.IT math.CA math.IT</categories><comments>6 pages, 6 tables, VI International Telecommunications Symposium
  (ITS2006), September 3-6, Fortaleza, Brazil</comments><doi>10.1109/ITS.2006.4433232</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new interpretation for the wavelet analysis is reported, which can is
viewed as an information processing technique. It was recently proposed that
every basic wavelet could be associated with a proper probability density,
allowing defining the entropy of a wavelet. Introducing now the concept of
wavelet mutual information between a signal and an analysing wavelet fulfils
the foundations of a wavelet information theory (WIT). Both continuous and
discrete time signals are considered. Finally, we showed how to compute the
information provided by a multiresolution analysis by means of the
inhomogeneous wavelet expansion. Highlighting ideas behind the WIT are
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05880</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05880</id><created>2015-02-20</created><authors><author><keyname>de Oliveira</keyname><forenames>R. C.</forenames></author><author><keyname>de Oliveira</keyname><forenames>H. M.</forenames></author><author><keyname>de Souza</keyname><forenames>R. M. Campello</forenames></author><author><keyname>Santos</keyname><forenames>E. J. P.</forenames></author></authors><title>A Flexible Implementation of a Matrix Laurent Series-Based 16-Point Fast
  Fourier and Hartley Transforms</title><categories>cs.NA cs.DM</categories><comments>4 pages, 4 figures. IEEE VI Southern Programmable Logic Conference
  2010</comments><doi>10.1109/SPL.2010.5483017</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a flexible architecture for implementing a new fast
computation of the discrete Fourier and Hartley transforms, which is based on a
matrix Laurent series. The device calculates the transforms based on a single
bit selection operator. The hardware structure and synthesis are presented,
which handled a 16-point fast transform in 65 nsec, with a Xilinx SPARTAN 3E
device.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05881</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05881</id><created>2015-02-20</created><authors><author><keyname>de Oliveira</keyname><forenames>H. M.</forenames></author><author><keyname>de Souza</keyname><forenames>R. M. Campello</forenames></author></authors><title>Orthogonal Multilevel Spreading Sequence Design</title><categories>cs.IT math.IT</categories><comments>9 pages, 5 figures. In: Coding, Communication and Broadcasting.1
  ed.Hertfordshire: Reseach Studies Press (RSP), 2000. ISBN 0-86380-259-1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finite field transforms are offered as a new tool of spreading sequence
design. This approach exploits orthogonality properties of synchronous
non-binary sequences defined over a complex finite field. It is promising for
channels supporting a high signal-to-noise ratio. New digital multiplex schemes
based on such sequences have also been introduced, which are multilevel Code
Division Multiplex. These schemes termed Galois-field Division Multiplex (GDM)
are based on transforms for which there exists fast algorithms. They are also
convenient from the hardware viewpoint since they can be implemented by a
Digital Signal Processor. A new Efficient-bandwidth
code-division-multiple-access (CDMA) is introduced, which is based on
multilevel spread spectrum sequences over a Galois field. The primary advantage
of such schemes regarding classical multiple access digital schemes is their
better spectral efficiency. Galois-Fourier transforms contain some redundancy
and only cyclotomic coefficients are needed to be transmitted yielding compact
spectrum requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05883</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05883</id><created>2015-02-20</created><updated>2015-03-02</updated><authors><author><keyname>Mennle</keyname><forenames>Timo</forenames></author><author><keyname>Seuken</keyname><forenames>Sven</forenames></author></authors><title>The Efficient Frontier in Randomized Social Choice</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since the celebrated Gibbard-Satterthwaite impossibility results and
Gibbard's 1977 extension for randomized rules, it is known that
strategyproofness imposes severe restrictions on the design of social choice
rules. In this paper, we employ approximate strategyproofness and the notion of
score deficit to study the possible and necessary trade-offs between
strategyproofness and efficiency in the randomized social choice domain. In
particular, we analyze which social choice rules make optimal trade-offs, i.e.,
we analyze the efficient frontier. Our main result is that the efficient
frontier consists of two building blocks: (1) we identify a finite set of
&quot;manipulability bounds&quot; B and the rules that are optimal at each of them; (2)
for all other bounds not in B, we show that the optimal rules at those bounds
are mixtures of two rules that are optimal at the two nearest manipulability
bounds from B. We provide algorithms that exploit this structure to identify
the entire efficient frontier for any given scoring function. Finally, we
provide applications of our results to illustrate the structure of the
efficient frontier for the scoring functions v=(1,0,0) and v=(1,1,0).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05886</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05886</id><created>2015-02-20</created><authors><author><keyname>Le</keyname><forenames>Lei</forenames></author><author><keyname>Ferrara</keyname><forenames>Emilio</forenames></author><author><keyname>Flammini</keyname><forenames>Alessandro</forenames></author></authors><title>On predictability of rare events leveraging social media: a machine
  learning perspective</title><categories>cs.SI cs.LG physics.data-an physics.soc-ph</categories><comments>10 pages, 10 tables, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information extracted from social media streams has been leveraged to
forecast the outcome of a large number of real-world events, from political
elections to stock market fluctuations. An increasing amount of studies
demonstrates how the analysis of social media conversations provides cheap
access to the wisdom of the crowd. However, extents and contexts in which such
forecasting power can be effectively leveraged are still unverified at least in
a systematic way. It is also unclear how social-media-based predictions compare
to those based on alternative information sources. To address these issues,
here we develop a machine learning framework that leverages social media
streams to automatically identify and predict the outcomes of soccer matches.
We focus in particular on matches in which at least one of the possible
outcomes is deemed as highly unlikely by professional bookmakers. We argue that
sport events offer a systematic approach for testing the predictive power of
social media, and allow to compare such power against the rigorous baselines
set by external sources. Despite such strict baselines, our framework yields
above 8% marginal profit when used to inform simple betting strategies. The
system is based on real-time sentiment analysis and exploits data collected
immediately before the games, allowing for informed bets. We discuss the
rationale behind our approach, describe the learning framework, its prediction
performance and the return it provides as compared to a set of betting
strategies. To test our framework we use both historical Twitter data from the
2014 FIFA World Cup games, and real-time Twitter data collected by monitoring
the conversations about all soccer matches of four major European tournaments
(FA Premier League, Serie A, La Liga, and Bundesliga), and the 2014 UEFA
Champions League, during the period between Oct. 25th 2014 and Nov. 26th 2014.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05887</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05887</id><created>2015-02-20</created><updated>2015-02-24</updated><authors><author><keyname>Chopra</keyname><forenames>Jatin</forenames></author></authors><title>Analysis of Lithography Based Approaches In development of Semi
  Conductors</title><categories>cs.OH</categories><comments>12 pages, 6 figures in International Journal of Computer Science &amp;
  Infromation Technology(IJCSIT) Vol6,No6,December 2014</comments><doi>10.5121/ijcsit2014.6604</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The end of the 19th century brought about a change in the dynamics of
computing by the development of the microprocessor. Huge bedroom size computers
began being replaced by portable, smaller sized desktops. Today the world is
dominated by silicon, which has circumscribed chip development for computers
through microprocessors. Majority of the integrated circuits that are
manufactured at present are developed using the concept of Lithography. This
paper presents a detailed analysis of multiple Lithography methodologies as a
means for advanced integrated circuit development. The study paper primarily
restricts to examples in the context of Lithography, surveying the various
existing techniques of Lithography in literature, examining feasible and
efficient methods, highlighting the various pros and cons of each of them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05888</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05888</id><created>2015-02-20</created><updated>2016-02-18</updated><authors><author><keyname>Lang</keyname><forenames>Jer&#xf4;me</forenames></author><author><keyname>Pigozzi</keyname><forenames>Gabriella</forenames></author><author><keyname>Slavkovik</keyname><forenames>Marija</forenames></author><author><keyname>van der Torre</keyname><forenames>Leendert</forenames></author><author><keyname>Vesic</keyname><forenames>Srdjan</forenames></author></authors><title>A partial taxonomy of judgment aggregation rules, and their properties</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The literature on judgment aggregation is moving from studying impossibility
results regarding aggregation rules towards studying specific judgment
aggregation rules. Here we give a structured list of most rules that have been
proposed and studied recently in the literature, together with various
properties of such rules. We fi?rst focus on the majority-preservation
property, which generalizes Condorcet-consistency, and identify which of the
rules satisfy it. We study the inclusion relationships that hold between the
rules. Finally, we consider two forms of unanimity, monotonicity, homogeneity,
and reinforcement, and we identify which of the rules satisfy these properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05890</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05890</id><created>2015-02-20</created><updated>2015-03-04</updated><authors><author><keyname>Krishnamurthy</keyname><forenames>Akshay</forenames></author><author><keyname>Agarwal</keyname><forenames>Alekh</forenames></author><author><keyname>Dudik</keyname><forenames>Miroslav</forenames></author></authors><title>Efficient Contextual Semi-Bandit Learning</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a variant of the contextual bandit problem, where on each round, the
learner plays a sequence of actions, receives a feature for each individual
action, and reward that is linearly related to these features. This setting has
applications to network routing, crowd-sourcing, personalized search, and many
other domains. If the linear transformation is known, we analyze an algorithm
that is structurally similar to the algorithm of Agarwal et a. [2014] and show
that it enjoys a regret bound between $\tilde{O}(\sqrt{KLT \ln N})$ and
$\tilde{O}(L\sqrt{KT \ln N})$, where $K$ is the number of actions, $L$ is the
length of each action sequence, $T$ is the number of rounds, and $N$ is the
number of policies. If the linear transformation is unknown, we show that an
algorithm that first explores to learn the unknown weights via linear
regression and thereafter uses the estimated weights can achieve
$\tilde{O}(\|w\|_1(KT)^{3/4} \sqrt{\ln N})$ regret, where $w$ is the true
(unknown) weight vector. Both algorithms use an optimization oracle to avoid
explicit enumeration of the policies and consequently are computationally
efficient whenever an efficient algorithm for the fully supervised setting is
available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05906</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05906</id><created>2015-02-20</created><updated>2015-03-05</updated><authors><author><keyname>Silva</keyname><forenames>A. R. Gomes e</forenames></author><author><keyname>de Oliveira</keyname><forenames>H. M.</forenames></author><author><keyname>Lins</keyname><forenames>R. D.</forenames></author></authors><title>Converting ECG and other paper legated biomedical maps into digital
  signals</title><categories>cs.OH</categories><comments>6 pages, 9 figures</comments><journal-ref>Lecture Notes in Computer Science, LNCS 5046, GREC 2007, pp.
  21-28, 2008</journal-ref><doi>10.1007/978-3-540-88188-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a digital signal processing tool developed using
MatlabTM, which provides a very low-cost and effective strategy for
analog-to-digital conversion of legated paper biomedical maps without requiring
dedicated hardware. This software-based approach is particularly helpful for
digitalizing biomedical signals acquired from analogical devices equipped with
a plottingter. Albeit signals used in biomedical diagnosis are the primary
concern, this imaging processing tool is suitable to modernize facilities in a
non-expensive way. Legated paper ECG and EEG charts can be fast and efficiently
digitalized in order to be added in existing up-to-date medical data banks,
improving the follow-up of patients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05908</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05908</id><created>2015-02-20</created><updated>2015-04-13</updated><authors><author><keyname>Wohlhart</keyname><forenames>Paul</forenames></author><author><keyname>Lepetit</keyname><forenames>Vincent</forenames></author></authors><title>Learning Descriptors for Object Recognition and 3D Pose Estimation</title><categories>cs.CV</categories><comments>CVPR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detecting poorly textured objects and estimating their 3D pose reliably is
still a very challenging problem. We introduce a simple but powerful approach
to computing descriptors for object views that efficiently capture both the
object identity and 3D pose. By contrast with previous manifold-based
approaches, we can rely on the Euclidean distance to evaluate the similarity
between descriptors, and therefore use scalable Nearest Neighbor search methods
to efficiently handle a large number of objects under a large range of poses.
To achieve this, we train a Convolutional Neural Network to compute these
descriptors by enforcing simple similarity and dissimilarity constraints
between the descriptors. We show that our constraints nicely untangle the
images from different objects and different views into clusters that are not
only well-separated but also structured as the corresponding sets of poses: The
Euclidean distance between descriptors is large when the descriptors are from
different objects, and directly related to the distance between the poses when
the descriptors are from the same object. These important properties allow us
to outperform state-of-the-art object views representations on challenging RGB
and RGB-D data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05910</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05910</id><created>2015-02-20</created><authors><author><keyname>Bulian</keyname><forenames>Jannis</forenames></author><author><keyname>Dawar</keyname><forenames>Anuj</forenames></author></authors><title>Fixed-parameter Tractable Distances to Sparse Graph Classes</title><categories>cs.DS cs.CC cs.DM cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that for various classes C of sparse graphs, and several measures of
distance to such classes (such as edit distance and elimination distance), the
problem of determining the distance of a given graph G to C is fixed-parameter
tractable. The results are based on two general techniques. The first of these,
building on recent work of Grohe et al. establishes that any class of graphs
that is slicewise nowhere dense and slicewise first-order definable is FPT. The
second shows that determining the elimination distance of a graph G to a
minor-closed class C is FPT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05911</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05911</id><created>2015-02-20</created><authors><author><keyname>Ladas</keyname><forenames>Alexandros</forenames></author><author><keyname>Ferguson</keyname><forenames>Eamonn</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author><author><keyname>Garibaldi</keyname><forenames>Jon</forenames></author></authors><title>A Data Mining framework to model Consumer Indebtedness with
  Psychological Factors</title><categories>cs.LG cs.CE</categories><comments>IEEE International Conference of Data Mining: The Seventh
  International Workshop on Domain Driven Data Mining 2014 (DDDM 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modelling Consumer Indebtedness has proven to be a problem of complex nature.
In this work we utilise Data Mining techniques and methods to explore the
multifaceted aspect of Consumer Indebtedness by examining the contribution of
Psychological Factors, like Impulsivity to the analysis of Consumer Debt. Our
results confirm the beneficial impact of Psychological Factors in modelling
Consumer Indebtedness and suggest a new approach in analysing Consumer Debt,
that would take into consideration more Psychological characteristics of
consumers and adopt techniques and practices from Data Mining.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05912</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05912</id><created>2015-02-20</created><authors><author><keyname>Berkholz</keyname><forenames>Christoph</forenames></author><author><keyname>Grohe</keyname><forenames>Martin</forenames></author></authors><title>Limitations of Algebraic Approaches to Graph Isomorphism Testing</title><categories>cs.CC cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the power of graph isomorphism algorithms based on algebraic
reasoning techniques like Gr\&quot;obner basis computation. The idea of these
algorithms is to encode two graphs into a system of equations that are
satisfiable if and only if if the graphs are isomorphic, and then to (try to)
decide satisfiability of the system using, for example, the Gr\&quot;obner basis
algorithm. In some cases this can be done in polynomial time, in particular, if
the equations admit a bounded degree refutation in an algebraic proof systems
such as Nullstellensatz or polynomial calculus. We prove linear lower bounds on
the polynomial calculus degree over all fields of characteristic different from
2 and also linear lower bounds for the degree of Positivstellensatz calculus
derivations.
  We compare this approach to recently studied linear and semidefinite
programming approaches to isomorphism testing, which are known to be related to
the combinatorial Weisfeiler-Lehman algorithm. We exactly characterise the
power of the Weisfeiler-Lehman algorithm in terms of an algebraic proof system
that lies between degree-k Nullstellensatz and degree-k polynomial calculus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05925</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05925</id><created>2015-02-20</created><authors><author><keyname>Nan</keyname><forenames>Feng</forenames></author><author><keyname>Wang</keyname><forenames>Joseph</forenames></author><author><keyname>Saligrama</keyname><forenames>Venkatesh</forenames></author></authors><title>Feature-Budgeted Random Forest</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We seek decision rules for prediction-time cost reduction, where complete
data is available for training, but during prediction-time, each feature can
only be acquired for an additional cost. We propose a novel random forest
algorithm to minimize prediction error for a user-specified {\it average}
feature acquisition budget. While random forests yield strong generalization
performance, they do not explicitly account for feature costs and furthermore
require low correlation among trees, which amplifies costs. Our random forest
grows trees with low acquisition cost and high strength based on greedy minimax
cost-weighted-impurity splits. Theoretically, we establish near-optimal
acquisition cost guarantees for our algorithm. Empirically, on a number of
benchmark datasets we demonstrate superior accuracy-cost curves against
state-of-the-art prediction-time algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05928</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05928</id><created>2015-02-20</created><authors><author><keyname>Gangeh</keyname><forenames>Mehrdad J.</forenames></author><author><keyname>Farahat</keyname><forenames>Ahmed K.</forenames></author><author><keyname>Ghodsi</keyname><forenames>Ali</forenames></author><author><keyname>Kamel</keyname><forenames>Mohamed S.</forenames></author></authors><title>Supervised Dictionary Learning and Sparse Representation-A Review</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dictionary learning and sparse representation (DLSR) is a recent and
successful mathematical model for data representation that achieves
state-of-the-art performance in various fields such as pattern recognition,
machine learning, computer vision, and medical imaging. The original
formulation for DLSR is based on the minimization of the reconstruction error
between the original signal and its sparse representation in the space of the
learned dictionary. Although this formulation is optimal for solving problems
such as denoising, inpainting, and coding, it may not lead to optimal solution
in classification tasks, where the ultimate goal is to make the learned
dictionary and corresponding sparse representation as discriminative as
possible. This motivated the emergence of a new category of techniques, which
is appropriately called supervised dictionary learning and sparse
representation (S-DLSR), leading to more optimal dictionary and sparse
representation in classification tasks. Despite many research efforts for
S-DLSR, the literature lacks a comprehensive view of these techniques, their
connections, advantages and shortcomings. In this paper, we address this gap
and provide a review of the recently proposed algorithms for S-DLSR. We first
present a taxonomy of these algorithms into six categories based on the
approach taken to include label information into the learning of the dictionary
and/or sparse representation. For each category, we draw connections between
the algorithms in this category and present a unified framework for them. We
then provide guidelines for applied researchers on how to represent and learn
the building blocks of an S-DLSR solution based on the problem at hand. This
review provides a broad, yet deep, view of the state-of-the-art methods for
S-DLSR and allows for the advancement of research and development in this
emerging area of research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05931</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05931</id><created>2015-02-19</created><authors><author><keyname>Hefeida</keyname><forenames>Mohamed S.</forenames></author><author><keyname>Chowdhury</keyname><forenames>Masud H.</forenames></author></authors><title>Improved Model for Wire-Length Estimation in Stochastic Wiring
  Distribution</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a pair of improved stochastic wiring distribution model
for better estimation of on-chip wire lengths. The proposed models provide 28 -
50% reduction in error when estimating the average on-chip wire length compared
to the estimation using the existing models. The impact of Rent's exponent on
the average wire length estimation is also investigated to demonstrate
limitations of the approximations used in some of the current models. To
improve the approximations of the model a new threshold for Rent's constant is
recommended. Simulation results demonstrate that proposed models with the new
threshold reduce the error of estimation by 38 to 75 percent compared to the
previous works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05934</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05934</id><created>2015-02-20</created><authors><author><keyname>Luo</keyname><forenames>Haipeng</forenames></author><author><keyname>Schapire</keyname><forenames>Robert E.</forenames></author></authors><title>Achieving All with No Parameters: Adaptive NormalHedge</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the classic online learning problem of predicting with expert
advice, and propose a truly parameter-free and adaptive algorithm that achieves
several objectives simultaneously without using any prior information. The main
component of this work is an improved version of the NormalHedge.DT algorithm
(Luo and Schapire, 2014), called AdaNormalHedge. On one hand, this new
algorithm ensures small regret when the competitor has small loss and almost
constant regret when the losses are stochastic. On the other hand, the
algorithm is able to compete with any convex combination of the experts
simultaneously, with a regret in terms of the relative entropy of the prior and
the competitor. This resolves an open problem proposed by Chaudhuri et al.
(2009) and Chernov and Vovk (2010). Moreover, we extend the results to the
sleeping expert setting and provide two applications to illustrate the power of
AdaNormalHedge: 1) competing with time-varying unknown competitors and 2)
predicting almost as well as the best pruning tree. Our results on these
applications significantly improve previous work from different aspects, and a
special case of the first application resolves another open problem proposed by
Warmuth and Koolen (2014) on whether one can simultaneously achieve optimal
shifting regret for both adversarial and stochastic losses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05937</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05937</id><created>2015-02-20</created><updated>2015-02-23</updated><authors><author><keyname>Belazzougui</keyname><forenames>Djamal</forenames></author><author><keyname>Cunial</keyname><forenames>Fabio</forenames></author><author><keyname>Gagie</keyname><forenames>Travis</forenames></author><author><keyname>Prezza</keyname><forenames>Nicola</forenames></author><author><keyname>Raffinot</keyname><forenames>Mathieu</forenames></author></authors><title>Composite repetition-aware data structures</title><categories>cs.DS</categories><comments>(the name of the third co-author was inadvertently omitted from
  previous version)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In highly repetitive strings, like collections of genomes from the same
species, distinct measures of repetition all grow sublinearly in the length of
the text, and indexes targeted to such strings typically depend only on one of
these measures. We describe two data structures whose size depends on multiple
measures of repetition at once, and that provide competitive tradeoffs between
the time for counting and reporting all the exact occurrences of a pattern, and
the space taken by the structure. The key component of our constructions is the
run-length encoded BWT (RLBWT), which takes space proportional to the number of
BWT runs: rather than augmenting RLBWT with suffix array samples, we combine it
with data structures from LZ77 indexes, which take space proportional to the
number of LZ77 factors, and with the compact directed acyclic word graph
(CDAWG), which takes space proportional to the number of extensions of maximal
repeats. The combination of CDAWG and RLBWT enables also a new representation
of the suffix tree, whose size depends again on the number of extensions of
maximal repeats, and that is powerful enough to support matching statistics and
constant-space traversal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05938</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05938</id><created>2015-02-20</created><authors><author><keyname>Reps</keyname><forenames>Jenna</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author></authors><title>Incorporating Spontaneous Reporting System Data to Aid Causal Inference
  in Longitudinal Healthcare Data</title><categories>cs.CE</categories><comments>IEEE International Conference of Data Mining: The Fifth Workshop on
  Biological Data Mining and its Applications in Healthcare, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inferring causality using longitudinal observational databases is challenging
due to the passive way the data are collected. The majority of associations
found within longitudinal observational data are often non-causal and occur due
to confounding.
  The focus of this paper is to investigate incorporating information from
additional databases to complement the longitudinal observational database
analysis. We investigate the detection of prescription drug side effects as
this is an example of a causal relationship. In previous work a framework was
proposed for detecting side effects only using longitudinal data. In this paper
we combine a measure of association derived from mining a spontaneous reporting
system database to previously proposed analysis that extracts domain expertise
features for causal analysis of a UK general practice longitudinal database.
  The results show that there is a significant improvement to the performance
of detecting prescription drug side effects when the longitudinal observation
data analysis is complemented by incorporating additional drug safety sources
into the framework. The area under the receiver operating characteristic curve
(AUC) for correctly classifying a side effect when other data were considered
was 0.967, whereas without it the AUC was 0.923 However, the results of this
paper may be biased by the evaluation and future work should overcome this by
developing an unbiased reference set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05943</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05943</id><created>2015-02-20</created><authors><author><keyname>Reps</keyname><forenames>Jenna M.</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author><author><keyname>Ma</keyname><forenames>Jiangang</forenames></author><author><keyname>Zhang</keyname><forenames>Yanchun</forenames></author></authors><title>Refining Adverse Drug Reactions using Association Rule Mining for
  Electronic Healthcare Data</title><categories>cs.DB cs.CE cs.LG</categories><comments>IEEE International Conference of Data Mining: Data Mining in
  Biomedical Informatics and Healthcare (DMBIH) Workshop 2014, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Side effects of prescribed medications are a common occurrence. Electronic
healthcare databases present the opportunity to identify new side effects
efficiently but currently the methods are limited due to confounding (i.e. when
an association between two variables is identified due to them both being
associated to a third variable).
  In this paper we propose a proof of concept method that learns common
associations and uses this knowledge to automatically refine side effect
signals (i.e. exposure-outcome associations) by removing instances of the
exposure-outcome associations that are caused by confounding. This leaves the
signal instances that are most likely to correspond to true side effect
occurrences. We then calculate a novel measure termed the confounding-adjusted
risk value, a more accurate absolute risk value of a patient experiencing the
outcome within 60 days of the exposure.
  Tentative results suggest that the method works. For the four signals (i.e.
exposure-outcome associations) investigated we are able to correctly filter the
majority of exposure-outcome instances that were unlikely to correspond to true
side effects. The method is likely to improve when tuning the association rule
mining parameters for specific health outcomes.
  This paper shows that it may be possible to filter signals at a patient level
based on association rules learned from considering patients' medical
histories. However, additional work is required to develop a way to automate
the tuning of the method's parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05947</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05947</id><created>2015-02-20</created><updated>2015-05-12</updated><authors><author><keyname>Wisnesky</keyname><forenames>Ryan</forenames></author><author><keyname>Spivak</keyname><forenames>David I.</forenames></author><author><keyname>Schultz</keyname><forenames>Patrick</forenames></author><author><keyname>Subrahmanian</keyname><forenames>Eswaran</forenames></author></authors><title>Functorial Data Migration: From Theory to Practice</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe a functorial data migration scenario about the
manufacturing service capability of a distributed supply chain. The scenario is
a category-theoretic analog of an OWL ontology-based semantic enrichment
scenario developed at the National Institute of Standards and Technology
(NIST). The scenario is presented using, and is included with, the open-source
FQL tool, available for download at categoricaldata.net/fql.html.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05955</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05955</id><created>2015-02-20</created><updated>2015-06-28</updated><authors><author><keyname>Cohen</keyname><forenames>Edith</forenames></author></authors><title>Stream Sampling for Frequency Cap Statistics</title><categories>cs.IR cs.DB stat.CO</categories><comments>21 pages, 4 figures, preliminary version will appear in KDD 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unaggregated data, in streamed or distributed form, is prevalent and come
from diverse application domains which include interactions of users with web
services and IP traffic. Data elements have {\em keys} (cookies, users,
queries) and elements with different keys interleave. Analytics on such data
typically utilizes statistics stated in terms of the frequencies of keys. The
two most common statistics are {\em distinct}, which is the number of active
keys in a specified segment, and {\em sum}, which is the sum of the frequencies
of keys in the segment. Both are special cases of {\em cap} statistics, defined
as the sum of frequencies {\em capped} by a parameter $T$, which are popular in
online advertising platforms. Aggregation by key, however, is costly, requiring
state proportional to the number of distinct keys, and therefore we are
interested in estimating these statistics or more generally, sampling the data,
without aggregation. We present a sampling framework for unaggregated data that
uses a single pass (for streams) or two passes (for distributed data) and state
proportional to the desired sample size. Our design provides the first
effective solution for general frequency cap statistics. Our $\ell$-capped
samples provide estimates with tight statistical guarantees for cap statistics
with $T=\Theta(\ell)$ and nonnegative unbiased estimates of {\em any} monotone
non-decreasing frequency statistics. An added benefit of our unified design is
facilitating {\em multi-objective samples}, which provide estimates with
statistical guarantees for a specified set of different statistics, using a
single, smaller sample.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05957</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05957</id><created>2015-02-20</created><authors><author><keyname>Cohen</keyname><forenames>Andrew R.</forenames><affiliation>Dept Electri. Comput. Eng., Drexel Univ.</affiliation></author><author><keyname>Vitanyi</keyname><forenames>Paul M. B.</forenames><affiliation>CWI and University of Amsterdam &amp; Drexel University</affiliation></author></authors><title>Web Similarity</title><categories>cs.IR cs.CL cs.CV</categories><comments>LaTeX 25 pages, 3 tables. A precursor is arXiv:1308.3177</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Normalized web distance (NWD) is a similarity or normalized semantic distance
based on the World Wide Web or any other large electronic database, for
instance Wikipedia, and a search engine that returns reliable aggregate page
counts. For sets of search terms the NWD gives a similarity on a scale from 0
(identical) to 1 (completely different). The NWD approximates the similarity
according to all (upper semi)computable properties. We develop the theory and
give applications. The derivation of the NWD method is based on Kolmogorov
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05968</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05968</id><created>2015-02-20</created><authors><author><keyname>Ghaderi</keyname><forenames>Javad</forenames></author><author><keyname>Shakkottai</keyname><forenames>Sanjay</forenames></author><author><keyname>Srikant</keyname><forenames>R</forenames></author></authors><title>Scheduling Storms and Streams in the Cloud</title><categories>cs.NI cs.DC math.OC</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by emerging big streaming data processing paradigms (e.g., Twitter
Storm, Streaming MapReduce), we investigate the problem of scheduling graphs
over a large cluster of servers. Each graph is a job, where nodes represent
compute tasks and edges indicate data-flows between these compute tasks. Jobs
(graphs) arrive randomly over time, and upon completion, leave the system. When
a job arrives, the scheduler needs to partition the graph and distribute it
over the servers to satisfy load balancing and cost considerations.
Specifically, neighboring compute tasks in the graph that are mapped to
different servers incur load on the network; thus a mapping of the jobs among
the servers incurs a cost that is proportional to the number of &quot;broken edges&quot;.
We propose a low complexity randomized scheduling algorithm that, without
service preemptions, stabilizes the system with graph arrivals/departures; more
importantly, it allows a smooth trade-off between minimizing average
partitioning cost and average queue lengths. Interestingly, to avoid service
preemptions, our approach does not rely on a Gibbs sampler; instead, we show
that the corresponding limiting invariant measure has an interpretation
stemming from a loss system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05974</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05974</id><created>2015-02-20</created><authors><author><keyname>Thomas</keyname><forenames>Brian</forenames></author></authors><title>Development of a VO Registry Subject Ontology using Automated Methods</title><categories>astro-ph.IM cs.AI</categories><journal-ref>Astronomical Data Analysis Software and Systems XX. ASP Conference
  Proceedings, Vol. 442, 2011, p.599</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report on our initial work to automate the generation of a domain ontology
using subject fields of resources held in the Virtual Observatory registry.
Preliminary results are comparable to more generalized ontology learning
software currently in use. We expect to be able to refine our solution to
improve both the depth and breadth of the generated ontology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05980</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05980</id><created>2014-12-21</created><updated>2015-11-14</updated><authors><author><keyname>Stankovic</keyname><forenames>Srdjan</forenames></author><author><keyname>Orovic</keyname><forenames>Irena</forenames></author></authors><title>An Approach to 2D Signals Recovering in Compressive Sensing Context</title><categories>cs.IT math.IT</categories><comments>13 pages, 5 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the compressive sensing effects on 2D signals
exhibiting sparsity in 2D DFT domain. A simple algorithm for reconstruction of
randomly under-sampled data is proposed. It is based on the analytically
determined threshold that precisely separates signal and non-signal components
in the 2D DFT domain. The algorithm operates fast in a single iteration
providing the accurate signal reconstruction. In the situations that are not
comprised by the analytic derivation and constrains, the algorithm is still
efficient and need just a couple of iterations. The proposed solution shows
promising results in ISAR imaging (simulated data are used), where the
reconstruction is achieved even in the case when less than 10% of data is
available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05983</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05983</id><created>2015-02-20</created><authors><author><keyname>Marinov</keyname><forenames>Martin</forenames></author><author><keyname>Gregg</keyname><forenames>David</forenames></author></authors><title>Sorting Networks: The Final Countdown</title><categories>cs.DM cs.DS</categories><comments>arXiv admin note: substantial text overlap with arXiv:1502.04748</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we extend the knowledge on the problem of empirically searching
for sorting networks of minimal depth. We present new search space pruning
techniques for the last four levels of a candidate sorting network by
considering only the output set representation of a network. We present an
algorithm for checking whether an $n$-input sorting network of depth $d$ exists
by considering the minimal up to permutation and reflection itemsets at each
level and using the pruning at the last four levels. We experimentally
evaluated this algorithm to find the optimal depth sorting networks for all $n
\leq 12$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.05988</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.05988</id><created>2014-12-17</created><authors><author><keyname>Read</keyname><forenames>Jesse</forenames></author><author><keyname>Perez-Cruz</keyname><forenames>Fernando</forenames></author></authors><title>Deep Learning for Multi-label Classification</title><categories>cs.LG cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In multi-label classification, the main focus has been to develop ways of
learning the underlying dependencies between labels, and to take advantage of
this at classification time. Developing better feature-space representations
has been predominantly employed to reduce complexity, e.g., by eliminating
non-helpful feature attributes from the input space prior to (or during)
training. This is an important task, since many multi-label methods typically
create many different copies or views of the same input data as they transform
it, and considerable memory can be saved by taking advantage of redundancy. In
this paper, we show that a proper development of the feature space can make
labels less interdependent and easier to model and predict at inference time.
For this task we use a deep learning approach with restricted Boltzmann
machines. We present a deep network that, in an empirical evaluation,
outperforms a number of competitive methods from the literature
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06004</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06004</id><created>2014-12-09</created><authors><author><keyname>Abdallah</keyname><forenames>Yara</forenames></author><author><keyname>Zheng</keyname><forenames>Zizhan</forenames></author><author><keyname>Shroff</keyname><forenames>Ness B.</forenames></author><author><keyname>Gamal</keyname><forenames>Hesham El</forenames></author></authors><title>The Impact of Stealthy Attacks on Smart Grid Performance: Tradeoffs and
  Implications</title><categories>math.OC cs.CR cs.NI</categories><comments>Technical report - this work was submitted to IEEE Transactions on
  Control of Network Systems, Mar 2014. arXiv admin note: substantial text
  overlap with arXiv:1209.1763</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The smart grid is envisioned to significantly enhance the efficiency of
energy consumption, by utilizing two-way communication channels between
consumers and operators. For example, operators can opportunistically leverage
the delay tolerance of energy demands in order to balance the energy load over
time, and hence, reduce the total operational cost. This opportunity, however,
comes with security threats, as the grid becomes more vulnerable to
cyber-attacks. In this paper, we study the impact of such malicious
cyber-attacks on the energy efficiency of the grid in a simplified setup. More
precisely, we consider a simple model where the energy demands of the smart
grid consumers are intercepted and altered by an active attacker before they
arrive at the operator, who is equipped with limited intrusion detection
capabilities. We formulate the resulting optimization problems faced by the
operator and the attacker and propose several scheduling and attack strategies
for both parties. Interestingly, our results show that, as opposed to
facilitating cost reduction in the smart grid, increasing the delay tolerance
of the energy demands potentially allows the attacker to force increased costs
on the system. This highlights the need for carefully constructed and robust
intrusion detection mechanisms at the operator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06007</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06007</id><created>2014-12-12</created><authors><author><keyname>Chorti</keyname><forenames>Arsenia</forenames></author><author><keyname>Freij</keyname><forenames>Ragnar</forenames></author><author><keyname>Karpuk</keyname><forenames>David</forenames></author></authors><title>Degrees of Freedom and Secrecy in Wireless Relay Networks</title><categories>cs.IT math.AG math.IT</categories><comments>Submitted to MEGA (Effective Methods in Algebraic Geometry) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We translate the problem of designing a secure communications protocol for
several users communicating through a relay in a wireless network into
understanding certain subvarieties of products of Grassmannians. We calculate
the dimension of these subvarieties and provide various results concerning
their defning equations. When the relay and all of the users have the same
number of antennas, this approach places fundamental limits on the amount of
data that can be passed through such a network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06013</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06013</id><created>2015-02-20</created><authors><author><keyname>Moy</keyname><forenames>Richard A.</forenames></author><author><keyname>Rolnick</keyname><forenames>David</forenames></author></authors><title>Novel structures in Stanley sequences</title><categories>math.CO cs.DM</categories><comments>15 pages, code for working with Stanley sequences available at
  https://github.com/drolnick/stanseq</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set of integers with no three in arithmetic progression, we construct
a Stanley sequence by adding integers greedily so that no arithmetic
progression is formed. This paper offers two main contributions to the theory
of Stanley sequences. First, we characterize well-structured Stanley sequences
as solutions to constraints in modular arithmetic, defining the modular Stanley
sequences. Second, we introduce the basic Stanley sequences, where elements
arise as the sums of subsets of a basis sequence, which in the simplest case is
the powers of 3. Applications of our results include the construction of
Stanley sequences with arbitrarily large gaps between terms, answering a weak
version of a problem by Erd\H{o}s et al. Finally, we generalize many results
about Stanley sequences to $p$-free sequences, where $p$ is any odd prime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06021</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06021</id><created>2014-12-23</created><authors><author><keyname>Blanqui</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames><affiliation>INRIA Paris-Rocquencourt</affiliation></author></authors><title>A point on fixpoints in posets</title><categories>math.LO cs.LO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $(X,\le)$ be a {\em non-empty strictly inductive poset}, that is, a
non-empty partially ordered set such that every non-empty chain $Y$ has a least
upper bound lub$(Y)\in X$, a chain being a subset of $X$ totally ordered by
$\le$. We are interested in sufficient conditions such that, given an element
$a_0\in X$ and a function $f:X\a X$, there is some ordinal $k$ such that
$a_{k+1}=a_k$, where $a\_k$ is the transfinite sequence of iterates of $f$
starting from $a_0$ (implying that $a_k$ is a fixpoint of $f$):
  \begin{itemize}\itemsep=0mm \item $a_{k+1}=f(a_k)$ \item $a_l=\lub\{a_k\mid k
\textless{} l\}$ if $l$ is a limit ordinal, i.e. $l=lub(l)$ \end{itemize}
  This note summarizes known results about this problem and provides a slight
generalization of some of them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06025</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06025</id><created>2015-02-20</created><authors><author><keyname>Good</keyname><forenames>Benjamin M.</forenames></author><author><keyname>Ha</keyname><forenames>Gavin</forenames></author><author><keyname>Ho</keyname><forenames>Chi K.</forenames></author><author><keyname>Wilkinson</keyname><forenames>Mark D.</forenames></author></authors><title>OntoLoki: an automatic, instance-based method for the evaluation of
  biological ontologies on the Semantic Web</title><categories>q-bio.QM cs.AI</categories><acm-class>I.2.4</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The delineation of logical definitions for each class in an ontology and the
consistent application of these definitions to the assignment of instances to
classes are important criteria for ontology evaluation. If ontologies are
specified with property-based restrictions on class membership, then such
consistency can be checked automatically. If no such logical restrictions are
applied, as is the case with many biological ontologies, there are currently no
automated methods for measuring the semantic consistency of instance assignment
on an ontology-wide scale, nor for inferring the patterns of properties that
might define a particular class. We constructed a program that takes as its
input an OWL/RDF knowledge base containing an ontology, instances associated
with each of the classes in the ontology, and properties of those instances.
For each class, it outputs: 1) a rule for determining class membership based on
the properties of the instances and 2) a quantitative score for the class that
reflects the ability of the identified rule to correctly predict class
membership for the instances in the knowledge base. We evaluated this program
using both artificial knowledge bases of known quality and real, widely used
ontologies. The results indicate that the suggested method can be used to
conduct objective, automatic, data-driven evaluations of biological ontologies
without formal class definitions in regards to the property-based consistency
of instance-assignment. This inductive method complements existing, purely
deductive approaches to automatic consistency checking, offering not just the
potential to help in the ontology engineering process but also in the knowledge
discovery process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06029</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06029</id><created>2015-02-20</created><authors><author><keyname>Jiang</keyname><forenames>Jing</forenames></author><author><keyname>Sun</keyname><forenames>Hongjian</forenames></author><author><keyname>Baglee</keyname><forenames>David</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Achieving Autonomous Compressive Spectrum Sensing for Cognitive Radios</title><categories>cs.IT math.IT</categories><comments>This paper is accepted by IEEE Transactions on Vehicular Technology
  2015. The detail will be updated soon</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressive sensing (CS) technologies present many advantages over other
existing approaches for implementing wideband spectrum sensing in cognitive
radios (CRs), such as reduced sampling rate and computational complexity.
However, there are two significant challenges: 1) choosing an appropriate
number of sub-Nyquist measurements, and 2) deciding when to terminate the
greedy recovery algorithm that reconstructs wideband spectrum. In this paper,
an autonomous compressive spectrum sensing (ACSS) framework is presented that
enables a CR to automatically choose the number of measurements while
guaranteeing the wideband spectrum recovery with a small predictable recovery
error. This is realized by the proposed measurement infrastructure and the
validation technique. The proposed ACSS can find a good spectral estimate with
high confidence by using only a small testing subset in both noiseless and
noisy environments. Furthermore, a sparsity-aware spectral recovery algorithm
is proposed to recover the wideband spectrum without requiring knowledge of the
instantaneous spectral sparsity level. Such an algorithm bridges the gap
between CS theory and practical spectrum sensing. Simulation results show that
ACSS can not only recover the spectrum using an appropriate number of
measurements, but can also considerably improve the spectral recovery
performance compared with existing CS approaches. The proposed recovery
algorithm can autonomously adopt a proper number of iterations, therefore
solving the problems of under-fitting or over-fitting which commonly exist in
most greedy recovery algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06030</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06030</id><created>2015-02-20</created><authors><author><keyname>Omidshafiei</keyname><forenames>Shayegan</forenames></author><author><keyname>Agha-mohammadi</keyname><forenames>Ali-akbar</forenames></author><author><keyname>Amato</keyname><forenames>Christopher</forenames></author><author><keyname>How</keyname><forenames>Jonathan P.</forenames></author></authors><title>Decentralized Control of Partially Observable Markov Decision Processes
  using Belief Space Macro-actions</title><categories>cs.MA cs.AI cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The focus of this paper is on solving multi-robot planning problems in
continuous spaces with partial observability. Decentralized partially
observable Markov decision processes (Dec-POMDPs) are general models for
multi-robot coordination problems, but representing and solving Dec-POMDPs is
often intractable for large problems. To allow for a high-level representation
that is natural for multi-robot problems and scalable to large discrete and
continuous problems, this paper extends the Dec-POMDP model to the
decentralized partially observable semi-Markov decision process (Dec-POSMDP).
The Dec-POSMDP formulation allows asynchronous decision-making by the robots,
which is crucial in multi-robot domains. We also present an algorithm for
solving this Dec-POSMDP which is much more scalable than previous methods since
it can incorporate closed-loop belief space macro-actions in planning. These
macro-actions are automatically constructed to produce robust solutions. The
proposed method's performance is evaluated on a complex multi-robot package
delivery problem under uncertainty, showing that our approach can naturally
represent multi-robot problems and provide high-quality solutions for
large-scale problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06062</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06062</id><created>2015-02-20</created><authors><author><keyname>Takaoka</keyname><forenames>Tadao</forenames></author></authors><title>Multi-level Loop-less Algorithm for Multi-set Permutations</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm that generates multiset permutations in O(1) time for
each permutation, that is, by a loop-less algorithm with O(n) extra memory
requirement. There already exist several such algorithms that generate multiset
permutations in various orders. For multiset permutations, we combine two
loop-less algorithms that are designed in the same principle of tree traversal.
Our order of generation is different from any existing order, and the algorithm
is simpler and faster than the previous ones. We also apply the new algorithm
to parking functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06064</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06064</id><created>2015-02-20</created><authors><author><keyname>Miura</keyname><forenames>Ken</forenames></author><author><keyname>Mano</keyname><forenames>Tetsuaki</forenames></author><author><keyname>Kanehira</keyname><forenames>Atsushi</forenames></author><author><keyname>Tsuchiya</keyname><forenames>Yuichiro</forenames></author><author><keyname>Harada</keyname><forenames>Tatsuya</forenames></author></authors><title>MILJS : Brand New JavaScript Libraries for Matrix Calculation and
  Machine Learning</title><categories>stat.ML cs.LG cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MILJS is a collection of state-of-the-art, platform-independent, scalable,
fast JavaScript libraries for matrix calculation and machine learning. Our core
library offering a matrix calculation is called Sushi, which exhibits far
better performance than any other leading machine learning libraries written in
JavaScript. Especially, our matrix multiplication is 177 times faster than the
fastest JavaScript benchmark. Based on Sushi, a machine learning library called
Tempura is provided, which supports various algorithms widely used in machine
learning research. We also provide Soba as a visualization library. The
implementations of our libraries are clearly written, properly documented and
thus can are easy to get started with, as long as there is a web browser. These
libraries are available from http://mil-tokyo.github.io/ under the MIT license.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06073</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06073</id><created>2015-02-21</created><updated>2015-02-27</updated><authors><author><keyname>Huang</keyname><forenames>Zengxi</forenames></author><author><keyname>Liu</keyname><forenames>Yiguang</forenames></author><author><keyname>Wang</keyname><forenames>Xiaoming</forenames></author><author><keyname>Hu</keyname><forenames>Jinrong</forenames></author></authors><title>Study on Sparse Representation based Classification for Biometric
  Verification</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a multimodal verification system integrating face
and ear based on sparse representation based classification (SRC). The face and
ear query samples are first encoded separately to derive sparsity-based match
scores, and which are then combined with sum-rule fusion for verification.
Apart from validating the encouraging performance of SRC-based multimodal
verification, this paper also dedicates to provide a clear understanding about
the characteristics of SRC-based biometric verification. To this end, two
sparsity-based metrics, i.e. spare coding error (SCE) and sparse contribution
rate (SCR), are involved, together with face and ear unimodal SRC-based
verification. As for the issue that SRC-based biometric verification may suffer
from heavy computational burden and verification accuracy degradation with
increase of enrolled subjects, we argue that it could be properly resolved by
exploiting small random dictionary for sparsity-based score computation, which
consists of training samples from a limited number of randomly selected
subjects. Experimental results demonstrate the superiority of SRC-based
multimodal verification compared to the state-of-the-art multimodal methods
like likelihood ratio (LLR), support vector machine (SVM), and the sum-rule
fusion methods using cosine similarity, meanwhile the idea of using small
random dictionary is feasible in both effectiveness and efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06075</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06075</id><created>2015-02-21</created><authors><author><keyname>Lin</keyname><forenames>Weiyao</forenames></author><author><keyname>Chen</keyname><forenames>Yuanzhe</forenames></author><author><keyname>Wu</keyname><forenames>Jianxin</forenames></author><author><keyname>Wang</keyname><forenames>Hanli</forenames></author><author><keyname>Sheng</keyname><forenames>Bin</forenames></author><author><keyname>Li</keyname><forenames>Hongxiang</forenames></author></authors><title>A new network-based algorithm for human activity recognition in video</title><categories>cs.CV</categories><comments>This manuscript is the accepted version for TCSVT (IEEE Transactions
  on Circuits and Systems for Video Technology)</comments><journal-ref>IEEE Trans. Circuits and Systems for Video Technology, vol. 24,
  no. 5, pp. 826-841, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new network-transmission-based (NTB) algorithm is proposed
for human activity recognition in videos. The proposed NTB algorithm models the
entire scene as an error-free network. In this network, each node corresponds
to a patch of the scene and each edge represents the activity correlation
between the corresponding patches. Based on this network, we further model
people in the scene as packages while human activities can be modeled as the
process of package transmission in the network. By analyzing these specific
&quot;package transmission&quot; processes, various activities can be effectively
detected. The implementation of our NTB algorithm into abnormal activity
detection and group activity recognition are described in detail in the paper.
Experimental results demonstrate the effectiveness of our proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06076</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06076</id><created>2015-02-21</created><authors><author><keyname>Lin</keyname><forenames>Weiyao</forenames></author><author><keyname>Chu</keyname><forenames>Hang</forenames></author><author><keyname>Wu</keyname><forenames>Jianxin</forenames></author><author><keyname>Sheng</keyname><forenames>Bin</forenames></author><author><keyname>Chen</keyname><forenames>Zhenzhong</forenames></author></authors><title>A Heat-Map-based Algorithm for Recognizing Group Activities in Videos</title><categories>cs.CV</categories><comments>This manuscript is the accepted version for TCSVT(IEEE Transactions
  on Circuits and Systems for Video Technology)</comments><journal-ref>IEEE Trans. Circuits and Systems for Video Technology, vol. 23,
  no. 11, pp. 1980-1992, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new heat-map-based (HMB) algorithm is proposed for group
activity recognition. The proposed algorithm first models human trajectories as
series of &quot;heat sources&quot; and then applies a thermal diffusion process to create
a heat map (HM) for representing the group activities. Based on this heat map,
a new key-point based (KPB) method is used for handling the alignments among
heat maps with different scales and rotations. And a surface-fitting (SF)
method is also proposed for recognizing group activities. Our proposed HM
feature can efficiently embed the temporal motion information of the group
activities while the proposed KPB and SF methods can effectively utilize the
characteristics of the heat map for activity recognition. Experimental results
demonstrate the effectiveness of our proposed algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06078</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06078</id><created>2015-02-21</created><authors><author><keyname>Comsa</keyname><forenames>Ioan Sorin</forenames></author><author><keyname>Arsinte</keyname><forenames>Radu</forenames></author></authors><title>Evaluating QoS Parameters for IPTV Distribution in Heterogeneous
  Networks</title><categories>cs.MM</categories><comments>7 pages, 19 figures</comments><report-no>EM 2/2010</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present work presents an architecture developed to evaluate the QoS
parameters for the IPTV heterogeneous network. At its very basic level lie two
software technologies: Video LAN and Windows Media Services with two operating
systems: Windows and Linux. Three types of streams are analyzed, which will be
transmitted to a Linux VLC client through means of the aggregation and access
servers. The first stream is generated in real time by a capture camera,
processed by the encapsulated VC-1 encoder and sent to the Media Server, while
the second one is of VoD(Video on Demand) type and the third one will be
handled by DVBViewer through the MPEG TS form. The first stream is transcoded
in H.264-AAC such that the Linux stations will recognize its format. Through
the simultaneous transmission of the three streams, we are analyzing their
performance from a QoS parameters point of view by means of an application
implemented in C programming language. The stream transporting the DVB-S
television content was proven to ensure the best performance regarding loss of
packets, delays and jitter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06079</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06079</id><created>2015-02-21</created><authors><author><keyname>de Berg</keyname><forenames>Mark</forenames></author><author><keyname>Gudmundsson</keyname><forenames>Joachim</forenames></author><author><keyname>Mehrabi</keyname><forenames>Ali D.</forenames></author></authors><title>Finding Pairwise Intersections Inside a Query Range</title><categories>cs.DS cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the following problem: preprocess a set O of objects into a data
structure that allows us to efficiently report all pairs of objects from O that
intersect inside an axis-aligned query range Q. We present data structures of
size $O(n({\rm polylog} n))$ and with query time $O((k+1)({\rm polylog} n))$
time, where k is the number of reported pairs, for two classes of objects in
the plane: axis-aligned rectangles and objects with small union complexity. For
the 3-dimensional case where the objects and the query range are axis-aligned
boxes in R^3, we present a data structures of size $O(n\sqrt{n}({\rm polylog}
n))$ and query time $O((\sqrt{n}+k)({\rm polylog} n))$. When the objects and
query are fat, we obtain $O((k+1)({\rm polylog} n))$ query time using $O(n({\rm
polylog} n))$ storage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06080</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06080</id><created>2015-02-21</created><authors><author><keyname>Chen</keyname><forenames>Yuanzhe</forenames></author><author><keyname>Lin</keyname><forenames>Weiyao</forenames></author><author><keyname>Zhang</keyname><forenames>Chongyang</forenames></author><author><keyname>Chen</keyname><forenames>Zhenzhong</forenames></author><author><keyname>Xu</keyname><forenames>Ning</forenames></author><author><keyname>Xie</keyname><forenames>Jun</forenames></author></authors><title>Intra-and-Inter-Constraint-based Video Enhancement based on Piecewise
  Tone Mapping</title><categories>cs.CV cs.MM</categories><comments>This manuscript is the accepted version for TCSVT (IEEE Transactions
  on Circuits and Systems for Video Technology)</comments><journal-ref>IEEE Trans. Circuits and Systems for Video Technology, vol. 23,
  no. 1, pp. 74-82, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Video enhancement plays an important role in various video applications. In
this paper, we propose a new intra-and-inter-constraint-based video enhancement
approach aiming to 1) achieve high intra-frame quality of the entire picture
where multiple region-of-interests (ROIs) can be adaptively and simultaneously
enhanced, and 2) guarantee the inter-frame quality consistencies among video
frames. We first analyze features from different ROIs and create a piecewise
tone mapping curve for the entire frame such that the intra-frame quality of a
frame can be enhanced. We further introduce new inter-frame constraints to
improve the temporal quality consistency. Experimental results show that the
proposed algorithm obviously outperforms the state-of-the-art algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06081</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06081</id><created>2015-02-21</created><authors><author><keyname>Arsinte</keyname><forenames>Radu</forenames></author></authors><title>Study of a Robust Algorithm Applied in the Optimal Position Tuning for
  the Camera Lens in Automated Visual Inspection Systems</title><categories>cs.CV</categories><comments>5 pages, 2 figures</comments><journal-ref>Proceedings of Fifth International Conference on Pattern
  Recognition and Information Processing - PRIP'99 - May 18-20, 1999 Minsk,
  Belarus - pag.237-242 - ISBN 83-87362-16-6</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper present the mathematical fundaments and experimental study of an
algorithm used to find the optimal position for the camera lens to obtain a
maximum of details. This information can be further applied to a appropriate
system to automatically correct this position. The algorithm is based on the
evaluation of a so called resolution function who calculates the maximum of
gradient in a certain zone of the image. The paper also presents alternative
forms of the function, results of measurements and set up a set of practical
rules for the right application of the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06084</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06084</id><created>2015-02-21</created><updated>2015-05-02</updated><authors><author><keyname>Zhu</keyname><forenames>Jieming</forenames></author><author><keyname>He</keyname><forenames>Pinjia</forenames></author><author><keyname>Zheng</keyname><forenames>Zibin</forenames></author><author><keyname>Lyu</keyname><forenames>Michael R.</forenames></author></authors><title>A Privacy-Preserving QoS Prediction Framework for Web Service
  Recommendation</title><categories>cs.CR cs.IR</categories><comments>This paper is published in IEEE International Conference on Web
  Services (ICWS'15)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  QoS-based Web service recommendation has recently gained much attention for
providing a promising way to help users find high-quality services. To
facilitate such recommendations, existing studies suggest the use of
collaborative filtering techniques for personalized QoS prediction. These
approaches, by leveraging partially observed QoS values from users, can achieve
high accuracy of QoS predictions on the unobserved ones. However, the
requirement to collect users' QoS data likely puts user privacy at risk, thus
making them unwilling to contribute their usage data to a Web service
recommender system. As a result, privacy becomes a critical challenge in
developing practical Web service recommender systems. In this paper, we make
the first attempt to cope with the privacy concerns for Web service
recommendation. Specifically, we propose a simple yet effective
privacy-preserving framework by applying data obfuscation techniques, and
further develop two representative privacy-preserving QoS prediction approaches
under this framework. Evaluation results from a publicly-available QoS dataset
of real-world Web services demonstrate the feasibility and effectiveness of our
privacy-preserving QoS prediction approaches. We believe our work can serve as
a good starting point to inspire more research efforts on privacy-preserving
Web service recommendation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06085</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06085</id><created>2015-02-21</created><authors><author><keyname>Zhang</keyname><forenames>Zhou</forenames></author><author><keyname>Zhou</keyname><forenames>Shuai</forenames></author><author><keyname>Jiang</keyname><forenames>Hai</forenames></author></authors><title>Opportunistic Cooperative Channel Access in Distributed Wireless
  Networks with Decode-and-Forward Relays</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter studies distributed opportunistic channel access in a wireless
network with decode-and-forward relays. All the sources use channel contention
to get transmission opportunity. If a source wins the contention, the channel
state information in the first-hop channel (from the source to its relay) is
estimated, and a decision is made for the winner source to either give up the
transmission opportunity and let all sources start a new contention, or
transmit to the relay. Once the relay gets the traffic, it may have a sequence
of probings of the second-hop channel (from the relay to the destination).
After each probing, if the second-hop channel is good enough, the relay
transmits to the destination and completes the transmission process of the
source; otherwise, the relay decides either to give up and let all sources
start a new contention, or to continue to probe the second-hop channel. The
optimal decision strategies for the two hops are derived in this letter. The
first-hop strategy is a pure-threshold strategy, i.e., when the first-hop
channel signal-to-noise ratio (SNR) is more than a threshold, the winner source
should transmit to the relay, and subsequently the second-hop strategy should
let the relay keep probing the second-hop channel until a good enough
second-hop channel is observed. Simulation results show that our scheme is
beneficial when the second-hop channels have larger average SNR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06086</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06086</id><created>2015-02-21</created><authors><author><keyname>Gupta</keyname><forenames>Suyash</forenames></author><author><keyname>Shrivastava</keyname><forenames>Rahul</forenames></author><author><keyname>Nandivada</keyname><forenames>V. Krishna</forenames></author></authors><title>DCAFE: Dynamic load-balanced loop Chunking &amp; Aggressive Finish
  Elimination for Recursive Task Parallel Programs</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present two symbiotic optimizations to optimize recursive
task parallel (RTP) programs by reducing the task creation and termination
overheads. Our first optimization Aggressive Finish-Elimination (AFE) helps
reduce the redundant join operations to a large extent. The second optimization
Dynamic Load-Balanced loop Chunking (DLBC) extends the prior work on loop
chunking to decide on the number of parallel tasks based on the number of
available worker threads, at runtime. Further, we discuss the impact of
exceptions on our optimizations and extend them to handle RTP programs that may
throw exceptions. We implemented DCAFE (= DLBC+AFE) in the X10v2.3 compiler and
tested it over a set of benchmark kernels on two different hardwares (a 16-core
Intel system and a 64-core AMD system). With respect to the base X10 compiler
extended with loop-chunking of Nandivada et al [Nandivada et
al.(2013)Nandivada, Shirako, Zhao, and Sarkar](LC), DCAFE achieved a geometric
mean speed up of 5.75x and 4.16x on the Intel and AMD system, respectively. We
also present an evaluation with respect to the energy consumption on the Intel
system and show that on average, compared to the LC versions, the DCAFE
versions consume 71.2% less energy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06094</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06094</id><created>2015-02-21</created><updated>2015-12-01</updated><authors><author><keyname>Ameloot</keyname><forenames>Tom J.</forenames></author><author><keyname>Bussche</keyname><forenames>Jan Van den</forenames></author></authors><title>Positive Neural Networks in Discrete Time Implement Monotone-Regular
  Behaviors</title><categories>cs.NE</categories><journal-ref>Neural Computation, December 2015, Vol. 27, No. 12 , Pages
  2623-2660</journal-ref><doi>10.1162/NECO_a_00789</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the expressive power of positive neural networks. The model uses
positive connection weights and multiple input neurons. Different behaviors can
be expressed by varying the connection weights. We show that in discrete time,
and in absence of noise, the class of positive neural networks captures the
so-called monotone-regular behaviors, that are based on regular languages. A
finer picture emerges if one takes into account the delay by which a
monotone-regular behavior is implemented. Each monotone-regular behavior can be
implemented by a positive neural network with a delay of one time unit. Some
monotone-regular behaviors can be implemented with zero delay. And,
interestingly, some simple monotone-regular behaviors can not be implemented
with zero delay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06095</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06095</id><created>2015-02-21</created><updated>2015-03-27</updated><authors><author><keyname>Bonchi</keyname><forenames>Filippo</forenames></author><author><keyname>Zanasi</keyname><forenames>Fabio</forenames></author></authors><title>Bialgebraic Semantics for Logic Programming</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 11, Issue 1 (March 30,
  2015) lmcs:1155</journal-ref><doi>10.2168/LMCS-11(1:14)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bialgebrae provide an abstract framework encompassing the semantics of
different kinds of computational models. In this paper we propose a bialgebraic
approach to the semantics of logic programming. Our methodology is to study
logic programs as reactive systems and exploit abstract techniques developed in
that setting. First we use saturation to model the operational semantics of
logic programs as coalgebrae on presheaves. Then, we make explicit the
underlying algebraic structure by using bialgebrae on presheaves. The resulting
semantics turns out to be compositional with respect to conjunction and term
substitution. Also, it encodes a parallel model of computation, whose soundness
is guaranteed by a built-in notion of synchronisation between different
threads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06096</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06096</id><created>2015-02-21</created><authors><author><keyname>Evans</keyname><forenames>Richard</forenames></author></authors><title>Reinforcement Learning in a Neurally Controlled Robot Using Dopamine
  Modulated STDP</title><categories>cs.NE cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work has shown that dopamine-modulated STDP can solve many of the
issues associated with reinforcement learning, such as the distal reward
problem. Spiking neural networks provide a useful technique in implementing
reinforcement learning in an embodied context as they can deal with continuous
parameter spaces and as such are better at generalizing the correct behaviour
to perform in a given context.
  In this project we implement a version of DA-modulated STDP in an embodied
robot on a food foraging task. Through simulated dopaminergic neurons we show
how the robot is able to learn a sequence of behaviours in order to achieve a
food reward. In tests the robot was able to learn food-attraction behaviour,
and subsequently unlearn this behaviour when the environment changed, in all 50
trials. Moreover we show that the robot is able to operate in an environment
whereby the optimal behaviour changes rapidly and so the agent must constantly
relearn. In a more complex environment, consisting of food-containers, the
robot was able to learn food-container attraction in 95% of trials, despite the
large temporal distance between the correct behaviour and the reward. This is
achieved by shifting the dopamine response from the primary stimulus (food) to
the secondary stimulus (food-container).
  Our work provides insights into the reasons behind some observed biological
phenomena, such as the bursting behaviour observed in dopaminergic neurons. As
well as demonstrating how spiking neural network controlled robots are able to
solve a range of reinforcement learning tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06103</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06103</id><created>2015-02-21</created><authors><author><keyname>Miletic</keyname><forenames>Ana</forenames></author><author><keyname>Ivanovic</keyname><forenames>Nemanja</forenames></author></authors><title>Compressive sensing based velocity estimation in video data</title><categories>cs.MM</categories><comments>4 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the use of compressive sensing based algorithms for
velocity estimation of moving vehicles. The procedure is based on sparse
reconstruction algorithms combined with time-frequency analysis applied to
video data. This algorithm provides an accurate estimation of object's velocity
even in the case of a very reduced number of available video frames. The
influence of crucial parameters is analysed for different types of moving
vehicles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06104</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06104</id><created>2015-02-21</created><authors><author><keyname>Tavassoli</keyname><forenames>Babak</forenames></author></authors><title>Minimal Switch Step Tracking Control of Switched Systems with
  Application to Induction Motor Control</title><categories>cs.SY math.OC</categories><comments>Submission to 53rd IEEE Conference on Decision and Control (CDC 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of step tracking control with a switching input and without any
continuous-valued inputs is considered. The control objective is to reduce the
number of switchings to a minimal value. This approach finds interesting
applications when switching comprises costs and should be avoided. To solve the
problem, a state dependent switching strategy should be designed and the
resulting closed loop is indeed a hybrid system. Therefore, first we
investigate the conditions on a hybrid system for being the desired solution.
Then, we propose a method for designing the switching strategy such that the
closed loop as a hybrid system solves the problem. The proposed method is
applied to the induction motor control problem which results in relatively
simple and efficient control algorithm. Comparison with the direct torque
control for induction motors show that our method has a superior performance in
reducing the number of mode switches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06105</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06105</id><created>2015-02-21</created><authors><author><keyname>Lee</keyname><forenames>Taehoon</forenames></author><author><keyname>Moon</keyname><forenames>Taesup</forenames></author><author><keyname>Kim</keyname><forenames>Seung Jean</forenames></author><author><keyname>Yoon</keyname><forenames>Sungroh</forenames></author></authors><title>Regularization and Kernelization of the Maximin Correlation Approach</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robust classification becomes challenging when classes contain multiple
subclasses. Examples include multi-font optical character recognition and
automated protein function prediction. In correlation-based nearest-neighbor
classification, the maximin correlation approach (MCA) provides the worst-case
optimal solution by minimizing the maximum misclassification risk through an
iterative procedure. Despite the optimality, the original MCA has drawbacks
that have limited its wide applicability in practice. That is, the MCA tends to
be sensitive to outliers, cannot effectively handle nonlinearities in datasets,
and suffers from having high computational complexity. To address these
limitations, we propose an improved solution, named regularized maximin
correlation approach (R-MCA). We first reformulate MCA as a quadratically
constrained linear programming (QCLP) problem, incorporate regularization by
introducing slack variables into the primal problem of the QCLP, and derive the
corresponding Lagrangian dual. The dual formulation enables us to apply the
kernel trick to R-MCA so that it can better handle nonlinearities. Our
experimental results demonstrate that the regularization and kernelization make
the proposed R-MCA more robust and accurate for various classification tasks
than the original MCA. Furthermore, when the data size or dimensionality grows,
R-MCA runs substantially faster by solving either the primal or dual (whichever
has a smaller variable dimension) of the QCLP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06108</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06108</id><created>2015-02-21</created><updated>2015-07-28</updated><authors><author><keyname>Lin</keyname><forenames>Xiao</forenames></author><author><keyname>Parikh</keyname><forenames>Devi</forenames></author></authors><title>Don't Just Listen, Use Your Imagination: Leveraging Visual Common Sense
  for Non-Visual Tasks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Artificial agents today can answer factual questions. But they fall short on
questions that require common sense reasoning. Perhaps this is because most
existing common sense databases rely on text to learn and represent knowledge.
But much of common sense knowledge is unwritten - partly because it tends not
to be interesting enough to talk about, and partly because some common sense is
unnatural to articulate in text. While unwritten, it is not unseen. In this
paper we leverage semantic common sense knowledge learned from images - i.e.
visual common sense - in two textual tasks: fill-in-the-blank and visual
paraphrasing. We propose to &quot;imagine&quot; the scene behind the text, and leverage
visual cues from the &quot;imagined&quot; scenes in addition to textual cues while
answering these questions. We imagine the scenes as a visual abstraction. Our
approach outperforms a strong text-only baseline on these tasks. Our proposed
tasks can serve as benchmarks to quantitatively evaluate progress in solving
tasks that go &quot;beyond recognition&quot;. Our code and datasets are publicly
available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06111</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06111</id><created>2015-02-21</created><updated>2015-04-13</updated><authors><author><keyname>Klamroth</keyname><forenames>Kathrin</forenames></author><author><keyname>Lacour</keyname><forenames>Renaud</forenames></author><author><keyname>Vanderpooten</keyname><forenames>Daniel</forenames></author></authors><title>On the representation of the search region in multi-objective
  optimization</title><categories>cs.DM</categories><comments>27 pages, to appear in European Journal of Operational Research</comments><doi>10.1016/j.ejor.2015.03.031</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a finite set $N$ of feasible points of a multi-objective optimization
(MOO) problem, the search region corresponds to the part of the objective space
containing all the points that are not dominated by any point of $N$, i.e. the
part of the objective space which may contain further nondominated points. In
this paper, we consider a representation of the search region by a set of tight
local upper bounds (in the minimization case) that can be derived from the
points of $N$. Local upper bounds play an important role in methods for
generating or approximating the nondominated set of an MOO problem, yet few
works in the field of MOO address their efficient incremental determination. We
relate this issue to the state of the art in computational geometry and provide
several equivalent definitions of local upper bounds that are meaningful in
MOO. We discuss the complexity of this representation in arbitrary dimension,
which yields an improved upper bound on the number of solver calls in
epsilon-constraint-like methods to generate the nondominated set of a discrete
MOO problem. We analyze and enhance a first incremental approach which operates
by eliminating redundancies among local upper bounds. We also study some
properties of local upper bounds, especially concerning the issue of redundant
local upper bounds, that give rise to a new incremental approach which avoids
such redundancies. Finally, the complexities of the incremental approaches are
compared from the theoretical and empirical points of view.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06124</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06124</id><created>2015-02-21</created><authors><author><keyname>Filatov</keyname><forenames>Dmytro</forenames></author><author><keyname>Filatov</keyname><forenames>Taras</forenames></author></authors><title>Unified vector space mapping for knowledge representation systems</title><categories>cs.AI cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most significant problems which inhibits further developments in
the areas of Knowledge Representation and Artificial Intelligence is a problem
of semantic alignment or knowledge mapping. The progress in its solution will
be greatly beneficial for further advances of information retrieval, ontology
alignment, relevance calculation, text mining, natural language processing etc.
In the paper the concept of multidimensional global knowledge map, elaborated
through unsupervised extraction of dependencies from large documents corpus, is
proposed. In addition, the problem of direct Human - Knowledge Representation
System interface is addressed and a concept of adaptive decoder proposed for
the purpose of interaction with previously described unified mapping model. In
combination these two approaches are suggested as basis for a development of a
new generation of knowledge representation systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06132</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06132</id><created>2015-02-21</created><authors><author><keyname>Guralnik</keyname><forenames>Dan P.</forenames></author><author><keyname>Koditschek</keyname><forenames>Daniel E.</forenames></author></authors><title>Universal Memory Architectures for Autonomous Machines</title><categories>cs.AI cs.LG cs.RO math.MG</categories><comments>Technical report, 31 pages, 1 table, 14 figures, 2 appendices</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a self-organizing memory architecture for perceptual experience,
capable of supporting autonomous learning and goal-directed problem solving in
the absence of any prior information about the agent's environment. The
architecture is simple enough to ensure (1) a quadratic bound (in the number of
available sensors) on space requirements, and (2) a quadratic bound on the
time-complexity of the update-execute cycle. At the same time, it is
sufficiently complex to provide the agent with an internal representation which
is (3) minimal among all representations of its class which account for every
sensory equivalence class subject to the agent's belief state; (4) capable, in
principle, of recovering the homotopy type of the system's state space; (5)
learnable with arbitrary precision through a random application of the
available actions. The provable properties of an effectively trained memory
structure exploit a duality between weak poc sets -- a symbolic (discrete)
representation of subset nesting relations -- and non-positively curved cubical
complexes, whose rich convexity theory underlies the planning cycle of the
proposed architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06133</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06133</id><created>2015-02-21</created><authors><author><keyname>Dhamal</keyname><forenames>Swapnil</forenames></author><author><keyname>J.</keyname><forenames>Prabuchandran K.</forenames></author><author><keyname>Narahari</keyname><forenames>Y.</forenames></author></authors><title>A Multi-phase Approach for Improving Information Diffusion in Social
  Networks</title><categories>cs.SI</categories><comments>To appear in Proceedings of The 14th International Conference on
  Autonomous Agents &amp; Multiagent Systems (AAMAS), 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For maximizing influence spread in a social network, given a certain budget
on the number of seed nodes, we investigate the effects of selecting and
activating the seed nodes in multiple phases. In particular, we formulate an
appropriate objective function for two-phase influence maximization under the
independent cascade model, investigate its properties, and propose algorithms
for determining the seed nodes in the two phases. We also study the problem of
determining an optimal budget-split and delay between the two phases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06134</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06134</id><created>2015-02-21</created><updated>2015-06-15</updated><authors><author><keyname>Liang</keyname><forenames>Tengyuan</forenames></author><author><keyname>Rakhlin</keyname><forenames>Alexander</forenames></author><author><keyname>Sridharan</keyname><forenames>Karthik</forenames></author></authors><title>Learning with Square Loss: Localization through Offset Rademacher
  Complexity</title><categories>stat.ML cs.LG math.ST stat.TH</categories><comments>21 pages, 1 figure</comments><journal-ref>Journal of Machine Learning Research W&amp;CP vol 40: 1260-1285, 2015
  (COLT 2015)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider regression with square loss and general classes of functions
without the boundedness assumption. We introduce a notion of offset Rademacher
complexity that provides a transparent way to study localization both in
expectation and in high probability. For any (possibly non-convex) class, the
excess loss of a two-step estimator is shown to be upper bounded by this offset
complexity through a novel geometric inequality. In the convex case, the
estimator reduces to an empirical risk minimizer. The method recovers the
results of \citep{RakSriTsy15} for the bounded case while also providing
guarantees without the boundedness assumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06144</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06144</id><created>2015-02-21</created><authors><author><keyname>Berthet</keyname><forenames>Quentin</forenames></author><author><keyname>Ellenberg</keyname><forenames>Jordan S.</forenames></author></authors><title>Detection of Planted Solutions for Flat Satisfiability Problems</title><categories>math.ST cs.CC cs.LG stat.TH</categories><msc-class>62C20, 68R01, 60C05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the detection problem of finding planted solutions in random
instances of flat satisfiability problems, a generalization of boolean
satisfiability formulas. We describe the properties of random instances of flat
satisfiability, as well of the optimal rates of detection of the associated
hypothesis testing problem. We also study the performance of an algorithmically
efficient testing procedure. We introduce a modification of our model, the
light planting of solutions, and show that it is as hard as the problem of
learning parity with noise. This hints strongly at the difficulty of detecting
planted flat satisfiability for a wide class of tests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06149</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06149</id><created>2015-02-21</created><authors><author><keyname>Milosavljevic</keyname><forenames>Nebojsa</forenames></author><author><keyname>Pawar</keyname><forenames>Sameer</forenames></author><author><keyname>Rouayheb</keyname><forenames>Salim El</forenames></author><author><keyname>Gastpar</keyname><forenames>Michael</forenames></author><author><keyname>Ramchandran</keyname><forenames>Kannan</forenames></author></authors><title>Efficient Algorithms for the Data Exchange Problem</title><categories>cs.IT math.IT</categories><comments>submitted to Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the data exchange problem where a set of users is
interested in gaining access to a common file, but where each has only partial
knowledge about it as side-information. Assuming that the file is broken into
packets, the side-information considered is in the form of linear combinations
of the file packets. Given that the collective information of all the users is
sufficient to allow recovery of the entire file, the goal is for each user to
gain access to the file while minimizing some communication cost. We assume
that users can communicate over a noiseless broadcast channel, and that the
communication cost is a sum of each user's cost function over the number of
bits it transmits. For instance, the communication cost could simply be the
total number of bits that needs to be transmitted. In the most general case
studied in this paper, each user can have any arbitrary convex cost function.
We provide deterministic, polynomial-time algorithms (in the number of users
and packets) which find an optimal communication scheme that minimizes the
communication cost. To further lower the complexity, we also propose a simple
randomized algorithm inspired by our deterministic algorithm which is based on
a random linear network coding scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06152</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06152</id><created>2015-02-21</created><updated>2015-04-07</updated><authors><author><keyname>Norton</keyname><forenames>Graham H.</forenames></author></authors><title>On Sequences, Rational Functions and Decomposition</title><categories>cs.SC</categories><comments>Several more typos corrected. To appear in J. Applied Algebra in
  Engineering, Communication and Computing. The final publication version is
  available at Springer via http://dx.doi.org/10.1007/s00200-015-0256-5</comments><doi>10.1007/s00200-015-0256-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our overall goal is to unify and extend some results in the literature
related to the approximation of generating functions of finite and infinite
sequences over a field by rational functions. In our approach, numerators play
a significant role. We revisit a theorem of Niederreiter on (i) linear
complexities and (ii) '$n^{th}$ minimal polynomials' of an infinite sequence,
proved using partial quotients. We prove (i) and its converse from first
principles and generalise (ii) to rational functions where the denominator need
not have minimal degree. We prove (ii) in two parts: firstly for geometric
sequences and then for sequences with a jump in linear complexity. The basic
idea is to decompose the denominator as a sum of polynomial multiples of two
polynomials of minimal degree; there is a similar decomposition for the
numerators. The decomposition is unique when the denominator has degree at most
the length of the sequence. The proof also applies to rational functions
related to finite sequences, generalising a result of Massey. We give a number
of applications to rational functions associated to sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06160</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06160</id><created>2015-02-21</created><updated>2015-11-22</updated><authors><author><keyname>Koczkodaj</keyname><forenames>W. W.</forenames></author><author><keyname>Szybowski</keyname><forenames>J.</forenames></author><author><keyname>Wajch</keyname><forenames>E.</forenames></author></authors><title>Inconsistency indicator maps on groups for pairwise comparisons</title><categories>cs.DM</categories><comments>17 pages, 2 figures (already accepted for publication)</comments><msc-class>Primary: 06F15, 54E35, secondary: 00A69, 91E45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study presents an abelian group approach to analyzing inconsistency in
pairwise comparisons. A notion of an inconsistency indicator map on a group,
taking values in an abelian linearly ordered group, is introduced. For it,
metrics and generalized metrics are utilized. Every inconsistency indicator map
generates both a metric on a group and an inconsistency indicator of an
arbitrary pairwise comparisons matrix over the group.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06161</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06161</id><created>2015-02-21</created><authors><author><keyname>Marzag&#xe3;o</keyname><forenames>Thiago</forenames></author></authors><title>Using NLP to measure democracy</title><categories>cs.CL cs.IR cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper uses natural language processing to create the first machine-coded
democracy index, which I call Automated Democracy Scores (ADS). The ADS are
based on 42 million news articles from 6,043 different sources and cover all
independent countries in the 1993-2012 period. Unlike the democracy indices we
have today the ADS are replicable and have standard errors small enough to
actually distinguish between cases.
  The ADS are produced with supervised learning. Three approaches are tried: a)
a combination of Latent Semantic Analysis and tree-based regression methods; b)
a combination of Latent Dirichlet Allocation and tree-based regression methods;
and c) the Wordscores algorithm. The Wordscores algorithm outperforms the
alternatives, so it is the one on which the ADS are based.
  There is a web application where anyone can change the training set and see
how the results change: democracy-scores.org
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06164</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06164</id><created>2015-02-21</created><authors><author><keyname>Mudunuru</keyname><forenames>M. K.</forenames></author><author><keyname>Nakshatrala</keyname><forenames>K. B.</forenames></author></authors><title>On mesh restrictions to satisfy comparison principles, maximum
  principles, and the non-negative constraint: Recent developments and new
  results</title><categories>cs.NA math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper concerns with mesh restrictions that are needed to satisfy several
important mathematical properties -- maximum principles, comparison principles,
and the non-negative constraint -- for a general linear second-order elliptic
partial differential equation. We critically review some recent developments in
the field of discrete maximum principles, derive new results, and discuss some
possible future research directions in this area. In particular, we derive
restrictions for a three-node triangular (T3) element and a four-node
quadrilateral (Q4) element to satisfy comparison principles, maximum
principles, and the non-negative constraint under the standard single-field
Galerkin formulation. Analysis is restricted to uniformly elliptic linear
differential operators in divergence form with Dirichlet boundary conditions
specified on the entire boundary of the domain. Various versions of maximum
principles and comparison principles are discussed in both continuous and
discrete settings. In the literature, it is well-known that an acute-angled
triangle is sufficient to satisfy the discrete weak maximum principle for pure
isotropic diffusion. An iterative algorithm is developed to construct
simplicial meshes that preserves discrete maximum principles using existing
open source mesh generators. Various numerical examples based on different
types of triangulations are presented to show the pros and cons of placing
restrictions on a computational mesh. We also quantify local and global mass
conservation errors using representative numerical examples, and illustrate the
performance of metric-based meshes with respect to mass conservation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06168</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06168</id><created>2015-02-21</created><authors><author><keyname>Shahrokhi</keyname><forenames>Farhad</forenames></author></authors><title>A new upper bound for the clique cover number with applications</title><categories>math.CO cs.DM cs.DS</categories><journal-ref>Congressus Numerantium 205 (2010), 105-111</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\alpha(G)$ and $\beta(G)$, denote the size of a largest independent set
and the clique cover number of an undirected graph $G$. Let $H$ be an interval
graph with $V(G)=V(H)$ and $E(G)\subseteq E(H)$, and let $\phi(G,H)$ denote the
maximum of ${\beta(G[W])\over \alpha(G[W])}$ overall induced subgraphs $G[W]$
of $G$ that are cliques in $H$. The main result of this paper is to prove that
for any graph $G$ $${\beta(G)}\le 2 \alpha(H)\phi(G,H)(\log \alpha(H)+1),$$
where, $\alpha(H)$ is the size of a largest independent set in $H$. We further
provide a generalization that significantly unifies or improves some past
algorithmic and structural results concerning the clique cover number for some
well known intersection graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06175</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06175</id><created>2015-02-21</created><authors><author><keyname>Shahrokhi</keyname><forenames>Farhad</forenames></author></authors><title>New representation results for planar graphs</title><categories>math.CO cs.CG</categories><comments>29th European Workshop on Computational Geometry March 17-20, 2013,
  177-181</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A universal representation theorem is derived that shows any graph is the
intersection graph of one chordal graph, a number of co-bipartite graphs, and
one unit interval graph. Central to the the result is the notion of the clique
cover width which is a generalization of the bandwidth parameter. Specifically,
we show that any planar graph is the intersection graph of one chordal graph,
four co-bipartite graphs, and one unit interval graph. Equivalently, any planar
graph is the intersection graph of a chordal graph and a graph that has {clique
cover width} of at most seven. We further describe the extensions of the
results to graphs drawn on surfaces and graphs excluding a minor of crossing
number of at most one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06176</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06176</id><created>2015-02-21</created><authors><author><keyname>Shahrokhi</keyname><forenames>Farhad</forenames></author></authors><title>New separation theorems and sub-exponential time algorithms for packing
  and piercing of fat objects</title><categories>cs.CG</categories><comments>28th European Workshop on Computational Geometry,2012 - Assisi,
  Perugia, Italy, 269-273</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For $\cal C$ a collection of $n$ objects in $R^d$, let the packing and
piercing numbers of $\cal C$, denoted by $Pack({\cal C})$, and $Pierce({\cal
C})$, respectively, be the largest number of pairwise disjoint objects in
${\cal C}$, and the smallest number of points in $R^d$ that are common to all
elements of ${\cal C}$, respectively. When elements of $\cal C$ are fat objects
of arbitrary sizes, we derive sub-exponential time algorithms for the NP-hard
problems of computing ${Pack}({\cal C})$ and $Pierce({\cal C})$, respectively,
that run in $n^{O_d({{Pack}({\cal C})}^{d-1\over d})}$ and
$n^{O_d({{Pierce}({\cal C})}^{d-1\over d})}$ time, respectively, and $O(n\log
n)$ storage. Our main tool which is interesting in its own way, is a new
separation theorem. The algorithms readily give rise to polynomial time
approximation schemes (PTAS) that run in $n^{O({({1\over\epsilon})}^{d-1})}$
time and $O(n\log n)$ storage. The results favorably compare with many related
best known results. Specifically, our separation theorem significantly improves
the splitting ratio of the previous result of Chan, whereas, the
sub-exponential time algorithms significantly improve upon the running times of
very recent algorithms of Fox and Pach for packing of spheres.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06177</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06177</id><created>2015-02-21</created><authors><author><keyname>Shalev-Shwartz</keyname><forenames>Shai</forenames></author></authors><title>SDCA without Duality</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic Dual Coordinate Ascent is a popular method for solving regularized
loss minimization for the case of convex losses. In this paper we show how a
variant of SDCA can be applied for non-convex losses. We prove linear
convergence rate even if individual loss functions are non-convex as long as
the expected loss is convex.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06187</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06187</id><created>2015-02-22</created><authors><author><keyname>Moran</keyname><forenames>Shay</forenames></author><author><keyname>Shpilka</keyname><forenames>Amir</forenames></author><author><keyname>Wigderson</keyname><forenames>Avi</forenames></author><author><keyname>Yehudayoff</keyname><forenames>Amir</forenames></author></authors><title>Teaching and compressing for low VC-dimension</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we study the quantitative relation between VC-dimension and two
other basic parameters related to learning and teaching. We present relatively
efficient constructions of {\em sample compression schemes} and {\em teaching
sets} for classes of low VC-dimension. Let $C$ be a finite boolean concept
class of VC-dimension $d$. Set $k = O(d 2^d \log \log |C|)$.
  We construct sample compression schemes of size $k$ for $C$, with additional
information of $k \log(k)$ bits. Roughly speaking, given any list of
$C$-labelled examples of arbitrary length, we can retain only $k$ labeled
examples in a way that allows to recover the labels of all others examples in
the list.
  We also prove that there always exists a concept $c$ in $C$ with a teaching
set (i.e. a list of $c$-labelled examples uniquely identifying $c$) of size
$k$. Equivalently, we prove that the recursive teaching dimension of $C$ is at
most $k$.
  The question of constructing sample compression schemes for classes of small
VC-dimension was suggested by Littlestone and Warmuth (1986), and the problem
of constructing teaching sets for classes of small VC-dimension was suggested
by Kuhlmann (1999). Previous constructions for general concept classes yielded
size $O(\log |C|)$ for both questions, even when the VC-dimension is constant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06188</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06188</id><created>2015-02-22</created><authors><author><keyname>Lin</keyname><forenames>Jiun-Ren</forenames></author><author><keyname>Talty</keyname><forenames>Timothy</forenames></author><author><keyname>Tonguz</keyname><forenames>Ozan K.</forenames></author></authors><title>An empirical performance study of Intra-vehicular Wireless Sensor
  Networks under WiFi and Bluetooth interference</title><categories>cs.NI</categories><journal-ref>IEEE Global Communications Conference (GLOBECOM), p. 581-586,
  December 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intra-Vehicular Wireless Sensor Network (IVWSN) is a new automotive
architecture that applies wireless technologies to the communications between
Electrical Control Units (ECUs) and sensors. It can potentially help achieve
better fuel economy, reduce wiring complexity, and support additional new
applications. In the existing works, most of the popular wireless technologies
applied on IVWSNs occupy the same 2.4 GHz ISM frequency bands as WiFi and
Bluetooth do. It is therefore essential to evaluate the performance of IVWSNs
under interference from WiFi and Bluetooth devices, especially when these
devices are inside the vehicle.
  In this paper, we report the results of a comprehensive experimental study of
IVWSNs based on ZigBee and Bluetooth Low Energy under WiFi and Bluetooth
interference. The impact of the interference from Bluetooth and WiFi devices
can be clearly observed from the experiments. The results of the experiments
conducted suggest that Bluetooth Low Energy technology outperforms ZigBee
technology in the context of IVWSNs when WiFi interference exists in the car.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06189</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06189</id><created>2015-02-22</created><authors><author><keyname>Firouzi</keyname><forenames>Hamed</forenames></author><author><keyname>Rajaratnam</keyname><forenames>Bala</forenames></author><author><keyname>Hero</keyname><forenames>Alfred</forenames></author></authors><title>Two-stage Sampling, Prediction and Adaptive Regression via Correlation
  Screening (SPARCS)</title><categories>stat.ML cs.LG</categories><comments>arXiv admin note: text overlap with arXiv:1303.2378</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a general adaptive procedure for budget-limited predictor
design in high dimensions called two-stage Sampling, Prediction and Adaptive
Regression via Correlation Screening (SPARCS). SPARCS can be applied to high
dimensional prediction problems in experimental science, medicine, finance, and
engineering, as illustrated by the following. Suppose one wishes to run a
sequence of experiments to learn a sparse multivariate predictor of a dependent
variable $Y$ (disease prognosis for instance) based on a $p$ dimensional set of
independent variables $\mathbf X=[X_1,\ldots, X_p]^T$ (assayed biomarkers).
Assume that the cost of acquiring the full set of variables $\mathbf X$
increases linearly in its dimension. SPARCS breaks the data collection into two
stages in order to achieve an optimal tradeoff between sampling cost and
predictor performance. In the first stage we collect a few ($n$) expensive
samples $\{y_i,\mathbf x_i\}_{i=1}^n$, at the full dimension $p\gg n$ of
$\mathbf X$, winnowing the number of variables down to a smaller dimension $l &lt;
p$ using a type of cross-correlation or regression coefficient screening. In
the second stage we collect a larger number $(t-n)$ of cheaper samples of the
$l$ variables that passed the screening of the first stage. At the second
stage, a low dimensional predictor is constructed by solving the standard
regression problem using all $t$ samples of the selected variables. SPARCS is
an adaptive online algorithm that implements false positive control on the
selected variables, is well suited to small sample sizes, and is scalable to
high dimensions. We establish asymptotic bounds for the Familywise Error Rate
(FWER), specify high dimensional convergence rates for support recovery, and
establish optimal sample allocation rules to the first and second stages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06194</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06194</id><created>2015-02-22</created><authors><author><keyname>Sebti</keyname><forenames>Nadia Ouali</forenames></author><author><keyname>Ziadi</keyname><forenames>Djelloul</forenames></author></authors><title>Algorithm for the k-Position Tree Automaton Construction</title><categories>cs.FL</categories><comments>arXiv admin note: text overlap with arXiv:1403.6251, arXiv:1401.5951</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The word position automaton was introduced by Glushkov and McNaughton in the
early 1960. This automaton is homogeneous and has (||\E||+1) states for a word
expression of alphabetic width ||\E||. This kind of automata is extended to
regular tree expressions.
  In this paper, we give an efficient algorithm that computes the \Follow sets,
which are used in different algorithms of conversion of a regular expression
into tree automata. In the following, we consider the k-position tree automaton
construction. We prove that for a regular expression \E of a size |\E| and
alphabetic width ||\E||, the \Follow sets can be computed in O(||\E||\cdot
|\E|) time complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06195</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06195</id><created>2015-02-22</created><updated>2015-04-28</updated><authors><author><keyname>Baram</keyname><forenames>Alon</forenames></author><author><keyname>Fogel</keyname><forenames>Efi</forenames></author><author><keyname>Hemmer</keyname><forenames>Michael</forenames></author><author><keyname>Halperin</keyname><forenames>Dan</forenames></author><author><keyname>Morr</keyname><forenames>Sebastian</forenames></author></authors><title>Exact Minkowski Sums of Polygons With Holes</title><categories>cs.CG</categories><comments>13 pages, 7 figures. Submitted to ESA 2015</comments><doi>10.1007/978-3-662-48350-3_7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an efficient algorithm that computes the Minkowski sum of two
polygons, which may have holes. The new algorithm is based on the convolution
approach. Its efficiency stems in part from a property for Minkowski sums of
polygons with holes, which in fact holds in any dimension: Given two polygons
with holes, for each input polygon we can fill up the holes that are relatively
small compared to the other polygon. Specifically, we can always fill up all
the holes of at least one polygon, transforming it into a simple polygon, and
still obtain exactly the same Minkowski sum. Obliterating holes in the input
summands speeds up the computation of Minkowski sums.
  We introduce a robust implementation of the new algorithm, which follows the
Exact Geometric Computation paradigm and thus guarantees exact results. We also
present an empirical comparison of the performance of Minkowski sum
construction of various input examples, where we show that the implementation
of the new algorithm exhibits better performance than several other
implementations in many cases. In particular, we compared the implementation of
the new algorithm, an implementation of the standard convolution algorithm, and
an implementation of the decomposition approach using various convex
decomposition methods, including two new methods that handle polygons with
holes - one is based on vertical decomposition and the other is based on
triangulation.
  The software has been developed as an extension of the &quot;2D Minkowski Sums&quot;
package of CGAL (Computational Geometry Algorithms Library). Additional
information and supplementary material is available at our project page
http://acg.cs.tau.ac.il/projects/rc
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06197</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06197</id><created>2015-02-22</created><updated>2015-03-03</updated><authors><author><keyname>Javanmard</keyname><forenames>Adel</forenames></author><author><keyname>Montanari</keyname><forenames>Andrea</forenames></author></authors><title>On Online Control of False Discovery Rate</title><categories>stat.ME cs.LG math.ST stat.AP stat.TH</categories><comments>31 pages, 6 figures (minor edits)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple hypotheses testing is a core problem in statistical inference and
arises in almost every scientific field. Given a sequence of null hypotheses
$\mathcal{H}(n) = (H_1,..., H_n)$, Benjamini and Hochberg
\cite{benjamini1995controlling} introduced the false discovery rate (FDR)
criterion, which is the expected proportion of false positives among rejected
null hypotheses, and proposed a testing procedure that controls FDR below a
pre-assigned significance level. They also proposed a different criterion,
called mFDR, which does not control a property of the realized set of tests;
rather it controls the ratio of expected number of false discoveries to the
expected number of discoveries.
  In this paper, we propose two procedures for multiple hypotheses testing that
we will call &quot;LOND&quot; and &quot;LORD&quot;. These procedures control FDR and mFDR in an
\emph{online manner}. Concretely, we consider an ordered --possibly infinite--
sequence of null hypotheses $\mathcal{H} = (H_1,H_2,H_3,...)$ where, at each
step $i$, the statistician must decide whether to reject hypothesis $H_i$
having access only to the previous decisions. To the best of our knowledge, our
work is the first that controls FDR in this setting. This model was introduced
by Foster and Stine \cite{alpha-investing} whose alpha-investing rule only
controls mFDR in online manner.
  In order to compare different procedures, we develop lower bounds on the
total discovery rate under the mixture model and prove that both LOND and LORD
have nearly linear number of discoveries. We further propose adjustment to LOND
to address arbitrary correlation among the $p$-values. Finally, we evaluate the
performance of our procedures on both synthetic and real data comparing them
with alpha-investing rule, Benjamin-Hochberg method and a Bonferroni procedure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06208</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06208</id><created>2015-02-22</created><authors><author><keyname>Gottlieb</keyname><forenames>Lee-Ad</forenames></author><author><keyname>Kontorovich</keyname><forenames>Aryeh</forenames></author></authors><title>Nearly optimal classification for semimetrics</title><categories>cs.LG cs.CC cs.DS</categories><msc-class>51F99, 51K05, 90C27, 90C48,</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We initiate the rigorous study of classification in semimetric spaces, which
are point sets with a distance function that is non-negative and symmetric, but
need not satisfy the triangle inequality. For metric spaces, the doubling
dimension essentially characterizes both the runtime and sample complexity of
classification algorithms --- yet we show that this is not the case for
semimetrics. Instead, we define the {\em density dimension} and discover that
it plays a central role in the statistical and algorithmic feasibility of
learning in semimetric spaces. We present nearly optimal sample compression
algorithms and use these to obtain generalization guarantees, including fast
rates. The latter hold for general sample compression schemes and may be of
independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06218</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06218</id><created>2015-02-22</created><authors><author><keyname>Alamir</keyname><forenames>Mazen</forenames></author></authors><title>On Probabilistic Certification of Combined Cancer Therapies Using
  Strongly Uncertain Models</title><categories>cs.SY</categories><comments>Submitted to Journal of theoretical Biology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a general framework for probabilistic certification of
cancer therapies. The certification is defined in terms of two key issues which
are the tumor contraction and the lower admissible bound on the circulating
lymphocytes which is viewed as indicator of the patient health. The
certification is viewed as the ability to guarantee with a predefined high
probability the success of the therapy over a finite horizon despite of the
unavoidable high uncertainties affecting the dynamic model that is used to
compute the optimal scheduling of drugs injection. The certification paradigm
can be viewed as a tool for tuning the treatment parameters and protocols as
well as for getting a rational use of limited or expensive drugs. The proposed
framework is illustrated using the specific problem of combined
immunotherapy/chemotherapy of cancer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06219</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06219</id><created>2015-02-22</created><authors><author><keyname>Shekar</keyname><forenames>B. H.</forenames></author><author><keyname>L.</keyname><forenames>Smitha M.</forenames></author></authors><title>Video Text Localization with an emphasis on Edge Features</title><categories>cs.CV</categories><comments>8 pages, Eighth International Conference on Image and Signal
  Processing, Elsevier Publications, ISBN: 9789351072522, pp: 324-330, held at
  UVCE, Bangalore in July 2014. arXiv admin note: text overlap with
  arXiv:1502.03913</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The text detection and localization plays a major role in video analysis and
understanding. The scene text embedded in video consist of high-level semantics
and hence contributes significantly to visual content analysis and retrieval.
This paper proposes a novel method to robustly localize the texts in natural
scene images and videos based on sobel edge emphasizing approach. The input
image is preprocessed and edge emphasis is done to detect the text clusters.
Further, a set of rules have been devised using morphological operators for
false positive elimination and connected component analysis is performed to
detect the text regions and hence text localization is performed. The
experimental results obtained on publicly available standard datasets
illustrate that the proposed method can detect and localize the texts of
various sizes, fonts and colors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06220</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06220</id><created>2015-02-22</created><updated>2015-03-12</updated><authors><author><keyname>Romano</keyname><forenames>Yaniv</forenames></author><author><keyname>Elad</keyname><forenames>Michael</forenames></author></authors><title>Boosting of Image Denoising Algorithms</title><categories>cs.CV cs.NA</categories><comments>33 pages, 9 figures, 3 tables, submitted to SIAM Journal on Imaging
  Sciences</comments><msc-class>68U10, 94A08, 62H35, 05C50, 47A52, 68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a generic recursive algorithm for improving image
denoising methods. Given the initial denoised image, we suggest repeating the
following &quot;SOS&quot; procedure: (i) (S)trengthen the signal by adding the previous
denoised image to the degraded input image, (ii) (O)perate the denoising method
on the strengthened image, and (iii) (S)ubtract the previous denoised image
from the restored signal-strengthened outcome. The convergence of this process
is studied for the K-SVD image denoising and related algorithms. Still in the
context of K-SVD image denoising, we introduce an interesting interpretation of
the SOS algorithm as a technique for closing the gap between the local
patch-modeling and the global restoration task, thereby leading to improved
performance. In a quest for the theoretical origin of the SOS algorithm, we
provide a graph-based interpretation of our method, where the SOS recursive
update effectively minimizes a penalty function that aims to denoise the image,
while being regularized by the graph Laplacian. We demonstrate the SOS boosting
algorithm for several leading denoising methods (K-SVD, NLM, BM3D, and EPLL),
showing tendency to further improve denoising performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06221</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06221</id><created>2015-02-22</created><updated>2015-07-26</updated><authors><author><keyname>Lu</keyname><forenames>Yi</forenames></author></authors><title>Sampling with Walsh Transforms</title><categories>cs.IT math.IT</categories><comments>in preparation for submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the advent of massive data outputs at a regular rate, admittedly, signal
processing technology plays an increasingly key role. Nowadays, signals are not
merely restricted to physical sources, they have been extended to digital
sources as well.
  Under the general assumption of discrete statistical signal sources, we
propose a practical problem of sampling incomplete noisy signals for which we
do not know a priori and the sample size is bounded. We approach this sampling
problem by Shannon's channel coding theorem. We use an extremal binary channel
with high probability of transmission error, which is rare in communication
theory. Our main result demonstrates that it is the large Walsh coefficient(s)
that characterize(s) discrete statistical signals, regardless of the signal
sources. Note that this is a known fact in specific application domains such as
images. By the connection of Shannon's theorem, we establish the necessary and
sufficient condition for our generic sampling problem for the first time.
Finally, we discuss the cryptographic significance of sparse Walsh transform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06222</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06222</id><created>2015-02-22</created><updated>2015-09-09</updated><authors><author><keyname>Krivulin</keyname><forenames>Nikolai</forenames></author></authors><title>Tropical optimization problems in project scheduling</title><categories>math.OC cs.SY</categories><comments>20 pages. ISSN 2305-249X</comments><msc-class>65K10 (Primary), 15A80, 65K05, 90C48, 90B35 (Secondary)</msc-class><journal-ref>MISTA 2015 Proceedings (Z. Hanzalek, G. Kendall, B. McCollum, P.
  Sucha, eds), pp. 492-506, 2015. Proceedings of the Multidisciplinary
  International Conference on Scheduling: Theory and Applications</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a project that consists of activities operating in parallel under
various temporal constraints, including start-to-start, start-to-finish and
finish-to-start precedence relations, early-start, late-start and late-finish
time boundaries, and due dates. Scheduling problems are formulated to find
optimal schedules for the project with respect to different objective functions
to be minimized, including the project makespan, the maximum deviation from the
due dates, the maximum flow-time, and the maximum deviation of finish times. We
represent the problems as optimization problems in terms of tropical
mathematics, and then solve these problems by applying direct solution methods
of tropical optimization. As a result, new direct solutions of the problems are
obtained in a compact vector form, which is ready for further analysis and
practical implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06229</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06229</id><created>2015-02-22</created><authors><author><keyname>Yan</keyname><forenames>Junchi</forenames></author><author><keyname>Gong</keyname><forenames>Min</forenames></author><author><keyname>Sun</keyname><forenames>Changhua</forenames></author><author><keyname>Huang</keyname><forenames>Jin</forenames></author><author><keyname>Chu</keyname><forenames>Stephen M.</forenames></author></authors><title>Sales pipeline win propensity prediction: a regression approach</title><categories>cs.CY</categories><comments>accepted by IM2015 as short paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sales pipeline analysis is fundamental to proactive management of an
enterprize's sales pipeline and critical for business success. In particular,
win propensity prediction, which involves quantitatively estimating the
likelihood that on-going sales opportunities will be won within a specified
time window, is a fundamental building block for sales management and lays the
foundation for many applications such as resource optimization and sales gap
analysis. With the proliferation of big data, the use of data-driven predictive
models as a means to drive better sales performance is increasingly widespread,
both in business-to-client (B2C) and business-to-business (B2B) markets.
However, the relatively small number of B2B transactions (compared with the
volume of B2C transactions), noisy data, and the fast-changing market
environment pose challenges to effective predictive modeling. This paper
proposes a machine learning-based unified framework for sales opportunity win
propensity prediction, aimed at addressing these challenges. We demonstrate the
efficacy of our proposed system using data from a top-500 enterprize in the
business-to-business market.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06230</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06230</id><created>2015-02-22</created><authors><author><keyname>Draganic</keyname><forenames>Andjela</forenames></author><author><keyname>Orovic</keyname><forenames>Irena</forenames></author><author><keyname>Lekic</keyname><forenames>Nedjeljko</forenames></author><author><keyname>Dakovic</keyname><forenames>Milos</forenames></author><author><keyname>Stankovic</keyname><forenames>Srdjan</forenames></author></authors><title>Hardware Architecture for Single Iteration Reconstruction Algorithm</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A hardware architecture for the single iteration algorithm is proposed in
this paper. Single iteration algorithm enables reconstruction of the full
signal when small number of signal samples is available. The algorithm is based
on the threshold calculation, and allows distinguishing between signal
components and noise that appears as a consequence of missing samples. The
proposed system for hardware realization is divided into three parts, each part
with different functionality. The system is suitable for the FPGA realization.
Realization of the blocks for which there are no standard components in FPGA,
is discussed as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06231</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06231</id><created>2015-02-22</created><updated>2015-08-04</updated><authors><author><keyname>Yang</keyname><forenames>Hui</forenames></author><author><keyname>Tang</keyname><forenames>Ming</forenames></author><author><keyname>Gross</keyname><forenames>Thilo</forenames></author></authors><title>Large epidemic thresholds emerge in heterogeneous networks of
  heterogeneous nodes</title><categories>physics.soc-ph cs.SI</categories><comments>18 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the famous results of network science states that networks with
heterogeneous connectivity are more susceptible to epidemic spreading than
their more homogeneous counterparts. In particular, in networks of identical
nodes it has been shown that heterogeneity can lower the epidemic threshold at
which epidemics can invade the system. Network heterogeneity can thus allow
diseases with lower transmission probabilities to persist and spread. Here, we
point out that for real world applications, this result should not be regarded
independently of the intra-individual heterogeneity between people. Our results
show that, if heterogeneity among people is taken into account, networks that
are more heterogeneous in connectivity can be more resistant to epidemic
spreading. We study a susceptible-infected-susceptible model with adaptive
disease avoidance. Results from this model suggest that this reversal of the
effect of network heterogeneity is likely to occur in populations in which the
individuals are aware of their subjective disease risk. For epidemiology, this
implies that network heterogeneity should not be studied in isolation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06235</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06235</id><created>2015-02-22</created><authors><author><keyname>Anti&#x107;</keyname><forenames>Borislav</forenames></author><author><keyname>Ommer</keyname><forenames>Bj&#xf6;rn</forenames></author></authors><title>Spatio-temporal Video Parsing for Abnormality Detection</title><categories>cs.CV</categories><comments>15 pages, 12 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Abnormality detection in video poses particular challenges due to the
infinite size of the class of all irregular objects and behaviors. Thus no (or
by far not enough) abnormal training samples are available and we need to find
abnormalities in test data without actually knowing what they are.
Nevertheless, the prevailing concept of the field is to directly search for
individual abnormal local patches or image regions independent of another. To
address this problem, we propose a method for joint detection of abnormalities
in videos by spatio-temporal video parsing. The goal of video parsing is to
find a set of indispensable normal spatio-temporal object hypotheses that
jointly explain all the foreground of a video, while, at the same time, being
supported by normal training samples. Consequently, we avoid a direct detection
of abnormalities and discover them indirectly as those hypotheses which are
needed for covering the foreground without finding an explanation for
themselves by normal samples. Abnormalities are localized by MAP inference in a
graphical model and we solve it efficiently by formulating it as a convex
optimization problem. We experimentally evaluate our approach on several
challenging benchmark sets, improving over the state-of-the-art on all standard
benchmarks both in terms of abnormality classification and localization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06236</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06236</id><created>2015-02-22</created><authors><author><keyname>Staecker</keyname><forenames>P. Christopher</forenames></author></authors><title>Some enumerations of binary digital images</title><categories>math.CO cs.CV math.GN</categories><msc-class>55P10, 68R10, 05B50</msc-class><acm-class>I.4.m</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The topology of digital images has been studied much in recent years, but no
attempt has been made to exhaustively catalog the structure of binary images of
small numbers of points. We produce enumerations of several classes of digital
images up to isomorphism and decide which among them are homotopy equivalent to
one another. Noting some patterns in the results, we make some conjectures
about digital images which are irreducible but not rigid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06250</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06250</id><created>2015-02-22</created><authors><author><keyname>Cariow</keyname><forenames>Aleksandr</forenames></author><author><keyname>Cariowa</keyname><forenames>Galina</forenames></author><author><keyname>Knapinski</keyname><forenames>Jaroslaw</forenames></author></authors><title>Derivation of a low multiplicative complexity algorithm for multiplying
  hyperbolic octonions</title><categories>cs.DS</categories><comments>15 pages, 4 figures</comments><msc-class>15A66, 65Y20, 65F30, 15A23</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an efficient algorithm to multiply two hyperbolic octonions. The
direct multiplication of two hyperbolic octonions requires 64 real
multiplications and 56 real additions. More effective solutions still do not
exist. We show how to compute a product of the hyperbolic octonions with 26
real multiplications and 92 real additions. During synthesis of the discussed
algorithm we use the fact that product of two hyperbolic octonions may be
represented as a matrix - vector product. The matrix multiplicand that
participates in the product calculating has unique structural properties that
allow performing its advantageous factorization. Namely this factorization
leads to significant reducing of the computational complexity of hyperbolic
octonions multiplication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06254</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06254</id><created>2015-02-22</created><updated>2015-06-28</updated><authors><author><keyname>Vovk</keyname><forenames>Vladimir</forenames></author></authors><title>The fundamental nature of the log loss function</title><categories>cs.LG stat.ME</categories><comments>12 pages</comments><msc-class>68T05, 68T37, 60G25, 62M20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The standard loss functions used in the literature on probabilistic
prediction are the log loss function, the Brier loss function, and the
spherical loss function; however, any computable proper loss function can be
used for comparison of prediction algorithms. This note shows that the log loss
function is most selective in that any prediction algorithm that is optimal for
a given data sequence (in the sense of the algorithmic theory of randomness)
under the log loss function will be optimal under any computable proper mixable
loss function; on the other hand, there is a data sequence and a prediction
algorithm that is optimal for that sequence under either of the two other
standard loss functions but not under the log loss function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06256</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06256</id><created>2015-02-22</created><updated>2015-07-09</updated><authors><author><keyname>Brinda</keyname><forenames>Karel</forenames></author><author><keyname>Sykulski</keyname><forenames>Maciej</forenames></author><author><keyname>Kucherov</keyname><forenames>Gregory</forenames></author></authors><title>Spaced seeds improve k-mer-based metagenomic classification</title><categories>q-bio.GN cs.CE cs.LG</categories><comments>23 pages</comments><doi>10.1093/bioinformatics/btv419</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Metagenomics is a powerful approach to study genetic content of environmental
samples that has been strongly promoted by NGS technologies. To cope with
massive data involved in modern metagenomic projects, recent tools [4, 39] rely
on the analysis of k-mers shared between the read to be classified and sampled
reference genomes. Within this general framework, we show in this work that
spaced seeds provide a significant improvement of classification accuracy as
opposed to traditional contiguous k-mers. We support this thesis through a
series a different computational experiments, including simulations of
large-scale metagenomic projects. Scripts and programs used in this study, as
well as supplementary material, are available from
http://github.com/gregorykucherov/spaced-seeds-for-metagenomics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06260</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06260</id><created>2015-02-22</created><authors><author><keyname>Yuan</keyname><forenames>Xin</forenames></author><author><keyname>Tsai</keyname><forenames>Tsung-Han</forenames></author><author><keyname>Zhu</keyname><forenames>Ruoyu</forenames></author><author><keyname>Llull</keyname><forenames>Patrick</forenames></author><author><keyname>Brady</keyname><forenames>David</forenames></author><author><keyname>Carin</keyname><forenames>Lawrence</forenames></author></authors><title>Compressive Hyperspectral Imaging with Side Information</title><categories>cs.CV</categories><comments>20 pages, 21 figures. To appear in the IEEE Journal of Selected
  Topics Signal Processing</comments><doi>10.1109/JSTSP.2015.2411575</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A blind compressive sensing algorithm is proposed to reconstruct
hyperspectral images from spectrally-compressed measurements.The
wavelength-dependent data are coded and then superposed, mapping the
three-dimensional hyperspectral datacube to a two-dimensional image. The
inversion algorithm learns a dictionary {\em in situ} from the measurements via
global-local shrinkage priors. By using RGB images as side information of the
compressive sensing system, the proposed approach is extended to learn a
coupled dictionary from the joint dataset of the compressed measurements and
the corresponding RGB images, to improve reconstruction quality. A prototype
camera is built using a liquid-crystal-on-silicon modulator. Experimental
reconstructions of hyperspectral datacubes from both simulated and real
compressed measurements demonstrate the efficacy of the proposed inversion
algorithm, the feasibility of the camera and the benefit of side information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06274</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06274</id><created>2015-02-22</created><authors><author><keyname>Evrard</keyname><forenames>August E.</forenames></author><author><keyname>Erdmann</keyname><forenames>Christopher</forenames></author><author><keyname>Holmquist</keyname><forenames>Jane</forenames></author><author><keyname>Damon</keyname><forenames>James</forenames></author><author><keyname>Dietrich</keyname><forenames>Dianne</forenames></author></authors><title>Persistent, Global Identity for Scientists via ORCID</title><categories>cs.DL astro-ph.IM physics.soc-ph</categories><comments>13 pages, 1 figure. The authors of this paper include members of the
  ORCID Ambassadors program</comments><report-no>MCTP-15-06</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scientists have an inherent interest in claiming their contributions to the
scholarly record, but the fragmented state of identity management across the
landscape of astronomy, physics, and other fields makes highlighting the
contributions of any single individual a formidable and often frustratingly
complex task. The problem is exacerbated by the expanding variety of academic
research products and the growing footprints of large collaborations and
interdisciplinary teams. In this essay, we outline the benefits of a unique
scholarly identifier with persistent value on a global scale and we review
astronomy and physics engagement with the Open Researcher and Contributor iD
(ORCID) service as a solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06277</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06277</id><created>2015-02-22</created><updated>2015-05-20</updated><authors><author><keyname>Abbes</keyname><forenames>Samy</forenames></author></authors><title>A cut-invariant law of large numbers for random heaps</title><categories>math.CO cs.DM math.PR</categories><comments>29 pages, 3 figures, 21 references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heap monoids equipped with Bernoulli measures are a model of probabilistic
asynchronous systems. We introduce in this framework the notion of asynchronous
stopping time, which is analogous to the notion of stopping time for classical
probabilistic processes. A Strong Bernoulli property is proved. A notion of
cut-invariance is formulated for convergent ergodic means. Then a version of
the Strong law of large numbers is proved for heap monoids with Bernoulli
measures. Finally, we study a sub-additive version of the Law of large numbers
in this framework based on Kingman sub-additive Ergodic Theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06286</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06286</id><created>2015-02-22</created><authors><author><keyname>Lin</keyname><forenames>Yixiao</forenames></author><author><keyname>Mitra</keyname><forenames>Sayan</forenames></author></authors><title>StarL: Towards a Unified Framework for Programming, Simulating and
  Verifying Distributed Robotic Systems</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We developed StarL as a framework for programming, simulating, and verifying
distributed systems that interacts with physical processes. StarL framework has
(a) a collection of distributed primitives for coordination, such as mutual
exclusion, registration and geocast that can be used to build sophisticated
applications, (b) theory libraries for verifying StarL applications in the PVS
theorem prover, and (c) an execution environment that can be used to deploy the
applications on hardware or to execute them in a discrete event simulator. The
primitives have (i) abstract, nondeterministic specifications in terms of
invariants, and assume-guarantee style progress properties, (ii)
implementations in Java/Android that always satisfy the invariants and attempt
progress using best effort strategies. The PVS theories specify the invariant
and progress properties of the primitives, and have to be appropriately
instantiated and composed with the application's state machine to prove
properties about the application. We have built two execution environments: one
for deploying applications on Android/iRobot Create platform and a second one
for simulating large instantiations of the applications in a discrete even
simulator. The capabilities are illustrated with a StarL application for
vehicle to vehicle coordination in a automatic intersection that uses
primitives for point-to-point motion, mutual exclusion, and registration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06287</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06287</id><created>2015-02-22</created><authors><author><keyname>Thrampoulidis</keyname><forenames>Christos</forenames></author><author><keyname>Panahi</keyname><forenames>Ashkan</forenames></author><author><keyname>Hassibi</keyname><forenames>Babak</forenames></author></authors><title>Asymptotically Exact Error Analysis for the Generalized $\ell_2^2$-LASSO</title><categories>math.ST cs.IT math.IT stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an unknown signal $\mathbf{x}_0\in\mathbb{R}^n$ and linear noisy
measurements
$\mathbf{y}=\mathbf{A}\mathbf{x}_0+\sigma\mathbf{v}\in\mathbb{R}^m$, the
generalized $\ell_2^2$-LASSO solves
$\hat{\mathbf{x}}:=\arg\min_{\mathbf{x}}\frac{1}{2}\|\mathbf{y}-\mathbf{A}\mathbf{x}\|_2^2
+ \sigma\lambda f(\mathbf{x})$. Here, $f$ is a convex regularization function
(e.g. $\ell_1$-norm, nuclear-norm) aiming to promote the structure of
$\mathbf{x}_0$ (e.g. sparse, low-rank), and, $\lambda\geq 0$ is the regularizer
parameter. A related optimization problem, though not as popular or well-known,
is often referred to as the generalized $\ell_2$-LASSO and takes the form
$\hat{\mathbf{x}}:=\arg\min_{\mathbf{x}}\|\mathbf{y}-\mathbf{A}\mathbf{x}\|_2 +
\lambda f(\mathbf{x})$, and has been analyzed in [1]. [1] further made
conjectures about the performance of the generalized $\ell_2^2$-LASSO. This
paper establishes these conjectures rigorously. We measure performance with the
normalized squared error
$\mathrm{NSE}(\sigma):=\|\hat{\mathbf{x}}-\mathbf{x}_0\|_2^2/\sigma^2$.
Assuming the entries of $\mathbf{A}$ and $\mathbf{v}$ be i.i.d. standard
normal, we precisely characterize the &quot;asymptotic NSE&quot;
$\mathrm{aNSE}:=\lim_{\sigma\rightarrow 0}\mathrm{NSE}(\sigma)$ when the
problem dimensions $m,n$ tend to infinity in a proportional manner. The role of
$\lambda,f$ and $\mathbf{x}_0$ is explicitly captured in the derived expression
via means of a single geometric quantity, the Gaussian distance to the
subdifferential. We conjecture that $\mathrm{aNSE} =
\sup_{\sigma&gt;0}\mathrm{NSE}(\sigma)$. We include detailed discussions on the
interpretation of our result, make connections to relevant literature and
perform computational experiments that validate our theoretical findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06297</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06297</id><created>2015-02-22</created><updated>2016-01-15</updated><authors><author><keyname>Correia</keyname><forenames>Anacleto</forenames></author></authors><title>Elements of style of BPMN language</title><categories>cs.SE</categories><comments>103 pages, 82 figures</comments><acm-class>D.2.9</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several BPMN graphical tools support, at least partly, the OMG's BPMN
specification. The BPMN standard is an essential guide for tools' makers when
implementing the rules regarding depiction of BPMN diagrammatic constructs.
Process modelers should also know how to rigorously use BPMN constructs when
depicting business processes either for business or IT purposes. Several
already published OMG's standards include the formal specification of
well-formedness rules concern-ing the metamodels they address. However, the
BPMN standard does not. Instead, the rules regarding BPMN elements are only
informally specified in natural language throughout the overall BPMN
documentation. Without strict rules concerning the correct usage of BPMN
elements, no wonder that plenty of available BPMN tools fail to enforce BPMN
process models' correctness. To mitigate this problem, and therefore contribute
for achieving BPMN models' correctness, we propose to supplement the BPMN
metamodel with well-formedness rules expressed by OCL invariants. So, this
document contributes to bring together a set of requirements that tools' makers
must comply with, in order to claim a broader BPMN 2 compliance. For the
regular process modeler, this report provides an extensive and pragmatic
catalog of BPMN elements' usage, to be followed in order to attain correct BPMN
process models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06306</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06306</id><created>2015-02-22</created><authors><author><keyname>Kim</keyname><forenames>Jinseok</forenames></author><author><keyname>Diesner</keyname><forenames>Jana</forenames></author></authors><title>Distortive Effects of Initial-Based Name Disambiguation on Measurements
  of Large-Scale Coauthorship Networks</title><categories>cs.DL cs.SI physics.soc-ph</categories><comments>This is a preprint of an article accepted for publication in Journal
  of the Association for Information Science and Technology</comments><doi>10.1002/asi.23489</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scholars have often relied on name initials to resolve name ambiguities in
large-scale coauthorship network research. This approach bears the risk of
incorrectly merging or splitting author identities. The use of initial-based
disambiguation has been justified by the assumption that such errors would not
affect research findings too much. This paper tests this assumption by
analyzing coauthorship networks from five academic fields - biology, computer
science, nanoscience, neuroscience, and physics - and an interdisciplinary
journal, PNAS. Name instances in datasets of this study were disambiguated
based on heuristics gained from previous algorithmic disambiguation solutions.
We use disambiguated data as a proxy of ground-truth to test the performance of
three types of initial-based disambiguation. Our results show that
initial-based disambiguation can misrepresent statistical properties of
coauthorship networks: it deflates the number of unique authors, number of
component, average shortest paths, clustering coefficient, and assortativity,
while it inflates average productivity, density, average coauthor number per
author, and largest component size. Also, on average, more than half of top 10
productive or collaborative authors drop off the lists. Asian names were found
to account for the majority of misidentification by initial-based
disambiguation due to their common surname and given name initials.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06309</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06309</id><created>2015-02-22</created><updated>2015-06-23</updated><authors><author><keyname>Wang</keyname><forenames>Yu-Xiang</forenames></author><author><keyname>Lei</keyname><forenames>Jing</forenames></author><author><keyname>Fienberg</keyname><forenames>Stephen E.</forenames></author></authors><title>Learning with Differential Privacy: Stability, Learnability and the
  Sufficiency and Necessity of ERM Principle</title><categories>stat.ML cs.CR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While machine learning has proven to be a powerful data-driven solution to
many real-life problems, its use in sensitive domains that involve human
subjects has been limited due to privacy concerns. The cryptographic approach
known as &quot;differential privacy&quot; offers provable privacy guarantees. In this
paper we study the learnability under Vapnik's general learning setting with
differential privacy constraint, and reveal some intricate relationships
between privacy, stability and learnability.
  In particular, we show that a problem is privately learnable \emph{if an only
if} there is a private algorithm that asymptotically minimizes the empirical
risk (AERM). This is rather surprising because for non-private learning, AERM
alone is not sufficient for learnability. This result suggests that when
searching for private learning algorithms, we can restrict the search to
algorithms that are AERM. In light of this, we propose a conceptual procedure
that always finds a universally consistent algorithm whenever the problem is
learnable under privacy constraint. We also propose a generic and practical
algorithm and show that under very general conditions it privately learns a
wide class of learning problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06314</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06314</id><created>2015-02-22</created><authors><author><keyname>Chen</keyname><forenames>Fei</forenames></author><author><keyname>Zhang</keyname><forenames>Cong</forenames></author><author><keyname>Wang</keyname><forenames>Feng</forenames></author><author><keyname>Liu</keyname><forenames>Jiangchuan</forenames></author></authors><title>Crowdsourced Live Streaming over the Cloud</title><categories>cs.MM cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Empowered by today's rich tools for media generation and distribution, and
the convenient Internet access, crowdsourced streaming generalizes the
single-source streaming paradigm by including massive contributors for a video
channel. It calls a joint optimization along the path from crowdsourcers,
through streaming servers, to the end-users to minimize the overall latency.
The dynamics of the video sources, together with the globalized request demands
and the high computation demand from each sourcer, make crowdsourced live
streaming challenging even with powerful support from modern cloud computing.
In this paper, we present a generic framework that facilitates a cost-effective
cloud service for crowdsourced live streaming. Through adaptively leasing, the
cloud servers can be provisioned in a fine granularity to accommodate
geo-distributed video crowdsourcers. We present an optimal solution to deal
with service migration among cloud instances of diverse lease prices. It also
addresses the location impact to the streaming quality. To understand the
performance of the proposed strategies in the realworld, we have built a
prototype system running over the planetlab and the Amazon/Microsoft Cloud. Our
extensive experiments demonstrate that the effectiveness of our solution in
terms of deployment cost and streaming quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06318</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06318</id><created>2015-02-23</created><authors><author><keyname>Aziz</keyname><forenames>Haris</forenames></author><author><keyname>Seedig</keyname><forenames>Hans Georg</forenames></author><author><keyname>von Wedel</keyname><forenames>Jana Karina</forenames></author></authors><title>On the Susceptibility of the Deferred Acceptance Algorithm</title><categories>cs.GT</categories><msc-class>91A12, 68Q15</msc-class><acm-class>F.2; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Deferred Acceptance Algorithm (DAA) is the most widely accepted and used
algorithm to match students, workers, or residents to colleges, firms or
hospitals respectively. In this paper, we consider for the first time, the
complexity of manipulating DAA by agents such as colleges that have capacity
more than one. For such agents, truncation is not an exhaustive strategy. We
present efficient algorithms to compute a manipulation for the colleges when
the colleges are proposing or being proposed to. We then conduct detailed
experiments on the frequency of manipulable instances in order to get better
insight into strategic aspects of two-sided matching markets. Our results bear
somewhat negative news: assuming that agents have information other agents'
preference, they not only often have an incentive to misreport but there exist
efficient algorithms to find such a misreport.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06321</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06321</id><created>2015-02-23</created><authors><author><keyname>Cui</keyname><forenames>Ying</forenames></author><author><keyname>M&#xe9;dard</keyname><forenames>Muriel</forenames></author><author><keyname>Yeh</keyname><forenames>Edmund</forenames></author><author><keyname>Leith</keyname><forenames>Douglas</forenames></author><author><keyname>Duffy</keyname><forenames>Ken</forenames></author></authors><title>A Linear Network Code Construction for General Integer Connections Based
  on the Constraint Satisfaction Problem</title><categories>cs.IT math.IT</categories><comments>3 figures, submitted to ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of finding network codes for general connections is inherently
difficult. Resource minimization for general connections with network coding is
further complicated. The existing solutions mainly rely on very restricted
classes of network codes, and are almost all centralized. In this paper, we
introduce linear network mixing coefficients for code constructions of general
connections that generalize random linear network coding (RLNC) for multicast
connections. For such code constructions, we pose the problem of cost
minimization for the subgraph involved in the coding solution and relate this
minimization to a Constraint Satisfaction Problem (CSP) which we show can be
simplified to have a moderate number of constraints. While CSPs are NP-complete
in general, we present a probabilistic distributed algorithm with almost sure
convergence in finite time by applying Communication Free Learning (CFL). Our
approach allows fairly general coding across flows, guarantees no greater cost
than routing, and shows a possible distributed implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06323</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06323</id><created>2015-02-23</created><authors><author><keyname>Mollanoori</keyname><forenames>Mohsen</forenames></author></authors><title>CSMA-SIC: Carrier Sensing with Successive Interference Cancellation</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Successive interference cancellation (SIC) is a physical layer technique that
enables the decoders to decode multiple simultaneously transmitted signals. The
complicated model of SIC requires careful design of the MAC protocol and
accurate adjustment of transmission parameters. We propose a new MAC protocol,
known as CSMA-SIC, that employs the multi-packet reception capability of SIC.
The proposed protocol adjusts the transmission probabilities to achieve
throughput optimality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06327</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06327</id><created>2015-02-23</created><updated>2015-04-19</updated><authors><author><keyname>Khalili</keyname><forenames>Mohammad Mahdi</forenames></author><author><keyname>Gao</keyname><forenames>Lin</forenames></author><author><keyname>Huang</keyname><forenames>Jianwei</forenames></author><author><keyname>Khalaj</keyname><forenames>Babak Hossein</forenames></author></authors><title>Incentive Design and Market Evolution of Mobile User-Provided Networks</title><categories>cs.GT cs.NI</categories><comments>This manuscript serves as the online technical report of the article
  published in IEEE Workshop on Smart Data Pricing (SDP), 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An operator-assisted user-provided network (UPN) has the potential to achieve
a low cost ubiquitous Internet connectivity, without significantly increasing
the network infrastructure investment. In this paper, we consider such a
network where the network operator encourages some of her subscribers to
operate as mobile Wi-Fi hotspots (hosts), providing Internet connectivity for
other subscribers (clients). We formulate the interaction between the operator
and mobile users as a two-stage game. In Stage I, the operator determines the
usage-based pricing and quota-based incentive mechanism for the data usage. In
Stage II, the mobile users make their decisions about whether to be a host, or
a client, or not a subscriber at all. We characterize how the users' membership
choices will affect each other's payoffs in Stage II, and how the operator
optimizes her decision in Stage I to maximize her profit. Our theoretical and
numerical results show that the operator's maximum profit increases with the
user density under the proposed hybrid pricing mechanism, and the profit gain
can be up to 50\% in a dense network comparing with a pricing-only approach
with no incentives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06329</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06329</id><created>2015-02-23</created><authors><author><keyname>Rahman</keyname><forenames>Md. Asadur</forenames></author></authors><title>QoS Provisioning Using Optimal Call Admission Control for Wireless
  Cellular Networks</title><categories>cs.NI</categories><comments>72 pages, 29 figures, 47 references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing demand for advanced services in wireless networks raises the
problem for quality of service (QoS) provisioning with proper resource
management. In this research, such a provisioning technique for wireless
networks is performed by Call Admission Control (CAC). A new approach in CAC
named by Uniform Fractional Band (UFB) is proposed in this work for the
wireless networks for providing proper priority between new calls and handover
calls. This UFB scheme is basically a new style of handover priority scheme.
Handover priority is provided by two stages in this scheme which help the
network to utilize more resources. In addition, the handover call rate
estimation and its impact on QoS provisioning is discussed widely to attain the
optimum QoS in proposed handover priority scheme. In multiple services
providing wireless network, excessive call blocking of lower priority traffic
is very often event at very high traffic rate which is a concerning issue for
QoS provisioning. To attain such QoS provisioning for multiple services,
another CAC scheme is proposed in this research work. This scheme is recognized
by Uniform Band Thinning (UBT) scheme which is based on uniform thinning
technique (UTT) and this is quite similar idea as UFB scheme. In this scheme, a
set of channels experiences the fractionizing policy. This scheme reduces the
call blocking probabilities (CBP) of lower priority traffic classes without
notably increasing the CBP of the higher priority traffic classes. The
analytical functions of this scheme are deduced in general form which is useful
to deduce for any number of traffic classes. In addition, numerical analysis of
the proposed UBT scheme shows that the performances in terms of call blocking
probability, overall call blocking probability, and channel utilization are
improved and optimized compared to the conventional fixed guard channel scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06343</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06343</id><created>2015-02-23</created><authors><author><keyname>Boros</keyname><forenames>Endre</forenames></author><author><keyname>Chiarelli</keyname><forenames>Nina</forenames></author><author><keyname>Milani&#x10d;</keyname><forenames>Martin</forenames></author></authors><title>Equistarable bipartite graphs</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, Milani\v{c} and Trotignon introduced the class of equistarable
graphs as graphs without isolated vertices admitting positive weights on the
edges such that a subset of edges is of total weight $1$ if and only if it
forms a maximal star. Based on equistarable graphs, counterexamples to three
conjectures on equistable graphs were constructed, in particular to Orlin's
conjecture, which states that every equistable graph is a general partition
graph.
  In this paper we characterize equistarable bipartite graphs. We show that a
bipartite graph is equistarable if and only if every $2$-matching of the graph
extends to a matching covering all vertices of degree at least $2$. As a
consequence of this result, we obtain that Orlin's conjecture holds within the
class of complements of line graphs of bipartite graphs.
  We also connect equistarable graphs to the triangle condition, a
combinatorial condition known to be necessary (but in general not sufficient)
for equistability. We show that the triangle condition implies general
partitionability for complements of line graphs of forests, and construct an
infinite family of triangle non-equistable graphs within the class of
complements of line graphs of bipartite graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06344</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06344</id><created>2015-02-23</created><authors><author><keyname>Brust</keyname><forenames>Clemens-Alexander</forenames></author><author><keyname>Sickert</keyname><forenames>Sven</forenames></author><author><keyname>Simon</keyname><forenames>Marcel</forenames></author><author><keyname>Rodner</keyname><forenames>Erik</forenames></author><author><keyname>Denzler</keyname><forenames>Joachim</forenames></author></authors><title>Convolutional Patch Networks with Spatial Prior for Road Detection and
  Urban Scene Understanding</title><categories>cs.CV</categories><comments>VISAPP 2015 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classifying single image patches is important in many different applications,
such as road detection or scene understanding. In this paper, we present
convolutional patch networks, which are convolutional networks learned to
distinguish different image patches and which can be used for pixel-wise
labeling. We also show how to incorporate spatial information of the patch as
an input to the network, which allows for learning spatial priors for certain
categories jointly with an appearance model. In particular, we focus on road
detection and urban scene understanding, two application areas where we are
able to achieve state-of-the-art results on the KITTI as well as on the
LabelMeFacade dataset.
  Furthermore, our paper offers a guideline for people working in the area and
desperately wandering through all the painstaking details that render training
CNs on image patches extremely difficult.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06354</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06354</id><created>2015-02-23</created><updated>2015-06-10</updated><authors><author><keyname>Neu</keyname><forenames>Gergely</forenames></author></authors><title>First-order regret bounds for combinatorial semi-bandits</title><categories>cs.LG stat.ML</categories><comments>To appear at COLT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of online combinatorial optimization under
semi-bandit feedback, where a learner has to repeatedly pick actions from a
combinatorial decision set in order to minimize the total losses associated
with its decisions. After making each decision, the learner observes the losses
associated with its action, but not other losses. For this problem, there are
several learning algorithms that guarantee that the learner's expected regret
grows as $\widetilde{O}(\sqrt{T})$ with the number of rounds $T$. In this
paper, we propose an algorithm that improves this scaling to
$\widetilde{O}(\sqrt{{L_T^*}})$, where $L_T^*$ is the total loss of the best
action. Our algorithm is among the first to achieve such guarantees in a
partial-feedback scheme, and the first one to do so in a combinatorial setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06359</identifier>
 <datestamp>2015-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06359</id><created>2015-02-23</created><updated>2015-11-25</updated><authors><author><keyname>Amaru</keyname><forenames>Luca</forenames></author><author><keyname>Gaillardon</keyname><forenames>Pierre-Emmanuel</forenames></author><author><keyname>Chattopadhyay</keyname><forenames>Anupam</forenames></author><author><keyname>De Micheli</keyname><forenames>Giovanni</forenames></author></authors><title>A Sound and Complete Axiomatization of Majority-n Logic</title><categories>cs.LO</categories><comments>Accepted by the IEEE Transactions on Computers</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Manipulating logic functions via majority operators recently drew the
attention of researchers in computer science. For example, circuit optimization
based on majority operators enables superior results as compared to traditional
logic systems. Also, the Boolean satisfiability problem finds new solving
approaches when described in terms of majority decisions. To support computer
logic applications based on majority a sound and complete set of axioms is
required. Most of the recent advances in majority logic deal only with ternary
majority (MAJ- 3) operators because the axiomatization with solely MAJ-3 and
complementation operators is well understood. However, it is of interest
extending such axiomatization to n-ary majority operators (MAJ-n) from both the
theoretical and practical perspective. In this work, we address this issue by
introducing a sound and complete axiomatization of MAJ-n logic. Our
axiomatization naturally includes existing majority logic systems. Based on
this general set of axioms, computer applications can now fully exploit the
expressive power of majority logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06360</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06360</id><created>2015-02-23</created><updated>2015-04-13</updated><authors><author><keyname>Bernardi</keyname><forenames>Giovanni</forenames><affiliation>Trinity College Dublin</affiliation></author><author><keyname>Hennessy</keyname><forenames>Matthew</forenames><affiliation>Trinity College Dublin</affiliation></author></authors><title>Mutually Testing Processes</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 11, Issue 2 (April 14,
  2015) lmcs:776</journal-ref><doi>10.2168/LMCS-11(2:1)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the standard testing theory of DeNicola-Hennessy one process is considered
to be a refinement of another if every test guaranteed by the former is also
guaranteed by the latter. In the domain of web services this has been recast,
with processes viewed as servers and tests as clients. In this way the standard
refinement preorder between servers is determined by their ability to satisfy
clients. But in this setting there is also a natural refinement preorder
between clients, determined by their ability to be satisfied by servers. In
more general settings where there is no distinction between clients and
servers, but all processes are peers, there is a further refinement preorder
based on the mutual satisfaction of peers. We give a uniform account of these
three preorders. In particular we give two characterisations. The first is
behavioural, in terms of traces and ready sets. The second, for finite
processes, is equational.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06362</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06362</id><created>2015-02-23</created><updated>2015-06-13</updated><authors><author><keyname>Dud&#xed;k</keyname><forenames>Miroslav</forenames></author><author><keyname>Hofmann</keyname><forenames>Katja</forenames></author><author><keyname>Schapire</keyname><forenames>Robert E.</forenames></author><author><keyname>Slivkins</keyname><forenames>Aleksandrs</forenames></author><author><keyname>Zoghi</keyname><forenames>Masrour</forenames></author></authors><title>Contextual Dueling Bandits</title><categories>cs.LG</categories><comments>25 pages, 4 figures, Published at COLT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of learning to choose actions using contextual
information when provided with limited feedback in the form of relative
pairwise comparisons. We study this problem in the dueling-bandits framework of
Yue et al. (2009), which we extend to incorporate context. Roughly, the
learner's goal is to find the best policy, or way of behaving, in some space of
policies, although &quot;best&quot; is not always so clearly defined. Here, we propose a
new and natural solution concept, rooted in game theory, called a von Neumann
winner, a randomized policy that beats or ties every other policy. We show that
this notion overcomes important limitations of existing solutions, particularly
the Condorcet winner which has typically been used in the past, but which
requires strong and often unrealistic assumptions. We then present three
efficient algorithms for online learning in our setting, and for approximating
a von Neumann winner from batch-like data. The first of these algorithms
achieves particularly low regret, even when data is adversarial, although its
time and space requirements are linear in the size of the policy space. The
other two algorithms require time and space only logarithmic in the size of the
policy space when provided access to an oracle for solving classification
problems on the space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06369</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06369</id><created>2015-02-23</created><authors><author><keyname>Chowdhury</keyname><forenames>Mostafa Zaman</forenames></author><author><keyname>Trung</keyname><forenames>Bui Minh</forenames></author><author><keyname>Jang</keyname><forenames>Yeong Min</forenames></author></authors><title>Neighbor Cell List Optimization for Femtocell-to-Femtocell Handover in
  Dense Femtocellular Networks</title><categories>cs.NI</categories><comments>International Conference on Ubiquitous and Future Networks (ICUFN),
  June 2011, Dalian, China, pp 241-245</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Dense femtocells are the ultimate goal of the femtocellular network
deployment. Among three types of handovers: femtocell-to-macrocell,
macrocell-to-femtocell, and femtocell-to-femtocell, the latter two are the main
concern for the dense femtocellular network deployment. For these handover
cases, minimum as well appropriate neighbor cell list is the key element for
the successful handover. In this paper, we propose an algorithm to make minimum
but appropriate number of neighbor femtocell list for the
femtocell-to-femtocell handover. Our algorithm considers received signal level
from femto APs (FAPs); open and close access cases; and detected frequencyfrom
the neighbor femtocells. The simulation results show that the proposed scheme
is able to attain minimum but optimal number of neighbor femtocell list for the
possible femtocell-to-femtocell handover.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06370</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06370</id><created>2015-02-23</created><authors><author><keyname>Belazzougui</keyname><forenames>Djamal</forenames></author><author><keyname>Cunial</keyname><forenames>Fabio</forenames></author></authors><title>A framework for space-efficient string kernels</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  String kernels are typically used to compare genome-scale sequences whose
length makes alignment impractical, yet their computation is based on data
structures that are either space-inefficient, or incur large slowdowns. We show
that a number of exact string kernels, like the $k$-mer kernel, the substrings
kernels, a number of length-weighted kernels, the minimal absent words kernel,
and kernels with Markovian corrections, can all be computed in $O(nd)$ time and
in $o(n)$ bits of space in addition to the input, using just a
$\mathtt{rangeDistinct}$ data structure on the Burrows-Wheeler transform of the
input strings, which takes $O(d)$ time per element in its output. The same
bounds hold for a number of measures of compositional complexity based on
multiple value of $k$, like the $k$-mer profile and the $k$-th order empirical
entropy, and for calibrating the value of $k$ using the data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06378</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06378</id><created>2015-02-23</created><authors><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>Bibliometrics/Citation networks</title><categories>cs.DL</categories><comments>preprint version of: George A. Barnett (Ed.), The Encyclopedia of
  Social Networks, Sage, 2011, pp. 72-74</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In addition to shaping social networks, for example, in terms of
co-authorship relations, scientific communications induce and reproduce
cognitive structures. Scientific literature is intellectually organized in
terms of disciplines and specialties; these structures are reproduced and
networked reflexively by making references to the authors, concepts and texts
embedded in these literatures. The concept of a cognitive structure was
introduced in social network analysis (SNA) in 1987 by David Krackhardt, but
the focus in SNA has hitherto been on cognition as a psychological attribute of
human agency. In bibliometrics, and in science and technology studies (STS)
more generally, socio-cognitive structures refer to intellectual organization
at the supra-individual level. This intellectual organization emerges and is
reproduced by the collectives of authors who are organized not only in terms of
inter-personal relations, but also more abstractly in terms of codes of
communication that are field-specific. Citations can serve as indicators of
this codification process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06381</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06381</id><created>2015-02-23</created><authors><author><keyname>Zimmermann</keyname><forenames>Markus</forenames></author><author><keyname>collaboration</keyname><forenames>for the ALICE</forenames></author></authors><title>The ALICE analysis train system</title><categories>hep-ex cs.DC</categories><comments>5 pages, 3 figures, proceedings of the conference ACAT 2014 (Advanced
  Computing and Analysis Techniques in physics), Prague, Czech Republic,
  September 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the ALICE experiment hundreds of users are analyzing big datasets on a
Grid system. High throughput and short turn-around times are achieved by a
centralized system called the LEGO trains. This system combines analysis from
different users in so-called analysis trains which are then executed within the
same Grid jobs thereby reducing the number of times the data needs to be read
from the storage systems. The centralized trains improve the performance, the
usability for users and the bookkeeping in comparison to single user analysis.
The train system builds upon the already existing ALICE tools, i.e. the
analysis framework as well as the Grid submission and monitoring
infrastructure. The entry point to the train system is a web interface which is
used to configure the analysis and the desired datasets as well as to test and
submit the train. Several measures have been implemented to reduce the time a
train needs to finish and to increase the CPU efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06388</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06388</id><created>2015-02-23</created><authors><author><keyname>Chowdhury</keyname><forenames>Mostafa Zaman</forenames></author><author><keyname>Jang</keyname><forenames>Yeong Min</forenames></author><author><keyname>Haas</keyname><forenames>andZygmunt J.</forenames></author></authors><title>Call Admission Control based on Adaptive Bandwidth Allocation for
  Multi-Class Services in Wireless Networks</title><categories>cs.NI</categories><comments>International Conference on ICT Convergence (ICTC), Nov. 2010, Jeju,
  Korea,pp. 358-361</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the fact that Quality of Service (QoS) requirements are not as
stringent for non-real-time traffic types, as opposed to real-time traffic,
more calls can be accommodated by releasing some bandwidth from the existing
non-real-time traffic calls. If the released bandwidth to accept a handover
call is larger than to accept a new call, then the probability of dropping a
call is smaller than the probability of blocking a call. In this paper we
propose an efficient Call Admission Control (CAC) that relies on adaptive
multi-level bandwidth-allocation scheme for non-real-time calls. The features
of the scheme allow reduction of the call dropping probability along with the
increase of the bandwidth utilization. The numerical results show that the
proposed scheme is able to attain negligible handover call dropping probability
without sacrificing bandwidth utilization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06392</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06392</id><created>2015-02-23</created><authors><author><keyname>Chowdhury</keyname><forenames>Mostafa Zaman</forenames></author><author><keyname>Choi</keyname><forenames>Sunwoong</forenames></author><author><keyname>Jang</keyname><forenames>Yeong Min</forenames></author><author><keyname>Park</keyname><forenames>Kap-Suk</forenames></author><author><keyname>Yoo</keyname><forenames>Geun Il</forenames></author></authors><title>Dynamic SLA Negotiation using Bandwidth Broker for Femtocell Networks</title><categories>cs.NI</categories><comments>International Conference on Ubiquitous and Future Networks (ICUFN),
  June 2009, Hong Kong, pp 12-15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Satisfaction level of femtocell users' depends on the availability of
requested bandwidth. But the xDSL line that can be used for the backhauling of
femtocell traffic cannot always provide sufficient bandwidth due to the
inequality between the xDSL capacity and demanded bandwidth of home
applications like, IPTV, PC, WiFi, and others. A Service Level Agreement (SLA)
between xDSL and femtocell operator (mobile operator) to reserve some bandwidth
for the upcoming femtocell calls can increase the satisfaction level for
femtocell users. In this paper we propose a SLA negotiation procedure for
femtocell networks. The Bandwidth Broker controls the allocated bandwidth for
femtocell users. Then we propose the dynamically reserve bandwidth scheme to
increase the femtocell user's satisfaction level. Finally, we present our
simulation results to validate the proposed scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06398</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06398</id><created>2015-02-23</created><authors><author><keyname>Bubeck</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Dekel</keyname><forenames>Ofer</forenames></author><author><keyname>Koren</keyname><forenames>Tomer</forenames></author><author><keyname>Peres</keyname><forenames>Yuval</forenames></author></authors><title>Bandit Convex Optimization: sqrt{T} Regret in One Dimension</title><categories>cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the minimax regret of the adversarial bandit convex optimization
problem. Focusing on the one-dimensional case, we prove that the minimax regret
is $\widetilde\Theta(\sqrt{T})$ and partially resolve a decade-old open
problem. Our analysis is non-constructive, as we do not present a concrete
algorithm that attains this regret rate. Instead, we use minimax duality to
reduce the problem to a Bayesian setting, where the convex loss functions are
drawn from a worst-case distribution, and then we solve the Bayesian version of
the problem with a variant of Thompson Sampling. Our analysis features a novel
use of convexity, formalized as a &quot;local-to-global&quot; property of convex
functions, that may be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06419</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06419</id><created>2015-02-14</created><authors><author><keyname>Bhatti</keyname><forenames>Zeeshan</forenames></author><author><keyname>Shah</keyname><forenames>Asadullah</forenames></author><author><keyname>Waqas</keyname><forenames>Ahmad</forenames></author><author><keyname>Mahmood</keyname><forenames>Nadeem</forenames></author></authors><title>Analysis of Design Principles and Requirements for Procedural Rigging of
  Bipeds and Quadrupeds Characters with Custom Manipulators for Animation</title><categories>cs.GR</categories><comments>21 pages, 24 figures, 4 Algorithms, Journal Paper</comments><journal-ref>International Journal of Computer Graphics &amp; Animation (IJCGA)
  Vol.5, No.1, January 2015</journal-ref><doi>10.5121/ijcga.2015.5104</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Character rigging is a process of endowing a character with a set of custom
manipulators and controls making it easy to animate by the animators. These
controls consist of simple joints, handles, or even separate character
selection windows.This research paper present an automated rigging system for
quadruped characters with custom controls and manipulators for animation.The
full character rigging mechanism is procedurally driven based on various
principles and requirements used by the riggers and animators. The automation
is achieved initially by creating widgets according to the character type.
These widgets then can be customized by the rigger according to the character
shape, height and proportion. Then joint locations for each body parts are
calculated and widgets are replaced programmatically.Finally a complete and
fully operational procedurally generated character control rig is created and
attached with the underlying skeletal joints. The functionality and feasibility
of the rig was analyzed from various source of actual character motion and a
requirements criterion was met. The final rigged character provides an
efficient and easy to manipulate control rig with no lagging and at high frame
rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06422</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06422</id><created>2015-02-23</created><authors><author><keyname>Goztepe</keyname><forenames>Kerim</forenames></author></authors><title>Recommendations on Future Operational Environments Command Control and
  Cyber Security</title><categories>cs.CR</categories><doi>10.13140/2.1.2406.1121</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  It is a well-known fact that today a nation's telecommunication networks,
critical infrastructure, and information systems are vulnerable to growing
number of attacks in cyberspace. Cyber space contains very different problems
involving various sets of threats, targets and costs. Cyber security is not
only problem of banking, communication or transportation. It also threatens
core systems of army as command control. Some significant recommendations on
command control (C2) and cyber security have been suggested for army computing
environment in this paper. This study addresses priorities of &quot;what should be
done for a better army cyber future&quot; to cyber security researchers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06428</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06428</id><created>2015-02-23</created><updated>2015-03-10</updated><authors><author><keyname>Sason</keyname><forenames>Igal</forenames></author></authors><title>Tight Bounds for Symmetric Divergence Measures and a New Inequality
  Relating $f$-Divergences</title><categories>cs.IT math.IT</categories><comments>A final version of the conference paper at the 2015 IEEE Information
  Theory Workshop, Jerusalem, Israel</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tight bounds for several symmetric divergence measures are introduced, given
in terms of the total variation distance. Each of these bounds is attained by a
pair of 2 or 3-element probability distributions. An application of these
bounds for lossless source coding is provided, refining and improving a certain
bound by Csisz\'ar. A new inequality relating $f$-divergences is derived, and
its use is exemplified. The last section of this conference paper is not
included in the recent journal paper that was published in the February 2015
issue of the IEEE Trans. on Information Theory (see arXiv:1403.7164), as well
as some new paragraphs throughout the paper which are linked to new references.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06430</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06430</id><created>2015-02-23</created><updated>2015-09-02</updated><authors><author><keyname>Giardini</keyname><forenames>Francesca</forenames></author><author><keyname>Vilone</keyname><forenames>Daniele</forenames></author><author><keyname>Conte</keyname><forenames>Rosaria</forenames></author></authors><title>Consensus Emerging from the Bottom-up: the Role of Cognitive Variables
  in Opinion Dynamics</title><categories>physics.soc-ph cs.SI nlin.AO</categories><comments>14 pages, 8 figures</comments><journal-ref>Frontiers in Physics, 3, 00064 (2015)</journal-ref><doi>10.3389/fphy.2015.00064</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study of opinions $-$ e.g., their formation and change, and their effects
on our society $-$ by means of theoretical and numerical models has been one of
the main goals of sociophysics until now, but it is one of the defining topics
addressed by social psychology and complexity science. Despite the flourishing
of different models and theories, several key questions still remain
unanswered. The aim of this paper is to provide a cognitively grounded
computational model of opinions in which they are described as mental
representations and defined in terms of distinctive mental features. We also
define how these representations change dynamically through different
processes, describing the interplay between mental and social dynamics of
opinions. We present two versions of the model, one with discrete opinions
(voter model-like), and one with continuous ones (Deffuant-like). By means of
numerical simulations, we compare the behaviour of our cognitive model with the
classical sociophysical models, and we identify interesting differences in the
dynamics of consensus for each of the models considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06434</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06434</id><created>2014-12-17</created><authors><author><keyname>Wanjawa</keyname><forenames>B. W.</forenames></author><author><keyname>Muchemi</keyname><forenames>L.</forenames></author></authors><title>ANN Model to Predict Stock Prices at Stock Exchange Markets</title><categories>q-fin.ST cs.CE cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stock exchanges are considered major players in financial sectors of many
countries. Most Stockbrokers, who execute stock trade, use technical,
fundamental or time series analysis in trying to predict stock prices, so as to
advise clients. However, these strategies do not usually guarantee good returns
because they guide on trends and not the most likely price. It is therefore
necessary to explore improved methods of prediction.
  The research proposes the use of Artificial Neural Network that is
feedforward multi-layer perceptron with error backpropagation and develops a
model of configuration 5:21:21:1 with 80% training data in 130,000 cycles. The
research develops a prototype and tests it on 2008-2012 data from stock markets
e.g. Nairobi Securities Exchange and New York Stock Exchange, where prediction
results show MAPE of between 0.71% and 2.77%. Validation done with Encog and
Neuroph realized comparable results. The model is thus capable of prediction on
typical stock markets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06435</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06435</id><created>2015-02-23</created><updated>2015-08-04</updated><authors><author><keyname>Vila</keyname><forenames>Jeremy</forenames></author><author><keyname>Schniter</keyname><forenames>Philip</forenames></author><author><keyname>Meola</keyname><forenames>Joseph</forenames></author></authors><title>Hyperspectral Unmixing via Turbo Bilinear Approximate Message Passing</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of hyperspectral unmixing is to decompose an electromagnetic
spectral dataset measured over M spectral bands and T pixels into N constituent
material spectra (or &quot;end-members&quot;) with corresponding spatial abundances. In
this paper, we propose a novel approach to hyperspectral unmixing based on
loopy belief propagation (BP) that enables the exploitation of spectral
coherence in the endmembers and spatial coherence in the abundances. In
particular, we partition the factor graph into spectral coherence, spatial
coherence, and bilinear subgraphs, and pass messages between them using a
&quot;turbo&quot; approach. To perform message passing within the bilinear subgraph, we
employ the bilinear generalized approximate message passing algorithm
(BiG-AMP), a recently proposed belief-propagation-based approach to matrix
factorization. Furthermore, we propose an expectation-maximization (EM)
strategy to tune the prior parameters and a model-order selection strategy to
select the number of materials N. Numerical experiments conducted with both
synthetic and real-world data show favorable unmixing performance relative to
existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06455</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06455</id><created>2015-02-23</created><updated>2015-03-31</updated><authors><author><keyname>Yousefi</keyname><forenames>Mansoor I.</forenames></author><author><keyname>Kramer</keyname><forenames>Gerhard</forenames></author><author><keyname>Kschischang</keyname><forenames>Frank R.</forenames></author></authors><title>Upper Bound on the Capacity of the Nonlinear Schr\&quot;odinger Channel</title><categories>cs.IT math.IT</categories><comments>To be presented at the 14th Canadian Workshop on Information Theory
  (CWIT), St. John's, NL, Canada, July 6-9, 2015. This is the final version
  submitted to the CWIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is shown that the capacity of the channel modeled by (a discretized
version of) the stochastic nonlinear Schr\&quot;odinger (NLS) equation is
upper-bounded by $\log(1+\text{SNR})$ with $\text{SNR}=\mathcal
P_0/\sigma^2(z)$, where $\mathcal P_0$ is the average input signal power and
$\sigma^2(z)$ is the total noise power up to distance $z$. The result is a
consequence of the fact that the deterministic NLS equation is a Hamiltonian
energy-preserving dynamical system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06460</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06460</id><created>2015-02-23</created><updated>2015-02-24</updated><authors><author><keyname>Tonejc</keyname><forenames>Jernej</forenames></author><author><keyname>Kaur</keyname><forenames>Jaspreet</forenames></author><author><keyname>Karsten</keyname><forenames>Adrian</forenames></author><author><keyname>Wendzel</keyname><forenames>Steffen</forenames></author></authors><title>Visualizing BACnet data to facilitate humans in building-security
  decision-making</title><categories>cs.CR</categories><comments>12 pages, 5 figures, 2 tables, presented at HCI International 2015,
  Los Angeles, CA, USA, 2-7 August 2015. Updated lab figure</comments><acm-class>C.2.2; C.2.3; K.6.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Building automation systems (BAS) are interlinked networks of hardware and
software, which monitor and control events in the buildings. One of the data
communication protocols used in BAS is Building Automation and Control
networking protocol (BACnet) which is an internationally adopted ISO standard
for the communication between BAS devices. Although BAS focus on providing
safety for inhabitants, decreasing the energy consumption of buildings and
reducing their operational cost, their security suffers due to the inherent
complexity of the modern day systems. The issues such as monitoring of BAS
effectively present a significant challenge, i.e., BAS operators generally
possess only partial situation awareness. Especially in large and
inter-connected buildings, the operators face the challenge of spotting
meaningful incidents within large amounts of simultaneously occurring events,
causing the anomalies in the BAS network to go unobserved. In this paper, we
present the techniques to analyze and visualize the data for several events
from BAS devices in a way that determines the potential importance of such
unusual events and helps with the building-security decision making. We
implemented these techniques as a mobile (Android) based application for
displaying application data and as tools to analyze the communication flows
using directed graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06461</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06461</id><created>2015-02-23</created><updated>2015-10-06</updated><authors><author><keyname>Zave</keyname><forenames>Pamela</forenames></author></authors><title>How to Make Chord Correct</title><categories>cs.DC</categories><comments>5 figures; 11 two-column pages including references and appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Chord distributed hash table (DHT) is well-known and frequently used to
implement peer-to-peer systems. Chord peers find other peers, and access their
data, through a ring-shaped pointer structure in a large identifier space.
Despite claims of proven correctness, i.e., eventual reachability, previous
work has shown that the Chord ring-maintenance protocol is not correct under
its original operating assumptions. It has not, however, discovered whether
Chord could be made correct with reasonable operating assumptions. The
contribution of this paper is to provide the first specification of correct
operations and initialization for Chord, an inductive invariant that is
necessary and sufficient to support a proof of correctness, and the proof
itself. Most of the proof is carried out by automated analysis of an Alloy
model. The inductive invariant reflects the fact that a Chord network must have
a minimum ring size (the minimum being the length of successor lists plus one)
to be correct. The invariant relies on an assumption that there is a stable
base, of the minimum size, of permanent ring members. Because a stable base has
only a few members and a Chord network can have millions, we learn that the
obstacles to provable correctness are anomalies in small networks, and that a
stable base need not be maintained once a Chord network grows large.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06464</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06464</id><created>2015-02-23</created><updated>2015-06-11</updated><authors><author><keyname>Clevert</keyname><forenames>Djork-Arn&#xe9;</forenames></author><author><keyname>Mayr</keyname><forenames>Andreas</forenames></author><author><keyname>Unterthiner</keyname><forenames>Thomas</forenames></author><author><keyname>Hochreiter</keyname><forenames>Sepp</forenames></author></authors><title>Rectified Factor Networks</title><categories>cs.LG cs.CV cs.NE stat.ML</categories><comments>9 pages + 49 pages supplement</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose rectified factor networks (RFNs) to efficiently construct very
sparse, non-linear, high-dimensional representations of the input. RFN models
identify rare and small events in the input, have a low interference between
code units, have a small reconstruction error, and explain the data covariance
structure. RFN learning is a generalized alternating minimization algorithm
derived from the posterior regularization method which enforces non-negative
and normalized posterior means. We proof convergence and correctness of the RFN
learning algorithm. On benchmarks, RFNs are compared to other unsupervised
methods like autoencoders, RBMs, factor analysis, ICA, and PCA. In contrast to
previous sparse coding methods, RFNs yield sparser codes, capture the data's
covariance structure more precisely, and have a significantly smaller
reconstruction error. We test RFNs as pretraining technique for deep networks
on different vision datasets, where RFNs were superior to RBMs and
autoencoders. On gene expression data from two pharmaceutical drug discovery
studies, RFNs detected small and rare gene modules that revealed highly
relevant new biological insights which were so far missed by other unsupervised
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06470</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06470</id><created>2015-02-23</created><updated>2015-12-09</updated><authors><author><keyname>Tramel</keyname><forenames>Eric W.</forenames></author><author><keyname>Dr&#xe9;meau</keyname><forenames>Ang&#xe9;lique</forenames></author><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author></authors><title>Approximate Message Passing with Restricted Boltzmann Machine Priors</title><categories>cs.IT cond-mat.dis-nn math.IT physics.data-an stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Approximate Message Passing (AMP) has been shown to be an excellent
statistical approach to signal inference and compressed sensing problem. The
AMP framework provides modularity in the choice of signal prior; here we
propose a hierarchical form of the Gauss-Bernouilli prior which utilizes a
Restricted Boltzmann Machine (RBM) trained on the signal support to push
reconstruction performance beyond that of simple iid priors for signals whose
support can be well represented by a trained binary RBM. We present and analyze
two methods of RBM factorization and demonstrate how these affect signal
reconstruction performance within our proposed algorithm. Finally, using the
MNIST handwritten digit dataset, we show experimentally that using an RBM
allows AMP to approach oracle-support performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06471</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06471</id><created>2015-02-23</created><updated>2015-03-27</updated><authors><author><keyname>Taati</keyname><forenames>Siamak</forenames></author></authors><title>Restricted density classification in one dimension</title><categories>math.PR cs.DC nlin.CG</categories><comments>13 pages, 5 figures</comments><msc-class>60K35, 82B26, 37B15, 68Q80, 82C43, 68Q87</msc-class><acm-class>F.1.1; F.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The density classification task is to determine which of the symbols
appearing in an array has the majority. A cellular automaton solving this task
is required to converge to a uniform configuration with the majority symbol at
each site. It is not known whether a one-dimensional cellular automaton with
binary alphabet can classify all Bernoulli random configurations almost surely
according to their densities. We show that any cellular automaton that washes
out finite islands in linear time classifies all Bernoulli random
configurations with parameters close to 0 or 1 almost surely correctly. The
proof is a direct application of a &quot;percolation&quot; argument which goes back to
Gacs (1986).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06491</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06491</id><created>2015-02-23</created><authors><author><keyname>Forney</keyname><forenames>G. David</forenames><suffix>Jr</suffix></author></authors><title>Unique Factorization and Controllability of Tail-Biting Trellis
  Realizations via Controller Granule Decompositions</title><categories>cs.IT cs.SY math.IT</categories><comments>5 pages, 2 figures. To be presented at the IEEE Information Theory
  Workshop, Jerusalem, April 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Conti-Boston factorization theorem (CBFT) for linear tail-biting trellis
realizations is extended to group realizations with a new and simpler proof,
based on a controller granule decomposition of the behavior and known
controllability results for group realizations. Further controllability results
are given; e.g., a trellis realization is controllable if and only if its top
(controllability) granule is trivial.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06492</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06492</id><created>2015-02-23</created><authors><author><keyname>Thomas</keyname><forenames>Brian</forenames></author><author><keyname>Shaya</keyname><forenames>Edward</forenames></author></authors><title>A User Interface for Semantically Oriented Data Mining of Astronomy
  Repositories</title><categories>astro-ph.IM cs.HC</categories><comments>ADASS ASP Conference Series, Vol. 394, Proceedings of the conference
  held 23-26 September, 2007, in Kensington Town Hall, London, United Kingdom.
  Edited by Robert W. Argyle, Peter S. Bunclark, and James R. Lewis., p.361</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a user-friendly, but powerful interface for the data mining of
scientific repositories. We present the tool in use with actual astronomy data
and show how it may be used to achieve many different types of powerful
semantic queries. The tool itself hides the gory details of query formulation,
and data retrieval from the user, and allows the user to create workflows which
may be used to transform the data into a convenient form.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06501</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06501</id><created>2015-02-23</created><authors><author><keyname>Thomas</keyname><forenames>Brian</forenames></author><author><keyname>Shaya</keyname><forenames>Edward</forenames></author><author><keyname>Huang</keyname><forenames>Zenping</forenames></author><author><keyname>Teuben</keyname><forenames>Peter</forenames></author></authors><title>Knowledge Discovery Framework for the Virtual Observatory</title><categories>astro-ph.IM cs.HC</categories><comments>ADASS XVI ASP Conference Series, Vol. 376, proceedings of the
  conference held 15-18 October 2006 in Tucson, Arizona, USA. Edited by Richard
  A. Shaw, Frank Hill and David J. Bell., p.563</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a framework that allows a scientist-user to easily query for
information across all Virtual Observatory (VO) repositories and pull it back
for analysis. This framework hides the gory details of meta-data remediation
and data formatting from the user, allowing them to get on with search,
retrieval and analysis of VO data as if they were drawn from a single source
using a science based terminology rather than a data-centric one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06512</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06512</id><created>2015-02-23</created><authors><author><keyname>Yampolskiy</keyname><forenames>Roman V.</forenames></author></authors><title>From Seed AI to Technological Singularity via Recursively Self-Improving
  Software</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software capable of improving itself has been a dream of computer scientists
since the inception of the field. In this work we provide definitions for
Recursively Self-Improving software, survey different types of self-improving
software, review the relevant literature, analyze limits on computation
restricting recursive self-improvement and introduce RSI Convergence Theory
which aims to predict general behavior of RSI systems. Finally, we address
security implications from self-improving intelligent software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06519</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06519</id><created>2015-02-23</created><authors><author><keyname>Cai</keyname><forenames>Haipeng</forenames></author></authors><title>Enhancing Programming Interface to Effectively Meet Multiple Information
  Needs of Developers</title><categories>cs.SE cs.HC</categories><comments>Position paper; 10 pages, 2 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In the past decades, integrated development environments (IDEs) have been
largely advanced to facilitate common software engineering tasks. Yet, with
growing information needs driven by increasing complexity in developing modern
high-quality software, developers often need to switch among multiple user
interfaces, even across different applications, in their development process,
which breaks their mental workflow thus tends to adversely affect their working
efficiency and productivity. This position paper discusses challenges faced by
current IDE designs mainly from working context transitions of developers
during the process of seeking multiple information needs for their development
tasks. It remarks the primary blockades behind and initially explores some
high-level design considerations for overcoming such challenges in the
next-generation IDEs. Specifically, a few design enhancements on top of modern
IDEs are envisioned, attempting to reduce the overheads of frequent context
switching commonly seen in the multitasking of developers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06528</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06528</id><created>2015-02-23</created><authors><author><keyname>Boutsidis</keyname><forenames>Christos</forenames></author><author><keyname>Liberty</keyname><forenames>Edo</forenames></author><author><keyname>Sviridenko</keyname><forenames>Maxim</forenames></author></authors><title>Greedy Minimization of Weakly Supermodular Set Functions</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper defines weak-$\alpha$-supermodularity for set functions. Many
optimization objectives in machine learning and data mining seek to minimize
such functions under cardinality constrains. We prove that such problems
benefit from a greedy extension phase. Explicitly, let $S^*$ be the optimal set
of cardinality $k$ that minimizes $f$ and let $S_0$ be an initial solution such
that $f(S_0)/f(S^*) \le \rho$. Then, a greedy extension $S \supset S_0$ of size
$|S| \le |S_0| + \lceil \alpha k \ln(\rho/\varepsilon) \rceil$ yields
$f(S)/f(S^*) \le 1+\varepsilon$. As example usages of this framework we give
new bicriteria results for $k$-means, sparse regression, and columns subset
selection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06531</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06531</id><created>2015-02-23</created><updated>2015-02-24</updated><authors><author><keyname>Djolonga</keyname><forenames>Josip</forenames></author><author><keyname>Krause</keyname><forenames>Andreas</forenames></author></authors><title>Scalable Variational Inference in Log-supermodular Models</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of approximate Bayesian inference in log-supermodular
models. These models encompass regular pairwise MRFs with binary variables, but
allow to capture high-order interactions, which are intractable for existing
approximate inference techniques such as belief propagation, mean field, and
variants. We show that a recently proposed variational approach to inference in
log-supermodular models -L-FIELD- reduces to the widely-studied minimum norm
problem for submodular minimization. This insight allows to leverage powerful
existing tools, and hence to solve the variational problem orders of magnitude
more efficiently than previously possible. We then provide another natural
interpretation of L-FIELD, demonstrating that it exactly minimizes a specific
type of R\'enyi divergence measure. This insight sheds light on the nature of
the variational approximations produced by L-FIELD. Furthermore, we show how to
perform parallel inference as message passing in a suitable factor graph at a
linear convergence rate, without having to sum up over all the configurations
of the factor. Finally, we apply our approach to a challenging image
segmentation task. Our experiments confirm scalability of our approach, high
quality of the marginals, and the benefit of incorporating higher-order
potentials.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06556</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06556</id><created>2015-02-23</created><authors><author><keyname>Sparavigna</keyname><forenames>Amelia Carolina</forenames></author></authors><title>Shannon, Tsallis and Kaniadakis entropies in bi-level image thresholding</title><categories>cs.CV</categories><comments>Keywords: Kaniadakis Entropy, Image Processing, Image Segmentation,
  Image Thresholding, Texture Transitions</comments><journal-ref>International Journal of Sciences 4(2), 35-43, 2015</journal-ref><doi>10.18483/ijSci.626</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The maximum entropy principle is often used for bi-level or multi-level
thresholding of images. For this purpose, some methods are available based on
Shannon and Tsallis entropies. In this paper, we discuss them and propose a
method based on Kaniadakis entropy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06564</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06564</id><created>2014-12-05</created><authors><author><keyname>Isea</keyname><forenames>Raul</forenames></author><author><keyname>Montes</keyname><forenames>Esther</forenames></author><author><keyname>Rubio-Montero</keyname><forenames>Antonio J.</forenames></author><author><keyname>Mayo</keyname><forenames>Rafael</forenames></author></authors><title>Challenges and characterization of a Biological system on Grid by means
  of the PhyloGrid application</title><categories>cs.CE cs.DC q-bio.QM</categories><comments>8 pages, 3 figures, appears in Proceedings of the First EELA-2
  Conference, 2009</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this work we present a new application that is being developed. PhyloGrid
is able to perform large-scale phylogenetic calculations as those that have
been made for estimating the phylogeny of all the sequences already stored in
the public NCBI database. The further analysis has been focused on checking the
origin of the HIV-1 disease by means of a huge number of sequences that sum up
to 2900 taxa. Such a study has been able to be done by the implementation of a
workflow in Taverna.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06577</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06577</id><created>2015-02-23</created><authors><author><keyname>Book</keyname><forenames>Theodore</forenames></author><author><keyname>Wallach</keyname><forenames>Dan S.</forenames></author></authors><title>An Empirical Study of Mobile Ad Targeting</title><categories>cs.CR cs.CY</categories><comments>Submitted to USENIX Security 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advertising, long the financial mainstay of the web ecosystem, has become
nearly ubiquitous in the world of mobile apps. While ad targeting on the web is
fairly well understood, mobile ad targeting is much less studied. In this
paper, we use empirical methods to collect a database of over 225,000 ads on 32
simulated devices hosting one of three distinct user profiles. We then analyze
how the ads are targeted by correlating ads to potential targeting profiles
using Bayes' rule and Pearson's chi squared test. This enables us to measure
the prevalence of different forms of targeting. We find that nearly all ads
show the effects of application- and time-based targeting, while we are able to
identify location-based targeting in 43% of the ads and user-based targeting in
39%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06583</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06583</id><created>2015-02-23</created><authors><author><keyname>Ranganath</keyname><forenames>Suhas</forenames></author><author><keyname>Tang</keyname><forenames>Jiliang</forenames></author><author><keyname>Hu</keyname><forenames>Xia</forenames></author><author><keyname>Sundaram</keyname><forenames>Hari</forenames></author><author><keyname>Liu</keyname><forenames>Huan</forenames></author></authors><title>Leveraging Social Foci for Information Seeking in Social Media</title><categories>cs.SI</categories><comments>AAAI 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rise of social media provides a great opportunity for people to reach out
to their social connections to satisfy their information needs. However,
generic social media platforms are not explicitly designed to assist
information seeking of users. In this paper, we propose a novel framework to
identify the social connections of a user able to satisfy his information
needs. The information need of a social media user is subjective and personal,
and we investigate the utility of his social context to identify people able to
satisfy it. We present questions users post on Twitter as instances of
information seeking activities in social media. We infer soft community
memberships of the asker and his social connections by integrating network and
content information. Drawing concepts from the social foci theory, we identify
answerers who share communities with the asker w.r.t. the question. Our
experiments demonstrate that the framework is effective in identifying
answerers to social media questions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06590</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06590</id><created>2015-02-23</created><authors><author><keyname>Deshpande</keyname><forenames>Yash</forenames></author><author><keyname>Montanari</keyname><forenames>Andrea</forenames></author></authors><title>Improved Sum-of-Squares Lower Bounds for Hidden Clique and Hidden
  Submatrix Problems</title><categories>cs.CC cs.IT math.IT math.ST stat.ML stat.TH</categories><comments>40 pages, 1 table, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a large data matrix $A\in\mathbb{R}^{n\times n}$, we consider the
problem of determining whether its entries are i.i.d. with some known marginal
distribution $A_{ij}\sim P_0$, or instead $A$ contains a principal submatrix
$A_{{\sf Q},{\sf Q}}$ whose entries have marginal distribution $A_{ij}\sim
P_1\neq P_0$. As a special case, the hidden (or planted) clique problem
requires to find a planted clique in an otherwise uniformly random graph.
  Assuming unbounded computational resources, this hypothesis testing problem
is statistically solvable provided $|{\sf Q}|\ge C \log n$ for a suitable
constant $C$. However, despite substantial effort, no polynomial time algorithm
is known that succeeds with high probability when $|{\sf Q}| = o(\sqrt{n})$.
Recently Meka and Wigderson \cite{meka2013association}, proposed a method to
establish lower bounds within the Sum of Squares (SOS) semidefinite hierarchy.
  Here we consider the degree-$4$ SOS relaxation, and study the construction of
\cite{meka2013association} to prove that SOS fails unless $k\ge C\,
n^{1/3}/\log n$. An argument presented by Barak implies that this lower bound
cannot be substantially improved unless the witness construction is changed in
the proof. Our proof uses the moments method to bound the spectrum of a certain
random association scheme, i.e. a symmetric random matrix whose rows and
columns are indexed by the edges of an Erd\&quot;os-Renyi random graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06593</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06593</id><created>2015-02-23</created><updated>2016-01-12</updated><authors><author><keyname>Galanis</keyname><forenames>Andreas</forenames></author><author><keyname>Stefankovic</keyname><forenames>Daniel</forenames></author><author><keyname>Vigoda</keyname><forenames>Eric</forenames></author></authors><title>Swendsen-Wang Algorithm on the Mean-Field Potts Model</title><categories>cs.DM cond-mat.stat-mech math-ph math.MP math.PR</categories><comments>The current version refines the main result of the paper (tight upper
  bound for Item (i) and matching lower bounds for Items (ii) and (iv))</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the $q$-state ferromagnetic Potts model on the $n$-vertex complete
graph known as the mean-field (Curie-Weiss) model. We analyze the Swendsen-Wang
algorithm which is a Markov chain that utilizes the random cluster
representation for the ferromagnetic Potts model to recolor large sets of
vertices in one step and potentially overcomes obstacles that inhibit
single-site Glauber dynamics. The case $q=2$ (the Swendsen-Wang algorithm for
the ferromagnetic Ising model) undergoes a slow-down at the
uniqueness/non-uniqueness critical temperature for the infinite
$\Delta$-regular tree (Long et al., 2014) but yet still has polynomial mixing
time at all (inverse) temperatures $\beta&gt;0$ (Cooper et al., 2000). In contrast
for $q\geq 3$ there are two critical temperatures
$0&lt;\beta_{\mathrm{u}}&lt;\beta_{\mathrm{rc}}$ that are relevant, these two
critical points relate to phase transitions in the infinite tree. We prove that
the mixing time of the Swendsen-Wang algorithm for the ferromagnetic Potts
model on the $n$-vertex complete graph satisfies: (i) $O(1)$ for
$\beta&lt;\beta_{\mathrm{u}}$, (ii) $\Theta(n^{1/3})$ for
$\beta=\beta_{\mathrm{u}}$, (iii) $\exp(n^{\Omega(1)})$ for
$\beta_{\mathrm{u}}&lt;\beta&lt;\beta_{\mathrm{rc}}$, and (iv) $\Theta(\log{n})$ for
$\beta\geq\beta_{\mathrm{rc}}$. These results complement refined results of
Cuff et al. (2012) on the mixing time of the Glauber dynamics for the
ferromagnetic Potts model. The most interesting aspect of our analysis is at
the critical temperature $\beta=\beta_{\mathrm{u}}$, which requires a delicate
choice of a potential function to balance the conflating factors for the slow
drift away from a fixed point (which is repulsive but not Jacobian repulsive):
close to the fixed point the variance from the percolation step dominates and
sufficiently far from the fixed point the dynamics of the size of the dominant
color class takes over.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06601</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06601</id><created>2015-02-23</created><updated>2015-02-27</updated><authors><author><keyname>Cui</keyname><forenames>Ying</forenames></author><author><keyname>M&#xe9;dard</keyname><forenames>Muriel</forenames></author><author><keyname>Yeh</keyname><forenames>Edmund</forenames></author><author><keyname>Leith</keyname><forenames>Douglas</forenames></author><author><keyname>Duffy</keyname><forenames>Ken</forenames></author></authors><title>Optimization-Based Linear Network Coding for General Connections of
  Continuous Flows</title><categories>cs.IT math.IT</categories><comments>1 fig, technical report of ICC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For general connections, the problem of finding network codes and optimizing
resources for those codes is intrinsically difficult and little is known about
its complexity. Most of the existing solutions rely on very restricted classes
of network codes in terms of the number of flows allowed to be coded together,
and are not entirely distributed. In this paper, we consider a new method for
constructing linear network codes for general connections of continuous flows
to minimize the total cost of edge use based on mixing. We first formulate the
minimumcost network coding design problem. To solve the optimization problem,
we propose two equivalent alternative formulations with discrete mixing and
continuous mixing, respectively, and develop distributed algorithms to solve
them. Our approach allows fairly general coding across flows and guarantees no
greater cost than any solution without network coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06626</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06626</id><created>2015-02-23</created><authors><author><keyname>Magdon-Ismail</keyname><forenames>Malik</forenames></author><author><keyname>Boutsidis</keyname><forenames>Christos</forenames></author></authors><title>Optimal Sparse Linear Auto-Encoders and Sparse PCA</title><categories>cs.LG cs.AI cs.IT math.IT stat.CO stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Principal components analysis (PCA) is the optimal linear auto-encoder of
data, and it is often used to construct features. Enforcing sparsity on the
principal components can promote better generalization, while improving the
interpretability of the features. We study the problem of constructing optimal
sparse linear auto-encoders. Two natural questions in such a setting are: i)
Given a level of sparsity, what is the best approximation to PCA that can be
achieved? ii) Are there low-order polynomial-time algorithms which can
asymptotically achieve this optimal tradeoff between the sparsity and the
approximation quality?
  In this work, we answer both questions by giving efficient low-order
polynomial-time algorithms for constructing asymptotically \emph{optimal}
linear auto-encoders (in particular, sparse features with near-PCA
reconstruction error) and demonstrate the performance of our algorithms on real
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06631</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06631</id><created>2015-02-23</created><authors><author><keyname>Ivanyos</keyname><forenames>Gabor</forenames></author><author><keyname>Karpinski</keyname><forenames>Marek</forenames></author><author><keyname>Santha</keyname><forenames>Miklos</forenames></author><author><keyname>Saxena</keyname><forenames>Nitin</forenames></author><author><keyname>Shparlinski</keyname><forenames>Igor</forenames></author></authors><title>Polynomial Interpolation and Identity Testing from High Powers over
  Finite Fields</title><categories>math.NT cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of recovering (that is, interpolating) and identity
testing of a &quot;hidden&quot; monic polynomial $f$, given an oracle access to $f(x)^e$
for $x\in{\mathbb F_q}$ (extension fields access is not permitted). The naive
interpolation algorithm needs $O(e\, \mathrm{deg}\, f)$ queries and thus
requires $e\, \mathrm{deg}\, f&lt;q$. We design algorithms that are asymptotically
better in certain cases; requiring only $e^{o(1)}$ queries to the oracle. In
the randomized (and quantum) setting, we give a substantially better
interpolation algorithm, that requires only $O(\mathrm{deg}\, f \log q)$
queries. Such results have been known before only for the special case of a
linear $f$, called the hidden shifted power problem.
  We use techniques from algebra, such as effective versions of Hilbert's
Nullstellensatz, and analytic number theory, such as results on the
distribution of rational functions in subgroups and character sum estimates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06641</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06641</id><created>2015-02-23</created><authors><author><keyname>Mourad</keyname><forenames>Bousaaid</forenames></author><author><keyname>Tarik</keyname><forenames>Ayaou</forenames></author><author><keyname>Karim</keyname><forenames>Afdel</forenames></author><author><keyname>Pascal</keyname><forenames>Estraillier</forenames></author></authors><title>System Interactive Cyber Presence for E learning to Break Down Learner
  Isolation</title><categories>cs.CY</categories><doi>10.5120/19626-1544</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  The development of technologies of multimedia, linked to that of Internet and
democratization of high speed, has made henceforth E-learning possible for
learners being in virtual classes and geographically distributed. One benefit
to taking course online is that the online course structure is typically more
student focused than teacher centered and encouraging more active participation
by students in collaborative learning activities. The quality and quantity of
asynchronous and synchronous communications are the key elements for E-learning
success. A potential problem that has received little exploration is student's
feeling of isolation. It is important to have a propitious supervision to
breaking down learner feeling isolation in E learning environment. This feeling
of isolation is among the main causes of loss and high rates of dropout in
E-learning. It impacts on their levels of participation, satisfaction and
learning. To overcome this feeling of isolation, we aim, by this research, to
provide the trainer and each learner with an environment allowing them to
behave as if being face to face; in other words, to approach the pedagogy of
classroom teaching. Our contribution to reduce the feeling of isolation is to
ensure the presence of the teacher in the educational tools. These tools aim to
establish a real dialogue with the learner, forcing him to take an active part
in their learning. Among the tools we offer, video conference Openmeeting
integrated in Moodle providing the possibility of using the notion of class and
whiteboard, the indicator of motivation quantification tool based hand gesture
that we developed and finally social networks web 2. 0 like Facebook, youtube,
twitter to promote collaboration, sharing and communication of the learner with
his peers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06644</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06644</id><created>2015-02-23</created><authors><author><keyname>Vandermeulen</keyname><forenames>Robert A.</forenames></author><author><keyname>Scott</keyname><forenames>Clayton D.</forenames></author></authors><title>On The Identifiability of Mixture Models from Grouped Samples</title><categories>stat.ML cs.LG math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finite mixture models are statistical models which appear in many problems in
statistics and machine learning. In such models it is assumed that data are
drawn from random probability measures, called mixture components, which are
themselves drawn from a probability measure P over probability measures. When
estimating mixture models, it is common to make assumptions on the mixture
components, such as parametric assumptions. In this paper, we make no
assumption on the mixture components, and instead assume that observations from
the mixture model are grouped, such that observations in the same group are
known to be drawn from the same component. We show that any mixture of m
probability measures can be uniquely identified provided there are 2m-1
observations per group. Moreover we show that, for any m, there exists a
mixture of m probability measures that cannot be uniquely identified when
groups have 2m-2 observations. Our results hold for any sample space with more
than one element.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06648</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06648</id><created>2015-02-23</created><updated>2015-10-15</updated><authors><author><keyname>Rohrbach</keyname><forenames>Marcus</forenames></author><author><keyname>Rohrbach</keyname><forenames>Anna</forenames></author><author><keyname>Regneri</keyname><forenames>Michaela</forenames></author><author><keyname>Amin</keyname><forenames>Sikandar</forenames></author><author><keyname>Andriluka</keyname><forenames>Mykhaylo</forenames></author><author><keyname>Pinkal</keyname><forenames>Manfred</forenames></author><author><keyname>Schiele</keyname><forenames>Bernt</forenames></author></authors><title>Recognizing Fine-Grained and Composite Activities using Hand-Centric
  Features and Script Data</title><categories>cs.CV</categories><comments>in International Journal of Computer Vision (IJCV) 2015</comments><doi>10.1007/s11263-015-0851-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Activity recognition has shown impressive progress in recent years. However,
the challenges of detecting fine-grained activities and understanding how they
are combined into composite activities have been largely overlooked. In this
work we approach both tasks and present a dataset which provides detailed
annotations to address them. The first challenge is to detect fine-grained
activities, which are defined by low inter-class variability and are typically
characterized by fine-grained body motions. We explore how human pose and hands
can help to approach this challenge by comparing two pose-based and two
hand-centric features with state-of-the-art holistic features. To attack the
second challenge, recognizing composite activities, we leverage the fact that
these activities are compositional and that the essential components of the
activities can be obtained from textual descriptions or scripts. We show the
benefits of our hand-centric approach for fine-grained activity classification
and detection. For composite activity recognition we find that decomposition
into attributes allows sharing information across composites and is essential
to attack this hard task. Using script data we can recognize novel composites
without having training data for them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06654</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06654</id><created>2015-02-23</created><authors><author><keyname>Kim</keyname><forenames>Seong Hwan</forenames></author><author><keyname>Sung</keyname><forenames>Dan Keun</forenames></author><author><keyname>Le-Ngoc</keyname><forenames>Tho</forenames></author></authors><title>Variable-Length Feedback Codes under a Strict Delay Constraint</title><categories>cs.IT math.IT</categories><comments>5pages, 1 figure, Accepted for publication in IEEE Communications
  Letters</comments><doi>10.1109/LCOMM.2015.2398866</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study variable-length feedback (VLF) codes under a strict delay constraint
to maximize their average transmission rate (ATR) in a discrete memoryless
channel (DMC) while considering periodic decoding attempts. We first derive a
lower bound on the maximum achievable ATR, and confirm that the VLF code can
outperform non-feedback codes with a larger delay constraint. We show that for
a given decoding period, as the strict delay constraint, L, increases, the gap
between the ATR of the VLF code and the DMC capacity scales at most on the
order of O(L^{-1}) instead of O(L^{-1/2}) for non-feedback codes as shown in
Polyanskiy et al. [&quot;Channel coding rate in the finite blocklengh regime,&quot; IEEE
Trans. Inf. Theory, vol. 56, no. 5, pp. 2307-2359, May 2010.]. We also develop
an approximation indicating that, for a given L, the achievable ATR increases
as the decoding period decreases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06657</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06657</id><created>2015-02-23</created><authors><author><keyname>Geyik</keyname><forenames>Sahin Cem</forenames></author><author><keyname>Saxena</keyname><forenames>Abhishek</forenames></author><author><keyname>Dasdan</keyname><forenames>Ali</forenames></author></authors><title>Multi-Touch Attribution Based Budget Allocation in Online Advertising</title><categories>cs.AI</categories><comments>This paper has been published in ADKDD 2014, August 24, New York
  City, New York, U.S.A</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Budget allocation in online advertising deals with distributing the campaign
(insertion order) level budgets to different sub-campaigns which employ
different targeting criteria and may perform differently in terms of
return-on-investment (ROI). In this paper, we present the efforts at Turn on
how to best allocate campaign budget so that the advertiser or campaign-level
ROI is maximized. To do this, it is crucial to be able to correctly determine
the performance of sub-campaigns. This determination is highly related to the
action-attribution problem, i.e. to be able to find out the set of ads, and
hence the sub-campaigns that provided them to a user, that an action should be
attributed to. For this purpose, we employ both last-touch (last ad gets all
credit) and multi-touch (many ads share the credit) attribution methodologies.
We present the algorithms deployed at Turn for the attribution problem, as well
as their parallel implementation on the large advertiser performance datasets.
We conclude the paper with our empirical comparison of last-touch and
multi-touch attribution-based budget allocation in a real online advertising
setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06665</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06665</id><created>2015-02-23</created><authors><author><keyname>Steinhardt</keyname><forenames>Jacob</forenames></author><author><keyname>Liang</keyname><forenames>Percy</forenames></author></authors><title>Reified Context Models</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A classic tension exists between exact inference in a simple model and
approximate inference in a complex model. The latter offers expressivity and
thus accuracy, but the former provides coverage of the space, an important
property for confidence estimation and learning with indirect supervision. In
this work, we introduce a new approach, reified context models, to reconcile
this tension. Specifically, we let the amount of context (the arity of the
factors in a graphical model) be chosen &quot;at run-time&quot; by reifying it---that is,
letting this choice itself be a random variable inside the model. Empirically,
we show that our approach obtains expressivity and coverage on three natural
language tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06667</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06667</id><created>2015-02-23</created><authors><author><keyname>Xu</keyname><forenames>Yuhua</forenames></author><author><keyname>Zhang</keyname><forenames>Yuli</forenames></author><author><keyname>Wu</keyname><forenames>Qihui</forenames></author><author><keyname>Shen</keyname><forenames>Liang</forenames></author><author><keyname>Wang</keyname><forenames>Jinlong</forenames></author></authors><title>Distributed Spectrum Access for Cognitive Small Cell Networks: A Robust
  Graphical Game Approach</title><categories>cs.IT cs.GT math.IT</categories><comments>7 pages, 5 figures, Submitted to IEEE Transactions on Vehicular
  Technology as a correspondence</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter investigates the problem of distributed spectrum access for
cognitive small cell networks. Compared with existing work, two inherent
features are considered: i) the transmission of a cognitive small cell base
station only interferes with its neighbors due to the low power, i.e., the
interference is local, and ii) the channel state is time-varying due to fading.
We formulate the problem as a robust graphical game, and prove that it is an
ordinal potential game which has at least one pure strategy Nash equilibrium
(NE). Also, the lower throughput bound of NE solutions is analytically
obtained. To cope with the dynamic and incomplete information constraints, we
propose a distribute spectrum access algorithm to converge to some stable
results. Simulation results validate the effectiveness of the proposed
game-theoretic distributed learning solution in time-varying spectrum
environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06668</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06668</id><created>2015-02-23</created><authors><author><keyname>Steinhardt</keyname><forenames>Jacob</forenames></author><author><keyname>Liang</keyname><forenames>Percy</forenames></author></authors><title>Learning Fast-Mixing Models for Structured Prediction</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Markov Chain Monte Carlo (MCMC) algorithms are often used for approximate
inference inside learning, but their slow mixing can be difficult to diagnose
and the approximations can seriously degrade learning. To alleviate these
issues, we define a new model family using strong Doeblin Markov chains, whose
mixing times can be precisely controlled by a parameter. We also develop an
algorithm to learn such models, which involves maximizing the data likelihood
under the induced stationary distribution of these chains. We show empirical
improvements on two challenging inference tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06669</identifier>
 <datestamp>2015-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06669</id><created>2015-02-23</created><updated>2015-07-03</updated><authors><author><keyname>Xu</keyname><forenames>Yuhua</forenames></author><author><keyname>Xu</keyname><forenames>Yitao</forenames></author><author><keyname>Anpalagan</keyname><forenames>Alagan</forenames></author></authors><title>Database-assisted Spectrum Access in Dynamic Networks: A Distributed
  Learning Solution</title><categories>cs.IT cs.GT math.IT</categories><comments>7 pages, 6 figures, to appear in IEEE Access</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the problem of database-assisted spectrum access in
dynamic TV white spectrum networks, in which the active user set is varying.
Since there is no central controller and information exchange, it encounters
dynamic and incomplete information constraints. To solve this challenge, we
formulate a state-based spectrum access game and a robust spectrum access game.
It is proved that the two games are ordinal potential games with the (expected)
aggregate weighted interference serving as the potential functions. A
distributed learning algorithm is proposed to achieve the pure strategy Nash
equilibrium (NE) of the games. It is shown that the best NE is almost the same
with the optimal solution and the achievable throughput of the proposed
learning algorithm is very close to the optimal one, which validates the
effectiveness of the proposed game-theoretic solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06670</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06670</id><created>2015-02-23</created><updated>2015-09-16</updated><authors><author><keyname>Xu</keyname><forenames>Yuhua</forenames></author><author><keyname>Wang</keyname><forenames>Chenggui</forenames></author><author><keyname>Chen</keyname><forenames>Junhong</forenames></author><author><keyname>Wang</keyname><forenames>Jinlong</forenames></author><author><keyname>Xu</keyname><forenames>Yitao</forenames></author><author><keyname>Wu</keyname><forenames>Qihui</forenames></author><author><keyname>Anpalagan</keyname><forenames>Alagan</forenames></author></authors><title>Load-aware Dynamic Spectrum Access for Small Cell Networks: A Graphical
  Game Approach</title><categories>cs.IT cs.GT math.IT</categories><comments>6 pages, 5 figures, Submitted to IEEE Transactions on Vehicular
  Technology as a correspondence (under third round review)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we investigate the problem of dynamic spectrum access for
small cell networks, using a graphical game approach. Compared with existing
studies, we take the features of different cell loads and local interference
relationship into account. It is proved that the formulated spectrum access
game is an exact potential game with the aggregate interference level as the
potential function, and Nash equilibrium (NE) of the game corresponds to the
global or local optima of the original optimization problem. A lower bound of
the achievable aggregate interference level is rigorously derived. Finally, we
propose an autonomous best response learning algorithm to converge towards its
NE. It is shown that the proposed game-theoretic solution converges rapidly and
its achievable performance is close to the optimum solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06671</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06671</id><created>2015-02-23</created><authors><author><keyname>Wang</keyname><forenames>Pinghui</forenames></author><author><keyname>Lui</keyname><forenames>John C. S.</forenames></author><author><keyname>Towsley</keyname><forenames>Don</forenames></author></authors><title>Minfer: Inferring Motif Statistics From Sampled Edges</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Characterizing motif (i.e., locally connected subgraph patterns) statistics
is important for understanding complex networks such as online social networks
and communication networks. Previous work made the strong assumption that the
graph topology of interest is known, and that the dataset either fits into main
memory or stored on disks such that it is not expensive to obtain all neighbors
of any given node. In practice, researchers have to deal with the situation
where the graph topology is unknown, either because the graph is dynamic, or
because it is expensive to collect and store all topological and meta
information on disk. Hence, what is available to researchers is only a snapshot
of the graph generated by sampling edges from the graph at random, which we
called a &quot;RESampled graph&quot;. Clearly, a RESampled graph's motif statistics may
be quite different from the underlying original graph. To solve this challenge,
we propose a framework and implement a system called Minfer, which can take the
given RESampled graph and accurately infer the underlying graph's motif
statistics. We also use Fisher information to bound the error of our estimates.
Experiments using large scale datasets show that our method to be accurate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06672</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06672</id><created>2015-02-23</created><updated>2015-03-25</updated><authors><author><keyname>Xu</keyname><forenames>Yuhua</forenames></author><author><keyname>Wang</keyname><forenames>Jinlong</forenames></author><author><keyname>Wu</keyname><forenames>Qihui</forenames></author><author><keyname>Zheng</keyname><forenames>Jianchao</forenames></author><author><keyname>Shen</keyname><forenames>Liang</forenames></author><author><keyname>Anpalagan</keyname><forenames>Alagan</forenames></author></authors><title>Dynamic Spectrum Access with Statistical QoS Provisioning: A Distributed
  Learning Approach Beyond Expectation Optimization</title><categories>cs.IT cs.GT math.IT</categories><comments>13 pages, 9 figures, submitted as a regular paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article investigates the problem of dynamic spectrum access with
statistical quality of service (QoS) provisioning for dynamic canonical
networks, in which the channel states are time-varying from slot to slot. In
the existing work with time-varying environment, the commonly used optimization
objective is to maximize the expectation of a certain metric (e.g., throughput
or achievable rate). However, it is realized that expectation alone is not
enough since some applications are sensitive to the channel fluctuations.
Effective capacity is a promising metric for time-varying service process since
it characterizes the packet delay violating probability (regarded as an
important statistical QoS index), by taking into account not only the
expectation but also other high-order statistic. We formulate the interactions
among the users in the time-varying environment as a non-cooperative game, in
which the utility function is defined as the achieved effective capacity. We
prove that it is an ordinal potential game which has at least one pure strategy
Nash equilibrium. In addition, we propose a multi-agent learning algorithm
which is proved to achieve stable solutions with uncertain, dynamic and
incomplete information constraints. The convergence of the proposed learning
algorithm is verified by simulation results. Also, it is shown that the
proposed multi-agent learning algorithm achieves satisfactory performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06682</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06682</id><created>2015-02-23</created><updated>2015-02-26</updated><authors><author><keyname>Shen</keyname><forenames>Chih-Ya</forenames></author><author><keyname>Yang</keyname><forenames>De-Nian</forenames></author><author><keyname>Lee</keyname><forenames>Wang-Chien</forenames></author><author><keyname>Chen</keyname><forenames>Ming-Syan</forenames></author></authors><title>Maximizing Friend-Making Likelihood for Social Activity Organization</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The social presence theory in social psychology suggests that
computer-mediated online interactions are inferior to face-to-face, in-person
interactions. In this paper, we consider the scenarios of organizing in person
friend-making social activities via online social networks (OSNs) and formulate
a new research problem, namely, Hop-bounded Maximum Group Friending (HMGF), by
modeling both existing friendships and the likelihood of new friend making. To
find a set of attendees for socialization activities, HMGF is unique and
challenging due to the interplay of the group size, the constraint on existing
friendships and the objective function on the likelihood of friend making. We
prove that HMGF is NP-Hard, and no approximation algorithm exists unless P =
NP. We then propose an error-bounded approximation algorithm to efficiently
obtain the solutions very close to the optimal solutions. We conduct a user
study to validate our problem formulation and per- form extensive experiments
on real datasets to demonstrate the efficiency and effectiveness of our
proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06686</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06686</id><created>2015-02-23</created><authors><author><keyname>Xu</keyname><forenames>Kai</forenames></author><author><keyname>Kim</keyname><forenames>Vladimir G.</forenames></author><author><keyname>Huang</keyname><forenames>Qixing</forenames></author><author><keyname>Kalogerakis</keyname><forenames>Evangelos</forenames></author></authors><title>Data-Driven Shape Analysis and Processing</title><categories>cs.GR</categories><comments>10 pages, 19 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data-driven methods play an increasingly important role in discovering
geometric, structural, and semantic relationships between 3D shapes in
collections, and applying this analysis to support intelligent modeling,
editing, and visualization of geometric data. In contrast to traditional
approaches, a key feature of data-driven approaches is that they aggregate
information from a collection of shapes to improve the analysis and processing
of individual shapes. In addition, they are able to learn models that reason
about properties and relationships of shapes without relying on hard-coded
rules or explicitly programmed instructions. We provide an overview of the main
concepts and components of these techniques, and discuss their application to
shape classification, segmentation, matching, reconstruction, modeling and
exploration, as well as scene analysis and synthesis, through reviewing the
literature and relating the existing works with both qualitative and numerical
comparisons. We conclude our report with ideas that can inspire future research
in data-driven shape analysis and processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06691</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06691</id><created>2015-02-24</created><authors><author><keyname>Lee</keyname><forenames>Kwangsu</forenames></author><author><keyname>Lee</keyname><forenames>Dong Hoon</forenames></author><author><keyname>Yung</keyname><forenames>Moti</forenames></author></authors><title>Sequential Aggregate Signatures with Short Public Keys without Random
  Oracles</title><categories>cs.CR</categories><comments>39 pages in Theoret. Comput. Sci. (2015)</comments><doi>10.1016/j.tcs.2015.02.019</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The notion of aggregate signature has been motivated by applications and it
enables any user to compress different signatures signed by different signers
on different messages into a short signature. Sequential aggregate signature,
in turn, is a special kind of aggregate signature that only allows a signer to
add his signature into an aggregate signature in sequential order. This latter
scheme has applications in diversified settings such as in reducing bandwidth
of certificate chains and in secure routing protocols. Lu, Ostrovsky, Sahai,
Shacham, and Waters (EUROCRYPT 2006) presented the first sequential aggregate
signature scheme in the standard model. The size of their public key, however,
is quite large (i.e., the number of group elements is proportional to the
security parameter), and therefore, they suggested as an open problem the
construction of such a scheme with short keys.
  In this paper, we propose the first sequential aggregate signature schemes
with short public keys (i.e., a constant number of group elements) in prime
order (asymmetric) bilinear groups that are secure under static assumptions in
the standard model. Furthermore, our schemes employ a constant number of
pairing operations per message signing and message verification operation.
Technically, we start with a public-key signature scheme based on the recent
dual system encryption technique of Lewko and Waters (TCC 2010). This technique
cannot directly provide an aggregate signature scheme since, as we observed,
additional elements should be published in a public key to support aggregation.
Thus, our constructions are careful augmentation techniques for the dual system
technique to allow it to support sequential aggregate signature schemes. We
also propose a multi-signature scheme with short public parameters in the
standard model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06703</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06703</id><created>2015-02-24</created><authors><author><keyname>Shekar</keyname><forenames>B. H.</forenames></author><author><keyname>L.</keyname><forenames>Smitha M.</forenames></author><author><keyname>Shivakumara</keyname><forenames>P.</forenames></author></authors><title>Discrete Wavelet Transform and Gradient Difference based approach for
  text localization in videos</title><categories>cs.CV</categories><comments>Fifth International Conference on Signals and Image Processing, IEEE,
  DOI 10.1109/ICSIP.2014.50, pp. 280-284, held at BNMIT, Bangalore in January
  2014</comments><doi>10.1109/ICSIP.2014.50</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The text detection and localization is important for video analysis and
understanding. The scene text in video contains semantic information and thus
can contribute significantly to video retrieval and understanding. However,
most of the approaches detect scene text in still images or single video frame.
Videos differ from images in temporal redundancy. This paper proposes a novel
hybrid method to robustly localize the texts in natural scene images and videos
based on fusion of discrete wavelet transform and gradient difference. A set of
rules and geometric properties have been devised to localize the actual text
regions. Then, morphological operation is performed to generate the text
regions and finally the connected component analysis is employed to localize
the text in a video frame. The experimental results obtained on publicly
available standard ICDAR 2003 and Hua dataset illustrate that the proposed
method can accurately detect and localize texts of various sizes, fonts and
colors. The experimentation on huge collection of video databases reveal the
suitability of the proposed method to video databases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06719</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06719</id><created>2015-02-24</created><updated>2015-08-27</updated><authors><author><keyname>Chhabra</keyname><forenames>Anamika</forenames></author><author><keyname>Iyengar</keyname><forenames>S. R. S.</forenames></author><author><keyname>Saini</keyname><forenames>Poonam</forenames></author><author><keyname>Bhat</keyname><forenames>Rajesh Shreedhar</forenames></author><author><keyname>Kumar</keyname><forenames>Vijay</forenames></author></authors><title>Ecosystem: A Characteristic Of Crowdsourced Environments</title><categories>cs.CY</categories><comments>21 pages, 9 figures, 7 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The phenomenal success of certain crowdsourced online platforms, such as
Wikipedia, is accredited to their ability to tap the crowd's potential to
collaboratively build knowledge. While it is well known that the crowd's
collective wisdom surpasses the cumulative individual expertise, little is
understood on the dynamics of knowledge building in a crowdsourced environment.
A proper understanding of the dynamics of knowledge building in a crowdsourced
environment would enable one in the better designing of such environments to
solicit knowledge from the crowd. Our experiment on crowdsourced systems based
on annotations shows that an important reason for the rapid knowledge building
in such environments is due to variance in expertise. First, we used as our
test bed, a customized Crowdsourced Annotation System (CAS) which provides a
group of users the facility to annotate a given document while trying to
understand it. Our results showed the presence of different genres of
proficiency amongst the users of an annotation system. We observed that the
ecosystem in crowdsourced annotation system comprised of mainly four categories
of contributors, namely: Probers, Solvers, Articulators and Explorers. We
inferred from our experiment that the knowledge garnering mainly happens due to
the synergetic interaction across these categories. Further, we conducted an
analysis on the dataset of Wikipedia and Stack Overflow and noticed the
ecosystem presence in these portals as well. From this study, we claim that the
ecosystem is a universal characteristic of all crowdsourced portals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06732</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06732</id><created>2015-02-24</created><authors><author><keyname>Zeng</keyname><forenames>Zhiwen</forenames></author><author><keyname>Wang</keyname><forenames>Xiangke</forenames></author><author><keyname>Zheng</keyname><forenames>Zhiqiang</forenames></author></authors><title>Convergence Analysis using the Edge Laplacian: Robust Consensus of
  Nonlinear Multi-agent Systems via ISS Method</title><categories>cs.SY cs.MA</categories><comments>22 pages, 10 figures; Submitted to International Journal of Robust
  and Nonlinear Control</comments><msc-class>93A14</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study develops an original and innovative matrix representation with
respect to the information flow for networked multi-agent system. To begin
with, the general concepts of the edge Laplacian of digraph are proposed with
its algebraic properties. Benefit from this novel graph-theoretic tool, we can
build a bridge between the consensus problem and the edge agreement problem; we
also show that the edge Laplacian sheds a new light on solving the leaderless
consensus problem. Based on the edge agreement framework, the technical
challenges caused by unknown but bounded disturbances and inherently nonlinear
dynamics can be well handled. In particular, we design an integrated procedure
for a new robust consensus protocol that is based on a blend of algebraic graph
theory and the newly developed cyclic-small-gain theorem. Besides, to highlight
the intricate relationship between the original graph and cyclic-small-gain
theorem, the concept of edge-interconnection graph is introduced for the first
time. Finally, simulation results are provided to verify the theoretical
analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06733</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06733</id><created>2015-02-24</created><updated>2015-02-25</updated><authors><author><keyname>Guermouche</keyname><forenames>Amina</forenames><affiliation>UVSQ</affiliation></author><author><keyname>Triquenaux</keyname><forenames>Nicolas</forenames><affiliation>UVSQ</affiliation></author><author><keyname>Pradelle</keyname><forenames>Benoit</forenames><affiliation>UVSQ</affiliation></author><author><keyname>Jalby</keyname><forenames>William</forenames><affiliation>UVSQ</affiliation></author></authors><title>Minimizing Energy Consumption of MPI Programs in Realistic Environment</title><categories>cs.DC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic voltage and frequency scaling proves to be an efficient way of
reducing energy consumption of servers. Energy savings are typically achieved
by setting a well-chosen frequency during some program phases. However,
determining suitable program phases and their associated optimal frequencies is
a complex problem. Moreover, hardware is constrained by non negligible
frequency transition latencies. Thus, various heuristics were proposed to
determine and apply frequencies, but evaluating their efficiency remains an
issue. In this paper, we translate the energy minimization problem into a mixed
integer program that specifically models most current hardware limitations. The
problem solution then estimates the minimal energy consumption and the
associated frequency schedule. The paper provides two different formulations
and a discussion on the feasibility of each of them on realistic applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06735</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06735</id><created>2015-02-24</created><authors><author><keyname>Mirbel</keyname><forenames>Isabelle</forenames><affiliation>INRIA Sophia Antipolis / Laboratoire I3S</affiliation></author><author><keyname>Crescenzo</keyname><forenames>Pierre</forenames></author><author><keyname>Cerezo</keyname><forenames>Nadia</forenames></author></authors><title>Empowering Web Service Search with Business Know-How</title><categories>cs.SE cs.CY</categories><comments>Knowledge Engineering for Software Development Life Cycles: Support
  Technologies and Applications, IGI Global, pp.161-176, 2011, 9781609605094</comments><proxy>ccsd</proxy><doi>10.4018/978-1-60960-509-4.ch009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose first to start by presenting a state of the art of
existing approaches about scientific workflows (including neuroscience
workflows) in order to highlight business users' needs in terms of Web Services
combination. Then we discuss about intentional process modeling for scientific
workflows especially to search for Web Services. Next we present our approach
SATIS to provide reasoning and traceability capabilities on Web Services
business combination know-how, in order to bridge the gap between workflows
providers and users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06743</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06743</id><created>2015-02-24</created><updated>2015-03-02</updated><authors><author><keyname>Jaziri</keyname><forenames>Aymen</forenames></author><author><keyname>Nasri</keyname><forenames>Ridha</forenames></author><author><keyname>Chahed</keyname><forenames>Tijani</forenames></author></authors><title>Traffic Hotspot localization in 3G and 4G wireless networks using OMC
  metrics</title><categories>cs.NI</categories><comments>7 pages, 7 figures, published in Proc. IEEE International Symposium
  on Personal, Indoor and Mobile Radio Communications 2014 (PIMRC); IEEE
  International Symposium on Personal, Indoor and Mobile Radio Communications
  2014 (PIMRC)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, there has been an increasing awareness to traffic
localization techniques driven by the emergence of heterogeneous networks
(HetNet) with small cells deployment and the green networks. The localization
of hotspot data traffic with a very high accuracy is indeed of great interest
to know where the small cells should be deployed and how can be managed for
sleep mode concept. In this paper, we propose a new traffic localization
technique based on the combination of different key performance indicators
(KPI) extracted from the operation and maintenance center (OMC). The proposed
localization algorithm is composed with five main steps; each one corresponds
to the determination of traffic weight per area using only one KPI. These KPIs
are Timing Advance (TA), Angle of Arrival (AoA), Neighbor cell level, the load
of each cell and the Harmonic mean throughput (HMT) versus the Arithmetic mean
throughput (AMT). The five KPIs are finally combined by a function taking as
variables the values computed from the five steps. By mixing such KPIs, we show
that it is possible to lessen significantly the errors of localization in a
high precision attaining small cell dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06752</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06752</id><created>2015-02-24</created><authors><author><keyname>Schmitt</keyname><forenames>Clara</forenames><affiliation>GC</affiliation></author><author><keyname>Rey-Coyrehourcq</keyname><forenames>S&#xe9;bastien</forenames><affiliation>GC</affiliation></author><author><keyname>Reuillon</keyname><forenames>Romain</forenames><affiliation>GC, ISC-PIF</affiliation></author><author><keyname>Pumain</keyname><forenames>Denise</forenames><affiliation>GC</affiliation></author></authors><title>Half a billion simulations: evolutionary algorithms and distributed
  computing for calibrating the SimpopLocal geographical model</title><categories>cs.CY physics.soc-ph</categories><proxy>ccsd</proxy><doi>10.1068/b130064p</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-agent geographical models integrate very large numbers of spatial
interactions. In order to validate those models large amount of computing is
necessary for their simulation and calibration. Here a new data processing
chain including an automated calibration procedure is experimented on a
computational grid using evolutionary algorithms. This is applied for the first
time to a geographical model designed to simulate the evolution of an early
urban settlement system. The method enables us to reduce the computing time and
provides robust results. Using this method, we identify several parameter
settings that minimise three objective functions that quantify how closely the
model results match a reference pattern. As the values of each parameter in
different settings are very close, this estimation considerably reduces the
initial possible domain of variation of the parameters. The model is thus a
useful tool for further multiple applications on empirical historical
situations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06757</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06757</id><created>2015-02-24</created><authors><author><keyname>Dias</keyname><forenames>Mart&#xed;n</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Bacchelli</keyname><forenames>Alberto</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Gousios</keyname><forenames>Georgios</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Cassou</keyname><forenames>Damien</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Ducasse</keyname><forenames>St&#xe9;phane</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author></authors><title>Untangling Fine-Grained Code Changes</title><categories>cs.SE</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  After working for some time, developers commit their code changes to a
version control system. When doing so, they often bundle unrelated changes
(e.g., bug fix and refactoring) in a single commit, thus creating a so-called
tangled commit. Sharing tangled commits is problematic because it makes review,
reversion, and integration of these commits harder and historical analyses of
the project less reliable. Researchers have worked at untangling existing
commits, i.e., finding which part of a commit relates to which task. In this
paper, we contribute to this line of work in two ways: (1) A publicly available
dataset of untangled code changes, created with the help of two developers who
accurately split their code changes into self contained tasks over a period of
four months; (2) a novel approach, EpiceaUntangler, to help developers share
untangled commits (aka. atomic commits) by using fine-grained code change
information. EpiceaUntangler is based and tested on the publicly available
dataset, and further evaluated by deploying it to 7 developers, who used it for
2 weeks. We recorded a median success rate of 91% and average one of 75%, in
automatically creating clusters of untangled fine-grained code changes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06759</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06759</id><created>2015-02-24</created><updated>2015-11-04</updated><authors><author><keyname>Brunet</keyname><forenames>Olivier</forenames></author></authors><title>Quantum Measurements from a Logical Point of View</title><categories>quant-ph cs.LO math.LO</categories><comments>In Proceedings QPL 2015, arXiv:1511.01181</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 195, 2015, pp. 84-95</journal-ref><doi>10.4204/EPTCS.195.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a logic modelling some aspects of the behaviour of the
measurement process, in such a way that no direct mention of quantum states is
made, thus avoiding the problems associated to this rather evasive notion. We
then study some properties of the models of this logic, and deduce some
characteristics that any model (and hence, any formulation of quantum mechanics
compatible with its predictions and relying on a notion of measurement) should
verify. The main results we obtain are that in the case of a Hilbert space of
dimension at least 3, using a strengthening of the Kochen-Specker theorem, we
show that no model can lead to the certain prediction of more than one atomic
outcome. Moreover, if the Hilbert space is finite dimensional, then we are able
to precisely describe the structure of the predictions of any model of our
logic. In particular, we show that all the models of our logic do exactly make
the same predictions regarding whether a given sequence of outcomes is possible
or not, so that quantum mechanics can be considered complete as long as the
possibility of outcomes is considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06761</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06761</id><created>2015-02-24</created><authors><author><keyname>Behrisch</keyname><forenames>Mike</forenames></author><author><keyname>Hermann</keyname><forenames>Miki</forenames></author><author><keyname>Mengel</keyname><forenames>Stefan</forenames></author><author><keyname>Salzer</keyname><forenames>Gernot</forenames></author></authors><title>Minimal Distance of Propositional Models</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the complexity of three optimization problems in Boolean
propositional logic related to information theory: Given a conjunctive formula
over a set of relations, find a satisfying assignment with minimal Hamming
distance to a given assignment that satisfies the formula
($\mathsf{NeareastOtherSolution}$, $\mathsf{NOSol}$) or that does not need to
satisfy it ($\mathsf{NearestSolution}$, $\mathsf{NSol}$). The third problem
asks for two satisfying assignments with a minimal Hamming distance among all
such assignments ($\mathsf{MinSolutionDistance}$, $\mathsf{MSD}$).
  For all three problems we give complete classifications with respect to the
relations admitted in the formula. We give polynomial time algorithms for
several classes of constraint languages. For all other cases we prove hardness
or completeness regarding APX, APX, NPO, or equivalence to well-known hard
optimization problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06764</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06764</id><created>2015-02-24</created><authors><author><keyname>Chang</keyname><forenames>Ching-Lueh</forenames></author></authors><title>A deterministic sublinear-time nonadaptive algorithm for metric
  $1$-median selection</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a deterministic $O(hn^{1+1/h})$-time $(2h)$-approximation nonadaptive
algorithm for $1$-median selection in $n$-point metric spaces, where
$h\in\mathbb{Z}^+\setminus\{1\}$ is arbitrary. Our proof generalizes that of
Chang.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06775</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06775</id><created>2015-02-24</created><updated>2015-06-09</updated><authors><author><keyname>Kawamoto</keyname><forenames>Tatsuro</forenames></author><author><keyname>Kabashima</keyname><forenames>Yoshiyuki</forenames></author></authors><title>Limitations in the spectral method for graph partitioning: detectability
  threshold and localization of eigenvectors</title><categories>cs.SI cond-mat.stat-mech physics.soc-ph</categories><comments>26 pages, 13 figures</comments><journal-ref>Phys. Rev. E 91, 062803 (2015)</journal-ref><doi>10.1103/PhysRevE.91.062803</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Investigating the performance of different methods is a fundamental problem
in graph partitioning. In this paper, we estimate the so-called detectability
threshold for the spectral method with both unnormalized and normalized
Laplacians in sparse graphs. The detectability threshold is the critical point
at which the result of the spectral method is completely uncorrelated to the
planted partition. We also analyze whether the localization of eigenvectors
affects the partitioning performance in the detectable region. We use the
replica method, which is often used in the field of spin-glass theory, and
focus on the case of bisection. We show that the gap between the estimated
threshold for the spectral method and the threshold obtained from Bayesian
inference is considerable in sparse graphs, even without eigenvector
localization. This gap closes in a dense limit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06777</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06777</id><created>2015-02-24</created><updated>2015-06-24</updated><authors><author><keyname>Goulart</keyname><forenames>Jos&#xe9; Henrique De Morais</forenames><affiliation>SATIE</affiliation></author><author><keyname>Boizard</keyname><forenames>Maxime</forenames><affiliation>SATIE</affiliation></author><author><keyname>Boyer</keyname><forenames>R&#xe9;my</forenames><affiliation>GIPSA-CICS</affiliation></author><author><keyname>Favier</keyname><forenames>G&#xe9;rard</forenames><affiliation>GIPSA-CICS</affiliation></author><author><keyname>Comon</keyname><forenames>Pierre</forenames><affiliation>GIPSA-CICS</affiliation></author></authors><title>Statistical efficiency of structured cpd estimation applied to
  Wiener-Hammerstein modeling</title><categories>stat.CO cs.NA</categories><comments>Accepted for publication in the Proceedings of the European Signal
  Processing Conference (EUSIPCO) Aug 2015, Nice, France. 2015</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The computation of a structured canonical polyadic decomposition (CPD) is
useful to address several important modeling problems in real-world
applications. In this paper, we consider the identification of a nonlinear
system by means of a Wiener-Hammerstein model, assuming a high-order Volterra
kernel of that system has been previously estimated. Such a kernel, viewed as a
tensor, admits a CPD with banded circulant factors which comprise the model
parameters. To estimate them, we formulate specialized estimators based on
recently proposed algorithms for the computation of structured CPDs. Then,
considering the presence of additive white Gaussian noise, we derive a
closed-form expression for the Cramer-Rao bound (CRB) associated with this
estimation problem. Finally, we assess the statistical performance of the
proposed estimators via Monte Carlo simulations, by comparing their mean-square
error with the CRB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06791</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06791</id><created>2015-02-24</created><authors><author><keyname>Huang</keyname><forenames>Yang</forenames></author><author><keyname>Clerckx</keyname><forenames>Bruno</forenames></author></authors><title>Joint Wireless Information and Power Transfer in a Three-Node Autonomous
  MIMO Relay Network</title><categories>cs.IT math.IT</categories><comments>To appear in ICC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates a three-node amplify-and-forward (AF) multiple-input
multiple-output (MIMO) relay network, where an autonomous relay harvests power
from the source information flow and is further helped by an energy flow in the
form of a wireless power transfer (WPT) at the destination. An
energy-flow-assisted two-phase relaying scheme is proposed, where a source and
relay joint optimization is formulated to maximize the rate. By diagonalizing
the channel, the problem is simplified to a power optimization, where a relay
channel pairing problem is solved by an ordering operation. The proposed
algorithm, which iteratively optimizes the relay and source power, is shown to
converge. Closed-form solutions can be obtained for the separate relay and
source optimizations. Besides, a two-phase relaying without energy flow is also
studied. Simulation results show that the energy-flow-assisted scheme is
beneficial to the rate enhancement, if the transmit power of the energy flow is
adequately larger than that of the information flow. Otherwise, the scheme
without energy flow would be preferable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06792</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06792</id><created>2015-02-24</created><authors><author><keyname>Speicher</keyname><forenames>Maximilian</forenames></author></authors><title>What is Usability? A Characterization based on ISO 9241-11 and ISO/IEC
  25010</title><categories>cs.HC</categories><comments>Technical Report; Department of Computer Science, Technische
  Universit\&quot;at Chemnitz; also available from
  https://www.tu-chemnitz.de/informatik/service/ib/2015.php.en</comments><report-no>CSR-15-02</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  According to Brooke* &quot;Usability does not exist in any absolute sense; it can
only be defined with reference to particular contexts.&quot; That is, one cannot
speak of usability without specifying what that particular usability is
characterized by. Driven by the feedback of a reviewer at an international
conference, I explore in which way one can precisely specify the kind of
usability they are investigating in a given setting. Finally, I come up with a
formalism that defines usability as a quintuple comprising the elements level
of usability metrics, product, users, goals and context of use. Providing
concrete values for these elements then constitutes the investigated type of
usability. The use of this formalism is demonstrated in two case studies.
  * J. Brooke. SUS: A &quot;quick and dirty&quot; usability scale. In P. W. Jordan, B.
Thomas, B. A. Weerdmeester, and A. L. McClelland, editors, Usability Evaluation
in Industry. Taylor and Francis, 1996.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06795</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06795</id><created>2015-02-24</created><authors><author><keyname>Cohen</keyname><forenames>Albert</forenames><affiliation>LPMC</affiliation></author><author><keyname>Devore</keyname><forenames>Ronald</forenames><affiliation>TAMU</affiliation></author></authors><title>Kolmogorov widths under holomorphic mappings</title><categories>math.AP cs.NA math.NA</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  If $L$ is a bounded linear operator mapping the Banach space $X$ into the
Banach space $Y$ and $K$ is a compact set in $X$, then the Kolmogorov widths of
the image $L(K)$ do not exceed those of $K$ multiplied by the norm of $L$. We
extend this result from linear maps to holomorphic mappings $u$ from $X$ to $Y$
in the following sense: when the $n$ widths of $K$ are $O(n^{-r})$ for some
$r\textgreater{}1$, then those of $u(K)$ are $O(n^{-s})$ for any $s \textless{}
r-1$, We then use these results to prove various theorems about Kolmogorov
widths of manifolds consisting of solutions to certain parametrized PDEs.
Results of this type are important in the numerical analysis of reduced bases
and other reduced modeling methods, since the best possible performance of such
methods is governed by the rate of decay of the Kolmogorov widths of the
solution manifold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06796</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06796</id><created>2015-02-24</created><authors><author><keyname>Hong</keyname><forenames>Seunghoon</forenames></author><author><keyname>You</keyname><forenames>Tackgeun</forenames></author><author><keyname>Kwak</keyname><forenames>Suha</forenames></author><author><keyname>Han</keyname><forenames>Bohyung</forenames></author></authors><title>Online Tracking by Learning Discriminative Saliency Map with
  Convolutional Neural Network</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an online visual tracking algorithm by learning discriminative
saliency map using Convolutional Neural Network (CNN). Given a CNN pre-trained
on a large-scale image repository in offline, our algorithm takes outputs from
hidden layers of the network as feature descriptors since they show excellent
representation performance in various general visual recognition problems. The
features are used to learn discriminative target appearance models using an
online Support Vector Machine (SVM). In addition, we construct target-specific
saliency map by backpropagating CNN features with guidance of the SVM, and
obtain the final tracking result in each frame based on the appearance model
generatively constructed with the saliency map. Since the saliency map
visualizes spatial configuration of target effectively, it improves target
localization accuracy and enable us to achieve pixel-level target segmentation.
We verify the effectiveness of our tracking algorithm through extensive
experiment on a challenging benchmark, where our method illustrates outstanding
performance compared to the state-of-the-art tracking algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06797</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06797</id><created>2015-02-24</created><updated>2015-03-03</updated><authors><author><keyname>Cohen</keyname><forenames>Albert</forenames><affiliation>LPMC</affiliation></author><author><keyname>Devore</keyname><forenames>Ronald</forenames><affiliation>TAMU</affiliation></author></authors><title>Approximation of high-dimensional parametric PDEs</title><categories>math.AP cs.NA math.NA</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parametrized families of PDEs arise in various contexts such as inverse
problems, control and optimization, risk assessment, and uncertainty
quantification. In most of these applications, the number of parameters is
large or perhaps even infinite. Thus, the development of numerical methods for
these parametric problems is faced with the possible curse of dimensionality.
This article is directed at (i) identifying and understanding which properties
of parametric equations allow one to avoid this curse and (ii) developing and
analyzing effective numerical methodd which fully exploit these properties and,
in turn, are immune to the growth in dimensionality. The first part of this
article studies the smoothness and approximability of the solution map, that
is, the map $a\mapsto u(a)$ where $a$ is the parameter value and $u(a)$ is the
corresponding solution to the PDE. It is shown that for many relevant
parametric PDEs, the parametric smoothness of this map is typically holomorphic
and also highly anisotropic in that the relevant parameters are of widely
varying importance in describing the solution. These two properties are then
exploited to establish convergence rates of $n$-term approximations to the
solution map for which each term is separable in the parametric and physical
variables. These results reveal that, at least on a theoretical level, the
solution map can be well approximated by discretizations of moderate
complexity, thereby showing how the curse of dimensionality is broken. This
theoretical analysis is carried out through concepts of approximation theory
such as best $n$-term approximation, sparsity, and $n$-widths. These notions
determine a priori the best possible performance of numerical methods and thus
serve as a benchmark for concrete algorithms. The second part of this article
turns to the development of numerical algorithms based on the theoretically
established sparse separable approximations. The numerical methods studied fall
into two general categories. The first uses polynomial expansions in terms of
the parameters to approximate the solution map. The second one searches for
suitable low dimensional spaces for simultaneously approximating all members of
the parametric family. The numerical implementation of these approaches is
carried out through adaptive and greedy algorithms. An a priori analysis of the
performance of these algorithms establishes how well they meet the theoretical
benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06800</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06800</id><created>2015-02-24</created><updated>2015-11-09</updated><authors><author><keyname>Bach</keyname><forenames>Francis</forenames><affiliation>LIENS, SIERRA</affiliation></author></authors><title>On the Equivalence between Kernel Quadrature Rules and Random Feature
  Expansions</title><categories>cs.LG math.NA stat.ML</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that kernel-based quadrature rules for computing integrals can be
seen as a special case of random feature expansions for positive definite
kernels, for a particular decomposition that always exists for such kernels. We
provide a theoretical analysis of the number of required samples for a given
approximation error, leading to both upper and lower bounds that are based
solely on the eigenvalues of the associated integral operator and match up to
logarithmic terms. In particular, we show that the upper bound may be obtained
from independent and identically distributed samples from a specific
non-uniform distribution, while the lower bound if valid for any set of points.
Applying our results to kernel-based quadrature, while our results are fairly
general, we recover known upper and lower bounds for the special cases of
Sobolev spaces. Moreover, our results extend to the more general problem of
full function approximations (beyond simply computing an integral), with
results in L2- and L$\infty$-norm that match known results for special cases.
Applying our results to random features, we show an improvement of the number
of random features needed to preserve the generalization guarantees for
learning with Lipschitz-continuous losses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06807</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06807</id><created>2015-02-24</created><authors><author><keyname>Oberweger</keyname><forenames>Markus</forenames></author><author><keyname>Wohlhart</keyname><forenames>Paul</forenames></author><author><keyname>Lepetit</keyname><forenames>Vincent</forenames></author></authors><title>Hands Deep in Deep Learning for Hand Pose Estimation</title><categories>cs.CV</categories><journal-ref>In Proceedings of 20th Computer Vision Winter Workshop (CVWW)
  2015, pp. 21-30</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce and evaluate several architectures for Convolutional Neural
Networks to predict the 3D joint locations of a hand given a depth map. We
first show that a prior on the 3D pose can be easily introduced and
significantly improves the accuracy and reliability of the predictions. We also
show how to use context efficiently to deal with ambiguities between fingers.
These two contributions allow us to significantly outperform the
state-of-the-art on several challenging benchmarks, both in terms of accuracy
and computation times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06809</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06809</id><created>2015-02-24</created><authors><author><keyname>Zeh</keyname><forenames>Alexander</forenames></author><author><keyname>Yaakobi</keyname><forenames>Eitan</forenames></author></authors><title>Optimal Linear and Cyclic Locally Repairable Codes over Small Fields</title><categories>cs.IT cs.NI math.CO math.IT</categories><comments>IEEE Information Theory Workshop (ITW) 2015, Apr 2015, Jerusalem,
  Israel</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider locally repairable codes over small fields and propose
constructions of optimal cyclic and linear codes in terms of the dimension for
a given distance and length. Four new constructions of optimal linear codes
over small fields with locality properties are developed. The first two
approaches give binary cyclic codes with locality two. While the first
construction has availability one, the second binary code is characterized by
multiple available repair sets based on a binary Simplex code. The third
approach extends the first one to q-ary cyclic codes including (binary)
extension fields, where the locality property is determined by the properties
of a shortened first-order Reed-Muller code. Non-cyclic optimal binary linear
codes with locality greater than two are obtained by the fourth construction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06810</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06810</id><created>2015-02-24</created><authors><author><keyname>Haro</keyname><forenames>Carles Anton</forenames></author><author><keyname>Ribas</keyname><forenames>Luis Castedo</forenames></author><author><keyname>Lorente</keyname><forenames>Javier del Ser</forenames></author><author><keyname>Dekorsy</keyname><forenames>Armin</forenames></author><author><keyname>Cortes</keyname><forenames>Miguel Egido</forenames></author><author><keyname>Gelabert</keyname><forenames>Xavier</forenames></author><author><keyname>Giupponi</keyname><forenames>Lorenza</forenames></author><author><keyname>Mestre</keyname><forenames>Xavier</forenames></author><author><keyname>Monserrat</keyname><forenames>Jose</forenames></author><author><keyname>Mosquera</keyname><forenames>Carlos</forenames></author><author><keyname>Soriano</keyname><forenames>Miquel</forenames></author><author><keyname>van der Perre</keyname><forenames>Liesbet</forenames></author><author><keyname>Arambarri</keyname><forenames>Jon</forenames></author><author><keyname>Romo</keyname><forenames>Juan Antonio</forenames></author></authors><title>White Paper: Radio y Redes Cognitivas</title><categories>cs.NI</categories><comments>AEI eMOV White Paper, published 04/11/2011, In Spanish</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditionally, two different policies to access the radio spectrum have
coexisted: licensed regulation, whereby the rights to use specific spectral
bands are granted in exclusivity to an individual operator; or unlicensed
regulation, according to which certain spectral bands are declared open for
free use by any operator or individual following specific rules. While these
paradigms have allowed the wireless communications sector to blossom in the
past, in recent years they have evidenced shortcomings and given signs of
exhaustion. For instance, it is quite usual to encounter fully overloaded
mobile communication systems coexisting with unused contiguous spectral bands.
This clearly advocates for a more flexible and dynamic allocation of the
spectrum resources which can only be achieved with the advent of the so-called
cognitive radios and networks. This whitepaper provides an accurate description
of priority research activities and open challenges related to the different
functionalities of cognitive radios and networks. First, we outline the main
open problems related to the theoretical characterization of cognitive radios,
spectrum sensing techniques as well as the optimization of physical layer
functionalities in these networks. Second, we provide a description of the main
research challenges that arise from a system point of view: MAC protocol
optimization, traffic modelling, RRM strategies, routing paradigms or security
issues. Next, we point out other problems related to the practical hardware
implementation of cognitive radios, giving especial emphasis to sensing
capabilities, reconfigurability and cognitive control and management. Finally,
we succinctly report on a number of current activities related to the
standardization of cognitive radio systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06811</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06811</id><created>2015-02-24</created><authors><author><keyname>Duong</keyname><forenames>Ngoc Q. K.</forenames><affiliation>HUMG</affiliation></author><author><keyname>Duong</keyname><forenames>Hien-Thanh</forenames><affiliation>HUMG</affiliation></author></authors><title>A Review of Audio Features and Statistical Models Exploited for Voice
  Pattern Design</title><categories>cs.SD stat.ML</categories><comments>http://www.iaria.org/conferences2015/PATTERNS15.html ; Seventh
  International Conferences on Pervasive Patterns and Applications (PATTERNS
  2015), Mar 2015, Nice, France</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Audio fingerprinting, also named as audio hashing, has been well-known as a
powerful technique to perform audio identification and synchronization. It
basically involves two major steps: fingerprint (voice pattern) design and
matching search. While the first step concerns the derivation of a robust and
compact audio signature, the second step usually requires knowledge about
database and quick-search algorithms. Though this technique offers a wide range
of real-world applications, to the best of the authors' knowledge, a
comprehensive survey of existing algorithms appeared more than eight years ago.
Thus, in this paper, we present a more up-to-date review and, for emphasizing
on the audio signal processing aspect, we focus our state-of-the-art survey on
the fingerprint design step for which various audio features and their
tractable statistical models are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06818</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06818</id><created>2015-02-24</created><authors><author><keyname>Usman</keyname><forenames>Ben</forenames></author><author><keyname>Oseledets</keyname><forenames>Ivan</forenames></author></authors><title>Tensor SimRank for Heterogeneous Information Networks</title><categories>cs.AI</categories><comments>Submited on KDD'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a generalization of SimRank similarity measure for heterogeneous
information networks. Given the information network, the intraclass similarity
score s(a, b) is high if the set of objects that are related with a and the set
of objects that are related with b are pair-wise similar according to all
imposed relations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06819</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06819</id><created>2015-02-24</created><updated>2015-03-13</updated><authors><author><keyname>Shuai</keyname><forenames>Hong-Han</forenames></author><author><keyname>Yang</keyname><forenames>De-Nian</forenames></author><author><keyname>Yu</keyname><forenames>Philip S.</forenames></author><author><keyname>Chen</keyname><forenames>Ming-Syan</forenames></author></authors><title>Scale-Adaptive Group Optimization for Social Activity Planning</title><categories>cs.SI</categories><comments>20 pages. arXiv admin note: substantial text overlap with
  arXiv:1305.1502</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Studies have shown that each person is more inclined to enjoy a group
activity when 1) she is interested in the activity, and 2) many friends with
the same interest join it as well. Nevertheless, even with the interest and
social tightness information available in online social networks, nowadays many
social group activities still need to be coordinated manually. In this paper,
therefore, we first formulate a new problem, named Participant Selection for
Group Activity (PSGA), to decide the group size and select proper participants
so that the sum of personal interests and social tightness of the participants
in the group is maximized, while the activity cost is also carefully examined.
To solve the problem, we design a new randomized algorithm, named Budget-Aware
Randomized Group Selection (BARGS), to optimally allocate the computation
budgets for effective selection of the group size and participants, and we
prove that BARGS can acquire the solution with a guaranteed performance bound.
The proposed algorithm was implemented in Facebook, and experimental results
demonstrate that social groups generated by the proposed algorithm
significantly outperform the baseline solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06820</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06820</id><created>2015-02-24</created><updated>2016-01-31</updated><authors><author><keyname>Yang</keyname><forenames>Song</forenames></author><author><keyname>Trajanovski</keyname><forenames>Stojan</forenames></author><author><keyname>Kuipers</keyname><forenames>Fernando A.</forenames></author></authors><title>Optimization Problems in Correlated Networks</title><categories>cs.DS cs.NI</categories><comments>11 pages, 10 figures, accepted for publication in Computational
  Social Networks, Springer</comments><journal-ref>Computational Social Networks 2016, 3:1</journal-ref><doi>10.1186/s40649-016-0026-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Solving the shortest path and the min-cut problems are key in achieving high
performance and robust communication networks. Those problems have often beeny
studied in deterministic and independent networks both in their original
formulations as well as in several constrained variants. However, in real-world
networks, link weights (e.g., delay, bandwidth, failure probability) are often
correlated due to spatial or temporal reasons, and these correlated link
weights together behave in a different manner and are not always additive.
  In this paper, we first propose two correlated link-weight models, namely (i)
the deterministic correlated model and (ii) the (log-concave) stochastic
correlated model. Subsequently, we study the shortest path problem and the
min-cut problem under these two correlated models. We prove that these two
problems are NP-hard under the deterministic correlated model, and even cannot
be approximated to arbitrary degree in polynomial time. However, these two
problems are polynomial-time solvable under the (constrained) nodal
deterministic correlated model, and can be solved by convex optimization under
the (log-concave) stochastic correlated model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06821</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06821</id><created>2015-02-24</created><authors><author><keyname>Khalil</keyname><forenames>Kassim</forenames><affiliation>IEMN</affiliation></author><author><keyname>CORLAY</keyname><forenames>Patrick</forenames><affiliation>IEMN</affiliation></author><author><keyname>Coudoux</keyname><forenames>Fran&#xe7;ois-Xavier</forenames><affiliation>IEMN</affiliation></author><author><keyname>Gazalet</keyname><forenames>Marc G.</forenames><affiliation>IEMN</affiliation></author><author><keyname>Gharbi</keyname><forenames>Mohamed</forenames><affiliation>IEMN</affiliation></author></authors><title>Analysis of the Impact of Impulsive Noise Parameters on BER Performance
  of OFDM Power-Line Communications</title><categories>cs.IT math.IT</categories><comments>The 7th International Symposium on Signal, Image, Video and
  Communications (ISIVC 2014) , Nov 2014, Marrakech, Morocco</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that asynchronous impulsive noise is the main source of
distortion that drastically affects the power-line communications (PLC)
performance. Recently, more realistic models have been proposed in the
literature which better fit the physical properties of real impulsive noise. In
this paper, we consider a pulse train model and propose a thorough analysis of
the impact of impulsive noise parameters, namely impulse width and amplitude as
well as inter-arrival time, on the bit error rate (BER) performance of
orthogonal frequency division multiplexing (OFDM) broadband PLC. A comparison
with the conventional Bernoulli-Gaussian (BG) impulsive noise model exhibits
the difference between the two approaches, showing the necessity of more
realistic models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06823</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06823</id><created>2015-02-24</created><authors><author><keyname>Rekatsinas</keyname><forenames>Theodoros</forenames></author><author><keyname>Deshpande</keyname><forenames>Amol</forenames></author><author><keyname>Parameswaran</keyname><forenames>Aditya</forenames></author></authors><title>CrowdGather: Entity Extraction over Structured Domains</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crowdsourced entity extraction is often used to acquire data for many
applications, including recommendation systems, construction of aggregated
listings and directories, and knowledge base construction. Current solutions
focus on entity extraction using a single query, e.g., only using &quot;give me
another restaurant&quot;, when assembling a list of all restaurants. Due to the cost
of human labor, solutions that focus on a single query can be highly
impractical.
  In this paper, we leverage the fact that entity extraction often focuses on
{\em structured domains}, i.e., domains that are described by a collection of
attributes, each potentially exhibiting hierarchical structure. Given such a
domain, we enable a richer space of queries, e.g., &quot;give me another Moroccan
restaurant in Manhattan that does takeout&quot;. Naturally, enabling a richer space
of queries comes with a host of issues, especially since many queries return
empty answers. We develop new statistical tools that enable us to reason about
the gain of issuing {\em additional queries} given little to no information,
and show how we can exploit the overlaps across the results of queries for
different points of the data domain to obtain accurate estimates of the gain.
We cast the problem of {\em budgeted entity extraction} over large domains as
an adaptive optimization problem that seeks to maximize the number of extracted
entities, while minimizing the overall extraction costs. We evaluate our
techniques with experiments on both synthetic and real-world datasets,
demonstrating a yield of up to 4X over competing approaches for the same
budget.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06824</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06824</id><created>2015-02-24</created><authors><author><keyname>Marforio</keyname><forenames>Claudio</forenames></author><author><keyname>Masti</keyname><forenames>Ramya Jayaram</forenames></author><author><keyname>Soriente</keyname><forenames>Claudio</forenames></author><author><keyname>Kostiainen</keyname><forenames>Kari</forenames></author><author><keyname>Capkun</keyname><forenames>Srdjan</forenames></author></authors><title>Personalized Security Indicators to Detect Application Phishing Attacks
  in Mobile Platforms</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Phishing in mobile applications is a relevant threat with successful attacks
reported in the wild. In such attacks, malicious mobile applications masquerade
as legitimate ones to steal user credentials. In this paper we categorize
application phishing attacks in mobile platforms and possible countermeasures.
We show that personalized security indicators can help users to detect phishing
attacks and have very little deployment cost. Personalized security indicators,
however, rely on the user alertness to detect phishing attacks. Previous work
in the context of website phishing has shown that users tend to ignore the
absence of security indicators and fall victim of the attacker. Consequently,
the research community has deemed personalized security indicators as an
ineffective phishing detection mechanism.
  We evaluate personalized security indicators as a phishing detection solution
in the context of mobile applications. We conducted a large-scale user study
where a significant amount of participants that used personalized security
indicators were able to detect phishing. All participants that did not use
indicators could not detect the attack and entered their credentials to a
phishing application. We found the difference in the attack detection ratio to
be statistically significant. Personalized security indicators can, therefore,
help phishing detection in mobile applications and their reputation as an
anti-phishing mechanism should be reconsidered.
  We also propose a novel protocol to setup personalized security indicators
under a strong adversarial model and provide details on its performance and
usability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06866</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06866</id><created>2015-02-24</created><authors><author><keyname>Aledavood</keyname><forenames>Talayeh</forenames></author><author><keyname>L&#xf3;pez</keyname><forenames>Eduardo</forenames></author><author><keyname>Roberts</keyname><forenames>Sam G. B.</forenames></author><author><keyname>Reed-Tsochas</keyname><forenames>Felix</forenames></author><author><keyname>Moro</keyname><forenames>Esteban</forenames></author><author><keyname>Dunbar</keyname><forenames>Robin I. M.</forenames></author><author><keyname>Saram&#xe4;ki</keyname><forenames>Jari</forenames></author></authors><title>Daily rhythms in mobile telephone communication</title><categories>physics.soc-ph cs.SI</categories><doi>10.1371/journal.pone.0138098</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Circadian rhythms are known to be important drivers of human activity and the
recent availability of electronic records of human behaviour has provided
fine-grained data of temporal patterns of activity on a large scale. Further,
questionnaire studies have identified important individual differences in
circadian rhythms, with people broadly categorised into morning-like or
evening-like individuals. However, little is known about the social aspects of
these circadian rhythms, or how they vary across individuals. In this study we
use a unique 18-month dataset that combines mobile phone calls and
questionnaire data to examine individual differences in the daily rhythms of
mobile phone activity. We demonstrate clear individual differences in daily
patterns of phone calls, and show that these individual differences are
persistent despite a high degree of turnover in the individuals' social
networks. Further, women's calls were longer than men's calls, especially
during the evening and at night, and these calls were typically focused on a
small number of emotionally intense relationships. These results demonstrate
that individual differences in circadian rhythms are not just related to broad
patterns of morningness and eveningness, but have a strong social component, in
directing phone calls to specific individuals at specific times of day.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06871</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06871</id><created>2015-02-24</created><authors><author><keyname>Frolov</keyname><forenames>Alexey</forenames></author><author><keyname>Zyablov</keyname><forenames>Victor</forenames></author></authors><title>On the Multiple Threshold Decoding of LDPC codes over GF(q)</title><categories>cs.IT math.IT</categories><comments>5 pages, submitted to ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the decoding of LDPC codes over GF(q) with the low-complexity
majority algorithm from [1]. A modification of this algorithm with multiple
thresholds is suggested. A lower estimate on the decoding radius realized by
the new algorithm is derived. The estimate is shown to be better than the
estimate for a single threshold majority decoder. At the same time the
transition to multiple thresholds does not affect the order of complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06874</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06874</id><created>2015-02-24</created><authors><author><keyname>Frolov</keyname><forenames>Alexey</forenames></author></authors><title>An Upper Bound on the Minimum Distance of LDPC Codes over GF(q)</title><categories>cs.IT math.IT</categories><comments>4 pages, submitted to ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In [1] a syndrome counting based upper bound on the minimum distance of
regular binary LDPC codes is given. In this paper we extend the bound to the
case of irregular and generalized LDPC codes over GF(q). The comparison to the
lower bound for LDPC codes over GF(q) and to the upper bound for non-binary
codes is done. The new bound is shown to lie under the Gilbert-Varshamov bound
at high rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06875</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06875</id><created>2015-02-24</created><updated>2015-03-25</updated><authors><author><keyname>Jurdzi&#x144;ski</keyname><forenames>Marcin</forenames></author><author><keyname>Lazi&#x107;</keyname><forenames>Ranko</forenames></author><author><keyname>Schmitz</keyname><forenames>Sylvain</forenames></author></authors><title>Fixed-Dimensional Energy Games are in Pseudo-Polynomial Time</title><categories>cs.GT cs.LO</categories><comments>Corrected proof of Lemma 6.2 (thanks to Dmitry Chistikov for spotting
  an error in the previous proof)</comments><journal-ref>Proceedings of ICALP 2015, Lecture Notes in Computer Science vol.
  9135, pp. 260--272, Springer</journal-ref><doi>10.1007/978-3-662-47666-6_21</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalise the hyperplane separation technique (Chatterjee and Velner,
2013) from multi-dimensional mean-payoff to energy games, and achieve an
algorithm for solving the latter whose running time is exponential only in the
dimension, but not in the number of vertices of the game graph. This answers an
open question whether energy games with arbitrary initial credit can be solved
in pseudo-polynomial time for fixed dimensions 3 or larger (Chaloupka, 2013).
It also improves the complexity of solving multi-dimensional energy games with
given initial credit from non-elementary (Br\'azdil, Jan\v{c}ar, and
Ku\v{c}era, 2010) to 2EXPTIME, thus establishing their 2EXPTIME-completeness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06878</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06878</id><created>2015-02-24</created><authors><author><keyname>Chattopadhyay</keyname><forenames>Arpan</forenames></author><author><keyname>Coupechoux</keyname><forenames>Marceau</forenames></author><author><keyname>Kumar</keyname><forenames>Anurag</forenames></author></authors><title>Sequential Decision Algorithms for Measurement-Based Impromptu
  Deployment of a Wireless Relay Network along a Line</title><categories>cs.NI</categories><comments>31 pages. arXiv admin note: text overlap with arXiv:1308.0686</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are motivated by the need, in some applications, for impromptu or
as-you-go deployment of wireless sensor networks. A person walks along a line,
starting from a sink node (e.g., a base-station), and proceeds towards a source
node (e.g., a sensor) which is at an a priori unknown location. At equally
spaced locations, he makes link quality measurements to the previous relay, and
deploys relays at some of these locations, with the aim to connect the source
to the sink by a multihop wireless path. In this paper, we consider two
approaches for impromptu deployment: (i) the deployment agent can only move
forward (which we call a pure as-you-go approach), and (ii) the deployment
agent can make measurements over several consecutive steps before selecting a
placement location among them (which we call an explore-forward approach). We
consider a light traffic regime, and formulate the problem as a Markov decision
process, where the trade-off is among the power used by the nodes, the outage
probabilities in the links, and the number of relays placed per unit distance.
We obtain the structures of the optimal policies for the pure as-you-go
approach as well as for the explore-forward approach. We also consider natural
heuristic algorithms, for comparison. Numerical examples show that the
explore-forward approach significantly outperforms the pure as-you-go approach.
Next, we propose two learning algorithms for the explore-forward approach,
based on Stochastic Approximation, which asymptotically converge to the set of
optimal policies, without using any knowledge of the radio propagation model.
We demonstrate numerically that the learning algorithms can converge (as
deployment progresses) to the set of optimal policies reasonably fast and,
hence, can be practical, model-free algorithms for deployment over large
regions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06882</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06882</id><created>2015-02-24</created><updated>2015-05-25</updated><authors><author><keyname>Bouajjani</keyname><forenames>Ahmed</forenames></author><author><keyname>Emmi</keyname><forenames>Michael</forenames></author><author><keyname>Enea</keyname><forenames>Constantin</forenames></author><author><keyname>Hamza</keyname><forenames>Jad</forenames></author></authors><title>On Reducing Linearizability to State Reachability</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient implementations of atomic objects such as concurrent stacks and
queues are especially susceptible to programming errors, and necessitate
automatic verification. Unfortunately their correctness criteria -
linearizability with respect to given ADT specifications - are hard to verify.
Even on classes of implementations where the usual temporal safety properties
like control-state reachability are decidable, linearizability is undecidable.
  In this work we demonstrate that verifying linearizability for certain fixed
ADT specifications is reducible to control-state reachability, despite being
harder for arbitrary ADTs. We effectuate this reduction for several of the most
popular atomic objects. This reduction yields the first decidability results
for verification without bounding the number of concurrent threads.
Furthermore, it enables the application of existing safety-verification tools
to linearizability verification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06895</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06895</id><created>2015-02-24</created><updated>2015-06-06</updated><authors><author><keyname>Wang</keyname><forenames>Xiangyu</forenames></author><author><keyname>Leng</keyname><forenames>Chenlei</forenames></author><author><keyname>Dunson</keyname><forenames>David B.</forenames></author></authors><title>On the consistency theory of high dimensional variable screening</title><categories>math.ST cs.LG stat.ML stat.TH</categories><comments>adding comments on REC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Variable screening is a fast dimension reduction technique for assisting high
dimensional feature selection. As a preselection method, it selects a moderate
size subset of candidate variables for further refining via feature selection
to produce the final model. The performance of variable screening depends on
both computational efficiency and the ability to dramatically reduce the number
of variables without discarding the important ones. When the data dimension $p$
is substantially larger than the sample size $n$, variable screening becomes
crucial as 1) Faster feature selection algorithms are needed; 2) Conditions
guaranteeing selection consistency might fail to hold. This article studies a
class of linear screening methods and establishes consistency theory for this
special class. In particular, we prove the restricted diagonally dominant (RDD)
condition is a necessary and sufficient condition for strong screening
consistency. As concrete examples, we show two screening methods $SIS$ and
$HOLP$ are both strong screening consistent (subject to additional constraints)
with large probability if $n &gt; O((\rho s + \sigma/\tau)^2\log p)$ under random
designs. In addition, we relate the RDD condition to the irrepresentable
condition, and highlight limitations of $SIS$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06899</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06899</id><created>2015-02-24</created><updated>2015-10-25</updated><authors><author><keyname>Schloemann</keyname><forenames>Javier</forenames></author><author><keyname>Dhillon</keyname><forenames>Harpreet S.</forenames></author><author><keyname>Buehrer</keyname><forenames>R. Michael</forenames></author></authors><title>Towards a Tractable Analysis of Localization Fundamentals in Cellular
  Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>To appear in IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When dedicated positioning systems, such as GPS, are unavailable, a mobile
device has no choice but to fall back on its cellular network for localization.
Due to random variations in the channel conditions to its surrounding base
stations (BS), the mobile device is likely to face a mix of both favorable and
unfavorable geometries for localization. Analytical studies of localization
performance (e.g., using the Cram\'{e}r-Rao lower bound) usually require that
one fix the BS geometry, and favorable geometries have always been the
preferred choice in the literature. However, not only are the resulting
analytical results constrained to the selected geometry, this practice is
likely to lead to overly-optimistic expectations of typical localization
performance. Ideally, localization performance should be studied across all
possible geometric setups, thereby also removing any selection bias. This,
however, is known to be hard and has been carried out only in simulation. In
this paper, we develop a new tractable approach where we endow the BS locations
with a distribution by modeling them as a Poisson point process (PPP), and use
tools from stochastic geometry to obtain easy-to-use expressions for key
performance metrics. In particular, we focus on the probability of detecting
some minimum number of BSs, which is shown to be closely coupled with a network
operator's ability to obtain satisfactory localization performance (e.g., meet
FCC E911 requirements). This metric is indifferent to the localization
technique (e.g., TOA, TDOA, AOA, or hybrids thereof), though different
techniques will presumably lead to different BS hearability requirements. In
order to mitigate excessive interference due to the presence of dominant
interferers in the form of other BSs, we incorporate both BS coordination and
frequency reuse in the proposed framework and quantify the resulting
performance gains analytically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06904</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06904</id><created>2015-02-24</created><authors><author><keyname>Sneps-Sneppe</keyname><forenames>Manfred</forenames></author><author><keyname>Namiot</keyname><forenames>Dmitry</forenames></author></authors><title>Smart Socket for Activity Monitoring</title><categories>cs.CY cs.NI</categories><comments>submitted to FRUCT-17 conference (http://fruct.org)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this short paper we consider the problem of monitoring physical activity
in the smart house. The authors suggested a simple device that allows medical
staff and relatives to monitor the activity for older adults living alone. This
sensor monitors the switching-on of electrical devices. The fact of switching
is seen as confirmation of physical activity. It is confirmed by SMS
notifications to observers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06910</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06910</id><created>2015-02-24</created><authors><author><keyname>Centola</keyname><forenames>Damon</forenames></author><author><keyname>Baronchelli</keyname><forenames>Andrea</forenames></author></authors><title>The Spontaneous Emergence of Conventions: An Experimental Study of
  Cultural Evolution</title><categories>physics.soc-ph cs.MA cs.SI q-bio.PE</categories><journal-ref>Proc. Natl. Acad. Sci. USA 112, 1989 (2015)</journal-ref><doi>10.1073/pnas.1418838112</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How do shared conventions emerge in complex decentralized social systems?
This question engages fields as diverse as linguistics, sociology and cognitive
science. Previous empirical attempts to solve this puzzle all presuppose that
formal or informal institutions, such as incentives for global agreement,
coordinated leadership, or aggregated information about the population, are
needed to facilitate a solution. Evolutionary theories of social conventions,
by contrast, hypothesize that such institutions are not necessary in order for
social conventions to form. However, empirical tests of this hypothesis have
been hindered by the difficulties of evaluating the real-time creation of new
collective behaviors in large decentralized populations. Here, we present
experimental results - replicated at several scales - that demonstrate the
spontaneous creation of universally adopted social conventions, and show how
simple changes in a population's network structure can direct the dynamics of
norm formation, driving human populations with no ambition for large scale
coordination to rapidly evolve shared social conventions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06922</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06922</id><created>2015-02-24</created><updated>2016-01-16</updated><authors><author><keyname>Palangi</keyname><forenames>Hamid</forenames></author><author><keyname>Deng</keyname><forenames>Li</forenames></author><author><keyname>Shen</keyname><forenames>Yelong</forenames></author><author><keyname>Gao</keyname><forenames>Jianfeng</forenames></author><author><keyname>He</keyname><forenames>Xiaodong</forenames></author><author><keyname>Chen</keyname><forenames>Jianshu</forenames></author><author><keyname>Song</keyname><forenames>Xinying</forenames></author><author><keyname>Ward</keyname><forenames>Rabab</forenames></author></authors><title>Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis
  and Application to Information Retrieval</title><categories>cs.CL cs.IR cs.LG cs.NE</categories><comments>To appear in IEEE/ACM Transactions on Audio, Speech, and Language
  Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops a model that addresses sentence embedding, a hot topic in
current natural language processing research, using recurrent neural networks
with Long Short-Term Memory (LSTM) cells. Due to its ability to capture long
term memory, the LSTM-RNN accumulates increasingly richer information as it
goes through the sentence, and when it reaches the last word, the hidden layer
of the network provides a semantic representation of the whole sentence. In
this paper, the LSTM-RNN is trained in a weakly supervised manner on user
click-through data logged by a commercial web search engine. Visualization and
analysis are performed to understand how the embedding process works. The model
is found to automatically attenuate the unimportant words and detects the
salient keywords in the sentence. Furthermore, these detected keywords are
found to automatically activate different cells of the LSTM-RNN, where words
belonging to a similar topic activate the same cell. As a semantic
representation of the sentence, the embedding vector can be used in many
different applications. These automatic keyword detection and topic allocation
abilities enabled by the LSTM-RNN allow the network to perform document
retrieval, a difficult language processing task, where the similarity between
the query and documents can be measured by the distance between their
corresponding sentence embedding vectors computed by the LSTM-RNN. On a web
search task, the LSTM-RNN embedding is shown to significantly outperform
several existing state of the art methods. We emphasize that the proposed model
generates sentence embedding vectors that are specially useful for web document
retrieval tasks. A comparison with a well known general sentence embedding
method, the Paragraph Vector, is performed. The results show that the proposed
method in this paper significantly outperforms it for web document retrieval
task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06934</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06934</id><created>2015-02-24</created><updated>2015-04-28</updated><authors><author><keyname>Bhat</keyname><forenames>Satyanath</forenames></author><author><keyname>Jain</keyname><forenames>Shweta</forenames></author><author><keyname>Gujar</keyname><forenames>Sujit</forenames></author><author><keyname>Narahari</keyname><forenames>Y.</forenames></author></authors><title>An Optimal Bidimensional Multi-Armed Bandit Auction for Multi-unit
  Procurement</title><categories>cs.GT</categories><comments>To appear as Extended abstract in AAMAS 2015</comments><acm-class>I.2.11; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of a buyer (aka auctioneer) who gains stochastic rewards
by procuring multiple units of a service or item from a pool of heterogeneous
strategic agents. The reward obtained for a single unit from an allocated agent
depends on the inherent quality of the agent; the agent's quality is fixed but
unknown. Each agent can only supply a limited number of units (capacity of the
agent). The costs incurred per unit and capacities are private information of
the agents. The auctioneer is required to elicit costs as well as capacities
(making the mechanism design bidimensional) and further, learn the qualities of
the agents as well, with a view to maximize her utility. Motivated by this, we
design a bidimensional multi-armed bandit procurement auction that seeks to
maximize the expected utility of the auctioneer subject to incentive
compatibility and individual rationality while simultaneously learning the
unknown qualities of the agents. We first assume that the qualities are known
and propose an optimal, truthful mechanism 2D-OPT for the auctioneer to elicit
costs and capacities. Next, in order to learn the qualities of the agents in
addition, we provide sufficient conditions for a learning algorithm to be
Bayesian incentive compatible and individually rational. We finally design a
novel learning mechanism, 2D-UCB that is stochastic Bayesian incentive
compatible and individually rational.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06938</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06938</id><created>2015-01-07</created><authors><author><keyname>Arghandeh</keyname><forenames>Reza</forenames></author><author><keyname>Gahr</keyname><forenames>Martin</forenames></author><author><keyname>von Meier</keyname><forenames>Alexandra</forenames></author><author><keyname>Cavraro</keyname><forenames>Guido</forenames></author><author><keyname>Ruh</keyname><forenames>Monika</forenames></author><author><keyname>Andersson</keyname><forenames>G&#xf6;ran</forenames></author></authors><title>Topology Detection in Microgrids with Micro-Synchrophasors</title><categories>cs.SY physics.data-an</categories><comments>5 Pages, PESGM2015, Denver, CO</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network topology in distribution networks is often unknown, because most
switches are not equipped with measurement devices and communication links.
However, knowledge about the actual topology is critical for safe and reliable
grid operation. This paper proposes a voting-based topology detection method
based on micro-synchrophasor measurements. The minimal difference between
measured and calculated voltage angle or voltage magnitude, respectively,
indicates the actual topology. Micro-synchrophasors or micro-Phasor Measurement
Units ({\mu}PMU) are high-precision devices that can measure voltage angle
differences on the order of ten millidegrees. This accuracy is important for
distribution networks due to the smaller angle differences as compared to
transmission networks. For this paper, a microgrid test bed is implemented in
MATLAB with simulated measurements from {\mu}PMUs as well as SCADA measurement
devices. The results show that topologies can be detected with high accuracy.
Additionally, topology detection by voltage angle shows better results than
detection by voltage magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06945</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06945</id><created>2015-02-24</created><authors><author><keyname>Kaya</keyname><forenames>Abidin</forenames></author><author><keyname>T&#xfc;fek&#xe7;i</keyname><forenames>Nesibe</forenames></author></authors><title>New extremal binary self-dual codes of lengths 66 and 68 from codes over
  r_k,m</title><categories>cs.IT math.IT</categories><msc-class>94B05, 94B60, 94B65</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, four circulant and quadratic double circulant (QDC)
constructions are applied to the family of the rings R_k,m. Self-dual binary
codes are obtained as the Gray images of self-dual QDC codes over R_k,m.
Extremal binary self-dual codes of length 64 are obtained as Gray images of
?-four circulant codes over R_2,1 and R_2,2. Extremal binary self-dual codes of
lengths 66 and 68 are constructed by applying extension theorems to the F_2 and
R_2,1 images of these codes. More precisely, 11 new codes of length 66 and 39
new codes of length 68 are discovered. The codes with these weight enumerators
are constructed for the ?first time in literature. The results are tabulated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06948</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06948</id><created>2015-02-24</created><updated>2015-09-28</updated><authors><author><keyname>Brandst&#xe4;dt</keyname><forenames>Andreas</forenames></author><author><keyname>Dabrowski</keyname><forenames>Konrad K.</forenames></author><author><keyname>Huang</keyname><forenames>Shenwei</forenames></author><author><keyname>Paulusma</keyname><forenames>Dani&#xeb;l</forenames></author></authors><title>Bounding the Clique-Width of $H$-free Chordal Graphs</title><categories>cs.DM math.CO</categories><comments>32 pages, 10 figures. An extended abstract of this paper appeared in
  the proceedings of MFCS 2015</comments><msc-class>05C75</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph is $H$-free if it has no induced subgraph isomorphic to $H$.
Brandst\&quot;adt, Engelfriet, Le and Lozin proved that the class of chordal graphs
with independence number at most 3 has unbounded clique-width. Brandst\&quot;adt, Le
and Mosca erroneously claimed that the gem and the co-gem are the only two
1-vertex $P_4$-extensions $H$ for which the class of $H$-free chordal graphs
has bounded clique-width. In fact we prove that bull-free chordal and
co-chair-free chordal graphs have clique-width at most 3 and 4, respectively.
In particular, we find four new classes of $H$-free chordal graphs of bounded
clique-width. Our main result, obtained by combining new and known results,
provides a classification of all but two stubborn cases, that is, with two
potential exceptions we determine all graphs $H$ for which the class of
$H$-free chordal graphs has bounded clique-width. We illustrate the usefulness
of this classification for classifying other types of graph classes by proving
that the class of $(2P_1+P_3,K_4)$-free graphs has bounded clique-width via a
reduction to $K_4$-free chordal graphs. Finally, we give a complete
classification of the (un)boundedness of clique-width of $H$-free weakly
chordal graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06956</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06956</id><created>2015-02-23</created><authors><author><keyname>Deng</keyname><forenames>Xinyang</forenames></author><author><keyname>Deng</keyname><forenames>Yong</forenames></author></authors><title>Transformation of basic probability assignments to probabilities based
  on a new entropy measure</title><categories>cs.AI</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dempster-Shafer evidence theory is an efficient mathematical tool to deal
with uncertain information. In that theory, basic probability assignment (BPA)
is the basic element for the expression and inference of uncertainty.
Decision-making based on BPA is still an open issue in Dempster-Shafer evidence
theory. In this paper, a novel approach of transforming basic probability
assignments to probabilities is proposed based on Deng entropy which is a new
measure for the uncertainty of BPA. The principle of the proposed method is to
minimize the difference of uncertainties involving in the given BPA and
obtained probability distribution. Numerical examples are given to show the
proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06967</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06967</id><created>2015-02-24</created><updated>2015-03-22</updated><authors><author><keyname>Chubb</keyname><forenames>Christopher T.</forenames></author><author><keyname>Flammia</keyname><forenames>Steven T.</forenames></author></authors><title>Computing the Degenerate Ground Space of Gapped Spin Chains in
  Polynomial Time</title><categories>quant-ph cond-mat.str-el cs.DS</categories><comments>32 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Given a gapped Hamiltonian of a spin chain, we give a polynomial-time
algorithm for finding the degenerate ground space projector. The output is an
orthonormal set of matrix product states that approximate the true ground space
projector up to an inverse polynomial error in any Schatten norm, with a
runtime exponential in the degeneracy. Our algorithm is an extension of the
recent algorithm of Landau, Vazirani, and Vidick for the nondegenerate case,
and it includes the recent improvements due to Huang. The main new idea is to
incorporate the local distinguishability of ground states on the half-chain to
ensure that the algorithm returns a complete set of global ground states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.06993</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.06993</id><created>2015-02-24</created><authors><author><keyname>Chergui</keyname><forenames>Rachid</forenames></author></authors><title>Privacy preserving distributed profile matching in mobile social network</title><categories>cs.CR cs.IT math.IT</categories><msc-class>62B10, 62B99</msc-class><acm-class>F.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this document, a privacy-preserving distributed profile matching protocol
is proposed in a particular network context called \emph{mobile social
network}. Such networks are often deployed in more or less hostile
environments, requiring rigorous security mechanisms. In the same time, energy
and computational resources are limited as these heterogeneous networks are
frequently constituted by wireless components like tablets or mobile phones.
This is why a new encryption algorithm having an high level of security while
preserving resources is proposed in this paper. The approach is based on
elliptic curve cryptography, more specifically on an almost completely
homomorphic cryptosystem over a supersingular elliptic curve, leading to a
secure and efficient preservation of privacy in distributed profile matching.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07014</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07014</id><created>2015-02-24</created><authors><author><keyname>Currie</keyname><forenames>James D.</forenames></author><author><keyname>Rampersad</keyname><forenames>Narad</forenames></author></authors><title>Growth rate of binary words avoiding $xxx^R$</title><categories>cs.FL</categories><msc-class>68R15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the set of those binary words with no non-empty factors of the form
$xxx^R$. Du, Mousavi, Schaeffer, and Shallit asked whether this set of words
grows polynomially or exponentially with length. In this paper, we demonstrate
the existence of upper and lower bounds on the number of such words of length
$n$, where each of these bounds is asymptotically equivalent to a (different)
function of the form $Cn^{\lg n+c}$, where $C$, $c$ are constants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07015</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07015</id><created>2015-02-24</created><authors><author><keyname>Dinh</keyname><forenames>Thanh-Cong</forenames></author><author><keyname>Bae</keyname><forenames>Hyerim</forenames></author><author><keyname>Park</keyname><forenames>Jaehun</forenames></author><author><keyname>Bae</keyname><forenames>Joonsoo</forenames></author></authors><title>A framework to discover potential ideas of new product development from
  crowdsourcing application</title><categories>cs.IR</categories><comments>International Conference on Computer, Networks, Systems, and
  Industrial Applications (CNSI 2012), Jeju Island, Korea, July 16-18, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study idea mining from crowdsourcing applications which
encourage a group of people, who are usually undefined and very large sized, to
generate ideas for new product development (NPD). In order to isolate the
relatively small number of potential ones among ideas from crowd, decision
makers not only have to identify the key textual information representing the
ideas, but they also need to consider online opinions of people who gave
comments and votes on the ideas. Due to the extremely large size of text data
generated by people on the Internet, identifying textual information has been
carried out in manual ways, and has been considered very time consuming and
costly. To overcome the ineffectiveness, this paper introduces a novel
framework that can help decision makers discover ideas having the potential to
be used in an NPD process. To achieve this, a semi-automatic text mining
technique that retrieves useful text patterns from ideas posted on
crowdsourcing application is proposed. Then, we provide an online learning
algorithm to evaluate whether the idea is potential or not. Finally to verify
the effectiveness of our algorithm, we conducted experiments on the data, which
are collected from an existing crowd sourcing website.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07016</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07016</id><created>2015-02-24</created><authors><author><keyname>Brunson</keyname><forenames>Jason Cory</forenames></author></authors><title>Triadic analysis of affiliation networks</title><categories>math.CO cs.SI physics.soc-ph</categories><comments>37 pages, 19 figures, 5 tables</comments><journal-ref>Network Science / Volume 3 / Issue 04 / December 2015, pp 480-508</journal-ref><doi>10.1017/nws.2015.38</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The widely-used notion of triadic closure has been conceptualized and
measured in a variety of ways, most famously the clustering coefficient. This
paper proposes a measure of triadic closure in affiliation networks designed to
control for the influence of bicliques. In order to avoid arbitrariness, the
paper introduces a triadic framework for affiliation networks, within which a
range of possible statistics can be considered; it then takes an axiomatic
approach to narrowing this range. The paper conducts an instrumental assessment
of the proposed statistic alongside two previous proposals, for reliability,
validity, and usefulness. Finally, these tools demonstrate their collective
applicability in a multi-perspective investigation into triadic closure in
several empirical social networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07019</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07019</id><created>2015-02-24</created><authors><author><keyname>Daftry</keyname><forenames>Shreyansh</forenames></author><author><keyname>Hoppe</keyname><forenames>Christof</forenames></author><author><keyname>Bischof</keyname><forenames>Horst</forenames></author></authors><title>Building with Drones: Accurate 3D Facade Reconstruction using MAVs</title><categories>cs.RO cs.AI cs.CV</categories><comments>8 Pages, 2015 IEEE International Conference on Robotics and
  Automation (ICRA '15), Seattle, WA, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic reconstruction of 3D models from images using multi-view
Structure-from-Motion methods has been one of the most fruitful outcomes of
computer vision. These advances combined with the growing popularity of Micro
Aerial Vehicles as an autonomous imaging platform, have made 3D vision tools
ubiquitous for large number of Architecture, Engineering and Construction
applications among audiences, mostly unskilled in computer vision. However, to
obtain high-resolution and accurate reconstructions from a large-scale object
using SfM, there are many critical constraints on the quality of image data,
which often become sources of inaccuracy as the current 3D reconstruction
pipelines do not facilitate the users to determine the fidelity of input data
during the image acquisition. In this paper, we present and advocate a
closed-loop interactive approach that performs incremental reconstruction in
real-time and gives users an online feedback about the quality parameters like
Ground Sampling Distance (GSD), image redundancy, etc on a surface mesh. We
also propose a novel multi-scale camera network design to prevent scene drift
caused by incremental map building, and release the first multi-scale image
sequence dataset as a benchmark. Further, we evaluate our system on real
outdoor scenes, and show that our interactive pipeline combined with a
multi-scale camera network approach provides compelling accuracy in multi-view
reconstruction tasks when compared against the state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07026</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07026</id><created>2015-02-24</created><authors><author><keyname>Ranganath</keyname><forenames>Suhas</forenames></author><author><keyname>Thiagarajan</keyname><forenames>JJ</forenames></author><author><keyname>Ramamurthy</keyname><forenames>KN</forenames></author><author><keyname>Hu</keyname><forenames>Shuang</forenames></author><author><keyname>Banavar</keyname><forenames>Mahesh</forenames></author><author><keyname>Spanias</keyname><forenames>Andreas</forenames></author></authors><title>Undergraduate Signal Processing Laboratories for the Android Operating
  System</title><categories>cs.CY</categories><comments>ASEE (American Society for Engineering Education) 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a DSP simulation environment that will enable students to perform
laboratory exercises using Android mobile devices and tablets. Due to the
pervasive nature of the mobile technology, education applications designed for
mobile devices have the potential to stimulate student interest in addition to
offering convenient access and interaction capabilities. This paper describes a
portable signal processing laboratory for the Android platform. This software
is intended to be an educational tool for students and instructors in DSP, and
signals and systems courses. The development of Android JDSP (A-JDSP) is
carried out using the Android SDK, which is a Java-based open source
development platform. The proposed application contains basic DSP functions for
convolution, sampling, FFT, filtering and frequency domain analysis, with a
convenient graphical user interface. A description of the architecture,
functions and planned assessments are presented in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07027</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07027</id><created>2015-02-24</created><authors><author><keyname>Felber</keyname><forenames>David</forenames></author><author><keyname>Ostrovsky</keyname><forenames>Rafail</forenames></author></authors><title>Variability in data streams</title><categories>cs.DS</categories><comments>submitted to ICALP 2015 (here, fullpage formatting)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of tracking with small relative error an integer
function $f(n)$ defined by a distributed update stream $f'(n)$. Existing
streaming algorithms with worst-case guarantees for this problem assume $f(n)$
to be monotone; there are very large lower bounds on the space requirements for
summarizing a distributed non-monotonic stream, often linear in the size $n$ of
the stream.
  Input streams that give rise to large space requirements are highly variable,
making relatively large jumps from one timestep to the next. However, streams
often vary slowly in practice. What has heretofore been lacking is a framework
for non-monotonic streams that admits algorithms whose worst-case performance
is as good as existing algorithms for monotone streams and degrades gracefully
for non-monotonic streams as those streams vary more quickly.
  In this paper we propose such a framework. We introduce a new stream
parameter, the &quot;variability&quot; $v$, deriving its definition in a way that shows
it to be a natural parameter to consider for non-monotonic streams. It is also
a useful parameter. From a theoretical perspective, we can adapt existing
algorithms for monotone streams to work for non-monotonic streams, with only
minor modifications, in such a way that they reduce to the monotone case when
the stream happens to be monotone, and in such a way that we can refine the
worst-case communication bounds from $\Theta(n)$ to $\tilde{O}(v)$. From a
practical perspective, we demonstrate that $v$ can be small in practice by
proving that $v$ is $O(\log f(n))$ for monotone streams and $o(n)$ for streams
that are &quot;nearly&quot; monotone or that are generated by random walks. We expect $v$
to be $o(n)$ for many other interesting input classes as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07038</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07038</id><created>2015-02-24</created><authors><author><keyname>Ng</keyname><forenames>Dominick</forenames></author><author><keyname>Bansal</keyname><forenames>Mohit</forenames></author><author><keyname>Curran</keyname><forenames>James R.</forenames></author></authors><title>Web-scale Surface and Syntactic n-gram Features for Dependency Parsing</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop novel first- and second-order features for dependency parsing
based on the Google Syntactic Ngrams corpus, a collection of subtree counts of
parsed sentences from scanned books. We also extend previous work on surface
$n$-gram features from Web1T to the Google Books corpus and from first-order to
second-order, comparing and analysing performance over newswire and web
treebanks.
  Surface and syntactic $n$-grams both produce substantial and complementary
gains in parsing accuracy across domains. Our best system combines the two
feature sets, achieving up to 0.8% absolute UAS improvements on newswire and
1.4% on web text.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07041</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07041</id><created>2015-02-24</created><authors><author><keyname>Ahmad</keyname><forenames>Jamil</forenames></author><author><keyname>Sajjad</keyname><forenames>Muhammad</forenames></author><author><keyname>Mehmood</keyname><forenames>Irfan</forenames></author><author><keyname>Rho</keyname><forenames>Seungmin</forenames></author><author><keyname>Baik</keyname><forenames>Sung Wook</forenames></author></authors><title>Describing Colors, Textures and Shapes for Content Based Image Retrieval
  - A Survey</title><categories>cs.IR cs.CV</categories><journal-ref>(2014), Journal of Platform Technology 2(4): 34-48</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Visual media has always been the most enjoyed way of communication. From the
advent of television to the modern day hand held computers, we have witnessed
the exponential growth of images around us. Undoubtedly it's a fact that they
carry a lot of information in them which needs be utilized in an effective
manner. Hence intense need has been felt to efficiently index and store large
image collections for effective and on- demand retrieval. For this purpose
low-level features extracted from the image contents like color, texture and
shape has been used. Content based image retrieval systems employing these
features has proven very successful. Image retrieval has promising applications
in numerous fields and hence has motivated researchers all over the world. New
and improved ways to represent visual content are being developed each day.
Tremendous amount of research has been carried out in the last decade. In this
paper we will present a detailed overview of some of the powerful color,
texture and shape descriptors for content based image retrieval. A comparative
analysis will also be carried out for providing an insight into outstanding
challenges in this field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07045</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07045</id><created>2015-02-24</created><updated>2015-05-21</updated><authors><author><keyname>Francis</keyname><forenames>Andrew R.</forenames></author><author><keyname>Steel</keyname><forenames>Mike</forenames></author></authors><title>Which phylogenetic networks are merely trees with additional arcs?</title><categories>q-bio.PE cs.DS</categories><comments>The final version of this article will appear in Systematic Biology.
  20 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A binary phylogenetic network may or may not be obtainable from a tree by the
addition of directed edges (arcs) between tree arcs. Here, we establish a
precise and easily tested criterion (based on `2-SAT') that efficiently
determines whether or not any given network can be realized in this way.
Moreover, the proof provides a polynomial-time algorithm for finding one or
more trees (when they exist) on which the network can be based. A number of
interesting consequences are presented as corollaries; these lead to some
further relevant questions and observations, which we outline in the
conclusion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07055</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07055</id><created>2015-02-25</created><authors><author><keyname>Mukherjee</keyname><forenames>Atin</forenames></author><author><keyname>Sinha</keyname><forenames>Amitabha</forenames></author><author><keyname>Choudhury</keyname><forenames>Debesh</forenames></author></authors><title>A Novel Architecture of Area Efficient FFT Algorithm for FPGA
  Implementation</title><categories>cs.AR</categories><comments>6 pages, 10 figures; Accepted in ACM SIGARCH Computer Architecture
  News, December 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fast Fourier transform (FFT) of large number of samples requires huge
hardware resources of field programmable gate arrays (FPGA), which needs more
area and power. In this paper, we present an area efficient architecture of FFT
processor that reuses the butterfly elements several times. The FFT processor
is simulated using VHDL and the results are validated on a Virtex-6 FPGA. The
proposed architecture outperforms the conventional architecture of a $N$-point
FFT processor in terms of area which is reduced by a factor of $log_N 2$ with
negligible increase in processing time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07058</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07058</id><created>2015-02-25</created><authors><author><keyname>Harley</keyname><forenames>Adam W.</forenames></author><author><keyname>Ufkes</keyname><forenames>Alex</forenames></author><author><keyname>Derpanis</keyname><forenames>Konstantinos G.</forenames></author></authors><title>Evaluation of Deep Convolutional Nets for Document Image Classification
  and Retrieval</title><categories>cs.CV cs.IR cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new state-of-the-art for document image classification
and retrieval, using features learned by deep convolutional neural networks
(CNNs). In object and scene analysis, deep neural nets are capable of learning
a hierarchical chain of abstraction from pixel inputs to concise and
descriptive representations. The current work explores this capacity in the
realm of document analysis, and confirms that this representation strategy is
superior to a variety of popular hand-crafted alternatives. Experiments also
show that (i) features extracted from CNNs are robust to compression, (ii) CNNs
trained on non-document images transfer well to document analysis tasks, and
(iii) enforcing region-specific feature-learning is unnecessary given
sufficient training data. This work also makes available a new labelled subset
of the IIT-CDIP collection, containing 400,000 document images across 16
categories, useful for training new CNNs for document analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07063</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07063</id><created>2015-02-25</created><authors><author><keyname>Mohjazi</keyname><forenames>Lina</forenames></author><author><keyname>Dianati</keyname><forenames>Mehrdad</forenames></author><author><keyname>Karagiannidis</keyname><forenames>George K.</forenames></author><author><keyname>Muhaidat</keyname><forenames>Sami</forenames></author><author><keyname>Al-Qutayri</keyname><forenames>Mahmoud</forenames></author></authors><title>RF-Powered Cognitive Radio Networks: Technical Challenges and
  Limitations</title><categories>cs.IT math.IT</categories><comments>8 pages, 2 figures, 1 table, Accepted in IEEE Communications Magazine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing demand for spectral and energy efficient communication
networks has spurred a great interest in energy harvesting (EH) cognitive radio
networks (CRNs). Such a revolutionary technology represents a paradigm shift in
the development of wireless networks, as it can simultaneously enable the
efficient use of the available spectrum and the exploitation of radio frequency
(RF) energy in order to reduce the reliance on traditional energy sources. This
is mainly triggered by the recent advancements in microelectronics that puts
forward RF energy harvesting as a plausible technique in the near future. On
the other hand, it is suggested that the operation of a network relying on
harvested energy needs to be redesigned to allow the network to reliably
function in the long term. To this end, the aim of this survey paper is to
provide a comprehensive overview of the recent development and the challenges
regarding the operation of CRNs powered by RF energy. In addition, the
potential open issues that might be considered for the future research are also
discussed in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07066</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07066</id><created>2015-02-25</created><updated>2015-10-21</updated><authors><author><keyname>Xing</keyname><forenames>Hong</forenames></author><author><keyname>Wong</keyname><forenames>Kai-Kit</forenames></author><author><keyname>Chu</keyname><forenames>Zheng</forenames></author><author><keyname>Nallanathan</keyname><forenames>Arumugam</forenames></author></authors><title>To Harvest and Jam: A Paradigm of Self-Sustaining Friendly Jammers for
  Secure AF Relaying</title><categories>cs.IT math.IT</categories><comments>16 pages (double column), 8 figures, submitted for possible journal
  publication</comments><doi>10.1109/TSP.2015.2477800</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the use of multi-antenna harvest-and-jam (HJ) helpers in a
multi-antenna amplify-and-forward (AF) relay wiretap channel assuming that the
direct link between the source and destination is broken. Our objective is to
maximize the secrecy rate at the destination subject to the transmit power
constraints of the AF relay and the HJ helpers. In the case of perfect channel
state information (CSI), the joint optimization of the artificial noise (AN)
covariance matrix for cooperative jamming and the AF beamforming matrix is
studied using semi-definite relaxation (SDR) which is tight, while suboptimal
solutions are also devised with lower complexity. For the imperfect CSI case,
we provide the equivalent reformulation of the worst-case robust optimization
to maximize the minimum achievable secrecy rate. Inspired by the optimal
solution to the case of perfect CSI, a suboptimal robust scheme is proposed
striking a good tradeoff between complexity and performance. Finally, numerical
results for various settings are provided to evaluate the proposed schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07073</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07073</id><created>2015-02-25</created><updated>2015-06-19</updated><authors><author><keyname>Daniely</keyname><forenames>Amit</forenames></author><author><keyname>Gonen</keyname><forenames>Alon</forenames></author><author><keyname>Shalev-Shwartz</keyname><forenames>Shai</forenames></author></authors><title>Strongly Adaptive Online Learning</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Strongly adaptive algorithms are algorithms whose performance on every time
interval is close to optimal. We present a reduction that can transform
standard low-regret algorithms to strongly adaptive. As a consequence, we
derive simple, yet efficient, strongly adaptive algorithms for a handful of
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07085</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07085</id><created>2015-02-25</created><authors><author><keyname>Sardroud</keyname><forenames>Asghar Asgharian</forenames></author><author><keyname>Bagheri</keyname><forenames>Alireza</forenames></author></authors><title>An approximation algorithm for the longest cycle problem in solid grid
  graphs</title><categories>cs.DS cs.DM math.CO</categories><comments>11 pages, 6 figures</comments><msc-class>68R10, 05C85</msc-class><acm-class>G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although, the Hamiltonicity of solid grid graphs are polynomial-time
decidable, the complexity of the longest cycle problem in these graphs is still
open. In this paper, by presenting a linear-time constant-factor approximation
algorithm, we show that the longest cycle problem in solid grid graphs is in
APX. More precisely, our algorithm finds a cycle of length at least
$\frac{2n}{3}+1$ in 2-connected $n$-node solid grid graphs.
  Keywords: Longest cycle, Hamiltonian cycle, Approximation algorithm, Solid
grid graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07106</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07106</id><created>2015-02-25</created><authors><author><keyname>Metwalley</keyname><forenames>Hassan</forenames></author><author><keyname>Traverso</keyname><forenames>Stefano</forenames></author><author><keyname>Mellia</keyname><forenames>Marco</forenames></author><author><keyname>Miskovic</keyname><forenames>Stanislav</forenames></author><author><keyname>Baldi</keyname><forenames>Mario</forenames></author></authors><title>CrowdSurf: Empowering Informed Choices in the Web</title><categories>cs.CY cs.CR cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When surfing the Internet, individuals leak personal and corporate
information to third parties whose (legitimate or not) businesses revolve
around the value of collected data. The implications are serious, from a person
unwillingly exposing private information to an unknown third party, to a
company unable to manage the flow of its information to the outside world. The
point is that individuals and companies are more and more kept out of the loop
when it comes to control private data. With the goal of empowering informed
choices in information leakage through the Internet, we propose CROWDSURF, a
system for comprehensive and collaborative auditing of data that flows to
Internet services. Similarly to open-source efforts, we enable users to
contribute in building awareness and control over privacy and communication
vulnerabilities. CROWDSURF provides the core infrastructure and algorithms to
let individuals and enterprises regain control on the information exposed on
the web. We advocate CROWDSURF as a data processing layer positioned right
below HTTP in the host protocol stack. This enables the inspection of
clear-text data even when HTTPS is deployed and the application of processing
rules that are customizable to fit any need. Preliminary results obtained
executing a prototype implementation on ISP traffic traces demonstrate the
feasibility of CROWDSURF.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07118</identifier>
 <datestamp>2015-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07118</id><created>2015-02-25</created><updated>2015-11-25</updated><authors><author><keyname>Haas</keyname><forenames>Andreas</forenames></author><author><keyname>Henzinger</keyname><forenames>Thomas A.</forenames></author><author><keyname>Holzer</keyname><forenames>Andreas</forenames></author><author><keyname>Kirsch</keyname><forenames>Christoph M.</forenames></author><author><keyname>Lippautz</keyname><forenames>Michael</forenames></author><author><keyname>Payer</keyname><forenames>Hannes</forenames></author><author><keyname>Sezgin</keyname><forenames>Ali</forenames></author><author><keyname>Sokolova</keyname><forenames>Ana</forenames></author><author><keyname>Veith</keyname><forenames>Helmut</forenames></author></authors><title>Local Linearizability</title><categories>cs.PL cs.DC cs.DS</categories><acm-class>D.3.1; E.1; D.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The semantics of concurrent data structures is usually given by a sequential
specification and a consistency condition. Linearizability is the most popular
consistency condition due to its simplicity and general applicability. Then
again, linearizability is known to require extensive synchronization which may
jeopardize performance and scalability. For applications that do not require
all guarantees offered by linearizability, recent research has therefore
focused on improving performance and scalability by relaxing the semantics of
concurrent data structures.
  In this paper, we present local linearizability, a relaxed consistency
condition that is applicable to pools, queues, stacks, and many other
container-type concurrent data structures. While linearizability requires that
the effect of each operation is observed by all threads at the same time, local
linearizability only requires that for each thread T, the effects of its local
insertion operations and the effects of those removal operations that remove
values inserted by T are observed by all threads at the same time. Local
linearizability is strictly weaker than linearizability on most containers
including pools, queues, and stacks.
  We present a generic and easy method for implementing locally linearizable
data structures using existing linearizable data structures as building blocks.
In our experiments, our implementations show improvements in performance and
scalability compared to the original linearizable building blocks. Moreover,
our locally linearizable implementations outperform the fastest existing
container-type implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07120</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07120</id><created>2015-02-25</created><authors><author><keyname>Hermanns</keyname><forenames>Holger</forenames></author><author><keyname>Kr&#x10d;&#xe1;l</keyname><forenames>Jan</forenames></author><author><keyname>Nies</keyname><forenames>Gilles</forenames></author></authors><title>Recharging Probably Keeps Batteries Alive</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The kinetic battery model is a popular model of the dynamic behavior of a
conventional battery, useful to predict or optimize the time until battery
depletion. The model however lacks certain obvious aspects of batteries
in-the-wild, especially with respect to (i) the effects of random influences
and (ii) the behavior when charging up to capacity bounds. This paper considers
the kinetic battery model with bounded capacity in the context of piecewise
constant yet random charging and discharging. The resulting model enables the
time-dependent evaluation of the risk of battery depletion. This is exemplified
in a power dependability study of a nano satellite mission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07123</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07123</id><created>2015-02-25</created><authors><author><keyname>Hao</keyname><forenames>Chenxi</forenames></author><author><keyname>Clerckx</keyname><forenames>Bruno</forenames></author></authors><title>Degrees-of-Freedom of the K-User MISO Interference Channel with Delayed
  Local CSIT</title><categories>cs.IT math.IT</categories><comments>6 pages, 2 figures, accepted by IEEE ICC'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a K-user Multiple-Input-Single-Output (MISO)
Interference Channel (IC), where the channel state information obtained by the
transmitters (CSIT) is perfect, but completely outdated. A Retrospective
Interference Alignment (RIA) using such delayed CSIT was proposed by the
authors of [1] for the MISO Broadcast Channel (BC), but the extension to the
MISO IC is a non-trivial step as each transmitter only has the message intended
for the corresponding user. Recently, [7] focused on a
Single-Input-Single-Output (SISO) IC and solved such bottleneck by inventing a
distributed higher order symbol generation. Our main work is to extend [7] to
the MISO case by integrating some features of the scheme proposed in [1]. The
achieved sum Degrees-of-Freedom (DoF) performance is asymptotically given by
64/15 when $K\to\infty$, outperforming all the previously known results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07133</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07133</id><created>2015-02-25</created><authors><author><keyname>Hadjioannou</keyname><forenames>Vasos</forenames></author></authors><title>On the Performance comparison of RIP, OSPF, IS-IS and EIGRP routing
  protocols</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, routing protocols have become a crucial part of the modern
communication networks. A routing protocol's responsibility lies in determining
the way routers communicate with each other in order to forward any kind of
packets, from a source to a destination, using the optimal path that would
provide the most efficiency. There are many routing protocols out there today,
some old and some new, but all are used for the same purpose. In general, to
ideally select routes between any two nodes on a computer network and
disseminate information. This paper takes into consideration four of such
routing protocols (RIP, OSPF, IS-IS and EIGRP), expresses them and analyzes
their way of operation. It also presents the results of a simulation, that took
place for the sole purpose of studying the behavior of those four protocols,
under the same circumstances, as well as the evaluation of the comparison with
one another based on the results of the simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07143</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07143</id><created>2015-02-25</created><authors><author><keyname>Herbster</keyname><forenames>Mark</forenames></author><author><keyname>Rubenstein</keyname><forenames>Paul</forenames></author><author><keyname>Townsend</keyname><forenames>James</forenames></author></authors><title>The VC-Dimension of Similarity Hypotheses Spaces</title><categories>cs.LG</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set $X$ and a function $h:X\longrightarrow\{0,1\}$ which labels each
element of $X$ with either $0$ or $1$, we may define a function $h^{(s)}$ to
measure the similarity of pairs of points in $X$ according to $h$.
Specifically, for $h\in \{0,1\}^X$ we define $h^{(s)}\in \{0,1\}^{X\times X}$
by $h^{(s)}(w,x):= \mathbb{1}[h(w) = h(x)]$. This idea can be extended to a set
of functions, or hypothesis space $\mathcal{H} \subseteq \{0,1\}^X$ by defining
a similarity hypothesis space $\mathcal{H}^{(s)}:=\{h^{(s)}:h\in\mathcal{H}\}$.
We show that ${{vc-dimension}}(\mathcal{H}^{(s)}) \in
\Theta({{vc-dimension}}(\mathcal{H}))$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07157</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07157</id><created>2015-02-25</created><updated>2015-02-26</updated><authors><author><keyname>Marteau</keyname><forenames>Pierre-Fran&#xe7;ois</forenames><affiliation>IRISA</affiliation></author><author><keyname>Ke</keyname><forenames>Guiyao</forenames><affiliation>IRISA</affiliation></author></authors><title>Exploiting a comparability mapping to improve bi-lingual data
  categorization: a three-mode data analysis perspective</title><categories>cs.IR cs.CL</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address in this paper the co-clustering and co-classification of bilingual
data laying in two linguistic similarity spaces when a comparability measure
defining a mapping between these two spaces is available. A new approach that
we can characterized as a three-mode analysis scheme, is proposed to mix the
comparability measure with the two similarity measures. Our aim is to improve
jointly the accuracy of classification and clustering tasks performed in each
of the two linguistic spaces, as well as the quality of the final alignment of
comparable clusters that can be obtained. We used first some purely synthetic
random data sets to assess our formal similarity-comparability mixing model. We
then propose two variants of the comparability measure that has been defined by
(Li and Gaussier 2010) in the context of bilingual lexicon extraction to adapt
it to clustering or categorizing tasks. These two variant measures are
subsequently used to evaluate our similarity-comparability mixing model in the
context of the co-classification and co-clustering of comparable textual data
sets collected from Wikipedia categories for the English and French languages.
Our experiments show clear improvements in clustering and classification
accuracies when mixing comparability with similarity measures, with, as
expected, a higher robustness obtained when the two comparability variant
measures that we propose are used. We believe that this approach is
particularly well suited for the construction of thematic comparable corpora of
controllable quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07162</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07162</id><created>2015-02-25</created><updated>2015-10-28</updated><authors><author><keyname>Nikolov</keyname><forenames>Dimitar</forenames></author><author><keyname>Oliveira</keyname><forenames>Diego F. M.</forenames></author><author><keyname>Flammini</keyname><forenames>Alessandro</forenames></author><author><keyname>Menczer</keyname><forenames>Filippo</forenames></author></authors><title>Measuring Online Social Bubbles</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social media have quickly become a prevalent channel to access information,
spread ideas, and influence opinions. However, it has been suggested that
social and algorithmic filtering may cause exposure to less diverse points of
view, and even foster polarization and misinformation. Here we explore and
validate this hypothesis quantitatively for the first time, at the collective
and individual levels, by mining three massive datasets of web traffic, search
logs, and Twitter posts. Our analysis shows that collectively, people access
information from a significantly narrower spectrum of sources through social
media and email, compared to search. The significance of this finding for
individual exposure is revealed by investigating the relationship between the
diversity of information sources experienced by users at the collective and
individual level. There is a strong correlation between collective and
individual diversity, supporting the notion that when we use social media we
find ourselves inside &quot;social bubbles&quot;. Our results could lead to a deeper
understanding of how technology biases our exposure to new information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07167</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07167</id><created>2015-02-25</created><authors><author><keyname>Oseledets</keyname><forenames>I. V.</forenames></author><author><keyname>Ovchinnikov</keyname><forenames>G. V.</forenames></author><author><keyname>Katrutsa</keyname><forenames>A. M.</forenames></author></authors><title>Linear complexity SimRank computation based on the iterative diagonal
  estimation</title><categories>cs.DS cs.NA math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a deterministic linear time complexity IDE-SimRank method
to approximately compute SimRank with proved error bound. SimRank is a
well-known similarity measure between graph vertices which relies on graph
topology only and is built on intuition that &quot;two objects are similar if they
are related to similar objects&quot;. The fixed point equation for direct SimRank
computation is the discrete Lyapunov equation with specific diagonal matrix in
the right hand side. The proposed method is based on estimation of this
diagonal matrix with GMRES and use this estimation to compute singe-source and
single pairs queries. These computations are executed with the part of series
converging to the discrete Lyapunov equation solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07169</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07169</id><created>2015-02-25</created><updated>2015-11-02</updated><authors><author><keyname>Roediger</keyname><forenames>Wolf</forenames></author><author><keyname>Muehlbauer</keyname><forenames>Tobias</forenames></author><author><keyname>Kemper</keyname><forenames>Alfons</forenames></author><author><keyname>Neumann</keyname><forenames>Thomas</forenames></author></authors><title>High-Speed Query Processing over High-Speed Networks</title><categories>cs.DB cs.DC</categories><comments>12 pages, accepted at VLDB 2016</comments><acm-class>H.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern database clusters entail two levels of networks: connecting CPUs and
NUMA regions inside a single server in the small and multiple servers in the
large. The huge performance gap between these two types of networks used to
slow down distributed query processing to such an extent that a cluster of
machines actually performed worse than a single many-core server. The increased
main-memory capacity of the cluster remained the sole benefit of such a
scale-out.
  The economic viability of high-speed interconnects such as InfiniBand has
narrowed this performance gap considerably. However, InfiniBand's higher
network bandwidth alone does not improve query performance as expected when the
distributed query engine is left unchanged. The scalability of distributed
query processing is impaired by TCP overheads, switch contention due to
uncoordinated communication, and load imbalances resulting from the
inflexibility of the classic exchange operator model. This paper presents the
blueprint for a distributed query engine that addresses these problems by
considering both levels of networks holistically. It consists of two parts:
First, hybrid parallelism that distinguishes local and distributed parallelism
for better scalability in both the number of cores as well as servers. Second,
a novel communication multiplexer tailored for analytical database workloads
using remote direct memory access (RDMA) and low-latency network scheduling for
high-speed communication with almost no CPU overhead. An extensive evaluation
within the HyPer database system using the TPC-H benchmark shows that our
holistic approach indeed enables high-speed query processing over high-speed
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07190</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07190</id><created>2015-02-25</created><updated>2015-10-16</updated><authors><author><keyname>Tan</keyname><forenames>Linda S. L.</forenames></author><author><keyname>Chan</keyname><forenames>Aik Hui</forenames></author><author><keyname>Zheng</keyname><forenames>Tian</forenames></author></authors><title>Topic-adjusted visibility metric for scientific articles</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Measuring the impact of scientific articles is important for evaluating the
research output of individual scientists, academic institutions and journals.
While citations are raw data for constructing impact measures, there exist
biases and potential issues if factors affecting citation patterns are not
properly accounted for. In this work, we address the problem of field variation
and introduce an article level metric useful for evaluating individual
articles' visibility. This measure derives from joint probabilistic modeling of
the content in the articles and the citations amongst them using latent
Dirichlet allocation (LDA) and the mixed membership stochastic blockmodel
(MMSB). Our proposed model provides a visibility metric for individual articles
adjusted for field variation in citation rates, a structural understanding of
citation behavior in different fields, and article recommendations which take
into account article visibility and citation patterns. We develop an efficient
algorithm for model fitting using variational methods. To scale up to large
networks, we develop an online variant using stochastic gradient methods and
case-control likelihood approximation. We apply our methods to the benchmark
KDD Cup 2003 dataset with approximately 30,000 high energy physics papers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07191</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07191</id><created>2015-02-25</created><updated>2015-10-22</updated><authors><author><keyname>Dea&#xf1;o</keyname><forenames>Alfredo</forenames></author><author><keyname>Huybrechs</keyname><forenames>Daan</forenames></author><author><keyname>Opsomer</keyname><forenames>Peter</forenames></author></authors><title>Construction and implementation of asymptotic expansions for
  Jacobi--type orthogonal polynomials</title><categories>cs.MS math.CA</categories><comments>39 pages, 5 figures, 35 references. The article mentioned is
  arXiv:math/0111252 and the implementation is available on
  http://nines.cs.kuleuven.be/software/JACOBI/. The final publication is
  available at Springer via http://dx.doi.org/10.1007/s10444-015-9442-z</comments><report-no>TW658</report-no><msc-class>26C04 (Primary), 30E10, 33C45, 35Q15, 65D15 (Secondary)</msc-class><acm-class>D.3.m; G.1.2; G.4</acm-class><doi>10.1007/s10444-015-9442-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are interested in the asymptotic behavior of orthogonal polynomials of the
generalized Jacobi type as their degree $n$ goes to $\infty$. These are defined
on the interval $[-1,1]$ with weight function
$w(x)=(1-x)^{\alpha}(1+x)^{\beta}h(x)$, $\alpha,\beta&gt;-1$ and $h(x)$ a real,
analytic and strictly positive function on $[-1,1]$. This information is
available in the work of Kuijlaars, McLaughlin, Van Assche and Vanlessen, where
the authors use the Riemann--Hilbert formulation and the Deift--Zhou non-linear
steepest descent method. We show that computing higher-order terms can be
simplified, leading to their efficient construction. The resulting asymptotic
expansions in every region of the complex plane are implemented both
symbolically and numerically, and the code is made publicly available. The main
advantage of these expansions is that they lead to increasing accuracy for
increasing degree of the polynomials, at a computational cost that is actually
independent of the degree. In contrast, the typical use of the recurrence
relation for orthogonal polynomials in computations leads to a cost that is at
least linear in the degree. Furthermore, the expansions may be used to compute
Gaussian quadrature rules in $\mathcal{O}(n)$ operations, rather than
$\mathcal{O}(n^2)$ based on the recurrence relation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07193</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07193</id><created>2015-02-25</created><authors><author><keyname>Kalise</keyname><forenames>Dante</forenames></author><author><keyname>Kr&#xf6;ner</keyname><forenames>Axel</forenames></author><author><keyname>Kunisch</keyname><forenames>Karl</forenames></author></authors><title>Local minimization algorithms for dynamic programming equations</title><categories>math.OC cs.SY math.NA</categories><comments>27 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The numerical realization of the dynamic programming principle for
continuous-time optimal control leads to nonlinear Hamilton-Jacobi-Bellman
equations which require the minimization of a nonlinear mapping over the set of
admissible controls. This minimization is often performed by comparison over a
finite number of elements of the control set. In this paper we demonstrate the
importance of an accurate realization of these minimization problems and
propose algorithms by which this can be achieved effectively. The considered
class of equations includes nonsmooth control problems with
$\ell_1$-penalization which lead to sparse controls.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07206</identifier>
 <datestamp>2015-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07206</id><created>2015-02-25</created><updated>2015-04-17</updated><authors><author><keyname>Ausiello</keyname><forenames>Giorgio</forenames></author><author><keyname>Franciosa</keyname><forenames>Paolo G.</forenames></author><author><keyname>Italiano</keyname><forenames>Giuseppe F.</forenames></author><author><keyname>Ribichini</keyname><forenames>Andrea</forenames></author></authors><title>Incremental DFS Trees on Arbitrary Directed Graphs</title><categories>cs.DS</categories><comments>The paper has been withdrawn by the authors due to a flaw in the
  complexity analysis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new algorithm for maintaining a DFS tree of an arbitrary
directed graph under any sequence of edge insertions. Our algorithm requires a
total of $O(m\cdot n)$ time in the worst case to process a sequence of edge
insertions, where $n$ is the number of vertices in the graph and $m$ is the
total number of edges in the final graph. We also prove lower bounds for
variations of this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07209</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07209</id><created>2015-02-25</created><authors><author><keyname>Jiang</keyname><forenames>Yu-Gang</forenames></author><author><keyname>Wu</keyname><forenames>Zuxuan</forenames></author><author><keyname>Wang</keyname><forenames>Jun</forenames></author><author><keyname>Xue</keyname><forenames>Xiangyang</forenames></author><author><keyname>Chang</keyname><forenames>Shih-Fu</forenames></author></authors><title>Exploiting Feature and Class Relationships in Video Categorization with
  Regularized Deep Neural Networks</title><categories>cs.CV cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the challenging problem of categorizing videos
according to high-level semantics such as the existence of a particular human
action or a complex event. Although extensive efforts have been devoted in
recent years, most existing works combined multiple video features using simple
fusion strategies and neglected the utilization of inter-class semantic
relationships. This paper proposes a novel unified framework that jointly
exploits the feature relationships and the class relationships for improved
categorization performance. Specifically, these two types of relationships are
estimated and utilized by rigorously imposing regularizations in the learning
process of a deep neural network (DNN). Such a regularized DNN (rDNN) can be
efficiently realized using a GPU-based implementation with an affordable
training cost. Through arming the DNN with better capability of harnessing both
the feature and the class relationships, the proposed rDNN is more suitable for
modeling video semantics. With extensive experimental evaluations, we show that
rDNN produces superior performance over several state-of-the-art approaches. On
the well-known Hollywood2 and Columbia Consumer Video benchmarks, we obtain
very competitive results: 66.9\% and 73.5\% respectively in terms of mean
average precision. In addition, to substantially evaluate our rDNN and
stimulate future research on large scale video categorization, we collect and
release a new benchmark dataset, called FCVID, which contains 91,223 Internet
videos and 239 manually annotated categories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07220</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07220</id><created>2015-02-25</created><updated>2015-02-26</updated><authors><author><keyname>van Hoeij</keyname><forenames>Mark</forenames></author></authors><title>Groebner basis in Boolean rings is not polynomial-space</title><categories>cs.SC</categories><comments>3 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an example where the number of elements of a Groebner basis in a
Boolean ring is not polynomially bounded in terms of the bitsize and degrees of
the input.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07228</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07228</id><created>2015-02-25</created><authors><author><keyname>Niu</keyname><forenames>Yong</forenames></author><author><keyname>Li</keyname><forenames>Yong</forenames></author><author><keyname>Jin</keyname><forenames>Depeng</forenames></author><author><keyname>Su</keyname><forenames>Li</forenames></author><author><keyname>Vasilakos</keyname><forenames>Athanasios V.</forenames></author></authors><title>A Survey of Millimeter Wave (mmWave) Communications for 5G:
  Opportunities and Challenges</title><categories>cs.NI</categories><comments>17 pages, 8 figures, 7 tables, Journal paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the explosive growth of mobile data demand, the fifth generation (5G)
mobile network would exploit the enormous amount of spectrum in the millimeter
wave (mmWave) bands to greatly increase communication capacity. There are
fundamental differences between mmWave communications and existing other
communication systems, in terms of high propagation loss, directivity, and
sensitivity to blockage. These characteristics of mmWave communications pose
several challenges to fully exploit the potential of mmWave communications,
including integrated circuits and system design, interference management,
spatial reuse, anti-blockage, and dynamics control. To address these
challenges, we carry out a survey of existing solutions and standards, and
propose design guidelines in architectures and protocols for mmWave
communications. We also discuss the potential applications of mmWave
communications in the 5G network, including the small cell access, the cellular
access, and the wireless backhaul. Finally, we discuss relevant open research
issues including the new physical layer technology, software-defined network
architecture, measurements of network state information, efficient control
mechanisms, and heterogeneous networking, which should be further investigated
to facilitate the deployment of mmWave communication systems in the future 5G
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07229</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07229</id><created>2015-02-25</created><authors><author><keyname>Ying</keyname><forenames>Yiming</forenames></author><author><keyname>Zhou</keyname><forenames>Ding-Xuan</forenames></author></authors><title>Online Pairwise Learning Algorithms with Kernels</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pairwise learning usually refers to a learning task which involves a loss
function depending on pairs of examples, among which most notable ones include
ranking, metric learning and AUC maximization. In this paper, we study an
online algorithm for pairwise learning with a least-square loss function in an
unconstrained setting of a reproducing kernel Hilbert space (RKHS), which we
refer to as the Online Pairwise lEaRning Algorithm (OPERA). In contrast to
existing works \cite{Kar,Wang} which require that the iterates are restricted
to a bounded domain or the loss function is strongly-convex, OPERA is
associated with a non-strongly convex objective function and learns the target
function in an unconstrained RKHS. Specifically, we establish a general theorem
which guarantees the almost surely convergence for the last iterate of OPERA
without any assumptions on the underlying distribution. Explicit convergence
rates are derived under the condition of polynomially decaying step sizes. We
also establish an interesting property for a family of widely-used kernels in
the setting of pairwise learning and illustrate the above convergence results
using such kernels. Our methodology mainly depends on the characterization of
RKHSs using its associated integral operators and probability inequalities for
random variables with values in a Hilbert space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07241</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07241</id><created>2015-02-25</created><updated>2015-02-26</updated><authors><author><keyname>Hannig</keyname><forenames>Frank</forenames></author><author><keyname>Fey</keyname><forenames>Dietmar</forenames></author><author><keyname>Lokhmotov</keyname><forenames>Anton</forenames></author></authors><title>Proceedings of the DATE Friday Workshop on Heterogeneous Architectures
  and Design Methods for Embedded Image Systems (HIS 2015)</title><categories>cs.AR cs.CV cs.DC</categories><comments>Website of the workshop: https://www12.cs.fau.de/ws/his2015/</comments><proxy>Frank Hannig</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the papers accepted at the DATE Friday Workshop on
Heterogeneous Architectures and Design Methods for Embedded Image Systems (HIS
2015), held in Grenoble, France, March 13, 2015. HIS 2015 was co-located with
the Conference on Design, Automation and Test in Europe (DATE).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07242</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07242</id><created>2015-02-25</created><updated>2015-09-20</updated><authors><author><keyname>Lam</keyname><forenames>Albert Y. S.</forenames></author><author><keyname>Leung</keyname><forenames>Yiu-Wing</forenames></author><author><keyname>Chu</keyname><forenames>Xiaowen</forenames></author></authors><title>Autonomous Vehicle Public Transportation System: Scheduling and
  Admission Control</title><categories>cs.SY</categories><comments>16 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Technology of autonomous vehicles (AVs) is getting mature and many AVs will
appear on the roads in the near future. AVs become connected with the support
of various vehicular communication technologies and they possess high degree of
control to respond to instantaneous situations cooperatively with high
efficiency and flexibility. In this paper, we propose a new public
transportation system based on AVs. It manages a fleet of AVs to accommodate
transportation requests, offering point-to-point services with ride sharing. We
focus on the two major problems of the system: scheduling and admission
control. The former is to configure the most economical schedules and routes
for the AVs to satisfy the admissible requests while the latter is to determine
the set of admissible requests among all requests to produce maximum profit.
The scheduling problem is formulated as a mixed-integer linear program and the
admission control problem is cast as a bilevel optimization, which embeds the
scheduling problem as the major constraint. By utilizing the analytical
properties of the problem, we develop an effective genetic-algorithm-based
method to tackle the admission control problem. We validate the performance of
the algorithm with real-world transportation service data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07243</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07243</id><created>2014-12-08</created><authors><author><keyname>Mourad</keyname><forenames>Bousaaid</forenames></author><author><keyname>Tarik</keyname><forenames>Ayaou</forenames></author><author><keyname>Karim</keyname><forenames>Afdel</forenames></author><author><keyname>Pascal</keyname><forenames>Estraillier</forenames></author></authors><title>Real-Time System of Hand Detection And Gesture Recognition In Cyber
  Presence Interactive System For E-Learning</title><categories>cs.CV</categories><comments>5 pages. arXiv admin note: substantial text overlap with
  arXiv:1502.06641</comments><journal-ref>Journal of Engineering Research and Applications Vol. 4, Issue 9
  (Version 1), September 2014, pp.1-5</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The development of technologies of multimedia, linked to that of Internet and
democratization of high outflow, has made henceforth E-learning possible for
learners being in virtual classes and geographically distributed. The quality
and quantity of asynchronous and synchronous communications are the key
elements for E-learning success. It is important to have a propitious
supervision to reduce the feeling of isolation in E-learning. This feeling of
isolation is among the main causes of loss and high rates of stalling in
E-learning. The researches to be conducted in this domain aim to bring
solutions of convergence coming from real time image for the capture and
recognition of hand gestures. These gestures will be analyzed by the system and
transformed as indicator of participation. This latter is displayed in the
table of performance of the tutor as a curve according to the time. In case of
isolation of learner, the indicator of participation will become red and the
tutor will be informed of learners with difficulties to participate during
learning session.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07257</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07257</id><created>2015-02-25</created><updated>2015-11-15</updated><authors><author><keyname>Bartunov</keyname><forenames>Sergey</forenames></author><author><keyname>Kondrashkin</keyname><forenames>Dmitry</forenames></author><author><keyname>Osokin</keyname><forenames>Anton</forenames></author><author><keyname>Vetrov</keyname><forenames>Dmitry</forenames></author></authors><title>Breaking Sticks and Ambiguities with Adaptive Skip-gram</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently proposed Skip-gram model is a powerful method for learning
high-dimensional word representations that capture rich semantic relationships
between words. However, Skip-gram as well as most prior work on learning word
representations does not take into account word ambiguity and maintain only
single representation per word. Although a number of Skip-gram modifications
were proposed to overcome this limitation and learn multi-prototype word
representations, they either require a known number of word meanings or learn
them using greedy heuristic approaches. In this paper we propose the Adaptive
Skip-gram model which is a nonparametric Bayesian extension of Skip-gram
capable to automatically learn the required number of representations for all
words at desired semantic resolution. We derive efficient online variational
learning algorithm for the model and empirically demonstrate its efficiency on
word-sense induction task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07258</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07258</id><created>2015-02-25</created><updated>2015-04-07</updated><authors><author><keyname>Hirahara</keyname><forenames>Shuichi</forenames></author></authors><title>Identifying an Honest ${\rm EXP}^{\rm NP}$ Oracle Among Many</title><categories>cs.CC</categories><comments>20 pages; a simplified proof for the main theorem</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a general framework to remove short advice by formulating the
following computational task for a function $f$: given two oracles at least one
of which is honest (i.e. correctly computes $f$ on all inputs) as well as an
input, the task is to compute $f$ on the input with the help of the oracles by
a probabilistic polynomial-time machine, which we shall call a selector. We
characterize the languages for which short advice can be removed by the notion
of selector: a paddable language has a selector if and only if short advice of
a probabilistic machine that accepts the language can be removed under any
relativized world. Previously, instance checkers have served as a useful tool
to remove short advice of probabilistic computation. We indicate that existence
of instance checkers is a property stronger than that of removing short advice:
although no instance checker for ${\rm EXP}^{\rm NP}$-complete languages exists
unless ${\rm EXP}^{\rm NP} = {\rm NEXP}$, we prove that there exists a selector
for any ${\rm EXP}^{\rm NP}$-complete language, by building on the proof of
${\rm MIP} = {\rm NEXP}$ by Babai, Fortnow, and Lund (1991).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07267</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07267</id><created>2015-02-25</created><authors><author><keyname>Daoud</keyname><forenames>Ahmad</forenames></author><author><keyname>Dessouki</keyname><forenames>Ahmed</forenames></author><author><keyname>Abuelenin</keyname><forenames>Sherif</forenames></author></authors><title>Accuracy Enhancement of Pickett Tunnelling Barrier Memristor Model</title><categories>cs.ET</categories><comments>5 pages, 5 figures, presented at the ICITACEE 2014 conference;
  http://icitacee.undip.ac.id/index.php/icitacee/2014/paper/view/89</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Titanium dioxide (TiO2) memristors exhibit complex conduction mechanism.
Several models of different complexity have been developed in order to mimic
the experimental results for physical behaviors observed in memristor devices.
Pickett's tunneling barrier model describes the TiO2 memristors, and utilizes
complex derivative of tunnel barrier width. It attains a large error in the ON
switching region. Variety of research consider it as the reference model for
the TiO2 memristors. In this paper, we first analyze the theory of operation of
the memristor and discuss Pickett's model. Then, we propose a modification to
its derivative functions to provide a lower error and closer agreement with
physical behavior. This modification is represented by two additional fitting
parameters to damp or accelerate the tunnel width derivative. Also, we
incorporate a hard limiter term to limit the tunnel width to its physical
extremes 1 nm and 2 nm. We run simulations to test the model modifications and
we compare the results to the experimental and original Pickett's model
results. The modified model more closely resembles the experimental behavior of
TiO2 memristors and potentially enables the memristor to be used as a
multilevel memory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07281</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07281</id><created>2015-02-22</created><authors><author><keyname>Liu</keyname><forenames>Xiaogang</forenames></author></authors><title>A problem related to the divisibility of exponential sums</title><categories>math.NT cs.IT math.IT</categories><comments>3 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Francis Castro, et al computed the exact divisibility of families of
exponential sums associated to binomials F(X) = aXd1 + bXd2 over Fp, and a
conjecture is presented for related work. Here we study this question.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07282</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07282</id><created>2015-02-25</created><updated>2015-02-28</updated><authors><author><keyname>Abuelenin</keyname><forenames>Sherif M.</forenames></author><author><keyname>Abul-Magd</keyname><forenames>Adel Y.</forenames></author></authors><title>Empirical Study of Traffic Velocity Distribution and its Effect on
  VANETs Connectivity</title><categories>cs.NI physics.soc-ph</categories><comments>5 pages, 5 figures, presented at the ICCVE 2014 (International
  conference on connected vehicles &amp; expo); http://www.iccve.org/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we use real traffic data to confirm that vehicle velocities
follow Gaussian distribution in steady state traffic regimes (free-flow, and
congestion). We also show that in the transition between free-flow and
congestion, the velocity distribution is better modeled by generalized extreme
value distribution (GEV). We study the effect of the different models on
estimating the probability distribution of connectivity duration between
vehicles in vehicular ad-hoc networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07288</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07288</id><created>2015-02-25</created><authors><author><keyname>Mohri</keyname><forenames>Mehryar</forenames></author><author><keyname>Riley</keyname><forenames>Michael</forenames></author><author><keyname>Suresh</keyname><forenames>Ananda Theertha</forenames></author></authors><title>Automata and Graph Compression</title><categories>cs.IT cs.DS cs.FL math.IT</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a theoretical framework for the compression of automata, which are
widely used in speech processing and other natural language processing tasks.
The framework extends to graph compression. Similar to stationary ergodic
processes, we formulate a probabilistic process of graph and automata
generation that captures real world phenomena and provide a universal
compression scheme LZA for this probabilistic model. Further, we show that LZA
significantly outperforms other compression techniques such as gzip and the
UNIX compress command for several synthetic and real data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07310</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07310</id><created>2015-02-25</created><updated>2016-01-05</updated><authors><author><keyname>Yu</keyname><forenames>Amy Zhao</forenames></author><author><keyname>Ronen</keyname><forenames>Shahar</forenames></author><author><keyname>Hu</keyname><forenames>Kevin</forenames></author><author><keyname>Lu</keyname><forenames>Tiffany</forenames></author><author><keyname>Hidalgo</keyname><forenames>C&#xe9;sar A.</forenames></author></authors><title>Pantheon 1.0, a manually verified dataset of globally famous biographies</title><categories>physics.soc-ph cs.SI</categories><comments>Scientific Data 2:150075</comments><doi>10.1038/sdata.2015.75</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the Pantheon 1.0 dataset: a manually verified dataset of
individuals that have transcended linguistic, temporal, and geographic
boundaries. The Pantheon 1.0 dataset includes the 11,341 biographies present in
more than 25 languages in Wikipedia and is enriched with: (i) manually verified
demographic information (place and date of birth, gender) (ii) a taxonomy of
occupations classifying each biography at three levels of aggregation and (iii)
two measures of global popularity including the number of languages in which a
biography is present in Wikipedia (L), and the Historical Popularity Index
(HPI) a metric that combines information on L, time since birth, and page-views
(2008-2013). We compare the Pantheon 1.0 dataset to data from the 2003 book,
Human Accomplishments, and also to external measures of accomplishment in
individual games and sports: Tennis, Swimming, Car Racing, and Chess. In all of
these cases we find that measures of popularity (L and HPI) correlate highly
with individual accomplishment, suggesting that measures of global popularity
proxy the historical impact of individuals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07314</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07314</id><created>2015-02-25</created><updated>2015-06-08</updated><authors><author><keyname>Tolpin</keyname><forenames>David</forenames></author><author><keyname>Paige</keyname><forenames>Brooks</forenames></author><author><keyname>van de Meent</keyname><forenames>Jan Willem</forenames></author><author><keyname>Wood</keyname><forenames>Frank</forenames></author></authors><title>Path Finding under Uncertainty through Probabilistic Inference</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new approach to solving path-finding problems under
uncertainty by representing them as probabilistic models and applying
domain-independent inference algorithms to the models. This approach separates
problem representation from the inference algorithm and provides a framework
for efficient learning of path-finding policies. We evaluate the new approach
on the Canadian Traveler Problem, which we formulate as a probabilistic model,
and show how probabilistic inference allows high performance stochastic
policies to be obtained for this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07326</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07326</id><created>2015-02-25</created><authors><author><keyname>Vychodil</keyname><forenames>Vilem</forenames></author></authors><title>Rational fuzzy attribute logic</title><categories>cs.LO</categories><msc-class>03B52, 03B70, 68P15</msc-class><acm-class>F.4.1; I.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a logic for reasoning with if-then formulas which involve
constants for rational truth degrees from the unit interval. We introduce
graded semantic and syntactic entailment of formulas. We prove the logic is
complete in Pavelka style and depending on the choice of structure of truth
degrees, the logic is a decidable fragment of the Rational Pavelka logic (RPL)
or the Rational Product Logic (R{\Pi}L). We also present a characterization of
the entailment based on least models and study related closure structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07327</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07327</id><created>2015-02-25</created><updated>2015-04-28</updated><authors><author><keyname>Kolmogorov</keyname><forenames>Vladimir</forenames></author><author><keyname>Krokhin</keyname><forenames>Andrei</forenames></author><author><keyname>Rolinek</keyname><forenames>Michal</forenames></author></authors><title>The Complexity of General-Valued CSPs</title><categories>cs.CC cs.DM</categories><comments>fixed some minor things in the presentation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An instance of the Valued Constraint Satisfaction Problem (VCSP) is given by
a finite set of variables, a finite domain of labels, and a sum of functions,
each function depending on a subset of the variables. Each function can take
finite values specifying costs of assignments of labels to its variables or the
infinite value, which indicates infeasible assignments. The goal is to find an
assignment of labels to the variables that minimizes the sum.
  We study (assuming that P $\ne$ NP) how the complexity of this very general
problem depends on the set of functions allowed in the instances, the so-called
constraint language. The case when all allowed functions take values in
$\{0,\infty\}$ corresponds to ordinary CSPs, where one deals only with the
feasibility issue and there is no optimization. This case is the subject of the
Algebraic CSP Dichotomy Conjecture predicting for which constraint languages
CSPs are tractable and for which NP-hard. The case when all allowed functions
take only finite values corresponds to finite-valued CSP, where the feasibility
aspect is trivial and one deals only with the optimization issue. The
complexity of finite-valued CSPs was fully classified by Thapper and
\v{Z}ivn\'y.
  An algebraic necessary condition for tractability of a general-valued CSP
with a fixed constraint language was recently given by Kozik and Ochremiak. As
our main result, we prove that if a constraint language satisfies this
algebraic necessary condition, and the feasibility CSP corresponding to the
VCSP with this language is tractable, then the VCSP is tractable. The algorithm
is a simple combination of the assumed algorithm for the feasibility CSP and
the standard LP relaxation. As a corollary, we obtain that a dichotomy for
ordinary CSPs would imply a dichotomy for general-valued CSPs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07331</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07331</id><created>2015-02-25</created><authors><author><keyname>Prandi</keyname><forenames>Dario</forenames></author><author><keyname>Remizov</keyname><forenames>Alexey</forenames></author><author><keyname>Chertovskih</keyname><forenames>Roman</forenames></author><author><keyname>Boscain</keyname><forenames>Ugo</forenames></author><author><keyname>Gauthier</keyname><forenames>Jean-Paul</forenames></author></authors><title>Highly corrupted image inpainting through hypoelliptic diffusion</title><categories>cs.CV math.AP</categories><comments>13 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new image inpainting algorithm, the Averaging and Hypoelliptic
Evolution (AHE) algorithm, inspired by the one presented in [1] and based upon
a (semi-discrete) variation of the Citti--Petitot--Sarti model of the primary
visual cortex V1. In particular, we focus on reconstructing highly corrupted
images (i.e. where more than the 80% of the image is missing).
  [1] U. Boscain, R. A. Chertovskih, J. P. Gauthier, and A. O. Remizov,
Hypoelliptic diffusion and human vision: a semidiscrete new twist, SIAM J.
Imaging Sci., vol. 7, no. 2, pp. 669--695, 2014.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07363</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07363</id><created>2015-01-19</created><authors><author><keyname>Melnik</keyname><forenames>S. S.</forenames></author><author><keyname>Usatenko</keyname><forenames>O. V.</forenames></author></authors><title>Entropy of finite random binary sequences with weak long-range
  correlations</title><categories>cond-mat.stat-mech cond-mat.dis-nn cs.IT math.IT physics.data-an</categories><comments>9 pages, 4 figures. arXiv admin note: substantial text overlap with
  arXiv:1411.2761, arXiv:1412.3692</comments><journal-ref>Phys. Rev. E 90, 052106 (2014)</journal-ref><doi>10.1103/PhysRevE.90.052106</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the N-step binary stationary ergodic Markov chain and analyze its
differential entropy. Supposing that the correlations are weak we express the
conditional probability function of the chain through the pair correlation
function and represent the entropy as a functional of the pair correlator.
Since the model uses the two-point correlators instead of the block
probability, it makes it possible to calculate the entropy of strings at much
longer distances than using standard methods. A fluctuation contribution to the
entropy due to finiteness of random chains is examined. This contribution can
be of the same order as its regular part even at the relatively short lengths
of subsequences. A self-similar structure of entropy with respect to the
decimation transformations is revealed for some specific forms of the pair
correlation function. Application of the theory to the DNA sequence of the R3
chromosome of Drosophila melanogaster is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07364</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07364</id><created>2015-02-12</created><authors><author><keyname>Wolfe</keyname><forenames>Nikolas</forenames></author></authors><title>Open Source Remote Monitoring for Rural Solar Electrification Projects</title><categories>cs.CY</categories><comments>60 pages, 25 diagrams</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Renewable energy systems are an increasingly popular way to generate
electricity around the world. As wind and solar technologies gradually begin to
supplant the use of fossil fuels as preferred means of energy production, new
challenges are emerging which are unique to the experience of decentralized
power generation. One such challenge is the development of effective monitoring
technologies to relay diagnostic information from remote energy systems to data
analysis centers. The ability to easily obtain, synthesize, and evaluate data
pertaining to the behavior of a potentially vast number of individual power
sources is of critical importance to the maintainability of the next generation
of intelligent grid infrastructure. However, the application space of remote
monitoring extends well beyond this. This paper details the development and
implementation of an open-source monitoring framework for remote solar energy
systems. The necessity for such a framework to be open is much better
understood when considered through the lens of the theoretical potential for
remote monitoring technologies in developing countries. The United States and
other industrialized nations in the so-called 'first world' are likely to be
slow to seriously adopt renewable energy on account of the massive investment
and infrastructural changes required for its integration into the existing
electrical grid. In countries where grid infrastructure is generally inadequate
or nonexistent, this barrier is far less of a concern, and renewable energy
technologies are viewed more as an enabling tool for progress than as a
disruptive and expensive technological tangent. In this context as well, remote
monitoring has a role to play.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07373</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07373</id><created>2015-02-25</created><updated>2015-03-01</updated><authors><author><keyname>Oren</keyname><forenames>Yossef</forenames></author><author><keyname>Kemerlis</keyname><forenames>Vasileios P.</forenames></author><author><keyname>Sethumadhavan</keyname><forenames>Simha</forenames></author><author><keyname>Keromytis</keyname><forenames>Angelos D.</forenames></author></authors><title>The Spy in the Sandbox -- Practical Cache Attacks in Javascript</title><categories>cs.CR cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the first micro-architectural side-channel attack which runs
entirely in the browser. In contrast to other works in this genre, this attack
does not require the attacker to install any software on the victim's machine
-- to facilitate the attack, the victim needs only to browse to an untrusted
webpage with attacker-controlled content. This makes the attack model highly
scalable and extremely relevant and practical to today's web, especially since
most desktop browsers currently accessing the Internet are vulnerable to this
attack. Our attack, which is an extension of the last-level cache attacks of
Yarom et al., allows a remote adversary recover information belonging to other
processes, other users and even other virtual machines running on the same
physical host as the victim web browser. We describe the fundamentals behind
our attack, evaluate its performance using a high bandwidth covert channel and
finally use it to construct a system-wide mouse/network activity logger.
Defending against this attack is possible, but the required countermeasures can
exact an impractical cost on other benign uses of the web browser and of the
computer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07379</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07379</id><created>2015-01-18</created><authors><author><keyname>Bellini</keyname><forenames>Emanuele</forenames></author><author><keyname>Guerrini</keyname><forenames>Eleonora</forenames></author><author><keyname>Meneghetti</keyname><forenames>Alessio</forenames></author><author><keyname>Sala</keyname><forenames>Massimiliano</forenames></author></authors><title>On the Griesmer bound for nonlinear codes</title><categories>cs.IT math.CO math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most bounds on the size of codes hold for any code, whether linear or
nonlinear. Notably, the Griesmer bound, holds only in the linear case. In this
paper we characterize a family of systematic nonlinear codes for which the
Griesmer bound holds. Moreover, we show that the Griesmer bound does not
necessarily hold for a systematic code by showing explicit counterexamples. On
the other hand, we are also able to provide (weaker) versions of the Griesmer
bound holding for all systematic codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07384</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07384</id><created>2015-02-25</created><updated>2015-09-16</updated><authors><author><keyname>Ogura</keyname><forenames>Masaki</forenames></author><author><keyname>Preciado</keyname><forenames>Victor M.</forenames></author></authors><title>Spreading Processes over Socio-Technical Networks with Phase-Type
  Transmissions</title><categories>cs.SI math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most theoretical tools available for the analysis of spreading processes over
networks assume exponentially distributed transmission and recovery times. In
practice, the empirical distribution of transmission times for many real
spreading processes, such as the spread of web content through the Internet,
are far from exponential. To bridge this gap between theory and practice, we
propose a methodology to model and analyze spreading processes with arbitrary
transmission times using phase-type distributions. Phase-type distributions are
a family of distributions that is dense in the set of positive-valued
distributions and can be used to approximate any given distributions. To
illustrate our methodology, we focus on a popular model of spreading over
networks: the susceptible-infected-susceptible (SIS) networked model. In the
standard version of this model, individuals informed about a piece of
information transmit this piece to its neighbors at an exponential rate. In
this paper, we extend this model to the case of transmission rates following a
phase-type distribution. Using this extended model, we analyze the dynamics of
the spread based on a vectorial representations of phase-type distributions. We
illustrate our results by analyzing spreading processes over networks with
transmission and recovery rates following a Weibull distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07391</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07391</id><created>2015-02-25</created><updated>2015-03-19</updated><authors><author><keyname>Segev</keyname><forenames>Gideon</forenames></author><author><keyname>Amit</keyname><forenames>Iddo</forenames></author><author><keyname>Godkin</keyname><forenames>Andrey</forenames></author><author><keyname>Henning</keyname><forenames>Alex</forenames></author><author><keyname>Rosenwaks</keyname><forenames>Yossi</forenames></author></authors><title>Multiple State EFN Transistors</title><categories>cs.ET cond-mat.mes-hall</categories><doi>10.1109/LED.2015.2434793</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Electrostatically Formed Nanowire (EFN) based transistors have been suggested
in the past as gas sensing devices. These transistors are multiple gate
transistors in which the source to drain conduction path is determined by the
bias applied to the back gate, and two junction gates. If a specific bias is
applied to the side gates, the conduction band electrons between them are
confined to a well-defined area forming a narrow channel- the Electrostatically
Formed Nanowire. Recent work has shown that by applying non-symmetric bias on
the side gates, the lateral position of the EFN can be controlled. We propose a
novel Multiple State EFN Transistor (MSET) that utilizes this degree of freedom
for the implementation of complete multiplexer functionality in a single
transistor like device. The multiplexer functionality allows a very simple
implementation of binary and multiple valued logic functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07404</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07404</id><created>2015-02-25</created><authors><author><keyname>Tong</keyname><forenames>Zhen</forenames></author><author><keyname>Haenggi</keyname><forenames>Martin</forenames></author></authors><title>Throughput Analysis for Full-Duplex Wireless Networks with Imperfect
  Self-interference Cancellation</title><categories>cs.IT math.IT</categories><comments>6 figures. arXiv admin note: substantial text overlap with
  arXiv:1409.7433</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the throughput for wireless network with full-duplex
radios using stochastic geometry. Full-duplex (FD) radios can exchange data
simultaneously with each other. On the other hand, the downside of FD
transmission is that it will inevitably cause extra interference to the network
compared to half-duplex (HD) transmission. Moreover, the residual
self-interference has negative effects on the network throughput. In this
paper, we focus on a wireless network of nodes with both HD and FD capabilities
and derive and optimize the throughput in such a network. Our analytical result
shows that if the network is adapting an ALOHA protocol, the maximal throughput
is achieved by scheduling all concurrently transmitting nodes to work in either
FD mode or HD mode depending on one simple condition. Moreover, the effects of
imperfect self-interference cancellation on the signal-to-interference ratio
(SIR) loss and throughput are also analyzed based on our mathematical model. We
rigorously quantify the impact of imperfect self-interference cancellation on
the throughput gain, transmission range, and other metrics, and we establish
the minimum amount of self-interference suppression needed for FD to be
beneficial.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07405</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07405</id><created>2015-02-25</created><authors><author><keyname>Ghysels</keyname><forenames>Pieter</forenames></author><author><keyname>Li</keyname><forenames>Xiaoye S.</forenames></author><author><keyname>Rouet</keyname><forenames>Francois-Henry</forenames></author><author><keyname>Williams</keyname><forenames>Samuel</forenames></author><author><keyname>Napov</keyname><forenames>Artem</forenames></author></authors><title>An efficient multi-core implementation of a novel HSS-structured
  multifrontal solver using randomized sampling</title><categories>cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a sparse linear system solver that is based on a multifrontal
variant of Gaussian elimination, and exploits low-rank approximation of the
resulting dense frontal matrices. We use hierarchically semiseparable (HSS)
matrices, which have low-rank off-diagonal blocks, to approximate the frontal
matrices. For HSS matrix construction, a randomized sampling algorithm is used
together with interpolative decompositions. The combination of the randomized
compression with a fast ULV HSS factorization leads to a solver with lower
computational complexity than the standard multifrontal method for many
applications, resulting in speedups up to 7 fold for problems in our test
suite. The implementation targets many-core systems by using task parallelism
with dynamic runtime scheduling. Numerical experiments show performance
improvements over state-of-the-art sparse direct solvers. The implementation
achieves high performance and good scalability on a range of modern shared
memory parallel systems, including the Intel Xeon Phi (MIC). The code is part
of a software package called STRUMPACK -- STRUctured Matrices PACKage, which
also has a distributed memory component for dense rank-structured matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07406</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07406</id><created>2015-02-25</created><authors><author><keyname>Iwata</keyname><forenames>Satoru</forenames></author><author><keyname>Tanigawa</keyname><forenames>Shin-ichi</forenames></author><author><keyname>Yoshida</keyname><forenames>Yuichi</forenames></author></authors><title>Improved Approximation Algorithms for k-Submodular Function Maximization</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a polynomial-time $1/2$-approximation algorithm for
maximizing nonnegative $k$-submodular functions. This improves upon the
previous $\max\{1/3, 1/(1+a)\}$-approximation by Ward and
\v{Z}ivn\'y~(SODA'14), where $a=\max\{1, \sqrt{(k-1)/4}\}$. We also show that
for monotone $k$-submodular functions there is a polynomial-time
$k/(2k-1)$-approximation algorithm while for any $\varepsilon&gt;0$ a
$((k+1)/2k+\varepsilon)$-approximation algorithm for maximizing monotone
$k$-submodular functions would require exponentially many queries. In
particular, our hardness result implies that our algorithms are asymptotically
tight.
  We also extend the approach to provide constant factor approximation
algorithms for maximizing skew-bisubmodular functions, which were recently
introduced as generalizations of bisubmodular functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07410</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07410</id><created>2015-02-25</created><updated>2015-08-31</updated><authors><author><keyname>Chandrasekaran</keyname><forenames>Karthekeyan</forenames></author><author><keyname>Velingker</keyname><forenames>Ameya</forenames></author></authors><title>Towards Constructing Ramanujan Graphs Using Shift Lifts</title><categories>math.CO cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a breakthrough work, Marcus-Spielman-Srivastava recently showed that every
$d$-regular bipartite Ramanujan graph has a 2-lift that is also $d$-regular
bipartite Ramanujan. As a consequence, a straightforward iterative brute-force
search algorithm leads to the construction of a $d$-regular bipartite Ramanujan
graph on $N$ vertices in time $2^{O(dN)}$. Shift $k$-lifts studied by
Agarwal-Kolla-Madan lead to a natural approach for constructing Ramanujan
graphs more efficiently. The number of possible shift $k$-lifts of a
$d$-regular $n$-vertex graph is $k^{nd/2}$. Suppose the following holds for
$k=2^{\Omega(n)}$:
  There exists a shift $k$-lift that maintains the Ramanujan property of
$d$-regular bipartite graphs on $n$ vertices for all $n$. (*)
  Then, by performing a similar brute-force search algorithm, one would be able
to construct an $N$-vertex bipartite Ramanujan graph in time $2^{O(d\,log^2
N)}$. Furthermore, if (*) holds for all $k \geq 2$, then one would obtain an
algorithm that runs in $\mathrm{poly}_d(N)$ time. In this work, we take a first
step towards proving (*) by showing the existence of shift $k$-lifts that
preserve the Ramanujan property in $d$-regular bipartite graphs for $k=3,4$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07411</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07411</id><created>2015-02-25</created><updated>2015-11-24</updated><authors><author><keyname>Liu</keyname><forenames>Fayao</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Lin</keyname><forenames>Guosheng</forenames></author><author><keyname>Reid</keyname><forenames>Ian</forenames></author></authors><title>Learning Depth from Single Monocular Images Using Deep Convolutional
  Neural Fields</title><categories>cs.CV</categories><comments>Appearing in IEEE T. Pattern Analysis and Machine Intelligence.
  Journal version of arXiv:1411.6387 . Test code is available at
  https://bitbucket.org/fayao/dcnf-fcsp</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we tackle the problem of depth estimation from single
monocular images. Compared with depth estimation using multiple images such as
stereo depth perception, depth from monocular images is much more challenging.
Prior work typically focuses on exploiting geometric priors or additional
sources of information, most using hand-crafted features. Recently, there is
mounting evidence that features from deep convolutional neural networks (CNN)
set new records for various vision applications. On the other hand, considering
the continuous characteristic of the depth values, depth estimations can be
naturally formulated as a continuous conditional random field (CRF) learning
problem. Therefore, here we present a deep convolutional neural field model for
estimating depths from single monocular images, aiming to jointly explore the
capacity of deep CNN and continuous CRF. In particular, we propose a deep
structured learning scheme which learns the unary and pairwise potentials of
continuous CRF in a unified deep CNN framework. We then further propose an
equally effective model based on fully convolutional networks and a novel
superpixel pooling method, which is $\sim 10$ times faster, to speedup the
patch-wise convolutions in the deep model. With this more efficient model, we
are able to design deeper networks to pursue better performance. Experiments on
both indoor and outdoor scene datasets demonstrate that the proposed method
outperforms state-of-the-art depth estimation approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07414</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07414</id><created>2015-02-25</created><authors><author><keyname>La</keyname><forenames>Richard J.</forenames></author></authors><title>Interdependent Security with Strategic Agents and Cascades of Infection</title><categories>cs.SI cs.GT physics.soc-ph</categories><comments>16 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate cascades in networks consisting of strategic agents with
interdependent security. We assume that the strategic agents have choices
between i) investing in protecting themselves, ii) purchasing insurance to
transfer (some) risks, and iii) taking no actions. Using a population game
model, we study how various system parameters, such as node degrees, infection
propagation rate, and the probability with which infected nodes transmit
infection to neighbors, affect nodes' choices at Nash equilibria and the
resultant price of anarchy/stability. In addition, we examine how the
probability that a single infected node can spread the infection to a
significant portion of the entire network, called cascade probability, behaves
with respect to system parameters. In particular, we demonstrate that, at least
for some parameter regimes, the cascade probability increases with the average
degree of nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07423</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07423</id><created>2015-02-25</created><authors><author><keyname>Peng</keyname><forenames>Xi</forenames></author><author><keyname>Lu</keyname><forenames>Canyi</forenames></author><author><keyname>Yi</keyname><forenames>Zhang</forenames></author><author><keyname>Tang</keyname><forenames>Huajin</forenames></author></authors><title>Connections Between Nuclear Norm and Frobenius Norm Based Representation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several recent works have shown that Frobenius-Norm based Representation
(FNR) is comparable with Sparse Representation (SR) and Nuclear-Norm based
Representation (NNR) in face recognition and subspace clustering. Despite the
success of FNR in experimental studies, less theoretical analysis is provided
to understand its working mechanism. In this paper, we fill this gap by
bridging FNR and NNR. More specially, we prove that: 1) when the dictionary can
provide enough representative capacity, FNR is exactly the NNR; 2) Otherwise,
FNR and NNR are two solutions on the column space of the dictionary. The first
result provides a novel theoretical explanation towards some existing FNR based
methods by crediting their success to low rank property. The second result
provides a new insight to understand FNR and NNR under a unified framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07424</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07424</id><created>2015-02-25</created><authors><author><keyname>Nikou</keyname><forenames>Alexandros</forenames></author><author><keyname>Gavridis</keyname><forenames>Georgios C.</forenames></author><author><keyname>Kyriakopoulos</keyname><forenames>Kostas J.</forenames></author></authors><title>Mechanical Design, Modelling and Control of a Novel Aerial Manipulator</title><categories>cs.RO cs.SY</categories><comments>Comments: 8 Pages, 2015 IEEE International Conference on Robotics and
  Automation (ICRA '15), Seattle, WA, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a novel aerial manipulation system is proposed. The mechanical
structure of the system, the number of thrusters and their geometry will be
derived from technical optimization problems. The aforementioned problems are
defined by taking into consideration the desired actuation forces and torques
applied to the end-effector of the system. The framework of the proposed system
is designed in a CAD Package in order to evaluate the system parameter values.
Following this, the kinematic and dynamic models are developed and an adaptive
backstepping controller is designed aiming to control the exact position and
orientation of the end-effector in the Cartesian space. Finally, the
performance of the system is demonstrated through a simulation study, where a
manipulation task scenario is investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07425</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07425</id><created>2015-02-25</created><authors><author><keyname>Wu</keyname><forenames>Yueping</forenames></author><author><keyname>Cui</keyname><forenames>Ying</forenames></author><author><keyname>Clerckx</keyname><forenames>Bruno</forenames></author></authors><title>Analysis and Optimization of Interference Nulling in Downlink
  Multi-Antenna HetNets with Offloading</title><categories>cs.IT math.IT</categories><comments>to appear in ICC 2015. arXiv admin note: text overlap with
  arXiv:1411.3271</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heterogeneous networks (HetNets) with offloading is considered as an
effective way to meet the high data rate demand of future wireless service.
However, the offloaded users suffer from strong inter-tier interference, which
reduces the benefits of offloading and is one of the main limiting factors of
the system performance. In this paper, we investigate the use of an
interference nulling (IN) beamforming scheme to improve the system performance
by carefully managing the inter-tier interference to the offloaded users in
downlink two-tier HetNets with multi-antenna base stations. Utilizing tools
from stochastic geometry, we derive a tractable expression for the rate
coverage probability of the IN scheme. Then, we optimize the design parameter,
i.e., the degrees of freedom that can be used for IN, to maximize the rate
coverage probability. Specifically, in the asymptotic scenario where the rate
threshold is small, by studying the order behavior of the rate coverage
probability, we characterize the optimal design parameter. For the general
scenario, we show some properties of the optimal design parameter. Finally, by
numerical simulations, we show the IN scheme can outperform both the simple
offloading scheme without interference management and the almost blank
subframes scheme in 3GPP LTE, especially in large antenna regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07428</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07428</id><created>2015-02-25</created><updated>2015-06-19</updated><authors><author><keyname>Liebman</keyname><forenames>Elad</forenames></author><author><keyname>Chor</keyname><forenames>Benny</forenames></author><author><keyname>Stone</keyname><forenames>Peter</forenames></author></authors><title>Representative Selection in Non Metric Datasets</title><categories>cs.AI</categories><doi>10.1080/08839514.2015.1071092</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of representative selection: choosing a
subset of data points from a dataset that best represents its overall set of
elements. This subset needs to inherently reflect the type of information
contained in the entire set, while minimizing redundancy. For such purposes,
clustering may seem like a natural approach. However, existing clustering
methods are not ideally suited for representative selection, especially when
dealing with non-metric data, where only a pairwise similarity measure exists.
In this paper we propose $\delta$-medoids, a novel approach that can be viewed
as an extension to the $k$-medoids algorithm and is specifically suited for
sample representative selection from non-metric data. We empirically validate
$\delta$-medoids in two domains, namely music analysis and motion analysis. We
also show some theoretical bounds on the performance of $\delta$-medoids and
the hardness of representative selection in general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07431</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07431</id><created>2015-02-25</created><authors><author><keyname>Wang</keyname><forenames>Zihe</forenames></author><author><keyname>Tang</keyname><forenames>Pingzhong</forenames></author></authors><title>Optimal commitments in auctions with incomplete information</title><categories>cs.GT</categories><comments>33 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are interested in the problem of optimal commitments in rank-and-bid based
auctions, a general class of auctions that include first price and all-pay
auctions as special cases. Our main contribution is a novel approach to solve
for optimal commitment in this class of auctions, for any continuous type
distributions. Applying our approach, we are able to solve optimal commitments
for first-price and all-pay auctions in closed-form for fairly general
distribution settings. The optimal commitments functions in these auctions
reveal two surprisingly opposite insights: in the optimal commitment, the
leader bids passively when he has a low type. We interpret this as a credible
way to alleviate competition and to collude. In sharp contrast, when his type
is high enough, the leader sometimes would go so far as to bid above his own
value. We interpret this as a credible way to threat. Combing both insights, we
show via concrete examples that the leader is indeed willing to do so to secure
more utility when his type is in the middle. Our main approach consists of a
series of nontrivial innovations. In particular we put forward a concept called
equal-bid function that connects both players' strategies, as well as a concept
called equal-utility curve that smooths any leader strategy into a continuous
and differentiable strategy. We believe these techniques and insights are
general and can be applied to similar problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07432</identifier>
 <datestamp>2015-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07432</id><created>2015-02-25</created><updated>2015-11-17</updated><authors><author><keyname>Chen</keyname><forenames>Yu-Hui</forenames></author><author><keyname>Wei</keyname><forenames>Dennis</forenames></author><author><keyname>Newstadt</keyname><forenames>Gregory</forenames></author><author><keyname>Simmons</keyname><forenames>Jeffrey</forenames></author><author><keyname>Hero</keyname><forenames>Alfred</forenames></author></authors><title>Coercive Region-level Registration for Multi-modal Images</title><categories>cs.CV physics.data-an</categories><comments>This work has been accepted to International Conference on Image
  Processing (ICIP) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a coercive approach to simultaneously register and segment
multi-modal images which share similar spatial structure. Registration is done
at the region level to facilitate data fusion while avoiding the need for
interpolation. The algorithm performs alternating minimization of an objective
function informed by statistical models for pixel values in different
modalities. Hypothesis tests are developed to determine whether to refine
segmentations by splitting regions. We demonstrate that our approach has
significantly better performance than the state-of-the-art registration and
segmentation methods on microscopy images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07436</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07436</id><created>2015-02-26</created><updated>2015-02-27</updated><authors><author><keyname>Chen</keyname><forenames>Yu-Hui</forenames></author><author><keyname>Park</keyname><forenames>Se Un</forenames></author><author><keyname>Wei</keyname><forenames>Dennis</forenames></author><author><keyname>Newstadt</keyname><forenames>Gregory</forenames></author><author><keyname>Jackson</keyname><forenames>Michael</forenames></author><author><keyname>Simmons</keyname><forenames>Jeff P.</forenames></author><author><keyname>De Graef</keyname><forenames>Marc</forenames></author><author><keyname>Hero</keyname><forenames>Alfred O.</forenames></author></authors><title>A Dictionary Approach to EBSD Indexing</title><categories>cs.CV physics.data-an stat.AP</categories><comments>This paper is in press in the Journal of Microscopy and
  Microanalysis, Cambridge University Press, Feb. 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a framework for indexing of grain and sub-grain structures in
electron backscatter diffraction (EBSD) images of polycrystalline materials.
The framework is based on a previously introduced physics-based forward model
by Callahan and De Graef (2013) relating measured patterns to grain
orientations (Euler angle). The forward model is tuned to the microscope and
the sample symmetry group. We discretize the domain of the forward model onto a
dense grid of Euler angles and for each measured pattern we identify the most
similar patterns in the dictionary. These patterns are used to identify
boundaries, detect anomalies, and index crystal orientations. The statistical
distribution of these closest matches is used in an unsupervised binary
decision tree (DT) classifier to identify grain boundaries and anomalous
regions. The DT classifies a pattern as an anomaly if it has an abnormally low
similarity to any pattern in the dictionary. It classifies a pixel as being
near a grain boundary if the highly ranked patterns in the dictionary differ
significantly over the pixels 3x3 neighborhood. Indexing is accomplished by
computing the mean orientation of the closest dictionary matches to each
pattern. The mean orientation is estimated using a maximum likelihood approach
that models the orientation distribution as a mixture of Von Mises-Fisher
distributions over the quaternionic 3-sphere. The proposed dictionary matching
approach permits segmentation, anomaly detection, and indexing to be performed
in a unified manner with the additional benefit of uncertainty quantification.
We demonstrate the proposed dictionary-based approach on a Ni-base IN100 alloy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07439</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07439</id><created>2015-02-26</created><updated>2016-02-14</updated><authors><author><keyname>Hung</keyname><forenames>Hui-Ju</forenames></author><author><keyname>Shuai</keyname><forenames>Hong-Han</forenames></author><author><keyname>Yang</keyname><forenames>De-Nian</forenames></author><author><keyname>Huang</keyname><forenames>Liang-Hao</forenames></author><author><keyname>Lee</keyname><forenames>Wang-Chien</forenames></author><author><keyname>Pei</keyname><forenames>Jian</forenames></author><author><keyname>Chen</keyname><forenames>Ming-Syan</forenames></author></authors><title>When Social Influence Meets Item Inference</title><categories>cs.SI</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research issues and data mining techniques for product recommendation and
viral marketing have been widely studied. Existing works on seed selection in
social networks do not take into account the effect of product recommendations
in e-commerce stores. In this paper, we investigate the seed selection problem
for viral marketing that considers both effects of social influence and item
inference (for product recommendation). We develop a new model, Social Item
Graph (SIG), that captures both effects in form of hyperedges. Accordingly, we
formulate a seed selection problem, called Social Item Maximization Problem
(SIMP), and prove the hardness of SIMP. We design an efficient algorithm with
performance guarantee, called Hyperedge-Aware Greedy (HAG), for SIMP and
develop a new index structure, called SIG-index, to accelerate the computation
of diffusion process in HAG. Moreover, to construct realistic SIG models for
SIMP, we develop a statistical inference based framework to learn the weights
of hyperedges from data. Finally, we perform a comprehensive evaluation on our
proposals with various baselines. Experimental result validates our ideas and
demonstrates the effectiveness and efficiency of the proposed model and
algorithms over baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07446</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07446</id><created>2015-02-26</created><authors><author><keyname>Schwambach</keyname><forenames>V&#xed;tor</forenames></author><author><keyname>Cleyet-Merle</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Issard</keyname><forenames>Alain</forenames></author><author><keyname>Mancini</keyname><forenames>St&#xe9;phane</forenames></author></authors><title>Estimating the Potential Speedup of Computer Vision Applications on
  Embedded Multiprocessors</title><categories>cs.CV cs.DC cs.PF</categories><comments>Presented at DATE Friday Workshop on Heterogeneous Architectures and
  Design Methods for Embedded Image Systems (HIS 2015) (arXiv:1502.07241)</comments><proxy>Frank Hannig</proxy><report-no>DATEHIS/2015/01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computer vision applications constitute one of the key drivers for embedded
multicore architectures. Although the number of available cores is increasing
in new architectures, designing an application to maximize the utilization of
the platform is still a challenge. In this sense, parallel performance
prediction tools can aid developers in understanding the characteristics of an
application and finding the most adequate parallelization strategy. In this
work, we present a method for early parallel performance estimation on embedded
multiprocessors from sequential application traces. We describe its
implementation in Parana, a fast trace-driven simulator targeting OpenMP
applications on the STMicroelectronics' STxP70 Application-Specific
Multiprocessor (ASMP). Results for the FAST key point detector application show
an error margin of less than 10% compared to the reference cycle-approximate
simulator, with lower modeling effort and up to 20x faster execution time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07447</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07447</id><created>2015-02-26</created><authors><author><keyname>Arslan</keyname><forenames>Mehmet Ali</forenames></author><author><keyname>Gruian</keyname><forenames>Flavius</forenames></author><author><keyname>Kuchcinski</keyname><forenames>Krzysztof</forenames></author></authors><title>A Comparative Study of Scheduling Techniques for Multimedia Applications
  on SIMD Pipelines</title><categories>cs.DC cs.PL</categories><comments>Presented at DATE Friday Workshop on Heterogeneous Architectures and
  Design Methods for Embedded Image Systems (HIS 2015) (arXiv:1502.07241)</comments><proxy>Frank Hannig</proxy><report-no>DATEHIS/2015/02</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parallel architectures are essential in order to take advantage of the
parallelism inherent in streaming applications. One particular branch of these
employ hardware SIMD pipelines. In this paper, we analyse several scheduling
techniques, namely ad hoc overlapped execution, modulo scheduling and modulo
scheduling with unrolling, all of which aim to efficiently utilize the special
architecture design. Our investigation focuses on improving throughput while
analysing other metrics that are important for streaming applications, such as
register pressure, buffer sizes and code size. Through experiments conducted on
several media benchmarks, we present and discuss trade-offs involved when
selecting any one of these scheduling techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07448</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07448</id><created>2015-02-26</created><authors><author><keyname>Reiche</keyname><forenames>Oliver</forenames></author><author><keyname>H&#xe4;ublein</keyname><forenames>Konrad</forenames></author><author><keyname>Reichenbach</keyname><forenames>Marc</forenames></author><author><keyname>Hannig</keyname><forenames>Frank</forenames></author><author><keyname>Teich</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Fey</keyname><forenames>Dietmar</forenames></author></authors><title>Automatic Optimization of Hardware Accelerators for Image Processing</title><categories>cs.PL cs.CV</categories><comments>Presented at DATE Friday Workshop on Heterogeneous Architectures and
  Design Methods for Embedded Image Systems (HIS 2015) (arXiv:1502.07241)</comments><proxy>Frank Hannig</proxy><report-no>DATEHIS/2015/03</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the domain of image processing, often real-time constraints are required.
In particular, in safety-critical applications, such as X-ray computed
tomography in medical imaging or advanced driver assistance systems in the
automotive domain, timing is of utmost importance. A common approach to
maintain real-time capabilities of compute-intensive applications is to offload
those computations to dedicated accelerator hardware, such as Field
Programmable Gate Arrays (FPGAs). Programming such architectures is a
challenging task, with respect to the typical FPGA-specific design criteria:
Achievable overall algorithm latency and resource usage of FPGA primitives
(BRAM, FF, LUT, and DSP). High-Level Synthesis (HLS) dramatically simplifies
this task by enabling the description of algorithms in well-known higher
languages (C/C++) and its automatic synthesis that can be accomplished by HLS
tools. However, algorithm developers still need expert knowledge about the
target architecture, in order to achieve satisfying results. Therefore, in
previous work, we have shown that elevating the description of image algorithms
to an even higher abstraction level, by using a Domain-Specific Language (DSL),
can significantly cut down the complexity for designing such algorithms for
FPGAs. To give the developer even more control over the common trade-off,
latency vs. resource usage, we will present an automatic optimization process
where these criteria are analyzed and fed back to the DSL compiler, in order to
generate code that is closer to the desired design specifications. Finally, we
generate code for stereo block matching algorithms and compare it with
handwritten implementations to quantify the quality of our results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07449</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07449</id><created>2015-02-26</created><authors><author><keyname>Shi</keyname><forenames>Lan</forenames></author><author><keyname>Soell</keyname><forenames>Christopher</forenames></author><author><keyname>Baenisch</keyname><forenames>Andreas</forenames></author><author><keyname>Weigel</keyname><forenames>Robert</forenames></author><author><keyname>Seiler</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Ussmueller</keyname><forenames>Thomas</forenames></author></authors><title>Concept for a CMOS Image Sensor Suited for Analog Image Pre-Processing</title><categories>cs.ET cs.AR cs.CV</categories><comments>Presented at DATE Friday Workshop on Heterogeneous Architectures and
  Design Methods for Embedded Image Systems (HIS 2015) (arXiv:1502.07241)</comments><proxy>Frank Hannig</proxy><report-no>DATEHIS/2015/04</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A concept for a novel CMOS image sensor suited for analog image
pre-processing is presented in this paper. As an example, an image restoration
algorithm for reducing image noise is applied as image pre-processing in the
analog domain. To supply low-latency data input for analog image preprocessing,
the proposed concept for a CMOS image sensor offers a new sensor signal
acquisition method in 2D. In comparison to image pre-processing in the digital
domain, the proposed analog image pre-processing promises an improved image
quality. Furthermore, the image noise at the stage of analog sensor signal
acquisition can be used to select the most effective restoration algorithm
applied to the analog circuit due to image processing prior to the A/D
converter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07451</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07451</id><created>2015-02-26</created><authors><author><keyname>Wu</keyname><forenames>Hao</forenames></author><author><keyname>Lohmann</keyname><forenames>Daniel</forenames></author><author><keyname>Schr&#xf6;der-Preikschat</keyname><forenames>Wolfgang</forenames></author></authors><title>A Graph-Partition-Based Scheduling Policy for Heterogeneous
  Architectures</title><categories>cs.DC</categories><comments>Presented at DATE Friday Workshop on Heterogeneous Architectures and
  Design Methods for Embedded Image Systems (HIS 2015) (arXiv:1502.07241)</comments><proxy>Frank Hannig</proxy><report-no>DATEHIS/2015/05</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to improve system performance efficiently, a number of systems
choose to equip multi-core and many-core processors (such as GPUs). Due to
their discrete memory these heterogeneous architectures comprise a distributed
system within a computer. A data-flow programming model is attractive in this
setting for its ease of expressing concurrency. Programmers only need to define
task dependencies without considering how to schedule them on the hardware.
However, mapping the resulting task graph onto hardware efficiently remains a
challenge. In this paper, we propose a graph-partition scheduling policy for
mapping data-flow workloads to heterogeneous hardware. According to our
experiments, our graph-partition-based scheduling achieves comparable
performance to conventional queue-base approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07453</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07453</id><created>2015-02-26</created><authors><author><keyname>Hartmann</keyname><forenames>Christian</forenames></author><author><keyname>Yupatova</keyname><forenames>Anna</forenames></author><author><keyname>Reichenbach</keyname><forenames>Marc</forenames></author><author><keyname>Fey</keyname><forenames>Dietmar</forenames></author><author><keyname>German</keyname><forenames>Reinhard</forenames></author></authors><title>A Holistic Approach for Modeling and Synthesis of Image Processing
  Applications for Heterogeneous Computing Architectures</title><categories>cs.CV</categories><comments>Presented at DATE Friday Workshop on Heterogeneous Architectures and
  Design Methods for Embedded Image Systems (HIS 2015) (arXiv:1502.07241)</comments><proxy>Frank Hannig</proxy><report-no>DATEHIS/2015/06</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image processing applications are common in every field of our daily life.
However, most of them are very complex and contain several tasks with different
complexities which result in varying requirements for computing architectures.
Nevertheless, a general processing scheme in every image processing application
has a similar structure, called image processing pipeline: (1) capturing an
image, (2) pre-processing using local operators, (3) processing with global
operators and (4) post-processing using complex operations. Therefore,
application-specialized hardware solutions based on heterogeneous architectures
are used for image processing. Unfortunately the development of applications
for heterogeneous hardware architectures is challenging due to the distribution
of computational tasks among processors and programmable logic units. Nowadays,
image processing systems are started from scratch which is time-consuming,
error-prone and inflexible. A new methodology for modeling and implementing is
needed in order to reduce the development time of heterogenous image processing
systems. This paper introduces a new holistic top down approach for image
processing systems. Two challenges have to be investigated. First, designers
ought to be able to model their complete image processing pipeline on an
abstract layer using UML. Second, we want to close the gap between the abstract
system and the system architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07454</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07454</id><created>2015-02-26</created><authors><author><keyname>Dasygenis</keyname><forenames>Minas</forenames></author></authors><title>Generation and Validation of Custom Multiplication IP Blocks from the
  Web</title><categories>cs.AR</categories><comments>Presented at DATE Friday Workshop on Heterogeneous Architectures and
  Design Methods for Embedded Image Systems (HIS 2015) (arXiv:1502.07241)</comments><proxy>Frank Hannig</proxy><report-no>DATEHIS/2015/07</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Every CPU carries one or more arithmetical and logical units. One popular
operation that is performed by these units is multiplication. Automatic
generation of custom VHDL models for performing this operation, allows the
designer to achieve a time efficient design space exploration. Although these
units are heavily utilized in modern digital circuits and DSP, there is no
tool, accessible from the web, to generate the HDL description of such designs
for arbitrary and different input bitwidths. In this paper, we present our web
accessible tool to construct completely custom optimized multiplication units
together with random generated test vectors for their verification. Our novel
tool is one of the firsts web based EDA tools to automate the design of such
units and simultaneously provide custom testbenches to verify their
correctness. Our synthesized circuits on Xilinx Virtex 6 FPGA, operate up to
589 Mhz.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07466</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07466</id><created>2015-02-26</created><authors><author><keyname>de Le&#xf3;n</keyname><forenames>Hern&#xe1;n Ponce</forenames></author><author><keyname>Bonigo</keyname><forenames>Gonzalo</forenames></author><author><keyname>Briones</keyname><forenames>Laura Brand&#xe1;n</forenames></author></authors><title>Distributed Analysis for Diagnosability in Concurrent Systems</title><categories>cs.SE cs.LO</categories><comments>In International Workshop on Principles of Diagnosis. 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex systems often exhibit unexpected faults that are difficult to handle.
Such systems are desirable to be diagnosable, i.e. faults can be automatically
detected as they occur (or shortly afterwards), enabling the system to handle
the fault or recover. A system is diagnosable if it is possible to detect every
fault, in a finite time after they occurred, by only observing the available
information from the system. Complex systems are usually built from simpler
components running concurrently. We study how to infer the diagnosability
property of a complex system (distributed and with multiple faults) from a
parallelized analysis of the diagnosability of each of its components
synchronizing with fault free versions of the others.
  In this paper we make the following contributions: (1) we address the
diagnosability problem of concurrent systems with arbitrary faults occurring
freely in each component. (2) We distribute the diagnosability analysis and
illustrate our approach with examples. Moreover, (3) we present a prototype
tool that implements our techniques showing promising results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07467</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07467</id><created>2015-02-26</created><updated>2015-04-28</updated><authors><author><keyname>Datta</keyname><forenames>Samir</forenames></author><author><keyname>Kulkarni</keyname><forenames>Raghav</forenames></author><author><keyname>Mukherjee</keyname><forenames>Anish</forenames></author><author><keyname>Schwentick</keyname><forenames>Thomas</forenames></author><author><keyname>Zeume</keyname><forenames>Thomas</forenames></author></authors><title>Reachability is in DynFO</title><categories>cs.LO cs.CC cs.DS</categories><comments>To appear in: ICALP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the dynamic complexity of some central graph problems such as
Reachability and Matching and linear algebraic problems such as Rank and
Inverse. As elementary change operations we allow insertion and deletion of
edges of a graph and the modification of a single entry in a matrix, and we are
interested in the complexity of maintaining a property or query. Our main
results are as follows: (1) Reachability is in DynFO; (2) Rank of a matrix is
in DynFO(+,x); (3) Maximum Matching (decision) is in non-uniform DynFO.
  Here, DynFO allows updates of the auxiliary data structure defined in
first-order logic, DynFO(+,x) additionally has arithmetics at initialization
time and non-uniform DynFO allows arbitrary auxiliary data at initialization
time. Alternatively, DynFO(+,x) and non-uniform DynFO allow updates by uniform
and non-uniform families of poly-size, bounded-depth circuits, respectively.
  The first result confirms a two decade old conjecture of Patnaik and Immerman
(1997). The proofs rely mainly on elementary Linear Algebra. The second result
can also be concluded from Frandsen and Frandsen (2009).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07469</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07469</id><created>2015-02-26</created><authors><author><keyname>Nair</keyname><forenames>Divya G.</forenames></author><author><keyname>Binu</keyname><forenames>V. P.</forenames></author><author><keyname>Kumar</keyname><forenames>G. Santhosh</forenames></author></authors><title>An Improved E-voting scheme using Secret Sharing based Secure
  Multi-party Computation</title><categories>cs.CR</categories><comments>Eighth International Conference on Computer communication networks
  (ICCN 2014) ISBN : 9789351072539</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  E-voting systems (EVS)are having potential advantages over many existing
voting schemes.Security, transparency, accuracy and reliability are the major
concern in these systems.EVS continues to grow as the technology advances.It is
inexpensive and efficient as the resources become reusable.Fast and accurate
computation of results with voter privacy is the added advantage.In the
proposed system we make use of secret sharing technique and secure multi party
computation(SMC) to achieve security and reliability.Secret sharing is an
important technique used for SMC. Multi-party computation is typically
accomplished using secret sharing by making shares of the input and
manipulating the shares to compute a typical function of the input.The proposed
system make use of bitwise representation of votes and only the shares are used
for transmission and computation of result.Secure sum evaluation can be done
with shares distributed using Shamir's secret sharing scheme.The scheme is
hence secure and reliable and does not make any number theoretic assumptions
for security.We also propose a unique method which calculates the candidates
individual votes keeping the anonymity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07475</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07475</id><created>2015-02-26</created><authors><author><keyname>Binu</keyname><forenames>V. P.</forenames></author><author><keyname>Sreekumar</keyname><forenames>A.</forenames></author></authors><title>Simple and Efficient Secret Sharing Schemes for Sharing Data and Image</title><categories>cs.CR</categories><journal-ref>(IJCSIT) International Journal of Computer Science and Information
  Technologies, Vol. 6 (1) , 2015, 404-409 ,</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Secret sharing is a new alternative for outsourcing data in a secure way.It
avoids the need for time consuming encryption decryption process and also the
complexity involved in key management.The data must also be protected from
untrusted cloud service providers.Secret sharing based solution provides secure
information dispersal by making shares of the original data and distribute them
among different servers.Data from the threshold number of servers can be used
to reconstruct the original data.It is often impractical to distribute data
among large number of servers.We have to achieve a trade off between security
and efficiency.An optimal choice is to use a $(2,3)$ or $(2,4)$ threshold
secret sharing scheme, where the data are distributed as shares among three or
four servers and shares from any two can be used to construct the original
data.This provides both security,reliability and efficiency.We propose some
efficient and easy to implement secret sharing schemes in this regard based on
number theory and bitwise XOR.These schemes are also suitable for secure
sharing of images.Secret image sharing based on Shamir's schemes are lossy and
involves complicated Lagrange interpolation.So the proposed scheme can also be
effectively utilized for lossless sharing of secret images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07481</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07481</id><created>2015-02-26</created><updated>2015-12-30</updated><authors><author><keyname>Liu</keyname><forenames>Zhongchang</forenames></author><author><keyname>Wong</keyname><forenames>Wing Shing</forenames></author></authors><title>Cluster Synchronization of Coupled Systems with Nonidentical Linear
  Dynamics</title><categories>cs.SY</categories><comments>22 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the cluster synchronization problem of generic linear
dynamical systems whose system models are distinct in different clusters. These
nonidentical linear models render control design and coupling conditions highly
correlated if static couplings are used for all individual systems. In this
paper, a dynamic coupling structure, which incorporates a global weighting
factor and a vanishing auxiliary control variable, is proposed for each agent
and is shown to be a feasible solution. Lower bounds on the global and local
weighting factors are derived under the condition that every interaction
subgraph associated with each cluster admits a directed spanning tree. The
spanning tree requirement is further shown to be a necessary condition when the
clusters connect acyclicly with each other. Simulations for two applications,
cluster heading alignment of nonidentical ships and cluster phase
synchronization of nonidentical harmonic oscillators, illustrate essential
parts of the derived theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07484</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07484</id><created>2015-02-26</created><authors><author><keyname>Maffray</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author></authors><title>Graphs with no induced wheel or antiwheel</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A wheel is a graph that consists of a chordless cycle of length at least 4
plus a vertex with at least three neighbors on the cycle. It was shown recently
that detecting induced wheels is an NP-complete problem. In contrast, it is
shown here that graphs that contain no wheel and no antiwheel have a very
simple structure and consequently can be recognized in polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07492</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07492</id><created>2015-02-26</created><authors><author><keyname>Hon</keyname><forenames>Wing-Kai</forenames></author><author><keyname>Kloks</keyname><forenames>Ton</forenames></author><author><keyname>Liu</keyname><forenames>Hsian-Hsuan</forenames></author><author><keyname>Wang</keyname><forenames>Hung-Lung</forenames></author></authors><title>Rainbow domination and related problems on some classes of perfect
  graphs</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $k \in \mathbb{N}$ and let $G$ be a graph. A function $f: V(G)
\rightarrow 2^{[k]}$ is a rainbow function if, for every vertex $x$ with
$f(x)=\emptyset$, $f(N(x)) =[k]$. The rainbow domination number
$\gamma_{kr}(G)$ is the minimum of $\sum_{x \in V(G)} |f(x)|$ over all rainbow
functions. We investigate the rainbow domination problem for some classes of
perfect graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07495</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07495</id><created>2015-02-26</created><authors><author><keyname>Georgatsos</keyname><forenames>Panos</forenames></author><author><keyname>Flegkas</keyname><forenames>Paris</forenames></author><author><keyname>Sourlas</keyname><forenames>Vasilis</forenames></author><author><keyname>Tassiulas</keyname><forenames>Leandros</forenames></author></authors><title>Object-Oriented Networking</title><categories>cs.NI</categories><comments>7 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose the object-oriented networking (OON) framework, for meeting the
generalized interconnection, mobility and technology integration requirements
underlining the Internet. In OON, the various objects that need to be accessed
through the Internet (content, smart things, services, people, etc.) are viewed
as network layer resources, rather than as application layer resources as in
the IP communications model. By abstracting them as computing objects -with
attributes and methods- they are identified by expressive, discoverable names,
while data are exchanged between them in the context of their methods, based on
suitably defined system-specific names. An OON-enabled Internet is not only a
global data delivery medium but also a universal object discovery and service
development platform; service-level interactions can be realized through native
network means, without requiring standardized protocols. OON can be realized
through existing software-defined networking or network functions
virtualization technologies and it can be deployed in an incremental fashion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07504</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07504</id><created>2015-02-26</created><authors><author><keyname>Nehar</keyname><forenames>Attia</forenames></author><author><keyname>Ziadi</keyname><forenames>Djelloul</forenames></author><author><keyname>Cherroun</keyname><forenames>Hadda</forenames></author></authors><title>Rational Kernels for Arabic Stemming and Text Classification</title><categories>cs.CL</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the problems of Arabic Text Classification and
stemming using Transducers and Rational Kernels. We introduce a new stemming
technique based on the use of Arabic patterns (Pattern Based Stemmer). Patterns
are modelled using transducers and stemming is done without depending on any
dictionary. Using transducers for stemming, documents are transformed into
finite state transducers. This document representation allows us to use and
explore rational kernels as a framework for Arabic Text Classification.
Stemming experiments are conducted on three word collections and classification
experiments are done on the Saudi Press Agency dataset. Results show that our
approach, when compared with other approaches, is promising specially in terms
of Accuracy, Recall and F1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07523</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07523</id><created>2015-02-26</created><authors><author><keyname>Shaghaghi</keyname><forenames>Mahdi</forenames></author><author><keyname>Vorobyov</keyname><forenames>Sergiy A.</forenames></author></authors><title>Cramer-Rao Bound for Sparse Signals Fitting the Low-Rank Model with
  Small Number of Parameters</title><categories>math.ST cs.IT math.IT stat.TH</categories><comments>14 pages, 1 figure, Submitted to IEEE Signal Processing Letters on
  December 2014</comments><journal-ref>IEEE Signal Processing Letters, vol. 22, no. 9, pp. 1497-1501,
  Sept. 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider signals with a low-rank covariance matrix which
reside in a low-dimensional subspace and can be written in terms of a finite
(small) number of parameters. Although such signals do not necessarily have a
sparse representation in a finite basis, they possess a sparse structure which
makes it possible to recover the signal from compressed measurements. We study
the statistical performance bound for parameter estimation in the low-rank
signal model from compressed measurements. Specifically, we derive the
Cramer-Rao bound (CRB) for a generic low-rank model and we show that the number
of compressed samples needs to be larger than the number of sources for the
existence of an unbiased estimator with finite estimation variance. We further
consider the applications to direction-of-arrival (DOA) and spectral estimation
which fit into the low-rank signal model. We also investigate the effect of
compression on the CRB by considering numerical examples of the DOA estimation
scenario, and show how the CRB increases by increasing the compression or
equivalently reducing the number of compressed samples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07526</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07526</id><created>2015-02-26</created><authors><author><keyname>Yuan</keyname><forenames>Ganzhao</forenames></author><author><keyname>Zhang</keyname><forenames>Zhenjie</forenames></author><author><keyname>Winslett</keyname><forenames>Marianne</forenames></author><author><keyname>Xiao</keyname><forenames>Xiaokui</forenames></author><author><keyname>Yang</keyname><forenames>Yin</forenames></author><author><keyname>Hao</keyname><forenames>Zhifeng</forenames></author></authors><title>Optimizing Batch Linear Queries under Exact and Approximate Differential
  Privacy</title><categories>cs.DB</categories><comments>ACM Transactions on Database Systems (ACM TODS). arXiv admin note:
  text overlap with arXiv:1212.2309</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Differential privacy is a promising privacy-preserving paradigm for
statistical query processing over sensitive data. It works by injecting random
noise into each query result, such that it is provably hard for the adversary
to infer the presence or absence of any individual record from the published
noisy results. The main objective in differentially private query processing is
to maximize the accuracy of the query results, while satisfying the privacy
guarantees. Previous work, notably \cite{LHR+10}, has suggested that with an
appropriate strategy, processing a batch of correlated queries as a whole
achieves considerably higher accuracy than answering them individually.
However, to our knowledge there is currently no practical solution to find such
a strategy for an arbitrary query batch; existing methods either return
strategies of poor quality (often worse than naive methods) or require
prohibitively expensive computations for even moderately large domains.
Motivated by this, we propose low-rank mechanism (LRM), the first practical
differentially private technique for answering batch linear queries with high
accuracy. LRM works for both exact (i.e., $\epsilon$-) and approximate (i.e.,
($\epsilon$, $\delta$)-) differential privacy definitions. We derive the
utility guarantees of LRM, and provide guidance on how to set the privacy
parameters given the user's utility expectation. Extensive experiments using
real data demonstrate that our proposed method consistently outperforms
state-of-the-art query processing solutions under differential privacy, by
large margins.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07540</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07540</id><created>2015-02-26</created><authors><author><keyname>Ray</keyname><forenames>Anupama</forenames></author><author><keyname>Rajeswar</keyname><forenames>Sai</forenames></author><author><keyname>Chaudhury</keyname><forenames>Santanu</forenames></author></authors><title>A hypothesize-and-verify framework for Text Recognition using Deep
  Recurrent Neural Networks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep LSTM is an ideal candidate for text recognition. However text
recognition involves some initial image processing steps like segmentation of
lines and words which can induce error to the recognition system. Without
segmentation, learning very long range context is difficult and becomes
computationally intractable. Therefore, alternative soft decisions are needed
at the pre-processing level. This paper proposes a hybrid text recognizer using
a deep recurrent neural network with multiple layers of abstraction and long
range context along with a language model to verify the performance of the deep
neural network. In this paper we construct a multi-hypotheses tree architecture
with candidate segments of line sequences from different segmentation
algorithms at its different branches. The deep neural network is trained on
perfectly segmented data and tests each of the candidate segments, generating
unicode sequences. In the verification step, these unicode sequences are
validated using a sub-string match with the language model and best first
search is used to find the best possible combination of alternative hypothesis
from the tree structure. Thus the verification framework using language models
eliminates wrong segmentation outputs and filters recognition errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07541</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07541</id><created>2015-02-26</created><updated>2015-08-15</updated><authors><author><keyname>Dokmanic</keyname><forenames>Ivan</forenames></author><author><keyname>Parhizkar</keyname><forenames>Reza</forenames></author><author><keyname>Ranieri</keyname><forenames>Juri</forenames></author><author><keyname>Vetterli</keyname><forenames>Martin</forenames></author></authors><title>Euclidean Distance Matrices: Essential Theory, Algorithms and
  Applications</title><categories>cs.OH</categories><comments>- 17 pages, 12 figures, to appear in IEEE Signal Processing Magazine
  - change of title in the last revision</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Euclidean distance matrices (EDM) are matrices of squared distances between
points. The definition is deceivingly simple: thanks to their many useful
properties they have found applications in psychometrics, crystallography,
machine learning, wireless sensor networks, acoustics, and more. Despite the
usefulness of EDMs, they seem to be insufficiently known in the signal
processing community. Our goal is to rectify this mishap in a concise tutorial.
We review the fundamental properties of EDMs, such as rank or
(non)definiteness. We show how various EDM properties can be used to design
algorithms for completing and denoising distance data. Along the way, we
demonstrate applications to microphone position calibration, ultrasound
tomography, room reconstruction from echoes and phase retrieval. By spelling
out the essential algorithms, we hope to fast-track the readers in applying
EDMs to their own problems. Matlab code for all the described algorithms, and
to generate the figures in the paper, is available online. Finally, we suggest
directions for further research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07545</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07545</id><created>2015-02-26</created><updated>2015-09-21</updated><authors><author><keyname>Pan</keyname><forenames>Feng</forenames></author></authors><title>SAT problem and statistical distance</title><categories>cs.CC</categories><comments>15 pages. arXiv admin note: text overlap with arXiv:quant-ph/0311110
  by other authors without attribution</comments><acm-class>F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper with two equivalent representations of the information
contained by a SAT formula, the reason why string generated by succinct SAT
formula can be greatly compressed is firstly presented based on Kolmogorov
complexity theory. Then what strings can be greatly compressed were classified
and discussed. In this way we discovered the SAT problem was composed of a
basic distinguish problem: distinguish two different distributions induced
under the computer with certain SAT formula ensemble. We then tried to map this
problem into quantum mechanics, or the quantum version basic distinguish
problem: this time two different distributions are induced under quantum
mechanics. Based on the equivalence of statistical distance between probability
space and Hilbert space, in the same time this distance is invariant under all
unitary transformations. The quantum version basic problem cannot be
efficiently solved by any quantum computer. In the worst case, any quantum
computer must perform exponential times measurement in order to solve it. In
the end we proposed the main theorem : The statistical distance in program
space and probability space are identical. We tried to prove it using the
relationship of Kolmogorov complexity and entropy. It showed there is no
difference to solve the basic problem in SAT formula space or probability
space. In the worst case, exponential trials must be performed to solve it.
NP!=P.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07549</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07549</id><created>2015-02-26</created><authors><author><keyname>Lin</keyname><forenames>T.</forenames></author></authors><title>Model-checking branching-time properties of probabilistic automata and
  probabilistic one-counter automata</title><categories>cs.LO cs.FL</categories><comments>Comments are welcome</comments><acm-class>F.4.1</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper studies the problem of model-checking of probabilistic automaton
and probabilistic one-counter automata against probabilistic branching-time
temporal logics (PCTL and PCTL$^*$). We show that it is undecidable for these
problems.
  We first show, by reducing to emptiness problem of probabilistic automata,
that the model-checking of probabilistic finite automata against branching-time
temporal logics are undecidable. And then, for each probabilistic automata, by
constructing a probabilistic one-counter automaton with the same behavior as
questioned probabilistic automata the undecidability of model-checking problems
against branching-time temporal logics are derived, herein.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07555</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07555</id><created>2015-02-26</created><authors><author><keyname>Andersen</keyname><forenames>Jakob L.</forenames></author><author><keyname>Flamm</keyname><forenames>Christoph</forenames></author><author><keyname>Merkle</keyname><forenames>Daniel</forenames></author><author><keyname>Stadler</keyname><forenames>Peter F.</forenames></author></authors><title>Support for Eschenmoser's Glyoxylate Scenario</title><categories>q-bio.MN cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A core topic of research in prebiotic chemistry is the search for plausible
synthetic routes that connect the building blocks of modern life such as
sugars, nucleotides, amino acids, and lipids to &quot;molecular food sources&quot; that
have likely been abundant on Early Earth. In a recent contribution, Albert
Eschenmoser emphasised the importance of catalytic and autocatalytic cycles in
establishing such abiotic synthesis pathways. The accumulation of intermediate
products furthermore provides additional catalysts that allow pathways to
change over time. We show here that generative models of chemical spaces based
on graph grammars make it possible to study such phenomena is a systematic
manner. In addition to repro- ducing the key steps of Eschenmoser's hypothesis
paper, we discovered previously unexplored potentially autocatalytic pathways
from HCN to glyoxylate. A cascading of autocatalytic cycles could efficiently
re-route matter, distributed over the combinatorial complex network of HCN
hydrolysation chemistry, towards a potential primordial metabolism. The
generative approach also has it intrinsic limitations: the unsupervised
expansion of the chemical space remains infeasible due to the exponential
growth of possible molecules and reactions between them. Here in particular the
combinatorial complexity of the HCN polymerisation and hydrolysation networks
forms the computational bottleneck. As a consequence, guidance of the
computational exploration by chemical experience is indispensable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07565</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07565</id><created>2015-02-26</created><updated>2016-02-17</updated><authors><author><keyname>Wu</keyname><forenames>Xiaofu</forenames></author><author><keyname>Yan</keyname><forenames>Zhen</forenames></author><author><keyname>Ling</keyname><forenames>Cong</forenames></author><author><keyname>Xia</keyname><forenames>Xiang-Gen</forenames></author></authors><title>Artificial-Noise-Aided Physical Layer Phase Challenge-Response
  Authentication for Practical OFDM Transmission</title><categories>cs.IT math.IT</categories><comments>33 pages, 13 figures, submitted for possible publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, we have developed a PHYsical layer Phase Challenge-Response
Authentication Scheme (PHY-PCRAS) for independent multicarrier transmission. In
this paper, we make a further step by proposing a novel artificial-noise-aided
PHY-PCRAS (ANA-PHY-PCRAS) for practical orthogonal frequency division
multiplexing (OFDM) transmission, where the Tikhonov-distributed artificial
noise is introduced to interfere with the phase-modulated key for resisting
potential key-recovery attacks whenever a static channel between two legitimate
users is unfortunately encountered. Then, we address various practical issues
for ANA-PHY-PCRAS with OFDM transmission, including correlation among
subchannels, imperfect carrier and timing recoveries. Among them, we show that
the effect of sampling offset is very significant and a search procedure in the
frequency domain should be incorporated for verification. With practical OFDM
transmission, the number of uncorrelated subchannels is often not sufficient.
Hence, we employ a time-separated approach for allocating enough subchannels
and a modified ANA-PHY-PCRAS is proposed to alleviate the discontinuity of
channel phase at far-separated time slots. Finally, the key equivocation is
derived for the worst case scenario. We conclude that the enhanced security of
ANA-PHY-PCRAS comes from the uncertainty of both the wireless channel and
introduced artificial noise, compared to the traditional challenge-response
authentication scheme implemented at the upper layer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07567</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07567</id><created>2015-02-26</created><authors><author><keyname>Wu</keyname><forenames>Xiaofu</forenames></author><author><keyname>Yang</keyname><forenames>Zhen</forenames></author></authors><title>A Channel Coding Approach for Physical-Layer Authentication</title><categories>cs.IT math.IT</categories><comments>5 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For physical-layer authentication, the authentication tags are often sent
concurrently with messages without much bandwidth expansion. In this paper, we
present a channel coding approach for physical-layer authentication. The
generation of authentication tags can be formulated as an encoding process for
an ensemble of codes, where the shared key between Alice and Bob is considered
as the input and the message is used to specify a code from the ensemble of
codes. Then, we show that the security of physical-layer authentication schemes
can be analyzed through decoding and physical-layer authentication schemes can
potentially achieve both information-theoretic and computational securities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07571</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07571</id><created>2015-02-26</created><updated>2015-02-28</updated><authors><author><keyname>Aleksandrov</keyname><forenames>Martin</forenames></author><author><keyname>Aziz</keyname><forenames>Haris</forenames></author><author><keyname>Gaspers</keyname><forenames>Serge</forenames></author><author><keyname>Walsh</keyname><forenames>Toby</forenames></author></authors><title>Online Fair Division: analysing a Food Bank problem</title><categories>cs.GT cs.AI cs.MA</categories><comments>7 pages, 2 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study an online model of fair division designed to capture features of a
real world charity problem. We consider two simple mechanisms for this model in
which agents simply declare what items they like. We analyse several axiomatic
properties of these mechanisms like strategy-proofness and envy-freeness.
Finally, we perform a competitive analysis and compute the price of anarchy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07576</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07576</id><created>2015-02-26</created><authors><author><keyname>Seba</keyname><forenames>Hamida</forenames></author><author><keyname>Lagraa</keyname><forenames>Sofiane</forenames></author><author><keyname>Ronando</keyname><forenames>Elsen</forenames></author></authors><title>Comparison Issues in Large Graphs: State of the Art and Future
  Directions</title><categories>cs.DS cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph comparison is fundamentally important for many applications such as the
analysis of social networks and biological data and has been a significant
research area in the pattern recognition and pattern analysis domains.
Nowadays, the graphs are large, they may have billions of nodes and edges.
Comparison issues in such huge graphs are a challenging research problem.
  In this paper, we survey the research advances of comparison problems in
large graphs. We review graph comparison and pattern matching approaches that
focus on large graphs. We categorize the existing approaches into three
classes: partition-based approaches, search space based approaches and summary
based approaches. All the existing algorithms in these approaches are described
in detail and analyzed according to multiple metrics such as time complexity,
type of graphs or comparison concept. Finally, we identify directions for
future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07577</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07577</id><created>2015-02-26</created><updated>2015-03-03</updated><authors><author><keyname>Dokmanic</keyname><forenames>Ivan</forenames></author><author><keyname>Lu</keyname><forenames>Yue M.</forenames></author></authors><title>Sampling Sparse Signals on the Sphere: Algorithms and Applications</title><categories>cs.IT cs.SD math.IT</categories><comments>14 pages, 8 figures, submitted to IEEE Transactions on Signal
  Processing</comments><doi>10.1109/TSP.2015.2478751</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a sampling scheme that can perfectly reconstruct a collection of
spikes on the sphere from samples of their lowpass-filtered observations.
Central to our algorithm is a generalization of the annihilating filter method,
a tool widely used in array signal processing and finite-rate-of-innovation
(FRI) sampling. The proposed algorithm can reconstruct $K$ spikes from
$(K+\sqrt{K})^2$ spatial samples. This sampling requirement improves over
previously known FRI sampling schemes on the sphere by a factor of four for
large $K$. We showcase the versatility of the proposed algorithm by applying it
to three different problems: 1) sampling diffusion processes induced by
localized sources on the sphere, 2) shot noise removal, and 3) sound source
localization (SSL) by a spherical microphone array. In particular, we show how
SSL can be reformulated as a spherical sparse sampling problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07586</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07586</id><created>2015-02-26</created><updated>2016-01-27</updated><authors><author><keyname>Dhifallah</keyname><forenames>Oussama</forenames></author><author><keyname>Dahrouj</keyname><forenames>Hayssam</forenames></author><author><keyname>Al-Naffouri</keyname><forenames>Tareq Y.</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Joint Hybrid Backhaul and Access Links Design in Cloud-Radio Access
  Networks</title><categories>cs.IT math.IT</categories><comments>6 pages, 3 figures, IWCPM 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The cloud-radio access network (CRAN) is expected to be the core network
architecture for next generation mobile radio systems. In this paper, we
consider the downlink of a CRAN formed of one central processor (the cloud) and
several base-station (BS), where each BS is connected to the cloud via either a
wireless or capacity-limited wireline backhaul link. The paper addresses the
joint design of the hybrid backhaul links (i.e., designing the wireline and
wireless backhaul connections from the cloud to the BSs) and the access links
(i.e., determining the sparse beamforming solution from the BSs to the users).
The paper formulates the hybrid backhaul and access link design problem by
minimizing the total network power consumption. The paper solves the problem
using a two-stage heuristic algorithm. At one stage, the sparse beamforming
solution is found using a weighted mixed `1=`2 norm minimization approach; the
correlation matrix of the quantization noise of the wireline backhaul links is
computed using the classical rate-distortion theory. At the second stage, the
transmit powers of the wireless backhaul links are found by solving a power
minimization problem subject to quality-of-service constraints, based on the
principle of conservation of rate by utilizing the rates found in the first
stage. Simulation results suggest that the performance of the proposed
algorithm approaches the global optimum solution, especially at high
signal-to-interference-plus-noise ratio (SINR).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07591</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07591</id><created>2015-02-26</created><updated>2015-03-04</updated><authors><author><keyname>Moore</keyname><forenames>Cristopher</forenames></author></authors><title>The phase transition in random regular exact cover</title><categories>cs.CC cond-mat.stat-mech math.CO math.PR</categories><comments>Added sentence pointing out that the threshold is never an integer</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A $k$-uniform, $d$-regular instance of Exact Cover is a family of $m$ sets
$F_{n,d,k} = \{ S_j \subseteq \{1,...,n\} \}$, where each subset has size $k$
and each $1 \le i \le n$ is contained in $d$ of the $S_j$. It is satisfiable if
there is a subset $T \subseteq \{1,...,n\}$ such that $|T \cap S_j|=1$ for all
$j$. Alternately, we can consider it a $d$-regular instance of Positive
1-in-$k$ SAT, i.e., a Boolean formula with $m$ clauses and $n$ variables where
each clause contains $k$ variables and demands that exactly one of them is
true. We determine the satisfiability threshold for random instances of this
type with $k &gt; 2$. Letting $d^\star = \frac{\ln k}{(k-1)(- \ln (1-1/k))} + 1$,
we show that $F_{n,d,k}$ is satisfiable with high probability if $d &lt; d^\star$
and unsatisfiable with high probability if $d &gt; d^\star$. We do this with a
simple application of the first and second moment methods, boosting the
probability of satisfiability below $d^\star$ to $1-o(1)$ using the small
subgraph conditioning method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07598</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07598</id><created>2015-02-26</created><updated>2015-05-16</updated><authors><author><keyname>Li</keyname><forenames>Hang</forenames></author><author><keyname>Huang</keyname><forenames>Chuan</forenames></author><author><keyname>Zhang</keyname><forenames>Ping</forenames></author><author><keyname>Cui</keyname><forenames>Shuguang</forenames></author><author><keyname>Zhang</keyname><forenames>Junshan</forenames></author></authors><title>Distributed Opportunistic Scheduling for Energy Harvesting Based
  Wireless Networks: A Two-Stage Probing Approach</title><categories>cs.IT math.IT</categories><comments>14 pages, 5 figures, accepted by IEEE/ACM Transactions on Networking</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a heterogeneous ad hoc network with multiple
transmitter-receiver pairs, in which all transmitters are capable of harvesting
renewable energy from the environment and compete for one shared channel by
random access. In particular, we focus on two different scenarios: the constant
energy harvesting (EH) rate model where the EH rate remains constant within the
time of interest and the i.i.d. EH rate model where the EH rates are
independent and identically distributed across different contention slots. To
quantify the roles of both the energy state information (ESI) and the channel
state information (CSI), a distributed opportunistic scheduling (DOS) framework
with two-stage probing and save-then-transmit energy utilization is proposed.
Then, the optimal throughput and the optimal scheduling strategy are obtained
via one-dimension search, i.e., an iterative algorithm consisting of the
following two steps in each iteration: First, assuming that the stored energy
level at each transmitter is stationary with a given distribution, the expected
throughput maximization problem is formulated as an optimal stopping problem,
whose solution is proved to exist and then derived for both models; second, for
a fixed stopping rule, the energy level at each transmitter is shown to be
stationary and an efficient iterative algorithm is proposed to compute its
steady-state distribution. Finally, we validate our analysis by numerical
results and quantify the throughput gain compared with the best-effort delivery
scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07600</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07600</id><created>2015-02-26</created><authors><author><keyname>Li</keyname><forenames>Zijia</forenames></author><author><keyname>Schicho</keyname><forenames>Josef</forenames></author><author><keyname>Schr&#xf6;cker</keyname><forenames>Hans-Peter</forenames></author></authors><title>Factorization of Motion Polynomials</title><categories>cs.SC cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the existence of a factorization of a monic,
bounded motion polynomial. We prove existence of factorizations, possibly after
multiplication with a real polynomial and provide algorithms for computing
polynomial factor and factorizations. The first algorithm is conceptually
simpler but may require a high degree of the polynomial factor. The second
algorithm gives an optimal degree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07601</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07601</id><created>2015-02-26</created><updated>2015-03-03</updated><authors><author><keyname>Drchal</keyname><forenames>Jan</forenames></author><author><keyname>&#x10c;ertick&#xfd;</keyname><forenames>Michal</forenames></author><author><keyname>Jakob</keyname><forenames>Michal</forenames></author></authors><title>Data Driven Validation Framework for Multi-agent Activity-based Models</title><categories>cs.MA</categories><comments>12 pages, submitted to MABS: Multi-Agent Systems and Agent-Based
  Simulation 2015 (The Sixteenth International Workshop on Multi-Agent-Based
  Simulation)</comments><acm-class>I.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Activity-based models, as a specific instance of agent-based models, deal
with agents that structure their activity in terms of (daily) activity
schedules. An activity schedule consists of a sequence of activity instances,
each with its assigned start time, duration and location, together with
transport modes used for travel between subsequent activity locations. A
critical step in the development of simulation models is validation. Despite
the growing importance of activity-based models in modelling transport and
mobility, there has been so far no work focusing specifically on statistical
validation of such models. In this paper, we propose a six-step Validation
Framework for Activity-based Models (VALFRAM) that allows exploiting historical
real-world data to assess the validity of activity-based models. The framework
compares temporal and spatial properties and the structure of activity
schedules against real-world travel diaries and origin-destination matrices. We
confirm the usefulness of the framework on three real-world activity-based
transport models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07608</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07608</id><created>2015-02-26</created><authors><author><keyname>Brinkmann</keyname><forenames>Steffen</forenames></author><author><keyname>Gracia</keyname><forenames>Jose</forenames></author></authors><title>CppSs -- a C++ Library for Efficient Task Parallelism</title><categories>cs.DC</categories><comments>accepted for publication at INFOCOMP, work-in-progress track</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the C++ library CppSs (C++ super-scalar), which provides efficient
task-parallelism without the need for special compilers or other software. Any
C++ compiler that supports C++11 is sufficient. CppSs features different
directionality clauses for defining data dependencies. While the variable
argument lists of the taskified functions are evaluated at compile time, the
resulting task dependencies are fixed by the runtime value of the arguments and
are thus analysed at runtime. With CppSs, we provide task-parallelism using
merely native C++.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07617</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07617</id><created>2015-02-26</created><authors><author><keyname>Alon</keyname><forenames>Noga</forenames></author><author><keyname>Cesa-Bianchi</keyname><forenames>Nicol&#xf2;</forenames></author><author><keyname>Dekel</keyname><forenames>Ofer</forenames></author><author><keyname>Koren</keyname><forenames>Tomer</forenames></author></authors><title>Online Learning with Feedback Graphs: Beyond Bandits</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a general class of online learning problems where the feedback is
specified by a graph. This class includes online prediction with expert advice
and the multi-armed bandit problem, but also several learning problems where
the online player does not necessarily observe his own loss. We analyze how the
structure of the feedback graph controls the inherent difficulty of the induced
$T$-round learning problem. Specifically, we show that any feedback graph
belongs to one of three classes: strongly observable graphs, weakly observable
graphs, and unobservable graphs. We prove that the first class induces learning
problems with $\widetilde\Theta(\alpha^{1/2} T^{1/2})$ minimax regret, where
$\alpha$ is the independence number of the underlying graph; the second class
induces problems with $\widetilde\Theta(\delta^{1/3}T^{2/3})$ minimax regret,
where $\delta$ is the domination number of a certain portion of the graph; and
the third class induces problems with linear minimax regret. Our results
subsume much of the previous work on learning with feedback graphs and reveal
new connections to partial monitoring games. We also show how the regret is
affected if the graphs are allowed to vary with time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07628</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07628</id><created>2015-02-26</created><authors><author><keyname>Aiguier</keyname><forenames>Marc</forenames></author><author><keyname>Atif</keyname><forenames>Jamal</forenames></author><author><keyname>Bloch</keyname><forenames>Isabelle</forenames></author><author><keyname>Hudelot</keyname><forenames>C&#xe9;line</forenames></author></authors><title>Relaxation-based revision operators in description logics</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As ontologies and description logics (DLs) reach out to a broader audience,
several reasoning services are developed in this context. Belief revision is
one of them, of prime importance when knowledge is prone to change and
inconsistency. In this paper we address both the generalization of the
well-known AGM postulates, and the definition of concrete and well-founded
revision operators in different DL families. We introduce a model-theoretic
version of the AGM postulates with a general definition of inconsistency, hence
enlarging their scope to a wide family of non-classical logics, in particular
negation-free DL families. We propose a general framework for defining revision
operators based on the notion of relaxation, introduced recently for defining
dissimilarity measures between DL concepts. A revision operator in this
framework amounts to relax the set of models of the old belief until it reaches
the sets of models of the new piece of knowledge. We demonstrate that such a
relaxation-based revision operator defines a faithful assignment and satisfies
the generalized AGM postulates. Another important contribution concerns the
definition of several concrete relaxation operators suited to the syntax of
some DLs (ALC and its fragments EL and ELU).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07634</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07634</id><created>2015-02-26</created><authors><author><keyname>Aiguier</keyname><forenames>Marc</forenames></author><author><keyname>Atif</keyname><forenames>Jamal</forenames></author><author><keyname>Bloch</keyname><forenames>Isabelle</forenames></author><author><keyname>Hudelot</keyname><forenames>C&#xe9;line</forenames></author></authors><title>Some algebraic results in Description logics : Free model and
  inclusions, finite basis theorem, and completion of knowledge bases</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method to complete description logic (DL) knowledge bases. For
this, we firstly build a canonical finite model from a given DL knowledge base
satisfying some constraints on the form of its axioms. Then, we build a new DL
knowledge base that infers all the properties of the canonical model. This
latter DL knowledge base necessarily completes (according to the sense given to
this notion in the paper) the starting DL knowledge base. This is the use and
adaptation of results in universal algebra that allow us to get an effective
process for completing DL knowledge bases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07639</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07639</id><created>2015-02-26</created><updated>2015-03-31</updated><authors><author><keyname>Chakraborty</keyname><forenames>Soham</forenames><affiliation>MPI-SWS</affiliation></author><author><keyname>Henzinger</keyname><forenames>Thomas A.</forenames><affiliation>IST Austria</affiliation></author><author><keyname>Sezgin</keyname><forenames>Ali</forenames><affiliation>University of Cambridge</affiliation></author><author><keyname>Vafeiadis</keyname><forenames>Viktor</forenames><affiliation>MPI-SWS</affiliation></author></authors><title>Aspect-oriented linearizability proofs</title><categories>cs.LO cs.PL</categories><comments>33 pages, LMCS</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 11, Issue 1 (April 1,
  2015) lmcs:1051</journal-ref><doi>10.2168/LMCS-11(1:20)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linearizability of concurrent data structures is usually proved by monolithic
simulation arguments relying on the identification of the so-called
linearization points. Regrettably, such proofs, whether manual or automatic,
are often complicated and scale poorly to advanced non-blocking concurrency
patterns, such as helping and optimistic updates. In response, we propose a
more modular way of checking linearizability of concurrent queue algorithms
that does not involve identifying linearization points. We reduce the task of
proving linearizability with respect to the queue specification to establishing
four basic properties, each of which can be proved independently by simpler
arguments. As a demonstration of our approach, we verify the Herlihy and Wing
queue, an algorithm that is challenging to verify by a simulation proof.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07641</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07641</id><created>2015-02-26</created><updated>2015-03-23</updated><authors><author><keyname>Barber</keyname><forenames>Rina Foygel</forenames></author><author><keyname>Kolar</keyname><forenames>Mladen</forenames></author></authors><title>ROCKET: Robust Confidence Intervals via Kendall's Tau for
  Transelliptical Graphical Models</title><categories>math.ST cs.LG stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Undirected graphical models are used extensively in the biological and social
sciences to encode a pattern of conditional independences between variables,
where the absence of an edge between two nodes $a$ and $b$ indicates that the
corresponding two variables $X_a$ and $X_b$ are believed to be conditionally
independent, after controlling for all other measured variables. In the
Gaussian case, conditional independence corresponds to a zero entry in the
precision matrix $\Omega$ (the inverse of the covariance matrix $\Sigma$). Real
data often exhibits heavy tail dependence between variables, which cannot be
captured by the commonly-used Gaussian or nonparanormal (Gaussian copula)
graphical models. In this paper, we study the transelliptical model, an
elliptical copula model that generalizes Gaussian and nonparanormal models to a
broader family of distributions. We propose the ROCKET method, which constructs
an estimator of $\Omega_{ab}$ that we prove to be asymptotically normal under
mild assumptions. Empirically, ROCKET outperforms the nonparanormal and
Gaussian models in terms of achieving accurate inference on simulated data. We
also compare the three methods on real data (daily stock returns), and find
that the ROCKET estimator is the only method whose behavior across subsamples
agrees with the distribution predicted by the theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07643</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07643</id><created>2015-02-26</created><updated>2015-11-11</updated><authors><author><keyname>Robinson</keyname><forenames>Ryan</forenames></author></authors><title>Dynamic Belief Fusion for Object Detection</title><categories>cs.CV</categories><comments>The paper has been withdrawn and an updated paper has been uploaded
  by a co-author: http://arxiv.org/pdf/1511.03183.pdf</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel approach for the fusion of detection scores from disparate object
detection methods is proposed. In order to effectively integrate the outputs of
multiple detectors, the level of ambiguity in each individual detection score
(called &quot;uncertainty&quot;) is estimated using the precision/recall relationship of
the corresponding detector. The proposed fusion method, called Dynamic Belief
Fusion (DBF), dynamically assigns basic probabilities to propositions (target,
non-target, uncertain) based on confidence levels in the detection results of
individual approaches. A joint basic probability assignment, containing
information from all detectors, is determined using Dempster's combination
rule, and is easily reduced to a single fused detection score. Experiments on
ARL and PASCAL VOC 07 datasets demonstrate that the detection accuracy of DBF
is considerably greater than conventional fusion approaches as well as
state-of-the-art individual detectors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07645</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07645</id><created>2015-02-26</created><updated>2015-04-11</updated><authors><author><keyname>Wang</keyname><forenames>Yu-Xiang</forenames></author><author><keyname>Fienberg</keyname><forenames>Stephen E.</forenames></author><author><keyname>Smola</keyname><forenames>Alex</forenames></author></authors><title>Privacy for Free: Posterior Sampling and Stochastic Gradient Monte Carlo</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of Bayesian learning on sensitive datasets and
present two simple but somewhat surprising results that connect Bayesian
learning to &quot;differential privacy:, a cryptographic approach to protect
individual-level privacy while permiting database-level utility. Specifically,
we show that that under standard assumptions, getting one single sample from a
posterior distribution is differentially private &quot;for free&quot;. We will see that
estimator is statistically consistent, near optimal and computationally
tractable whenever the Bayesian model of interest is consistent, optimal and
tractable. Similarly but separately, we show that a recent line of works that
use stochastic gradient for Hybrid Monte Carlo (HMC) sampling also preserve
differentially privacy with minor or no modifications of the algorithmic
procedure at all, these observations lead to an &quot;anytime&quot; algorithm for
Bayesian learning under privacy constraint. We demonstrate that it performs
much better than the state-of-the-art differential private methods on synthetic
and real datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07659</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07659</id><created>2015-02-26</created><authors><author><keyname>Montejano</keyname><forenames>Luis Pedro</forenames></author><author><keyname>Sau</keyname><forenames>Ignasi</forenames></author></authors><title>On the complexity of computing the $k$-restricted edge-connectivity of a
  graph</title><categories>cs.DS cs.DM</categories><comments>15 pages, 3 figures</comments><msc-class>68R10, 05C85</msc-class><acm-class>G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The \emph{$k$-restricted edge-connectivity} of a graph $G$, denoted by
$\lambda_k(G)$, is defined as the minimum size of an edge set whose removal
leaves exactly two connected components each containing at least $k$ vertices.
This graph invariant, which can be seen as a generalization of a minimum
edge-cut, has been extensively studied from a combinatorial point of view.
However, very little is known about the complexity of computing $\lambda_k(G)$.
Very recently, in the parameterized complexity community the notion of
\emph{good edge separation} of a graph has been defined, which happens to be
essentially the same as the $k$-restricted edge-connectivity. Motivated by the
relevance of this invariant from both combinatorial and algorithmic points of
view, in this article we initiate a systematic study of its computational
complexity, with special emphasis on its parameterized complexity for several
choices of the parameters. We provide a number of NP-hardness and W[1]-hardness
results, as well as FPT-algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07661</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07661</id><created>2015-02-26</created><authors><author><keyname>Alshahwan</keyname><forenames>Nadia</forenames></author><author><keyname>Barr</keyname><forenames>Earl T.</forenames></author><author><keyname>Clark</keyname><forenames>David</forenames></author><author><keyname>Danezis</keyname><forenames>George</forenames></author></authors><title>Detecting Malware with Information Complexity</title><categories>cs.CR cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work focuses on a specific front of the malware detection arms-race,
namely the detection of persistent, disk-resident malware. We exploit
normalised compression distance (NCD), an information theoretic measure,
applied directly to binaries. Given a zoo of labelled malware and benign-ware,
we ask whether a suspect program is more similar to our malware or to our
benign-ware. Our approach classifies malware with 97.1% accuracy and a false
positive rate of 3%. We achieve our results with off-the-shelf compressors and
a standard machine learning classifier and without any specialised knowledge.
An end-user need only collect a zoo of malware and benign-ware and then can
immediately apply our techniques.
  We apply statistical rigour to our experiments and our selection of data. We
demonstrate that accuracy can be optimised by combining NCD with the
compressibility rates of the executables. We demonstrate that malware reported
within a more narrow time frame of a few days is more homogenous than malware
reported over a longer one of two years but that our method still classifies
the latter with 95.2% accuracy and a 5% false positive rate. Due to the use of
compression, the time and computation cost of our method is non-trivial. We
show that simple approximation techniques can improve the time complexity of
our approach by up to 63%.
  We compare our results to the results of applying the 59 anti-malware
programs used on the VirusTotal web site to our malware. Our approach does
better than any single one of them as well as the 59 used collectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07663</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07663</id><created>2015-02-26</created><updated>2015-04-26</updated><authors><author><keyname>Gawrychowski</keyname><forenames>Pawel</forenames></author><author><keyname>Mozes</keyname><forenames>Shay</forenames></author><author><keyname>Weimann</keyname><forenames>Oren</forenames></author></authors><title>Submatrix Maximum Queries in Monge Matrices are Equivalent to
  Predecessor Search</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an optimal data structure for submatrix maximum queries in n x n
Monge matrices. Our result is a two-way reduction showing that the problem is
equivalent to the classical predecessor problem in a universe of polynomial
size. This gives a data structure of O(n) space that answers submatrix maximum
queries in O(loglogn) time. It also gives a matching lower bound, showing that
O(loglogn) query-time is optimal for any data structure of size O(n
polylog(n)).
  Our result concludes a line of improvements that started in SODA'12 with
O(log^2 n) query-time and continued in ICALP'14 with O(log n) query-time.
Finally, we show that partial Monge matrices can be handled in the same bounds
as full Monge matrices. In both previous results, partial Monge matrices
incurred additional inverse-Ackerman factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07666</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07666</id><created>2015-02-03</created><authors><author><keyname>Bauer</keyname><forenames>Martin</forenames></author><author><keyname>Eslitzbichler</keyname><forenames>Markus</forenames></author><author><keyname>Grasmair</keyname><forenames>Markus</forenames></author></authors><title>Landmark-Guided Elastic Shape Analysis of Human Character Motions</title><categories>cs.CV cs.GR</categories><msc-class>65D18, 58D10, 49Q10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motions of virtual characters in movies or video games are typically
generated by recording actors using motion capturing methods. Animations
generated this way often need postprocessing, such as improving the periodicity
of cyclic animations or generating entirely new motions by interpolation of
existing ones. Furthermore, search and classification of recorded motions
becomes more and more important as the amount of recorded motion data grows.
  In this paper, we will apply methods from shape analysis to the processing of
animations. More precisely, we will use the by now classical elastic metric
model used in shape matching, and extend it by incorporating additional inexact
feature point information, which leads to an improved temporal alignment of
different animations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07687</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07687</id><created>2015-02-26</created><updated>2015-04-27</updated><authors><author><keyname>Restuccia</keyname><forenames>Francesco</forenames></author><author><keyname>Das</keyname><forenames>Sajal K.</forenames></author><author><keyname>Payton</keyname><forenames>Jamie</forenames></author></authors><title>Incentive Mechanisms for Participatory Sensing: Survey and Research
  Challenges</title><categories>cs.GT</categories><comments>Updated version, 4/25/2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Participatory sensing is a powerful paradigm which takes advantage of
smartphones to collect and analyze data beyond the scale of what was previously
possible. Given that participatory sensing systems rely completely on the
users' willingness to submit up-to-date and accurate information, it is
paramount to effectively incentivize users' active and reliable participation.
In this paper, we survey existing literature on incentive mechanisms for
participatory sensing systems. In particular, we present a taxonomy of existing
incentive mechanisms for participatory sensing systems, which are subsequently
discussed in depth by comparing and contrasting different approaches. Finally,
we discuss an agenda of open research challenges in incentivizing users in
participatory sensing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07693</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07693</id><created>2015-01-07</created><authors><author><keyname>Rueda</keyname><forenames>Urko</forenames></author><author><keyname>Espa&#xf1;a</keyname><forenames>Sergio</forenames></author><author><keyname>Ruiz</keyname><forenames>Marcela</forenames></author></authors><title>GREAT Process Modeller user manual</title><categories>cs.OH</categories><comments>8 pages</comments><report-no>PROS-TR-2015-01</report-no><msc-class>68N01</msc-class><acm-class>D.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report contains instructions to install, uninstall and use GREAT Process
Modeller, a tool that supports Communication Analysis, a communication-oriented
business process modelling method. GREAT allows creating communicative event
diagrams (i.e. business process models), specifying message structures (which
describe the messages associated to each communicative event), and
automatically generating a class diagram (representing the data model of an
information system that would support such organisational communication). This
report briefly describes the methodological background of the tool. This
handbook explains the modelling techniques in detail: Espa\~na, S., A.
Gonz\'alez, \'O. Pastor and M. Ruiz (2012). Communication Analysis modelling
techniques. Technical report ProS-TR-2012-02, PROS Research Centre, Universitat
Polit\`ecnica de Val\`encia, Spain, arXiv:1205.0987.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07697</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07697</id><created>2015-02-26</created><updated>2015-07-01</updated><authors><author><keyname>Gaillard</keyname><forenames>Pierre</forenames><affiliation>GREGHEC, EDF R\&amp;D</affiliation></author><author><keyname>Gerchinovitz</keyname><forenames>S&#xe9;bastien</forenames><affiliation>IMT, UPS</affiliation></author></authors><title>A Chaining Algorithm for Online Nonparametric Regression</title><categories>stat.ML cs.LG</categories><comments>Published in the proceedings of COLT 2015:
  http://jmlr.org/proceedings/papers/v40/Gaillard15.html</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of online nonparametric regression with arbitrary
deterministic sequences. Using ideas from the chaining technique, we design an
algorithm that achieves a Dudley-type regret bound similar to the one obtained
in a non-constructive fashion by Rakhlin and Sridharan (2014). Our regret bound
is expressed in terms of the metric entropy in the sup norm, which yields
optimal guarantees when the metric and sequential entropies are of the same
order of magnitude. In particular our algorithm is the first one that achieves
optimal rates for online regression over H{\&quot;o}lder balls. In addition we show
for this example how to adapt our chaining algorithm to get a reasonable
computational efficiency with similar regret guarantees (up to a log factor).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07710</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07710</id><created>2015-02-26</created><updated>2015-03-01</updated><authors><author><keyname>Sarma</keyname><forenames>Akash Das</forenames></author><author><keyname>Parameswaran</keyname><forenames>Aditya</forenames></author><author><keyname>Widom</keyname><forenames>Jennifer</forenames></author></authors><title>Globally Optimal Crowdsourcing Quality Management</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study crowdsourcing quality management, that is, given worker responses to
a set of tasks, our goal is to jointly estimate the true answers for the tasks,
as well as the quality of the workers. Prior work on this problem relies
primarily on applying Expectation-Maximization (EM) on the underlying maximum
likelihood problem to estimate true answers as well as worker quality.
Unfortunately, EM only provides a locally optimal solution rather than a
globally optimal one. Other solutions to the problem (that do not leverage EM)
fail to provide global optimality guarantees as well. In this paper, we focus
on filtering, where tasks require the evaluation of a yes/no predicate, and
rating, where tasks elicit integer scores from a finite domain. We design
algorithms for finding the global optimal estimates of correct task answers and
worker quality for the underlying maximum likelihood problem, and characterize
the complexity of these algorithms. Our algorithms conceptually consider all
mappings from tasks to true answers (typically a very large number), leveraging
two key ideas to reduce, by several orders of magnitude, the number of mappings
under consideration, while preserving optimality. We also demonstrate that
these algorithms often find more accurate estimates than EM-based algorithms.
This paper makes an important contribution towards understanding the inherent
complexity of globally optimal crowdsourcing quality management.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07713</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07713</id><created>2015-02-26</created><authors><author><keyname>Bousquet</keyname><forenames>Nicolas</forenames></author><author><keyname>Li</keyname><forenames>Zhentao</forenames></author><author><keyname>Vetta</keyname><forenames>Adrian</forenames></author></authors><title>Coalition Games on Interaction Graphs: A Horticultural Perspective</title><categories>cs.GT math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine cooperative games where the viability of a coalition is determined
by whether or not its members have the ability to communicate amongst
themselves independently of non-members. This necessary condition for viability
was proposed by Myerson (1977) and is modeled via an interaction graph
$G=(V,E)$; a coalition $S\subseteq V$ is then viable if and only if the induced
graph $G[S]$ is connected. The non-emptiness of the core of a coalition game
can be tested by a well-known covering LP. Moreover, the integrality gap of its
dual packing LP defines exactly the multiplicative least-core and the relative
cost of stability of the coalition game. This gap is upper bounded by the
packing-covering ratio which, for graphical coalition games, is known to be at
most the treewidth of the interaction graph plus one (Meir et al. 2013).
  We examine the packing-covering ratio and integrality gaps of graphical
coalition games in more detail. We introduce the thicket parameter of a graph,
and prove it precisely measures the packing-covering ratio. It also
approximately measures the primal and dual integrality gaps. The thicket number
provides an upper bound of both integrality gaps. Moreover we show that for any
interaction graph, the primal integrality gap is, in the worst case, linear in
terms of the thicket number while the dual integrality gap is polynomial in
terms of it. At the heart of our results, is a graph theoretic minmax theorem
showing the thicket number is equal to the minimum width of a vine
decomposition of the coalition graph (a vine decomposition is a generalization
of a tree decomposition). We also explain how the thicket number relates to the
VC-dimension of the set system produced by the game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07718</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07718</id><created>2015-02-26</created><updated>2015-03-04</updated><authors><author><keyname>Panda</keyname><forenames>B. S.</forenames></author><author><keyname>Pandey</keyname><forenames>Arti</forenames></author><author><keyname>Paul</keyname><forenames>S.</forenames></author></authors><title>Algorithmic aspects of disjunctive domination in graphs</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a graph $G=(V,E)$, a set $D\subseteq V$ is called a \emph{disjunctive
dominating set} of $G$ if for every vertex $v\in V\setminus D$, $v$ is either
adjacent to a vertex of $D$ or has at least two vertices in $D$ at distance $2$
from it. The cardinality of a minimum disjunctive dominating set of $G$ is
called the \emph{disjunctive domination number} of graph $G$, and is denoted by
$\gamma_{2}^{d}(G)$. The \textsc{Minimum Disjunctive Domination Problem} (MDDP)
is to find a disjunctive dominating set of cardinality $\gamma_{2}^{d}(G)$.
Given a positive integer $k$ and a graph $G$, the \textsc{Disjunctive
Domination Decision Problem} (DDDP) is to decide whether $G$ has a disjunctive
dominating set of cardinality at most $k$. In this article, we first propose a
linear time algorithm for MDDP in proper interval graphs. Next we tighten the
NP-completeness of DDDP by showing that it remains NP-complete even in chordal
graphs. We also propose a $(\ln(\Delta^{2}+\Delta+2)+1)$-approximation
algorithm for MDDP in general graphs and prove that MDDP can not be
approximated within $(1-\epsilon) \ln(|V|)$ for any $\epsilon&gt;0$ unless NP
$\subseteq$ DTIME$(|V|^{O(\log \log |V|)})$. Finally, we show that MDDP is
APX-complete for bipartite graphs with maximum degree $3$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07725</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07725</id><created>2015-02-26</created><updated>2015-05-11</updated><authors><author><keyname>Zehavi</keyname><forenames>Meirav</forenames></author></authors><title>The $k$-Leaf Spanning Tree Problem Admits a Klam Value of 39</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an undirected graph $G$ and a parameter $k$, the $k$-Leaf Spanning Tree
($k$-LST) problem asks if $G$ contains a spanning tree with at least $k$
leaves. This problem has been extensively studied over the past three decades.
In 2000, Fellows et al. [FSTTCS'00] explicitly asked whether it admits a klam
value of 50. A steady progress towards an affirmative answer continued until 5
years ago, when an algorithm of klam value 37 was discovered. In this paper, we
present an $O^*(3.188^k)$-time parameterized algorithm for $k$-LST, which shows
that the problem admits a klam value of 39. Our algorithm is based on an
interesting application of the well-known bounded search trees technique, where
the correctness of rules crucially depends on the history of previously applied
rules in a non-standard manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07738</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07738</id><created>2015-02-26</created><updated>2015-05-26</updated><authors><author><keyname>Hajek</keyname><forenames>Bruce</forenames></author><author><keyname>Wu</keyname><forenames>Yihong</forenames></author><author><keyname>Xu</keyname><forenames>Jiaming</forenames></author></authors><title>Achieving Exact Cluster Recovery Threshold via Semidefinite Programming:
  Extensions</title><categories>stat.ML cs.SI math.PR</categories><comments>This work was in part presented at the Workshop on Community
  Detection, February 26-27, Institut Henri Poincar\'e, Paris</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Resolving a conjecture of Abbe, Bandeira and Hall, the authors have recently
shown that the semidefinite programming (SDP) relaxation of the maximum
likelihood estimator achieves the sharp threshold for exactly recovering the
community structure under the binary stochastic block model of two equal-sized
clusters. The same was shown for the case of a single cluster and outliers.
Extending the proof techniques, in this paper it is shown that SDP relaxations
also achieve the sharp recovery threshold in the following cases: (1) Binary
stochastic block model with two clusters of sizes proportional to network size
but not necessarily equal; (2) Stochastic block model with a fixed number of
equal-sized clusters; (3) Binary censored block model with the background graph
being Erd\H{o}s-R\'enyi. Furthermore, a sufficient condition is given for an
SDP procedure to achieve exact recovery for the general case of a fixed number
of clusters plus outliers. These results demonstrate the versatility of SDP
relaxation as a simple, general purpose, computationally feasible methodology
for community detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07743</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07743</id><created>2015-01-21</created><authors><author><keyname>Judd</keyname><forenames>Kevin</forenames></author></authors><title>Tracking an Object with Unknown Accelerations using a Shadowing Filter</title><categories>cs.SY cs.CV math.OC</categories><comments>20 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A commonly encountered problem is the tracking of a physical object, like a
maneuvering ship, aircraft, land vehicle, spacecraft or animate creature
carrying a wireless device. The sensor data is often limited and inaccurate
observations of range or bearing. This problem is more difficult than tracking
a ballistic trajectory, because an operative affects unknown and arbitrarily
changing accelerations. Although stochastic methods of filtering or state
estimation (Kalman filters and particle filters) are widely used, out of vogue
variational methods are more appropriate in this tracking context, because the
objects do not typically display any significant random motions at the length
and time scales of interest. This leads us to propose a rather elegant approach
based on a \emph{shadowing filter}. The resulting filter is efficient (reduces
to the solution of linear equations) and robust (uneffected by missing data and
singular correlations that would cause catastrophic failure of Bayesian
filters.) The tracking is so robust, that in some common situations it actually
performs better by ignoring error correlations that are so vital to Kalman
filters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07744</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07744</id><created>2015-02-26</created><authors><author><keyname>Brand&#xe1;n-Briones</keyname><forenames>Laura</forenames></author><author><keyname>Madalinski</keyname><forenames>Agnes</forenames></author><author><keyname>Ponce-de-Le&#xf3;n</keyname><forenames>Hern&#xe1;n</forenames></author></authors><title>Distributed Diagnosability Analysis with Petri Nets</title><categories>cs.LO</categories><comments>In International Workshop on Principles of Diagnosis. 2014. arXiv
  admin note: text overlap with arXiv:1502.07466</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a framework to distributed diagnos- ability analysis of concurrent
systems modeled with Petri nets as a collection of components synchronizing on
common observable transitions, where faults can occur in several components.
The diagnosability analysis of the entire system is done in parallel by
verifying the interaction of each component with the fault free versions of the
other components. Furthermore, we use existing efficient methods and tools, in
particular parallel LTL-X model checking based on unfoldings, for
diagnosability verification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07758</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07758</id><created>2015-02-26</created><updated>2015-10-07</updated><authors><author><keyname>Blackman</keyname><forenames>Jonathan</forenames></author><author><keyname>Field</keyname><forenames>Scott E.</forenames></author><author><keyname>Galley</keyname><forenames>Chad R.</forenames></author><author><keyname>Szilagyi</keyname><forenames>Bela</forenames></author><author><keyname>Scheel</keyname><forenames>Mark A.</forenames></author><author><keyname>Tiglio</keyname><forenames>Manuel</forenames></author><author><keyname>Hemberger</keyname><forenames>Daniel A.</forenames></author></authors><title>Fast and accurate prediction of numerical relativity waveforms from
  binary black hole coalescences using surrogate models</title><categories>gr-qc astro-ph.HE cs.CE physics.data-an</categories><comments>Updated to published version, which includes a section comparing the
  surrogate and effective-one-body models. The surrogate is publicly available
  for download at http://www.black-holes.org/surrogates/ . 6 pages, 6 figures</comments><journal-ref>Phys. Rev. Lett. 115, 121102 (2015)</journal-ref><doi>10.1103/PhysRevLett.115.121102</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simulating a binary black hole (BBH) coalescence by solving Einstein's
equations is computationally expensive, requiring days to months of
supercomputing time. Using reduced order modeling techniques, we construct an
accurate surrogate model, which is evaluated in a millisecond to a second, for
numerical relativity (NR) waveforms from non-spinning BBH coalescences with
mass ratios in $[1, 10]$ and durations corresponding to about $15$ orbits
before merger. We assess the model's uncertainty and show that our modeling
strategy predicts NR waveforms {\em not} used for the surrogate's training with
errors nearly as small as the numerical error of the NR code. Our model
includes all spherical-harmonic ${}_{-2}Y_{\ell m}$ waveform modes resolved by
the NR code up to $\ell=8.$ We compare our surrogate model to Effective One
Body waveforms from $50$-$300 M_\odot$ for advanced LIGO detectors and find
that the surrogate is always more faithful (by at least an order of magnitude
in most cases).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07762</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07762</id><created>2015-01-30</created><authors><author><keyname>Rutkowski</keyname><forenames>Tomasz M.</forenames></author><author><keyname>Mori</keyname><forenames>Hiromu</forenames></author><author><keyname>Kodama</keyname><forenames>Takumi</forenames></author><author><keyname>Shinoda</keyname><forenames>Hiroyuki</forenames></author></authors><title>Airborne Ultrasonic Tactile Display Brain-computer Interface -- A Small
  Robotic Arm Online Control Study</title><categories>cs.HC cs.RO q-bio.NC</categories><comments>2 pages, 1 figure, accepted for 10th AEARU Workshop on Computer
  Science and Web Technology February 25-27, 2015, University of Tsukuba, Japan</comments><acm-class>H.5.2; H.1.2</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We report on an extended robot control application of a contact-less and
airborne ultrasonic tactile display (AUTD) stimulus-based brain-computer
interface (BCI) paradigm, which received last year The Annual BCI Research
Award 2014. In the award winning human communication augmentation paradigm the
six palm positions are used to evoke somatosensory brain responses, in order to
define a novel contactless tactile BCI. An example application of a small robot
management is also presented in which the users control a small robot online.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07770</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07770</id><created>2015-02-26</created><updated>2015-03-25</updated><authors><author><keyname>Kolmogorov</keyname><forenames>Vladimir</forenames></author><author><keyname>Pock</keyname><forenames>Thomas</forenames></author><author><keyname>Rolinek</keyname><forenames>Michal</forenames></author></authors><title>Total variation on a tree</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of minimizing the continuous valued total variation
subject to different unary terms on trees and propose fast direct algorithms
based on dynamic programming to solve these problems. We treat both the convex
and the non-convex case and derive worst case complexities that are equal or
better than existing methods. We show applications to total variation based 2D
image processing and computer vision problems based on a Lagrangian
decomposition approach. The resulting algorithms are very efficient, offer a
high degree of parallelism and come along with memory requirements which are
only in the order of the number of image pixels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07776</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07776</id><created>2015-02-26</created><authors><author><keyname>Bellaouar</keyname><forenames>Slimane</forenames></author><author><keyname>Cherroun</keyname><forenames>Hadda</forenames></author><author><keyname>Ziadi</keyname><forenames>Djelloul</forenames></author></authors><title>Efficient Geometric-based Computation of the String Subsequence Kernel</title><categories>cs.LG cs.CG</categories><comments>24 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kernel methods are powerful tools in machine learning. They have to be
computationally efficient. In this paper, we present a novel Geometric-based
approach to compute efficiently the string subsequence kernel (SSK). Our main
idea is that the SSK computation reduces to range query problem. We started by
the construction of a match list $L(s,t)=\{(i,j):s_{i}=t_{j}\}$ where $s$ and
$t$ are the strings to be compared; such match list contains only the required
data that contribute to the result. To compute efficiently the SSK, we extended
the layered range tree data structure to a layered range sum tree, a
range-aggregation data structure. The whole process takes $ O(p|L|\log|L|)$
time and $O(|L|\log|L|)$ space, where $|L|$ is the size of the match list and
$p$ is the length of the SSK. We present empiric evaluations of our approach
against the dynamic and the sparse programming approaches both on synthetically
generated data and on newswire article data. Such experiments show the
efficiency of our approach for large alphabet size except for very short
strings. Moreover, compared to the sparse dynamic approach, the proposed
approach outperforms absolutely for long strings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07781</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07781</id><created>2015-02-26</created><authors><author><keyname>Bunyak</keyname><forenames>Yuriy A.</forenames></author><author><keyname>Kvetnyy</keyname><forenames>Roman N.</forenames></author><author><keyname>Sofina</keyname><forenames>Olga Yu.</forenames></author></authors><title>The conjugated null space method of blind PSF estimation and
  deconvolution optimization</title><categories>cs.CV</categories><comments>arXiv admin note: text overlap with arXiv:1206.3594</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have shown that the vector of the point spread function (PSF)
lexicographical presentation belongs to the left side conjugated null space
(NS) of the autoregression (AR) matrix operator on condition the AR parameters
are common for original and blurred images. The method of the PSF and inverse
PSF (IPSF) evaluation in the basis of the NS eigenfunctions is offered. The
optimization of the PSF and IPSF shape with the aim of fluctuation elimination
is considered in NS spectral domain and image space domain. The function of
surface area was used as the regularization functional. Two methods of original
image estimate optimization were designed basing on maximum entropy
generalization of sought and blurred images conditional probability density and
regularization. The first method uses balanced variations of convolutions with
the PSF and IPSF to obtaining iterative schema of image optimization. The
variations balance is providing by dynamic regularization basing on condition
of the iteration process convergence. The regularization has dynamic character
because depends on current and previous image estimate variations. The second
method implements the regularization of the deconvolution optimization in
curved space with metric defined on image estimate surface. The given iterative
schemas have fast convergence and therefore can be used for reconstruction of
high resolution images series in real time. The NS can be used for design of
denoising bilateral linear filter which does not introduce image smoothing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07786</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07786</id><created>2015-02-26</created><authors><author><keyname>Clements</keyname><forenames>John</forenames></author></authors><title>Generating 56-bit passwords using Markov Models (and Charles Dickens)</title><categories>cs.CR</categories><comments>5 pages, 2 figures</comments><acm-class>K.6.5; E.4; D.4.6; K.4.2; H.1.2; I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a password generation scheme based on Markov models built from
English text (specifically, Charles Dickens' *A Tale Of Two Cities*). We show a
(linear-running-time) bijection between random bitstrings of any desired length
and generated text, ensuring that all passwords are generated with equal
probability. We observe that the generated passwords appear to strike a
reasonable balance between memorability and security. Using the system, we get
56-bit passwords like 'The cusay is wither?&quot; t', rather than passwords like
'tQ$%Xc4Ef'.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07787</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07787</id><created>2015-02-26</created><authors><author><keyname>Achlioptas</keyname><forenames>Dimitris</forenames></author><author><keyname>Siminelakis</keyname><forenames>Paris</forenames></author></authors><title>Product Measure Approximation of Symmetric Graph Properties</title><categories>cs.DM</categories><comments>16 pages</comments><msc-class>05C80</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the study of random structures we often face a trade-off between realism
and tractability, the latter typically enabled by assuming some form of
independence. In this work we initiate an effort to bridge this gap by
developing tools that allow us to work with independence without assuming it.
Let $\mathcal{G}_{n}$ be the set of all graphs on $n$ vertices and let $S$ be
an arbitrary subset of $\mathcal{G}_{n}$, e.g., the set of graphs with $m$
edges. The study of random networks can be seen as the study of properties that
are true for most elements of $S$, i.e., that are true with high probability
for a uniformly random element of $S$. With this in mind, we pursue the
following question: What are general sufficient conditions for the uniform
measure on a set of graphs $S \subseteq \mathcal{G}_{n}$ to be approximable by
a product measure?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07788</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07788</id><created>2015-02-26</created><authors><author><keyname>Joki&#x107;</keyname><forenames>Slavoljub</forenames></author><author><keyname>Nikovi&#x107;</keyname><forenames>Ljindita</forenames></author><author><keyname>Kadovi&#x107;</keyname><forenames>Jelena</forenames></author></authors><title>Analysis of Gradient based Algorithm for Signal reconstruction in the
  Presence of Noise</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Common problem in signal processing is reconstruction of the missing signal
samples. Missing samples can occur by intentionally omitting signal
coefficients to reduce memory requirements, or to speed up the transmission
process. Also, noisy signal coefficients can be considered as missing ones,
since they have wrong values due to the noise. The reconstruction of these
coefficients is demanding task, considered within the Compressive sensing area.
Signal with large number of missing samples can be recovered, if certain
conditions are satisfied. There is a number of algorithms used for signal
reconstruction. In this paper we have analyzed the performance of iterative
gradient-based algorithm for sparse signal reconstruction. The parameters
influence on the optimal performances of this algorithm is tested. Two cases
are observed: non-noisy and noisy signal case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07790</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07790</id><created>2015-02-26</created><updated>2015-03-08</updated><authors><author><keyname>Cagirici</keyname><forenames>Onur</forenames></author></authors><title>Exploiting Coplanar Clusters to Enhance 3D Localization in Wireless
  Sensor Networks</title><categories>cs.NI cs.CG</categories><comments>60 pages, thesis (v2: corrected typo in abstract)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This thesis studies range-based WSN localization problem in 3D environments
that induce coplanarity. In most real-world applications, even though the
environment is 3D, the grounded sensor nodes are usually deployed on 2D planar
surfaces. Examples of these surfaces include structures seen in both indoor
(e.g. floors, doors, walls, tables etc.) and outdoor (e.g. mountains, valleys,
hills etc.) environments. In such environments, sensor nodes typically appear
as coplanar node clusters. We refer to this type of a deployment as a planar
deployment. When there is a planar deployment, the coplanarity causes
difficulties to the traditional range-based multilateration algorithms because
a node cannot be unambiguously localized if the distance measurements to that
node are from coplanar nodes. Thus, many already localized groups of nodes are
rendered ineffective in the process just because they are coplanar. We,
therefore propose an algorithm called Coplanarity Based Localization (CBL) that
can be used as an extension of any localization algorithm to avoid most flips
caused by coplanarity. CBL first performs a 2D localization among the nodes
that are clustered on the same surface, and then finds the positions of these
clusters in 3D. We have carried out experiments using trilateration for 2D
localization, and quadrilateration for 3D localization, and experimentally
verified that exploiting the clustering information leads to a more precise
localization than mere quadrilateration. We also propose a heuristic to extract
the clustering information in case it is not available, which is yet to be
improved in the future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07792</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07792</id><created>2015-02-26</created><updated>2015-04-19</updated><authors><author><keyname>Nusrat</keyname><forenames>Sabrina</forenames></author><author><keyname>Kobourov</keyname><forenames>Stephen</forenames></author></authors><title>Visualizing Cartograms: Goals and Task Taxonomy</title><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cartograms are maps in which areas of geographic regions (countries, states)
appear in proportion to some variable of interest (population, income).
Cartograms are popular visualizations for geo-referenced data that have been
around for over a century. Newspapers, magazines, textbooks, blogs, and
presentations frequently employ cartograms to show voting results, popularity,
and in general, geographic patterns. Despite the popularity of cartograms and
the large number of cartogram variants, there are very few studies evaluating
the effectiveness of cartograms in conveying information. In order to design
cartograms as a useful visualization tool and to be able to compare the
effectiveness of cartograms generated by different methods, we need to study
the nature of information conveyed and the specific tasks that can be performed
on cartograms. In this paper we consider a set of cartogram visualization
tasks, based on standard taxonomies from cartography and information
visualization. We then propose a cartogram task taxonomy that can be used to
organize not only the tasks considered here but also other tasks that might be
added later.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07796</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07796</id><created>2015-02-26</created><authors><author><keyname>Marcolli</keyname><forenames>Matilde</forenames></author><author><keyname>Port</keyname><forenames>Alexander</forenames></author></authors><title>Graph Grammars, Insertion Lie Algebras, and Quantum Field Theory</title><categories>cs.FL math-ph math.MP</categories><comments>19 pages, LaTeX, 3 jpeg figures</comments><msc-class>68Q42, 81T18</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph grammars extend the theory of formal languages in order to model
distributed parallelism in theoretical computer science. We show here that to
certain classes of context-free and context-sensitive graph grammars one can
associate a Lie algebra, whose structure is reminiscent of the insertion Lie
algebras of quantum field theory. We also show that the Feynman graphs of
quantum field theories are graph languages generated by a theory dependent
graph grammar.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07802</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07802</id><created>2015-02-26</created><authors><author><keyname>Ge</keyname><forenames>ZongYuan</forenames></author><author><keyname>McCool</keyname><forenames>Chris</forenames></author><author><keyname>Sanderson</keyname><forenames>Conrad</forenames></author><author><keyname>Corke</keyname><forenames>Peter</forenames></author></authors><title>Modelling Local Deep Convolutional Neural Network Features to Improve
  Fine-Grained Image Classification</title><categories>cs.CV</categories><comments>5 pages, three figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a local modelling approach using deep convolutional neural
networks (CNNs) for fine-grained image classification. Recently, deep CNNs
trained from large datasets have considerably improved the performance of
object recognition. However, to date there has been limited work using these
deep CNNs as local feature extractors. This partly stems from CNNs having
internal representations which are high dimensional, thereby making such
representations difficult to model using stochastic models. To overcome this
issue, we propose to reduce the dimensionality of one of the internal fully
connected layers, in conjunction with layer-restricted retraining to avoid
retraining the entire network. The distribution of low-dimensional features
obtained from the modified layer is then modelled using a Gaussian mixture
model. Comparative experiments show that considerable performance improvements
can be achieved on the challenging Fish and UEC FOOD-100 datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07808</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07808</id><created>2015-02-26</created><authors><author><keyname>Muhammad</keyname><forenames>Khan</forenames></author><author><keyname>Ahmad</keyname><forenames>Jamil</forenames></author><author><keyname>Rehman</keyname><forenames>Naeem Ur</forenames></author><author><keyname>Jan</keyname><forenames>Zahoor</forenames></author><author><keyname>Qureshi</keyname><forenames>Rashid Jalal</forenames></author></authors><title>A Secure Cyclic Steganographic Technique for Color Images using
  Randomization</title><categories>cs.MM cs.CR</categories><comments>8</comments><journal-ref>Technical Journal, University of Engineering and Technology
  Taxila, Pakistan, vol. 19, pp. 57-64, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information Security is a major concern in today's modern era. Almost all the
communicating bodies want the security, confidentiality and integrity of their
personal data. But this security goal cannot be achieved easily when we are
using an open network like Internet. Steganography provides one of the best
solutions to this problem. This paper represents a new Cyclic Steganographic T
echnique (CST) based on Least Significant Bit (LSB) for true color (RGB)
images. The proposed method hides the secret data in the LSBs of cover image
pixels in a randomized cyclic manner. The proposed technique is evaluated using
both subjective and objective analysis using histograms changeability, Peak
Signal-to-Noise Ratio (PSNR) and Mean Square Error (MSE). Experimentally it is
found that the proposed method gives promising results in terms of security,
imperceptibility and robustness as compared to some existent methods and
vindicates this new algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07809</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07809</id><created>2015-02-26</created><authors><author><keyname>Guo</keyname><forenames>Xueying</forenames></author><author><keyname>Singh</keyname><forenames>Rahul</forenames></author><author><keyname>Kumar</keyname><forenames>P. R.</forenames></author><author><keyname>Niu</keyname><forenames>Zhisheng</forenames></author></authors><title>Optimal Energy-Efficient Regular Delivery of Packets in Cyber-Physical
  Systems</title><categories>cs.SY cs.NI</categories><comments>6 pages conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In cyber-physical systems such as in-vehicle wireless sensor networks, a
large number of sensor nodes continually generate measurements that should be
received by other nodes such as actuators in a regular fashion. Meanwhile,
energy-efficiency is also important in wireless sensor networks. Motivated by
these, we develop scheduling policies which are energy efficient and
simultaneously maintain &quot;regular&quot; deliveries of packets. A tradeoff parameter
is introduced to balance these two conflicting objectives. We employ a Markov
Decision Process (MDP) model where the state of each client is the
time-since-last-delivery of its packet, and reduce it into an equivalent
finite-state MDP problem. Although this equivalent problem can be solved by
standard dynamic programming techniques, it suffers from a high-computational
complexity. Thus we further pose the problem as a restless multi-armed bandit
problem and employ the low-complexity Whittle Index policy. It is shown that
this problem is indexable and the Whittle indexes are derived. Also, we prove
the Whittle Index policy is asymptotically optimal and validate its optimality
via extensive simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07812</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07812</id><created>2015-02-26</created><authors><author><keyname>Lee</keyname><forenames>Kwangsu</forenames></author><author><keyname>Park</keyname><forenames>Jong Hwan</forenames></author><author><keyname>Lee</keyname><forenames>Dong Hoon</forenames></author></authors><title>Anonymous HIBE with Short Ciphertexts: Full Security in Prime Order
  Groups</title><categories>cs.CR</categories><comments>31 pages, 1 figure</comments><journal-ref>Designs, Codes and Cryptography, vol. 74, no. 2, pp. 395-425, Feb.
  2015</journal-ref><doi>10.1007/s10623-013-9868-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Anonymous Hierarchical Identity-Based Encryption (HIBE) is an extension of
Identity-Based Encryption (IBE), and it provides not only a message hiding
property but also an identity hiding property. Anonymous HIBE schemes can be
applicable to anonymous communication systems and public key encryption systems
with keyword searching. However, previous anonymous HIBE schemes have some
disadvantages that the security was proven in the weaker model, the size of
ciphertexts is not short, or the construction was based on composite order
bilinear groups. In this paper, we propose the first efficient anonymous HIBE
scheme with short ciphertexts in prime order (asymmetric) bilinear groups, and
prove its security in the full model with an efficient reduction. To achieve
this, we use the dual system encryption methodology of Waters. We also present
the benchmark results of our scheme by measuring the performance of our
implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07813</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07813</id><created>2015-02-26</created><authors><author><keyname>Kasarapu</keyname><forenames>Parthan</forenames></author><author><keyname>Allison</keyname><forenames>Lloyd</forenames></author></authors><title>Minimum message length estimation of mixtures of multivariate Gaussian
  and von Mises-Fisher distributions</title><categories>cs.LG stat.ML</categories><comments>46 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mixture modelling involves explaining some observed evidence using a
combination of probability distributions. The crux of the problem is the
inference of an optimal number of mixture components and their corresponding
parameters. This paper discusses unsupervised learning of mixture models using
the Bayesian Minimum Message Length (MML) criterion. To demonstrate the
effectiveness of search and inference of mixture parameters using the proposed
approach, we select two key probability distributions, each handling
fundamentally different types of data: the multivariate Gaussian distribution
to address mixture modelling of data distributed in Euclidean space, and the
multivariate von Mises-Fisher (vMF) distribution to address mixture modelling
of directional data distributed on a unit hypersphere. The key contributions of
this paper, in addition to the general search and inference methodology,
include the derivation of MML expressions for encoding the data using
multivariate Gaussian and von Mises-Fisher distributions, and the analytical
derivation of the MML estimates of the parameters of the two distributions. Our
approach is tested on simulated and real world data sets. For instance, we
infer vMF mixtures that concisely explain experimentally determined
three-dimensional protein conformations, providing an effective null model
description of protein structures that is central to many inference problems in
structural bioinformatics. The experimental results demonstrate that the
performance of our proposed search and inference method along with the encoding
schemes improve on the state of the art mixture modelling techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07816</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07816</id><created>2015-02-26</created><updated>2015-06-21</updated><authors><author><keyname>Glaser</keyname><forenames>Joshua I.</forenames></author><author><keyname>Zamft</keyname><forenames>Bradley M.</forenames></author><author><keyname>Church</keyname><forenames>George M.</forenames></author><author><keyname>Kording</keyname><forenames>Konrad P.</forenames></author></authors><title>Puzzle Imaging: Using Large-scale Dimensionality Reduction Algorithms
  for Localization</title><categories>q-bio.NC cs.CE cs.CV q-bio.QM</categories><doi>10.1371/journal.pone.0131593</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current high-resolution imaging techniques require an intact sample that
preserves spatial relationships. We here present a novel approach, &quot;puzzle
imaging,&quot; that allows imaging a spatially scrambled sample. This technique
takes many spatially disordered samples, and then pieces them back together
using local properties embedded within the sample. We show that puzzle imaging
can efficiently produce high-resolution images using dimensionality reduction
algorithms. We demonstrate the theoretical capabilities of puzzle imaging in
three biological scenarios, showing that (1) relatively precise 3-dimensional
brain imaging is possible; (2) the physical structure of a neural network can
often be recovered based only on the neural connectivity matrix; and (3) a
chemical map could be reproduced using bacteria with chemosensitive DNA and
conjugative transfer. The ability to reconstruct scrambled images promises to
enable imaging based on DNA sequencing of homogenized tissue samples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07823</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07823</id><created>2015-02-27</created><authors><author><keyname>Deng</keyname><forenames>Yuan</forenames></author><author><keyname>Shen</keyname><forenames>Weiran</forenames></author><author><keyname>Tang</keyname><forenames>Pingzhong</forenames></author></authors><title>Coalition manipulations of the Gale-Shapley algorithm</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well-known that the Gale-Shapley algorithm is not truthful for all
agents. Previous studies on this front mostly focus on blacklist manipulations
by a single woman and by the set of all women. Little is known about
manipulations by a coalition of women or other types of manipulations, such as
manipulation by permuting preference lists.
  In this paper, we consider the problem of finding an equilibrium for a
coalition of women (aka. liars) in the Gale-Shapley algorithm. We restrict
attentions on manipulations that induce stable matchings. For the incomplete
preference list setting, where liars can truncate their preference lists, we
show that a strong Nash equilibrium always exists and the matching from such
equilibria is unique. The equilibrium outcome is strongly Pareto dominant for
all liars among the set of matchings achievable by manipulation: every woman is
matched with the same man as the one she matches in her best single-agent
manipulation. For the complete preference list setting where liars can permute
their preference list, we first show that a coalition of women can get worse
off by manipulating jointly than each performing a single-agent manipulation,
thus a strongly Pareto-dominant outcome may not exist by manipulation. We then
put forward an efficient algorithm to compute a strong Nash equilibrium that is
strongly Pareto-optimal for all liars. We derive connections between the stable
marriage problem and stable roommate problem, and use tools there to prove our
results for this part. This approach is highly nontrivial and of independent
interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07828</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07828</id><created>2015-02-27</created><authors><author><keyname>Baroffio</keyname><forenames>Luca</forenames></author><author><keyname>Cesana</keyname><forenames>Matteo</forenames></author><author><keyname>Redondi</keyname><forenames>Alessandro</forenames></author><author><keyname>Tagliasacchi</keyname><forenames>Marco</forenames></author><author><keyname>Tubaro</keyname><forenames>Stefano</forenames></author></authors><title>Hybrid coding of visual content and local image features</title><categories>cs.MM cs.CV</categories><comments>submitted to IEEE International Conference on Image Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed visual analysis applications, such as mobile visual search or
Visual Sensor Networks (VSNs) require the transmission of visual content on a
bandwidth-limited network, from a peripheral node to a processing unit.
Traditionally, a Compress-Then-Analyze approach has been pursued, in which
sensing nodes acquire and encode the pixel-level representation of the visual
content, that is subsequently transmitted to a sink node in order to be
processed. This approach might not represent the most effective solution, since
several analysis applications leverage a compact representation of the content,
thus resulting in an inefficient usage of network resources. Furthermore,
coding artifacts might significantly impact the accuracy of the visual task at
hand. To tackle such limitations, an orthogonal approach named
Analyze-Then-Compress has been proposed. According to such a paradigm, sensing
nodes are responsible for the extraction of visual features, that are encoded
and transmitted to a sink node for further processing. In spite of improved
task efficiency, such paradigm implies the central processing node not being
able to reconstruct a pixel-level representation of the visual content. In this
paper we propose an effective compromise between the two paradigms, namely
Hybrid-Analyze-Then-Compress (HATC) that aims at jointly encoding visual
content and local image features. Furthermore, we show how a target tradeoff
between image quality and task accuracy might be achieved by accurately
allocating the bitrate to either visual content or local features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07830</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07830</id><created>2015-02-27</created><authors><author><keyname>Wang</keyname><forenames>Qiwen</forenames></author><author><keyname>Cadambe</keyname><forenames>Viveck</forenames></author><author><keyname>Jaggi</keyname><forenames>Sidharth</forenames></author><author><keyname>Schwartz</keyname><forenames>Moshe</forenames></author><author><keyname>M&#xe9;dard</keyname><forenames>Muriel</forenames></author></authors><title>File Updates Under Random/Arbitrary Insertions And Deletions</title><categories>cs.IT math.IT</categories><comments>The paper is an extended version of our paper to be appeared at ITW
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A client/encoder edits a file, as modeled by an insertion-deletion (InDel)
process. An old copy of the file is stored remotely at a data-centre/decoder,
and is also available to the client. We consider the problem of throughput- and
computationally-efficient communication from the client to the data-centre, to
enable the server to update its copy to the newly edited file. We study two
models for the source files/edit patterns: the random pre-edit sequence
left-to-right random InDel (RPES-LtRRID) process, and the arbitrary pre-edit
sequence arbitrary InDel (APES-AID) process. In both models, we consider the
regime in which the number of insertions/deletions is a small (but constant)
fraction of the original file. For both models we prove information-theoretic
lower bounds on the best possible compression rates that enable file updates.
Conversely, our compression algorithms use dynamic programming (DP) and entropy
coding, and achieve rates that are approximately optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07838</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07838</id><created>2015-02-27</created><authors><author><keyname>Mikhalev</keyname><forenames>A. Yu.</forenames></author><author><keyname>Oseledets</keyname><forenames>I. V.</forenames></author></authors><title>Rectangular maximum-volume submatrices and their applications</title><categories>math.NA cs.NA</categories><comments>11 pages, 1 figure, submitted to SIAM J. Matrix Analysis</comments><msc-class>65F99, 45A05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A definition of $p$-volume of rectangular matrices is given. We generalize
the results for square maximum-volume submatrices to the case rectangular
maximal-volume submatrices and provide estimates for the growth of the
coefficients. Three promising applications of such submatrices are presented:
recommender systems, finding maximal elements in low-rank matrices and
preconditioning of overdetermined linear systems. The code is available online
at \url{https://bitbucket.org/muxas/rect_maxvol}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07839</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07839</id><created>2015-02-27</created><authors><author><keyname>Cheung</keyname><forenames>Man Hon</forenames></author><author><keyname>Huang</keyname><forenames>Jianwei</forenames></author></authors><title>DAWN: Delay-Aware Wi-Fi Offloading and Network Selection</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To accommodate the explosive growth in mobile data traffic, both mobile
cellular operators and mobile users are increasingly interested in offloading
the traffic from cellular networks to Wi-Fi networks. However, previously
proposed offloading schemes mainly focus on reducing the cellular data usage,
without paying too much attention on the quality of service (QoS) requirements
of the applications. In this paper, we study the Wi-Fi offloading problem with
delay-tolerant applications under usage-based pricing. We aim to achieve a good
tradeoff between the user's payment and its QoS characterized by the file
transfer deadline. We first propose a general Delay-Aware Wi-Fi Offloading and
Network Selection (DAWN) algorithm for a general single-user decision scenario.
We then analytically establish the sufficient conditions, under which the
optimal policy exhibits a threshold structure in terms of both the time and
file size. As a result, we propose a monotone DAWN algorithm that approximately
solves the general offloading problem, and has a much lower computational
complexity comparing to the optimal algorithm. Simulation results show that
both the general and monotone DAWN schemes achieve a high probability of
completing file transfer under a stringent deadline, and require the lowest
payment under a non-stringent deadline as compared with three heuristic
schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07847</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07847</id><created>2015-02-27</created><updated>2015-07-29</updated><authors><author><keyname>Coffrin</keyname><forenames>Carleton</forenames></author><author><keyname>Hijazi</keyname><forenames>Hassan L.</forenames></author><author><keyname>Van Hentenryck</keyname><forenames>Pascal</forenames></author></authors><title>The QC Relaxation: Theoretical and Computational Results on Optimal
  Power Flow</title><categories>cs.CE math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convex relaxations of the power flow equations and, in particular, the
Semi-Definite Programming (SDP) and Second-Order Cone (SOC) relaxations, have
attracted significant interest in recent years. The Quadratic Convex (QC)
relaxation is a departure from these relaxations in the sense that it imposes
constraints to preserve stronger links between the voltage variables through
convex envelopes of the polar representation. This paper is a systematic study
of the QC relaxation for AC Optimal Power Flow with realistic side constraints.
The main theoretical result shows that the QC relaxation is stronger than the
SOC relaxation and neither dominates nor is dominated by the SDP relaxation. In
addition, comprehensive computational results show that the QC relaxation may
produce significant improvements in accuracy over the SOC relaxation at a
reasonable computational cost, especially for networks with tight bounds on
phase angle differences. The QC and SOC relaxations are also shown to be
significantly faster and reliable compared to the SDP relaxation given the
current state of the respective solvers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07870</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07870</id><created>2015-02-27</created><authors><author><keyname>Alatabbi</keyname><forenames>Ali</forenames></author><author><keyname>Rahman</keyname><forenames>M. Sohel</forenames></author><author><keyname>Smyth</keyname><forenames>W. F.</forenames></author></authors><title>Inferring an Indeterminate String from a Prefix Graph</title><categories>cs.DS</categories><comments>13 pages, 1 figure</comments><doi>10.1016/j.jda.2014.12.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An \itbf{indeterminate string} (or, more simply, just a \itbf{string}) $\s{x}
= \s{x}[1..n]$ on an alphabet $\Sigma$ is a sequence of nonempty subsets of
$\Sigma$. We say that $\s{x}[i_1]$ and $\s{x}[i_2]$ \itbf{match} (written
$\s{x}[i_1] \match \s{x}[i_2]$) if and only if $\s{x}[i_1] \cap \s{x}[i_2] \ne
\emptyset$. A \itbf{feasible array} is an array $\s{y} = \s{y}[1..n]$ of
integers such that $\s{y}[1] = n$ and for every $i \in 2..n$, $\s{y}[i] \in
0..n\- i\+ 1$. A \itbf{prefix table} of a string $\s{x}$ is an array $\s{\pi} =
\s{\pi}[1..n]$ of integers such that, for every $i \in 1..n$, $\s{\pi}[i] = j$
if and only if $\s{x}[i..i\+ j\- 1]$ is the longest substring at position $i$
of \s{x} that matches a prefix of \s{x}. It is known from \cite{CRSW13} that
every feasible array is a prefix table of some indetermintate string. A
\itbf{prefix graph} $\mathcal{P} = \mathcal{P}_{\s{y}}$ is a labelled simple
graph whose structure is determined by a feasible array \s{y}. In this paper we
show, given a feasible array \s{y}, how to use $\mathcal{P}_{\s{y}}$ to
construct a lexicographically least indeterminate string on a minimum alphabet
whose prefix table $\s{\pi} = \s{y}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07888</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07888</id><created>2015-02-27</created><authors><author><keyname>Hoske</keyname><forenames>Daniel</forenames></author><author><keyname>Lukarski</keyname><forenames>Dimitar</forenames></author><author><keyname>Meyerhenke</keyname><forenames>Henning</forenames></author><author><keyname>Wegner</keyname><forenames>Michael</forenames></author></authors><title>Is Nearly-linear the same in Theory and Practice? A Case Study with a
  Combinatorial Laplacian Solver</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear system solving is one of the main workhorses in applied mathematics.
Recently, theoretical computer scientists have contributed sophisticated
algorithms for solving linear systems with symmetric diagonally dominant
matrices (a class to which Laplacian matrices belong) in provably nearly-linear
time. While these algorithms are highly interesting from a theoretical
perspective, there are no published results how they perform in practice.
  With this paper we address this gap. We provide the first implementation of
the combinatorial solver by [Kelner et al., STOC 2013], which is particularly
appealing for implementation due to its conceptual simplicity. The algorithm
exploits that a Laplacian matrix corresponds to a graph; solving Laplacian
linear systems amounts to finding an electrical flow in this graph with the
help of cycles induced by a spanning tree with the low-stretch property.
  The results of our comprehensive experimental study are ambivalent. They
confirm a nearly-linear running time, but for reasonable inputs the constant
factors make the solver much slower than methods with higher asymptotic
complexity. One other aspect predicted by theory is confirmed by our findings,
though: Spanning trees with lower stretch indeed reduce the solver's running
time. Yet, simple spanning tree algorithms perform in practice better than
those with a guaranteed low stretch.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07889</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07889</id><created>2015-02-27</created><authors><author><keyname>Enqvist</keyname><forenames>Sebastian</forenames></author><author><keyname>Seifan</keyname><forenames>Fatemeh</forenames></author><author><keyname>Venema</keyname><forenames>Yde</forenames></author></authors><title>Expressiveness of the modal mu-calculus on monotone neighborhood
  structures</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We characterize the expressive power of the modal mu-calculus on monotone
neighborhood structures, in the style of the Janin-Walukiewicz theorem for the
standard modal mu-calculus. For this purpose we consider a monadic second-order
logic for monotone neighborhood structures. Our main result shows that the
monotone modal mu-calculus corresponds exactly to the fragment of this
second-order language that is invariant for neighborhood bisimulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07920</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07920</id><created>2015-02-27</created><authors><author><keyname>Zhang</keyname><forenames>Jiajun</forenames></author></authors><title>Local Translation Prediction with Global Sentence Representation</title><categories>cs.CL</categories><comments>7 pages and 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistical machine translation models have made great progress in improving
the translation quality. However, the existing models predict the target
translation with only the source- and target-side local context information. In
practice, distinguishing good translations from bad ones does not only depend
on the local features, but also rely on the global sentence-level information.
In this paper, we explore the source-side global sentence-level features for
target-side local translation prediction. We propose a novel
bilingually-constrained chunk-based convolutional neural network to learn
sentence semantic representations. With the sentence-level feature
representation, we further design a feed-forward neural network to better
predict translations using both local and global information. The large-scale
experiments show that our method can obtain substantial improvements in
translation quality over the strong baseline: the hierarchical phrase-based
translation model augmented with the neural network joint model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07926</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07926</id><created>2015-02-27</created><authors><author><keyname>Wan</keyname><forenames>Yiming</forenames></author><author><keyname>Keviczky</keyname><forenames>Tamas</forenames></author><author><keyname>Verhaegen</keyname><forenames>Michel</forenames></author><author><keyname>Gustafsson</keyname><forenames>Fredrik</forenames></author></authors><title>Data-Driven Robust Receding Horizon Fault Estimation</title><categories>cs.SY</categories><comments>submitted to Automatica</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a data-driven receding horizon fault estimation method
for additive actuator and sensor faults in unknown linear time-invariant
systems, with enhanced robustness to stochastic identification errors.
State-of-the-art methods construct fault estimators with identified state-space
models or Markov parameters, but they do not compensate for identification
errors. Motivated by this limitation, we first propose a receding horizon fault
estimator parameterized by predictor Markov parameters. This estimator provides
(asymptotically) unbiased fault estimates as long as the subsystem from faults
to outputs has no unstable transmission zeros. When the identified Markov
parameters are used to construct the above fault estimator, zero-mean
stochastic identification errors appear as model uncertainty multiplied with
unknown fault signals and online system inputs/outputs (I/O). Based on this
fault estimation error analysis, we formulate a mixed-norm problem for the
offline robust design that regards online I/O data as unknown. An alternative
online mixed-norm problem is also proposed that can further reduce estimation
errors when the online I/O data have large amplitudes, at the cost of increased
computational burden. Based on a geometrical interpretation of the two proposed
mixed-norm problems, systematic methods to tune the user-defined parameters
therein are given to achieve desired performance trade-offs. Simulation
examples illustrate the benefits of our proposed methods compared to recent
literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07930</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07930</id><created>2015-02-27</created><updated>2015-03-10</updated><authors><author><keyname>Paschos</keyname><forenames>Vangelis Th.</forenames></author></authors><title>Combinatorial approximation of maximum $k$-vertex cover in bipartite
  graphs within ratio~0.7</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a \textit{purely combinatorial algorithm} for \mkvc{} in bipartite
graphs, achieving approximation ratio~0.7. The only combinatorial algorithms
currently known until now for this problem are the natural greedy algorithm,
that achieves ratio 0.632, and an easy~$2/3$-approximation algorithm presented
in \cite{DBLP:journals/corr/BonnetEPS14}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07938</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07938</id><created>2015-02-27</created><authors><author><keyname>Balabantaray</keyname><forenames>Rakesh Chandra</forenames></author><author><keyname>Sarma</keyname><forenames>Chandrali</forenames></author><author><keyname>Jha</keyname><forenames>Monica</forenames></author></authors><title>Document Clustering using K-Means and K-Medoids</title><categories>cs.IR</categories><journal-ref>International Journal of Knowledge Based Computer Systems, Volume
  1 Issue 1 (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the huge upsurge of information in day-to-days life, it has become
difficult to assemble relevant information in nick of time. But people, always
are in dearth of time, they need everything quick. Hence clustering was
introduced to gather the relevant information in a cluster. There are several
algorithms for clustering information out of which in this paper, we accomplish
K-means and K-Medoids clustering algorithm and a comparison is carried out to
find which algorithm is best for clustering. On the best clusters formed,
document summarization is executed based on sentence weight to focus on key
point of the whole document, which makes it easier for people to ascertain the
information they want and thus read only those documents which is relevant in
their point of view.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07939</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07939</id><created>2015-02-26</created><authors><author><keyname>Baroffio</keyname><forenames>Luca</forenames></author><author><keyname>Canclini</keyname><forenames>Antonio</forenames></author><author><keyname>Cesana</keyname><forenames>Matteo</forenames></author><author><keyname>Redondi</keyname><forenames>Alessandro</forenames></author><author><keyname>Tagliasacchi</keyname><forenames>Marco</forenames></author><author><keyname>Tubaro</keyname><forenames>Stefano</forenames></author></authors><title>Coding local and global binary visual features extracted from video
  sequences</title><categories>cs.MM cs.CV</categories><comments>submitted to IEEE Transactions on Image Processing</comments><doi>10.1109/TIP.2015.2445294</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Binary local features represent an effective alternative to real-valued
descriptors, leading to comparable results for many visual analysis tasks,
while being characterized by significantly lower computational complexity and
memory requirements. When dealing with large collections, a more compact
representation based on global features is often preferred, which can be
obtained from local features by means of, e.g., the Bag-of-Visual-Word (BoVW)
model. Several applications, including for example visual sensor networks and
mobile augmented reality, require visual features to be transmitted over a
bandwidth-limited network, thus calling for coding techniques that aim at
reducing the required bit budget, while attaining a target level of efficiency.
In this paper we investigate a coding scheme tailored to both local and global
binary features, which aims at exploiting both spatial and temporal redundancy
by means of intra- and inter-frame coding. In this respect, the proposed coding
scheme can be conveniently adopted to support the Analyze-Then-Compress (ATC)
paradigm. That is, visual features are extracted from the acquired content,
encoded at remote nodes, and finally transmitted to a central controller that
performs visual analysis. This is in contrast with the traditional approach, in
which visual content is acquired at a node, compressed and then sent to a
central unit for further processing, according to the Compress-Then-Analyze
(CTA) paradigm. In this paper we experimentally compare ATC and CTA by means of
rate-efficiency curves in the context of two different visual analysis tasks:
homography estimation and content-based retrieval. Our results show that the
novel ATC paradigm based on the proposed coding primitives can be competitive
with CTA, especially in bandwidth limited scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07943</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07943</id><created>2015-02-27</created><authors><author><keyname>Jamieson</keyname><forenames>Kevin</forenames></author><author><keyname>Talwalkar</keyname><forenames>Ameet</forenames></author></authors><title>Non-stochastic Best Arm Identification and Hyperparameter Optimization</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the task of hyperparameter optimization, we introduce the
non-stochastic best-arm identification problem. Within the multi-armed bandit
literature, the cumulative regret objective enjoys algorithms and analyses for
both the non-stochastic and stochastic settings while to the best of our
knowledge, the best-arm identification framework has only been considered in
the stochastic setting. We introduce the non-stochastic setting under this
framework, identify a known algorithm that is well-suited for this setting, and
analyze its behavior. Next, by leveraging the iterative nature of standard
machine learning algorithms, we cast hyperparameter optimization as an instance
of non-stochastic best-arm identification, and empirically evaluate our
proposed algorithm on this task. Our empirical results show that, by allocating
more resources to promising hyperparameter settings, we typically achieve
comparable test accuracies an order of magnitude faster than baseline methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07948</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07948</id><created>2015-02-27</created><authors><author><keyname>Cunha</keyname><forenames>J&#xe1;come</forenames></author><author><keyname>Fernandes</keyname><forenames>Jo&#xe3;o Paulo</forenames></author><author><keyname>Pereira</keyname><forenames>Rui</forenames></author><author><keyname>Saraiva</keyname><forenames>Jo&#xe3;o</forenames></author></authors><title>Querying Spreadsheets: An Empirical Study</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most important assets of any company is being able to easily
access information on itself and on its business. In this line, it has been
observed that this important information is often stored in one of the millions
of spreadsheets created every year, due to simplicity in using and manipulating
such an artifact. Unfortunately, in many cases it is quite difficult to
retrieve the intended information from a spreadsheet: information is often
stored in a huge unstructured matrix, with no care for readability or
comprehensiveness. In an attempt to aid users in the task of extracting
information from a spreadsheet, researchers have been working on models,
languages and tools to query. In this paper we present an empirical study
evaluating such proposals assessing their usage to query spreadsheets. We
investigate the use of the Google Query Function, textual model-driven
querying, and visual model-driven querying. To compare these different querying
approaches we present an empirical study whose results show that the end-users'
productivity increases when using model-driven queries, specially using its
visual representation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07966</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07966</id><created>2015-02-27</created><authors><author><keyname>Wang</keyname><forenames>Wei</forenames></author><author><keyname>Lau</keyname><forenames>Vincent K. N.</forenames></author><author><keyname>Peng</keyname><forenames>Mugen</forenames></author></authors><title>Delay-Aware Uplink Fronthaul Allocation in Cloud Radio Access Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In cloud radio access networks (C-RANs), the baseband units and radio units
of base stations are separated, which requires high-capacity fronthaul links
connecting both parts. In this paper, we consider the delay-aware fronthaul
allocation problem for C-RANs. The stochastic optimization problem is
formulated as an infinite horizon average cost Markov decision process. To deal
with the curse of dimensionality, we derive a closed-form approximate priority
function and the associated error bound using perturbation analysis. Based on
the closed-form approximate priority function, we propose a low-complexity
delay-aware fronthaul allocation algorithm solving the per-stage optimization
problem. The proposed solution is further shown to be asymptotically optimal
for sufficiently small cross link path gains. Finally, the proposed fronthaul
allocation algorithm is compared with various baselines through simulations,
and it is shown that significant performance gain can be achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07971</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07971</id><created>2015-02-27</created><updated>2015-03-16</updated><authors><author><keyname>Chen</keyname><forenames>Ricky X. F.</forenames></author><author><keyname>Reidys</keyname><forenames>Christian M.</forenames></author></authors><title>A simple framework on sorting permutations</title><categories>math.CO cs.IT math.IT</categories><comments>13 pages. This is the second part from division of the paper:
  arXiv:1411.5552v2 [math.CO], into two parts. The first part is:
  arXiv:1502.07674 [math.CO]. The original paper arXiv:1411.5552v2 [math.CO]
  will be removed soon. Comments are welcome. [v2]:Theorem 3 has been
  generalized to arbitrary permutations</comments><msc-class>05A05, 92B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a simple framework to study various distance
problems of permutations, including the transposition and block-interchange
distance of permutations as well as the reversal distance of signed
permutations. These problems are very important in the study of the evolution
of genomes. We give a general formulation for lower bounds of the transposition
and block-interchange distance from which the existing lower bounds obtained by
Bafna and Pevzner, and Christie can be easily derived. As to the reversal
distance of signed permutations, we translate it into a block-interchange
distance problem of permutations so that we obtain a new lower bound.
Furthermore, studying distance problems via our framework motivates several
interesting combinatorial problems related to product of permutations, some of
which are studied in this paper as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07973</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07973</id><created>2015-02-27</created><authors><author><keyname>Berta</keyname><forenames>Mario</forenames></author><author><keyname>Tomamichel</keyname><forenames>Marco</forenames></author></authors><title>The Fidelity of Recovery is Multiplicative</title><categories>quant-ph cs.IT math-ph math.IT math.MP</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fawzi and Renner (arXiv:1410.0664) recently established a lower bound on the
conditional quantum mutual information of tripartite quantum states
$\rho_{ABC}$ in terms of the fidelity of recovery, i.e. the maximal fidelity of
the state $\rho_{ABC}$ with a state reconstructed from its marginal $\rho_{BC}$
by acting only on the $C$ system. In this brief note we show that the fidelity
of recovery is multiplicative by utilizing semi-definite programming duality.
This allows us to simplify an operational proof by Brandao et al.
(arXiv:1411.4921) of the above-mentioned lower bound that is based on quantum
state redistribution. In particular, in contrast to the previous approaches,
our proof does not rely on de Finetti reductions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07974</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07974</id><created>2015-02-27</created><authors><author><keyname>Bemporad</keyname><forenames>Alberto</forenames></author><author><keyname>Bernardini</keyname><forenames>Daniele</forenames></author><author><keyname>Patrinos</keyname><forenames>Panagiotis</forenames></author></authors><title>A Convex Feasibility Approach to Anytime Model Predictive Control</title><categories>cs.SY</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes to decouple performance optimization and enforcement of
asymptotic convergence in Model Predictive Control (MPC) so that convergence to
a given terminal set is achieved independently of how much performance is
optimized at each sampling step. By embedding an explicit decreasing condition
in the MPC constraints and thanks to a novel and very easy-to-implement convex
feasibility solver proposed in the paper, it is possible to run an outer
performance optimization algorithm on top of the feasibility solver and
optimize for an amount of time that depends on the available CPU resources
within the current sampling step (possibly going open-loop at a given sampling
step in the extreme case no resources are available) and still guarantee
convergence to the terminal set. While the MPC setup and the solver proposed in
the paper can deal with quite general classes of functions, we highlight the
synthesis method and show numerical results in case of linear MPC and
ellipsoidal and polyhedral terminal sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07976</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07976</id><created>2015-02-27</created><updated>2015-03-05</updated><authors><author><keyname>Bautista</keyname><forenames>Miguel Angel</forenames></author><author><keyname>Pujol</keyname><forenames>Oriol</forenames></author><author><keyname>de la Torre</keyname><forenames>Fernando</forenames></author><author><keyname>Escalera</keyname><forenames>Sergio</forenames></author></authors><title>Error-Correcting Factorization</title><categories>cs.CV cs.LG</categories><comments>Under review at TPAMI</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Error Correcting Output Codes (ECOC) is a successful technique in multi-class
classification, which is a core problem in Pattern Recognition and Machine
Learning. A major advantage of ECOC over other methods is that the multi- class
problem is decoupled into a set of binary problems that are solved
independently. However, literature defines a general error-correcting
capability for ECOCs without analyzing how it distributes among classes,
hindering a deeper analysis of pair-wise error-correction. To address these
limitations this paper proposes an Error-Correcting Factorization (ECF) method,
our contribution is three fold: (I) We propose a novel representation of the
error-correction capability, called the design matrix, that enables us to build
an ECOC on the basis of allocating correction to pairs of classes. (II) We
derive the optimal code length of an ECOC using rank properties of the design
matrix. (III) ECF is formulated as a discrete optimization problem, and a
relaxed solution is found using an efficient constrained block coordinate
descent approach. (IV) Enabled by the flexibility introduced with the design
matrix we propose to allocate the error-correction on classes that are prone to
confusion. Experimental results in several databases show that when allocating
the error-correction to confusable classes ECF outperforms state-of-the-art
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07977</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07977</id><created>2015-02-27</created><authors><author><keyname>Berta</keyname><forenames>Mario</forenames></author><author><keyname>Seshadreesan</keyname><forenames>Kaushik P.</forenames></author><author><keyname>Wilde</keyname><forenames>Mark M.</forenames></author></authors><title>R\'enyi generalizations of quantum information measures</title><categories>quant-ph cs.IT hep-th math-ph math.IT math.MP</categories><comments>9 pages, related to and extends the results from arXiv:1403.6102</comments><journal-ref>Physical Review A vol. 91, no. 2, page 022333, February 2015</journal-ref><doi>10.1103/PhysRevA.91.022333</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum information measures such as the entropy and the mutual information
find applications in physics, e.g., as correlation measures. Generalizing such
measures based on the R\'enyi entropies is expected to enhance their scope in
applications. We prescribe R\'enyi generalizations for any quantum information
measure which consists of a linear combination of von Neumann entropies with
coefficients chosen from the set {-1,0,1}. As examples, we describe R\'enyi
generalizations of the conditional quantum mutual information, some quantum
multipartite information measures, and the topological entanglement entropy.
Among these, we discuss the various properties of the R\'enyi conditional
quantum mutual information and sketch some potential applications. We
conjecture that the proposed R\'enyi conditional quantum mutual informations
are monotone increasing in the R\'enyi parameter, and we have proofs of this
conjecture for some special cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07979</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07979</id><created>2015-02-27</created><updated>2015-03-17</updated><authors><author><keyname>Noulas</keyname><forenames>Anastasios</forenames></author><author><keyname>Shaw</keyname><forenames>Blake</forenames></author><author><keyname>Lambiotte</keyname><forenames>Renaud</forenames></author><author><keyname>Mascolo</keyname><forenames>Cecilia</forenames></author></authors><title>Topological Properties and Temporal Dynamics of Place Networks in Urban
  Environments</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the spatial networks formed by the trajectories of mobile users
can be beneficial to applications ranging from epidemiology to local search.
Despite the potential for impact in a number of fields, several aspects of
human mobility networks remain largely unexplored due to the lack of
large-scale data at a fine spatiotemporal resolution. Using a longitudinal
dataset from the location-based service Foursquare, we perform an empirical
analysis of the topological properties of place networks and note their
resemblance to online social networks in terms of heavy-tailed degree
distributions, triadic closure mechanisms and the small world property. Unlike
social networks however, place networks present a mixture of connectivity
trends in terms of assortativity that are surprisingly similar to those of the
web graph. We take advantage of additional semantic information to interpret
how nodes that take on functional roles such as `travel hub', or `food spot'
behave in these networks. Finally, motivated by the large volume of new links
appearing in place networks over time, we formulate the classic link prediction
problem in this new domain. We propose a novel variant of gravity models that
brings together three essential elements of inter-place connectivity in urban
environments: network-level interactions, human mobility dynamics, and
geographic distance. We evaluate this model and find it outperforms a number of
baseline predictors and supervised learning algorithms on a task of predicting
new links in a sample of one hundred popular cities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07981</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07981</id><created>2015-02-27</created><authors><author><keyname>Bondarenko</keyname><forenames>I.</forenames></author><author><keyname>D'Angeli</keyname><forenames>D.</forenames></author><author><keyname>Rodaro</keyname><forenames>E.</forenames></author></authors><title>The lamplighter group $\mathbb{Z}_3\wr\mathbb{Z}$ generated by a
  bireversible automaton</title><categories>math.GR cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct a bireversible self-dual automaton with $3$ states over an
alphabet with $3$ letters which generates the lamplighter group
$\mathbb{Z}_3\wr\mathbb{Z}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07990</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07990</id><created>2015-02-27</created><updated>2015-03-15</updated><authors><author><keyname>Margenstern</keyname><forenames>Maurice</forenames></author></authors><title>Infinigons of the hyperbolic plane and grossone</title><categories>cs.DM</categories><comments>13 pages, 2 figures, correction of typos</comments><msc-class>68R01</msc-class><acm-class>F.2.2; F.4.1; I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the contribution of the theory of grossone to the
study of infinigons in the hyperbolic plane. We can see that the theory of
grossone can help us to obtain much more classification for these objects than
in the traditional setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07993</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07993</id><created>2015-02-26</created><authors><author><keyname>Sreela</keyname><forenames>S. R.</forenames></author><author><keyname>Kumar</keyname><forenames>G. Santhosh</forenames></author><author><keyname>Binu</keyname><forenames>V. P.</forenames></author></authors><title>Secret Image Sharing Based CTS with Cheating Detection</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cheque Truncation System(CTS) is an automatic cheque clearance system
implemented by RBI.CTS uses cheque image, instead of the physical cheque
itself, for cheque clearance thus reducing the turn around time drastically.
This approach holds back the physical movement of cheque from presenting bank
to the drawee bank. In CTS, digital image of the cheque is protected using
standard public key and symmetric key encryptions like RSA, triple DES etc.
This involves a lot of computation overhead and key management. The security
also depends on the hard mathematical problem and is only computationally
secure.Information theoretically secure, secret image sharing techniques can be
used in the CTS for the secure and efficient processing of cheque image .In
this paper, we propose two simple and efficient secret image sharing schemes
and a Cheque Truncation System based on these algorithms . In the proposed
scheme,the presenting bank is acting as the dealer and the participants are the
customer, and the drawee bank.The dealer should generate the shares of cheque
and distributes it to customer and drawee bank.The validity of the shares are
important during the reconstruction process. The proposed scheme also suggests
a method for cheating detection which identify any invalid shares submitted by
the customers, using the hashing technique. The experimental results shows that
the proposed scheme is efficient and secure compared with the existing scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07994</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07994</id><created>2015-02-26</created><authors><author><keyname>Nair</keyname><forenames>Divya G.</forenames></author><author><keyname>Binu</keyname><forenames>V. P.</forenames></author><author><keyname>Kumar</keyname><forenames>G. Santhosh</forenames></author></authors><title>An Effective Private Data storage and Retrieval System using Secret
  sharing scheme based on Secure Multi-party Computation</title><categories>cs.CR</categories><comments>Data Science &amp; Engineering (ICDSE), 2014 International Conference,
  CUSAT</comments><doi>10.1109/ICDSE.2014.6974639</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Privacy of the outsourced data is one of the major challenge.Insecurity of
the network environment and untrustworthiness of the service providers are
obstacles of making the database as a service.Collection and storage of
personally identifiable information is a major privacy concern.On-line public
databases and resources pose a significant risk to user privacy, since a
malicious database owner may monitor user queries and infer useful information
about the customer.The challenge in data privacy is to share data with
third-party and at the same time securing the valuable information from
unauthorized access and use by third party.A Private Information Retrieval(PIR)
scheme allows a user to query database while hiding the identity of the data
retrieved.The naive solution for confidentiality is to encrypt data before
outsourcing.Query execution,key management and statistical inference are major
challenges in this case.The proposed system suggests a mechanism for secure
storage and retrieval of private data using the secret sharing technique.The
idea is to develop a mechanism to store private information with a highly
available storage provider which could be accessed from anywhere using queries
while hiding the actual data values from the storage provider.The private
information retrieval system is implemented using Secure Multi-party
Computation(SMC) technique which is based on secret sharing. Multi-party
Computation enable parties to compute some joint function over their private
inputs.The query results are obtained by performing a secure computation on the
shares owned by the different servers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07996</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07996</id><created>2015-02-27</created><authors><author><keyname>Orovic</keyname><forenames>Irena</forenames></author><author><keyname>Draganic</keyname><forenames>Andjela</forenames></author><author><keyname>Stankovic</keyname><forenames>Srdjan</forenames></author></authors><title>Sparse Time-Frequency Representation for Signals with Fast Varying
  Instantaneous Frequency</title><categories>cs.IT math.IT</categories><comments>submitted to the IET Radar, Sonar and Navigation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time-frequency distributions have been used to provide high resolution
representation in a large number of signal processing applications. However,
high resolution and accurate instantaneous frequency (IF) estimation usually
depend on the employed distribution and complexity of signal phase function. To
ensure an efficient IF tracking for various types of signals, the class of
complex time distributions has been developed. These distributions facilitate
analysis in the cases when standard distributions cannot provide satisfactory
results (e.g., for highly nonstationary signal phase). In that sense, an
ambiguity based form of the forth order complex-time distribution is
considered, in a new compressive sensing (CS) context. CS is an intensively
growing approach in signal processing that allows efficient analysis and
reconstruction of randomly undersampled signals. In this paper, the randomly
chosen ambiguity domain coefficients serve as CS measurements. By exploiting
sparsity in the time-frequency plane, it is possible to obtain highly
concentrated IF using just small number of random coefficients from ambiguity
domain. Moreover, in noisy signal case, this CS approach can be efficiently
combined with the L-statistics producing robust time-frequency representations.
Noisy coefficients are firstly removed using the L-statistics and then
reconstructed by using CS algorithm. The theoretical considerations are
illustrated using experimental results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.07999</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.07999</id><created>2015-02-27</created><authors><author><keyname>Blake</keyname><forenames>Christopher</forenames></author><author><keyname>Kschischang</keyname><forenames>Frank R.</forenames></author></authors><title>On the Energy Complexity of LDPC Decoder Circuits</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is shown that in a sequence of randomly generated bipartite configurations
with number of left nodes approaching infinity, the probability that a
particular configuration in the sequence has a minimum bisection width
proportional to the number of vertices in the configuration approaches $1$ so
long as a sufficient condition on the node degree distribution is satisfied.
This graph theory result implies an almost sure $\Omega\left(n^{2}\right)$
scaling rule for the energy of capacity-approaching LDPC decoder circuits that
directly instantiate their Tanner Graphs and are generated according to a
uniform configuration model, where $n$ is the block length of the code. For a
sequence of circuits that have a full set of check nodes but do not necessarily
directly instantiate a Tanner graph, this implies an
$\Omega\left(n^{1.5}\right)$ scaling rule. In another theorem, it is shown that
all (as opposed to almost all) capacity-approaching LDPC decoding circuits that
directly implement their Tanner graphs must have energy that scales as
$\Omega\left(n\left(\log n\right)^{2}\right)$. These results further imply
scaling rules for the energy of LDPC decoder circuits as a function of gap to
capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.08003</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.08003</id><created>2015-02-27</created><authors><author><keyname>Cabibihan</keyname><forenames>John-John</forenames></author><author><keyname>Joshi</keyname><forenames>Deepak</forenames></author><author><keyname>Srinivasa</keyname><forenames>Yeshwin Mysore</forenames></author><author><keyname>Chan</keyname><forenames>Mark Aaron</forenames></author><author><keyname>Muruganantham</keyname><forenames>Arrchana</forenames></author></authors><title>Illusory Sense of Human Touch from a Warm and Soft Artificial Hand</title><categories>physics.med-ph cs.RO physics.ins-det</categories><comments>23 pages, 12 figures, supplementary video at:
  http://youtu.be/lATSgG7CuQU; contact info at: http://www.johncabibihan.com,
  IEEE Trans on Neural Systems and Rehabilitation Engineering, 2015</comments><doi>10.1109/TNSRE.2014.2360533</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  To touch and be touched are vital to human development, well being, and
relationships. However, to those who have lost their arms and hands due to
accident or war, touching becomes a serious concern that often leads to
psychosocial issues and social stigma. In this paper, we demonstrate that the
touch from a warm and soft rubber hand can be perceived by another person as if
the touch were coming from a human hand. We describe a three step process
toward this goal. First, we made participants select artificial skin samples
according to their preferred warmth and softness characteristics. At room
temperature, the preferred warmth was found to be 28.4 deg C at the skin
surface of a soft silicone rubber material that has a Shore durometer value of
30 at the OO scale. Second, we developed a process to create a rubber hand
replica of a human hand. To compare the skin softness of a human hand and
artificial hands, a robotic indenter was employed to produce a softness map by
recording the displacement data when constant indentation force of 1 N was
applied to 780 data points on the palmar side of the hand. Results showed that
an artificial hand with skeletal structure is as soft as a human hand. Lastly,
the participants arms were touched with human and artificial hands, but they
were prevented to see the hand that touched them. Receiver operating
characteristic curve analysis suggests that a warm and soft artificial hand can
create an illusion that the touch is from a human hand. These findings open the
possibilities for prosthetic and robotic hands that are lifelike and are more
socially acceptable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.08008</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.08008</id><created>2015-02-27</created><authors><author><keyname>Cruz-Filipe</keyname><forenames>Lu&#xed;s</forenames></author><author><keyname>Schneider-Kamp</keyname><forenames>Peter</forenames></author></authors><title>Optimizing a Certified Proof Checker for a Large-Scale
  Computer-Generated Proof</title><categories>cs.LO</categories><comments>IMADA-preprint-cs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent work, we formalized the theory of optimal-size sorting networks
with the goal of extracting a verified checker for the large-scale
computer-generated proof that 25 comparisons are optimal when sorting 9 inputs,
which required more than a decade of CPU time and produced 27 GB of proof
witnesses. The checker uses an untrusted oracle based on these witnesses and is
able to verify the smaller case of 8 inputs within a couple of days, but it did
not scale to the full proof for 9 inputs. In this paper, we describe several
non-trivial optimizations of the algorithm in the checker, obtained by
appropriately changing the formalization and capitalizing on the symbiosis with
an adequate implementation of the oracle. We provide experimental evidence of
orders of magnitude improvements to both runtime and memory footprint for 8
inputs, and actually manage to check the full proof for 9 inputs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.08009</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.08009</id><created>2015-02-27</created><authors><author><keyname>Koolen</keyname><forenames>Wouter M.</forenames></author><author><keyname>van Erven</keyname><forenames>Tim</forenames></author></authors><title>Second-order Quantile Methods for Experts and Combinatorial Games</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We aim to design strategies for sequential decision making that adjust to the
difficulty of the learning problem. We study this question both in the setting
of prediction with expert advice, and for more general combinatorial decision
tasks. We are not satisfied with just guaranteeing minimax regret rates, but we
want our algorithms to perform significantly better on easy data. Two popular
ways to formalize such adaptivity are second-order regret bounds and quantile
bounds. The underlying notions of 'easy data', which may be paraphrased as &quot;the
learning problem has small variance&quot; and &quot;multiple decisions are useful&quot;, are
synergetic. But even though there are sophisticated algorithms that exploit one
of the two, no existing algorithm is able to adapt to both.
  In this paper we outline a new method for obtaining such adaptive algorithms,
based on a potential function that aggregates a range of learning rates (which
are essential tuning parameters). By choosing the right prior we construct
efficient algorithms and show that they reap both benefits by proving the first
bounds that are both second-order and incorporate quantiles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.08010</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.08010</id><created>2015-02-27</created><authors><author><keyname>Grigoriev</keyname><forenames>Dima</forenames></author></authors><title>Tropical differential equations</title><categories>cs.SC math.AG</categories><msc-class>14T05</msc-class><acm-class>I.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tropical differential equations are introduced and an algorithm is designed
which tests solvability of a system of tropical linear differential equations
within the complexity polynomial in the size of the system and in its
coefficients. Moreover, we show that there exists a minimal solution, and the
algorithm constructs it (in case of solvability). This extends a similar
complexity bound established for tropical linear systems. In case of tropical
linear differential systems in one variable a polynomial complexity algorithm
for testing its solvability is designed.
  We prove also that the problem of solvability of a system of tropical
non-linear differential equations in one variable is $NP$-hard, and this
problem for arbitrary number of variables belongs to $NP$. Similar to tropical
algebraic equations, a tropical differential equation expresses the (necessary)
condition on the dominant term in the issue of solvability of a differential
equation in power series.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.08014</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.08014</id><created>2015-01-17</created><updated>2015-03-11</updated><authors><author><keyname>Ahmad</keyname><forenames>Sk. Safique</forenames></author><author><keyname>Ali</keyname><forenames>Istkhar</forenames></author></authors><title>Localization theorems for eigenvalues of quaternionic matrices</title><categories>math.RA cs.NA</categories><comments>14 pages</comments><msc-class>15A18, 15A66</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ostrowski type and Brauer type theorems are derived for the left eigenvalues
of quaternionic matrix. We see that the above theorems for the left eigenvalues
are also true for the case of right eigenvalues, when the diagonals of
quaternionic matrix are real. Some distribution theorems are given in terms of
ovals of Cassini that are sharper than the Ostrowski type theorems,
respectively, for the left and right eigenvalues of quaternionic matrix. In
addition, generalizations of the Gerschgorin type theorems are discussed for
both the left and right eigenvalues of quaternionic matrix, and finally, we see
that our framework is so developed that generalizes the existing results in the
literatures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.08029</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.08029</id><created>2015-02-27</created><updated>2015-09-30</updated><authors><author><keyname>Yao</keyname><forenames>Li</forenames></author><author><keyname>Torabi</keyname><forenames>Atousa</forenames></author><author><keyname>Cho</keyname><forenames>Kyunghyun</forenames></author><author><keyname>Ballas</keyname><forenames>Nicolas</forenames></author><author><keyname>Pal</keyname><forenames>Christopher</forenames></author><author><keyname>Larochelle</keyname><forenames>Hugo</forenames></author><author><keyname>Courville</keyname><forenames>Aaron</forenames></author></authors><title>Describing Videos by Exploiting Temporal Structure</title><categories>stat.ML cs.AI cs.CL cs.CV cs.LG</categories><comments>Accepted to ICCV15. This version comes with code release and
  supplementary material</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent progress in using recurrent neural networks (RNNs) for image
description has motivated the exploration of their application for video
description. However, while images are static, working with videos requires
modeling their dynamic temporal structure and then properly integrating that
information into a natural language description. In this context, we propose an
approach that successfully takes into account both the local and global
temporal structure of videos to produce descriptions. First, our approach
incorporates a spatial temporal 3-D convolutional neural network (3-D CNN)
representation of the short temporal dynamics. The 3-D CNN representation is
trained on video action recognition tasks, so as to produce a representation
that is tuned to human motion and behavior. Second we propose a temporal
attention mechanism that allows to go beyond local temporal modeling and learns
to automatically select the most relevant temporal segments given the
text-generating RNN. Our approach exceeds the current state-of-art for both
BLEU and METEOR metrics on the Youtube2Text dataset. We also present results on
a new, larger and more challenging dataset of paired video and natural language
descriptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.08030</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.08030</id><created>2015-02-27</created><authors><author><keyname>Tran</keyname><forenames>Hung Nghiep</forenames></author><author><keyname>Huynh</keyname><forenames>Tin</forenames></author><author><keyname>Do</keyname><forenames>Tien</forenames></author></authors><title>Author Name Disambiguation by Using Deep Neural Network</title><categories>cs.DL cs.CL cs.LG</categories><journal-ref>LNCS, Vol. 8397 (2014), pp. 123-132</journal-ref><doi>10.1007/978-3-319-05476-6_13</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Author name ambiguity decreases the quality and reliability of information
retrieved from digital libraries. Existing methods have tried to solve this
problem by predefining a feature set based on expert's knowledge for a specific
dataset. In this paper, we propose a new approach which uses deep neural
network to learn features automatically from data. Additionally, we propose the
general system architecture for author name disambiguation on any dataset. In
this research, we evaluate the proposed method on a dataset containing
Vietnamese author names. The results show that this method significantly
outperforms other methods that use predefined feature set. The proposed method
achieves 99.31% in terms of accuracy. Prediction error rate decreases from
1.83% to 0.69%, i.e., it decreases by 1.14%, or 62.3% relatively compared with
other methods that use predefined feature set (Table 3).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.08033</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.08033</id><created>2015-02-27</created><authors><author><keyname>Anh</keyname><forenames>Vu Le</forenames></author><author><keyname>Hai</keyname><forenames>Vo Hoang</forenames></author><author><keyname>Tran</keyname><forenames>Hung Nghiep</forenames></author><author><keyname>Jung</keyname><forenames>Jason J.</forenames></author></authors><title>SciRecSys: A Recommendation System for Scientific Publication by
  Discovering Keyword Relationships</title><categories>cs.DL cs.CL cs.IR</categories><journal-ref>LNCS, Vol. 8733 (2014), pp. 72-82</journal-ref><doi>10.1007/978-3-319-11289-3_8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose a new approach for discovering various relationships
among keywords over the scientific publications based on a Markov Chain model.
It is an important problem since keywords are the basic elements for
representing abstract objects such as documents, user profiles, topics and many
things else. Our model is very effective since it combines four important
factors in scientific publications: content, publicity, impact and randomness.
Particularly, a recommendation system (called SciRecSys) has been presented to
support users to efficiently find out relevant articles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.08037</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.08037</id><created>2015-02-27</created><authors><author><keyname>Boskos</keyname><forenames>Dimitris</forenames></author><author><keyname>Dimarogonas</keyname><forenames>Dimos V.</forenames></author></authors><title>Decentralized Abstractions for Feedback Interconnected Multi-Agent
  Systems</title><categories>cs.SY</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this report is to define abstractions for multi-agent systems
under coupled constraints. In the proposed decentralized framework, we specify
a finite or countable transition system for each agent which only takes into
account the discrete positions of its neighbors. The dynamics of the considered
systems consist of two components. An appropriate feedback law which guarantees
that certain performance requirements (eg. connectivity) are preserved and
induces the coupled constraints and additional free inputs which we exploit in
order to accomplish high level tasks. In this work we provide sufficient
conditions on the space and time discretization of the system which ensure that
we can extract a well posed and hence meaningful finite transition system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.08039</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.08039</id><created>2015-02-27</created><authors><author><keyname>Hamm</keyname><forenames>Jihun</forenames></author><author><keyname>Belkin</keyname><forenames>Mikhail</forenames></author></authors><title>Probabilistic Zero-shot Classification with Semantic Rankings</title><categories>cs.LG cs.AI cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a non-metric ranking-based representation of
semantic similarity that allows natural aggregation of semantic information
from multiple heterogeneous sources. We apply the ranking-based representation
to zero-shot learning problems, and present deterministic and probabilistic
zero-shot classifiers which can be built from pre-trained classifiers without
retraining. We demonstrate their the advantages on two large real-world image
datasets. In particular, we show that aggregating different sources of semantic
information, including crowd-sourcing, leads to more accurate classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.08040</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.08040</id><created>2015-02-27</created><updated>2015-03-23</updated><authors><author><keyname>Kumar</keyname><forenames>Mayank</forenames></author><author><keyname>Veeraraghavan</keyname><forenames>Ashok</forenames></author><author><keyname>Sabharval</keyname><forenames>Ashutosh</forenames></author></authors><title>DistancePPG: Robust non-contact vital signs monitoring using a camera</title><categories>cs.CV</categories><comments>24 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vital signs such as pulse rate and breathing rate are currently measured
using contact probes. But, non-contact methods for measuring vital signs are
desirable both in hospital settings (e.g. in NICU) and for ubiquitous in-situ
health tracking (e.g. on mobile phone and computers with webcams). Recently,
camera-based non-contact vital sign monitoring have been shown to be feasible.
However, camera-based vital sign monitoring is challenging for people with
darker skin tone, under low lighting conditions, and/or during movement of an
individual in front of the camera. In this paper, we propose distancePPG, a new
camera-based vital sign estimation algorithm which addresses these challenges.
DistancePPG proposes a new method of combining skin-color change signals from
different tracked regions of the face using a weighted average, where the
weights depend on the blood perfusion and incident light intensity in the
region, to improve the signal-to-noise ratio (SNR) of camera-based estimate.
One of our key contributions is a new automatic method for determining the
weights based only on the video recording of the subject. The gains in SNR of
camera-based PPG estimated using distancePPG translate into reduction of the
error in vital sign estimation, and thus expand the scope of camera-based vital
sign monitoring to potentially challenging scenarios. Further, a dataset will
be released, comprising of synchronized video recordings of face and pulse
oximeter based ground truth recordings from the earlobe for people with
different skin tones, under different lighting conditions and for various
motion scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.08046</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.08046</id><created>2015-02-27</created><authors><author><keyname>P&#x142;o&#x144;ski</keyname><forenames>Piotr</forenames></author><author><keyname>Stefan</keyname><forenames>Dorota</forenames></author><author><keyname>Sulej</keyname><forenames>Robert</forenames></author><author><keyname>Zaremba</keyname><forenames>Krzysztof</forenames></author></authors><title>Image Segmentation in Liquid Argon Time Projection Chamber Detector</title><categories>cs.CV hep-ex</categories><comments>10 pages, 4 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Liquid Argon Time Projection Chamber (LAr-TPC) detectors provide
excellent imaging and particle identification ability for studying neutrinos.
An efficient and automatic reconstruction procedures are required to exploit
potential of this imaging technology. Herein, a novel method for segmentation
of images from LAr-TPC detectors is presented. The proposed approach computes a
feature descriptor for each pixel in the image, which characterizes amplitude
distribution in pixel and its neighbourhood. The supervised classifier is
employed to distinguish between pixels representing particle's track and noise.
The classifier is trained and evaluated on the hand-labeled dataset. The
proposed approach can be a preprocessing step for reconstructing algorithms
working directly on detector images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.08048</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.08048</id><created>2015-02-27</created><authors><author><keyname>Cohen</keyname><forenames>Michael B.</forenames></author><author><keyname>Fasy</keyname><forenames>Brittany Terese</forenames></author><author><keyname>Miller</keyname><forenames>Gary L.</forenames></author><author><keyname>Nayyeri</keyname><forenames>Amir</forenames></author><author><keyname>Sheehy</keyname><forenames>Donald R.</forenames></author><author><keyname>Velingker</keyname><forenames>Ameya</forenames></author></authors><title>Approximating Nearest Neighbor Distances</title><categories>cs.CG</categories><comments>corrected author name</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several researchers proposed using non-Euclidean metrics on point sets in
Euclidean space for clustering noisy data. Almost always, a distance function
is desired that recognizes the closeness of the points in the same cluster,
even if the Euclidean cluster diameter is large. Therefore, it is preferred to
assign smaller costs to the paths that stay close to the input points.
  In this paper, we consider the most natural metric with this property, which
we call the nearest neighbor metric. Given a point set P and a path $\gamma$,
our metric charges each point of $\gamma$ with its distance to P. The total
charge along $\gamma$ determines its nearest neighbor length, which is formally
defined as the integral of the distance to the input points along the curve. We
describe a $(3+\varepsilon)$-approximation algorithm and a
$(1+\varepsilon)$-approximation algorithm to compute the nearest neighbor
metric. Both approximation algorithms work in near-linear time. The former uses
shortest paths on a sparse graph using only the input points. The latter uses a
sparse sample of the ambient space, to find good approximate geodesic paths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.08053</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.08053</id><created>2015-02-27</created><authors><author><keyname>Csiba</keyname><forenames>Dominik</forenames></author><author><keyname>Qu</keyname><forenames>Zheng</forenames></author><author><keyname>Richt&#xe1;rik</keyname><forenames>Peter</forenames></author></authors><title>Stochastic Dual Coordinate Ascent with Adaptive Probabilities</title><categories>math.OC cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces AdaSDCA: an adaptive variant of stochastic dual
coordinate ascent (SDCA) for solving the regularized empirical risk
minimization problems. Our modification consists in allowing the method
adaptively change the probability distribution over the dual variables
throughout the iterative process. AdaSDCA achieves provably better complexity
bound than SDCA with the best fixed probability distribution, known as
importance sampling. However, it is of a theoretical character as it is
expensive to implement. We also propose AdaSDCA+: a practical variant which in
our experiments outperforms existing non-adaptive methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00010</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00010</id><created>2015-02-27</created><authors><author><keyname>Tian</keyname><forenames>Chao</forenames></author></authors><title>A Note on the Fundamental Limits of Coded Caching</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fundamental limit of coded caching is investigated for the case with
$N=3$ files and $K=3$ users. An improved outer bound is obtained through the
computational approach developed by the author in an earlier work. This result
is part of the online collection of &quot;Solutions of Computed Information
Theoretic Limits (SCITL)&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00011</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00011</id><created>2015-02-27</created><authors><author><keyname>Tian</keyname><forenames>Chao</forenames></author></authors><title>A Note on the Rate Region of Exact-Repair Regenerating Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rate region of the $(5,4,4)$ exact-repair regenerating codes is provided.
The outer bound is obtained through extension of the computational approach
developed in an earlier work, and this region is indeed achievable using the
canonical layered codes. This result is part of the online collection of
&quot;Solutions of Computed Information Theoretic Limits (SCITL)&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00013</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00013</id><created>2015-02-27</created><authors><author><keyname>Tian</keyname><forenames>Chao</forenames></author><author><keyname>Liu</keyname><forenames>Tie</forenames></author></authors><title>Multilevel Diversity Coding with Regeneration</title><categories>cs.IT math.IT</categories><comments>21 pages,2 figures. Submitted to IEEE Trans. IT</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Digital contents in large scale distributed storage systems may have
different reliability and access delay requirements, and for this reason,
erasure codes with different strengths need to be utilized to achieve the best
storage efficiency. At the same time, in such large scale distributed storage
systems, nodes fail on a regular basis, and the contents stored on them need to
be regenerated and stored on other healthy nodes, the efficiency of which is an
important factor affecting the overall quality of service. In this work, we
formulate the problem of multilevel diversity coding with regeneration to
address these considerations, for which the storage vs. repair-bandwidth
tradeoff is investigated. We show that the extreme point on this tradeoff
corresponding to the minimum possible storage can be achieved by a simple
coding scheme, where contents with different reliability requirements are
encoded separately with individual regenerating codes without any mixing. On
the other hand, we establish the complete storage-repair-bandwidth tradeoff for
the case of four storage nodes, which reveals that codes mixing different
contents can strictly improve this tradeoff over the separate coding solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00021</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00021</id><created>2015-02-27</created><updated>2016-02-23</updated><authors><author><keyname>Gorodetsky</keyname><forenames>Alex A.</forenames></author><author><keyname>Marzouk</keyname><forenames>Youssef M.</forenames></author></authors><title>Mercer kernels and integrated variance experimental design: connections
  between Gaussian process regression and polynomial approximation</title><categories>cs.NA math.NA stat.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines experimental design procedures used to develop surrogates
of computational models, exploring the interplay between experimental designs
and approximation algorithms. We focus on two widely used approximation
approaches, Gaussian process (GP) regression and non-intrusive polynomial
approximation. First, we introduce algorithms for minimizing a posterior
integrated variance (IVAR) design criterion for GP regression. Our formulation
treats design as a continuous optimization problem that can be solved with
gradient-based methods on complex input domains, without resorting to greedy
approximations. We show that minimizing IVAR in this way yields point sets with
good interpolation properties, and that it enables more accurate GP regression
than designs based on entropy minimization or mutual information maximization.
Second, using a Mercer kernel/eigenfunction perspective on GP regression, we
identify conditions under which GP regression coincides with pseudospectral
polynomial approximation. Departures from these conditions can be understood as
changes either to the kernel or to the experimental design itself. We then show
how IVAR-optimal designs, while sacrificing discrete orthogonality of the
kernel eigenfunctions, can yield lower approximation error than orthogonalizing
point sets. Finally, we compare the performance of adaptive Gaussian process
regression and adaptive pseudospectral approximation for several classes of
target functions, identifying features that are favorable to the GP + IVAR
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00022</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00022</id><created>2015-02-27</created><authors><author><keyname>De</keyname><forenames>Soham</forenames></author><author><keyname>Roy</keyname><forenames>Indradyumna</forenames></author><author><keyname>Prabhakar</keyname><forenames>Tarunima</forenames></author><author><keyname>Suneja</keyname><forenames>Kriti</forenames></author><author><keyname>Chaudhuri</keyname><forenames>Sourish</forenames></author><author><keyname>Singh</keyname><forenames>Rita</forenames></author><author><keyname>Raj</keyname><forenames>Bhiksha</forenames></author></authors><title>Plagiarism Detection in Polyphonic Music using Monaural Signal
  Separation</title><categories>cs.SD cs.AI</categories><comments>Preprint version</comments><journal-ref>INTERSPEECH-2012, 1744-1747 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given the large number of new musical tracks released each year, automated
approaches to plagiarism detection are essential to help us track potential
violations of copyright. Most current approaches to plagiarism detection are
based on musical similarity measures, which typically ignore the issue of
polyphony in music. We present a novel feature space for audio derived from
compositional modelling techniques, commonly used in signal separation, that
provides a mechanism to account for polyphony without incurring an inordinate
amount of computational overhead. We employ this feature representation in
conjunction with traditional audio feature representations in a classification
framework which uses an ensemble of distance features to characterize pairs of
songs as being plagiarized or not. Our experiments on a database of about 3000
musical track pairs show that the new feature space characterization produces
significant improvements over standard baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00024</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00024</id><created>2015-02-27</created><updated>2015-04-13</updated><authors><author><keyname>Vaswani</keyname><forenames>Sharan</forenames></author><author><keyname>Lakshmanan</keyname><forenames>Laks. V. S.</forenames></author></authors><title>Influence Maximization with Bandits</title><categories>cs.SI cs.LG stat.ML</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most work on influence maximization assumes network influence probabilities
are given. The few papers that propose algorithms for learning these
probabilities assume the availability of a batch of diffusion cascades and
learn the probabilities offline. We tackle the real but difficult problems of
(i)learning in influence probabilities and (ii) maximizing influence spread,
when no cascades are available as input, by adopting a combinatorial
multi-armed bandit (CMAB) paradigm. We formulate the above problems
respectively as network exploration, i.e., minimizing the error in learned
influence probabilities, and minimization of loss in spread from choosing
suboptimal seed sets over the rounds of a CMAB game. We propose algorithms for
both problems and establish bounds on their performance. Finally, we
demonstrate the effectiveness and usefulness of the proposed algorithms via a
comprehensive set of experiments over three real datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00030</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00030</id><created>2015-02-27</created><authors><author><keyname>Fern&#xe1;ndez-Gonz&#xe1;lez</keyname><forenames>Daniel</forenames></author><author><keyname>Martins</keyname><forenames>Andr&#xe9; F. T.</forenames></author></authors><title>Parsing as Reduction</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We reduce phrase-representation parsing to dependency parsing. Our reduction
is grounded on a new intermediate representation, &quot;head-ordered dependency
trees&quot;, shown to be isomorphic to constituent trees. By encoding order
information in the dependency labels, we show that any off-the-shelf, trainable
dependency parser can be used to produce constituents. When this parser is
non-projective, we can perform discontinuous parsing in a very natural manner.
Despite the simplicity of our approach, experiments show that the resulting
parsers are on par with strong baselines, such as the Berkeley parser for
English and the best single system in the SPMRL-2014 shared task. Results are
particularly striking for discontinuous parsing of German, where we surpass the
current state of the art by a wide margin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00034</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00034</id><created>2015-02-27</created><authors><author><keyname>Shankar</keyname><forenames>Varun</forenames></author><author><keyname>Olson</keyname><forenames>Sarah D.</forenames></author></authors><title>Radial Basis Function (RBF)-based Parametric Models for Closed and Open
  Curves within the Method of Regularized Stokeslets</title><categories>math.NA cs.NA q-bio.QM</categories><comments>23 pages, 11 figures, 1 table</comments><doi>10.1002/fld.4048</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The method of regularized Stokeslets (MRS) is a numerical approach using
regularized fundamental solutions to compute the flow due to an object in a
viscous fluid where inertial effects can be neglected. The elastic object is
represented as a Lagrangian structure, exerting point forces on the fluid. The
forces on the structure are often determined by a bending or tension model,
previously calculated using finite difference approximations. In this paper, we
study Spherical Basis Function (SBF), Radial Basis Function (RBF) and
Lagrange-Chebyshev parametric models to represent and calculate forces on
elastic structures that can be represented by an open curve, motivated by the
study of cilia and flagella. The evaluation error for static open curves for
the different interpolants, as well as errors for calculating normals and
second derivatives using different types of clustered parametric nodes, are
given for the case of an open planar curve. We determine that SBF and RBF
interpolants built on clustered nodes are competitive with Lagrange-Chebyshev
interpolants for modeling twice-differentiable open planar curves. We propose
using SBF and RBF parametric models within the MRS for evaluating and updating
the elastic structure. Results for open and closed elastic structures immersed
in a 2D fluid are presented, showing the efficacy of the RBF-Stokeslets method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00035</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00035</id><created>2015-02-27</created><authors><author><keyname>Kari</keyname><forenames>Lila</forenames></author><author><keyname>Konstantinidis</keyname><forenames>Stavros</forenames></author><author><keyname>Kopecki</keyname><forenames>Steffen</forenames></author></authors><title>Transducer Descriptions of DNA Code Properties and Undecidability of
  Antimorphic Problems</title><categories>cs.FL cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work concerns formal descriptions of DNA code properties, and builds on
previous work on transducer descriptions of classic code properties and on
trajectory descriptions of DNA code properties. This line of research allows us
to give a property as input to an algorithm, in addition to any regular
language, which can then answer questions about the language and the property.
Here we define DNA code properties via transducers and show that this method is
strictly more expressive than that of trajectories, without sacrificing the
efficiency of deciding the satisfaction question. We also show that the
maximality question can be undecidable. Our undecidability results hold not
only for the fixed DNA involution but also for any fixed antimorphic
permutation. Moreover, we also show the undecidability of the antimorphic
version of the Post Corresponding Problem, for any fixed antimorphic
permutation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00036</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00036</id><created>2015-02-27</created><updated>2015-04-14</updated><authors><author><keyname>Neyshabur</keyname><forenames>Behnam</forenames></author><author><keyname>Tomioka</keyname><forenames>Ryota</forenames></author><author><keyname>Srebro</keyname><forenames>Nathan</forenames></author></authors><title>Norm-Based Capacity Control in Neural Networks</title><categories>cs.LG cs.AI cs.NE stat.ML</categories><comments>29 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the capacity, convexity and characterization of a general
family of norm-constrained feed-forward networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00038</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00038</id><created>2015-02-27</created><authors><author><keyname>Siddiqui</keyname><forenames>Md Amran</forenames></author><author><keyname>Fern</keyname><forenames>Alan</forenames></author><author><keyname>Dietterich</keyname><forenames>Thomas G.</forenames></author><author><keyname>Wong</keyname><forenames>Weng-Keen</forenames></author></authors><title>Sequential Feature Explanations for Anomaly Detection</title><categories>cs.AI cs.LG stat.ML</categories><comments>9 pages, 4 figures and submitted to KDD 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many applications, an anomaly detection system presents the most anomalous
data instance to a human analyst, who then must determine whether the instance
is truly of interest (e.g. a threat in a security setting). Unfortunately, most
anomaly detectors provide no explanation about why an instance was considered
anomalous, leaving the analyst with no guidance about where to begin the
investigation. To address this issue, we study the problems of computing and
evaluating sequential feature explanations (SFEs) for anomaly detectors. An SFE
of an anomaly is a sequence of features, which are presented to the analyst one
at a time (in order) until the information contained in the highlighted
features is enough for the analyst to make a confident judgement about the
anomaly. Since analyst effort is related to the amount of information that they
consider in an investigation, an explanation's quality is related to the number
of features that must be revealed to attain confidence. One of our main
contributions is to present a novel framework for large scale quantitative
evaluations of SFEs, where the quality measure is based on analyst effort. To
do this we construct anomaly detection benchmarks from real data sets along
with artificial experts that can be simulated for evaluation. Our second
contribution is to evaluate several novel explanation approaches within the
framework and on traditional anomaly detection benchmarks, offering several
insights into the approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00040</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00040</id><created>2015-02-27</created><authors><author><keyname>Hegde</keyname><forenames>Chinmay</forenames></author><author><keyname>Tuzel</keyname><forenames>Oncel</forenames></author><author><keyname>Porikli</keyname><forenames>Fatih</forenames></author></authors><title>Efficient Upsampling of Natural Images</title><categories>cs.CV cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel method of efficient upsampling of a single natural image.
Current methods for image upsampling tend to produce high-resolution images
with either blurry salient edges, or loss of fine textural detail, or spurious
noise artifacts.
  In our method, we mitigate these effects by modeling the input image as a sum
of edge and detail layers, operating upon these layers separately, and merging
the upscaled results in an automatic fashion. We formulate the upsampled output
image as the solution to a non-convex energy minimization problem, and propose
an algorithm to obtain a tractable approximate solution. Our algorithm
comprises two main stages. 1) For the edge layer, we use a nonparametric
approach by constructing a dictionary of patches from a given image, and
synthesize edge regions in a higher-resolution version of the image. 2) For the
detail layer, we use a global parametric texture enhancement approach to
synthesize detail regions across the image.
  We demonstrate that our method is able to accurately reproduce sharp edges as
well as synthesize photorealistic textures, while avoiding common artifacts
such as ringing and haloing. In addition, our method involves no training phase
or estimation of model parameters, and is easily parallelizable. We demonstrate
the utility of our method on a number of challenging standard test photos.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00043</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00043</id><created>2015-02-27</created><authors><author><keyname>Bozzelli</keyname><forenames>Laura</forenames></author><author><keyname>Pearce</keyname><forenames>David</forenames></author></authors><title>On the complexity of Temporal Equilibrium Logic</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Temporal Equilibrium Logic (TEL) is a promising framework that extends the
knowledge representation and reasoning capabilities of Answer Set Programming
with temporal operators in the style of LTL. To our knowledge it is the first
nonmonotonic logic that accommodates fully the syntax of a standard temporal
logic (specifically LTL) without requiring further constructions. This paper
provides a systematic complexity analysis for the (consistency) problem of
checking the existence of a temporal equilibrium model of a TEL formula. It was
previously shown that this problem in the general case lies somewhere between
PSPACE and EXPSPACE. Here we establish a lower bound matching the known
EXPSPACE upper bound. Additionally we analyse the complexity for various
natural subclasses of TEL formulas, identifying both tractable and intractable
fragments. Finally the paper offers some new insights on the logic LTL by
addressing satisfiability for minimal LTL models. The complexity results
obtained highlight a substantial difference between interpreting LTL over
finite or infinite words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00049</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00049</id><created>2015-02-27</created><authors><author><keyname>Alatabbi</keyname><forenames>Ali</forenames></author><author><keyname>Iliopoulos</keyname><forenames>Costas S.</forenames></author><author><keyname>Langiu</keyname><forenames>Alessio</forenames></author><author><keyname>Rahman</keyname><forenames>M. Sohel</forenames></author></authors><title>Algorithms for Longest Common Abelian Factors</title><categories>cs.DS</categories><comments>13 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the problem of computing the longest common abelian
factor (LCAF) between two given strings. We present a simple $O(\sigma~ n^2)$
time algorithm, where $n$ is the length of the strings and $\sigma$ is the
alphabet size, and a sub-quadratic running time solution for the binary string
case, both having linear space requirement. Furthermore, we present a modified
algorithm applying some interesting tricks and experimentally show that the
resulting algorithm runs faster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00054</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00054</id><created>2015-02-27</created><authors><author><keyname>Liu</keyname><forenames>Lanchao</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author></authors><title>Multi-Block ADMM for Big Data Optimization in Smart Grid</title><categories>cs.SY cs.CE</categories><comments>6 pages, 1 figure, ICNC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we review the parallel and distributed optimization algorithms
based on alternating direction method of multipliers (ADMM) for solving &quot;big
data&quot; optimization problem in smart grid communication networks. We first
introduce the canonical formulation of the large-scale optimization problem.
Next, we describe the general form of ADMM and then focus on several direct
extensions and sophisticated modifications of ADMM from $2$-block to $N$-block
settings to deal with the optimization problem. The iterative schemes and
convergence properties of each extension/modification are given, and the
implementation on large-scale computing facilities is also illustrated.
Finally, we numerate several applications in power system for distributed
robust state estimation, network energy management and security constrained
optimal power flow problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00064</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00064</id><created>2015-02-27</created><authors><author><keyname>Lin</keyname><forenames>Dahua</forenames></author><author><keyname>Kong</keyname><forenames>Chen</forenames></author><author><keyname>Fidler</keyname><forenames>Sanja</forenames></author><author><keyname>Urtasun</keyname><forenames>Raquel</forenames></author></authors><title>Generating Multi-Sentence Lingual Descriptions of Indoor Scenes</title><categories>cs.CV cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel framework for generating lingual descriptions of
indoor scenes. Whereas substantial efforts have been made to tackle this
problem, previous approaches focusing primarily on generating a single sentence
for each image, which is not sufficient for describing complex scenes. We
attempt to go beyond this, by generating coherent descriptions with multiple
sentences. Our approach is distinguished from conventional ones in several
aspects: (1) a 3D visual parsing system that jointly infers objects,
attributes, and relations; (2) a generative grammar learned automatically from
training text; and (3) a text generation algorithm that takes into account the
coherence among sentences. Experiments on the augmented NYU-v2 dataset show
that our framework can generate natural descriptions with substantially higher
ROGUE scores compared to those produced by the baseline.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00067</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00067</id><created>2015-02-27</created><authors><author><keyname>Takaoka</keyname><forenames>Tadao</forenames></author></authors><title>O(1) Time Generation of Adjacent Multiset Combinations</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We solve the problem of designing an O(1) time algorithm for generating
adjacent multiset combinations in a different approach from Walsh. By the word
adjacent, we mean that two adjacent multiset combinations are different at two
places by one in their vector forms. Previous O(1) time algorithms for multiset
combinations generated non-adjacent multiset combinations. Our algorithm in
this paper can be derived from a general framework of combinatorial Gray code,
which we characterise to suit our need for combinations and multiset
combinations. The central idea is a twisted lexico tree, which is obtained from
the lexicographic tree for the given set of combinatorial objects by twisting
branches depending on the parity of each node. An iterative algorithm which
traverses this tree will generate the given set of combinatorial objects in
constant time as well as with a fixed number of changes from the present
combinatorial object to the next.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00071</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00071</id><created>2015-02-28</created><authors><author><keyname>Ahuja</keyname><forenames>Garima</forenames></author><author><keyname>Karlapalem</keyname><forenames>Kamalakar</forenames></author></authors><title>Crowd Congestion and Stampede Management through Multi Robotic Agents</title><categories>cs.MA</categories><comments>Extended version of paper &quot;Managing Multi Robotic Agents to Avoid
  Congestion and Stampedes&quot;, Garima Ahuja and Kamalakar Karlapalem, to appear
  in AAMAS 2015 as a short paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crowd management is a complex, challenging and crucial task. Lack of
appropriate management of crowd has, in past, led to many unfortunate stampedes
with significant loss of life. To increase the crowd management efficiency, we
deploy automated real time detection of stampede prone areas. Then, we use
robotic agents to aid the crowd management police in controlling the crowd in
these stampede prone areas. While doing so, we aim for minimum interference by
robotic agents in our environment. Thereby not disturbing the ambiance and
aesthetics of the place. We evaluate the effectiveness of our model in dealing
with difficult scenarios like emergency evacuation and presence of localized
congestion. Lastly, we simulate a multi agent system based on our model and use
it to illustrate the utility of robotic agents for detecting and reducing
congestion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00072</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00072</id><created>2015-02-28</created><authors><author><keyname>Li</keyname><forenames>Hanxi</forenames></author><author><keyname>Li</keyname><forenames>Yi</forenames></author><author><keyname>Porikli</keyname><forenames>Fatih</forenames></author></authors><title>DeepTrack: Learning Discriminative Feature Representations Online for
  Robust Visual Tracking</title><categories>cs.CV</categories><comments>12 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Deep neural networks, albeit their great success on feature learning in
various computer vision tasks, are usually considered as impractical for online
visual tracking because they require very long training time and a large number
of training samples. In this work, we present an efficient and very robust
tracking algorithm using a single Convolutional Neural Network (CNN) for
learning effective feature representations of the target object, in a purely
online manner. Our contributions are multifold: First, we introduce a novel
truncated structural loss function that maintains as many training samples as
possible and reduces the risk of tracking error accumulation. Second, we
enhance the ordinary Stochastic Gradient Descent approach in CNN training with
a robust sample selection mechanism. The sampling mechanism randomly generates
positive and negative samples from different temporal distributions, which are
generated by taking the temporal relations and label noise into account.
Finally, a lazy yet effective updating scheme is designed for CNN training.
Equipped with this novel updating algorithm, the CNN model is robust to some
long-existing difficulties in visual tracking such as occlusion or incorrect
detections, without loss of the effective adaption for significant appearance
changes. In the experiment, our CNN tracker outperforms all compared
state-of-the-art methods on two recently proposed benchmarks which in total
involve over 60 video sequences. The remarkable performance improvement over
the existing trackers illustrates the superiority of the feature
representations which are learned
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00075</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00075</id><created>2015-02-28</created><updated>2015-05-30</updated><authors><author><keyname>Tai</keyname><forenames>Kai Sheng</forenames></author><author><keyname>Socher</keyname><forenames>Richard</forenames></author><author><keyname>Manning</keyname><forenames>Christopher D.</forenames></author></authors><title>Improved Semantic Representations From Tree-Structured Long Short-Term
  Memory Networks</title><categories>cs.CL cs.AI cs.LG</categories><comments>Accepted for publication at ACL 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Because of their superior ability to preserve sequence information over time,
Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with
a more complex computational unit, have obtained strong results on a variety of
sequence modeling tasks. The only underlying LSTM structure that has been
explored so far is a linear chain. However, natural language exhibits syntactic
properties that would naturally combine words to phrases. We introduce the
Tree-LSTM, a generalization of LSTMs to tree-structured network topologies.
Tree-LSTMs outperform all existing systems and strong LSTM baselines on two
tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task
1) and sentiment classification (Stanford Sentiment Treebank).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00080</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00080</id><created>2015-02-28</created><authors><author><keyname>Guruswamy</keyname><forenames>Anand</forenames></author><author><keyname>Blum</keyname><forenames>Rick S.</forenames></author><author><keyname>Kishore</keyname><forenames>Shalinee</forenames></author><author><keyname>Bordogna</keyname><forenames>Mark</forenames></author></authors><title>Minimax Optimum Estimators for Phase Synchronization in IEEE 1588</title><categories>stat.AP cs.IT math.IT</categories><comments>11 pages, 19 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The IEEE 1588 protocol has received recent interest as a means of delivering
sub-microsecond level clock phase synchronization over packet-switched mobile
backhaul networks. Due to the randomness of the end-to-end delays in packet
networks, the recovery of clock phase from packet timestamps in IEEE 1588 must
be treated as a statistical estimation problem. A number of estimators for this
problem have been suggested in the literature, but little is known about the
best achievable performance. In this paper, we describe new minimax estimators
for this problem, that are optimum in terms of minimizing the maximum mean
squared error over all possible values of the unknown parameters. Minimax
estimators that utilize information from past timestamps to improve accuracy
are also introduced. Simulation results indicate that significant performance
gains over conventional estimators can be obtained via such optimum processing
techniques. These minimax estimators also provide fundamental limits on the
performance of phase offset estimation schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00081</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00081</id><created>2015-02-28</created><authors><author><keyname>Lin</keyname><forenames>Weiyao</forenames></author><author><keyname>Sun</keyname><forenames>Ming-Ting</forenames></author><author><keyname>Poovendran</keyname><forenames>Radha</forenames></author><author><keyname>Zhang</keyname><forenames>Zhengyou</forenames></author></authors><title>Activity Recognition Using A Combination of Category Components And
  Local Models for Video Surveillance</title><categories>cs.CV cs.MM</categories><comments>This manuscript is the accepted version for TCSVT (IEEE Transactions
  on Circuits and Systems for Video Technology)</comments><journal-ref>IEEE Trans. Circuits and Systems for Video Technology, vol. 18,
  pp. 1128-1139, 2008</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel approach for automatic recognition of human
activities for video surveillance applications. We propose to represent an
activity by a combination of category components, and demonstrate that this
approach offers flexibility to add new activities to the system and an ability
to deal with the problem of building models for activities lacking training
data. For improving the recognition accuracy, a Confident-Frame- based
Recognition algorithm is also proposed, where the video frames with high
confidence for recognizing an activity are used as a specialized local model to
help classify the remainder of the video frames. Experimental results show the
effectiveness of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00082</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00082</id><created>2015-02-28</created><authors><author><keyname>Lin</keyname><forenames>Weiyao</forenames></author><author><keyname>Sun</keyname><forenames>Ming-Ting</forenames></author><author><keyname>Poovendran</keyname><forenames>Radha</forenames></author><author><keyname>Zhang</keyname><forenames>Zhengyou</forenames></author></authors><title>Group Event Detection with a Varying Number of Group Members for Video
  Surveillance</title><categories>cs.CV cs.AI cs.MM</categories><comments>This manuscript is the accepted version for TCSVT (IEEE Transactions
  on Circuits and Systems for Video Technology)</comments><journal-ref>IEEE Trans. Circuits and Systems for Video Technology, vol. 20,
  no. 8, pp. 1057-1067, 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel approach for automatic recognition of group
activities for video surveillance applications. We propose to use a group
representative to handle the recognition with a varying number of group
members, and use an Asynchronous Hidden Markov Model (AHMM) to model the
relationship between people. Furthermore, we propose a group activity detection
algorithm which can handle both symmetric and asymmetric group activities, and
demonstrate that this approach enables the detection of hierarchical
interactions between people. Experimental results show the effectiveness of our
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00083</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00083</id><created>2015-02-28</created><authors><author><keyname>Lin</keyname><forenames>Weiyao</forenames></author><author><keyname>Panusopone</keyname><forenames>Krit</forenames></author><author><keyname>Baylon</keyname><forenames>David M.</forenames></author><author><keyname>Sun</keyname><forenames>Ming-Ting</forenames></author></authors><title>A Computation Control Motion Estimation Method for Complexity-Scalable
  Video Coding</title><categories>cs.MM</categories><comments>This manuscript is the accepted version for TCSVT (IEEE Transactions
  on Circuits and Systems for Video Technology)</comments><journal-ref>IEEE Trans. Circuits and Systems for Video Technology, vol. 20,
  no. 11, pp. 1533-1543, 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new Computation-Control Motion Estimation (CCME) method is
proposed which can perform Motion Estimation (ME) adaptively under different
computation or power budgets while keeping high coding performance. We first
propose a new class-based method to measure the Macroblock (MB) importance
where MBs are classified into different classes and their importance is
measured by combining their class information as well as their initial matching
cost information. Based on the new MB importance measure, a complete CCME
framework is then proposed to allocate computation for ME. The proposed method
performs ME in a one-pass flow. Experimental results demonstrate that the
proposed method can allocate computation more accurately than previous methods
and thus has better performance under the same computation budget.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00085</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00085</id><created>2015-02-28</created><authors><author><keyname>Lin</keyname><forenames>Weiyao</forenames></author><author><keyname>Panusopone</keyname><forenames>Krit</forenames></author><author><keyname>Baylon</keyname><forenames>David M.</forenames></author><author><keyname>Sun</keyname><forenames>Ming-Ting</forenames></author><author><keyname>Chen</keyname><forenames>Zhenzhong</forenames></author><author><keyname>Li</keyname><forenames>Hongxiang</forenames></author></authors><title>A Fast Sub-Pixel Motion Estimation Algorithm for H.264/AVC Video Coding</title><categories>cs.MM</categories><comments>This manuscript is the accepted version for TCSVT (IEEE Transactions
  on Circuits and Systems for Video Technology)</comments><journal-ref>IEEE Trans. Circuits and Systems for Video Technology, vol. 21,
  no. 2, pp. 237-242, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motion Estimation (ME) is one of the most time-consuming parts in video
coding. The use of multiple partition sizes in H.264/AVC makes it even more
complicated when compared to ME in conventional video coding standards. It is
important to develop fast and effective sub-pixel ME algorithms since (a) The
computation overhead by sub-pixel ME has become relatively significant while
the complexity of integer-pixel search has been greatly reduced by fast
algorithms, and (b) Reducing sub-pixel search points can greatly save the
computation for sub-pixel interpolation. In this paper, a novel fast sub-pixel
ME algorithm is proposed which performs a 'rough' sub-pixel search before the
partition selection, and performs a 'precise' sub-pixel search for the best
partition. By reducing the searching load for the large number of non-best
partitions, the computation complexity for sub-pixel search can be greatly
decreased. Experimental results show that our method can reduce the sub-pixel
search points by more than 50% compared to existing fast sub-pixel ME methods
with negligible quality degradation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00087</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00087</id><created>2015-02-28</created><authors><author><keyname>Lin</keyname><forenames>Weiyao</forenames></author><author><keyname>Sun</keyname><forenames>Ming-Ting</forenames></author><author><keyname>Li</keyname><forenames>Hongxiang</forenames></author><author><keyname>Chen</keyname><forenames>Zhenzhong</forenames></author><author><keyname>Li</keyname><forenames>Wei</forenames></author><author><keyname>Zhou</keyname><forenames>Bing</forenames></author></authors><title>Macroblock Classification Method for Video Applications Involving
  Motions</title><categories>cs.MM cs.CV</categories><comments>This manuscript is the accepted version for TB (IEEE Transactions on
  Broadcasting)</comments><journal-ref>IEEE Trans. Broadcasting, vol. 58, no. 1, pp. 34-46, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a macroblock classification method is proposed for various
video processing applications involving motions. Based on the analysis of the
Motion Vector field in the compressed video, we propose to classify Macroblocks
of each video frame into different classes and use this class information to
describe the frame content. We demonstrate that this low-computation-complexity
method can efficiently catch the characteristics of the frame. Based on the
proposed macroblock classification, we further propose algorithms for different
video processing applications, including shot change detection, motion
discontinuity detection, and outlier rejection for global motion estimation.
Experimental results demonstrate that the methods based on the proposed
approach can work effectively on these applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00088</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00088</id><created>2015-02-28</created><authors><author><keyname>Zhang</keyname><forenames>Yihao</forenames></author><author><keyname>Lin</keyname><forenames>Weiyao</forenames></author><author><keyname>Zhou</keyname><forenames>Bing</forenames></author><author><keyname>Chen</keyname><forenames>Zhenzhong</forenames></author><author><keyname>Sheng</keyname><forenames>Bin</forenames></author><author><keyname>Wu</keyname><forenames>Jianxin</forenames></author><author><keyname>Zhang</keyname><forenames>Wenjun</forenames></author></authors><title>Facial Expression Cloning with Elastic and Muscle Models</title><categories>cs.GR cs.MM</categories><comments>This manuscript is the accepted version for JVCI (Journal of Visual
  Communication and Image Representation)</comments><journal-ref>Journal of Visual Communication and Image Representation, vol. 25,
  no. 5, pp. 916-927, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Expression cloning plays an important role in facial expression synthesis. In
this paper, a novel algorithm is proposed for facial expression cloning. The
proposed algorithm first introduces a new elastic model to balance the global
and local warping effects, such that the impacts from facial feature diversity
among people can be minimized, and thus more effective geometric warping
results can be achieved. Furthermore, a muscle-distribution-based (MD) model is
proposed, which utilizes the muscle distribution of the human face and results
in more accurate facial illumination details. In addition, we also propose a
new distance-based metric to automatically select the optimal parameters such
that the global and local warping effects in the elastic model can be suitably
balanced. Experimental results show that our proposed algorithm outperforms the
existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00090</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00090</id><created>2015-02-28</created><authors><author><keyname>Zhang</keyname><forenames>Chongyang</forenames></author><author><keyname>Lin</keyname><forenames>Weiyao</forenames></author><author><keyname>Li</keyname><forenames>Wei</forenames></author><author><keyname>Zhou</keyname><forenames>Bing</forenames></author><author><keyname>Xie</keyname><forenames>Jun</forenames></author><author><keyname>Li</keyname><forenames>Jijia</forenames></author></authors><title>Improved Image Deblurring based on Salient-region Segmentation</title><categories>cs.CV</categories><comments>This manuscript is the accepted version for Image Comm (Signal
  Processing: Image Communication)</comments><journal-ref>Signal Processing: Image Communication, vol. 28, no. 9, pp.
  1171-1186, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image deblurring techniques play important roles in many image processing
applications. As the blur varies spatially across the image plane, it calls for
robust and effective methods to deal with the spatially-variant blur problem.
In this paper, a Saliency-based Deblurring (SD) approach is proposed based on
the saliency detection for salient-region segmentation and a corresponding
compensate method for image deblurring. We also propose a PDE-based deblurring
method which introduces an anisotropic Partial Differential Equation (PDE)
model for latent image prediction and employs an adaptive optimization model in
the kernel estimation and deconvolution steps. Experimental results demonstrate
the effectiveness of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00091</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00091</id><created>2015-02-28</created><authors><author><keyname>Brandst&#xe4;dt</keyname><forenames>Andreas</forenames></author><author><keyname>Eschen</keyname><forenames>Elaine M.</forenames></author><author><keyname>Friese</keyname><forenames>Erik</forenames></author></authors><title>Efficient Domination for Some Subclasses of $P_6$-Free Graphs in
  Polynomial Time</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G$ be a finite undirected graph. A vertex {\em dominates} itself and all
its neighbors in $G$. A vertex set $D$ is an {\em efficient dominating set}
(\emph{e.d.}\ for short) of $G$ if every vertex of $G$ is dominated by exactly
one vertex of $D$. The \emph{Efficient Domination} (ED) problem, which asks for
the existence of an e.d.\ in $G$, is known to be \NP-complete even for very
restricted graph classes such as $P_7$-free chordal graphs. The ED problem on a
graph $G$ can be reduced to the Maximum Weight Independent Set (MWIS) problem
on the square of $G$. The complexity of the ED problem is an open question for
$P_6$-free graphs and was open even for the subclass of $P_6$-free chordal
graphs. In this paper, we show that squares of $P_6$-free chordal graphs that
have an e.d. are chordal; this even holds for the larger class of ($P_6$,
house, hole, domino)-free graphs. This implies that ED/WeightedED is solvable
in polynomial time for ($P_6$, house, hole, domino)-free graphs; in particular,
for $P_6$-free chordal graphs. Moreover, based on our result that squares of
$P_6$-free graphs that have an e.d. are hole-free and some properties
concerning odd antiholes, we show that squares of ($P_6$, house)-free graphs
(($P_6$, bull)-free graphs, respectively) that have an e.d. are perfect. This
implies that ED/WeightedED is solvable in polynomial time for ($P_6$,
house)-free graphs and for ($P_6$, bull)-free graphs (the time bound for
($P_6$, house, hole, domino)-free graphs is better than that for ($P_6$,
house)-free graphs). The complexity of the ED problem for $P_6$-free graphs
remains an open question.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00094</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00094</id><created>2015-02-28</created><authors><author><keyname>Liu</keyname><forenames>Jingwei</forenames></author><author><keyname>Liu</keyname><forenames>Yi</forenames></author><author><keyname>Xu</keyname><forenames>Meizhi</forenames></author></authors><title>Parameter Estimation of Jelinski-Moranda Model Based on Weighted
  Nonlinear Least Squares and Heteroscedasticity</title><categories>cs.OH</categories><comments>17 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parameter estimation method of Jelinski-Moranda (JM) model based on weighted
nonlinear least squares (WNLS) is proposed. The formulae of resolving the
parameter WNLS estimation (WNLSE) are derived, and the empirical weight
function and heteroscedasticity problem are discussed. The effects of
optimization parameter estimation selection based on maximum likelihood
estimation (MLE) method, least squares estimation (LSE) method and weighted
nonlinear least squares estimation (WNLSE) method are also investigated. Two
strategies of heteroscedasticity decision and weighting methods embedded in JM
model prediction process are also investigated. The experimental results on
standard software reliability analysis database-Naval Tactical Data System
(NTDS) and three datasets used by J.D. Musa demonstrate that WNLSE method can
be superior to LSE and MLE under the relative error (RE) criterion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00095</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00095</id><created>2015-02-28</created><updated>2015-06-22</updated><authors><author><keyname>Hashimoto</keyname><forenames>Kazuma</forenames></author><author><keyname>Stenetorp</keyname><forenames>Pontus</forenames></author><author><keyname>Miwa</keyname><forenames>Makoto</forenames></author><author><keyname>Tsuruoka</keyname><forenames>Yoshimasa</forenames></author></authors><title>Task-Oriented Learning of Word Embeddings for Semantic Relation
  Classification</title><categories>cs.CL</categories><comments>The Nineteenth Conference on Computational Natural Language Learning
  (CoNLL 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel learning method for word embeddings designed for relation
classification. Our word embeddings are trained by predicting words between
noun pairs using lexical relation-specific features on a large unlabeled
corpus. This allows us to explicitly incorporate relation-specific information
into the word embeddings. The learned word embeddings are then used to
construct feature vectors for a relation classification model. On a
well-established semantic relation classification task, our method
significantly outperforms a baseline based on a previously introduced word
embedding method, and compares favorably to previous state-of-the-art models
that use syntactic information or manually constructed external resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00100</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00100</id><created>2015-02-28</created><authors><author><keyname>Delavar</keyname><forenames>Rahim</forenames></author><author><keyname>Tavassoli</keyname><forenames>Babak</forenames></author><author><keyname>Beheshti</keyname><forenames>Mohammad Taghi Hamidi</forenames></author></authors><title>Improved Stability Analysis of Nonlinear Networked Control Systems over
  Multiple Communication Links</title><categories>cs.SY</categories><comments>Submission to 14th annual European Control Conference (ECC 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a nonlinear networked control system (NCS) in
which controllers, sensors and actuators are connected via several
communication links. In each link, networking effects such as the transmission
delay, packet loss, sampling jitter and data packet miss-ordering are captured
by time-varying delays. Stability analysis is carried out based on the Lyapunov
Krasovskii method to obtain a condition for stability of the nonlinear NCS in
the form of linear matrix inequality (LMI). The results are applied to a two
degrees of freedom robot arm NCS which shows a considerable improvement with
respect to the previous works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00102</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00102</id><created>2015-02-28</created><authors><author><keyname>Zhu</keyname><forenames>Jieming</forenames></author><author><keyname>Zheng</keyname><forenames>Zibin</forenames></author><author><keyname>Lyu</keyname><forenames>Michael R.</forenames></author></authors><title>Context-Aware Reliability Prediction of Black-Box Services</title><categories>cs.SE</categories><comments>Technical report, 6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reliability prediction is an important research problem in software
reliability engineering, which has been widely studied in the last decades.
However, modelling and predicting user-perceived reliability of black-box
services remain an open problem. Software services, such as Web services and
Web APIs, generally provide black-box functionalities to users through the
Internet, leading to a lack of their internal information for reliability
analysis. Furthermore, the user-perceived service reliability depends not only
on the service itself, but also heavily on the invocation context (e.g.,
service workloads, network conditions), whereby traditional reliability models
become ineffective and inappropriate. To address these new challenges posed by
black-box services, in this paper, we propose CARP, a context-aware reliability
prediction approach that leverages historical usage data from users for
reliability prediction. Through context-aware model construction and
prediction, CARP is able to alleviate the data sparsity problem that heavily
limits the prediction accuracy of other existing approaches. The preliminary
evaluation results show that CARP can make significant improvement on
reliability prediction accuracy, e.g., 41% for MAE and 38% for RMSE when only
5% of data are available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00107</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00107</id><created>2015-02-28</created><authors><author><keyname>Huang</keyname><forenames>Shujian</forenames></author><author><keyname>Chen</keyname><forenames>Huadong</forenames></author><author><keyname>Dai</keyname><forenames>Xinyu</forenames></author><author><keyname>Chen</keyname><forenames>Jiajun</forenames></author></authors><title>Non-linear Learning for Statistical Machine Translation</title><categories>cs.CL cs.NE</categories><comments>submitted to a conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern statistical machine translation (SMT) systems usually use a linear
combination of features to model the quality of each translation hypothesis.
The linear combination assumes that all the features are in a linear
relationship and constrains that each feature interacts with the rest features
in an linear manner, which might limit the expressive power of the model and
lead to a under-fit model on the current data. In this paper, we propose a
non-linear modeling for the quality of translation hypotheses based on neural
networks, which allows more complex interaction between features. A learning
framework is presented for training the non-linear models. We also discuss
possible heuristics in designing the network structure which may improve the
non-linear learning performance. Experimental results show that with the basic
features of a hierarchical phrase-based machine translation system, our method
produce translations that are better than a linear model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00118</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00118</id><created>2015-02-28</created><authors><author><keyname>Chen</keyname><forenames>Mingliang</forenames></author><author><keyname>Lin</keyname><forenames>Weiyao</forenames></author><author><keyname>Zheng</keyname><forenames>Xiaozhen</forenames></author></authors><title>An Efficient Coding Method for Coding Region-of-Interest Locations in
  AVS2</title><categories>cs.MM</categories><comments>This manuscript is the accepted version for ICMEW (IEEE Intl. Conf.
  Multimedia &amp; Expo Workshop), IEEE Intl. Conf. Multimedia &amp; Expo Workshop
  (ICME), 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Region-of-Interest (ROI) location information in videos has many practical
usages in video coding field, such as video content analysis and user
experience improvement. Although ROI-based coding has been studied widely by
many researchers to improve coding efficiency for video contents, the ROI
location information itself is seldom coded in video bitstream. In this paper,
we will introduce our proposed ROI location coding tool which has been adopted
in surveillance profile of AVS2 video coding standard (surveillance profile).
Our tool includes three schemes: direct-coding scheme, differential- coding
scheme, and reconstructed-coding scheme. We will illustrate the details of
these schemes, and perform analysis of their advantages and disadvantages,
respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00121</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00121</id><created>2015-02-28</created><authors><author><keyname>Hu</keyname><forenames>Hai-Miao</forenames></author><author><keyname>Li</keyname><forenames>Bo</forenames></author><author><keyname>Lin</keyname><forenames>Weiyao</forenames></author><author><keyname>Li</keyname><forenames>Wei</forenames></author><author><keyname>Sun</keyname><forenames>Ming-Ting</forenames></author></authors><title>Region-Based Rate-Control for H.264/AVC for Low Bit-Rate Applications</title><categories>cs.MM</categories><comments>This manuscript is the accepted version for TCSVT (IEEE Transactions
  on Circuits and Systems for Video Technology)</comments><journal-ref>IEEE Trans. Circuits and Systems for Video Technology, vol. 22,
  no. 11, pp. 1564-1576, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rate-control plays an important role in video coding. However, in the
conventional rate-control algorithms, the number and position of Macroblocks
(MBs) inside one basic unit for rate-control is inflexible and predetermined.
The different characteristics of the MBs are not fully considered. Also, there
is no overall optimization of the coding of basic units. This paper proposes a
new region-based rate-control scheme for H.264/AVC to improve the coding
efficiency. The inter-frame information is explored to objectively divide one
frame into multiple regions based on their rate-distortion behaviors. The MBs
with the similar characteristics are classified into the same region, and the
entire region instead of a single MB or a group of contiguous MBs is treated as
a basic unit for rate-control. A linear rate-quantization stepsize model and a
linear distortion-quantization stepsize model are proposed to accurately
describe the rate-distortion characteristics for the region-based basic units.
Moreover, based on the above linear models, an overall optimization model is
proposed to obtain suitable Quantization Parameters (QPs) for the region-based
basic units. Experimental results demonstrate that the proposed region-based
rate-control approach can achieve both better subjective and objective quality
by performing the rate-control adaptively with the content, compared to the
conventional rate-control approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00138</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00138</id><created>2015-02-28</created><updated>2015-03-20</updated><authors><author><keyname>Basu</keyname><forenames>Saugata</forenames></author><author><keyname>Riener</keyname><forenames>Cordian</forenames></author></authors><title>On the isotypic decomposition of cohomology modules of symmetric
  semi-algebraic sets: polynomial bounds on multiplicities</title><categories>math.AG cs.CC math.AT math.CO</categories><comments>38 pages, 1 figure. Minor additions and corrections from last
  version. The last section has been expanded</comments><msc-class>Primary 14P10, 14P25, Secondary 68W30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider symmetric (as well as multi-symmetric) real algebraic varieties
and semi-algebraic sets, as well as symmetric complex varieties in affine and
projective spaces, defined by polynomials of fixed degrees. We give polynomial
(in the dimension of the ambient space) bounds on the number of irreducible
representations of the symmetric group which acts on these sets, as well as
their multiplicities, appearing in the isotypic decomposition of their
cohomology modules with coefficients in a field of characteristic $0$. We also
give some applications of our methods in proving lower bounds on the degrees of
defining polynomials of certain symmetric semi-algebraic sets, as well as
better bounds on the Betti numbers of the images under projections of (not
necessarily symmetric) bounded real algebraic sets.
  Finally, we conjecture that the multiplicities of the irreducible
representations of the symmetric group in the cohomology modules of symmetric
semi-algebraic sets are computable with polynomial complexity, which would
imply that the Betti numbers of such sets are also computable with polynomial
complexity. This is in contrast with general semi-algebraic sets, for which
this problem is provably hard ($\#\mathbf{P}$-hard). We also formulate a
question asking whether these multiplicities can be expressed as a polynomial
in the number $k$ of variables for all large enough $k$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00140</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00140</id><created>2015-02-28</created><authors><author><keyname>Bonomi</keyname><forenames>Silvia</forenames></author><author><keyname>Dolev</keyname><forenames>Shlomi</forenames></author><author><keyname>Potop-Butucaru</keyname><forenames>Maria</forenames></author><author><keyname>Raynal</keyname><forenames>Michel</forenames></author></authors><title>Stabilizing Server-Based Storage in Byzantine Asynchronous
  Message-Passing Systems</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A stabilizing Byzantine single-writer single-reader (SWSR) regular register,
which stabilizes after the first invoked write operation, is first presented.
Then, new/old ordering inversions are eliminated by the use of a (bounded)
sequence number for writes, obtaining a practically stabilizing SWSR atomic
register. A practically stabilizing Byzantine single-writer multi-reader (SWMR)
atomic register is then obtained by using several copies of SWSR atomic
registers. Finally, bounded time-stamps, with a time-stamp per writer, together
with SWMR atomic registers, are used to construct a practically stabilizing
Byzantine multi-writer multi-reader (MWMR) atomic register. In a system of $n$
servers implementing an atomic register, and in addition to transient failures,
the constructions tolerate t&lt;n/8 Byzantine servers if communication is
asynchronous, and t&lt;n/3 Byzantine servers if it is synchronous. The noteworthy
feature of the proposed algorithms is that (to our knowledge) these are the
first that build an atomic read/write storage on top of asynchronous servers
prone to transient failures, and where up to t of them can be Byzantine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00141</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00141</id><created>2015-02-28</created><updated>2015-03-10</updated><authors><author><keyname>Weed</keyname><forenames>Jonathan</forenames></author></authors><title>Multinational War is Hard</title><categories>cs.CC</categories><comments>10 pages [fixed references]</comments><acm-class>G.2; F.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we show that the problem of determining whether one player can
force a win in a multiplayer version of the children's card game War is
PSPACE-hard. The same reduction shows that a related problem, asking whether a
player can survive k rounds, is PSPACE-complete when k is polynomial in the
size of the input.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00145</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00145</id><created>2015-02-28</created><authors><author><keyname>Yang</keyname><forenames>Han-Xin</forenames></author><author><keyname>Wu</keyname><forenames>Zhi-Xi</forenames></author></authors><title>Suppressing traffic-driven epidemic spreading by use of the efficient
  routing protocol</title><categories>physics.soc-ph cs.SI</categories><journal-ref>J. Stat. Mech. P03018 (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite extensive work on the interplay between traffic dynamics and epidemic
spreading, the control of epidemic spreading by routing strategies has not
received adequate attention. In this paper, we study the impact of efficient
routing protocol on epidemic spreading. In the case of infinite node-delivery
capacity, where the traffic is free of congestion, we find that that there
exists optimal values of routing parameter, leading to the maximal epidemic
threshold. This means that epidemic spreading can be effectively controlled by
fine tuning the routing scheme. Moreover, we find that an increase in the
average network connectivity and the emergence of traffic congestion can
suppress the epidemic outbreak.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00146</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00146</id><created>2015-02-28</created><authors><author><keyname>Yang</keyname><forenames>Han-Xin</forenames></author><author><keyname>Wang</keyname><forenames>Bing-Hong</forenames></author></authors><title>Disassortative mixing accelerates consensus in the naming game</title><categories>physics.soc-ph cs.SI</categories><journal-ref>J. Stat. Mech. P01009 (2015)</journal-ref><doi>10.1088/1742-5468/2015/01/P01009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the role of degree mixing in the naming game. It is
found that consensus can be accelerated on disassortative networks. We provide
a qualitative explanation of this phenomenon based on clusters statistics.
Compared with assortative mixing, disassortative mixing can promote the merging
of different clusters, thus resulting in a shorter convergence time. Other
quantities, including the evolutions of the success rate, the number of total
words and the number of different words, are also studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00149</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00149</id><created>2015-02-28</created><authors><author><keyname>Yang</keyname><forenames>Han-Xin</forenames></author><author><keyname>Wu</keyname><forenames>Zhi-Xi</forenames></author><author><keyname>Wang</keyname><forenames>Bing-Hong</forenames></author></authors><title>Suppressing traffic-driven epidemic spreading by edge-removal strategies</title><categories>physics.soc-ph cs.SI</categories><journal-ref>Phys. Rev. E 87, 064801 (2013)</journal-ref><doi>10.1103/PhysRevE.87.064801</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The interplay between traffic dynamics and epidemic spreading on complex
networks has received increasing attention in recent years. However, the
control of traffic-driven epidemic spreading remains to be a challenging
problem. In this Brief Report, we propose a method to suppress traffic-driven
epidemic outbreak by properly removing some edges in a network. We find that
the epidemic threshold can be enhanced by the targeted cutting of links among
large-degree nodes or edges with the largest algorithmic betweeness. In
contrast, the epidemic threshold will be reduced by the random edge removal.
These findings are robust with respect to traffic-flow conditions, network
structures and routing strategies. Moreover, we find that the shutdown of
targeted edges can effectively release traffic load passing through
large-degree nodes, rendering a relatively low probability of infection to
these nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00158</identifier>
 <datestamp>2015-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00158</id><created>2015-02-28</created><updated>2015-11-17</updated><authors><author><keyname>Freund</keyname><forenames>Daniel</forenames></author><author><keyname>Poloczek</keyname><forenames>Matthias</forenames></author><author><keyname>Reichman</keyname><forenames>Daniel</forenames></author></authors><title>Contagious Sets in Dense Graphs</title><categories>cs.DM math.CO</categories><comments>Extended version of the IWOCA'15 paper that generalizes the results
  on the minimum degree condition and the speed of the activation process to
  arbitrary values for the threshold parameter r</comments><msc-class>05C35, 68R10</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the activation process in undirected graphs known as bootstrap
percolation: a vertex is active either if it belongs to a set of initially
activated vertices or if at some point it had at least r active neighbors, for
a threshold r that is identical for all vertices. A contagious set is a vertex
set whose activation results with the entire graph being active. Let m(G,r) be
the size of a smallest contagious set in a graph G on n vertices.
  We examine density conditions that ensure m(G,r) = r for all r &gt;= 2. With
respect to the minimum degree, we prove that such a smallest possible
contagious set is guaranteed to exist if and only if G has minimum degree at
least (k-1)/k * n. Moreover, we study the speed with which the activation
spreads and provide tight upper bounds on the number of rounds it takes until
all nodes are activated in such a graph.
  We also investigate what average degree asserts the existence of small
contagious sets. For n &gt;= k &gt;= r, we denote by M(n,k,r) the maximum number of
edges in an n-vertex graph G satisfying m(G,r)&gt;k. We determine the precise
value of M(n,k,2) and M(n,k,k), assuming that n is sufficiently large compared
to k.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00164</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00164</id><created>2015-02-28</created><authors><author><keyname>Osting</keyname><forenames>Braxton</forenames></author><author><keyname>Xiong</keyname><forenames>Jiechao</forenames></author><author><keyname>Xu</keyname><forenames>Qianqian</forenames></author><author><keyname>Yao</keyname><forenames>Yuan</forenames></author></authors><title>Analysis of Crowdsourced Sampling Strategies for HodgeRank with Sparse
  Random Graphs</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crowdsourcing platforms are now extensively used for conducting subjective
pairwise comparison studies. In this setting, a pairwise comparison dataset is
typically gathered via random sampling, either \emph{with} or \emph{without}
replacement. In this paper, we use tools from random graph theory to analyze
these two random sampling methods for the HodgeRank estimator. Using the
Fiedler value of the graph as a measurement for estimator stability
(informativeness), we provide a new estimate of the Fiedler value for these two
random graph models. In the asymptotic limit as the number of vertices tends to
infinity, we prove the validity of the estimate. Based on our findings, for a
small number of items to be compared, we recommend a two-stage sampling
strategy where a greedy sampling method is used initially and random sampling
\emph{without} replacement is used in the second stage. When a large number of
items is to be compared, we recommend random sampling with replacement as this
is computationally inexpensive and trivially parallelizable. Experiments on
synthetic and real-world datasets support our analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00168</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00168</id><created>2015-02-28</created><authors><author><keyname>Li</keyname><forenames>Jiwei</forenames></author><author><keyname>Hovy</keyname><forenames>Eduard</forenames></author></authors><title>The NLP Engine: A Universal Turing Machine for NLP</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is commonly accepted that machine translation is a more complex task than
part of speech tagging. But how much more complex? In this paper we make an
attempt to develop a general framework and methodology for computing the
informational and/or processing complexity of NLP applications and tasks. We
define a universal framework akin to a Turning Machine that attempts to fit
(most) NLP tasks into one paradigm. We calculate the complexities of various
NLP tasks using measures of Shannon Entropy, and compare `simple' ones such as
part of speech tagging to `complex' ones such as machine translation. This
paper provides a first, though far from perfect, attempt to quantify NLP tasks
under a uniform paradigm. We point out current deficiencies and suggest some
avenues for fruitful research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00173</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00173</id><created>2015-02-28</created><authors><author><keyname>Mei</keyname><forenames>Jonathan</forenames></author><author><keyname>Moura</keyname><forenames>Jos&#xe9; M. F.</forenames></author></authors><title>Signal Processing on Graphs: Modeling (Causal) Relations in Big Data</title><categories>cs.IT math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many big data applications collect a large number of time series, for
example, the financial data of companies quoted in a stock exchange, the health
care data of all patients that visit the emergency room of a hospital, or the
temperature sequences continuously measured by weather stations across the US.
A first task in the analytics of these data is to derive a low dimensional
representation, a graph or discrete manifold, that describes well the
interrelations among the time series and their intrarelations across time. This
paper presents a computationally tractable algorithm for estimating this graph
structure from the available data. This graph is directed and weighted,
possibly representing causation relations, not just correlations as in most
existing approaches in the literature. The algorithm is demonstrated on random
graph and real network time series datasets, and its performance is compared to
that of related methods. The adjacency matrices estimated with the new method
are close to the true graph in the simulated data and consistent with prior
physical knowledge in the real dataset tested.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00174</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00174</id><created>2015-02-28</created><authors><author><keyname>Haines</keyname><forenames>Julia Katherine</forenames></author></authors><title>CSCW Principles to Support Citizen Science</title><categories>cs.CY cs.HC</categories><comments>Proceedings of the 5th International Conference on Collaborative
  Innovation Networks COINs15, Tokyo, Japan March 12-14, 2015
  (arXiv:1502.01142)</comments><report-no>coins15/2015/33</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Citizen science changes the way scientific research is pursued. It opens up
data collection and analysis to the general public, to the wisdom of crowds. In
this emerging area, there is much research to be done to better understand how
we can develop citizen science infrastructure and continue the democratization
of science. In creating such systems, there is much we can learn from
principles that have emerged out of computer-supported cooperative work (CSCW)
research. In this paper, I use a nine-step framework to highlight where CSCW
knowledge can contribute.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00184</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00184</id><created>2015-02-28</created><updated>2015-09-12</updated><authors><author><keyname>Liu</keyname><forenames>Yu</forenames></author><author><keyname>Feng</keyname><forenames>Jianghua</forenames></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames></author><author><keyname>Tang</keyname><forenames>Jun</forenames></author><author><keyname>Wen</keyname><forenames>Zheng</forenames></author><author><keyname>Haimovich</keyname><forenames>Alexander M.</forenames></author><author><keyname>Zhou</keyname><forenames>MengChu</forenames></author></authors><title>Topology Discovery for Linear Wireless Networks with Application to
  Train Backbone Inauguration</title><categories>cs.NI</categories><comments>Submitted to IEEE Transactions on Intelligent Transportation Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A train backbone network consists of a sequence of nodes arranged in a linear
topology. A key step that enables communication in such a network is that of
topology discovery, or train inauguration, whereby nodes learn in a distributed
fashion the physical topology of the backbone network. While the current
standard for train inauguration assumes wired links between adjacent backbone
nodes, this work investigates the more challenging scenario in which the nodes
communicate wirelessly. The implementation of topology discovery over wireless
channels is made difficult by the broadcast nature of the wireless medium, and
by fading and interference. A novel topology discovery protocol is proposed
that overcomes these issues and requires relatively minor changes to the wired
standard. The protocol is shown via analysis and numerical results to be robust
to the impairments caused by the wireless channel including interference from
other trains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00185</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00185</id><created>2015-02-28</created><updated>2015-08-18</updated><authors><author><keyname>Li</keyname><forenames>Jiwei</forenames></author><author><keyname>Luong</keyname><forenames>Minh-Thang</forenames></author><author><keyname>Jurafsky</keyname><forenames>Dan</forenames></author><author><keyname>Hovy</keyname><forenames>Eudard</forenames></author></authors><title>When Are Tree Structures Necessary for Deep Learning of Representations?</title><categories>cs.AI cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recursive neural models, which use syntactic parse trees to recursively
generate representations bottom-up, are a popular architecture. But there have
not been rigorous evaluations showing for exactly which tasks this syntax-based
method is appropriate. In this paper we benchmark {\bf recursive} neural models
against sequential {\bf recurrent} neural models (simple recurrent and LSTM
models), enforcing apples-to-apples comparison as much as possible. We
investigate 4 tasks: (1) sentiment classification at the sentence level and
phrase level; (2) matching questions to answer-phrases; (3) discourse parsing;
(4) semantic relation extraction (e.g., {\em component-whole} between nouns).
  Our goal is to understand better when, and why, recursive models can
outperform simpler models. We find that recursive models help mainly on tasks
(like semantic relation extraction) that require associating headwords across a
long distance, particularly on very long sequences. We then introduce a method
for allowing recurrent models to achieve similar performance: breaking long
sentences into clause-like units at punctuation and processing them separately
before combining. Our results thus help understand the limitations of both
classes of models, and suggest directions for improving recurrent models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00190</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00190</id><created>2015-02-28</created><updated>2016-03-02</updated><authors><author><keyname>Grohe</keyname><forenames>Martin</forenames></author><author><keyname>Schweitzer</keyname><forenames>Pascal</forenames></author></authors><title>Computing with Tangles</title><categories>cs.DM cs.DS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tangles of graphs have been introduced by Robertson and Seymour in the
context of their graph minor theory. Tangles may be viewed as describing
&quot;k-connected components&quot; of a graph (though in a twisted way). They play an
important role in graph minor theory. An interesting aspect of tangles is that
they cannot only be defined for graphs, but more generally for arbitrary
connectivity functions (that is, integer-valued submodular and symmetric set
functions).
  However, tangles are difficult to deal with algorithmically. To start with,
it is unclear how to represent them, because they are families of separations
and as such may be exponentially large. Our first contribution is a data
structure for representing and accessing all tangles of a graph up to some
fixed order.
  Using this data structure, we can prove an algorithmic version of a very
general structure theorem due to Carmesin, Diestel, Harman and Hundertmark (for
graphs) and Hundertmark (for arbitrary connectivity functions) that yields a
canonical tree decomposition whose parts correspond to the maximal tangles.
(This may be viewed as a generalisation of the decomposition of a graph into
its 3-connected components.)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00193</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00193</id><created>2015-02-28</created><updated>2015-03-29</updated><authors><author><keyname>Leike</keyname><forenames>Jan</forenames><affiliation>The Australian National University</affiliation></author><author><keyname>Heizmann</keyname><forenames>Matthias</forenames><affiliation>University of Freiburg</affiliation></author></authors><title>Ranking Templates for Linear Loops</title><categories>cs.LO</categories><proxy>Logical Methods In Computer Science</proxy><journal-ref>Logical Methods in Computer Science, Volume 11, Issue 1 (March 31,
  2015) lmcs:797</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new method for the constraint-based synthesis of termination
arguments for linear loop programs based on linear ranking templates. Linear
ranking templates are parameterized, well-founded relations such that an
assignment to the parameters gives rise to a ranking function. Our approach
generalizes existing methods and enables us to use templates for many different
ranking functions with affine-linear components. We discuss templates for
multiphase, nested, piecewise, parallel, and lexicographic ranking functions.
These ranking templates can be combined to form more powerful templates.
Because these ranking templates require both strict and non-strict
inequalities, we use Motzkin's transposition theorem instead of Farkas' lemma
to transform the generated $\exists\forall$-constraint into an
$\exists$-constraint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00197</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00197</id><created>2015-02-28</created><authors><author><keyname>Horon</keyname><forenames>Jeff</forenames></author></authors><title>Emerging Methods and Tools for Sparking New Global Creative Networks</title><categories>cs.CY cs.HC</categories><comments>Proceedings of the 5th International Conference on Collaborative
  Innovation Networks COINs15, Tokyo, Japan March 12-14, 2015
  (arXiv:1502.01142)</comments><report-no>coins15/2015/29</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Emerging methods and tools are changing the ways participants in global
creative networks become aware of each other and proceed to interact. Some
web-based resources intended to spark new collaborations in creative networks
have been plagued by dependence on fragmented or out-of-date information,
having shallow recall (e.g. limited to a list of manually curated keywords),
offering poor interconnectivity with other systems, and/or obtaining low
end-user adoption. Increased availability of information about creative network
participants' activities and outputs (e.g. completed sponsored research
projects and published results, aggregated into global databases), coupled with
advancement in information processing techniques like Natural Language
Processing, enables new web-based technologies for discovering subject matter
experts, facilities, and networks of current and potential collaborators.
Large-scale data resources and NLP allow modern versions of these tools to
avoid the problems of having sparse data and also provide for deep recall
across many disciplinary vocabularies. These are &quot;passive&quot; technologies, from
the perspective of the network participant, because the agent must undertake an
action to use the information resources. Emerging &quot;active&quot; methods and tools
utilize the same types of information and technologies, but actively intervene
in the formation of the creative network by suggesting connections and
arranging virtual or physical interactions. Active approaches can achieve very
high end-user adoption rates. Both active and passive methods strive to use
data-driven approaches to form better-than-chance awareness among networks of
potential collaborators. Recent case studies suggest the existence of
repeatable strategies for facilitating data-driven matching and
better-than-chance interactions designed to spark new global creative networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00202</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00202</id><created>2015-02-28</created><authors><author><keyname>Li</keyname><forenames>Keqian</forenames></author></authors><title>On Integrating Information Visualization Techniques into Data Mining: A
  Review</title><categories>cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The exploding growth of digital data in the information era and its
immeasurable potential value has called for different types of data-driven
techniques to exploit its value for further applications. Information
visualization and data mining are two research field with such goal. While the
two communities advocates different approaches of problem solving, the vision
of combining the sophisticated algorithmic techniques from data mining as well
as the intuitivity and interactivity of information visualization is tempting.
In this paper, we attempt to survey recent researches and real world systems
integrating the wisdom in two fields towards more effective and efficient data
analytics. More specifically, we study the intersection from a data mining
point of view, explore how information visualization can be used to complement
and improve different stages of data mining through established theories for
optimized visual presentation as well as practical toolsets for rapid
development. We organize the survey by identifying three main stages of typical
process of data mining, the preliminary analysis of data, the model
construction, as well as the model evaluation, and study how each stage can
benefit from information visualization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00205</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00205</id><created>2015-02-28</created><updated>2015-03-05</updated><authors><author><keyname>Xu</keyname><forenames>Yuhua</forenames></author><author><keyname>Wang</keyname><forenames>Jinlong</forenames></author><author><keyname>Wu</keyname><forenames>Qihui</forenames></author><author><keyname>Du</keyname><forenames>Zhiyong</forenames></author><author><keyname>Shen</keyname><forenames>Liang</forenames></author><author><keyname>Anpalagan</keyname><forenames>Alagan</forenames></author></authors><title>A Game Theoretic Perspective on Self-organizing Optimization for
  Cognitive Small Cells</title><categories>cs.IT cs.GT math.IT</categories><comments>8 Pages, 8 Figures, to appear in IEEE Communications Magazine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we investigate self-organizing optimization for cognitive
small cells (CSCs), which have the ability to sense the environment, learn from
historical information, make intelligent decisions, and adjust their
operational parameters. By exploring the inherent features, some fundamental
challenges for self-organizing optimization in CSCs are presented and
discussed. Specifically, the dense and random deployment of CSCs brings about
some new challenges in terms of scalability and adaptation; furthermore, the
uncertain, dynamic and incomplete information constraints also impose some new
challenges in terms of convergence and robustness. For providing better service
to the users and improving the resource utilization, four requirements for
self-organizing optimization in CSCs are presented and discussed. Following the
attractive fact that the decisions in game-theoretic models are exactly
coincident with those in self-organizing optimization, i.e., distributed and
autonomous, we establish a framework of game-theoretic solutions for
self-organizing optimization in CSCs, and propose some featured game models.
Specifically, their basic models are presented, some examples are discussed and
future research directions are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00207</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00207</id><created>2015-02-28</created><authors><author><keyname>Mao</keyname><forenames>Xinhua</forenames></author></authors><title>Knowledge-aided Two-dimensional Autofocus for Spotlight SAR Polar Format
  Imagery</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Conventional two-dimensional (2-D) autofocus algorithms blindly estimate the
phase error in the sense that they do not exploit any a priori information on
the structure of the 2-D phase error. As such, they often suffer from low
computational efficiency and lack of data redundancy to accurately estimate the
2-D phase error. In this paper, a knowledge-aided (KA) 2-D autofocus algorithm
which is based on exploiting a priori knowledge about the 2-D phase error
structure, is presented. First, as a prerequisite of the proposed KA method,
the analytical structure of residual 2-D phase error in SAR imagery is
investigated in the polar format algorithm (PFA) framework. Then, by
incorporating this a priori information, a novel 2-D autofocus approach is
proposed. The new method only requires an estimate of azimuth phase error
and/or residual range cell migration, while the 2-D phase error can then be
computed directly from the estimated azimuth phase error or residual range cell
migration. This 2-D autofocus method can also be applied to refocus moving
targets in PFA imagery. Experimental results clearly demonstrate the
effectiveness and robustness of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00208</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00208</id><created>2015-02-28</created><authors><author><keyname>Ghasemi</keyname><forenames>Zohreh</forenames></author></authors><title>Google-based Mode Choice Modeling Approach</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Microsimulation based frameworks have become very popular in many research
areas including travel demand modeling where activity-based models have been in
the center of attention for the past decade. Advanced activity-based models
synthesize the entire population of the study region and simulate their
activities in a way that they can keep track of agents resources as well as
their spatial location. However, the models that are built for these frameworks
do not take into account this information mainly because they do not have them
at the modeling stage. This paper tries to describe the importance of this
information by analyzing a travel survey and generate the actual alternatives
that individuals had when making their trips. With a focus on transit, the
study reveals how transit alternatives are limited\unavailable in certain areas
which must be taken in to account in our mode choice models. Some statistics
regarding available alternatives and the constraints people encounter when
making a choice are presented with a comprehensive choice set formation. A mode
choice model is then developed based on this approach to represent the
importance of such information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00215</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00215</id><created>2015-02-28</created><authors><author><keyname>Chen</keyname><forenames>Yongxin</forenames></author><author><keyname>Georgiou</keyname><forenames>Tryphon</forenames></author><author><keyname>Pavon</keyname><forenames>Michele</forenames></author></authors><title>Optimal mass transport over bridges</title><categories>math.OC cs.SY</categories><comments>9 pages, 1 figure</comments><msc-class>93E20,</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an overview of our recent work on implementable solutions to the
Schroedinger bridge problem and their potential application to optimal
transport and various generalizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00233</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00233</id><created>2015-03-01</created><authors><author><keyname>Qian</keyname><forenames>Jiang-Hai</forenames></author><author><keyname>Chen</keyname><forenames>Qu</forenames></author><author><keyname>Han</keyname><forenames>Ding-Ding</forenames></author><author><keyname>Ma</keyname><forenames>Yu-Gang</forenames></author></authors><title>Crossover transition in the Fluctuation of Internet</title><categories>physics.soc-ph cs.SI</categories><comments>5pages,4 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Gibrat's law predicts that the standard deviation of the growth rate of a
node's degree is constant. On the other hand, the preferential attachment(PA)
indicates that such standard deviation decreases with initial degree as a power
law of exponent $-0.5$. While both models have been applied to Internet
modeling, this inconsistency requires the verification of their validation.
Therefore we empirically study the fluctuation of Internet of three different
time intervals(daily, monthly and yearly). We find a crossover transition from
PA model to Gibrat's law, which has never been reported. Specifically
Gibrat-law starts from small degree region and extends gradually with the
increase of the observed period. We determine the validated periods for both
models and find that the correlation between internal links has large
contribution to the emergence of Gibrat law. These findings indicate neither PA
nor Gibrat law is applicable to the actual Internet, which requires a more
complete model theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00237</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00237</id><created>2015-03-01</created><authors><author><keyname>Jamshidpey</keyname><forenames>Aryo</forenames></author><author><keyname>Afsharchi</keyname><forenames>Mohsen</forenames></author></authors><title>Task Allocation in Robotic Swarms: Explicit Communication Based
  Approaches</title><categories>cs.MA cs.AI cs.RO</categories><comments>A short version of this paper is accepted by AI2015(conference). It
  has 13 pages and 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study multi robot cooperative task allocation issue in a
situation where a swarm of robots is deployed in a confi?ned unknown
environment where the number of colored spots which represent tasks and the
ratios of them are unknown. The robots should cover this spots as far as
possible to do cleaning and sampling actions desirably. It means that they
should discover the spots cooperatively and spread proportional to the spots
area and avoid from remaining idle. We proposed 4 self-organized distributed
methods which are called hybrid methods for coping with this scenario. In two
diffe?rent experiments the performance of the methods is analyzed. We compared
them with each other and investigated their scalability and robustness in term
of single point of failure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00244</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00244</id><created>2015-03-01</created><authors><author><keyname>Bari</keyname><forenames>Nima</forenames></author><author><keyname>Vichr</keyname><forenames>Roman</forenames></author><author><keyname>Kowsari</keyname><forenames>Kamran</forenames></author><author><keyname>Berkovich</keyname><forenames>Simon Y.</forenames></author></authors><title>23-bit Metaknowledge Template Towards Big Data Knowledge Discovery and
  Management</title><categories>cs.DB cs.AI</categories><comments>IEEE Data Science and Advanced Analytics (DSAA'2014)</comments><doi>10.1109/DSAA.2014.7058121</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The global influence of Big Data is not only growing but seemingly endless.
The trend is leaning towards knowledge that is attained easily and quickly from
massive pools of Big Data. Today we are living in the technological world that
Dr. Usama Fayyad and his distinguished research fellows discussed in the
introductory explanations of Knowledge Discovery in Databases (KDD) predicted
nearly two decades ago. Indeed, they were precise in their outlook on Big Data
analytics. In fact, the continued improvement of the interoperability of
machine learning, statistics, database building and querying fused to create
this increasingly popular science- Data Mining and Knowledge Discovery. The
next generation computational theories are geared towards helping to extract
insightful knowledge from even larger volumes of data at higher rates of speed.
As the trend increases in popularity, the need for a highly adaptive solution
for knowledge discovery will be necessary. In this research paper, we are
introducing the investigation and development of 23 bit-questions for a
Metaknowledge template for Big Data Processing and clustering purposes. This
research aims to demonstrate the construction of this methodology and proves
the validity and the beneficial utilization that brings Knowledge Discovery
from Big Data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00245</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00245</id><created>2015-03-01</created><authors><author><keyname>Bari</keyname><forenames>Nima</forenames></author><author><keyname>Vichr</keyname><forenames>Roman</forenames></author><author><keyname>Kowsari</keyname><forenames>Kamran</forenames></author><author><keyname>Berkovich</keyname><forenames>Simon Y.</forenames></author></authors><title>Novel Metaknowledge-based Processing Technique for Multimedia Big Data
  clustering challenges</title><categories>cs.DB cs.AI</categories><comments>IEEE Multimedia Big Data (BigMM 2015)</comments><doi>10.1109/BigMM.2015.78</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Past research has challenged us with the task of showing relational patterns
between text-based data and then clustering for predictive analysis using Golay
Code technique. We focus on a novel approach to extract metaknowledge in
multimedia datasets. Our collaboration has been an on-going task of studying
the relational patterns between datapoints based on metafeatures extracted from
metaknowledge in multimedia datasets. Those selected are significant to suit
the mining technique we applied, Golay Code algorithm. In this research paper
we summarize findings in optimization of metaknowledge representation for
23-bit representation of structured and unstructured multimedia data in order
to
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00249</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00249</id><created>2015-03-01</created><authors><author><keyname>Alkhalaf</keyname><forenames>Salem</forenames></author></authors><title>Improvement of Control System Performance by Modification of Time Delay</title><categories>cs.SY</categories><comments>5</comments><journal-ref>International Journal of Advanced Computer Science and
  Applications(IJACSA), Volume 6 Issue 2, 2015</journal-ref><doi>10.14569/IJACSA.2015.060226</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a mathematical approach for improving the performance of
a control system by modifying the time delay at certain operating conditions.
This approach converts a continuous time loop into a discrete time loop. The
formula derived is applied successfully to an applicable control system. The
results show that the proposed approach efficiently improves the control system
performance. The relation between the sampling time and the time delay is
obtained. Two different operating conditions are examined to assess the
proposed approach in improving the performance of the control system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00255</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00255</id><created>2015-03-01</created><authors><author><keyname>Shimkin</keyname><forenames>Nahum</forenames></author></authors><title>An Online Convex Optimization Approach to Blackwell's Approachability</title><categories>cs.GT cs.LG</categories><msc-class>91A20 (Primary) 68W27, 68Q32 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The notion of approachability in repeated games with vector payoffs was
introduced by Blackwell in the 1950s, along with geometric conditions for
approachability and corresponding strategies that rely on computing {\em
steering directions} as projections from the current average payoff vector to
the (convex) target set. Recently, Abernethy, Batlett and Hazan (2011) proposed
a class of approachability algorithms that rely on the no-regret properties of
Online Linear Programming for computing a suitable sequence of steering
directions. This is first carried out for target sets that are convex cones,
and then generalized to any convex set by embedding it in a higher-dimensional
convex cone. In this paper we present a more direct formulation that relies on
the support function of the set, along with suitable Online Convex Optimization
algorithms, which leads to a general class of approachability algorithms. We
further show that Blackwell's original algorithm and its convergence follow as
a special case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00258</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00258</id><created>2015-03-01</created><updated>2016-02-14</updated><authors><author><keyname>Iosif</keyname><forenames>Radu</forenames></author></authors><title>Decidable Horn Systems with Difference Constraints Arithmetic</title><categories>cs.FL cs.LO</categories><comments>This paper has been withdrawn by the author due to a crucial error in
  Lemma 5</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper tackles the problem of the existence of solutions for recursive
systems of Horn clauses with second-order variables interpreted as integer
relations, and harnessed by quantifier-free difference bounds arithmetic. We
start by proving the decidability of the problem &quot;does the system have a
solution ?&quot; for a simple class of Horn systems with one second-order variable
and one non-linear recursive rule. The proof relies on a construction of a tree
automaton recognizing all cycles in the weighted graph corresponding to every
unfolding tree of the Horn system. We constrain the tree to recognize only
cycles of negative weight by adding a Presburger formula that harnesses the
number of times each rule is fired, and reduce our problem to the universality
of a Presburger-constrained tree automaton. We studied the complexity of this
problem and found it to be in \textsc{NEXPtime} with an \textsc{EXPtime}-hard
lower bound. Second, we drop the univariate restriction and consider
multivariate second-order Horn systems with a structural restriction, called
\emph{flatness}. This more general class of Horn systems is found to be
decidable, within the same complexity bounds. Finally, we encode the
reachability problem for Alternating Branching Vector Addition Systems (ABVASS)
using Horn systems and prove that, for flat ABVASS, this problem is in
co-\textsc{NEXPtime}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00260</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00260</id><created>2015-03-01</created><authors><author><keyname>Chen</keyname><forenames>Hubie</forenames></author></authors><title>Parameter Compilation</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In resolving instances of a computational problem, if multiple instances of
interest share a feature in common, it may be fruitful to compile this feature
into a format that allows for more efficient resolution, even if the
compilation is relatively expensive. In this article, we introduce a formal
framework for classifying problems according to their compilability. The basic
object in our framework is that of a parameterized problem, which here is a
language along with a parameterization---a map which provides, for each
instance, a so-called parameter on which compilation may be performed. Our
framework is positioned within the paradigm of parameterized complexity, and
our notions are relatable to established concepts in the theory of
parameterized complexity. Indeed, we view our framework as playing a unifying
role, integrating together parameterized complexity and compilability theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00265</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00265</id><created>2015-03-01</created><authors><author><keyname>Shariatpanahi</keyname><forenames>Seyed Pooya</forenames></author><author><keyname>Motahari</keyname><forenames>Seyed Abolfazl</forenames></author><author><keyname>Khalaj</keyname><forenames>Babak Hossein</forenames></author></authors><title>Multi-Server Coded Caching</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider multiple cache-enabled clients connected to
multiple servers through an intermediate network. We design several
topology-aware coding strategies for such networks. Based on topology richness
of the intermediate network, and types of coding operations at internal nodes,
we define three classes of networks, namely, dedicated, flexible, and linear
networks. For each class, we propose an achievable coding scheme, analyze its
coding delay, and also, compare it with an information theoretic lower bound.
For flexible networks, we show that our scheme is order-optimal in terms of
coding delay and, interestingly, the optimal memory-delay curve is achieved in
certain regimes. In general, our results suggest that, in case of networks with
multiple servers, type of network topology can be exploited to reduce service
delay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00267</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00267</id><created>2015-03-01</created><authors><author><keyname>Dahrouj</keyname><forenames>Hayssam</forenames></author><author><keyname>Al-Naffouri</keyname><forenames>Tareq Y.</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Distributed Cloud Association in Downlink Multicloud Radio Access
  Networks</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a multicloud radio access network (M-CRAN), wherein each
cloud serves a cluster of base-stations (BS's) which are connected to the
clouds through high capacity digital links. The network comprises several
remote users, where each user can be connected to one (and only one) cloud.
This paper studies the user-to-cloud-assignment problem by maximizing a
network-wide utility subject to practical cloud connectivity constraints. The
paper solves the problem by using an auction-based iterative algorithm, which
can be implemented in a distributed fashion through a reasonable exchange of
information between the clouds. The paper further proposes a centralized
heuristic algorithm, with low computational complexity. Simulations results
show that the proposed algorithms provide appreciable performance improvements
as compared to the conventional cloud-less assignment solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00269</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00269</id><created>2015-03-01</created><updated>2015-05-10</updated><authors><author><keyname>Loog</keyname><forenames>Marco</forenames></author></authors><title>Contrastive Pessimistic Likelihood Estimation for Semi-Supervised
  Classification</title><categories>stat.ML cs.LG stat.ME</categories><comments>32 pages, minor revision submitted to TPAMI, April 2015</comments><msc-class>68T10 (Primary), 62Fxx (Secondary), 62H12, 97K80, 68T05</msc-class><acm-class>I.2.6; I.5.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Improvement guarantees for semi-supervised classifiers can currently only be
given under restrictive conditions on the data. We propose a general way to
perform semi-supervised parameter estimation for likelihood-based classifiers
for which, on the full training set, the estimates are never worse than the
supervised solution in terms of the log-likelihood. We argue, moreover, that we
may expect these solutions to really improve upon the supervised classifier in
particular cases. In a worked-out example for LDA, we take it one step further
and essentially prove that its semi-supervised version is strictly better than
its supervised counterpart. The two new concepts that form the core of our
estimation principle are contrast and pessimism. The former refers to the fact
that our objective function takes the supervised estimates into account,
enabling the semi-supervised solution to explicitly control the potential
improvements over this estimate. The latter refers to the fact that our
estimates are conservative and therefore resilient to whatever form the true
labeling of the unlabeled data takes on. Experiments demonstrate the
improvements in terms of both the log-likelihood and the classification error
rate on independent test sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00275</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00275</id><created>2015-03-01</created><authors><author><keyname>Komarath</keyname><forenames>Balagopal</forenames></author><author><keyname>Sarma</keyname><forenames>Jayalal</forenames></author><author><keyname>Sunil</keyname><forenames>K. S.</forenames></author></authors><title>Comparator Circuits over Finite Bounded Posets</title><categories>cs.CC</categories><comments>21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Comparator circuit model was originally introduced by Mayr and Subramanian
(1992) (and further studied by Cook, Filmus and Le (2012)) to capture problems
which are not known to be P-complete but still not known to admit efficient
parallel algorithms. The class CC is the complexity class of problems many-one
logspace reducible to the Comparator Circuit Value Problem and we know that NL
is contained in CC which is inturn contained in P. Cook, Filmus and Le (2012)
showed that CC is also the class of languages decided by polynomial size
comparator circuits.
  We study generalizations of the comparator circuit model that work over fixed
finite bounded posets. We observe that there are universal comparator circuits
even over arbitrary fixed finite bounded posets. Building on this, we show that
general (resp. skew) comparator circuits of polynomial size over fixed finite
distributive lattices characterizes CC (resp. L). Complementing this, we show
that general comparator circuits of polynomial size over arbitrary fixed finite
lattices exactly characterizes P and that when the comparator circuit is skew
they characterize NL. In addition, we show a characterization of the class NP
by a family of polynomial sized comparator circuits over fixed {\em finite
bounded posets}. These results generalize the results by Cook, Filmus and Le
(2012) regarding the power of comparator circuits. As an aside, we consider
generalizations of Boolean formulae over arbitrary lattices. We show that
Spira's theorem (1971) can be extended to this setting as well and show that
polynomial sized Boolean formulae over finite fixed lattices capture exactly
NC^1.
  Our results indicate potential new approaches towards the problems P vs CC
and NL vs L using lattice theoretic methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00278</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00278</id><created>2015-03-01</created><authors><author><keyname>Michail</keyname><forenames>Othon</forenames></author></authors><title>An Introduction to Temporal Graphs: An Algorithmic Perspective</title><categories>cs.DM cs.DS</categories><comments>42 pages, 9 figures</comments><msc-class>05C85, 05C40, 05C38, 05C78, 05C80, 68R10, 68M14, 68M10, 68W25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A \emph{temporal graph} is, informally speaking, a graph that changes with
time. When time is discrete and only the relationships between the
participating entities may change and not the entities themselves, a temporal
graph may be viewed as a sequence $G_1,G_2\ldots,G_l$ of static graphs over the
same (static) set of nodes $V$. Though static graphs have been extensively
studied, for their temporal generalization we are still far from having a
concrete set of structural and algorithmic principles. Recent research shows
that many graph properties and problems become radically different and usually
substantially more difficult when an extra time dimension in added to them.
Moreover, there is already a rich and rapidly growing set of modern systems and
applications that can be naturally modeled and studied via temporal graphs.
This, further motivates the need for the development of a temporal extension of
graph theory. We survey here recent results on temporal graphs and temporal
graph problems that have appeared in the Computer Science community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00279</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00279</id><created>2015-03-01</created><authors><author><keyname>Broda</keyname><forenames>Sabine</forenames></author><author><keyname>Machiavelo</keyname><forenames>Ant&#xf3;nio</forenames></author><author><keyname>Moreira</keyname><forenames>Nelma</forenames></author><author><keyname>Reis</keyname><forenames>Rog&#xe9;rio</forenames></author></authors><title>Partial Derivative Automaton for Regular Expressions with Shuffle</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalize the partial derivative automaton to regular expressions with
shuffle and study its size in the worst and in the average case. The number of
states of the partial derivative automata is in the worst case at most 2^m,
where m is the number of letters in the expression, while asymptotically and on
average it is no more than (4/3)^m.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00288</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00288</id><created>2015-03-01</created><authors><author><keyname>Song</keyname><forenames>Yang</forenames></author><author><keyname>van Boeschoten</keyname><forenames>Robert</forenames></author></authors><title>Success factors for Crowdfunding founders and funders</title><categories>cs.CY cs.HC cs.SI</categories><comments>Proceedings of the 5th International Conference on Collaborative
  Innovation Networks COINs15, Tokyo, Japan March 12-14, 2015
  (arXiv:1502.01142)</comments><report-no>coins15/2015/26</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crowdfunding has been used as one of the effective ways for entrepreneurs to
raise funding especially in creative industries. Individuals as well as
organizations are paying more attentions to the emergence of new crowdfunding
platforms. In the Netherlands, the government is also trying to help artists
access financial resources through crowdfunding platforms. This research aims
at discovering the success factors for crowdfunding projects through
crowdfunding platforms from both founders and funders perspective. We designed
our own website for founders and funders to observe crowdfunding behaviors. Our
research will contribute to crowdfunding success factors related to issues of
trust and decision making and provide practical recommendations for
practitioners and researchers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00295</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00295</id><created>2015-03-01</created><updated>2015-04-22</updated><authors><author><keyname>Rubtsov</keyname><forenames>Alexander A.</forenames></author><author><keyname>Vyalyi</keyname><forenames>Mikhail N.</forenames></author></authors><title>Regular realizability problems and context-free languages</title><categories>cs.FL</categories><comments>conference DCFS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate regular realizability (RR) problems, which are the problems of
verifying whether intersection of a regular language -- the input of the
problem -- and fixed language called filter is non-empty. In this paper we
focus on the case of context-free filters. Algorithmic complexity of the RR
problem is a very coarse measure of context-free languages complexity. This
characteristic is compatible with rational dominance. We present examples of
P-complete RR problems as well as examples of RR problems in the class NL. Also
we discuss RR problems with context-free filters that might have intermediate
complexity. Possible candidates are the languages with polynomially bounded
rational indices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00301</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00301</id><created>2015-03-01</created><authors><author><keyname>Metzler</keyname><forenames>Saskia</forenames></author><author><keyname>Miettinen</keyname><forenames>Pauli</forenames></author></authors><title>On Defining SPARQL with Boolean Tensor Algebra</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Resource Description Framework (RDF) represents information as
subject-predicate-object triples. These triples are commonly interpreted as a
directed labelled graph. We propose an alternative approach, interpreting the
data as a 3-way Boolean tensor. We show how SPARQL queries - the standard
queries for RDF - can be expressed as elementary operations in Boolean algebra,
giving us a complete re-interpretation of RDF and SPARQL. We show how the
Boolean tensor interpretation allows for new optimizations and analyses of the
complexity of SPARQL queries. For example, estimating the size of the results
for different join queries becomes much simpler.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00302</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00302</id><created>2015-03-01</created><authors><author><keyname>Dong</keyname><forenames>Xin Luna</forenames></author><author><keyname>Gabrilovich</keyname><forenames>Evgeniy</forenames></author><author><keyname>Heitz</keyname><forenames>Geremy</forenames></author><author><keyname>Horn</keyname><forenames>Wilko</forenames></author><author><keyname>Murphy</keyname><forenames>Kevin</forenames></author><author><keyname>Sun</keyname><forenames>Shaohua</forenames></author><author><keyname>Zhang</keyname><forenames>Wei</forenames></author></authors><title>From Data Fusion to Knowledge Fusion</title><categories>cs.DB</categories><comments>VLDB'2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The task of {\em data fusion} is to identify the true values of data items
(eg, the true date of birth for {\em Tom Cruise}) among multiple observed
values drawn from different sources (eg, Web sites) of varying (and unknown)
reliability. A recent survey\cite{LDL+12} has provided a detailed comparison of
various fusion methods on Deep Web data. In this paper, we study the
applicability and limitations of different fusion techniques on a more
challenging problem: {\em knowledge fusion}. Knowledge fusion identifies true
subject-predicate-object triples extracted by multiple information extractors
from multiple information sources. These extractors perform the tasks of entity
linkage and schema alignment, thus introducing an additional source of noise
that is quite different from that traditionally considered in the data fusion
literature, which only focuses on factual errors in the original sources. We
adapt state-of-the-art data fusion techniques and apply them to a knowledge
base with 1.6B unique knowledge triples extracted by 12 extractors from over 1B
Web pages, which is three orders of magnitude larger than the data sets used in
previous data fusion papers. We show great promise of the data fusion
approaches in solving the knowledge fusion problem, and suggest interesting
research directions through a detailed error analysis of the methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00303</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00303</id><created>2015-03-01</created><authors><author><keyname>Li</keyname><forenames>Xian</forenames></author><author><keyname>Dong</keyname><forenames>Xin Luna</forenames></author><author><keyname>Lyons</keyname><forenames>Kenneth</forenames></author><author><keyname>Meng</keyname><forenames>Weiyi</forenames></author><author><keyname>Srivastava</keyname><forenames>Divesh</forenames></author></authors><title>Truth Finding on the Deep Web: Is the Problem Solved?</title><categories>cs.DB cs.IR</categories><comments>VLDB'2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The amount of useful information available on the Web has been growing at a
dramatic pace in recent years and people rely more and more on the Web to
fulfill their information needs. In this paper, we study truthfulness of Deep
Web data in two domains where we believed data are fairly clean and data
quality is important to people's lives: {\em Stock} and {\em Flight}. To our
surprise, we observed a large amount of inconsistency on data from different
sources and also some sources with quite low accuracy. We further applied on
these two data sets state-of-the-art {\em data fusion} methods that aim at
resolving conflicts and finding the truth, analyzed their strengths and
limitations, and suggested promising research directions. We wish our study can
increase awareness of the seriousness of conflicting data on the Web and in
turn inspire more research in our community to tackle this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00306</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00306</id><created>2015-03-01</created><authors><author><keyname>Pochampally</keyname><forenames>Ravali</forenames></author><author><keyname>Sarma</keyname><forenames>Anish Das</forenames></author><author><keyname>Dong</keyname><forenames>Xin Luna</forenames></author><author><keyname>Meliou</keyname><forenames>Alexandra</forenames></author><author><keyname>Srivastava</keyname><forenames>Divesh</forenames></author></authors><title>Fusing Data with Correlations</title><categories>cs.DB</categories><comments>Sigmod'2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many applications rely on Web data and extraction systems to accomplish
knowledge-driven tasks. Web information is not curated, so many sources provide
inaccurate, or conflicting information. Moreover, extraction systems introduce
additional noise to the data. We wish to automatically distinguish correct data
and erroneous data for creating a cleaner set of integrated data. Previous work
has shown that a na\&quot;ive voting strategy that trusts data provided by the
majority or at least a certain number of sources may not work well in the
presence of copying between the sources. However, correlation between sources
can be much broader than copying: sources may provide data from complementary
domains (\emph{negative correlation}), extractors may focus on different types
of information (\emph{negative correlation}), and extractors may apply common
rules in extraction (\emph{positive correlation, without copying}). In this
paper we present novel techniques modeling correlations between sources and
applying it in truth finding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00309</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00309</id><created>2015-03-01</created><authors><author><keyname>Li</keyname><forenames>Xian</forenames></author><author><keyname>Dong</keyname><forenames>Xin Luna</forenames></author><author><keyname>Lyons</keyname><forenames>Kenneth B.</forenames></author><author><keyname>Meng</keyname><forenames>Weiyi</forenames></author><author><keyname>Srivastava</keyname><forenames>Divesh</forenames></author></authors><title>Scaling up Copy Detection</title><categories>cs.DB</categories><comments>ICDE 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent research shows that copying is prevalent for Deep-Web data and
considering copying can significantly improve truth finding from conflicting
values. However, existing copy detection techniques do not scale for large
sizes and numbers of data sources, so truth finding can be slowed down by one
to two orders of magnitude compared with the corresponding techniques that do
not consider copying. In this paper, we study {\em how to improve scalability
of copy detection on structured data}.
  Our algorithm builds an inverted index for each \emph{shared} value and
processes the index entries in decreasing order of how much the shared value
can contribute to the conclusion of copying. We show how we use the index to
prune the data items we consider for each pair of sources, and to incrementally
refine our results in iterative copy detection. We also apply a sampling
strategy with which we are able to further reduce copy-detection time while
still obtaining very similar results as on the whole data set. Experiments on
various real data sets show that our algorithm can reduce the time for copy
detection by two to three orders of magnitude; in other words, truth finding
can benefit from copy detection with very little overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00310</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00310</id><created>2015-03-01</created><authors><author><keyname>Dong</keyname><forenames>Xin Luna</forenames></author><author><keyname>Berti-Equille</keyname><forenames>Laure</forenames></author><author><keyname>Srivastava</keyname><forenames>Divesh</forenames></author></authors><title>Data Fusion: Resolving Conflicts from Multiple Sources</title><categories>cs.DB</categories><comments>WAIM 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many data management applications, such as setting up Web portals, managing
enterprise data, managing community data, and sharing scientific data, require
integrating data from multiple sources. Each of these sources provides a set of
values and different sources can often provide conflicting values. To present
quality data to users, it is critical to resolve conflicts and discover values
that reflect the real world; this task is called {\em data fusion}. This paper
describes a novel approach that finds true values from conflicting information
when there are a large number of sources, among which some may copy from
others. We present a case study on real-world data showing that the described
algorithm can significantly improve accuracy of truth discovery and is scalable
when there are a large number of data sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00311</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00311</id><created>2015-03-01</created><authors><author><keyname>Hashemi</keyname><forenames>Morteza</forenames></author></authors><title>Reducing ADC Sampling Rate with Compressive Sensing</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many communication systems involve high bandwidth, while sparse, radio
frequency (RF) signals. Working with high frequency signals requires
appropriate system-level components such as high-speed analog-to-digital
converters (ADC). In particular, an analog signal should be sampled at rates
that meet the Nyquist requirements to avoid aliasing. However, implementing
high-speed ADC devices can be a limiting factor as well as expensive. To
mitigate the caveats with high-speed ADC, the solution space can be explored in
several dimensions such as utilizing the compressive sensing (CS) framework in
order to reduce the sampling rate to the order of information rate of the
signal rather than a rate dictated by the Nyquist. In this note, we review the
compressive sensing structure and its extensions for continuous-time signals,
which is ultimately used to reduce the sampling rate of high-speed ADC devices.
Moreover, we consider the application of the compressive sensing framework in
wireless sensor networks to save power by reducing the transmission rate of
sensor nodes. We propose an alternative solution for the CS minimization
problem that can be solved using gradient descent methods. The modified
minimization problem is potentially faster and simpler to implement at the
hardware level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00321</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00321</id><created>2015-03-01</created><authors><author><keyname>Dutta</keyname><forenames>Chinmoy</forenames></author><author><keyname>Radhakrishnan</keyname><forenames>Jaikumar</forenames></author></authors><title>A Sampling Technique of Proving Lower Bounds for Noisy Computations</title><categories>cs.CC cs.DC</categories><acm-class>C.2.1; C.2.2; C.2.4; D.4.4; F.1.1; F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a technique of proving lower bounds for noisy computations. This
is achieved by a theorem connecting computations on a kind of randomized
decision trees and sampling based algorithms. This approach is surprisingly
powerful, and applicable to several models of computation previously studied.
  As a first illustration we show how all the results of Evans and Pippenger
(SIAM J. Computing, 1999) for noisy decision trees, some of which were derived
using Fourier analysis, follow immediately if we consider the sampling-based
algorithms that naturally arise from these decision trees.
  Next, we show a tight lower bound of $\Omega(N \log\log N)$ on the number of
transmissions required to compute several functions (including the parity
function and the majority function) in a network of $N$ randomly placed
sensors, communicating using local transmissions, and operating with power near
the connectivity threshold. This result considerably simplifies and strengthens
an earlier result of Dutta, Kanoria Manjunath and Radhakrishnan (SODA 08) that
such networks cannot compute the parity function reliably with significantly
fewer than $N\log \log N$ transmissions. The lower bound for parity shown
earlier made use of special properties of the parity function and is
inapplicable, e.g., to the majority function. In this paper, we use our
approach to develop an interesting connection between computation of boolean
functions on noisy networks that make few transmissionss, and algorithms that
work by sampling only a part of the input. It is straightforward to verify that
such sampling-based algorithms cannot compute the majority function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00322</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00322</id><created>2015-03-01</created><updated>2015-12-11</updated><authors><author><keyname>Kloster</keyname><forenames>Kyle</forenames></author><author><keyname>Gleich</keyname><forenames>David F.</forenames></author></authors><title>Seeded PageRank Solution Paths</title><categories>cs.SI</categories><comments>29 pages, 8 figures</comments><msc-class>91D30 (Primary)</msc-class><acm-class>I.5.3; G.1.3; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the behavior of network diffusions based on the PageRank random walk
from a set of seed nodes. These diffusions are known to reveal small, localized
clusters (or communities) and also large macro-scale clusters by varying a
parameter that has a dual-interpretation as an accuracy bound and as a
regularization level. We propose a new method that quickly approximates the
result of the diffusion for all values of this parameter. Our method
efficiently generates an approximate $\textit{solution path}$ or
$\textit{regularization path}$ associated with a PageRank diffusion, and it
reveals cluster structures at multiple size-scales between small and large. We
formally prove a runtime bound on this method that is independent of the size
of the network, and we investigate multiple optimizations to our method that
can be more practical in some settings. We demonstrate that these methods
identify refined clustering structure on a number of real-world networks with
up to 2 billion edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00323</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00323</id><created>2015-03-01</created><authors><author><keyname>Cort&#xe9;s</keyname><forenames>E. Cruz</forenames></author><author><keyname>Scott</keyname><forenames>C.</forenames></author></authors><title>Sparse Approximation of a Kernel Mean</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kernel means are frequently used to represent probability distributions in
machine learning problems. In particular, the well known kernel density
estimator and the kernel mean embedding both have the form of a kernel mean.
Unfortunately, kernel means are faced with scalability issues. A single point
evaluation of the kernel density estimator, for example, requires a computation
time linear in the training sample size. To address this challenge, we present
a method to efficiently construct a sparse approximation of a kernel mean. We
do so by first establishing an incoherence-based bound on the approximation
error, and then noticing that, for the case of radial kernels, the bound can be
minimized by solving the $k$-center problem. The outcome is a linear time
construction of a sparse kernel mean, which also lends itself naturally to an
automatic sparsity selection scheme. We show the computational gains of our
method by looking at three problems involving kernel means: Euclidean embedding
of distributions, class proportion estimation, and clustering using the
mean-shift algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00327</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00327</id><created>2015-03-01</created><updated>2015-08-18</updated><authors><author><keyname>Gilbert</keyname><forenames>Oscar</forenames></author><author><keyname>Hendricks</keyname><forenames>Jacob</forenames></author><author><keyname>Patitz</keyname><forenames>Matthew J.</forenames></author><author><keyname>Rogers</keyname><forenames>Trent A.</forenames></author></authors><title>Computing in continuous space with self-assembling polygonal tiles</title><categories>cs.CG</categories><comments>Added a few more images, including full examples of bit reading
  gadgets</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate the computational power of the polygonal tile
assembly model (polygonal TAM) at temperature 1, i.e. in non-cooperative
systems. The polygonal TAM is an extension of Winfree's abstract tile assembly
model (aTAM) which not only allows for square tiles (as in the aTAM) but also
allows for tile shapes that are polygons. Although a number of self-assembly
results have shown computational universality at temperature 1, these are the
first results to do so by fundamentally relying on tile placements in
continuous, rather than discrete, space. With the square tiles of the aTAM, it
is conjectured that the class of temperature 1 systems is not computationally
universal. Here we show that the class of systems whose tiles are composed of a
regular polygon P with n &gt; 6 sides is computationally universal. On the other
hand, we show that the class of systems whose tiles consist of a regular
polygon P with n &lt;= 6 cannot compute using any known techniques. In addition,
we show a number of classes of systems whose tiles consist of a non-regular
polygon with n &gt;= 3 sides are computationally universal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00330</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00330</id><created>2015-03-01</created><authors><author><keyname>Williams</keyname><forenames>Grady</forenames></author><author><keyname>Rombokas</keyname><forenames>Eric</forenames></author><author><keyname>Daniel</keyname><forenames>Tom</forenames></author></authors><title>GPU Based Path Integral Control with Learned Dynamics</title><categories>cs.RO</categories><comments>6 pages, NIPS 2014 - Autonomously Learning Robots Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm which combines recent advances in model based path
integral control with machine learning approaches to learning forward dynamics
models. We take advantage of the parallel computing power of a GPU to quickly
take a massive number of samples from a learned probabilistic dynamics model,
which we use to approximate the path integral form of the optimal control. The
resulting algorithm runs in a receding-horizon fashion in realtime, and is
subject to no restrictive assumptions about costs, constraints, or dynamics. A
simple change to the path integral control formulation allows the algorithm to
take model uncertainty into account during planning, and we demonstrate its
performance on a quadrotor navigation task. In addition to this novel
adaptation of path integral control, this is the first time that a
receding-horizon implementation of iterative path integral control has been run
on a real system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00332</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00332</id><created>2015-03-01</created><updated>2015-06-05</updated><authors><author><keyname>Huggins</keyname><forenames>Jonathan H.</forenames></author><author><keyname>Narasimhan</keyname><forenames>Karthik</forenames></author><author><keyname>Saeedi</keyname><forenames>Ardavan</forenames></author><author><keyname>Mansinghka</keyname><forenames>Vikash K.</forenames></author></authors><title>JUMP-Means: Small-Variance Asymptotics for Markov Jump Processes</title><categories>stat.ML cs.LG</categories><comments>In Proceedings of the 32nd International Conference on Machine
  Learning (ICML 2015)</comments><journal-ref>JMLR: W&amp;CP Volume 37, 2015 pp. 693-701</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Markov jump processes (MJPs) are used to model a wide range of phenomena from
disease progression to RNA path folding. However, maximum likelihood estimation
of parametric models leads to degenerate trajectories and inferential
performance is poor in nonparametric models. We take a small-variance
asymptotics (SVA) approach to overcome these limitations. We derive the
small-variance asymptotics for parametric and nonparametric MJPs for both
directly observed and hidden state models. In the parametric case we obtain a
novel objective function which leads to non-degenerate trajectories. To derive
the nonparametric version we introduce the gamma-gamma process, a novel
extension to the gamma-exponential process. We propose algorithms for each of
these formulations, which we call \emph{JUMP-means}. Our experiments
demonstrate that JUMP-means is competitive with or outperforms widely used MJP
inference approaches in terms of both speed and reconstruction accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00336</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00336</id><created>2015-03-01</created><authors><author><keyname>Dundua</keyname><forenames>Besik</forenames></author><author><keyname>Florido</keyname><forenames>M&#xe1;rio</forenames></author><author><keyname>Kutsia</keyname><forenames>Temur</forenames></author><author><keyname>Marin</keyname><forenames>Mircea</forenames></author></authors><title>CLP(H): Constraint Logic Programming for Hedges</title><categories>cs.LO</categories><comments>To appear in Theory and Practice of Logic Programming (TPLP)</comments><msc-class>68N17, 68Q70, 68Q55</msc-class><acm-class>D.1.6; D.3.1; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  CLP(H) is an instantiation of the general constraint logic programming scheme
with the constraint domain of hedges. Hedges are finite sequences of unranked
terms, built over variadic function symbols and three kinds of variables: for
terms, for hedges, and for function symbols. Constraints involve equations
between unranked terms and atoms for regular hedge language membership. We
study algebraic semantics of CLP(H) programs, define a sound, terminating, and
incomplete constraint solver, investigate two fragments of constraints for
which the solver returns a complete set of solutions, and describe classes of
programs that generate such constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00338</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00338</id><created>2015-03-01</created><authors><author><keyname>Lesieur</keyname><forenames>Thibault</forenames></author><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author><author><keyname>Zdeborova</keyname><forenames>Lenka</forenames></author></authors><title>Phase Transitions in Sparse PCA</title><categories>cs.IT cond-mat.stat-mech math.IT stat.ML</categories><comments>6 pages, 3 figures</comments><journal-ref>Information Theory (ISIT), 2015 IEEE International Symposium on ,
  vol., no., pp.1635-1639, 14-19 June 2015</journal-ref><doi>10.1109/ISIT.2015.7282733</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study optimal estimation for sparse principal component analysis when the
number of non-zero elements is small but on the same order as the dimension of
the data. We employ approximate message passing (AMP) algorithm and its state
evolution to analyze what is the information theoretically minimal mean-squared
error and the one achieved by AMP in the limit of large sizes. For a special
case of rank one and large enough density of non-zeros Deshpande and Montanari
[1] proved that AMP is asymptotically optimal. We show that both for low
density and for large rank the problem undergoes a series of phase transitions
suggesting existence of a region of parameters where estimation is information
theoretically possible, but AMP (and presumably every other polynomial
algorithm) fails. The analysis of the large rank limit is particularly
instructive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00339</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00339</id><created>2015-03-01</created><updated>2015-05-05</updated><authors><author><keyname>Kargin</keyname><forenames>Vladislav</forenames></author></authors><title>Variation of word frequencies in Russian literary texts</title><categories>cs.CL physics.soc-ph stat.AP</categories><comments>17 pages</comments><doi>10.1016/j.physa.2015.11.014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the variation of word frequencies in Russian literary texts. Our
findings indicate that the standard deviation of a word's frequency across
texts depends on its average frequency according to a power law with exponent
$0.62,$ showing that the rarer words have a relatively larger degree of
frequency volatility (i.e., &quot;burstiness&quot;).
  Several latent factors models have been estimated to investigate the
structure of the word frequency distribution. The dependence of a word's
frequency volatility on its average frequency can be explained by the asymmetry
in the distribution of latent factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00340</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00340</id><created>2015-03-01</created><authors><author><keyname>Anshelevich</keyname><forenames>Elliot</forenames></author><author><keyname>Kar</keyname><forenames>Koushik</forenames></author><author><keyname>Sekar</keyname><forenames>Shreyas</forenames></author></authors><title>Envy-Free Pricing in Large Markets: Approximating Revenue and Welfare</title><categories>cs.GT</categories><comments>(Under Review)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the classic setting of envy-free pricing, in which a single seller
chooses prices for its many items, with the goal of maximizing revenue once the
items are allocated. Despite the large body of work addressing such settings,
most versions of this problem have resisted good approximation factors for
maximizing revenue; this is true even for the classic unit-demand case. In this
paper we study envy-free pricing with unit-demand buyers, but unlike previous
work we focus on large markets: ones in which the demand of each buyer is
infinitesimally small compared to the size of the overall market. We assume
that the buyer valuations for the items they desire have a nice (although
reasonable) structure, i.e., that the aggregate buyer demand has a monotone
hazard rate and that the values of every buyer type come from the same support.
  For such large markets, our main contribution is a 1.88 approximation
algorithm for maximizing revenue, showing that good pricing schemes can be
computed when the number of buyers is large. We also give a (e,2)-bicriteria
algorithm that simultaneously approximates both maximum revenue and welfare,
thus showing that it is possible to obtain both good revenue and welfare at the
same time. We further generalize our results by relaxing some of our
assumptions, and quantify the necessary tradeoffs between revenue and welfare
in our setting. Our results are the first known approximations for large
markets, and crucially rely on new lower bounds which we prove for the
revenue-maximizing prices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00360</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00360</id><created>2015-03-01</created><authors><author><keyname>Shimobaba</keyname><forenames>Tomoyoshi</forenames></author><author><keyname>Kakue</keyname><forenames>Takashi</forenames></author><author><keyname>Endo</keyname><forenames>Yutaka</forenames></author><author><keyname>Hirayama</keyname><forenames>Ryuji</forenames></author><author><keyname>Hiyama</keyname><forenames>Daisuke</forenames></author><author><keyname>Hasegawa</keyname><forenames>Satoki</forenames></author><author><keyname>Nagahama</keyname><forenames>Yuki</forenames></author><author><keyname>Sano</keyname><forenames>Marie</forenames></author><author><keyname>Sugie</keyname><forenames>Takashige</forenames></author><author><keyname>Ito</keyname><forenames>Tomoyoshi</forenames></author></authors><title>Optical encryption for large-sized images using random phase-free method</title><categories>physics.optics cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an optical encryption framework that can encrypt and decrypt
large-sized images beyond the size of the encrypted image using our two
methods: random phase-free method and scaled diffraction. In order to record
the entire image information on the encrypted image, the large-sized images
require the random phase to widely diffuse the object light over the encrypted
image; however, the random phase gives rise to the speckle noise on the
decrypted images, and it may be difficult to recognize the decrypted images. In
order to reduce the speckle noise, we apply our random phase-free method to the
framework. In addition, we employ scaled diffraction that calculates light
propagation between planes with different sizes by changing the sampling rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00361</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00361</id><created>2015-03-01</created><authors><author><keyname>Kim</keyname><forenames>Jinseok</forenames></author><author><keyname>Diesner</keyname><forenames>Jana</forenames></author></authors><title>Coauthorship networks: A directed network approach considering the order
  and number of coauthors</title><categories>cs.DL cs.SI physics.soc-ph</categories><comments>This is a preprint of an article accepted for publication in Journal
  of the Association for Information Science and Technology</comments><doi>10.1002/asi.23361</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many scientific fields, the order of coauthors on a paper conveys
information about each individual's contribution to a piece of joint work. We
argue that in prior network analyses of coauthorship networks, the information
on ordering has been insufficiently considered because ties between authors are
typically symmetrized. This is basically the same as assuming that each
co-author has contributed equally to a paper. We introduce a solution to this
problem by adopting a coauthorship credit allocation model proposed by Kim and
Diesner (2014), which in its core conceptualizes co-authoring as a directed,
weighted, and self-looped network. We test and validate our application of the
adopted framework based on a sample data of 861 authors who have published in
the journal Psychometrika. Results suggest that this novel sociometric approach
can complement traditional measures based on undirected networks and expand
insights into coauthoring patterns such as the hierarchy of collaboration among
scholars. As another form of validation, we also show how our approach
accurately detects prominent scholars in the Psychometric Society affiliated
with the journal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00362</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00362</id><created>2015-03-01</created><authors><author><keyname>Achilleos</keyname><forenames>Antonis</forenames></author></authors><title>NEXP-completeness and Universal Hardness Results for Justification Logic</title><categories>cs.LO cs.CC</categories><comments>Shorter version has been accepted for publication by CSR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a lower complexity bound for the satisfiability problem of a
multi-agent justification logic, establishing that the general NEXP upper bound
from our previous work is tight. We then use a simple modification of the
corresponding reduction to prove that satisfiability for all multi-agent
justification logics from there is hard for the Sigma 2 p class of the second
level of the polynomial hierarchy - given certain reasonable conditions. Our
methods improve on these required conditions for the same lower bound for the
single-agent justification logics, proven by Buss and Kuznets in 2009, thus
answering one of their open questions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00364</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00364</id><created>2015-03-01</created><authors><author><keyname>Berman</keyname><forenames>Kiyoshi J</forenames></author><author><keyname>Glisson</keyname><forenames>William Bradley</forenames></author><author><keyname>Glisson</keyname><forenames>L. Milton</forenames></author></authors><title>Investigating the Impact of Global Positioning System Evidence</title><categories>cs.CY</categories><comments>This article was published at:
  http://www.hicss.hawaii.edu/hicss_48/apahome48.htm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The continued amalgamation of Global Positioning Systems (GPS) into everyday
activities stimulates the idea that these devices will increasingly contribute
evidential importance in digital forensics cases. This study investigates the
extent to which GPS devices are being used in criminal and civil court cases in
the United Kingdom through the inspection of Lexis Nexis, Westlaw, and the
British and Irish Legal Information Institute (BAILII) legal databases. The
research identified 83 cases which involved GPS evidence from within the United
Kingdom and Europe for the time period from 01 June 1993 to 01 June 2013. The
initial empirical analysis indicates that GPS evidence in court cases is rising
over time and the majority of those court cases are criminal cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00366</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00366</id><created>2015-03-01</created><authors><author><keyname>Awad</keyname><forenames>Abir</forenames></author></authors><title>A New Chaos-Based Cryptosystem for Secure Transmitted Images</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel and robust chaos-based cryptosystem for secure
transmitted images and four other versions. In the proposed block
encryption/decryption algorithm, a 2D chaotic map is used to shuffle the image
pixel positions. Then, substitution (confusion) and permutation (diffusion)
operations on every block, with multiple rounds, are combined using two
perturbed chaotic PWLCM maps. The perturbing orbit technique improves the
statistical properties of encrypted images. The obtained error propagation in
various standard cipher block modes demonstrates that the proposed cryptosystem
is suitable to transmit cipher data over a corrupted digital channel. Finally,
to quantify the security level of the proposed cryptosystem, many tests are
performed and experimental results show that the suggested cryptosystem has a
high security level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00368</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00368</id><created>2015-03-01</created><authors><author><keyname>Kelk</keyname><forenames>Steven</forenames></author><author><keyname>van Iersel</keyname><forenames>Leo</forenames></author><author><keyname>Scornavacca</keyname><forenames>Celine</forenames></author></authors><title>Phylogenetic incongruence through the lens of Monadic Second Order logic</title><categories>cs.DS cs.CE cs.LO q-bio.PE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Within the field of phylogenetics there is growing interest in measures for
summarising the dissimilarity, or 'incongruence', of two or more phylogenetic
trees. Many of these measures are NP-hard to compute and this has stimulated a
considerable volume of research into fixed parameter tractable algorithms. In
this article we use Monadic Second Order logic (MSOL) to give alternative,
compact proofs of fixed parameter tractability for several well-known
incongruency measures. In doing so we wish to demonstrate the considerable
potential of MSOL - machinery still largely unknown outside the algorithmic
graph theory community - within phylogenetics. A crucial component of this work
is the observation that many of these measures, when bounded, imply the
existence of an 'agreement forest' of bounded size, which in turn implies that
an auxiliary graph structure, the display graph, has bounded treewidth. It is
this bound on treewidth that makes the machinery of MSOL available for proving
fixed parameter tractability. We give a variety of different MSOL formulations.
Some are based on explicitly encoding agreement forests, while some only use
them implicitly to generate the treewidth bound. Our formulations introduce a
number of &quot;phylogenetics MSOL primitives&quot; which will hopefully be of use to
other researchers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00369</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00369</id><created>2015-03-01</created><authors><author><keyname>Chakraborti</keyname><forenames>Subhamoy</forenames></author><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author></authors><title>Heuristic Algorithm using Internet of Things and Mobility for solving
  demographic issues in Financial Inclusion projects</title><categories>cs.CY</categories><comments>5 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  All over the world, the population in rural and semi-urban areas is a major
concern for policy makers as they require technology enablement most to get
elevated from their living conditions. Financial Inclusion projects are meant
to work in this area as well. However the reach of financial institutions in
those areas is much lesser than in urban areas in general. New policies and
strategies are being made towards making banking services more accessible to
the people in rural and semi-urban area. Technology, in particular Internet of
Things (IoT) and Mobility can play a major role in this context as an enabler
and multiplier. This paper discusses about an IoT and Mobility driven approach,
which the Financial Institutions may consider while trying to work on financial
inclusion projects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00374</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00374</id><created>2015-03-01</created><authors><author><keyname>Boutsidis</keyname><forenames>Christos</forenames></author><author><keyname>Drineas</keyname><forenames>Petros</forenames></author><author><keyname>Kambadur</keyname><forenames>Prabhanjan</forenames></author><author><keyname>Zouzias</keyname><forenames>Anastasios</forenames></author></authors><title>A Randomized Algorithm for Approximating the Log Determinant of a
  Symmetric Positive Definite Matrix</title><categories>cs.DS</categories><comments>working paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a novel algorithm for approximating the logarithm of the
determinant of a symmetric positive definite matrix. The algorithm is
randomized and proceeds in two steps: first, it finds an approximation to the
largest eigenvalue of the matrix after running a few iterations of the
so-called &quot;power method&quot; from the numerical linear algebra literature. Then,
using this information, it approximates the traces of a small number of matrix
powers of a specially constructed matrix, using the method of Avron and
Toledo~\cite{AT11}. From a theoretical perspective, we present strong
worst-case analysis bounds for our algorithm. From an empirical perspective, we
demonstrate that a C++ implementation of our algorithm can approximate the
logarithm of the determinant of large matrices very accurately in a matter of
seconds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00375</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00375</id><created>2015-03-01</created><updated>2015-05-28</updated><authors><author><keyname>van Emden</keyname><forenames>M. H.</forenames></author></authors><title>The lambda mechanism in lambda calculus and in other calculi</title><categories>cs.PL</categories><comments>9 pages, 5 figures</comments><acm-class>D.1.1; D.3.1; D.3.3; F.3.2; F.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A comparison of Landin's form of lambda calculus with Church's shows that,
independently of the lambda calculus, there exists a mechanism for converting
functions with arguments indexed by variables to the usual kind of function
where the arguments are indexed numerically. We call this the &quot;lambda
mechanism&quot; and show how it can be used in other calculi. In first-order
predicate logic it can be used to define new functions and new predicates in
terms of existing ones. In a purely imperative programming language it can be
used to provide an Algol-like procedure facility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00388</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00388</id><created>2015-03-01</created><authors><author><keyname>Muhammad</keyname><forenames>Khan</forenames></author><author><keyname>Ahmad</keyname><forenames>Jamil</forenames></author><author><keyname>Farman</keyname><forenames>Haleem</forenames></author><author><keyname>Zubair</keyname><forenames>Muhammad</forenames></author></authors><title>A Novel Image Steganographic Approach for Hiding Text in Color Images
  using HSI Color Model</title><categories>cs.MM</categories><comments>An easy to follow paper of 11 pages. arXiv admin note: text overlap
  with arXiv:1502.07808</comments><journal-ref>Middle-East Journal of Scientific Research 22.5 (2014): 647-654</journal-ref><doi>10.5829/idosi.mejsr.2014.22.05.21946</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image Steganography is the process of embedding text in images such that its
existence cannot be detected by Human Visual System (HVS) and is known only to
sender and receiver. This paper presents a novel approach for image
steganography using Hue-Saturation-Intensity (HSI) color space based on Least
Significant Bit (LSB). The proposed method transforms the image from RGB color
space to Hue-Saturation-Intensity (HSI) color space and then embeds secret data
inside the Intensity Plane (I-Plane) and transforms it back to RGB color model
after embedding. The said technique is evaluated by both subjective and
Objective Analysis. Experimentally it is found that the proposed method have
larger Peak Signal-to Noise Ratio (PSNR) values, good imperceptibility and
multiple security levels which shows its superiority as compared to several
existing methods
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00411</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00411</id><created>2015-03-01</created><authors><author><keyname>Drmac</keyname><forenames>Zlatko</forenames></author><author><keyname>Gugercin</keyname><forenames>Serkan</forenames></author><author><keyname>Beattie</keyname><forenames>Christopher</forenames></author></authors><title>Vector Fitting for Matrix-valued Rational Approximation</title><categories>math.NA cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vector Fitting (VF) is a popular method of constructing rational approximants
that provides a least squares fit to frequency response measurements. In an
earlier work, we provided an analysis of VF for scalar-valued rational
functions and established a connection with optimal $H_2$ approximation. We
build on this work and extend the previous framework to include the
construction of effective rational approximations to matrix-valued functions, a
problem which presents significant challenges that do not appear in the scalar
case. Transfer functions associated with multi-input/multi-output (MIMO)
dynamical systems typify the class of functions that we consider here. Others
have also considered extensions of VF to matrix-valued functions and related
numerical implementations are readily available. However to our knowledge, a
detailed analysis of numerical issues that arise does not yet exist. We offer
such an analysis including critical implementation details here.
  One important issue that arises for VF on matrix-valued functions that has
remained largely unaddressed is the control of the McMillan degree of the
resulting rational approximant; the McMillan degree can grow very high in the
case of large input/output dimensions. We introduce two new mechanisms for
controlling the McMillan degree of the final approximant, one based on
alternating least-squares minimization and one based on ancillary
system-theoretic reduction methods. Motivated in part by our earlier work on
the scalar VF problem as well as by recent innovations for computing optimal
$H_2$ approximation, we establish a connection with optimal $H_2$
approximation, and are able to improve significantly the fidelity of VF through
numerical quadrature, with virtually no increase in cost or complexity. We
provide several numerical examples to support the theoretical discussion and
proposed algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00423</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00423</id><created>2015-03-02</created><updated>2015-07-20</updated><authors><author><keyname>Cole</keyname><forenames>Sam</forenames></author><author><keyname>Friedland</keyname><forenames>Shmuel</forenames></author><author><keyname>Reyzin</keyname><forenames>Lev</forenames></author></authors><title>A Simple Spectral Algorithm for Recovering Planted Partitions</title><categories>cs.DS cs.DM</categories><comments>17 pages + title page</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the planted partition model, in which $n = ks$
vertices of a random graph are partitioned into $k$ &quot;clusters,&quot; each of size
$s$. Edges between vertices in the same cluster and different clusters are
included with constant probability $p$ and $q$, respectively (where $0 \le q &lt;
p \le 1$). We give an efficient algorithm that, with high probability, recovers
the clustering as long as the cluster sizes are are least $\Omega(\sqrt{n})$.
Our algorithm is based on projecting the graph's adjacency matrix onto the
space spanned by its largest eigenvalues and using the result to recover one
cluster at a time. While certainly not the first to use the spectral approach,
our algorithm is arguably the simplest to do so: there is no need to randomly
partition the vertices beforehand, and hence there is no messy &quot;cleanup&quot; step
at the end. We also use a novel application of the Cauchy integral formula to
prove its correctness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00424</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00424</id><created>2015-03-02</created><updated>2015-03-09</updated><authors><author><keyname>Ge</keyname><forenames>Rong</forenames></author><author><keyname>Huang</keyname><forenames>Qingqing</forenames></author><author><keyname>Kakade</keyname><forenames>Sham M.</forenames></author></authors><title>Learning Mixtures of Gaussians in High Dimensions</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficiently learning mixture of Gaussians is a fundamental problem in
statistics and learning theory. Given samples coming from a random one out of k
Gaussian distributions in Rn, the learning problem asks to estimate the means
and the covariance matrices of these Gaussians. This learning problem arises in
many areas ranging from the natural sciences to the social sciences, and has
also found many machine learning applications. Unfortunately, learning mixture
of Gaussians is an information theoretically hard problem: in order to learn
the parameters up to a reasonable accuracy, the number of samples required is
exponential in the number of Gaussian components in the worst case. In this
work, we show that provided we are in high enough dimensions, the class of
Gaussian mixtures is learnable in its most general form under a smoothed
analysis framework, where the parameters are randomly perturbed from an
adversarial starting point. In particular, given samples from a mixture of
Gaussians with randomly perturbed parameters, when n &gt; {\Omega}(k^2), we give
an algorithm that learns the parameters with polynomial running time and using
polynomial number of samples. The central algorithmic ideas consist of new ways
to decompose the moment tensor of the Gaussian mixture by exploiting its
structural properties. The symmetries of this tensor are derived from the
combinatorial structure of higher order moments of Gaussian distributions
(sometimes referred to as Isserlis' theorem or Wick's theorem). We also develop
new tools for bounding smallest singular values of structured random matrices,
which could be useful in other smoothed analysis settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00426</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00426</id><created>2015-03-02</created><authors><author><keyname>Chen</keyname><forenames>Laming</forenames></author><author><keyname>Gu</keyname><forenames>Yuantao</forenames></author></authors><title>On the Null Space Constant for $l_p$ Minimization</title><categories>cs.IT math.IT</categories><comments>11 pages, 2 figure, journal manuscript</comments><doi>10.1109/LSP.2015.2416003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The literature on sparse recovery often adopts the $l_p$ &quot;norm&quot; $(p\in[0,1])$
as the penalty to induce sparsity of the signal satisfying an underdetermined
linear system. The performance of the corresponding $l_p$ minimization problem
can be characterized by its null space constant. In spite of the NP-hardness of
computing the constant, its properties can still help in illustrating the
performance of $l_p$ minimization. In this letter, we show the strict increase
of the null space constant in the sparsity level $k$ and its continuity in the
exponent $p$. We also indicate that the constant is strictly increasing in $p$
with probability $1$ when the sensing matrix ${\bf A}$ is randomly generated.
Finally, we show how these properties can help in demonstrating the performance
of $l_p$ minimization, mainly in the relationship between the the exponent $p$
and the sparsity level $k$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00434</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00434</id><created>2015-03-02</created><authors><author><keyname>Zhang</keyname><forenames>Suling</forenames></author><author><keyname>Xi</keyname><forenames>Feng</forenames></author><author><keyname>Chen</keyname><forenames>Shengyao</forenames></author><author><keyname>Zhang</keyname><forenames>Yimin D.</forenames></author><author><keyname>Liu</keyname><forenames>Zhong</forenames></author></authors><title>Segment-Sliding Reconstruction of Pulsed Radar Echoes with Sub-Nyquist
  Sampling</title><categories>cs.IT math.IT</categories><comments>13 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been shown that analog-to-information con- version (AIC) is an
efficient scheme to perform sub-Nyquist sampling of pulsed radar echoes.
However, it is often impractical, if not infeasible, to reconstruct full-range
Nyquist samples because of huge storage and computational load requirements.
Based on the analyses of AIC measurement system, this paper develops a novel
segment-sliding reconstruction (SegSR) scheme to effectively reconstruct the
Nyquist samples. The SegSR per- forms segment-by-segment reconstruction in a
sliding mode and can be implemented in real-time. An important characteristic
that distinguish the proposed SegSR from the existing methods is that the
measurement matrix in each segment satisfies the restricted isometry property
(RIP) condition. Partial support in the previous segment can be incorporated
into the estimation of the Nyquist samples in the current segment. The effect
of interference intro- duced from adjacent segments is theoretically analyzed,
and it is revealed that the interference consists of two interference levels
having different impacts to the signal reconstruction performance. With these
observations, a two-step orthogonal matching pursuit (OMP) procedure is
proposed for segment reconstruction, which takes into account different
interference levels and partially known support of the previous segment. The
proposed SegSR achieves nearly optimal reconstruction performance with a signi-
ficant reduction of computational loads and storage requirements. Theoretical
analyses and simulations verify its effectiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00435</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00435</id><created>2015-03-02</created><authors><author><keyname>Shim</keyname><forenames>Taehyoung</forenames></author><author><keyname>Kim</keyname><forenames>Seong-Lyun</forenames></author></authors><title>Robotic Wireless Networks in a Narrow Alley: A Game Theoretic Approach</title><categories>cs.RO cs.GT</categories><comments>4 pages, 6 figures, in Proc. 9th IEEE VTS Asia Pacific Wireless
  Communications Symposium (APWCS), Kyoto, Japan, Aug. 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are many situations where vehicles may compete with each other to
maximize their respective utilities.We consider a narrow alley where two
groups, eastbound and westbound, of autonomous vehicles are heading toward each
of their destination to minimize their travel distance. However, if the two
groups approach the road simultaneously, it will be blocked. The main goal of
this paper is to investigate how wireless communications among the vehicles can
lead the solution near to Pareto optimum. In addition, we implemented such a
vehicular test-bed, composed of networked robots that have an infrared sensor,
a DC motor, and a wireless communication module: ZigBee (IEEE 802.15.4).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00436</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00436</id><created>2015-03-02</created><authors><author><keyname>Xi</keyname><forenames>Feng</forenames></author><author><keyname>Chen</keyname><forenames>Shengyao</forenames></author><author><keyname>Zhang</keyname><forenames>Yimin D.</forenames></author><author><keyname>Liu</keyname><forenames>Zhong</forenames></author></authors><title>Gridless Quadrature Compressive Sampling with Interpolated Array
  Technique</title><categories>cs.IT math.IT</categories><comments>34 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quadrature compressive sampling (QuadCS) is a sub-Nyquist sampling scheme for
acquiring in-phase and quadrature (I/Q) components in radar. In this scheme,
the received intermediate frequency (IF) signals are expressed as a linear
combination of time-delayed and scaled replicas of the transmitted waveforms.
For sparse IF signals on discrete grids of time-delay space, the QuadCS can
efficiently reconstruct the I/Q components from sub-Nyquist samples. In
practice, the signals are characterized by a set of unknown time-delay
parameters in a continuous space. Then conventional sparse signal
reconstruction will deteriorate the QuadCS reconstruction performance. This
paper focuses on the reconstruction of the I/Q components with continuous delay
parameters. A parametric spectrum-matched dictionary is defined, which sparsely
describes the IF signals in the frequency domain by delay parameters and gain
coefficients, and the QuadCS system is reexamined under the new dictionary.
With the inherent structure of the QuadCS system, it is found that the
estimation of delay parameters can be decoupled from that of sparse gain
coefficients, yielding a beamspace direction-of-arrival (DOA) estimation
formulation with a time-varying beamforming matrix. Then an interpolated
beamspace DOA method is developed to perform the DOA estimation. An optimal
interpolated array is established and sufficient conditions to guarantee the
successful estimation of the delay parameters are derived. With the estimated
delays, the gain coefficients can be conveniently determined by solving a
linear least-squares problem. Extensive simulations demonstrate the superior
performance of the proposed algorithm in reconstructing the sparse signals with
continuous delay parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00439</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00439</id><created>2015-03-02</created><authors><author><keyname>Kaur</keyname><forenames>Savneet</forenames></author><author><keyname>Virmani</keyname><forenames>Deepali</forenames></author><author><keyname>Jain</keyname><forenames>Satbir</forenames></author></authors><title>A Novel Framework for Intelligent Information Retrieval in Wireless
  Sensor Networks</title><categories>cs.IR cs.NI</categories><comments>5 pages, 4 figures; ERCICA 2014 - Emerging Research in Computing,
  Information, Communication and Applications (Vol 2), Elsevier Science and
  Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in the development of the low-cost, power-efficient embedded
devices, coupled with the rising need for support of new information processing
paradigms such as smart spaces and military surveillance systems, have led to
active research in large-scale, highly distributed sensor networks of small,
wireless, low-power, unattended sensors and actuators. While applications keep
diversifying, one common property they share is the need for an efficient
network architecture tailored towards information retrieval in sensor networks.
Previous solutions designed for traditional networks serve as good references;
however, due to the vast differences between previous paradigms and needs of
sensor networks, a framework is required to gather and impart only the required
information .To achieve this goal in this paper we have proposed a framework
for intelligent information retrieval and dissemination to desired destination
node. The proposed frame work combines three major concern areas in WSNs i.e.
data aggregation, information retrieval and data dissemination in a single
scenario. In the proposed framework data aggregation is responsible for
combining information from all nodes and removing the redundant data.
Information retrieval filters the processed data to obtain final information
termed as intelligent data to be disseminated to the required destination node.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00445</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00445</id><created>2015-03-02</created><updated>2015-09-09</updated><authors><author><keyname>Traag</keyname><forenames>V. A.</forenames></author><author><keyname>Aldecoa</keyname><forenames>R.</forenames></author><author><keyname>Delvenne</keyname><forenames>J-C.</forenames></author></authors><title>Detecting communities using asymptotical Surprise</title><categories>physics.soc-ph cs.DM cs.SI stat.ME</categories><journal-ref>Phys. Rev. E 92, 022816 (2015)</journal-ref><doi>10.1103/PhysRevE.92.022816</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nodes in real-world networks are repeatedly observed to form dense clusters,
often referred to as communities. Methods to detect these groups of nodes
usually maximize an objective function, which implicitly contains the
definition of a community. We here analyze a recently proposed measure called
surprise, which assesses the quality of the partition of a network into
communities. In its current form, the formulation of surprise is rather
difficult to analyze. We here therefore develop an accurate asymptotic
approximation. This allows for the development of an efficient algorithm for
optimizing surprise. Incidentally, this leads to a straightforward extension of
surprise to weighted graphs. Additionally, the approximation makes it possible
to analyze surprise more closely and compare it to other methods, especially
modularity. We show that surprise is (nearly) unaffected by the well known
resolution limit, a particular problem for modularity. However, surprise may
tend to overestimate the number of communities, whereas they may be
underestimated by modularity. In short, surprise works well in the limit of
many small communities, whereas modularity works better in the limit of few
large communities. In this sense, surprise is more discriminative than
modularity, and may find communities where modularity fails to discern any
structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00448</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00448</id><created>2015-03-02</created><authors><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Li</keyname><forenames>Qiang</forenames></author><author><keyname>Sun</keyname><forenames>Xiaoming</forenames></author><author><keyname>Zhang</keyname><forenames>Jialin</forenames></author></authors><title>The Routing of Complex Contagion in Kleinberg's Small-World Networks</title><categories>cs.SI cs.CC physics.soc-ph</categories><acm-class>J.4; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Kleinberg's small-world network model, strong ties are modeled as
deterministic edges in the underlying base grid and weak ties are modeled as
random edges connecting remote nodes. The probability of connecting a node $u$
with node $v$ through a weak tie is proportional to $1/|uv|^\alpha$, where
$|uv|$ is the grid distance between $u$ and $v$ and $\alpha\ge 0$ is the
parameter of the model. Complex contagion refers to the propagation mechanism
in a network where each node is activated only after $k \ge 2$ neighbors of the
node are activated.
  In this paper, we propose the concept of routing of complex contagion (or
complex routing), where we can activate one node at one time step with the goal
of activating the targeted node in the end. We consider decentralized routing
scheme where only the weak ties from the activated nodes are revealed. We study
the routing time of complex contagion and compare the result with simple
routing and complex diffusion (the diffusion of complex contagion, where all
nodes that could be activated are activated immediately in the same step with
the goal of activating all nodes in the end).
  We show that for decentralized complex routing, the routing time is lower
bounded by a polynomial in $n$ (the number of nodes in the network) for all
range of $\alpha$ both in expectation and with high probability (in particular,
$\Omega(n^{\frac{1}{\alpha+2}})$ for $\alpha \le 2$ and
$\Omega(n^{\frac{\alpha}{2(\alpha+2)}})$ for $\alpha &gt; 2$ in expectation),
while the routing time of simple contagion has polylogarithmic upper bound when
$\alpha = 2$. Our results indicate that complex routing is harder than complex
diffusion and the routing time of complex contagion differs exponentially
compared to simple contagion at sweetspot.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00454</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00454</id><created>2015-03-02</created><authors><author><keyname>Domingo-Ferrer</keyname><forenames>Josep</forenames></author><author><keyname>Wu</keyname><forenames>Qianhong</forenames></author><author><keyname>Blanco-Justicia</keyname><forenames>Alberto</forenames></author></authors><title>Flexible and Robust Privacy-Preserving Implicit Authentication</title><categories>cs.CR</categories><comments>IFIP SEC 2015-Intl. Information Security and Privacy Conference, May
  26-28, 2015, IFIP AICT, Springer, to appear</comments><msc-class>94A60</msc-class><acm-class>D.4.6; K.6.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Implicit authentication consists of a server authenticating a user based on
the user's usage profile, instead of/in addition to relying on something the
user explicitly knows (passwords, private keys, etc.). While implicit
authentication makes identity theft by third parties more difficult, it
requires the server to learn and store the user's usage profile. Recently, the
first privacy-preserving implicit authentication system was presented, in which
the server does not learn the user's profile. It uses an ad hoc two-party
computation protocol to compare the user's fresh sampled features against an
encrypted stored user's profile. The protocol requires storing the usage
profile and comparing against it using two different cryptosystems, one of them
order-preserving; furthermore, features must be numerical. We present here a
simpler protocol based on set intersection that has the advantages of: i)
requiring only one cryptosystem; ii) not leaking the relative order of fresh
feature samples; iii) being able to deal with any type of features (numerical
or non-numerical).
  Keywords: Privacy-preserving implicit authentication, privacy-preserving set
intersection, implicit authentication, active authentication, transparent
authentication, risk mitigation, data brokers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00458</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00458</id><created>2015-03-02</created><authors><author><keyname>Dourado</keyname><forenames>Mitre C.</forenames></author><author><keyname>Sampaio</keyname><forenames>Rudini M.</forenames></author></authors><title>Complexity aspects of the triangle path convexity</title><categories>cs.DM</categories><comments>Submitted to WG 2015</comments><msc-class>05C99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A path $P = v_1, ..., v_t$ is a {\em triangle path} (respectively, {\em
monophonic path}) of $G$ if no edges exist joining vertices $v_i$ and $v_j$ of
$P$ such that $|j - i| &gt; 2$; (respectively, $|j - i| &gt; 1$). A set of vertices
$S$ is {\em convex} in the triangle path convexity (respectively, monophonic
convexity) of $G$ if the vertices of every triangle path (respectively,
monophonic path) joining two vertices of $S$ are in $S$. The cardinality of a
maximum proper convex set of $G$ is the {\em convexity number of $G$} and the
cardinality of a minimum set of vertices whose convex hull is $V(G)$ is the
{\em hull number of $G$}. Our main results are polynomial time algorithms for
determining the convexity number and the hull number of a graph in the triangle
path convexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00477</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00477</id><created>2015-03-02</created><authors><author><keyname>Park</keyname><forenames>Sung Joo</forenames></author><author><keyname>Kim</keyname><forenames>Jong Woo</forenames></author><author><keyname>Lee</keyname><forenames>Hong Joo</forenames></author><author><keyname>Park</keyname><forenames>Hyun Jung</forenames></author><author><keyname>Gloor</keyname><forenames>Peter</forenames></author></authors><title>Behavioral Aspects of Social Network Analysis</title><categories>cs.SI cs.CY</categories><comments>Proceedings of the 5th International Conference on Collaborative
  Innovation Networks COINs15, Tokyo, Japan March 12-14, 2015
  (arXiv:1502.01142)</comments><report-no>coins15/2015/34</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contrary to the structural aspect of conventional social network analysis, a
new method in behavioral analysis is proposed. We define behavioral measures
including self-loops and multiple links and illustrate the behavioral analysis
with the networks of Wikipedia editing. Behavioral social network analysis
provides an explanation of human behavior that may be further extended to the
explanation of culture through social phenomena.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00481</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00481</id><created>2015-03-02</created><authors><author><keyname>Fecher</keyname><forenames>Benedikt</forenames></author><author><keyname>Friesike</keyname><forenames>Sascha</forenames></author><author><keyname>Hebing</keyname><forenames>Marcel</forenames></author><author><keyname>Linek</keyname><forenames>Stephanie</forenames></author><author><keyname>Sauermann</keyname><forenames>Armin</forenames></author></authors><title>A Reputation Economy: Results from an Empirical Survey on Academic Data
  Sharing</title><categories>cs.DL stat.OT</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Academic data sharing is a way for researchers to collaborate and thereby
meet the needs of an increasingly complex research landscape. It enables
researchers to verify results and to pursuit new research questions with &quot;old&quot;
data. It is therefore not surprising that data sharing is advocated by funding
agencies, journals, and researchers alike. We surveyed 2661 individual academic
researchers across all disciplines on their dealings with data, their
publication practices, and motives for sharing or withholding research data.
The results for 1564 valid responses show that researchers across disciplines
recognise the benefit of secondary research data for their own work and for
scientific progress as a whole-still they only practice it in moderation. An
explanation for this evidence could be an academic system that is not driven by
monetary incentives, nor the desire for scientific progress, but by individual
reputation-expressed in (high ranked journal) publications. We label this
system a Reputation Economy. This special economy explains our findings that
show that researchers have a nuanced idea how to provide adequate formal
recognition for making data available to others-namely data citations. We
conclude that data sharing will only be widely adopted among research
professionals if sharing pays in form of reputation. Thus, policy measures that
intend to foster research collaboration need to understand academia as a
reputation economy. Successful measures must value intermediate products, such
as research data, more highly than it is the case now.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00484</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00484</id><created>2015-03-02</created><updated>2015-04-10</updated><authors><author><keyname>Skorski</keyname><forenames>Maciej</forenames></author></authors><title>Simulating Side Information: Better Provable Security for Leakage
  Resilient Stream Ciphers</title><categories>cs.CR cs.CC</categories><comments>Some typos present in the previous version have been corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a distribution $X$, any correlated information $Z$ can be represented
as a randomized function of $X$. However, it might be \emph{extremely
inefficient} when: (a) it involves a lot of computations or (b) a huge amount
of auxiliary randomness is required. We study this problem in the computational
setting, where \emph{efficiently simulating} $Z$ from $X$ becomes possible, if
we accept some mistakes and care only about a restricted class of adversaries.
We prove the following result: for any $X\in\{0,1\}^n$, any correlated
$Z\in\{0,1\}^\ell$ and every choice of $(\epsilon,s)$ there is a randomized
$h:\{0,1\}^n\rightarrow \{0,1\}^\ell$ of complexity $O(s\cdot
2^{2\ell}\epsilon^{-2})$ such that $Z$ and $h{X}$ are
$(\epsilon,s)$-indistinguishable given $X$. This is better than in the original
proof of Pietrzak and Jetchev (TCC'14) and much better for some practically
interesting settings than the alternative bound due to Vadhan and Zheng
(CRYPTO'13). Our approach is also simpler and modular (the standard min-max
theorem combined with an $L_2$-approximation argument). As an application we
give a better security analysis for the leakage-resilient stream cipher from
EUROCRYPT'09, increasing (at any security level) the maximal leakage length by
$33\%$. As a contribution of independent interests, we provide a clear analysis
of the provable security for the best known proof technique of this
construction. In particular we show that, contrarily to what have been
suggested, the key space of AES is not enough for this construction and one
needs a weak PRF with 512 bits of security, for example a weak PRF instantiated
with SHA512. Interestingly, we discover limitations for regularity theorems of
Vadhan and Zheng: in this setting they do not provide recommended security
contrarily to our approach based on the standard min-max theorem (in general,
our security level is up to $50\%$ higher).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00488</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00488</id><created>2015-03-02</created><updated>2015-03-02</updated><authors><author><keyname>Peng</keyname><forenames>Chunlei</forenames></author><author><keyname>Gao</keyname><forenames>Xinbo</forenames></author><author><keyname>Wang</keyname><forenames>Nannan</forenames></author><author><keyname>Li</keyname><forenames>Jie</forenames></author></authors><title>Graphical Representation for Heterogeneous Face Recognition</title><categories>cs.CV</categories><comments>7 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heterogeneous face recognition (HFR) refers to matching non-photograph face
images to face photos for identification. HFR plays an important role in both
biometrics research and industry. In spite of promising progresses achieved in
recent years, HFR is still a challenging problem due to the difficulty to
represent two heterogeneous images in a homogeneous manner. Existing HFR
methods either represent an image ignoring the spatial information, or rely on
a transformation procedure which complicates the recognition task. Considering
these problems, we propose a novel graphical representation based HFR method
(G-HFR) in this paper. Markov networks are deployed to represent heterogeneous
image patches separately, which take the spatial compatibility between
neighboring image patches into consideration. A coupled representation
similarity metric (CRSM) is designed to measure the similarity between obtained
graphical representations. Extensive experiments conducted on two viewed sketch
databases and a forensic sketch database show that the proposed method
outperforms state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00491</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00491</id><created>2015-03-02</created><authors><author><keyname>Berardi</keyname><forenames>Giacomo</forenames></author><author><keyname>Esuli</keyname><forenames>Andrea</forenames></author><author><keyname>Sebastiani</keyname><forenames>Fabrizio</forenames></author></authors><title>Utility-Theoretic Ranking for Semi-Automated Text Classification</title><categories>cs.LG</categories><comments>Forthcoming on ACM Transactions on Knowledge Discovery from Data</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  \emph{Semi-Automated Text Classification} (SATC) may be defined as the task
of ranking a set $\mathcal{D}$ of automatically labelled textual documents in
such a way that, if a human annotator validates (i.e., inspects and corrects
where appropriate) the documents in a top-ranked portion of $\mathcal{D}$ with
the goal of increasing the overall labelling accuracy of $\mathcal{D}$, the
expected increase is maximized. An obvious SATC strategy is to rank
$\mathcal{D}$ so that the documents that the classifier has labelled with the
lowest confidence are top-ranked. In this work we show that this strategy is
suboptimal. We develop new utility-theoretic ranking methods based on the
notion of \emph{validation gain}, defined as the improvement in classification
effectiveness that would derive by validating a given automatically labelled
document. We also propose a new effectiveness measure for SATC-oriented ranking
methods, based on the expected reduction in classification error brought about
by partially validating a list generated by a given ranking method. We report
the results of experiments showing that, with respect to the baseline method
above, and according to the proposed measure, our utility-theoretic ranking
methods can achieve substantially higher expected reductions in classification
error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00493</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00493</id><created>2015-03-02</created><authors><author><keyname>Berthomieu</keyname><forenames>B</forenames><affiliation>LAAS</affiliation></author><author><keyname>Bodeveix</keyname><forenames>J. -P</forenames><affiliation>IRIT</affiliation></author><author><keyname>Zilio</keyname><forenames>S Dal</forenames><affiliation>LAAS</affiliation></author><author><keyname>Filali</keyname><forenames>M</forenames><affiliation>IRIT</affiliation></author><author><keyname>Botlan</keyname><forenames>D Le</forenames><affiliation>LAAS</affiliation></author><author><keyname>Verdier</keyname><forenames>G</forenames><affiliation>IRIT</affiliation></author><author><keyname>Vernadat</keyname><forenames>F</forenames><affiliation>LAAS</affiliation></author></authors><title>Real-Time Model Checking Support for AADL</title><categories>cs.SE cs.LO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a model-checking toolchain for the behavioral verification of
AADL models that takes into account the realtime semantics of the language and
that is compatible with the AADL Behavioral Annex. We give a high-level view of
the tools and transformations involved in the verification process and focus on
the support offered by our framework for checking user-defined properties. We
also describe the experimental results obtained on a significant avionic
demonstrator, that models a network protocol in charge of data communications
between an airplane and ground stations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00503</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00503</id><created>2015-03-02</created><updated>2015-03-22</updated><authors><author><keyname>Panferov</keyname><forenames>Eugene</forenames></author></authors><title>A Next-Generation Data Language Proposal</title><categories>cs.DB cs.PL</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper attempts to explain consequences of the relational calculus not
allowing relations to be domains of relations, and to suggest a solution for
the issue. On the example of SQL we describe the consequent problem of the
multitude of different representations for relations; analyze in detail the
disadvantages of the notions &quot;TABLE&quot; and &quot;FOREIGN KEY&quot;; and propose a complex
solution which includes brand new data language, abandonment of tables as a
representation for relations, and relatively small yet very significant
alteration of the data storage concept, called &quot;multitable index&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00504</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00504</id><created>2015-03-02</created><authors><author><keyname>Thakur</keyname><forenames>Chetan Singh</forenames></author><author><keyname>Hamilton</keyname><forenames>Tara Julia</forenames></author><author><keyname>Tapson</keyname><forenames>Jonathan</forenames></author><author><keyname>Lyon</keyname><forenames>Richard F.</forenames></author><author><keyname>van Schaik</keyname><forenames>Andr&#xe9;</forenames></author></authors><title>FPGA Implementation of the CAR Model of the Cochlea</title><categories>cs.NE cs.AR</categories><comments>ISCAS-2014</comments><doi>10.1109/ISCAS.2014.6865519</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The front end of the human auditory system, the cochlea, converts sound
signals from the outside world into neural impulses transmitted along the
auditory pathway for further processing. The cochlea senses and separates sound
in a nonlinear active fashion, exhibiting remarkable sensitivity and frequency
discrimination. Although several electronic models of the cochlea have been
proposed and implemented, none of these are able to reproduce all the
characteristics of the cochlea, including large dynamic range, large gain and
sharp tuning at low sound levels, and low gain and broad tuning at intense
sound levels. Here, we implement the Cascade of Asymmetric Resonators (CAR)
model of the cochlea on an FPGA. CAR represents the basilar membrane filter in
the Cascade of Asymmetric Resonators with Fast-Acting Compression (CAR-FAC)
cochlear model. CAR-FAC is a neuromorphic model of hearing based on a pole-zero
filter cascade model of auditory filtering. It uses simple nonlinear extensions
of conventional digital filter stages that are well suited to FPGA
implementations, so that we are able to implement up to 1224 cochlear sections
on Virtex-6 FPGA to process sound data in real time. The FPGA implementation of
the electronic cochlea described here may be used as a front-end sound analyser
for various machine-hearing applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00505</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00505</id><created>2015-03-02</created><authors><author><keyname>Thakur</keyname><forenames>Chetan Singh</forenames></author><author><keyname>Hamilton</keyname><forenames>Tara Julia</forenames></author><author><keyname>Wang</keyname><forenames>Runchun</forenames></author><author><keyname>Tapson</keyname><forenames>Jonathan</forenames></author><author><keyname>van Schaik</keyname><forenames>Andr&#xe9;</forenames></author></authors><title>A neuromorphic hardware framework based on population coding</title><categories>cs.NE</categories><comments>In submission to IJCNN2015</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In the biological nervous system, large neuronal populations work
collaboratively to encode sensory stimuli. These neuronal populations are
characterised by a diverse distribution of tuning curves, ensuring that the
entire range of input stimuli is encoded. Based on these principles, we have
designed a neuromorphic system called a Trainable Analogue Block (TAB), which
encodes given input stimuli using a large population of neurons with a
heterogeneous tuning curve profile. Heterogeneity of tuning curves is achieved
using random device mismatches in VLSI (Very Large Scale Integration) process
and by adding a systematic offset to each hidden neuron. Here, we present
measurement results of a single test cell fabricated in a 65nm technology to
verify the TAB framework. We have mimicked a large population of neurons by
re-using measurement results from the test cell by varying offset. We thus
demonstrate the learning capability of the system for various regression tasks.
The TAB system may pave the way to improve the design of analogue circuits for
commercial applications, by rendering circuits insensitive to random mismatch
that arises due to the manufacturing process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00516</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00516</id><created>2015-03-02</created><updated>2016-01-20</updated><authors><author><keyname>Bengua</keyname><forenames>Johann A.</forenames></author><author><keyname>Phien</keyname><forenames>Ho N.</forenames></author><author><keyname>Tuan</keyname><forenames>Hoang D.</forenames></author><author><keyname>Do</keyname><forenames>Minh N.</forenames></author></authors><title>Matrix Product State for Feature Extraction of Higher-Order Tensors</title><categories>cs.CV cs.DS cs.LG</categories><comments>10 pages, 3 figures, updated introduction, submitted to IEEE
  Transactions on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces matrix product state (MPS) decomposition as a
computational tool for extracting features of multidimensional data represented
by higher-order tensors. Regardless of tensor order, MPS extracts its relevant
features to the so-called core tensor of maximum order three which can be used
for classification. Mainly based on a successive sequence of singular value
decompositions (SVD), MPS is quite simple to implement without any recursive
procedure needed for optimizing local tensors. Thus, it leads to substantial
computational savings compared to other tensor feature extraction methods such
as higher-order orthogonal iteration (HOOI) underlying the Tucker decomposition
(TD). Benchmark results show that MPS can reduce significantly the feature
space of data while achieving better classification performance compared to
HOOI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00524</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00524</id><created>2015-03-02</created><authors><author><keyname>Lin</keyname><forenames>Trista</forenames><affiliation>CITI Insa Lyon / Inria Grenoble Rh&#xf4;ne-Alpes</affiliation></author><author><keyname>Rivano</keyname><forenames>Herv&#xe9;</forenames><affiliation>CITI Insa Lyon / Inria Grenoble Rh&#xf4;ne-Alpes</affiliation></author><author><keyname>Mou&#xeb;l</keyname><forenames>Fr&#xe9;d&#xe9;ric Le</forenames><affiliation>CITI,CSE</affiliation></author></authors><title>Router deployment of Streetside Parking Sensor Networks in Urban Areas</title><categories>cs.NI</categories><comments>UM - Urban Modelling Symposium, Oct 2014, Lyon, France.
  \&amp;lt;http://urbanmodelling.sciencesconf.org/\&amp;gt</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The deployment of urban infrastructure is very important for urban sensor
applications. In this paper, we studied and introduced the deployment strategy
of wireless on-street parking sensor networks. We defined a multiple-objective
problem with four objectives, and solved them with real street parking map. The
results show two sets of Pareto Front with the minimum energy consumption,
sensing information delay and the amount of deployed routers and gateways. The
result can be considered to provide urban service roadside unit or be taken
into account while designing a deployment algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00540</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00540</id><created>2015-03-02</created><authors><author><keyname>Altarelli</keyname><forenames>Fabrizio</forenames></author><author><keyname>Braunstein</keyname><forenames>Alfredo</forenames></author><author><keyname>Dall'Asta</keyname><forenames>Luca</forenames></author><author><keyname>De Bacco</keyname><forenames>Caterina</forenames></author><author><keyname>Franz</keyname><forenames>Silvio</forenames></author></authors><title>The edge-disjoint path problem on random graphs by message-passing</title><categories>cond-mat.dis-nn cond-mat.stat-mech cs.DS</categories><comments>14 pages, 8 figures</comments><journal-ref>PLoS ONE 10(12): e0145222 (2015)</journal-ref><doi>10.1371/journal.pone.0145222</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a message-passing algorithm to solve the edge disjoint path
problem (EDP) on graphs incorporating under a unique framework both traffic
optimization and path length minimization. The min-sum equations for this
problem present an exponential computational cost in the number of paths. To
overcome this obstacle we propose an efficient implementation by mapping the
equations onto a weighted combinatorial matching problem over an auxiliary
graph. We perform extensive numerical simulations on random graphs of various
types to test the performance both in terms of path length minimization and
maximization of the number of accommodated paths. In addition, we test the
performance on benchmark instances on various graphs by comparison with
state-of-the-art algorithms and results found in the literature. Our
message-passing algorithm always outperforms the others in terms of the number
of accommodated paths when considering non trivial instances (otherwise it
gives the same trivial results). Remarkably, the largest improvement in
performance with respect to the other methods employed is found in the case of
benchmarks with meshes, where the validity hypothesis behind message-passing is
expected to worsen. In these cases, even though the exact message-passing
equations do not converge, by introducing a reinforcement parameter to force
convergence towards a sub optimal solution, we were able to always outperform
the other algorithms with a peak of 27% performance improvement in terms of
accommodated paths. On random graphs, we numerically observe two separated
regimes: one in which all paths can be accommodated and one in which this is
not possible. We also investigate the behaviour of both the number of paths to
be accommodated and their minimum total length.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00547</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00547</id><created>2015-03-02</created><authors><author><keyname>Kundu</keyname><forenames>Abhisek</forenames></author><author><keyname>Drineas</keyname><forenames>Petros</forenames></author><author><keyname>Magdon-Ismail</keyname><forenames>Malik</forenames></author></authors><title>Recovering PCA from Hybrid-$(\ell_1,\ell_2)$ Sparse Sampling of Data
  Elements</title><categories>cs.IT cs.LG math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses how well we can recover a data matrix when only given a
few of its elements. We present a randomized algorithm that element-wise
sparsifies the data, retaining only a few its elements. Our new algorithm
independently samples the data using sampling probabilities that depend on both
the squares ($\ell_2$ sampling) and absolute values ($\ell_1$ sampling) of the
entries. We prove that the hybrid algorithm recovers a near-PCA reconstruction
of the data from a sublinear sample-size: hybrid-($\ell_1,\ell_2$) inherits the
$\ell_2$-ability to sample the important elements as well as the regularization
properties of $\ell_1$ sampling, and gives strictly better performance than
either $\ell_1$ or $\ell_2$ on their own. We also give a one-pass version of
our algorithm and show experiments to corroborate the theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00555</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00555</id><created>2015-01-26</created><updated>2015-08-16</updated><authors><author><keyname>Ganesan</keyname><forenames>Abhinav</forenames></author><author><keyname>Jaggi</keyname><forenames>Sidharth</forenames></author><author><keyname>Saligrama</keyname><forenames>Venkatesh</forenames></author></authors><title>Learning Immune-Defectives Graph through Group Tests</title><categories>cs.IT math.IT</categories><comments>Double column, 17 pages. Updated with tighter lower bounds and other
  minor edits</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with an abstraction of a unified problem of drug discovery
and pathogen identification. Pathogen identification involves identification of
disease-causing biomolecules. Drug discovery involves finding chemical
compounds, called lead compounds, that bind to pathogenic proteins and
eventually inhibit the function of the protein. In this paper, the lead
compounds are abstracted as inhibitors, pathogenic proteins as defectives, and
the mixture of &quot;ineffective&quot; chemical compounds and non-pathogenic proteins as
normal items. A defective could be immune to the presence of an inhibitor in a
test. So, a test containing a defective is positive iff it does not contain its
&quot;associated&quot; inhibitor. The goal of this paper is to identify the defectives,
inhibitors, and their &quot;associations&quot; with high probability, or in other words,
learn the Immune Defectives Graph (IDG) efficiently through group tests. We
propose a probabilistic non-adaptive pooling design, a probabilistic two-stage
adaptive pooling design and decoding algorithms for learning the IDG. For the
two-stage adaptive-pooling design, we show that the sample complexity of the
number of tests required to guarantee recovery of the inhibitors, defectives,
and their associations with high probability, i.e., the upper bound, exceeds
the proposed lower bound by a logarithmic multiplicative factor in the number
of items. For the non-adaptive pooling design too, we show that the upper bound
exceeds the proposed lower bound by at most a logarithmic multiplicative factor
in the number of items.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00561</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00561</id><created>2015-03-02</created><updated>2015-12-17</updated><authors><author><keyname>Conti</keyname><forenames>Mauro</forenames></author><author><keyname>Guarisco</keyname><forenames>Claudio</forenames></author><author><keyname>Spolaor</keyname><forenames>Riccardo</forenames></author></authors><title>CAPTCHaStar! A novel CAPTCHA based on interactive shape discovery</title><categories>cs.HC</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the last years, most websites on which users can register (e.g., email
providers and social networks) adopted CAPTCHAs (Completely Automated Public
Turing test to tell Computers and Humans Apart) as a countermeasure against
automated attacks. The battle of wits between designers and attackers of
CAPTCHAs led to current ones being annoying and hard to solve for users, while
still being vulnerable to automated attacks.
  In this paper, we propose CAPTCHaStar, a new image-based CAPTCHA that relies
on user interaction. This novel CAPTCHA leverages the innate human ability to
recognize shapes in a confused environment. We assess the effectiveness of our
proposal for the two key aspects for CAPTCHAs, i.e., usability, and resiliency
to automated attacks. In particular, we evaluated the usability, carrying out a
thorough user study, and we tested the resiliency of our proposal against
several types of automated attacks: traditional ones; designed ad-hoc for our
proposal; and based on machine learning. Compared to the state of the art, our
proposal is more user friendly (e.g., only some 35% of the users prefer current
solutions, such as text-based CAPTCHAs) and more resilient to automated
attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00571</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00571</id><created>2015-03-02</created><authors><author><keyname>Lozin</keyname><forenames>Vadim</forenames></author><author><keyname>Razgon</keyname><forenames>Igor</forenames></author><author><keyname>Zamaraev</keyname><forenames>Viktor</forenames></author></authors><title>Well-quasi-ordering does not imply bounded clique-width</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a hereditary class of graphs of unbounded clique-width which is
well-quasi-ordered by the induced subgraph relation. This result provides a
negative answer to the question asked by Daligault, Rao and Thomass\'e in
(&quot;Well-quasi-order of relabel functions&quot;, Order, 27(3):301--315, 2010).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00576</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00576</id><created>2015-03-02</created><authors><author><keyname>Polak</keyname><forenames>Adam</forenames></author></authors><title>Counting Triangles in Large Graphs on GPU</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The clustering coefficient and the transitivity ratio are concepts often used
in network analysis, which creates a need for fast practical algorithms for
counting triangles in large graphs. Previous research in this area focused on
sequential algorithms, MapReduce parallelization, and fast approximations.
  In this paper we propose a parallel triangle counting algorithm for CUDA GPU.
We describe the implementation details necessary to achieve high performance
and present the experimental evaluation of our approach. Our algorithm achieves
8 to 15 times speedup over the CPU implementation and is capable of finding 3.8
billion triangles in an 89 million edges graph in less than 10 seconds on the
Nvidia Tesla C2050 GPU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00582</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00582</id><created>2015-03-02</created><authors><author><keyname>Saket</keyname><forenames>Bahador</forenames></author><author><keyname>Scheidegger</keyname><forenames>Carlos</forenames></author><author><keyname>Kobourov</keyname><forenames>Stephen</forenames></author></authors><title>Towards Understanding Enjoyment and Flow in Information Visualization</title><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditionally, evaluation studies in information visualization have measured
effectiveness by assessing performance time and accuracy. More recently, there
has been a concerted effort to understand aspects beyond time and errors. In
this paper we study enjoyment, which, while arguably not the primary goal of
visualization, has been shown to impact performance and memorability. Different
models of enjoyment have been proposed in psychology, education and gaming; yet
there is no standard approach to evaluate and measure enjoyment in
visualization. In this paper we relate the flow model of Csikszentmihalyi to
Munzner's nested model of visualization evaluation and previous work in the
area. We suggest that, even though previous papers tackled individual elements
of flow, in order to understand what specifically makes a visualization
enjoyable, it might be necessary to measure all specific elements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00586</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00586</id><created>2015-03-02</created><updated>2015-08-03</updated><authors><author><keyname>Grimm</keyname><forenames>Giso</forenames></author><author><keyname>Ewert</keyname><forenames>Stephan</forenames></author><author><keyname>Hohmann</keyname><forenames>Volker</forenames></author></authors><title>Evaluation of spatial audio reproduction schemes for application in
  hearing aid research</title><categories>cs.SD</categories><comments>The archived file is not the final published version of the article
  Evaluation of spatial audio reproduction schemes for application in hearing
  aid research, in Acta Acustica united with Acustica, volume 101, 2015, pp.
  842-854(13). The definitive publisher-authenticated version is available
  online at http://www.ingentaconnect.com/content/dav/aaua.
  http://dx.doi.org/10.3813/AAA.918878</comments><doi>10.3813/AAA.918878</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Loudspeaker-based spatial audio reproduction schemes are increasingly used
for evaluating hearing aids in complex acoustic conditions. To further
establish the feasibility of this approach, this study investigated the
interaction between spatial resolution of different reproduction methods and
technical and perceptual hearing aid performance measures using computer
simulations. Three spatial audio reproduction methods -- discrete speakers,
vector base amplitude panning and higher order ambisonics -- were compared in
regular circular loudspeaker arrays with 4 to 72 channels. The influence of
reproduction method and array size on performance measures of representative
multi-microphone hearing aid algorithm classes with spatially distributed
microphones and a representative single channel noise-reduction algorithm was
analyzed. Algorithm classes differed in their way of analyzing and exploiting
spatial properties of the sound field, requiring different accuracy of sound
field reproduction. Performance measures included beam pattern analysis,
signal-to-noise ratio analysis, perceptual localization prediction, and quality
modeling. The results show performance differences and interaction effects
between reproduction method and algorithm class that may be used for guidance
when selecting the appropriate method and number of speakers for specific tasks
in hearing aid research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00587</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00587</id><created>2015-02-24</created><authors><author><keyname>Reps</keyname><forenames>Jenna</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author><author><keyname>Garibaldi</keyname><forenames>Jonathan</forenames></author><author><keyname>Damski</keyname><forenames>Chris</forenames></author></authors><title>Personalising Mobile Advertising Based on Users Installed Apps</title><categories>cs.CY cs.LG</categories><comments>IEEE International Conference of Data Mining: The 4th International
  Workshop on Data Mining for Service (DMS), 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile advertising is a billion pound industry that is rapidly expanding. The
success of an advert is measured based on how users interact with it. In this
paper we investigate whether the application of unsupervised learning and
association rule mining could be used to enable personalised targeting of
mobile adverts with the aim of increasing the interaction rate. Over May and
June 2014 we recorded advert interactions such as tapping the advert or
watching the whole advert video along with the set of apps a user has installed
at the time of the interaction. Based on the apps that the users have installed
we applied k-means clustering to profile the users into one of ten classes. Due
to the large number of apps considered we implemented dimension reduction to
reduced the app feature space by mapping the apps to their iTunes category and
clustered users based on the percentage of their apps that correspond to each
iTunes app category. The clustering was externally validated by investigating
differences between the way the ten profiles interact with the various adverts
genres (lifestyle, finance and entertainment adverts). In addition association
rule mining was performed to find whether the time of the day that the advert
is served and the number of apps a user has installed makes certain profiles
more likely to interact with the advert genres. The results showed there were
clear differences in the way the profiles interact with the different advert
genres and the results of this paper suggest that mobile advert targeting would
improve the frequency that users interact with an advert.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00591</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00591</id><created>2015-03-02</created><authors><author><keyname>Zhang</keyname><forenames>Xu</forenames></author><author><keyname>Yu</keyname><forenames>Felix Xinnan</forenames></author><author><keyname>Chang</keyname><forenames>Shih-Fu</forenames></author><author><keyname>Wang</keyname><forenames>Shengjin</forenames></author></authors><title>Deep Transfer Network: Unsupervised Domain Adaptation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Domain adaptation aims at training a classifier in one dataset and applying
it to a related but not identical dataset. One successfully used framework of
domain adaptation is to learn a transformation to match both the distribution
of the features (marginal distribution), and the distribution of the labels
given features (conditional distribution). In this paper, we propose a new
domain adaptation framework named Deep Transfer Network (DTN), where the highly
flexible deep neural networks are used to implement such a distribution
matching process.
  This is achieved by two types of layers in DTN: the shared feature extraction
layers which learn a shared feature subspace in which the marginal
distributions of the source and the target samples are drawn close, and the
discrimination layers which match conditional distributions by classifier
transduction. We also show that DTN has a computation complexity linear to the
number of training samples, making it suitable to large-scale problems. By
combining the best paradigms in both worlds (deep neural networks in
recognition, and matching marginal and conditional distributions in domain
adaptation), we demonstrate by extensive experiments that DTN improves
significantly over former methods in both execution time and classification
accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00593</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00593</id><created>2015-03-02</created><updated>2015-04-12</updated><authors><author><keyname>Sun</keyname><forenames>Jian</forenames></author><author><keyname>Cao</keyname><forenames>Wenfei</forenames></author><author><keyname>Xu</keyname><forenames>Zongben</forenames></author><author><keyname>Ponce</keyname><forenames>Jean</forenames></author></authors><title>Learning a Convolutional Neural Network for Non-uniform Motion Blur
  Removal</title><categories>cs.CV</categories><comments>This is a final version accepted by CVPR 2015</comments><acm-class>I.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the problem of estimating and removing non-uniform
motion blur from a single blurry image. We propose a deep learning approach to
predicting the probabilistic distribution of motion blur at the patch level
using a convolutional neural network (CNN). We further extend the candidate set
of motion kernels predicted by the CNN using carefully designed image
rotations. A Markov random field model is then used to infer a dense
non-uniform motion blur field enforcing motion smoothness. Finally, motion blur
is removed by a non-uniform deblurring model using patch-level image prior.
Experimental evaluations show that our approach can effectively estimate and
remove complex non-uniform motion blur that is not handled well by previous
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00600</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00600</id><created>2015-03-02</created><authors><author><keyname>Wang</keyname><forenames>Weiran</forenames></author></authors><title>An $\mathcal{O}(n\log n)$ projection operator for weighted $\ell_1$-norm
  regularization with sum constraint</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a simple and efficient algorithm for the projection operator for
weighted $\ell_1$-norm regularization subject to a sum constraint, together
with an elementary proof. The implementation of the proposed algorithm can be
downloaded from the author's homepage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00603</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00603</id><created>2015-03-02</created><authors><author><keyname>Heck</keyname><forenames>D. J. F.</forenames></author><author><keyname>Saccon</keyname><forenames>A.</forenames></author><author><keyname>van de Wouw</keyname><forenames>N.</forenames></author><author><keyname>Nijmeijer</keyname><forenames>H.</forenames></author></authors><title>Switching control for tracking of a hybrid position-force trajectory</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work proposes a control law for a manipulator with the aim of realizing
desired time-varying motion-force profiles in the presence of a stiff
environment. In many cases, the interaction with the environment affects only
one degree of freedom of the end-effector of the manipulator. Therefore, the
focus is on this contact degree of freedom, and a switching position-force
controller is proposed to perform the hybrid position-force tracking task.
Sufficient conditions are presented to guarantee input-to-state stability of
the switching closed-loop system with respect to perturbations related to the
time-varying desired motion-force profile. The switching occurs when the
manipulator makes or breaks contact with the environment. The analysis shows
that to guarantee closed-loop stability while tracking arbitrary time-varying
motion-force profiles, the controller should implement a considerable (and
often unrealistic) amount of damping, resulting in inferior tracking
performance. Therefore, we propose to redesign the manipulator with a compliant
wrist. Guidelines are provided for the design of the compliant wrist while
employing the designed switching control strategy, such that stable tracking of
a motion-force reference trajectory can be achieved and bouncing of the
manipulator while making contact with the stiff environment can be avoided.
Finally, numerical simulations are presented to illustrate the effectiveness of
the approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00604</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00604</id><created>2015-03-02</created><authors><author><keyname>Li</keyname><forenames>Pei</forenames></author><author><keyname>Dong</keyname><forenames>Xin Luna</forenames></author><author><keyname>Guo</keyname><forenames>Songtao</forenames></author><author><keyname>Maurino</keyname><forenames>Andrea</forenames></author><author><keyname>Srivastava</keyname><forenames>Divesh</forenames></author></authors><title>Robust Group Linkage</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of group linkage: linking records that refer to entities
in the same group. Applications for group linkage include finding businesses in
the same chain, finding conference attendees from the same affiliation, finding
players from the same team, etc. Group linkage faces challenges not present for
traditional record linkage. First, although different members in the same group
can share some similar global values of an attribute, they represent different
entities so can also have distinct local values for the same or different
attributes, requiring a high tolerance for value diversity. Second, groups can
be huge (with tens of thousands of records), requiring high scalability even
after using good blocking strategies.
  We present a two-stage algorithm: the first stage identifies cores containing
records that are very likely to belong to the same group, while being robust to
possible erroneous values; the second stage collects strong evidence from the
cores and leverages it for merging more records into the same group, while
being tolerant to differences in local values of an attribute. Experimental
results show the high effectiveness and efficiency of our algorithm on various
real-world data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00609</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00609</id><created>2015-03-02</created><updated>2015-04-04</updated><authors><author><keyname>Abbe</keyname><forenames>Emmanuel</forenames></author><author><keyname>Sandon</keyname><forenames>Colin</forenames></author></authors><title>Community detection in general stochastic block models: fundamental
  limits and efficient recovery algorithms</title><categories>math.PR cs.IT cs.SI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  New phase transition phenomena have recently been discovered for the
stochastic block model, for the special case of two non-overlapping symmetric
communities. This gives raise in particular to new algorithmic challenges
driven by the thresholds. This paper investigates whether a general phenomenon
takes place for multiple communities, without imposing symmetry.
  In the general stochastic block model $\text{SBM}(n,p,Q)$, $n$ vertices are
split into $k$ communities of relative size $\{p_i\}_{i \in [k]}$, and vertices
in community $i$ and $j$ connect independently with probability
$\{Q_{i,j}\}_{i,j \in [k]}$. This paper investigates the partial and exact
recovery of communities in the general SBM (in the constant and logarithmic
degree regimes), and uses the generality of the results to tackle overlapping
communities.
  The contributions of the paper are: (i) an explicit characterization of the
recovery threshold in the general SBM in terms of a new divergence function
$D_+$, which generalizes the Hellinger and Chernoff divergences, and which
provides an operational meaning to a divergence function analog to the
KL-divergence in the channel coding theorem, (ii) the development of an
algorithm that recovers the communities all the way down to the optimal
threshold and runs in quasi-linear time, showing that exact recovery has no
information-theoretic to computational gap for multiple communities, in
contrast to the conjectures made for detection with more than 4 communities;
note that the algorithm is optimal both in terms of achieving the threshold and
in having quasi-linear complexity, (iii) the development of an efficient
algorithm that detects communities in the constant degree regime with an
explicit accuracy bound that can be made arbitrarily close to 1 when a
prescribed signal-to-noise ratio (defined in term of the spectrum of
$\diag(p)Q$) tends to infinity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00617</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00617</id><created>2015-03-02</created><authors><author><keyname>F&#xfc;rer</keyname><forenames>Martin</forenames></author></authors><title>Efficient Computation of the Characteristic Polynomial of a Threshold
  Graph</title><categories>cs.DS</categories><acm-class>F.2.1; G.2.2; I.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An efficient algorithm is presented to compute the characteristic polynomial
of a threshold graph. Threshold graphs were introduced by Chv\'atal and Hammer,
as well as by Henderson and Zalcstein in 1977. A threshold graph is obtained
from a one vertex graph by repeatedly adding either an isolated vertex or a
dominating vertex, which is a vertex adjacent to all the other vertices.
Threshold graphs are special kinds of cographs, which themselves are special
kinds of graphs of clique-width 2. We obtain a running time of $O(n \log^2 n)$
for computing the characteristic polynomial, while the previously fastest
algorithm ran in quadratic time. Keywords: Efficient Algorithms, Threshold
Graphs, Characteristic Polynomial.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00622</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00622</id><created>2015-03-02</created><updated>2015-07-12</updated><authors><author><keyname>Zaichenkov</keyname><forenames>Pavel</forenames></author><author><keyname>Tveretina</keyname><forenames>Olga</forenames></author><author><keyname>Shafarenko</keyname><forenames>Alex</forenames></author></authors><title>Interface Reconciliation in Kahn Process Networks using CSP and SAT</title><categories>cs.PL</categories><comments>20 pages, 3 figures, accepted to CSPSAT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new CSP- and SAT-based approach for coordinating interfaces of
distributed stream-connected components provided as closed-source services. The
Kahn Process Network (KPN) is taken as a formal model of computation and a
Message Definition Language (MDL) is introduced to describe the format of
messages communicated between the processes. MDL links input and output
interfaces of a node to support flow inheritance and contextualisation. Since
interfaces can also be linked by the existence of a data channel between them,
the match is generally not only partial but also substantially nonlocal. The
KPN communication graph thus becomes a graph of interlocked constraints to be
satisfied by specific instances of the variables. We present an algorithm that
solves the CSP by iterative approximation while generating an adjunct Boolean
SAT problem on the way. We developed a solver in OCaml as well as tools that
analyse the source code of KPN vertices to derive MDL terms and automatically
modify the code by propagating type definitions back to the vertices after the
CSP has been solved. Techniques and approaches are illustrated on a KPN
implementing an image processing algorithm as a running example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00623</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00623</id><created>2015-03-02</created><updated>2015-04-26</updated><authors><author><keyname>Ying</keyname><forenames>Yiming</forenames></author><author><keyname>Zhou</keyname><forenames>Ding-Xuan</forenames></author></authors><title>Unregularized Online Learning Algorithms with General Loss Functions</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider unregularized online learning algorithms in a
Reproducing Kernel Hilbert Spaces (RKHS). Firstly, we derive explicit
convergence rates of the unregularized online learning algorithms for
classification associated with a general gamma-activating loss (see Definition
1 in the paper). Our results extend and refine the results in Ying and Pontil
(2008) for the least-square loss and the recent result in Bach and Moulines
(2011) for the loss function with a Lipschitz-continuous gradient. Moreover, we
establish a very general condition on the step sizes which guarantees the
convergence of the last iterate of such algorithms. Secondly, we establish, for
the first time, the convergence of the unregularized pairwise learning
algorithm with a general loss function and derive explicit rates under the
assumption of polynomially decaying step sizes. Concrete examples are used to
illustrate our main results. The main techniques are tools from convex
analysis, refined inequalities of Gaussian averages, and an induction approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00626</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00626</id><created>2015-03-02</created><authors><author><keyname>Yan</keyname><forenames>Da</forenames></author><author><keyname>Cheng</keyname><forenames>James</forenames></author><author><keyname>Lu</keyname><forenames>Yi</forenames></author><author><keyname>Ng</keyname><forenames>Wilfred</forenames></author></authors><title>Effective Techniques for Message Reduction and Load Balancing in
  Distributed Graph Computation</title><categories>cs.DC</categories><comments>This is a long version of the corresponding WWW 2015 paper, with all
  proofs included</comments><acm-class>D.4.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive graphs, such as online social networks and communication networks,
have become common today. To efficiently analyze such large graphs, many
distributed graph computing systems have been developed. These systems employ
the &quot;think like a vertex&quot; programming paradigm, where a program proceeds in
iterations and at each iteration, vertices exchange messages with each other.
However, using Pregel's simple message passing mechanism, some vertices may
send/receive significantly more messages than others due to either the high
degree of these vertices or the logic of the algorithm used. This forms the
communication bottleneck and leads to imbalanced workload among machines in the
cluster. In this paper, we propose two effective message reduction techniques:
(1)vertex mirroring with message combining, and (2)an additional
request-respond API. These techniques not only reduce the total number of
messages exchanged through the network, but also bound the number of messages
sent/received by any single vertex. We theoretically analyze the effectiveness
of our techniques, and implement them on top of our open-source Pregel
implementation called Pregel+. Our experiments on various large real graphs
demonstrate that our message reduction techniques significantly improve the
performance of distributed graph computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00628</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00628</id><created>2015-03-02</created><authors><author><keyname>Pfander</keyname><forenames>G&#xf6;tz E.</forenames></author><author><keyname>Walnut</keyname><forenames>David F.</forenames></author></authors><title>Sampling and reconstruction of operators</title><categories>cs.IT math.FA math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the recovery of operators with bandlimited Kohn-Nirenberg symbol
from the action of such operators on a weighted impulse train, a procedure we
refer to as operator sampling. Kailath, and later Kozek and the authors have
shown that operator sampling is possible if the symbol of the operator is
bandlimited to a set with area less than one. In this paper we develop explicit
reconstruction formulas for operator sampling that generalize reconstruction
formulas for bandlimited functions. We give necessary and sufficient conditions
on the sampling rate that depend on size and geometry of the bandlimiting set.
Moreover, we show that under mild geometric conditions, classes of operators
bandlimited to an unknown set of area less than one-half permit sampling and
reconstruction. A similar result considering unknown sets of area less than one
was independently achieved by Heckel and Boelcskei.
  Operators with bandlimited symbols have been used to model doubly dispersive
communication channels with slowly-time-varying impulse response. The results
in this paper are rooted in work by Bello and Kailath in the 1960s.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00648</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00648</id><created>2015-03-02</created><authors><author><keyname>Sermpezis</keyname><forenames>Pavlos</forenames></author><author><keyname>Vigneri</keyname><forenames>Luigi</forenames></author><author><keyname>Spyropoulos</keyname><forenames>Thrasyvoulos</forenames></author></authors><title>Offloading on the Edge: Analysis and Optimization of Local Data Storage
  and Offloading in HetNets</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rapid increase in data traffic demand has overloaded existing cellular
networks. Planned upgrades in the communication architecture (e.g. LTE), while
helpful, are not expected to suffice to keep up with demand. As a result,
extensive densification through small cells, caching content closer to or even
at the device, and device-to-device (D2D) communications are seen as necessary
components for future heterogeneous cellular networks to withstand the data
crunch. Nevertheless, these options imply new CAPEX and OPEX costs, extensive
backhaul support, contract plan incentives for D2D, and a number of interesting
tradeoffs arise for the operator. In this paper, we propose an analytical model
to explore how much local storage and communication through &quot;edge&quot; nodes could
help offload traffic in various heterogeneous network (HetNet) setups and
levels of user tolerance to delays. We then use this model to optimize the
storage allocation and access mode of different contents as a tradeoff between
user satisfaction and cost to the operator. Finally, we validate our findings
through realistic simulations and show that considerable amounts of traffic can
be offloaded even under moderate densification levels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00650</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00650</id><created>2015-03-02</created><authors><author><keyname>Afrati</keyname><forenames>Foto N.</forenames></author><author><keyname>Kolaitis</keyname><forenames>Phokion G.</forenames></author><author><keyname>Vasilakopoulos</keyname><forenames>Angelos</forenames></author></authors><title>Consistent Answers of Conjunctive Queries on Graphs</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During the past decade, there has been an extensive investigation of the
computational complexity of the consistent answers of Boolean conjunctive
queries under primary key constraints. Much of this investigation has focused
on self-join-free Boolean conjunctive queries. In this paper, we study the
consistent answers of Boolean conjunctive queries involving a single binary
relation, i.e., we consider arbitrary Boolean conjunctive queries on directed
graphs. In the presence of a single key constraint, we show that for each such
Boolean conjunctive query, either the problem of computing its consistent
answers is expressible in first-order logic, or it is polynomial-time solvable,
but not expressible in first-order logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00656</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00656</id><created>2015-03-02</created><authors><author><keyname>Masouros</keyname><forenames>Christos</forenames></author><author><keyname>Matthaiou</keyname><forenames>Michail</forenames></author></authors><title>Space-Constrained Massive MIMO: Hitting the Wall of Favorable
  Propagation</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent development of the massive multiple-input multiple-output (MIMO)
paradigm, has been extensively based on the pursuit of favorable propagation:
in the asymptotic limit, the channel vectors become nearly orthogonal and
inter-user interference tends to zero [1]. In this context, previous studies
have considered fixed inter-antenna distance, which implies an increasing array
aperture as the number of elements increases. Here, we focus on a practical,
space-constrained topology, where an increase in the number of antenna elements
in a fixed total space imposes an inversely proportional decrease in the
inter-antenna distance. Our analysis shows that, contrary to existing studies,
inter-user interference does not vanish in the massive MIMO regime, thereby
creating a saturation effect on the achievable rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00658</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00658</id><created>2015-03-02</created><authors><author><keyname>Mitzenmacher</keyname><forenames>Michael</forenames></author></authors><title>More Analysis of Double Hashing for Balanced Allocations</title><categories>cs.DS</categories><comments>13 pages ; current draft ; will be submitted to conference shortly</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With double hashing, for a key $x$, one generates two hash values $f(x)$ and
$g(x)$, and then uses combinations $(f(x) +i g(x)) \bmod n$ for $i=0,1,2,...$
to generate multiple hash values in the range $[0,n-1]$ from the initial two.
For balanced allocations, keys are hashed into a hash table where each bucket
can hold multiple keys, and each key is placed in the least loaded of $d$
choices. It has been shown previously that asymptotically the performance of
double hashing and fully random hashing is the same in the balanced allocation
paradigm using fluid limit methods. Here we extend a coupling argument used by
Lueker and Molodowitch to show that double hashing and ideal uniform hashing
are asymptotically equivalent in the setting of open address hash tables to the
balanced allocation setting, providing further insight into this phenomenon. We
also discuss the potential for and bottlenecks limiting the use this approach
for other multiple choice hashing schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00669</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00669</id><created>2015-03-02</created><authors><author><keyname>Pehlevan</keyname><forenames>Cengiz</forenames></author><author><keyname>Hu</keyname><forenames>Tao</forenames></author><author><keyname>Chklovskii</keyname><forenames>Dmitri B.</forenames></author></authors><title>A Hebbian/Anti-Hebbian Neural Network for Linear Subspace Learning: A
  Derivation from Multidimensional Scaling of Streaming Data</title><categories>q-bio.NC cs.NE stat.ML</categories><comments>Accepted for publication in Neural Computation</comments><doi>10.1162/NECO_a_00745</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural network models of early sensory processing typically reduce the
dimensionality of streaming input data. Such networks learn the principal
subspace, in the sense of principal component analysis (PCA), by adjusting
synaptic weights according to activity-dependent learning rules. When derived
from a principled cost function these rules are nonlocal and hence biologically
implausible. At the same time, biologically plausible local rules have been
postulated rather than derived from a principled cost function. Here, to bridge
this gap, we derive a biologically plausible network for subspace learning on
streaming data by minimizing a principled cost function. In a departure from
previous work, where cost was quantified by the representation, or
reconstruction, error, we adopt a multidimensional scaling (MDS) cost function
for streaming data. The resulting algorithm relies only on biologically
plausible Hebbian and anti-Hebbian local learning rules. In a stochastic
setting, synaptic weights converge to a stationary state which projects the
input data onto the principal subspace. If the data are generated by a
nonstationary distribution, the network can track the principal subspace. Thus,
our result makes a step towards an algorithmic theory of neural computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00673</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00673</id><created>2015-02-13</created><updated>2016-01-04</updated><authors><author><keyname>Rotolo</keyname><forenames>Daniele</forenames></author><author><keyname>Hicks</keyname><forenames>Diana</forenames></author><author><keyname>Martin</keyname><forenames>Ben R.</forenames></author></authors><title>What Is an Emerging Technology?</title><categories>cs.OH</categories><comments>Research Policy (in press)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is considerable and growing interest in the emergence of novel
technologies, especially from the policy-making perspective. Yet as an area of
study, emerging technologies lacks key foundational elements, namely a
consensus on what classifies a technology as 'emergent' and strong research
designs that operationalize central theoretical concepts. The present paper
aims to fill this gap by developing a definition of 'emerging technologies' and
linking this conceptual effort with the development of a framework for the
operationalisation of technological emergence. The definition is developed by
combining a basic understanding of the term and in particular the concept of
'emergence' with a review of key innovation studies dealing with definitional
issues of technological emergence. The resulting definition identifies five
attributes that feature in the emergence of novel technologies. These are: (i)
radical novelty, (ii) relatively fast growth, (iii) coherence, (iv) prominent
impact, and (v) uncertainty and ambiguity. The framework for operationalising
emerging technologies is then elaborated on the basis of the proposed
attributes. To do so, we identify and review major empirical approaches (mainly
in, although not limited to, the scientometric domain) for the detection and
study of emerging technologies (these include indicators and trend analysis,
citation analysis, co-word analysis, overlay mapping, and combinations thereof)
and elaborate on how these can be used to operationalise the different
attributes of emergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00674</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00674</id><created>2015-02-16</created><authors><author><keyname>Hossain</keyname><forenames>Ekram</forenames></author><author><keyname>Hasan</keyname><forenames>Monowar</forenames></author></authors><title>5G Cellular: Key Enabling Technologies and Research Challenges</title><categories>cs.NI</categories><comments>IEEE Instrumentation and Measurement Magazine, to appear in the June
  2015 issue. arXiv admin note: text overlap with arXiv:1406.6470 by other
  authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The evolving fifth generation (5G) cellular wireless networks are envisioned
to provide higher data rates, enhanced end-user quality-of-experience (QoE),
reduced end-to-end latency, and lower energy consumption. This article presents
several emerging technologies, which will enable and define the 5G mobile
communications standards. The major research problems, which these new
technologies breed, as well as the measurement and test challenges for 5G
systems are also highlighted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00680</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00680</id><created>2015-03-02</created><authors><author><keyname>Pehlevan</keyname><forenames>Cengiz</forenames></author><author><keyname>Chklovskii</keyname><forenames>Dmitri B.</forenames></author></authors><title>A Hebbian/Anti-Hebbian Network Derived from Online Non-Negative Matrix
  Factorization Can Cluster and Discover Sparse Features</title><categories>q-bio.NC cs.NE stat.ML</categories><comments>2014 Asilomar Conference on Signals, Systems and Computers</comments><doi>10.1109/ACSSC.2014.7094553</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite our extensive knowledge of biophysical properties of neurons, there
is no commonly accepted algorithmic theory of neuronal function. Here we
explore the hypothesis that single-layer neuronal networks perform online
symmetric nonnegative matrix factorization (SNMF) of the similarity matrix of
the streamed data. By starting with the SNMF cost function we derive an online
algorithm, which can be implemented by a biologically plausible network with
local learning rules. We demonstrate that such network performs soft clustering
of the data as well as sparse feature discovery. The derived algorithm
replicates many known aspects of sensory anatomy and biophysical properties of
neurons including unipolar nature of neuronal activity and synaptic weights,
local synaptic plasticity rules and the dependence of learning rate on
cumulative neuronal activity. Thus, we make a step towards an algorithmic
theory of neuronal function, which should facilitate large-scale neural circuit
simulations and biologically inspired artificial intelligence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00687</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00687</id><created>2015-03-02</created><authors><author><keyname>Carreira-Perpi&#xf1;&#xe1;n</keyname><forenames>Miguel &#xc1;.</forenames></author></authors><title>A review of mean-shift algorithms for clustering</title><categories>cs.LG cs.CV stat.ML</categories><comments>28 pages, 9 figures. Invited book chapter to appear in the CRC
  Handbook of Cluster Analysis (eds. Roberto Rocci, Fionn Murtagh, Marina Meila
  and Christian Hennig)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A natural way to characterize the cluster structure of a dataset is by
finding regions containing a high density of data. This can be done in a
nonparametric way with a kernel density estimate, whose modes and hence
clusters can be found using mean-shift algorithms. We describe the theory and
practice behind clustering based on kernel density estimates and mean-shift
algorithms. We discuss the blurring and non-blurring versions of mean-shift;
theoretical results about mean-shift algorithms and Gaussian mixtures;
relations with scale-space theory, spectral clustering and other algorithms;
extensions to tracking, to manifold and graph data, and to manifold denoising;
K-modes and Laplacian K-modes algorithms; acceleration strategies for large
datasets; and applications to image segmentation, manifold denoising and
multivalued regression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00688</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00688</id><created>2015-02-20</created><updated>2015-07-21</updated><authors><author><keyname>Zhang</keyname><forenames>Zhilin</forenames></author></authors><title>Photoplethysmography-Based Heart Rate Monitoring in Physical Activities
  via Joint Sparse Spectrum Reconstruction</title><categories>cs.OH cs.CY stat.AP</categories><comments>Published in IEEE Transactions on Biomedical Engineering, Vol. 62,
  No. 8, PP. 1902-1910, August 2015</comments><journal-ref>IEEE Transactions on Biomedical Engineering, Vol. 62, No. 8, PP.
  1902-1910, August 2015</journal-ref><doi>10.1109/TBME.2015.2406332</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Goal: A new method for heart rate monitoring using photoplethysmography (PPG)
during physical activities is proposed. Methods: It jointly estimates spectra
of PPG signals and simultaneous acceleration signals, utilizing the multiple
measurement vector model in sparse signal recovery. Due to a common sparsity
constraint on spectral coefficients, the method can easily identify and remove
spectral peaks of motion artifact (MA) in PPG spectra. Thus, it does not need
any extra signal processing modular to remove MA as in some other algorithms.
Furthermore, seeking spectral peaks associated with heart rate is simplified.
Results: Experimental results on 12 PPG datasets sampled at 25 Hz and recorded
during subjects' fast running showed that it had high performance. The average
absolute estimation error was 1.28 beat per minute and the standard deviation
was 2.61 beat per minute. Conclusion and Significance: These results show that
the method has great potential to be used for PPG-based heart rate monitoring
in wearable devices for fitness tracking and health monitoring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00690</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00690</id><created>2015-03-02</created><updated>2015-11-30</updated><authors><author><keyname>Hu</keyname><forenames>Tao</forenames></author><author><keyname>Pehlevan</keyname><forenames>Cengiz</forenames></author><author><keyname>Chklovskii</keyname><forenames>Dmitri B.</forenames></author></authors><title>A Hebbian/Anti-Hebbian Network for Online Sparse Dictionary Learning
  Derived from Symmetric Matrix Factorization</title><categories>q-bio.NC cs.NE stat.ML</categories><comments>2014 Asilomar Conference on Signals, Systems and Computers. v2: fixed
  a typo in equation 23</comments><doi>10.1109/ACSSC.2014.7094519</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Olshausen and Field (OF) proposed that neural computations in the primary
visual cortex (V1) can be partially modeled by sparse dictionary learning. By
minimizing the regularized representation error they derived an online
algorithm, which learns Gabor-filter receptive fields from a natural image
ensemble in agreement with physiological experiments. Whereas the OF algorithm
can be mapped onto the dynamics and synaptic plasticity in a single-layer
neural network, the derived learning rule is nonlocal - the synaptic weight
update depends on the activity of neurons other than just pre- and postsynaptic
ones - and hence biologically implausible. Here, to overcome this problem, we
derive sparse dictionary learning from a novel cost-function - a regularized
error of the symmetric factorization of the input's similarity matrix. Our
algorithm maps onto a neural network of the same architecture as OF but using
only biologically plausible local learning rules. When trained on natural
images our network learns Gabor-filter receptive fields and reproduces the
correlation among synaptic weights hard-wired in the OF network. Therefore,
online symmetric matrix factorization may serve as an algorithmic theory of
neural computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00693</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00693</id><created>2015-03-02</created><authors><author><keyname>Yogatama</keyname><forenames>Dani</forenames></author><author><keyname>Smith</keyname><forenames>Noah A.</forenames></author></authors><title>Bayesian Optimization of Text Representations</title><categories>cs.CL cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When applying machine learning to problems in NLP, there are many choices to
make about how to represent input texts. These choices can have a big effect on
performance, but they are often uninteresting to researchers or practitioners
who simply need a module that performs well. We propose an approach to
optimizing over this space of choices, formulating the problem as global
optimization. We apply a sequential model-based optimization technique and show
that our method makes standard linear models competitive with more
sophisticated, expensive state-of-the-art methods based on latent variable
models or neural networks on various topic classification and sentiment
analysis problems. Our approach is a first step towards black-box NLP systems
that work with raw text and do not require manual tuning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00694</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00694</id><created>2015-02-21</created><updated>2015-09-30</updated><authors><author><keyname>Brandl</keyname><forenames>Florian</forenames></author><author><keyname>Brandt</keyname><forenames>Felix</forenames></author><author><keyname>Seedig</keyname><forenames>Hans Georg</forenames></author></authors><title>Consistent Probabilistic Social Choice</title><categories>cs.GT cs.CY cs.MA math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two fundamental axioms in social choice theory are consistency with respect
to a variable electorate and consistency with respect to components of similar
alternatives. In the context of traditional non-probabilistic social choice,
these axioms are incompatible with each other. We show that in the context of
probabilistic social choice, these axioms uniquely characterize a function
proposed by Fishburn (Rev. Econ. Stud., 51(4), 683--692, 1984). Fishburn's
function returns so-called maximal lotteries, i.e., lotteries that correspond
to optimal mixed strategies of the underlying plurality game. Maximal lotteries
are guaranteed to exist due to von Neumann's Minimax Theorem, are almost always
unique, and can be efficiently computed using linear programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00697</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00697</id><created>2015-03-02</created><updated>2015-07-12</updated><authors><author><keyname>Shokri-Ghadikolaei</keyname><forenames>Hossein</forenames></author><author><keyname>Fischione</keyname><forenames>Carlo</forenames></author><author><keyname>Fodor</keyname><forenames>Gabor</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author><author><keyname>Zorzi</keyname><forenames>Michele</forenames></author></authors><title>Millimeter Wave Cellular Networks: A MAC Layer Perspective</title><categories>cs.IT math.IT math.OC</categories><comments>21 pages, 9 figures, 2 tables, to appear in IEEE Transactions on
  Communications</comments><doi>10.1109/TCOMM.2015.2456093</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The millimeter wave (mmWave) frequency band is seen as a key enabler of
multi-gigabit wireless access in future cellular networks. In order to overcome
the propagation challenges, mmWave systems use a large number of antenna
elements both at the base station and at the user equipment, which lead to high
directivity gains, fully-directional communications, and possible noise-limited
operations. The fundamental differences between mmWave networks and traditional
ones challenge the classical design constraints, objectives, and available
degrees of freedom. This paper addresses the implications that highly
directional communication has on the design of an efficient medium access
control (MAC) layer. The paper discusses key MAC layer issues, such as
synchronization, random access, handover, channelization, interference
management, scheduling, and association. The paper provides an integrated view
on MAC layer issues for cellular networks, identifies new challenges and
tradeoffs, and provides novel insights and solution approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00698</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00698</id><created>2015-02-12</created><authors><author><keyname>Soares</keyname><forenames>L. R.</forenames></author><author><keyname>de Oliveira</keyname><forenames>H. M.</forenames></author></authors><title>Fault Analysis Using Gegenbauer Multiresolution Analysis</title><categories>math.CA cs.OH</categories><comments>6 pages, 12 figures. In: Transmission and Distribution IEEE/PES/T&amp;D
  Latin America, Sao Paulo, Brazil, 2004</comments><doi>10.1109/TDC.2004.1432473</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper exploits the multiresolution analysis in the fault analysis on
transmission lines. Faults were simulated using the ATP (Alternative Transient
Program), considering signals at 128/cycle. A nonorthogonal multiresolution
analysis was provided by Gegenbauer scaling and wavelet filters. In the cases
where the signal reconstruction is not required, orthogonality may be
immaterial. Gegenbauer filter banks are thereby offered in this paper as a tool
for analyzing fault signals on transmission lines. Results are compared to
those ones derived from a 4-coefficient Daubechies filter. The main advantages
in favor of Gegenbauer filters are their smaller computational effort and their
constant group delay, as they are symmetric filters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00704</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00704</id><created>2015-03-02</created><authors><author><keyname>Cygan</keyname><forenames>Marek</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Marcin</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Micha&#x142;</forenames></author><author><keyname>van Leeuwen</keyname><forenames>Erik Jan</forenames></author><author><keyname>Wrochna</keyname><forenames>Marcin</forenames></author></authors><title>Polynomial kernelization for removing induced claws and diamonds</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph is called (claw,diamond)-free if it contains neither a claw (a
$K_{1,3}$) nor a diamond (a $K_4$ with an edge removed) as an induced subgraph.
Equivalently, (claw,diamond)-free graphs can be characterized as line graphs of
triangle-free graphs, or as linear dominoes, i.e., graphs in which every vertex
is in at most two maximal cliques and every edge is in exactly one maximal
clique.
  In this paper we consider the parameterized complexity of the
(claw,diamond)-free Edge Deletion problem, where given a graph $G$ and a
parameter $k$, the question is whether one can remove at most $k$ edges from
$G$ to obtain a (claw,diamond)-free graph. Our main result is that this problem
admits a polynomial kernel. We complement this finding by proving that, even on
instances with maximum degree $6$, the problem is NP-complete and cannot be
solved in time $2^{o(k)}\cdot |V(G)|^{O(1)}$ unless the Exponential Time
Hypothesis fail
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00709</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00709</id><created>2015-03-02</created><authors><author><keyname>Banerjee</keyname><forenames>Pradeep Kr.</forenames></author></authors><title>Some new insights into information decomposition in complex systems
  based on common information</title><categories>cs.IT math.IT</categories><comments>17 pages, 5 figures, Conference Proceedings Paper, Entropy, Nov. 2014</comments><msc-class>94A15, 94A17</msc-class><doi>10.3390/ecea-1-c004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We take a closer look at the structure of bivariate dependency induced by a
pair of predictor random variables $(X_1, X_2)$ trying to synergistically,
redundantly or uniquely encode a target random variable $Y$. We evaluate a
recently proposed measure of redundancy based on the G\'acs-K\&quot;{o}rner common
information (Griffith et al., Entropy 2014) and show that the measure, in spite
of its elegance is degenerate for most non-trivial distributions. We show that
Wyner's common information also fails to capture the notion of redundancy as it
violates an intuitive monotonically non-increasing property. We identify a set
of conditions when a conditional version of G\'acs and K\&quot;{o}rner's common
information is an ideal measure of unique information. Finally, we show how the
notions of approximately sufficient statistics and conditional information
bottleneck can be used to quantify unique information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00711</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00711</id><created>2015-03-02</created><authors><author><keyname>Gauci</keyname><forenames>Rachel</forenames></author></authors><title>Smelling out Code Clones: Clone Detection Tool Evaluation and
  Corresponding Challenges</title><categories>cs.SE</categories><comments>8 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software clones have been an active area of research for the past two
decades. However, although numerous clone detection tools are now available,
only a small fraction of the literature has focused on tool evaluation, and
this is in fact still an open problem. This is mostly due to the fact that
standard information retrieval metrics such as recall and precision require a
priori knowledge of clones already in the system. Detection tools also
typically have a large number of parameters which are difficult to fine-tune
for optimal performance on a particular software system, and different outputs
produced by different tools add to the complexity of comparing one tool to
another. In this review, we further explore the reasons why tool evaluation is
still an open challenge, and present the current tools and frameworks targeted
at mitigating these problems, focusing on the current standard benchmarks used
to evaluate modern clone detection tools, and also presenting a recent method
aimed at finding optimal tool configurations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00713</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00713</id><created>2015-03-02</created><updated>2015-03-27</updated><authors><author><keyname>Margolus</keyname><forenames>Norman</forenames></author></authors><title>The ideal energy of classical lattice dynamics</title><categories>nlin.CG cs.IT gr-qc math.IT quant-ph</categories><comments>12 pages, 4 figures, includes revised portion of arXiv:0805.3357</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define, as local quantities, the least energy and momentum allowed by
quantum mechanics and special relativity for physical realizations of some
classical lattice dynamics. These definitions depend on local rates of
finite-state change. In two example dynamics, we see that these rates evolve
like classical mechanical energy and momentum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00745</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00745</id><created>2015-03-02</created><updated>2015-05-13</updated><authors><author><keyname>Leroux</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author><author><keyname>Schmitz</keyname><forenames>Sylvain</forenames></author></authors><title>Demystifying Reachability in Vector Addition Systems</title><categories>cs.LO</categories><comments>To appear in the Proceedings of LICS 2015</comments><journal-ref>Proceedings of LICS 2015, pp. 56--67, IEEE Press</journal-ref><doi>10.1109/LICS.2015.16</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  More than 30 years after their inception, the decidability proofs for
reachability in vector addition systems (VAS) still retain much of their
mystery. These proofs rely crucially on a decomposition of runs successively
refined by Mayr, Kosaraju, and Lambert, which appears rather magical, and for
which no complexity upper bound is known.
  We first offer a justification for this decomposition technique, by showing
that it computes the ideal decomposition of the set of runs, using the natural
embedding relation between runs as well quasi ordering. In a second part, we
apply recent results on the complexity of termination thanks to well quasi
orders and well orders to obtain a cubic Ackermann upper bound for the
decomposition algorithms, thus providing the first known upper bounds for
general VAS reachability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00753</identifier>
 <datestamp>2015-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00753</id><created>2015-03-02</created><updated>2015-11-26</updated><authors><author><keyname>Bazzi</keyname><forenames>Abbas</forenames></author><author><keyname>Fiorini</keyname><forenames>Samuel</forenames></author><author><keyname>Pokutta</keyname><forenames>Sebastian</forenames></author><author><keyname>Svensson</keyname><forenames>Ola</forenames></author></authors><title>No Small Linear Program Approximates Vertex Cover within a Factor $2 -
  \epsilon$</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The vertex cover problem is one of the most important and intensively studied
combinatorial optimization problems. Khot and Regev (2003) proved that the
problem is NP-hard to approximate within a factor $2 - \epsilon$, assuming the
Unique Games Conjecture (UGC). This is tight because the problem has an easy
2-approximation algorithm. Without resorting to the UGC, the best
inapproximability result for the problem is due to Dinur and Safra (2002):
vertex cover is NP-hard to approximate within a factor 1.3606. We prove the
following unconditional result about linear programming (LP) relaxations of the
problem: every LP relaxation that approximates vertex cover within a factor
$2-\epsilon$ has super-polynomially many inequalities. As a direct consequence
of our methods, we also establish that LP relaxations (as well as SDP
relaxations) that approximate the independent set problem within any constant
factor have super-polynomial size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00756</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00756</id><created>2015-03-02</created><updated>2015-05-21</updated><authors><author><keyname>Chatzikokolakis</keyname><forenames>Konstantinos</forenames></author><author><keyname>Palamidessi</keyname><forenames>Catuscia</forenames></author><author><keyname>Stronati</keyname><forenames>Marco</forenames></author></authors><title>Constructing elastic distinguishability metrics for location privacy</title><categories>cs.CR</categories><doi>10.1515/popets-2015-0023</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  With the increasing popularity of hand-held devices, location-based
applications and services have access to accurate and real-time location
information, raising serious privacy concerns for their users. The recently
introduced notion of geo-indistinguishability tries to address this problem by
adapting the well-known concept of differential privacy to the area of
location-based systems. Although geo-indistinguishability presents various
appealing aspects, it has the problem of treating space in a uniform way,
imposing the addition of the same amount of noise everywhere on the map. In
this paper we propose a novel elastic distinguishability metric that warps the
geometrical distance, capturing the different degrees of density of each area.
As a consequence, the obtained mechanism adapts the level of noise while
achieving the same degree of privacy everywhere. We also show how such an
elastic metric can easily incorporate the concept of a &quot;geographic fence&quot; that
is commonly employed to protect the highly recurrent locations of a user, such
as his home or work. We perform an extensive evaluation of our technique by
building an elastic metric for Paris' wide metropolitan area, using semantic
information from the OpenStreetMap database. We compare the resulting mechanism
against the Planar Laplace mechanism satisfying standard
geo-indistinguishability, using two real-world datasets from the Gowalla and
Brightkite location-based social networks. The results show that the elastic
mechanism adapts well to the semantics of each area, adjusting the noise as we
move outside the city center, hence offering better overall privacy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00757</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00757</id><created>2015-03-02</created><updated>2015-10-08</updated><authors><author><keyname>Mang</keyname><forenames>Andreas</forenames></author><author><keyname>Biros</keyname><forenames>George</forenames></author></authors><title>Constrained $H^1$-regularization schemes for diffeomorphic image
  registration</title><categories>math.OC cs.CV</categories><msc-class>68U10, 49J20, 35Q93, 65K10, 76D55, 90C20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose regularization schemes for deformable registration and efficient
algorithms for its numerical approximation. We treat image registration as a
variational optimal control problem. The deformation map is parametrized by its
velocity. Tikhonov regularization ensures well-posedness. Our scheme augments
standard smoothness regularization operators based on $H^1$- and
$H^2$-seminorms with a constraint on the divergence of the velocity field,
which resembles variational formulations for Stokes incompressible flows. In
our formulation, we invert for a stationary velocity field and a mass source
map. This allows us to explicitly control the compressibility of the
deformation map and by that the determinant of the deformation gradient. We
also introduce a new regularization scheme that allows us to control shear.
  We use a globalized, preconditioned, matrix-free, reduced space
Gauss-Newton-Krylov scheme for numerical optimization. We exploit variable
elimination techniques to reduce the number of unknowns of our system; we only
iterate on the reduced space of the velocity field. The numerical experiments
demonstrate that we can control the determinant of the deformation gradient
without compromising registration quality. This additional control allows us to
avoid oversmoothing of the deformation map. We also demonstrate that we can
promote or penalize shear whilst controlling the determinant of the deformation
gradient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00759</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00759</id><created>2015-03-02</created><updated>2015-09-28</updated><authors><author><keyname>Nickel</keyname><forenames>Maximilian</forenames></author><author><keyname>Murphy</keyname><forenames>Kevin</forenames></author><author><keyname>Tresp</keyname><forenames>Volker</forenames></author><author><keyname>Gabrilovich</keyname><forenames>Evgeniy</forenames></author></authors><title>A Review of Relational Machine Learning for Knowledge Graphs</title><categories>stat.ML cs.LG</categories><comments>To appear in Proceedings of the IEEE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Relational machine learning studies methods for the statistical analysis of
relational, or graph-structured, data. In this paper, we provide a review of
how such statistical models can be &quot;trained&quot; on large knowledge graphs, and
then used to predict new facts about the world (which is equivalent to
predicting new edges in the graph). In particular, we discuss two fundamentally
different kinds of statistical relational models, both of which can scale to
massive datasets. The first is based on latent feature models such as tensor
factorization and multiway neural networks. The second is based on mining
observable patterns in the graph. We also show how to combine these latent and
observable models to get improved modeling power at decreased computational
cost. Finally, we discuss how such statistical models of graphs can be combined
with text-based information extraction methods for automatically constructing
knowledge graphs from the Web. To this end, we also discuss Google's Knowledge
Vault project as an example of such combination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00760</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00760</id><created>2015-03-02</created><authors><author><keyname>Hampton</keyname><forenames>Andrew</forenames></author><author><keyname>Bhatt</keyname><forenames>Shreyansh</forenames></author><author><keyname>Smith</keyname><forenames>Alan</forenames></author><author><keyname>Brunn</keyname><forenames>Jeremy</forenames></author><author><keyname>Purohit</keyname><forenames>Hemant</forenames></author><author><keyname>Shalin</keyname><forenames>Valerie L.</forenames></author><author><keyname>Flach</keyname><forenames>John M.</forenames></author><author><keyname>Sheth</keyname><forenames>Amit P.</forenames></author></authors><title>On Using Synthetic Social Media Stimuli in an Emergency Preparedness
  Functional Exercise</title><categories>cs.SI</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper details the creation and use of a massive (over 32,000 messages)
artificially constructed 'Twitter' microblog stream for a regional emergency
preparedness functional exercise. By combining microblog conversion, manual
production, and a control set, we created a web based information stream
providing valid, misleading, and irrelevant information to public information
officers (PIOs) representing hospitals, fire departments, the local Red Cross,
and city and county government officials. PIOs searched, monitored, and
(through conventional channels) verified potentially acionable information that
could then be redistributed through a personalized screen name. Our case study
of a key PIO reveals several capabilities that social media can support,
including event detection, the distribution of information between functions
within the emergency response community, and the distribution of messages to
the public. We suggest that training as well as information filtering tools are
necessary to realize the potential of social media in both emergencies and
exercises.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00769</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00769</id><created>2015-03-02</created><authors><author><keyname>Kubota</keyname><forenames>Toshiro</forenames></author></authors><title>Grouping and Recognition of Dot Patterns with Straight Offset Polygons</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When the boundary of a familiar object is shown by a series of isolated dots,
humans can often recognize the object with ease. This ability can be sustained
with addition of distracting dots around the object. However, such capability
has not been reproduced algorithmically on computers. We introduce a new
algorithm that groups a set of dots into multiple non-disjoint subsets. It
connects the dots into a spanning tree using the proximity cue. It then applies
the straight polygon transformation to an initial polygon derived from the
spanning tree. The straight polygon divides the space into polygons recursively
and each polygon can be viewed as grouping of a subset of the dots. The number
of polygons generated is O($n$). We also introduce simple shape selection and
recognition algorithms that can be applied to the grouping result. We used both
natural and synthetic images to show effectiveness of these algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00771</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00771</id><created>2015-03-02</created><authors><author><keyname>Zhu</keyname><forenames>Qinyun</forenames></author></authors><title>Stable Cluster Core Detection in Correlated Hashtag Graph</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hashtags in twitter are used to track events, topics and activities.
Correlated hashtag graph represents contextual relationships among these
hashtags. Maximum clusters in the correlated hashtag graph can be contextually
meaningful hashtag groups. In order to track the changes of the clusters and
understand these hashtag groups, the hashtags in a cluster are categorized into
two types: stable core and temporary members which are subject to change. Some
initial studies are done in this project and 3 algorithms are designed,
implemented and experimented to test them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00778</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00778</id><created>2015-03-02</created><authors><author><keyname>Arora</keyname><forenames>Sanjeev</forenames></author><author><keyname>Ge</keyname><forenames>Rong</forenames></author><author><keyname>Ma</keyname><forenames>Tengyu</forenames></author><author><keyname>Moitra</keyname><forenames>Ankur</forenames></author></authors><title>Simple, Efficient, and Neural Algorithms for Sparse Coding</title><categories>cs.LG cs.DS cs.NE stat.ML</categories><comments>37 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse coding is a basic task in many fields including signal processing,
neuroscience and machine learning where the goal is to learn a basis that
enables a sparse representation of a given set of data, if one exists. Its
standard formulation is as a non-convex optimization problem which is solved in
practice by heuristics based on alternating minimization. Re- cent work has
resulted in several algorithms for sparse coding with provable guarantees, but
somewhat surprisingly these are outperformed by the simple alternating
minimization heuristics. Here we give a general framework for understanding
alternating minimization which we leverage to analyze existing heuristics and
to design new ones also with provable guarantees. Some of these algorithms seem
implementable on simple neural architectures, which was the original motivation
of Olshausen and Field (1997a) in introducing sparse coding. We also give the
first efficient algorithm for sparse coding that works almost up to the
information theoretic limit for sparse recovery on incoherent dictionaries. All
previous algorithms that approached or surpassed this limit run in time
exponential in some natural parameter. Finally, our algorithms improve upon the
sample complexity of existing approaches. We believe that our analysis
framework will have applications in other settings where simple iterative
algorithms are used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00783</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00783</id><created>2015-03-02</created><updated>2015-04-24</updated><authors><author><keyname>Modolo</keyname><forenames>Davide</forenames></author><author><keyname>Vezhnevets</keyname><forenames>Alexander</forenames></author><author><keyname>Russakovsky</keyname><forenames>Olga</forenames></author><author><keyname>Ferrari</keyname><forenames>Vittorio</forenames></author></authors><title>Joint calibration of Ensemble of Exemplar SVMs</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method for calibrating the Ensemble of Exemplar SVMs model.
Unlike the standard approach, which calibrates each SVM independently, our
method optimizes their joint performance as an ensemble. We formulate joint
calibration as a constrained optimization problem and devise an efficient
optimization algorithm to find its global optimum. The algorithm dynamically
discards parts of the solution space that cannot contain the optimum early on,
making the optimization computationally feasible. We experiment with EE-SVM
trained on state-of-the-art CNN descriptors. Results on the ILSVRC 2014 and
PASCAL VOC 2007 datasets show that (i) our joint calibration procedure
outperforms independent calibration on the task of classifying windows as
belonging to an object class or not; and (ii) this improved window classifier
leads to better performance on the object detection task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00787</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00787</id><created>2015-03-02</created><authors><author><keyname>Modolo</keyname><forenames>Davide</forenames></author><author><keyname>Vezhnevets</keyname><forenames>Alexander</forenames></author><author><keyname>Ferrari</keyname><forenames>Vittorio</forenames></author></authors><title>Context Forest for efficient object detection with large mixture models</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present Context Forest (ConF), a technique for predicting properties of
the objects in an image based on its global appearance. Compared to standard
nearest-neighbour techniques, ConF is more accurate, fast and memory efficient.
We train ConF to predict which aspects of an object class are likely to appear
in a given image (e.g. which viewpoint). This enables to speed-up
multi-component object detectors, by automatically selecting the most relevant
components to run on that image. This is particularly useful for detectors
trained from large datasets, which typically need many components to fully
absorb the data and reach their peak performance. ConF provides a speed-up of
2x for the DPM detector [1] and of 10x for the EE-SVM detector [2]. To show
ConF's generality, we also train it to predict at which locations objects are
likely to appear in an image. Incorporating this information in the detector
score improves mAP performance by about 2% by removing false positive
detections in unlikely locations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00789</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00789</id><created>2015-03-02</created><authors><author><keyname>Neil</keyname><forenames>Callum T.</forenames></author><author><keyname>Shafi</keyname><forenames>Mansoor</forenames></author><author><keyname>Smith</keyname><forenames>Peter J.</forenames></author><author><keyname>Dmochowski</keyname><forenames>Pawel A.</forenames></author></authors><title>On the Impact of Antenna Topologies for Massive MIMO Systems</title><categories>cs.IT math.IT</categories><comments>6 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Approximate expressions for the spatial correlation of cylindrical and
uniform rectangular arrays (URA) are derived using measured distributions of
angles of departure (AOD) for both the azimuth and zenith domains. We examine
massive multiple-input-multiple-output (MIMO) convergence properties of the
correlated channels by considering a number of convergence metrics. The
per-user matched filter (MF) signal-to-interference-plus-noise ratio (SINR)
performance and convergence rate, to respective limiting values, of the two
antenna topologies is also explored.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00791</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00791</id><created>2015-03-02</created><authors><author><keyname>Neil</keyname><forenames>Callum T.</forenames></author><author><keyname>Shafi</keyname><forenames>Mansoor</forenames></author><author><keyname>Smith</keyname><forenames>Peter J.</forenames></author><author><keyname>Dmochowski</keyname><forenames>Pawel A.</forenames></author></authors><title>Deployment Issues for Massive MIMO Systems</title><categories>cs.IT math.IT</categories><comments>6 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we examine a number of deployment issues which arise from
practical considerations in massive multiple-input-multiple-output (MIMO)
systems. We show both spatial correlation and line-of-sight (LOS) introduce an
interference component to the system which causes non-orthogonality between
user channels. Distributing the antennas into multiple clusters is shown to
reduce spatial correlation and improve performance. Furthermore, due to its
ability to minimize interference, zero forcing (ZF) precoding performs well in
massive MIMO systems compared to matched filter (MF) precoding which suffers
large penalties. However, the noise component in the ZF signal-to-noise-ratio
(SNR) increases significantly in the case of imperfect transmit channel state
information (CSI).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00792</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00792</id><created>2015-03-02</created><authors><author><keyname>Ma</keyname><forenames>Wentao</forenames></author><author><keyname>Qu</keyname><forenames>Hua</forenames></author><author><keyname>Zhao</keyname><forenames>Jihong</forenames></author><author><keyname>Chen</keyname><forenames>Badong</forenames></author><author><keyname>Gui</keyname><forenames>Guan</forenames></author></authors><title>Sparsity Aware Normalized Least Mean p-power Algorithms with Correntropy
  Induced Metric Penalty</title><categories>cs.IT math.IT</categories><comments>5 pages, 4 figures, submitted for DSP2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For identifying the non-Gaussian impulsive noise systems, normalized LMP
(NLMP) has been proposed to combat impulsive-inducing instability. However, the
standard algorithm is without considering the inherent sparse structure
distribution of unknown system. To exploit sparsity as well as to mitigate the
impulsive noise, this paper proposes a sparse NLMP algorithm, i.e., Correntropy
Induced Metric (CIM) constraint based NLMP (CIMNLMP). Based on the first
proposed algorithm, moreover, we propose an improved CIM constraint variable
regularized NLMP(CIMVRNLMP) algorithm by utilizing variable regularized
parameter(VRP) selection method which can further adjust convergence speed and
steady-state error. Numerical simulations are given to confirm the proposed
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00793</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00793</id><created>2015-03-02</created><authors><author><keyname>Biedl</keyname><forenames>Therese</forenames></author><author><keyname>Fischmeister</keyname><forenames>Sebastian</forenames></author><author><keyname>Kumar</keyname><forenames>Neeraj</forenames></author></authors><title>DAG-width of Control Flow Graphs with Applications to Model Checking</title><categories>cs.DM cs.LO cs.PL cs.SE</categories><comments>12 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The treewidth of control flow graphs arising from structured programs is
known to be at most six. However, as a control flow graph is inherently
directed, it makes sense to consider a measure of width for digraphs instead.
We use the so-called DAG-width and show that the DAG-width of control flow
graphs arising from structured (goto-free) programs is at most three.
Additionally, we also give a linear time algorithm to compute the DAG
decomposition of these control flow graphs. One consequence of this result is
that parity games (and hence the $\mu$-calculus model checking problem), which
are known to be tractable on graphs of bounded DAG-width, can be solved
efficiently in practice on control flow graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00796</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00796</id><created>2015-03-02</created><authors><author><keyname>Smith</keyname><forenames>Peter J.</forenames></author><author><keyname>Neil</keyname><forenames>Callum T.</forenames></author><author><keyname>Shafi</keyname><forenames>Mansoor</forenames></author><author><keyname>Dmochowski</keyname><forenames>Pawel A.</forenames></author></authors><title>On the Convergence and Performance of MF Precoding in Distributed
  Massive MU-MIMO Systems</title><categories>cs.IT math.IT</categories><comments>26 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze both the rate of convergence and the performance of
a matched-filter (MF) precoder in a massive multi-user (MU)
multiple-input-multiple-output (MIMO) system, with the aim of determining the
impact of distributing the transmit antennas into multiple clusters. We
consider cases of transmit spatial correlation, unequal link gains and
imperfect channel state information (CSI). Furthermore, we derive a MF
signal-to-interference-plus-noise-ratio (SINR) limit as both the number of
transmit antennas and the number of users tend to infinity. In our results, we
show that both the rate of convergence and performance is strongly dependent on
spatial correlation. In the presence of spatial correlation, distributing the
antennas into multiple clusters renders significant gains over a co-located
antenna array scenario. In uncorrelated scenarios, a co-located antenna cluster
has a marginally better mean per-user SINR performance due to its superior
single-user signal-to-noise-ratio (SNR) regime, i.e., when a user is close to
the base station (BS), the links between the user and all transmit antennas
becomes strong.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00798</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00798</id><created>2015-03-02</created><authors><author><keyname>Gui</keyname><forenames>Guan</forenames></author><author><keyname>Xu</keyname><forenames>Li</forenames></author><author><keyname>Matsushita</keyname><forenames>Shinya</forenames></author></authors><title>Improved adaptive sparse channel estimation using mixed square/fourth
  error criterion</title><categories>cs.IT math.IT</categories><comments>21 pages, 10 figures, submitted for journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse channel estimation problem is one of challenge technical issues in
stable broadband wireless communications. Based on square error criterion
(SEC), adaptive sparse channel estimation (ASCE) methods, e.g., zero-attracting
least mean square error (ZA-LMS) algorithm and reweighted ZA-LMS (RZA-LMS)
algorithm, have been proposed to mitigate noise interferences as well as to
exploit the inherent channel sparsity. However, the conventional SEC-ASCE
methods are vulnerable to 1) random scaling of input training signal; and 2)
imbalance between convergence speed and steady state mean square error (MSE)
performance due to fixed step-size of gradient descend method. In this paper, a
mixed square/fourth error criterion (SFEC) based improved ASCE methods are
proposed to avoid aforementioned shortcomings. Specifically, the improved
SFEC-ASCE methods are realized with zero-attracting least mean square/fourth
error (ZA-LMS/F) algorithm and reweighted ZA-LMS/F (RZA-LMS/F) algorithm,
respectively. Firstly, regularization parameters of the SFEC-ASCE methods are
selected by means of Monte-Carlo simulations. Secondly, lower bounds of the
SFEC-ASCE methods are derived and analyzed. Finally, simulation results are
given to show that the proposed SFEC-ASCE methods achieve better estimation
performance than the conventional SEC-ASCE methods. 1
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00800</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00800</id><created>2015-03-02</created><authors><author><keyname>Zhang</keyname><forenames>Tingping</forenames></author><author><keyname>Dan</keyname><forenames>Jingpei</forenames></author><author><keyname>Gui</keyname><forenames>Guan</forenames></author></authors><title>IMAC: Impulsive-mitigation adaptive sparse channel estimation based on
  Gaussian-mixture model</title><categories>cs.IT math.IT</categories><comments>12 pages, 10 figures, submitted for journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Broadband frequency-selective fading channels usually have the inherent
sparse nature. By exploiting the sparsity, adaptive sparse channel estimation
(ASCE) methods, e.g., reweighted L1-norm least mean square (RL1-LMS), could
bring a performance gain if additive noise satisfying Gaussian assumption. In
real communication environments, however, channel estimation performance is
often deteriorated by unexpected non-Gaussian noises which include conventional
Gaussian noises and impulsive interferences. To design stable communication
systems, hence, it is urgent to develop advanced channel estimation methods to
remove the impulsive interference and to exploit channel sparsity
simultaneously. In this paper, robust impulsive-mitigation adaptive sparse
channel estimation (IMAC) method is proposed for solving aforementioned
technical issues. Specifically, first of all, the non-Gaussian noise model is
described by Gaussian mixture model (GMM). Secondly, cost function of
reweighted L1-norm penalized least absolute error standard (RL1-LAE) algorithm
is constructed. Then, RL1-LAE algorithm is derived for realizing IMAC method.
Finally, representative simulation results are provided to corroborate the
studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00802</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00802</id><created>2015-03-02</created><updated>2015-04-14</updated><authors><author><keyname>Ma</keyname><forenames>Wentao</forenames></author><author><keyname>Qua</keyname><forenames>Hua</forenames></author><author><keyname>Gui</keyname><forenames>Guan</forenames></author><author><keyname>Xu</keyname><forenames>Li</forenames></author><author><keyname>Zhaoa</keyname><forenames>Jihong</forenames></author><author><keyname>Chen</keyname><forenames>Badong</forenames></author></authors><title>Maximum correntropy criterion based sparse adaptive filtering algorithms
  for robust channel estimation under non-Gaussian environments</title><categories>cs.IT math.IT</categories><comments>29 pages, 12 figures, accepted by Journal of the Franklin Institute</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse adaptive channel estimation problem is one of the most important
topics in broadband wireless communications systems due to its simplicity and
robustness. So far many sparsity-aware channel estimation algorithms have been
developed based on the well-known minimum mean square error (MMSE) criterion,
such as the zero-attracting least mean square (ZALMS), which are robust under
Gaussian assumption. In non-Gaussian environments, however, these methods are
often no longer robust especially when systems are disturbed by random
impulsive noises. To address this problem, we propose in this work a robust
sparse adaptive filtering algorithm using correntropy induced metric (CIM)
penalized maximum correntropy criterion (MCC) rather than conventional MMSE
criterion for robust channel estimation. Specifically, MCC is utilized to
mitigate the impulsive noise while CIM is adopted to exploit the channel
sparsity efficiently. Both theoretical analysis and computer simulations are
provided to corroborate the proposed methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00805</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00805</id><created>2015-03-02</created><authors><author><keyname>Emamjomeh-Zadeh</keyname><forenames>Ehsan</forenames></author><author><keyname>Kempe</keyname><forenames>David</forenames></author></authors><title>Binary Search in Graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the following natural generalization of Binary Search to arbitrary
connected graphs and finite metric spaces. In a given and known undirected
positively weighted graph, one vertex is a target. The algorithm's task is to
identify the target by adaptively querying vertices. In response to querying a
vertex q, the algorithm learns either that q is the target, or is given an edge
out of q that lies on a shortest path from q to the target.
  Our main positive result is that in undirected graphs, log_2(n) queries are
always sufficient to find the target. This result extends to directed graphs
that are &quot;almost undirected&quot; in the sense that each edge e with weight w(e) is
part of a cycle of total weight at most c.w(e): here, c.ln(n) queries are
sufficient.
  On the negative side, for strongly connected directed graphs, deciding
whether K queries are sufficient to identify the target in the worst case is
PSPACE-complete. This result also applies to undirected graphs with non-uniform
query costs. We also show hardness in the polynomial hierarchy for a
&quot;semi-adaptive&quot; version of the problem: the algorithm gets to query r vertices
each in k rounds. This version is Sigma_{2k-5}-hard and in Sigma_{2k-1} in the
polynomial hierarchy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00806</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00806</id><created>2015-03-02</created><authors><author><keyname>van Ditmarsch</keyname><forenames>Hans</forenames></author><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>van der Hoek</keyname><forenames>Wiebe</forenames></author><author><keyname>Kooi</keyname><forenames>Barteld</forenames></author></authors><title>An Introduction to Logics of Knowledge and Belief</title><categories>cs.AI cs.LO</categories><comments>FIrst chapter of &quot;Handbook of Epistemic Logic&quot;, by Hans van
  Ditmarsch, Joseph Y. Halpern, Wiebe van der Hoek, and Barteld Kooi</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This chapter provides an introduction to some basic concepts of epistemic
logic, basic formal languages, their semantics, and proof systems. It also
contains an overview of the handbook, and a brief history of epistemic logic
and pointers to the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00808</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00808</id><created>2015-03-02</created><authors><author><keyname>Mou</keyname><forenames>Shaoshuai</forenames></author><author><keyname>Liu</keyname><forenames>Ji</forenames></author><author><keyname>Morse</keyname><forenames>A. Stephen</forenames></author></authors><title>A Distributed Algorithm for Solving a Linear Algebraic Equation</title><categories>cs.SY cs.DC cs.MA</categories><comments>45pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A distributed algorithm is described for solving a linear algebraic equation
of the form $Ax=b$ assuming the equation has at least one solution. The
equation is simultaneously solved by $m$ agents assuming each agent knows only
a subset of the rows of the partitioned matrix $(A,b)$, the current estimates
of the equation's solution generated by its neighbors, and nothing more. Each
agent recursively updates its estimate by utilizing the current estimates
generated by each of its neighbors. Neighbor relations are characterized by a
time-dependent directed graph $\mathbb{N}(t)$ whose vertices correspond to
agents and whose arcs depict neighbor relations. It is shown that for any
matrix $A$ for which the equation has a solution and any sequence of
&quot;repeatedly jointly strongly connected graphs&quot; $\mathbb{N}(t)$, $t=1,2,\ldots$,
the algorithm causes all agents' estimates to converge exponentially fast to
the same solution to $Ax=b$. It is also shown that the neighbor graph sequence
must actually be repeatedly jointly strongly connected if exponential
convergence is to be assured. A worst case convergence rate bound is derived
for the case when $Ax=b$ has a unique solution. It is demonstrated that with
minor modification, the algorithm can track the solution to $Ax = b$, even if
$A$ and $b$ are changing with time, provided the rates of change of $A$ and $b$
are sufficiently small. It is also shown that in the absence of communication
delays, exponential convergence to a solution occurs even if the times at which
each agent updates its estimates are not synchronized with the update times of
its neighbors. A modification of the algorithm is outlined which enables it to
obtain a least squares solution to $Ax=b$ in a distributed manner, even if
$Ax=b$ does not have a solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00810</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00810</id><created>2015-03-02</created><authors><author><keyname>Abel</keyname><forenames>Daryl</forenames></author><author><keyname>Gavidi</keyname><forenames>Bulou</forenames></author><author><keyname>Rollings</keyname><forenames>Nicholas</forenames></author><author><keyname>Chandra</keyname><forenames>Rohitash</forenames></author></authors><title>Development of an Android Application for an Electronic Medical Record
  System in an Outpatient Environment for Healthcare in Fiji</title><categories>cs.CY</categories><comments>Technical Report, AICRG, Software Foundation, Fiji, March 2015</comments><report-no>TR-03-2015</report-no><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The outpatients department in a developing country is typically understaffed
and inadequately equipped to handle a large numbers of patients filing through
on an average day. The use of electronic medical record (EMR) systems can
resolve some of the longstanding medical inefficiencies common in developing
countries. This paper presents the design and implementation of a proposed
outpatient management system that enables efficient management of a patient's
medical details. We present a system to create appointments with medical
practitioners by integrating a proposed Android-based mobile application with a
selected open source EMR system. The application allows both the patient and
the medical practitioners to manage appointments and make use of the electronic
messaging facility to send reminders when the appointed time is approaching in
real-time. A mobile application prototype is developed and the road map for
implementation is also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00812</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00812</id><created>2015-03-02</created><authors><author><keyname>Mou</keyname><forenames>Shaoshuai</forenames></author><author><keyname>Morse</keyname><forenames>A. Stephen</forenames></author><author><keyname>Belabbas</keyname><forenames>Mohamed Ali</forenames></author><author><keyname>Sun</keyname><forenames>Zhiyong</forenames></author><author><keyname>Anderson</keyname><forenames>Brian D. O.</forenames></author></authors><title>Undirected Rigid Formations are Problematic</title><categories>cs.SY cs.MA</categories><comments>42pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By an undirected rigid formation of mobile autonomous agents is meant a
formation based on graph rigidity in which each pair of &quot;neighboring&quot; agents is
responsible for maintaining a prescribed target distance between them. In a
recent paper a systematic method was proposed for devising gradient control
laws for asymptotically stabilizing a large class of rigid, undirected
formations in two dimensional space assuming all agents are described by
kinematic point models. The aim of this paper is to explain what happens to
such formations if neighboring agents have slightly different understandings of
what the desired distance between them is supposed to be or equivalently if
neighboring agents have differing estimates of what the actual distance between
them is. In either case, what one would expect would be a gradual distortion of
the formation from its target shape as discrepancies in desired or sensed
distances increase. While this is observed for the gradient laws in question,
something else quite unexpected happens at the same time. It is shown that for
any rigidity-based, undirected formation of this type which is comprised of
three or more agents, that if some neighboring agents have slightly different
understandings of what the desired distances between them are suppose to be,
then almost for certain, the trajectory of the resulting distorted but rigid
formation will converge exponentially fast to a closed circular orbit in
two-dimensional space which is traversed periodically at a constant angular
speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00814</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00814</id><created>2015-03-02</created><authors><author><keyname>Reddy</keyname><forenames>Emmenual</forenames></author><author><keyname>Kumar</keyname><forenames>Sarnil</forenames></author><author><keyname>Rollings</keyname><forenames>Nicholas</forenames></author><author><keyname>Chandra</keyname><forenames>Rohitash</forenames></author></authors><title>Mobile Application for Dengue Fever Monitoring and Tracking via GPS:
  Case Study for Fiji</title><categories>cs.CY</categories><comments>Technical Report, AICRG, Software Foundation, Fiji, March 2015</comments><report-no>TR-04-2015</report-no><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The 2013 outbreak of Dengue in Fiji resulted in an alarming number of deaths
and has been is a matter of serious concern. Dengue fever is a disease caused
by the four types of the Dengue virus serotypes and transmitted mostly from
mosquito bites. In Fiji, dengue diagnosis is only done in hospitals which are
slow and time consuming. It is also important to monitor the spread of Dengue.
Fiji needs an convenient method of monitoring the spread of Dengue. With
increase in affordable smartphones and better Internet coverage, there is scope
for a mobile application for Dengue fever monitoring and tracking. This paper
proposes a mobile application for Dengue monitoring based on global positioning
system (GPS) enabled mobile phone technology. It also provides an information
network that shows the spread of dengue which will allow health authorities to
quickly identify dengue infected areas in Fiji. A mobile application prototype
is developed and tested and the scope for further testing and implementation is
also given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00826</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00826</id><created>2015-03-02</created><authors><author><keyname>DaCosta</keyname><forenames>Daniel</forenames></author></authors><title>Towards Reasoning About Properties of Imperative Programs using Linear
  Logic</title><categories>cs.LO</categories><comments>This report was submitted in partial fulfillment of my preliminary
  examination</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper we propose an approach to reasoning about properties of
imperative programs. We assume in this context that the meanings of program
constructs are described using rules in the natural semantics style with the
additional observation that these rules may involve the treatment of state. Our
approach involves modeling natural semantics style rules within a logic and
then reasoning about the behavior of particular programs by reasoning about
proofs in that logic. A key aspect of our proposal is to use a fragment of
linear logic called Lolli (invented by Hodas and Miller) to model natural
semantics style descriptions. Being based on linear logic, Lolli can provide
logical expression to resources such as state. Lolli additionally possesses
proof-theoretic properties that allow it to encode natural semantics style
descriptions in such a way that proofs in Lolli mimic the structure of
derivations based on the natural semantics rules. We will discuss these
properties of Lolli and demonstrate how they can be exploited in modeling the
semantics of imperative programs and in reasoning about such models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00827</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00827</id><created>2015-03-02</created><updated>2015-10-19</updated><authors><author><keyname>Sinop</keyname><forenames>Ali Kemal</forenames></author></authors><title>How to Round Subspaces: A New Spectral Clustering Algorithm</title><categories>cs.DS</categories><comments>Appeared in SODA 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A basic problem in spectral clustering is the following. If a solution
obtained from the spectral relaxation is close to an integral solution, is it
possible to find this integral solution even though they might be in completely
different basis? In this paper, we propose a new spectral clustering algorithm.
It can recover a $k$-partition such that the subspace corresponding to the span
of its indicator vectors is $O(\sqrt{opt})$ close to the original subspace in
spectral norm with $opt$ being the minimum possible ($opt \le 1$ always).
Moreover our algorithm does not impose any restriction on the cluster sizes.
Previously, no algorithm was known which could find a $k$-partition closer than
$o(k \cdot opt)$.
  We present two applications for our algorithm. First one finds a disjoint
union of bounded degree expanders which approximate a given graph in spectral
norm. The second one is for approximating the sparsest $k$-partition in a graph
where each cluster have expansion at most $\phi_k$ provided $\phi_k \le
O(\lambda_{k+1})$ where $\lambda_{k+1}$ is the $(k+1)^{st}$ eigenvalue of
Laplacian matrix. This significantly improves upon the previous algorithms,
which required $\phi_k \le O(\lambda_{k+1}/k)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00833</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00833</id><created>2015-03-03</created><authors><author><keyname>Haddadan</keyname><forenames>Arash</forenames></author><author><keyname>Ito</keyname><forenames>Takehiro</forenames></author><author><keyname>Mouawad</keyname><forenames>Amer E.</forenames></author><author><keyname>Nishimura</keyname><forenames>Naomi</forenames></author><author><keyname>Ono</keyname><forenames>Hirotaka</forenames></author><author><keyname>Suzuki</keyname><forenames>Akira</forenames></author><author><keyname>Tebbal</keyname><forenames>Youcef</forenames></author></authors><title>The complexity of dominating set reconfiguration</title><categories>cs.DM cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose that we are given two dominating sets $D_s$ and $D_t$ of a graph $G$
whose cardinalities are at most a given threshold $k$. Then, we are asked
whether there exists a sequence of dominating sets of $G$ between $D_s$ and
$D_t$ such that each dominating set in the sequence is of cardinality at most
$k$ and can be obtained from the previous one by either adding or deleting
exactly one vertex. This problem is known to be PSPACE-complete in general. In
this paper, we study the complexity of this decision problem from the viewpoint
of graph classes. We first prove that the problem remains PSPACE-complete even
for planar graphs, bounded bandwidth graphs, split graphs, and bipartite
graphs. We then give a general scheme to construct linear-time algorithms and
show that the problem can be solved in linear time for cographs, trees, and
interval graphs. Furthermore, for these tractable cases, we can obtain a
desired sequence such that the number of additions and deletions is bounded by
$O(n)$, where $n$ is the number of vertices in the input graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00840</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00840</id><created>2015-03-03</created><authors><author><keyname>Yurischev</keyname><forenames>M. A.</forenames></author></authors><title>On the quantum discord of general X states</title><categories>quant-ph cs.IT math.IT</categories><comments>23 pages, 12 figures. arXiv admin note: text overlap with
  arXiv:1404.5735</comments><journal-ref>Quantum Inf. Process. 14, 3399 (2015)</journal-ref><doi>10.1007/s11128-015-1046-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum discord Q is a function of density matrix elements. The domain of
such a function in the case of two-qubit system with X density matrix may
consist of three subdomains at most: two ones where the quantum discord is
expressed in closed analytical forms (Q_{\pi/2} and Q_0) and an intermediate
subdomain for which, to extract the quantum discord Q_\theta, it is required to
solve in general numerically a one-dimensional minimization problem to find the
optimal measurement angle \theta\in(0,\pi/2). Hence the quantum discord is
given by a piecewise-analytic-numerical formula Q=\min{Q_{\pi/2}, Q_\theta,
Q_0}. Equations for determining the boundaries between these subdomains are
obtained. The boundaries consist of bifurcation points. The Q_{\theta}
subdomains are discovered in the generalized Horodecki states, in the dynamical
phase flip channel model, in the anisotropic spin systems at thermal
equilibrium, in the heteronuclear dimers in an external magnetic field. We
found that transitions between Q_{\theta} subdomain and Q_{\pi/2} and Q_0 ones
occur suddenly but continuously and smoothly, i.e., nonanalyticity is hidden
and can be observed in higher derivatives of discord function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00841</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00841</id><created>2015-03-03</created><authors><author><keyname>Liu</keyname><forenames>Biao</forenames></author><author><keyname>Huang</keyname><forenames>Minlie</forenames></author></authors><title>Robustly Leveraging Prior Knowledge in Text Classification</title><categories>cs.CL cs.AI cs.IR cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Prior knowledge has been shown very useful to address many natural language
processing tasks. Many approaches have been proposed to formalise a variety of
knowledge, however, whether the proposed approach is robust or sensitive to the
knowledge supplied to the model has rarely been discussed. In this paper, we
propose three regularization terms on top of generalized expectation criteria,
and conduct extensive experiments to justify the robustness of the proposed
methods. Experimental results demonstrate that our proposed methods obtain
remarkable improvements and are much more robust than baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00843</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00843</id><created>2015-03-03</created><authors><author><keyname>N.</keyname><forenames>Sowmya K.</forenames></author><author><keyname>Chennamma</keyname><forenames>H. R.</forenames></author></authors><title>A Survey On Video Forgery Detection</title><categories>cs.MM cs.CV</categories><comments>11 pages, 3 figures, International Journal of Computer Engineering
  and Applications, Volume IX, Issue II, February 2015</comments><journal-ref>International Journal of Computer Engineering and Applications,
  Volume IX, Issue II, pp. 17-27, February 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Digital Forgeries though not visibly identifiable to human perception it
may alter or meddle with underlying natural statistics of digital content.
Tampering involves fiddling with video content in order to cause damage or make
unauthorized alteration/modification. Tampering detection in video is
cumbersome compared to image when considering the properties of the video.
Tampering impacts need to be studied and the applied technique/method is used
to establish the factual information for legal course in judiciary. In this
paper we give an overview of the prior literature and challenges involved in
video forgery detection where passive approach is found.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00848</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00848</id><created>2015-03-03</created><updated>2016-03-01</updated><authors><author><keyname>Pont-Tuset</keyname><forenames>Jordi</forenames></author><author><keyname>Arbelaez</keyname><forenames>Pablo</forenames></author><author><keyname>Barron</keyname><forenames>Jonathan T.</forenames></author><author><keyname>Marques</keyname><forenames>Ferran</forenames></author><author><keyname>Malik</keyname><forenames>Jitendra</forenames></author></authors><title>Multiscale Combinatorial Grouping for Image Segmentation and Object
  Proposal Generation</title><categories>cs.CV</categories><doi>10.1109/TPAMI.2016.2537320</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a unified approach for bottom-up hierarchical image segmentation
and object proposal generation for recognition, called Multiscale Combinatorial
Grouping (MCG). For this purpose, we first develop a fast normalized cuts
algorithm. We then propose a high-performance hierarchical segmenter that makes
effective use of multiscale information. Finally, we propose a grouping
strategy that combines our multiscale regions into highly-accurate object
proposals by exploring efficiently their combinatorial space. We also present
Single-scale Combinatorial Grouping (SCG), a faster version of MCG that
produces competitive proposals in under five second per image. We conduct an
extensive and comprehensive empirical validation on the BSDS500, SegVOC12, SBD,
and COCO datasets, showing that MCG produces state-of-the-art contours,
hierarchical regions, and object proposals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00849</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00849</id><created>2015-03-03</created><authors><author><keyname>Choudhury</keyname><forenames>Sutanay</forenames></author><author><keyname>Holder</keyname><forenames>Lawrence</forenames></author><author><keyname>Chin</keyname><forenames>George</forenames></author><author><keyname>Agarwal</keyname><forenames>Khushbu</forenames></author><author><keyname>Feo</keyname><forenames>John</forenames></author></authors><title>A Selectivity based approach to Continuous Pattern Detection in
  Streaming Graphs</title><categories>cs.DB</categories><comments>in 18th International Conference on Extending Database Technology
  (EDBT) (2015)</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Cyber security is one of the most significant technical challenges in current
times. Detecting adversarial activities, prevention of theft of intellectual
properties and customer data is a high priority for corporations and government
agencies around the world. Cyber defenders need to analyze massive-scale,
high-resolution network flows to identify, categorize, and mitigate attacks
involving networks spanning institutional and national boundaries. Many of the
cyber attacks can be described as subgraph patterns, with prominent examples
being insider infiltrations (path queries), denial of service (parallel paths)
and malicious spreads (tree queries). This motivates us to explore subgraph
matching on streaming graphs in a continuous setting. The novelty of our work
lies in using the subgraph distributional statistics collected from the
streaming graph to determine the query processing strategy. We introduce a
&quot;Lazy Search&quot; algorithm where the search strategy is decided on a
vertex-to-vertex basis depending on the likelihood of a match in the vertex
neighborhood. We also propose a metric named &quot;Relative Selectivity&quot; that is
used to select between different query processing strategies. Our experiments
performed on real online news, network traffic stream and a synthetic social
network benchmark demonstrate 10-100x speedups over selectivity agnostic
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00851</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00851</id><created>2015-03-03</created><updated>2015-04-24</updated><authors><author><keyname>Yilmaz</keyname><forenames>Ozgur</forenames></author></authors><title>Connectionist-Symbolic Machine Intelligence using Cellular Automata
  based Reservoir-Hyperdimensional Computing</title><categories>cs.ET</categories><comments>Corrected Typos. Responded some comments on section 8. Added appendix
  for details. Recurrent architecture emphasized</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a novel framework of reservoir computing, that is capable of
both connectionist machine intelligence and symbolic computation. Cellular
automaton is used as the reservoir of dynamical systems. Input is randomly
projected onto the initial conditions of automaton cells and nonlinear
computation is performed on the input via application of a rule in the
automaton for a period of time. The evolution of the automaton creates a
space-time volume of the automaton state space, and it is used as the
reservoir. The proposed framework is capable of long short-term memory and it
requires orders of magnitude less computation compared to Echo State Networks.
We prove that cellular automaton reservoir holds a distributed representation
of attribute statistics, which provides a more effective computation than local
representation. It is possible to estimate the kernel for linear cellular
automata via metric learning, that enables a much more efficient distance
computation in support vector machine framework. Also, binary reservoir feature
vectors can be combined using Boolean operations as in hyperdimensional
computing, paving a direct way for concept building and symbolic processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00855</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00855</id><created>2015-03-03</created><authors><author><keyname>Uyttendaele</keyname><forenames>Nathan</forenames></author></authors><title>How to speed up R code: an introduction</title><categories>stat.CO cs.DC cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most calculations performed by the average R user are unremarkable in the
sense that nowadays, any computer can crush the related code in a matter of
seconds. But more and more often, heavy calculations are also performed using
R, something especially true in some fields such as statistics. The user then
faces total execution times of his codes that are hard to work with: hours,
days, even weeks. In this paper, how to reduce the total execution time of
various codes will be shown and typical bottlenecks will be discussed. As a
last resort, how to run your code on a cluster of computers (most workplaces
have one) in order to make use of a larger processing power than the one
available on an average computer will also be discussed through two examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00877</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00877</id><created>2015-03-03</created><updated>2015-08-14</updated><authors><author><keyname>Selim</keyname><forenames>Bassant</forenames></author><author><keyname>Alhussein</keyname><forenames>Omar</forenames></author><author><keyname>Muhaidat</keyname><forenames>Sami</forenames></author><author><keyname>Karagiannidis</keyname><forenames>George K.</forenames></author><author><keyname>Liang</keyname><forenames>Jie</forenames></author></authors><title>Modeling and Analysis of Wireless Channels via the Mixture of Gaussian
  Distribution</title><categories>cs.IT math.IT</categories><comments>This paper is submitted to IEEE Trans. Veh. Tech. (Edited: V2):</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Considerable efforts have been devoted to statistical modeling and the
characterization of channels in a range of statistical models for fading
channels. In this paper, we consider a unified approach to model wireless
channels by the mixture of Gaussian (MoG) distribution. Simulations provided
have shown the new probability density function to accurately characterize
multipath fading as well as composite fading channels. We utilize the well
known expectation-maximization algorithm to estimate the parameters of the MoG
model and further utilize the Kullback-Leibler divergence and the mean square
error criteria to demonstrate that our model provides both high accuracy and
low computational complexity, in comparison with existing results.
Additionally, we provide closed form expressions for several performance
metrics used in wireless communication systems, including the moment generating
function, the raw moments, the amount of fading, the outage probability, the
average channel capacity, and the probability of energy detection for cognitive
radio. Numerical Analysis and Monte-Carlo simulations are presented to
corroborate the analytical results and to provide detailed performance
comparisons with the other models in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00879</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00879</id><created>2015-03-03</created><updated>2015-06-24</updated><authors><author><keyname>Galindo</keyname><forenames>Carlos</forenames></author><author><keyname>Hernando</keyname><forenames>Fernando</forenames></author><author><keyname>Ruano</keyname><forenames>Diego</forenames></author></authors><title>Stabilizer quantum codes from $J$-affine variety codes and a new
  Steane-like enlargement</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  New stabilizer codes with parameters better than the ones available in the
literature are provided in this work, in particular quantum codes with
parameters $[[127,63, \geq 12]]_2$ and $[[63,45, \geq 6]]_4$ that are records.
These codes are constructed with a new generalization of the Steane's
enlargement procedure and by considering orthogonal subfield-subcodes --with
respect to the Euclidean and Hermitian inner product-- of a new family of
linear codes, the $J$-affine variety codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00883</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00883</id><created>2015-03-03</created><authors><author><keyname>Amato</keyname><forenames>Gianluca</forenames></author><author><keyname>Scozzari</keyname><forenames>Francesca</forenames></author><author><keyname>Seidl</keyname><forenames>Helmut</forenames></author><author><keyname>Apinis</keyname><forenames>Kalmer</forenames></author><author><keyname>Vojdani</keyname><forenames>Vesal</forenames></author></authors><title>Efficiently intertwining widening and narrowing</title><categories>cs.PL</categories><comments>For submission to Science of Computer Programming</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-trivial analysis problems require posets with infinite ascending and
descending chains. In order to compute reasonably precise post-fixpoints of the
resulting systems of equations, Cousot and Cousot have suggested accelerated
fixpoint iteration by means of widening and narrowing.
  The strict separation into phases, however, may unnecessarily give up
precision that cannot be recovered later, as over-approximated interim results
have to be fully propagated through the equation the system. Additionally,
classical two-phased approach is not suitable for equation systems with
infinitely many unknowns---where demand driven solving must be used.
Construction of an intertwined approach must be able to answer when it is safe
to apply narrowing---or when widening must be applied. In general, this is a
difficult problem. In case the right-hand sides of equations are monotonic,
however, we can always apply narrowing whenever we have reached a post-fixpoint
for an equation. The assumption of monotonicity, though, is not met in presence
of widening. It is also not met by equation systems corresponding to
context-sensitive inter-procedural analysis, possibly combining
context-sensitive analysis of local information with flow-insensitive analysis
of globals.
  As a remedy, we present a novel operator that combines a given widening
operator with a given narrowing operator. We present adapted versions of
round-robin as well as of worklist iteration, local and side-effecting solving
algorithms for the combined operator and prove that the resulting solvers
always return sound results and are guaranteed to terminate for monotonic
systems whenever only finitely many unknowns (constraint variables) are
encountered. Practical remedies are proposed for termination in the
non-monotonic case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00886</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00886</id><created>2015-03-03</created><authors><author><keyname>Hamano</keyname><forenames>Masahiro</forenames></author><author><keyname>Scott</keyname><forenames>Philip</forenames></author></authors><title>On Geometry of Interaction for Polarized Linear Logic</title><categories>cs.LO</categories><comments>54 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present Geometry of Interaction (GoI) models for Multiplicative Polarized
Linear Logic, MLLP, the multiplicative fragment (without structural rules) of
Olivier Laurent's Polarized Linear Logic. This is done by uniformly adding
multipoints to various categorical models of GoI. Multipoints are shown to play
an essential role in semantically characterizing the dynamics of proof networks
in polarized proof theory. They permit us to characterize the key feature of
polarization, focusing, as well as playing a fundamental role in helping us
construct concrete polarized GoI models.
  Our approach to polarized GoI involves two independent studies based on
different categorical approaches to GoI.
  (i) Inspired by work of Abramsky, Haghverdi, and Scott, a polarized GoI
situation is defined which adds multipoints to a traced monoidal category with
an appropriate reflexive object U. Categorical versions of Girard's Execution
formula (taking into account the multipoints) are defined, as well as the GoI
interpretation of MLLP proofs. The Execution formula is shown to characterize
the focusing property (thus polarities) as well as the dynamics of
cut-elimination. (ii) The Int construction of Joyal-Street-Verity is another
fundamental categorical structure associated to GoI. Here, we investigate it in
a multipointed setting compatible with the existence of certain weak pullbacks.
This yields a method for constructing denotational models of MLLP, in
particular a compact version of Hamano-Scott's polarized categories. These are
built from a contravariant duality between so-called positive and negative
monoidal categories, along with an appropriate module structure (representing
&quot;non-focused proofs&quot;) between them.
  As a special case of (ii) above, a compact model of MLLP is also presented
based on Rel_+ with multi-points.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00899</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00899</id><created>2015-03-03</created><authors><author><keyname>Jovanovic</keyname><forenames>Raka</forenames></author><author><keyname>Tuba</keyname><forenames>Milan</forenames></author><author><keyname>Voss</keyname><forenames>Stefan</forenames></author></authors><title>An Ant Colony Optimization Algorithm for Partitioning Graphs with Supply
  and Demand</title><categories>cs.AI</categories><doi>10.1016/j.asoc.2016.01.013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we focus on finding high quality solutions for the problem of
maximum partitioning of graphs with supply and demand (MPGSD). There is a
growing interest for the MPGSD due to its close connection to problems
appearing in the field of electrical distribution systems, especially for the
optimization of self-adequacy of interconnected microgrids. We propose an ant
colony optimization algorithm for the problem. With the goal of further
improving the algorithm we combine it with a previously developed correction
procedure. In our computational experiments we evaluate the performance of the
proposed algorithm on both trees and general graphs. The tests show that the
method manages to find optimal solutions in more than 50% of the problem
instances, and has an average relative error of less than 0.5% when compared to
known optimal solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00900</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00900</id><created>2015-03-03</created><authors><author><keyname>Virmani</keyname><forenames>Deepali</forenames></author><author><keyname>Taneja</keyname><forenames>Shweta</forenames></author><author><keyname>Malhotra</keyname><forenames>Geetika</forenames></author></authors><title>Normalization based K means Clustering Algorithm</title><categories>cs.LG cs.DB</categories><comments>5 pages, 4 figures in International Journal of Advanced Engineering
  Research and Science (IJAERS)-Feb 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  K-means is an effective clustering technique used to separate similar data
into groups based on initial centroids of clusters. In this paper,
Normalization based K-means clustering algorithm(N-K means) is proposed.
Proposed N-K means clustering algorithm applies normalization prior to
clustering on the available data as well as the proposed approach calculates
initial centroids based on weights. Experimental results prove the betterment
of proposed N-K means clustering algorithm over existing K-means clustering
algorithm in terms of complexity and overall performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00916</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00916</id><created>2015-03-03</created><authors><author><keyname>Rassouli</keyname><forenames>Borzoo</forenames></author><author><keyname>Hao</keyname><forenames>Chenxi</forenames></author><author><keyname>Clerckx</keyname><forenames>Bruno</forenames></author></authors><title>DoF Analysis of the K-user MISO Broadcast Channel with Hybrid CSIT</title><categories>cs.IT math.IT</categories><comments>To appear in ICC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a $K$-user multiple-input single-output (MISO) broadcast channel
(BC) where the channel state information (CSI) of user $i(i=1,2,\ldots,K)$ may
be either instantaneously perfect (P), delayed (D) or not known (N) at the
transmitter with probabilities $\lambda_P^i$, $\lambda_D^i$ and $\lambda_N^i$,
respectively. In this setting, according to the three possible CSIT for each
user, knowledge of the joint CSIT of the $K$ users could have at most $3^K$
states. Although the results by Tandon et al. show that for the symmetric two
user MISO BC (i.e., $\lambda_Q^i=\lambda_Q,\ \forall i\in \{1,2\}, Q\in
\{P,D,N\}$), the Degrees of Freedom (DoF) region depends only on the marginal
probabilities, we show that this interesting result does not hold in general
when $K\geq3$. In other words, the DoF region is a function of all the joint
probabilities. In this paper, given the marginal probabilities of CSIT, we
derive an outer bound for the DoF region of the $K$-user MISO BC. Subsequently,
we investigate the achievability of the outer bound in some scenarios. Finally,
we show the dependence of the DoF region on the joint probabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00923</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00923</id><created>2015-03-03</created><authors><author><keyname>Misra</keyname><forenames>Prasant</forenames></author><author><keyname>Rajaraman</keyname><forenames>Vasanth</forenames></author><author><keyname>Dhotrad</keyname><forenames>Kumaresh</forenames></author><author><keyname>Warrior</keyname><forenames>Jay</forenames></author><author><keyname>Simmhan</keyname><forenames>Yogesh</forenames></author></authors><title>An Interoperable Realization of Smart Cities with Plug and Play based
  Device Management</title><categories>cs.SY</categories><comments>arXiv admin note: text overlap with arXiv:1502.00797</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The primal problem with Internet of Things (IoT) solutions for smart cities
is the lack of interoperability at various levels, and more predominately at
the device level. While there exist multitude of platforms from multiple
manufacturers, the existing ecosystem still remains highly closed. In this
paper, we propose SNaaS or Sensor/Network as a Service: a service layer that
enables the creation of the plug-n-play infrastructure, across platforms from
multiple vendors, necessary for interoperability and successful deployment of
large-scale city wide systems. In order to correctly position the new service
layer, we present a high level reference IoT architecture for smart city
implementations, and follow it up with the workflow details of SNaaS along with
preliminary microbenchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00941</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00941</id><created>2015-03-03</created><updated>2015-05-09</updated><authors><author><keyname>Amanatidis</keyname><forenames>Georgios</forenames></author><author><keyname>Markakis</keyname><forenames>Evangelos</forenames></author><author><keyname>Nikzad</keyname><forenames>Afshin</forenames></author><author><keyname>Saberi</keyname><forenames>Amin</forenames></author></authors><title>Approximation Algorithms for Computing Maximin Share Allocations</title><categories>cs.GT</categories><acm-class>F.2.2; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of computing maximin share guarantees, a recently
introduced fairness notion. Given a set of $n$ agents and a set of goods, the
maximin share of a single agent is the best that she can guarantee to herself,
if she would be allowed to partition the goods in any way she prefers, into $n$
bundles, and then receive her least desirable bundle. The objective then in our
problem is to find a partition, so that each agent is guaranteed her maximin
share. In settings with indivisible goods, such allocations are not guaranteed
to exist, so we resort to approximation algorithms. Our main result is a
$2/3$-approximation, that runs in polynomial time for any number of agents.
This improves upon the algorithm of Procaccia and Wang, which also produces a
$2/3$-approximation but runs in polynomial time only for a constant number of
agents. To achieve this, we redesign certain parts of their algorithm.
Furthermore, motivated by the apparent difficulty, both theoretically and
experimentally, in finding lower bounds on the existence of approximate
solutions, we undertake a probabilistic analysis. We prove that in randomly
generated instances, with high probability there exists a maximin share
allocation. This can be seen as a justification of the experimental evidence
reported in relevant works.
  Finally, we provide further positive results for two special cases that arise
from previous works. The first one is the intriguing case of $3$ agents, for
which it is already known that exact maximin share allocations do not always
exist (contrary to the case of $2$ agents). We provide a $7/8$-approximation
algorithm, improving the previously known result of $3/4$. The second case is
when all item values belong to $\{0, 1, 2\}$, extending the $\{0, 1\}$ setting
studied in Bouveret and Lema\^itre. We obtain an exact algorithm for any number
of agents in this case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00943</identifier>
 <datestamp>2015-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00943</id><created>2015-03-03</created><updated>2015-03-20</updated><authors><author><keyname>Khan</keyname><forenames>Muhammad Asad</forenames></author><author><keyname>Khan</keyname><forenames>Amir A</forenames></author><author><keyname>Mirza</keyname><forenames>Fauzan</forenames></author></authors><title>Transform Domain Analysis of Sequences</title><categories>cs.CR</categories><comments>This is a comprehensive report with over 20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In cryptanalysis, security of ciphers vis-a-vis attacks is gauged against
three criteria of complexities, i.e., computations, memory and time. Some
features may not be so apparent in a particular domain, and their analysis in a
transformed domain often reveals interesting patterns. Moreover, the complexity
criteria in different domains are different and performance improvements are
often achieved by transforming the problem in an alternate domain. Owing to the
results of coding theory and signal processing, Discrete Fourier Transform
(DFT) based attacks have proven to be efficient than algebraic attacks in terms
of their computational complexity. Motivated by DFT based attacks, we present a
transform domain analysis of Linear Feedback Shift Register(LFSR) based
sequence generators. The time and frequency domain behavior of non-linear
filter and combiner generators is discussed along with some novel observations
based on the Chinese Remainder Theorem (CRT). CRT is exploited to establish
patterns in LFSR sequences and underlying cyclic structures of finite fields.
Application of DFT spectra attacks on combiner generators is also demonstrated.
Our proposed method saves on the last stage computations of selective DFT
attacks for combiner generators. The proposed approach is demonstrated on some
examples of combiner generators and is scalable to general configuration of
combiner generators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00948</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00948</id><created>2015-03-03</created><updated>2015-10-08</updated><authors><author><keyname>Dumas</keyname><forenames>Jean-Guillaume</forenames><affiliation>LJK</affiliation></author><author><keyname>Duval</keyname><forenames>Dominique</forenames><affiliation>LJK</affiliation></author><author><keyname>Ekici</keyname><forenames>Burak</forenames><affiliation>LJK</affiliation></author><author><keyname>Pous</keyname><forenames>Damien</forenames><affiliation>LIP</affiliation></author><author><keyname>Reynaud</keyname><forenames>Jean-Claude</forenames><affiliation>RC</affiliation></author></authors><title>Hilbert-Post completeness for the state and the exception effects</title><categories>cs.LO</categories><comments>Siegfried Rump (Hamburg University of Technology), Chee Yap (Courant
  Institute, NYU). Sixth International Conference on Mathematical Aspects of
  Computer and Information Sciences , Nov 2015, Berlin, Germany. 2015, LNCS</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a novel framework for studying the syntactic
completeness of computational effects and we apply it to the exception effect.
When applied to the states effect, our framework can be seen as a
generalization of Pretnar's work on this subject. We first introduce a relative
notion of Hilbert-Post completeness, well-suited to the composition of effects.
Then we prove that the exception effect is relatively Hilbert-Post complete, as
well as the &quot;core&quot; language which may be used for implementing it; these proofs
have been formalized and checked with the proof assistant Coq.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00949</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00949</id><created>2015-03-03</created><updated>2016-02-22</updated><authors><author><keyname>Cinbis</keyname><forenames>Ramazan Gokberk</forenames></author><author><keyname>Verbeek</keyname><forenames>Jakob</forenames></author><author><keyname>Schmid</keyname><forenames>Cordelia</forenames></author></authors><title>Weakly Supervised Object Localization with Multi-fold Multiple Instance
  Learning</title><categories>cs.CV</categories><comments>To appear in IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Object category localization is a challenging problem in computer vision.
Standard supervised training requires bounding box annotations of object
instances. This time-consuming annotation process is sidestepped in weakly
supervised learning. In this case, the supervised information is restricted to
binary labels that indicate the absence/presence of object instances in the
image, without their locations. We follow a multiple-instance learning approach
that iteratively trains the detector and infers the object locations in the
positive training images. Our main contribution is a multi-fold multiple
instance learning procedure, which prevents training from prematurely locking
onto erroneous object locations. This procedure is particularly important when
using high-dimensional representations, such as Fisher vectors and
convolutional neural network features. We also propose a window refinement
method, which improves the localization accuracy by incorporating an objectness
prior. We present a detailed experimental evaluation using the PASCAL VOC 2007
dataset, which verifies the effectiveness of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00980</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00980</id><created>2015-03-03</created><authors><author><keyname>Lai</keyname><forenames>Xiangjing</forenames></author><author><keyname>Hao</keyname><forenames>Jin-Kao</forenames></author></authors><title>On memetic search for the max-mean dispersion problem</title><categories>cs.AI</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set $V$ of $n$ elements and a distance matrix $[d_{ij}]_{n\times n}$
among elements, the max-mean dispersion problem (MaxMeanDP) consists in
selecting a subset $M$ from $V$ such that the mean dispersion (or distance)
among the selected elements is maximized. Being a useful model to formulate
several relevant applications, MaxMeanDP is known to be NP-hard and thus
computationally difficult. In this paper, we present a highly effective memetic
algorithm for MaxMeanDP which relies on solution recombination and local
optimization to find high quality solutions. Computational experiments on the
set of 160 benchmark instances with up to 1000 elements commonly used in the
literature show that the proposed algorithm improves or matches the published
best known results for all instances in a short computing time, with only one
exception, while achieving a high success rate of 100\%. In particular, we
improve 59 previous best results out of the 60 most challenging instances.
Results on a set of 40 new large instances with 3000 and 5000 elements are also
presented. The key ingredients of the proposed algorithm are investigated to
shed light on how they affect the performance of the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00981</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00981</id><created>2015-03-03</created><authors><author><keyname>Stepanov</keyname><forenames>Sander</forenames></author><author><keyname>Venetsanopoulos</keyname><forenames>Anastasios</forenames></author></authors><title>Morphological Detector for Multilevel Signals in epsilon-Noise</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The novel approach was developed for multilevel signal detection in channels
with impulsive non-Gaussian noise. This approach consists of using
morphological nonlinear image filtration principles for two dimensional
signals. It is a new method of signal demodulation, using three - dimensional
image processing algorithms. Successful results of this morphological detector
encourage more investigation towards using image processing theory and
algorithms for two dimensional signal processing. As can be seen in the example
in section IV, this new approach of reusing well developed and extensively
developing image processing has significantly improved performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00992</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00992</id><created>2015-03-03</created><authors><author><keyname>Mirebeau</keyname><forenames>Jean-Marie</forenames><affiliation>CEREMADE</affiliation></author><author><keyname>Fehrenbach</keyname><forenames>J&#xe9;r&#xf4;me</forenames><affiliation>IMT</affiliation></author><author><keyname>Risser</keyname><forenames>Laurent</forenames><affiliation>IMT</affiliation></author><author><keyname>Tobji</keyname><forenames>Shaza</forenames><affiliation>IMT</affiliation></author></authors><title>Anisotropic Diffusion in ITK</title><categories>cs.CV math.AP</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Anisotropic Non-Linear Diffusion is a powerful image processing technique,
which allows to simultaneously remove the noise and enhance sharp features in
two or three dimensional images. Anisotropic Diffusion is understood here in
the sense of Weickert, meaning that diffusion tensors are anisotropic and
reflect the local orientation of image features. This is in contrast with the
non-linear diffusion filter of Perona and Malik, which only involves scalar
diffusion coefficients, in other words isotropic diffusion tensors. In this
paper, we present an anisotropic non-linear diffusion technique we implemented
in ITK. This technique is based on a recent adaptive scheme making the
diffusion stable and requiring limited numerical resources. (See supplementary
data.)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.00993</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.00993</id><created>2015-03-03</created><updated>2015-03-13</updated><authors><author><keyname>Tejada</keyname><forenames>Arturo</forenames></author><author><keyname>Horv&#xe1;th</keyname><forenames>Klaudia</forenames></author><author><keyname>Shiromoto</keyname><forenames>Humberto Stein</forenames></author><author><keyname>Bosman</keyname><forenames>Hedde</forenames></author></authors><title>Towards WaterLab: A Test Facility for New Cyber-Physical Technologies in
  Water Distribution Networks</title><categories>math.OC cs.SY</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reports the initial steps in the development of WaterLab, an
ambitious experimental facility for the testing of new cyber-physical
technologies in drinking water distribution networks (DWDN). WaterLab's initial
focus is on wireless control networks and on data-based, distributed anomaly
detection over wireless sensor networks. The former can be used to control the
hydraulic properties of a DWDN, while the latter can be used for in-situ
detection and isolation of contamination and hydraulic faults.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01002</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01002</id><created>2015-03-03</created><authors><author><keyname>Wang</keyname><forenames>Weiran</forenames></author><author><keyname>Lu</keyname><forenames>Canyi</forenames></author></authors><title>Projection onto the capped simplex</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a simple and efficient algorithm for computing the Euclidean
projection of a point onto the capped simplex---a simplex with an additional
uniform bound on each coordinate---together with an elementary proof. Both the
MATLAB and C++ implementations of the proposed algorithm can be downloaded at
https://eng.ucmerced.edu/people/wwang5.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01007</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01007</id><created>2015-03-03</created><updated>2015-06-01</updated><authors><author><keyname>Joulin</keyname><forenames>Armand</forenames></author><author><keyname>Mikolov</keyname><forenames>Tomas</forenames></author></authors><title>Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets</title><categories>cs.NE cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the recent achievements in machine learning, we are still very far
from achieving real artificial intelligence. In this paper, we discuss the
limitations of standard deep learning approaches and show that some of these
limitations can be overcome by learning how to grow the complexity of a model
in a structured way. Specifically, we study the simplest sequence prediction
problems that are beyond the scope of what is learnable with standard recurrent
networks, algorithmically generated sequences which can only be learned by
models which have the capacity to count and to memorize sequences. We show that
some basic algorithms can be learned from sequential data using a recurrent
network associated with a trainable memory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01008</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01008</id><created>2015-03-03</created><updated>2016-01-18</updated><authors><author><keyname>d'Auriac</keyname><forenames>J. -A. Angles</forenames></author><author><keyname>Bujtas</keyname><forenames>Cs.</forenames></author><author><keyname>Maftouhi</keyname><forenames>A. El</forenames></author><author><keyname>Karpinski</keyname><forenames>M.</forenames></author><author><keyname>Manoussakis</keyname><forenames>Y.</forenames></author><author><keyname>Montero</keyname><forenames>L.</forenames></author><author><keyname>Narayanan</keyname><forenames>N.</forenames></author><author><keyname>Rosaz</keyname><forenames>L.</forenames></author><author><keyname>Thapper</keyname><forenames>J.</forenames></author><author><keyname>Tuza</keyname><forenames>Zs.</forenames></author></authors><title>Tropical Dominating Sets in Vertex-Coloured Graphs</title><categories>cs.DM</categories><comments>19 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a vertex-coloured graph, a dominating set is said to be tropical if
every colour of the graph appears at least once in the set. Here, we study
minimum tropical dominating sets from structural and algorithmic points of
view. First, we prove that the tropical dominating set problem is NP-complete
even when restricted to a simple path. Then, we establish upper bounds related
to various parameters of the graph such as minimum degree and number of edges.
We also give upper bounds for random graphs. Last, we give approximability and
inapproximability results for general and restricted classes of graphs, and
establish a FPT algorithm for interval graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01034</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01034</id><created>2015-03-03</created><updated>2015-10-13</updated><authors><author><keyname>Kissinger</keyname><forenames>Aleks</forenames></author><author><keyname>Zamdzhiev</keyname><forenames>Vladimir</forenames></author></authors><title>Quantomatic: A Proof Assistant for Diagrammatic Reasoning</title><categories>cs.LO cs.MS math.CT</categories><comments>International Conference on Automated Deduction, CADE 2015 (CADE-25).
  The final publication is available at Springer via
  http://dx.doi.org/10.1007/978-3-319-21401-6_22</comments><doi>10.1007/978-3-319-21401-6_22</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Monoidal algebraic structures consist of operations that can have multiple
outputs as well as multiple inputs, which have applications in many areas
including categorical algebra, programming language semantics, representation
theory, algebraic quantum information, and quantum groups. String diagrams
provide a convenient graphical syntax for reasoning formally about such
structures, while avoiding many of the technical challenges of a term-based
approach. Quantomatic is a tool that supports the (semi-)automatic construction
of equational proofs using string diagrams. We briefly outline the theoretical
basis of Quantomatic's rewriting engine, then give an overview of the core
features and architecture and give a simple example project that computes
normal forms for commutative bialgebras.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01051</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01051</id><created>2015-03-03</created><authors><author><keyname>Beckers</keyname><forenames>Sander</forenames></author><author><keyname>Vennekens</keyname><forenames>Joost</forenames></author></authors><title>Combining Probabilistic, Causal, and Normative Reasoning in CP-logic</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years the search for a proper formal definition of actual causation
-- i.e., the relation of cause-effect as it is instantiated in specific
observations, rather than general causal relations -- has taken on impressive
proportions. In part this is due to the insight that this concept plays a
fundamental role in many different fields, such as legal theory, engineering,
medicine, ethics, etc. Because of this diversity in applications, some
researchers have shifted focus from a single idealized definition towards a
more pragmatic, context-based account. For instance, recent work by Halpern and
Hitchcock draws on empirical research regarding people's causal judgments, to
suggest a graded and context-sensitive notion of causation. Although we
sympathize with many of their observations, their restriction to a merely
qualitative ordering runs into trouble for more complex examples. Therefore we
aim to improve on their approach, by using the formal language of CP-logic
(Causal Probabilistic logic), and the framework for defining actual causation
that was developed by the current authors using it. First we rephrase their
ideas into our quantitative, probabilistic setting, after which we modify it to
accommodate a greater class of examples. Further, we introduce a formal
distinction between statistical and normative considerations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01052</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01052</id><created>2015-03-03</created><authors><author><keyname>Kara</keyname><forenames>Emre Can</forenames></author><author><keyname>Macdonald</keyname><forenames>Jason S.</forenames></author><author><keyname>Black</keyname><forenames>Douglas</forenames></author><author><keyname>Berges</keyname><forenames>Mario</forenames></author><author><keyname>Hug</keyname><forenames>Gabriela</forenames></author><author><keyname>Kiliccote</keyname><forenames>Sila</forenames></author></authors><title>Estimating the Benefits of Electric Vehicle Smart Charging at
  Non-Residential Locations: A Data-Driven Approach</title><categories>cs.SY</categories><comments>Pre-print, under review at Applied Energy</comments><msc-class>90C11</msc-class><acm-class>J.2</acm-class><journal-ref>Applied Energy, Volume 155, 1 October 2015, Pages 515 525</journal-ref><doi>10.1016/j.apenergy.2015.05.072</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we use data collected from over 2000 non-residential electric
vehicle supply equipments (EVSEs) located in Northern California for the year
of 2013 to estimate the potential benefits of smart electric vehicle (EV)
charging. We develop a smart charging framework to identify the benefits of
non-residential EV charging to the load aggregators and the distribution grid.
Using this extensive dataset, we aim to improve upon past studies focusing on
the benefits of smart EV charging by relaxing the assumptions made in these
studies regarding: (i) driving patterns, driver behavior and driver types; (ii)
the scalability of a limited number of simulated vehicles to represent
different load aggregation points in the power system with different customer
characteristics; and (iii) the charging profile of EVs. First, we study the
benefits of EV aggregations behind-the-meter, where a time-of-use pricing
schema is used to understand the benefits to the owner when EV aggregations
shift load from high cost periods to lower cost periods. For the year of 2013,
we show a reduction of up to 24.8% in the monthly bill is possible. Then,
following a similar aggregation strategy, we show that EV aggregations decrease
their contribution to the system peak load by approximately 40% when charging
is controlled within arrival and departure times. Our results also show that it
could be expected to shift approximately 0.25kWh (~2.8%) of energy per
non-residential EV charging session from peak periods (12PM-6PM) to off-peak
periods (after 6PM) in Northern California for the year of 2013.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01056</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01056</id><created>2015-03-03</created><authors><author><keyname>Lv</keyname><forenames>Tiejun</forenames></author><author><keyname>Gao</keyname><forenames>Hui</forenames></author><author><keyname>Yang</keyname><forenames>Shaoshi</forenames></author></authors><title>Secrecy Transmit Beamforming for Heterogeneous Networks</title><categories>cs.IT math.IT</categories><comments>17 pages, 14 figures, 3 algorithms and 1 table, to appear in IEEE
  Journal on Selected Areas in Communications, 2015</comments><journal-ref>IEEE Journal on Selected Areas in Communications -- Special Issue
  on Recent Advances in Heterogeneous Cellular Networks, vol. 33, no. 6, pp.
  1154-1170, Jun. 2015</journal-ref><doi>10.1109/JSAC.2015.2416984</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we pioneer the study of physical-layer security in
heterogeneous networks (HetNets). We investigate secure communications in a
two-tier downlink HetNet, which comprises one macrocell and several femtocells.
Each cell has multiple users and an eavesdropper attempts to wiretap the
intended macrocell user. Firstly, we consider an orthogonal spectrum allocation
strategy to eliminate co-channel interference, and propose the secrecy transmit
beamforming only operating in the macrocell (STB-OM) as a partial solution for
secure communication in HetNet. Next, we consider a secrecy-oriented
non-orthogonal spectrum allocation strategy and propose two cooperative STBs
which rely on the collaboration amongst the macrocell base station (MBS) and
the adjacent femtocell base stations (FBSs). Our first cooperative STB is the
STB sequentially operating in the macrocell and femtocells (STB-SMF), where the
cooperative FBSs individually design their STB matrices and then feed their
performance metrics to the MBS for guiding the STB in the macrocell. Aiming to
improve the performance of STB-SMF, we further propose the STB jointly designed
in the macrocell and femtocells (STB-JMF), where all cooperative FBSs feed
channel state information to the MBS for designing the joint STB. Unlike
conventional STBs conceived for broadcasting or interference channels, the
three proposed STB schemes all entail relatively sophisticated optimizations
due to QoS constraints of the legitimate users. In order to efficiently use
these STB schemes, the original optimization problems are reformulated and
convex optimization techniques, such as second-order cone programming and
semidefinite programming, are invoked to obtain the optimal solutions.
Numerical results demonstrate that the proposed STB schemes are highly
effective in improving the secrecy rate performance of HetNet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01057</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01057</id><created>2015-03-03</created><authors><author><keyname>Wilson</keyname><forenames>Andrew Gordon</forenames></author><author><keyname>Nickisch</keyname><forenames>Hannes</forenames></author></authors><title>Kernel Interpolation for Scalable Structured Gaussian Processes
  (KISS-GP)</title><categories>cs.LG stat.ML</categories><comments>19 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new structured kernel interpolation (SKI) framework, which
generalises and unifies inducing point methods for scalable Gaussian processes
(GPs). SKI methods produce kernel approximations for fast computations through
kernel interpolation. The SKI framework clarifies how the quality of an
inducing point approach depends on the number of inducing (aka interpolation)
points, interpolation strategy, and GP covariance kernel. SKI also provides a
mechanism to create new scalable kernel methods, through choosing different
kernel interpolation strategies. Using SKI, with local cubic kernel
interpolation, we introduce KISS-GP, which is 1) more scalable than inducing
point alternatives, 2) naturally enables Kronecker and Toeplitz algebra for
substantial additional gains in scalability, without requiring any grid data,
and 3) can be used for fast and expressive kernel learning. KISS-GP costs O(n)
time and storage for GP inference. We evaluate KISS-GP for kernel matrix
approximation, kernel learning, and natural sound modelling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01058</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01058</id><created>2015-03-03</created><authors><author><keyname>Cariow</keyname><forenames>Aleksandr</forenames></author><author><keyname>Cariowa</keyname><forenames>Galina</forenames></author><author><keyname>Kubsik</keyname><forenames>Bartosz</forenames></author></authors><title>An algorithm for multiplication of split-octonions</title><categories>cs.DS</categories><comments>14 pages, 4 figures. arXiv admin note: substantial text overlap with
  arXiv:1502.06250</comments><msc-class>15A23, 15A66, 65F30, 65Y20</msc-class><acm-class>F.2.1; I.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce efficient algorithm for the multiplication of
split-octonions. The direct multiplication of two split-octonions requires 64
real multiplications and 56 real additions. More effective solutions still do
not exist. We show how to compute a product of the split-octonions with 28 real
multiplications and 92 real additions. During synthesis of the discussed
algorithm we use the fact that product of two split-octonions may be
represented as vector-matrix product. The matrix that participates in the
product calculating has unique structural properties that allow performing its
advantageous decomposition. Namely this decomposition leads to significant
reducing of the multiplicative complexity of split-octonions multiplication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01061</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01061</id><created>2015-03-03</created><updated>2015-04-14</updated><authors><author><keyname>Marinescu</keyname><forenames>Dan C.</forenames></author><author><keyname>Paya</keyname><forenames>Ashkan</forenames></author><author><keyname>Morrison</keyname><forenames>John P.</forenames></author><author><keyname>Healy</keyname><forenames>Philip</forenames></author></authors><title>Distributed Hierarchical Control versus an Economic Model for Cloud
  Resource Management</title><categories>cs.DC</categories><comments>13 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate a hierarchically organized cloud infrastructure and compare
distributed hierarchical control based on resource monitoring with market
mechanisms for resource management. The latter do not require a model of the
system, incur a low overhead, are robust, and satisfy several other desiderates
of autonomic computing. We introduce several performance measures and report on
simulation studies which show that a straightforward bidding scheme supports an
effective admission control mechanism, while reducing the communication
complexity by several orders of magnitude and also increasing the acceptance
rate compared to hierarchical control and monitoring mechanisms. Resource
management based on market-based mechanisms can be seen as an intermediate step
towards cloud self-organization, an ideal alternative to current mechanisms for
cloud resource management.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01063</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01063</id><created>2015-03-03</created><authors><author><keyname>Voskoboynik</keyname><forenames>Niv</forenames></author><author><keyname>Permuter</keyname><forenames>Haim H.</forenames></author><author><keyname>Cohen</keyname><forenames>Asaf</forenames></author></authors><title>On the Capacity of Wireless Networks with Random Transmission Delay</title><categories>cs.IT math.IT</categories><comments>15 pages, 19 figures, submitted to the IEEE/ACM Transactions on
  Networking</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce novel coding schemes for wireless networks with
random transmission delays. These coding schemes obviate the need for
synchronicity, reduce the number of transmissions and achieve the optimal rate
region in the corresponding wired model for both multiple unicast and multicast
cases with up to three users under the equal rate constraint. The coding
schemes are presented in two phases; first, coding schemes for line, star and
line-star topologies with random transmission delays are provided. Second, any
general topology with multiple bidirectional unicast and multicast sessions is
shown to be decomposable into these canonical topologies to reduce the number
of transmissions without rate redundancy. As a result, the coding schemes
developed for the line, star and line-star topologies serve as building blocks
for the construction of more general coding schemes for all networks. The
proposed schemes are proved to be Real Time (RT) for wireless networks in the
sense that they achieve the minimal decoding delay. With a negligible size
header, these coding schemes are shown to be applicable to unsynchronized
networks, i.e., networks with random transmission delays. Finally, we
demonstrate the applicability of these schemes by extensive simulations. The
implementation of such coding schemes on a wireless network with random
transmission delay can improve performance and power efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01065</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01065</id><created>2015-02-28</created><authors><author><keyname>Kohls</keyname><forenames>Christian</forenames></author></authors><title>Collaboration Tools and Patterns for Creative Thinking</title><categories>cs.CY</categories><comments>Proceedings of the 5th International Conference on Collaborative
  Innovation Networks COINs15, Tokyo, Japan March 12-14, 2015
  (arXiv:1502.01142)</comments><report-no>coins15/2015/19</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many creativity methods follow similar structures and principles. Design
Patterns capture such invariants of proven good practices and discuss why, when
and how creative thinking methods match various situations of collaboration.
Moreover patterns connect different forms with each other. Once we understand
the underlying structures of creative thinking processes we can facilitate
digital tools to support them. While such tools can foster the effective
application of established methods and even change their properties, tools can
also enable new patterns of collaboration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01066</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01066</id><created>2015-02-28</created><authors><author><keyname>Matsuura</keyname><forenames>Rie</forenames></author><author><keyname>Okabe</keyname><forenames>Daisuke</forenames></author></authors><title>Collective achievement of making in cosplay culture</title><categories>cs.CY</categories><comments>Proceedings of the 5th International Conference on Collaborative
  Innovation Networks COINs15, Tokyo, Japan March 12-14, 2015
  (arXiv:1502.01142)</comments><report-no>coins15/2015/15</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper analyzes peer-based learning and the concept of Scaffolding
represented in ethnographic case studies of ten female informants aged 20-25
participating in the cosplay community. Cosplay is a female-dominated niche
subculture of extreme fans and mavens, who are devoted to dressing up as
characters from manga, games, and anime. Cosplayers are highly conscious of
quality standards for costumes, makeup, and accessories. Cosplay events and
dedicated SNSs for cosplayers are a valuable venue for exchanging information
about costume making. First we frame this work as an effort to think about
their learning environment using the concept of connected learning by Ito et
al(2013). Then we share an overview of cosplay culture in Japan and our
methodologies based on interviews and fieldwork. The interview transcripts were
analyzed according to the Steps for coding and Theorization method, a
qualitative data analysis technique by Otani (2008). In this study, We focus on
their reciprocal learning and expanding the concept of Scaffolding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01067</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01067</id><created>2015-02-28</created><authors><author><keyname>Hong</keyname><forenames>Jei-Hee</forenames></author><author><keyname>Akado</keyname><forenames>Yuma</forenames></author><author><keyname>Kogure</keyname><forenames>Sakurako</forenames></author><author><keyname>Sasabe</keyname><forenames>Alice</forenames></author><author><keyname>Saruwatari</keyname><forenames>Keishi</forenames></author><author><keyname>Iba</keyname><forenames>Takashi</forenames></author></authors><title>Exploring Cultures through Pattern Mining - Practices from Generative
  Beauty Workshops</title><categories>cs.CY</categories><comments>Proceedings of the 5th International Conference on Collaborative
  Innovation Networks COINs15, Tokyo, Japan March 12-14, 2015
  (arXiv:1502.01142)</comments><report-no>coins15/2015/21</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a method for understanding personal ways of thinking and
doing in daily lives among different countries by mining their ways as patterns
in a sense of pattern language. Pattern language is a methodology of describing
tacit practical knowledge, where each pattern consists of context, problem, and
solution. In this paper, patterns mined from the workshops we held in the
following three countries: Japan, Korea, and the United States, are analysed.
The results demonstrate similarities and reflect characteristics of the
patterns of each country. We anticipate that this workshop can be used as a
method for better understanding of cultural similarities and features in the
light of practical knowledge in daily lives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01068</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01068</id><created>2015-03-03</created><updated>2015-06-01</updated><authors><author><keyname>Zetzsche</keyname><forenames>Georg</forenames></author></authors><title>An approach to computing downward closures</title><categories>cs.FL</categories><comments>Full version of contribution to ICALP 2015. Comments welcome</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The downward closure of a word language is the set of all (not necessarily
contiguous) subwords of its members. It is well-known that the downward closure
of any language is regular. While the downward closure appears to be a powerful
abstraction, algorithms for computing a finite automaton for the downward
closure of a given language have been established only for few language
classes.
  This work presents a simple general method for computing downward closures.
For language classes that are closed under rational transductions, it is shown
that the computation of downward closures can be reduced to checking a certain
unboundedness property.
  This result is used to prove that downward closures are computable for (i)
every language class with effectively semilinear Parikh images that are closed
under rational transductions, (ii) matrix languages, and (iii) indexed
languages (equivalently, languages accepted by higher-order pushdown automata
of order 2).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01070</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01070</id><created>2015-03-03</created><authors><author><keyname>Torabi</keyname><forenames>Atousa</forenames></author><author><keyname>Pal</keyname><forenames>Christopher</forenames></author><author><keyname>Larochelle</keyname><forenames>Hugo</forenames></author><author><keyname>Courville</keyname><forenames>Aaron</forenames></author></authors><title>Using Descriptive Video Services to Create a Large Data Source for Video
  Annotation Research</title><categories>cs.CV cs.AI</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we introduce a dataset of video annotated with high quality
natural language phrases describing the visual content in a given segment of
time. Our dataset is based on the Descriptive Video Service (DVS) that is now
encoded on many digital media products such as DVDs. DVS is an audio narration
describing the visual elements and actions in a movie for the visually
impaired. It is temporally aligned with the movie and mixed with the original
movie soundtrack. We describe an automatic DVS segmentation and alignment
method for movies, that enables us to scale up the collection of a DVS-derived
dataset with minimal human intervention. Using this method, we have collected
the largest DVS-derived dataset for video description of which we are aware.
Our dataset currently includes over 84.6 hours of paired video/sentences from
92 DVDs and is growing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01073</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01073</id><created>2015-02-27</created><authors><author><keyname>Maurer</keyname><forenames>Vinzenz</forenames></author></authors><title>T3PS: Tool for Parallel Processing in Parameter Scans</title><categories>cs.MS cs.CE hep-ph</categories><comments>50 pages, 7 figures, available for download at
  http://t3ps.hepforge.org/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  T3PS is a program that can be used to quickly design and perform parameter
scans while easily taking advantage of the multi-core architecture of current
processors. It takes an easy to read and write parameter scan definition file
format as input. Based on the parameter ranges and other options contained
therein, it distributes the calculation of the parameter space over multiple
processes and possibly computers. The derived data is saved in a plain text
file format readable by most plotting software. The supported scanning
strategies include: grid scan, random scan, Markov Chain Monte Carlo, numerical
optimization. Several example parameter scans are shown and compared with
results in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01082</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01082</id><created>2015-03-03</created><updated>2015-03-15</updated><authors><author><keyname>Carevic</keyname><forenames>Zeljko</forenames></author><author><keyname>Krichel</keyname><forenames>Thomas</forenames></author><author><keyname>Mayr</keyname><forenames>Philipp</forenames></author></authors><title>Assessing a human mediated current awareness service</title><categories>cs.DL cs.IR</categories><comments>12 pages, 5 figures, accepted paper at the 14th International
  Symposium of Information Science (ISI 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an approach for analyzing the behavior of editors
in the large current awareness service &quot;NEP: New Economics Papers&quot;. We
processed data from more than 38,000 issues derived from 90 different NEP
reports over the past ten years. The aim of our analysis was to gain an inside
to the editor behaviour when creating an issue and to look for factors that
influence the success of a report. In our study we looked at the following
features: average editing time, the average number of papers in an issue and
the editor effort measured on presorted issues as relative search length (RSL).
We found an average issue size of 12.4 documents per issue. The average editing
time is rather low with 14.5 minute. We get to the point that the success of a
report is mainly driven by its topic and the number of subscribers, as well as
proactive action by the editor to promote the report in her community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01093</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01093</id><created>2015-03-03</created><updated>2015-03-11</updated><authors><author><keyname>Grabowski</keyname><forenames>Szymon</forenames></author></authors><title>A note on the longest common Abelian factor problem</title><categories>cs.DS</categories><comments>v3 is vastly different to the previous one</comments><msc-class>68W32</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Abelian string matching problems are becoming an object of considerable
interest in last years. Very recently, Alatabbi et al. \cite{AILR2015}
presented the first solution for the longest common Abelian factor problem for
a pair of strings, reaching $O(\sigma n^2)$ time with $O(\sigma n \log n)$ bits
of space, where $n$ is the length of the strings and $\sigma$ is the alphabet
size. In this note we show how the time complexity can be preserved while the
space is reduced by a factor of $\sigma$, and then how the time complexity can
be improved, if the alphabet is not too small, when superlinear space is
allowed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01095</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01095</id><created>2015-03-03</created><authors><author><keyname>Somogyi</keyname><forenames>Endre T.</forenames></author><author><keyname>Bouteiller</keyname><forenames>Jean-Marie</forenames></author><author><keyname>Glazier</keyname><forenames>James A.</forenames></author><author><keyname>K&#xf6;nig</keyname><forenames>Matthias</forenames></author><author><keyname>Medley</keyname><forenames>Kyle</forenames></author><author><keyname>Swat</keyname><forenames>Maciej H.</forenames></author><author><keyname>Sauro</keyname><forenames>Herbert M.</forenames></author></authors><title>libRoadRunner: A High Performance SBML Simulation and Analysis Library</title><categories>q-bio.SC cs.MS</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper presents libRoadRunner, an extensible, high-performance,
cross-platform, open-source software library for the simulation and analysis of
models \ expressed using Systems Biology Markup Language (SBML). SBML is the
most widely used standard for representing dynamic networks, especially
biochemical networks. libRoadRunner supports solution of both large models and
multiple replicas of a single model on desktop, mobile and cluster computers.
libRoadRunner is a self-contained library, able to run both as a component
inside other tools via its C++ and C bindings andnteractively through its
Python interface. The Python Application Programming Interface (API) is similar
to the APIs of Matlab and SciPy, making it fast and easy to learn, even for new
users. libRoadRunner uses a custom Just-In-Time (JIT) compiler built on the
widely-used LLVM JIT compiler framework to compile SBML-specified models
directly into very fast native machine code for a variety of processors, making
it appropriate for solving very large models or multiple replicas of smaller
models. libRoadRunner is flexible, supporting the bulk of the SBML
specification (except for delay and nonlinear algebraic equations) and several
of its extensions. It offers multiple deterministic and stochastic integrators,
as well as tools for steady-state, stability analyses and flux balance
analysis. We regularly update libRoadRunner binary distributions for Mac OS X,
Linux and Windows and license them under Apache License Version 2.0.
http://www.libroadrunner.org provides online documentation, full build
instructions, binaries and a git source repository.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01098</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01098</id><created>2015-03-03</created><authors><author><keyname>Kim</keyname><forenames>Eun Jung</forenames></author><author><keyname>Milanic</keyname><forenames>Martin</forenames></author><author><keyname>Schaudt</keyname><forenames>Oliver</forenames></author></authors><title>Recognizing k-equistable graphs in FPT time</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph $G = (V,E)$ is called equistable if there exist a positive integer
$t$ and a weight function $w : V \to \mathbb{N}$ such that $S \subseteq V$ is a
maximal stable set of $G$ if and only if $w(S) = t$. Such a function $w$ is
called an equistable function of $G$. For a positive integer $k$, a graph $G =
(V,E)$ is said to be $k$-equistable if it admits an equistable function which
is bounded by $k$.
  We prove that the problem of recognizing $k$-equistable graphs is fixed
parameter tractable when parameterized by $k$, affirmatively answering a
question of Levit et al. In fact, the problem admits an $O(k^5)$-vertex kernel
that can be computed in linear time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01102</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01102</id><created>2015-03-03</created><updated>2015-07-15</updated><authors><author><keyname>Park</keyname><forenames>Jeonghun</forenames></author><author><keyname>Lee</keyname><forenames>Namyoon</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>Cooperative Base Station Coloring for Pair-wise Multi-Cell Coordination</title><categories>cs.IT math.IT</categories><comments>IEEE Transactions on Communications, under revision</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a method for designing BS clusters and cluster patterns
for pair-wise BS coordination. The key idea is that each BS cluster is formed
by using the 2nd-order Voronoi region, and the BS clusters are assigned to a
specific cluster pattern by using edge-coloring for a graph drawn by Delaunay
triangulation. The main advantage of the proposed method is that the BS
selection conflict problem is prevented, while selected users are guaranteed to
communicate with their two closest BSs in any irregular BS topology. With the
proposed coordination method, analytical expressions for the rate distribution
and the ergodic spectral efficiency are derived as a function of relevant
system parameters in a fixed irregular network model. In a random network model
with a homogeneous Poisson point process, a lower bound on the ergodic spectral
efficiency is characterized. Through system level simulations, the performance
of the proposed method is compared with that of conventional coordination
methods: dynamic clustering and static clustering. Our major finding is that,
when users are dense enough in a network, the proposed method provides the same
level of coordination benefit with dynamic clustering to edge users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01105</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01105</id><created>2015-03-02</created><authors><author><keyname>Gui</keyname><forenames>Guan</forenames></author><author><keyname>Xu</keyname><forenames>Li</forenames></author><author><keyname>Shimoi</keyname><forenames>Nobuhiro</forenames></author></authors><title>ROSA: Robust sparse adaptive channel estimation in the presence of
  impulsive noises</title><categories>cs.IT math.IT</categories><comments>18 pages, 8 figures, submitted for journal. arXiv admin note: text
  overlap with arXiv:1502.05484; substantial text overlap with arXiv:1503.00800</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on the assumption of Gaussian noise model, conventional adaptive
filtering algorithms for reconstruction sparse channels were proposed to take
advantage of channel sparsity due to the fact that broadband wireless channels
usually have the sparse nature. However, state-of-the-art algorithms are
vulnerable to deteriorate under the assumption of non-Gaussian noise models
(e.g., impulsive noise) which often exist in many advanced communications
systems. In this paper, we study the problem of RObust Sparse Adaptive channel
estimation (ROSA) in the environment of impulsive noises using variable
step-size affine projection sign algorithm (VSS-APSA). Specifically, standard
VSS-APSA algorithm is briefly reviewed and three sparse VSS-APSA algorithms are
proposed to take advantage of channel sparsity with different sparse
constraints. To fairly evaluate the performance of these proposed algorithms,
alpha-stable noise is considered to approximately model the realistic impulsive
noise environments. Simulation results show that the proposed algorithms can
achieve better performance than standard VSS-APSA algorithm in different
impulsive environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01129</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01129</id><created>2015-03-03</created><authors><author><keyname>Montemurro</keyname><forenames>Marcelo A</forenames></author><author><keyname>Zanette</keyname><forenames>Dami&#xe1;n H</forenames></author></authors><title>Complexity and universality in the long-range order of words</title><categories>cs.CL physics.data-an physics.soc-ph</categories><comments>8 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As is the case of many signals produced by complex systems, language presents
a statistical structure that is balanced between order and disorder. Here we
review and extend recent results from quantitative characterisations of the
degree of order in linguistic sequences that give insights into two relevant
aspects of language: the presence of statistical universals in word ordering,
and the link between semantic information and the statistical linguistic
structure. We first analyse a measure of relative entropy that assesses how
much the ordering of words contributes to the overall statistical structure of
language. This measure presents an almost constant value close to 3.5 bits/word
across several linguistic families. Then, we show that a direct application of
information theory leads to an entropy measure that can quantify and extract
semantic structures from linguistic samples, even without prior knowledge of
the underlying language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01138</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01138</id><created>2015-03-03</created><updated>2015-06-16</updated><authors><author><keyname>Wang</keyname><forenames>Zhangyang</forenames></author><author><keyname>Yang</keyname><forenames>Yingzhen</forenames></author><author><keyname>Wang</keyname><forenames>Zhaowen</forenames></author><author><keyname>Chang</keyname><forenames>Shiyu</forenames></author><author><keyname>Yang</keyname><forenames>Jianchao</forenames></author><author><keyname>Huang</keyname><forenames>Thomas S.</forenames></author></authors><title>Learning Super-Resolution Jointly from External and Internal Examples</title><categories>cs.CV</categories><doi>10.1109/TIP.2015.2462113</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Single image super-resolution (SR) aims to estimate a high-resolution (HR)
image from a lowresolution (LR) input. Image priors are commonly learned to
regularize the otherwise seriously ill-posed SR problem, either using external
LR-HR pairs or internal similar patterns. We propose joint SR to adaptively
combine the advantages of both external and internal SR methods. We define two
loss functions using sparse coding based external examples, and epitomic
matching based on internal examples, as well as a corresponding adaptive weight
to automatically balance their contributions according to their reconstruction
errors. Extensive SR results demonstrate the effectiveness of the proposed
method over the existing state-of-the-art methods, and is also verified by our
subjective evaluation study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01143</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01143</id><created>2015-03-03</created><updated>2015-03-10</updated><authors><author><keyname>Meehan</keyname><forenames>John</forenames></author><author><keyname>Tatbul</keyname><forenames>Nesime</forenames></author><author><keyname>Zdonik</keyname><forenames>Stan</forenames></author><author><keyname>Aslantas</keyname><forenames>Cansu</forenames></author><author><keyname>Cetintemel</keyname><forenames>Ugur</forenames></author><author><keyname>Du</keyname><forenames>Jiang</forenames></author><author><keyname>Kraska</keyname><forenames>Tim</forenames></author><author><keyname>Madden</keyname><forenames>Samuel</forenames></author><author><keyname>Maier</keyname><forenames>David</forenames></author><author><keyname>Pavlo</keyname><forenames>Andrew</forenames></author><author><keyname>Stonebraker</keyname><forenames>Michael</forenames></author><author><keyname>Tufte</keyname><forenames>Kristin</forenames></author><author><keyname>Wang</keyname><forenames>Hao</forenames></author></authors><title>S-Store: Streaming Meets Transaction Processing</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stream processing addresses the needs of real-time applications. Transaction
processing addresses the coordination and safety of short atomic computations.
Heretofore, these two modes of operation existed in separate, stove-piped
systems. In this work, we attempt to fuse the two computational paradigms in a
single system called S-Store. In this way, S-Store can simultaneously
accommodate OLTP and streaming applications. We present a simple transaction
model for streams that integrates seamlessly with a traditional OLTP system. We
chose to build S-Store as an extension of H-Store, an open-source, in-memory,
distributed OLTP database system. By implementing S-Store in this way, we can
make use of the transaction processing facilities that H-Store already
supports, and we can concentrate on the additional implementation features that
are needed to support streaming. Similar implementations could be done using
other main-memory OLTP platforms. We show that we can actually achieve higher
throughput for streaming workloads in S-Store than an equivalent deployment in
H-Store alone. We also show how this can be achieved within H-Store with the
addition of a modest amount of new functionality. Furthermore, we compare
S-Store to two state-of-the-art streaming systems, Spark Streaming and Storm,
and show how S-Store matches and sometimes exceeds their performance while
providing stronger transactional guarantees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01144</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01144</id><created>2015-03-03</created><updated>2015-09-23</updated><authors><author><keyname>Durand</keyname><forenames>Arnaud</forenames></author><author><keyname>Kontinen</keyname><forenames>Juha</forenames></author><author><keyname>de Rugy-Altherre</keyname><forenames>Nicolas</forenames></author><author><keyname>V&#xe4;&#xe4;n&#xe4;nen</keyname><forenames>Jouko</forenames></author></authors><title>Tractability Frontier of Data Complexity in Team Semantics</title><categories>cs.LO</categories><comments>In Proceedings GandALF 2015, arXiv:1509.06858</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 193, 2015, pp. 73-85</journal-ref><doi>10.4204/EPTCS.193.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the data complexity of model-checking for logics with team
semantics. For dependence and independence logic, we completely characterize
the tractability/intractability frontier of data complexity of both
quantifier-free and quantified formulas. For inclusion logic formulas, we
reduce the model-checking problem to the satisfiability problem of so-called
Dual-Horn propositional formulas. Via this reduction, we give an alternative
proof for the recent result showing that the data complexity of inclusion logic
is in PTIME.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01145</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01145</id><created>2015-03-03</created><updated>2015-03-04</updated><authors><author><keyname>Xiao</keyname><forenames>Lu</forenames></author><author><keyname>Zhang</keyname><forenames>Weiyu</forenames></author><author><keyname>Przybylska</keyname><forenames>Anna</forenames></author><author><keyname>De Liddo</keyname><forenames>Anna</forenames></author><author><keyname>Convertino</keyname><forenames>Gregorio</forenames></author><author><keyname>Davies</keyname><forenames>Todd</forenames></author><author><keyname>Klein</keyname><forenames>Mark</forenames></author></authors><title>Design for Online Deliberative Processes and Technologies: Towards a
  Multidisciplinary Research Agenda</title><categories>cs.HC</categories><comments>CHI'15 Extended Abstracts, Apr 18-23, 2015, Seoul, Republic of Korea,
  ACM 978-1-4503-3146-3/15/04, 4 pages</comments><acm-class>H.5.3; K.4.1; K.4.3</acm-class><doi>10.1145/2702613.2727687</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been rapidly growing interest in studying and designing online
deliberative processes and technologies. This SIG aims at providing a venue for
continuous and constructive dialogue between social, political and cognitive
sciences as well as computer science, HCI, and CSCW. Through an online
community and a modified version of world cafe discussions, we contribute to
the definition of the theoretical building blocks, the identification of a
research agenda for the CHI community, and the network of individuals from
academia, industry, and the public sector who share interests in different
aspects of online deliberation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01147</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01147</id><created>2015-03-03</created><authors><author><keyname>Stepanov</keyname><forenames>Sander</forenames></author><author><keyname>Venetsanopoulos</keyname><forenames>Anastasios</forenames></author></authors><title>Random Pulse Train Spectrum Calculation Unleashed</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the first time the problem of the full solution for the calculation of
the power spectrum density of the random pulse train is solved. This well known
problem led to a mistaken publication in the past and even its partial solution
was considered worthy of publication in a textbook. The little known solution
for only the continues random pulse train spectrum is explained by examples and
is extended to cover each signal having a discrete spectrum, too. A developed
approach is used to derive the general equation for two important
representative pulse trains with unbalanced symbol duration: a signal with
stretched pulse with a transition from one to zero, and shortened blank
symbols. The developed theoretical results are validated by simulation. It is
shown that the pulse trains under consideration pose spectrum peaks. The
characteristics of these peaks are investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01156</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01156</id><created>2015-03-03</created><authors><author><keyname>Felber</keyname><forenames>David</forenames></author><author><keyname>Ostrovsky</keyname><forenames>Rafail</forenames></author></authors><title>A randomized online quantile summary in $O(\frac{1}{\varepsilon} \log
  \frac{1}{\varepsilon})$ words</title><categories>cs.DS</categories><comments>slight fixes to version submitted to ICALP 2015--mistake in time
  complexity, and a few minor numeric miscalculations in section 3</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A quantile summary is a data structure that approximates to
$\varepsilon$-relative error the order statistics of a much larger underlying
dataset.
  In this paper we develop a randomized online quantile summary for the cash
register data input model and comparison data domain model that uses
$O(\frac{1}{\varepsilon} \log \frac{1}{\varepsilon})$ words of memory. This
improves upon the previous best upper bound of $O(\frac{1}{\varepsilon}
\log^{3/2} \frac{1}{\varepsilon})$ by Agarwal et. al. (PODS 2012). Further, by
a lower bound of Hung and Ting (FAW 2010) no deterministic summary for the
comparison model can outperform our randomized summary in terms of space
complexity. Lastly, our summary has the nice property that
$O(\frac{1}{\varepsilon} \log \frac{1}{\varepsilon})$ words suffice to ensure
that the success probability is $1 - e^{-\text{poly}(1/\varepsilon)}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01158</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01158</id><created>2015-03-03</created><authors><author><keyname>Emmott</keyname><forenames>Andrew</forenames></author><author><keyname>Das</keyname><forenames>Shubhomoy</forenames></author><author><keyname>Dietterich</keyname><forenames>Thomas</forenames></author><author><keyname>Fern</keyname><forenames>Alan</forenames></author><author><keyname>Wong</keyname><forenames>Weng-Keen</forenames></author></authors><title>Systematic Construction of Anomaly Detection Benchmarks from Real Data</title><categories>cs.AI cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research in anomaly detection suffers from a lack of realistic and
publicly-available data sets. Because of this, most published experiments in
anomaly detection validate their algorithms with application-specific case
studies or benchmark datasets of the researchers' construction. This makes it
difficult to compare different methods or to measure progress in the field. It
also limits our ability to understand the factors that determine the
performance of anomaly detection algorithms. This article proposes a new
methodology for empirical analysis and evaluation of anomaly detection
algorithms. It is based on generating thousands of benchmark datasets by
transforming existing supervised learning benchmark datasets and manipulating
properties relevant to anomaly detection. The paper identifies and validates
four important dimensions: (a) point difficulty, (b) relative frequency of
anomalies, (c) clusteredness of anomalies, and (d) relevance of features. We
apply our generated datasets to analyze several leading anomaly detection
algorithms. The evaluation verifies the importance of these dimensions and
shows that, while some algorithms are clearly superior to others, anomaly
detection accuracy is determined more by variation in the four dimensions than
by the choice of algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01161</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01161</id><created>2015-03-03</created><authors><author><keyname>Kim</keyname><forenames>Been</forenames></author><author><keyname>Rudin</keyname><forenames>Cynthia</forenames></author><author><keyname>Shah</keyname><forenames>Julie</forenames></author></authors><title>The Bayesian Case Model: A Generative Approach for Case-Based Reasoning
  and Prototype Classification</title><categories>stat.ML cs.LG</categories><comments>Published in Neural Information Processing Systems (NIPS) 2014,
  Neural Information Processing Systems (NIPS) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the Bayesian Case Model (BCM), a general framework for Bayesian
case-based reasoning (CBR) and prototype classification and clustering. BCM
brings the intuitive power of CBR to a Bayesian generative framework. The BCM
learns prototypes, the &quot;quintessential&quot; observations that best represent
clusters in a dataset, by performing joint inference on cluster labels,
prototypes and important features. Simultaneously, BCM pursues sparsity by
learning subspaces, the sets of features that play important roles in the
characterization of the prototypes. The prototype and subspace representation
provides quantitative benefits in interpretability while preserving
classification accuracy. Human subject experiments verify statistically
significant improvements to participants' understanding when using explanations
produced by BCM, compared to those given by prior art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01170</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01170</id><created>2015-03-03</created><updated>2015-04-19</updated><authors><author><keyname>Kim</keyname><forenames>John Y.</forenames></author></authors><title>Integer Addition and Hamming Weight</title><categories>math.CO cs.CC</categories><comments>21 pages, 0 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the effect of addition on the Hamming weight of a positive integer.
Consider the first $2^n$ positive integers, and fix an $\alpha$ among them. We
show that if the binary representation of $\alpha$ consists of $\Theta(n)$
blocks of zeros and ones, then addition by $\alpha$ causes a constant fraction
of low Hamming weight integers to become high Hamming weight integers. This
result has applications in complexity theory to the hardness of computing
powering maps using bounded-depth arithmetic circuits over $\mathbb{F}_2$. Our
result implies that powering by $\alpha$ composed of many blocks require
exponential-size, bounded-depth arithmetic circuits over $\mathbb{F}_2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01173</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01173</id><created>2015-03-03</created><authors><author><keyname>Jurdak</keyname><forenames>Raja</forenames></author><author><keyname>Elfes</keyname><forenames>Alberto</forenames></author><author><keyname>Kusy</keyname><forenames>Branislav</forenames></author><author><keyname>Tews</keyname><forenames>Ashley</forenames></author><author><keyname>Hu</keyname><forenames>Wen</forenames></author><author><keyname>Hernandez</keyname><forenames>Emili</forenames></author><author><keyname>Kottege</keyname><forenames>Navinda</forenames></author><author><keyname>Sikka</keyname><forenames>Pavan</forenames></author></authors><title>Autonomous surveillance for biosecurity</title><categories>cs.RO cs.CY</categories><comments>26 pages, Trends in Biotechnology, 3 March 2015, ISSN 0167-7799,
  http://dx.doi.org/10.1016/j.tibtech.2015.01.003.
  (http://www.sciencedirect.com/science/article/pii/S0167779915000190)</comments><doi>10.1016/j.tibtech.2015.01.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The global movement of people and goods has increased the risk of biosecurity
threats and their potential to incur large economic, social, and environmental
costs. Conventional manual biosecurity surveillance methods are limited by
their scalability in space and time. This article focuses on autonomous
surveillance systems, comprising sensor networks, robots, and intelligent
algorithms, and their applicability to biosecurity threats. We discuss the
spatial and temporal attributes of autonomous surveillance technologies and map
them to three broad categories of biosecurity threat: (i) vector-borne
diseases; (ii) plant pests; and (iii) aquatic pests. Our discussion reveals a
broad range of opportunities to serve biosecurity needs through autonomous
surveillance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01179</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01179</id><created>2015-03-03</created><authors><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author></authors><title>Time Averaged Consensus in a Direct Coupled Coherent Quantum Observer
  Network for a Single Qubit Finite Level Quantum System</title><categories>quant-ph cs.SY math.OC</categories><comments>To appear in the proceedings of the 10th ASIAN CONTROL CONFERENCE
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of constructing a direct coupled quantum
observer network for a single qubit quantum system. The proposed observer
consists of a network of quantum harmonic oscillators and it is shown that the
observer network output converges to a consensus in a time averaged sense in
which each component of the observer estimates a specified output of the
quantum plant. An example and simulations are included.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01180</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01180</id><created>2015-03-03</created><updated>2015-03-16</updated><authors><author><keyname>Tan</keyname><forenames>Chenhao</forenames></author><author><keyname>Lee</keyname><forenames>Lillian</forenames></author></authors><title>All Who Wander: On the Prevalence and Characteristics of Multi-community
  Engagement</title><categories>cs.SI cs.CL physics.soc-ph</categories><comments>11 pages, data available at
  https://chenhaot.com/pages/multi-community.html, Proceedings of WWW 2015
  (updated references)</comments><acm-class>J.4; H.2.8</acm-class><doi>10.1145/2736277.2741661</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although analyzing user behavior within individual communities is an active
and rich research domain, people usually interact with multiple communities
both on- and off-line. How do users act in such multi-community environments?
Although there are a host of intriguing aspects to this question, it has
received much less attention in the research community in comparison to the
intra-community case. In this paper, we examine three aspects of
multi-community engagement: the sequence of communities that users post to, the
language that users employ in those communities, and the feedback that users
receive, using longitudinal posting behavior on Reddit as our main data source,
and DBLP for auxiliary experiments. We also demonstrate the effectiveness of
features drawn from these aspects in predicting users' future level of
activity.
  One might expect that a user's trajectory mimics the &quot;settling-down&quot; process
in real life: an initial exploration of sub-communities before settling down
into a few niches. However, we find that the users in our data continually post
in new communities; moreover, as time goes on, they post increasingly evenly
among a more diverse set of smaller communities. Interestingly, it seems that
users that eventually leave the community are &quot;destined&quot; to do so from the very
beginning, in the sense of showing significantly different &quot;wandering&quot; patterns
very early on in their trajectories; this finding has potentially important
design implications for community maintainers. Our multi-community perspective
also allows us to investigate the &quot;situation vs. personality&quot; debate from
language usage across different communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01183</identifier>
 <datestamp>2015-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01183</id><created>2015-03-03</created><updated>2015-03-05</updated><authors><author><keyname>Amiri</keyname><forenames>Saeid</forenames></author><author><keyname>Clarke</keyname><forenames>Bertrand</forenames></author><author><keyname>Clarke</keyname><forenames>Jennifer</forenames></author><author><keyname>Koepke</keyname><forenames>Hoyt A.</forenames></author></authors><title>A General Hybrid Clustering Technique</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Here, we propose a clustering technique for general clustering problems
including those that have non-convex clusters. For a given desired number of
clusters $K$, we use three stages to find a clustering. The first stage uses a
hybrid clustering technique to produce a series of clusterings of various sizes
(randomly selected). They key steps are to find a $K$-means clustering using
$K_\ell$ clusters where $K_\ell \gg K$ and then joins these small clusters by
using single linkage clustering. The second stage stabilizes the result of
stage one by reclustering via the `membership matrix' under Hamming distance to
generate a dendrogram. The third stage is to cut the dendrogram to get $K^*$
clusters where $K^* \geq K$ and then prune back to $K$ to give a final
clustering. A variant on our technique also gives a reasonable estimate for
$K_T$, the true number of clusters.
  We provide a series of arguments to justify the steps in the stages of our
methods and we provide numerous examples involving real and simulated data to
compare our technique with other related techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01185</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01185</id><created>2015-03-03</created><updated>2015-03-10</updated><authors><author><keyname>Feng</keyname><forenames>Yong</forenames></author><author><keyname>Wu</keyname><forenames>Jiasong</forenames></author><author><keyname>Zeng</keyname><forenames>Rui</forenames></author><author><keyname>Luo</keyname><forenames>Limin</forenames></author><author><keyname>Shu</keyname><forenames>Huazhong</forenames></author></authors><title>Gradient Compared Lp-LMS Algorithms for Sparse System Identification</title><categories>cs.SY</categories><comments>Submitted to 27th Chinese Control and Decision Conference (CCDC
  2015), 5 pages, 4 tables, 5 figures, 7 equations, 11 references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose two novel p-norm penalty least mean square (Lp-LMS)
algorithms as supplements of the conventional Lp-LMS algorithm established for
sparse adaptive filtering recently. A gradient comparator is employed to
selectively apply the zero attractor of p-norm constraint for only those taps
that have the same polarity as that of the gradient of the squared
instantaneous error, which leads to the new proposed gradient compared p-norm
constraint LMS algorithm (LpGC-LMS). We explain that the LpGC-LMS can achieve
lower mean square error than the standard Lp-LMS algorithm theoretically and
experimentally. To further improve the performance of the filter, the LpNGC-LMS
algorithm is derived using a new gradient comparator which takes the
sign-smoothed version of the previous one. The performance of the LpNGC-LMS is
superior to that of the LpGC-LMS in theory and in simulations. Moreover, these
two comparators can be easily applied to other norm constraint LMS algorithms
to derive some new approaches for sparse adaptive filtering. The numerical
simulation results show that the two proposed algorithms achieve better
performance than the standard LMS algorithm and Lp-LMS algorithm in terms of
convergence rate and steady-state behavior in sparse system identification
settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01186</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01186</id><created>2015-03-03</created><authors><author><keyname>Hosfelt</keyname><forenames>Diane Duros</forenames></author></authors><title>Automated detection and classification of cryptographic algorithms in
  binary programs through machine learning</title><categories>cs.CR</categories><comments>Thesis submitted in partial fulfillment of MSE CS degree at Johns
  Hopkins University, 25 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Threats from the internet, particularly malicious software (i.e., malware)
often use cryptographic algorithms to disguise their actions and even to take
control of a victim's system (as in the case of ransomware). Malware and other
threats proliferate too quickly for the time-consuming traditional methods of
binary analysis to be effective. By automating detection and classification of
cryptographic algorithms, we can speed program analysis and more efficiently
combat malware.
  This thesis will present several methods of leveraging machine learning to
automatically discover and classify cryptographic algorithms in compiled binary
programs.
  While further work is necessary to fully evaluate these methods on real-world
binary programs, the results in this paper suggest that machine learning can be
used successfully to detect and identify cryptographic primitives in compiled
code. Currently, these techniques successfully detect and classify
cryptographic algorithms in small single-purpose programs, and further work is
proposed to apply them to real-world examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01187</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01187</id><created>2015-03-03</created><authors><author><keyname>Peng</keyname><forenames>M.</forenames></author><author><keyname>Wang</keyname><forenames>C.</forenames></author><author><keyname>Lau</keyname><forenames>V.</forenames></author><author><keyname>Poor</keyname><forenames>H. V.</forenames></author></authors><title>Fronthaul-Constrained Cloud Radio Access Networks: Insights and
  Challenges</title><categories>cs.IT math.IT</categories><comments>5 Figures, accepted by IEEE Wireless Communications. arXiv admin
  note: text overlap with arXiv:1407.3855 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As a promising paradigm for fifth generation (5G) wireless communication
systems, cloud radio access networks (C-RANs) have been shown to reduce both
capital and operating expenditures, as well as to provide high spectral
efficiency (SE) and energy efficiency (EE). The fronthaul in such networks,
defined as the transmission link between a baseband unit (BBU) and a remote
radio head (RRH), requires high capacity, but is often constrained. This
article comprehensively surveys recent advances in fronthaul-constrained
C-RANs, including system architectures and key techniques. In particular, key
techniques for alleviating the impact of constrained fronthaul on SE/EE and
quality of service for users, including compression and quantization,
large-scale coordinated processing and clustering, and resource allocation
optimization, are discussed. Open issues in terms of software-defined
networking, network function virtualization, and partial centralization are
also identified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01189</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01189</id><created>2015-03-03</created><authors><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author></authors><title>Physical Interpretations of Negative Imaginary Systems Theory</title><categories>cs.SY math.OC</categories><comments>To appear in the Proceedings of the 10th ASIAN CONTROL CONFERENCE
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents some physical interpretations of recent stability results
on the feedback interconnection of negative imaginary systems. These
interpretations involve spring mass damper systems coupled together by springs
or RLC electrical networks coupled together via inductors or capacitors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01190</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01190</id><created>2015-03-03</created><authors><author><keyname>Prabhakaran</keyname><forenames>Vinodkumar</forenames></author><author><keyname>Bloodgood</keyname><forenames>Michael</forenames></author><author><keyname>Diab</keyname><forenames>Mona</forenames></author><author><keyname>Dorr</keyname><forenames>Bonnie</forenames></author><author><keyname>Levin</keyname><forenames>Lori</forenames></author><author><keyname>Piatko</keyname><forenames>Christine D.</forenames></author><author><keyname>Rambow</keyname><forenames>Owen</forenames></author><author><keyname>Van Durme</keyname><forenames>Benjamin</forenames></author></authors><title>Statistical modality tagging from rule-based annotations and
  crowdsourcing</title><categories>cs.CL cs.LG stat.ML</categories><comments>8 pages, 6 tables; appeared in Proceedings of the Workshop on
  Extra-Propositional Aspects of Meaning in Computational Linguistics, July
  2012; In Proceedings of the Workshop on Extra-Propositional Aspects of
  Meaning in Computational Linguistics, pages 57-64, Jeju, Republic of Korea,
  July 2012. Association for Computational Linguistics</comments><acm-class>I.2.7; I.2.6; I.5.1; I.5.4</acm-class><journal-ref>In Proceedings of the Workshop on Extra-Propositional Aspects of
  Meaning in Computational Linguistics, pages 57-64, Jeju, Republic of Korea,
  July 2012. Association for Computational Linguistics</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore training an automatic modality tagger. Modality is the attitude
that a speaker might have toward an event or state. One of the main hurdles for
training a linguistic tagger is gathering training data. This is particularly
problematic for training a tagger for modality because modality triggers are
sparse for the overwhelming majority of sentences. We investigate an approach
to automatically training a modality tagger where we first gathered sentences
based on a high-recall simple rule-based modality tagger and then provided
these sentences to Mechanical Turk annotators for further annotation. We used
the resulting set of training data to train a precise modality tagger using a
multi-class SVM that delivers good performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01191</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01191</id><created>2015-03-03</created><authors><author><keyname>Peng</keyname><forenames>Mugen</forenames></author><author><keyname>Xie</keyname><forenames>Xinqian</forenames></author><author><keyname>Hu</keyname><forenames>Qiang</forenames></author><author><keyname>Zhang</keyname><forenames>Jie</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Contract-Based Interference Coordination in Heterogeneous Cloud Radio
  Access Networks</title><categories>cs.IT math.IT</categories><comments>14 pages, 7 figures, and accepted by IEEE JOURNAL ON SELECTED AREAS
  IN COMMUNICATIONS, SECOND QUARTER, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heterogeneous cloud radio access networks (HCRANs) are potential solutions to
improve both spectral and energy efficiencies by embedding cloud computing into
heterogeneous networks (HetNets). The interference among remote radio heads
(RRHs) can be suppressed with centralized cooperative processing in the base
band unit (BBU) pool, while the intertier interference between RRHs and macro
base stations (MBSs) is still challenging in H-CRANs. In this paper, to
mitigate this inter-tier interference, a contract-based interference
coordination framework is proposed, where three scheduling schemes are
involved, and the downlink transmission interval is divided into three phases
accordingly. The core idea of the proposed framework is that the BBU pool
covering all RRHs is selected as the principal that would offer a contract to
the MBS, and the MBS as the agent decides whether to accept the contract or not
according to an individual rational constraint. An optimal contract design that
maximizes the rate-based utility is derived when perfect channel state
information (CSI) is acquired at both principal and agent. Furthermore,
contract optimization under the situation where only the partial CSI can be
obtained from practical channel estimation is addressed as well. Monte Carlo
simulations are provided to confirm the analysis, and simulation results show
that the proposed framework can significantly increase the transmission data
rates over baselines, thus demonstrating the effectiveness of the proposed
contract-based solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01192</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01192</id><created>2015-03-03</created><authors><author><keyname>Elmasry</keyname><forenames>Amr</forenames></author></authors><title>Counting Inversions Adaptively</title><categories>cs.DS</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a simple and efficient algorithm for adaptively counting inversions
in a sequence of $n$ integers. Our algorithm runs in $O(n + n
\sqrt{\lg{(Inv/n)}})$ time in the word-RAM model of computation, where $Inv$ is
the number of inversions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01203</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01203</id><created>2015-03-03</created><updated>2015-04-02</updated><authors><author><keyname>Gaspers</keyname><forenames>Serge</forenames></author><author><keyname>Mackenzie</keyname><forenames>Simon</forenames></author></authors><title>On the Number of Minimal Separators in Graphs</title><categories>cs.DS cs.DM math.CO</categories><comments>arXiv admin note: text overlap with arXiv:0909.5278 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the largest number of minimal separators a graph on n vertices
can have at most.
  We give a new proof that this number is in $O( ((1+\sqrt{5})/2)^n n )$.
  We prove that this number is in $\omega( 1.4521^n )$, improving on the
previous best lower bound of $\Omega(3^{n/3}) \subseteq \omega( 1.4422^n )$.
  This gives also an improved lower bound on the number of potential maximal
cliques in a graph. We would like to emphasize that our proofs are short,
simple, and elementary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01205</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01205</id><created>2015-03-03</created><updated>2015-08-11</updated><authors><author><keyname>Chou</keyname><forenames>Chun Tung</forenames></author></authors><title>A Markovian Approach to the Optimal Demodulation of Diffusion-based
  Molecular Communication Networks</title><categories>cs.IT math.IT q-bio.MN</categories><doi>10.1109/TCOMM.2015.2469784</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a diffusion-based molecular communication network, transmitters and
receivers communicate by using signalling molecules (or ligands) in a fluid
medium. This paper assumes that the transmitter uses different chemical
reactions to generate different emission patterns of signalling molecules to
represent different transmission symbols, and the receiver consists of
receptors. When the signalling molecules arrive at the receiver, they may react
with the receptors to form ligand-receptor complexes. Our goal is to study the
demodulation in this setup assuming that the transmitter and receiver are
synchronised. We derive an optimal demodulator using the continuous history of
the number of complexes at the receiver as the input to the demodulator. We do
that by first deriving a communication model which includes the chemical
reactions in the transmitter, diffusion in the transmission medium and the
ligand-receptor process in the receiver. This model, which takes the form of a
continuous-time Markov process, captures the noise in the receiver signal due
to the stochastic nature of chemical reactions and diffusion. We then adopt a
maximum a posterior framework and use Bayesian filtering to derive the optimal
demodulator. We use numerical examples to illustrate the properties of this
optimal demodulator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01210</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01210</id><created>2015-03-03</created><authors><author><keyname>Sanandaji</keyname><forenames>Borhan M.</forenames></author><author><keyname>Tascikaraoglu</keyname><forenames>Akin</forenames></author><author><keyname>Poolla</keyname><forenames>Kameshwar</forenames></author><author><keyname>Varaiya</keyname><forenames>Pravin</forenames></author></authors><title>Low-dimensional Models in Spatio-Temporal Wind Speed Forecasting</title><categories>cs.SY stat.ML</categories><comments>Initially submitted for review to the 2015 American Control
  Conference on September 22, 2014; Accepted for publication on January 22,
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Integrating wind power into the grid is challenging because of its random
nature. Integration is facilitated with accurate short-term forecasts of wind
power. The paper presents a spatio-temporal wind speed forecasting algorithm
that incorporates the time series data of a target station and data of
surrounding stations. Inspired by Compressive Sensing (CS) and
structured-sparse recovery algorithms, we claim that there usually exists an
intrinsic low-dimensional structure governing a large collection of stations
that should be exploited. We cast the forecasting problem as recovery of a
block-sparse signal $\boldsymbol{x}$ from a set of linear equations
$\boldsymbol{b} = A\boldsymbol{x}$ for which we propose novel structure-sparse
recovery algorithms. Results of a case study in the east coast show that the
proposed Compressive Spatio-Temporal Wind Speed Forecasting (CST-WSF) algorithm
significantly improves the short-term forecasts compared to a set of
widely-used benchmark models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01212</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01212</id><created>2015-03-03</created><updated>2015-06-11</updated><authors><author><keyname>Rakhlin</keyname><forenames>Alexander</forenames></author><author><keyname>Sridharan</keyname><forenames>Karthik</forenames></author></authors><title>Hierarchies of Relaxations for Online Prediction Problems with Evolving
  Constraints</title><categories>cs.LG cs.DS stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study online prediction where regret of the algorithm is measured against
a benchmark defined via evolving constraints. This framework captures online
prediction on graphs, as well as other prediction problems with combinatorial
structure. A key aspect here is that finding the optimal benchmark predictor
(even in hindsight, given all the data) might be computationally hard due to
the combinatorial nature of the constraints. Despite this, we provide
polynomial-time \emph{prediction} algorithms that achieve low regret against
combinatorial benchmark sets. We do so by building improper learning algorithms
based on two ideas that work together. The first is to alleviate part of the
computational burden through random playout, and the second is to employ
Lasserre semidefinite hierarchies to approximate the resulting integer program.
Interestingly, for our prediction algorithms, we only need to compute the
values of the semidefinite programs and not the rounded solutions. However, the
integrality gap for Lasserre hierarchy \emph{does} enter the generic regret
bound in terms of Rademacher complexity of the benchmark set. This establishes
a trade-off between the computation time and the regret bound of the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01214</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01214</id><created>2015-03-03</created><authors><author><keyname>Fanti</keyname><forenames>Giulia</forenames></author><author><keyname>Pihur</keyname><forenames>Vasyl</forenames></author><author><keyname>Erlingsson</keyname><forenames>&#xda;lfar</forenames></author></authors><title>Building a RAPPOR with the Unknown: Privacy-Preserving Learning of
  Associations and Data Dictionaries</title><categories>cs.CR</categories><comments>17 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Techniques based on randomized response enable the collection of potentially
sensitive data from clients in a privacy-preserving manner with strong local
differential privacy guarantees. One of the latest such technologies, RAPPOR,
allows the marginal frequencies of an arbitrary set of strings to be estimated
via privacy-preserving crowdsourcing. However, this original estimation process
requires a known set of possible strings; in practice, this dictionary can
often be extremely large and sometimes completely unknown.
  In this paper, we propose a novel decoding algorithm for the RAPPOR mechanism
that enables the estimation of &quot;unknown unknowns,&quot; i.e., strings we do not even
know we should be estimating. To enable learning without explicit knowledge of
the dictionary, we develop methodology for estimating the joint distribution of
two or more variables collected with RAPPOR. This is a critical step towards
understanding relationships between multiple variables collected in a
privacy-preserving manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01218</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01218</id><created>2015-03-03</created><authors><author><keyname>Soma</keyname><forenames>Tasuku</forenames></author><author><keyname>Yoshida</keyname><forenames>Yuichi</forenames></author></authors><title>Maximizing Submodular Functions with the Diminishing Return Property
  over the Integer Lattice</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of maximizing non-negative monotone submodular functions under a
certain constraint has been intensively studied in the last decade. In this
paper, we address the problem for functions defined over the integer lattice.
Suppose that a non-negative monotone submodular function $f:\mathbb{Z}_+^n \to
\mathbb{R}_+$ is given via an evaluation oracle. Furthermore, we assume that
$f$ satisfies the diminishing return property, which is not an immediate
consequence of the submodularity when the domain is the integer lattice. Then,
we show (i) a $(1-1/e-\epsilon)$-approximation algorithm for a cardinality
constraint with $\widetilde{O}(\frac{n}{\epsilon}\log \frac{r}{\epsilon})$
queries, where $r$ is the maximum cardinality of feasible solutions, (ii) a
$(1-1/e-\epsilon)$-approximation algorithm for a polymatroid constraint with
$\widetilde{O}(\frac{nr}{\epsilon^4}+n^6)$ queries, where $r$ is the rank of
the polymatroid, and (iii) a $(1-1/e-\epsilon)$-approximation algorithm for a
knapsack constraint with $\widetilde{O}(\frac{n^2}{\epsilon^{18}}\log
\frac{1}{w})(\frac{1}{\epsilon})^{O(1/\epsilon^8)}$ queries, where $w$ is the
minumum weight of elements.
  Our algorithms for polymatroid constraints and knapsack constraints first
extend the domain of the objective function to the Euclidean space and then run
the continuous greedy algorithm. We give two different kinds of continuous
extensions, one is for knapsack constraints and the other is for polymatroid
constraints, which might be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01220</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01220</id><created>2015-03-03</created><authors><author><keyname>Fazeli</keyname><forenames>Arastoo</forenames></author><author><keyname>Ajorlou</keyname><forenames>Amir</forenames></author><author><keyname>Jadbabaie</keyname><forenames>Ali</forenames></author></authors><title>Competitive Diffusion in Social Networks: Quality or Seeding?</title><categories>cs.GT cs.SI cs.SY math.OC</categories><comments>arXiv admin note: substantial text overlap with arXiv:1404.1405</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study a strategic model of marketing and product
consumption in social networks. We consider two firms in a market competing to
maximize the consumption of their products. Firms have a limited budget which
can be either invested on the quality of the product or spent on initial
seeding in the network in order to better facilitate spread of the product.
After the decision of firms, agents choose their consumptions following a
myopic best response dynamics which results in a local, linear update for their
consumption decision. We characterize the unique Nash equilibrium of the game
between firms and study the effect of the budgets as well as the network
structure on the optimal allocation. We show that at the equilibrium, firms
invest more budget on quality when their budgets are close to each other.
However, as the gap between budgets widens, competition in qualities becomes
less effective and firms spend more of their budget on seeding. We also show
that given equal budget of firms, if seeding budget is nonzero for a balanced
graph, it will also be nonzero for any other graph, and if seeding budget is
zero for a star graph it will be zero for any other graph as well. As a
practical extension, we then consider a case where products have some preset
qualities that can be only improved marginally. At some point in time, firms
learn about the network structure and decide to utilize a limited budget to
mount their market share by either improving the quality or new seeding some
agents to incline consumers towards their products. We show that the optimal
budget allocation in this case simplifies to a threshold strategy.
Interestingly, we derive similar results to that of the original problem, in
which preset qualities simulate the role that budgets had in the original
setup.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01224</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01224</id><created>2015-03-04</created><updated>2015-04-16</updated><authors><author><keyname>Wang</keyname><forenames>Peng</forenames></author><author><keyname>Cao</keyname><forenames>Yuanzhouhan</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Liu</keyname><forenames>Lingqiao</forenames></author><author><keyname>Shen</keyname><forenames>Heng Tao</forenames></author></authors><title>Temporal Pyramid Pooling Based Convolutional Neural Networks for Action
  Recognition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Encouraged by the success of Convolutional Neural Networks (CNNs) in image
classification, recently much effort is spent on applying CNNs to video based
action recognition problems. One challenge is that video contains a varying
number of frames which is incompatible to the standard input format of CNNs.
Existing methods handle this issue either by directly sampling a fixed number
of frames or bypassing this issue by introducing a 3D convolutional layer which
conducts convolution in spatial-temporal domain.
  To solve this issue, here we propose a novel network structure which allows
an arbitrary number of frames as the network input. The key of our solution is
to introduce a module consisting of an encoding layer and a temporal pyramid
pooling layer. The encoding layer maps the activation from previous layers to a
feature vector suitable for pooling while the temporal pyramid pooling layer
converts multiple frame-level activations into a fixed-length video-level
representation. In addition, we adopt a feature concatenation layer which
combines appearance information and motion information. Compared with the frame
sampling strategy, our method avoids the risk of missing any important frames.
Compared with the 3D convolutional method which requires a huge video dataset
for network training, our model can be learned on a small target dataset
because we can leverage the off-the-shelf image-level CNN for model parameter
initialization. Experiments on two challenging datasets, Hollywood2 and HMDB51,
demonstrate that our method achieves superior performance over state-of-the-art
methods while requiring much fewer training data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01228</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01228</id><created>2015-03-04</created><authors><author><keyname>Tang</keyname><forenames>Kui</forenames></author><author><keyname>Ruozzi</keyname><forenames>Nicholas</forenames></author><author><keyname>Belanger</keyname><forenames>David</forenames></author><author><keyname>Jebara</keyname><forenames>Tony</forenames></author></authors><title>Bethe Learning of Conditional Random Fields via MAP Decoding</title><categories>cs.LG cs.CV stat.ML</categories><comments>19 pages (9 supplementary), 10 figures (3 supplementary)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many machine learning tasks can be formulated in terms of predicting
structured outputs. In frameworks such as the structured support vector machine
(SVM-Struct) and the structured perceptron, discriminative functions are
learned by iteratively applying efficient maximum a posteriori (MAP) decoding.
However, maximum likelihood estimation (MLE) of probabilistic models over these
same structured spaces requires computing partition functions, which is
generally intractable. This paper presents a method for learning discrete
exponential family models using the Bethe approximation to the MLE. Remarkably,
this problem also reduces to iterative (MAP) decoding. This connection emerges
by combining the Bethe approximation with a Frank-Wolfe (FW) algorithm on a
convex dual objective which circumvents the intractable partition function. The
result is a new single loop algorithm MLE-Struct, which is substantially more
efficient than previous double-loop methods for approximate maximum likelihood
estimation. Our algorithm outperforms existing methods in experiments involving
image segmentation, matching problems from vision, and a new dataset of
university roommate assignments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01239</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01239</id><created>2015-03-04</created><authors><author><keyname>Li</keyname><forenames>Changsheng</forenames></author><author><keyname>Wang</keyname><forenames>Xiangfeng</forenames></author><author><keyname>Dong</keyname><forenames>Weishan</forenames></author><author><keyname>Yan</keyname><forenames>Junchi</forenames></author><author><keyname>Liu</keyname><forenames>Qingshan</forenames></author><author><keyname>Zha</keyname><forenames>Hongyuan</forenames></author></authors><title>Active Sample Learning and Feature Selection: A Unified Approach</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on the problem of simultaneous sample and feature
selection for machine learning in a fully unsupervised setting. Though most
existing works tackle these two problems separately that derives two
well-studied sub-areas namely active learning and feature selection, a unified
approach is inspirational since they are often interleaved with each other.
Noisy and high-dimensional features will bring adverse effect on sample
selection, while `good' samples will be beneficial to feature selection. We
present a unified framework to conduct active learning and feature selection
simultaneously. From the data reconstruction perspective, both the selected
samples and features can best approximate the original dataset respectively,
such that the selected samples characterized by the selected features are very
representative. Additionally our method is one-shot without iteratively
selecting samples for progressive labeling. Thus our model is especially
suitable when the initial labeled samples are scarce or totally absent, which
existing works hardly address particularly for simultaneous feature selection.
To alleviate the NP-hardness of the raw problem, the proposed formulation
involves a convex but non-smooth optimization problem. We solve it efficiently
by an iterative algorithm, and prove its global convergence. Experiments on
publicly available datasets validate that our method is promising compared with
the state-of-the-arts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01244</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01244</id><created>2015-03-04</created><authors><author><keyname>Hendricks</keyname><forenames>Jacob</forenames></author><author><keyname>Patitz</keyname><forenames>Matthew J.</forenames></author><author><keyname>Rogers</keyname><forenames>Trent A.</forenames></author></authors><title>Replication of arbitrary hole-free shapes via self-assembly with
  signal-passing tiles (extended abstract)</title><categories>cs.ET</categories><comments>A version which is slightly improved over the recently submitted
  version, including a few more explanatory images and clarifications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the abilities of systems of self-assembling
tiles which can each pass a constant number of signals to their immediate
neighbors to create replicas of input shapes. Namely, we work within the
Signal-passing Tile Assembly Model (STAM), and we provide a universal STAM tile
set which is capable of creating unbounded numbers of assemblies of shapes
identical to those of input assemblies. The shapes of the input assemblies can
be arbitrary 2-dimensional hole-free shapes at scale factor 2. This greatly
improves previous shape replication results in self-assembly that required
models in which multiple assembly stages and/or bins were required, and the
shapes which could be replicated were quite constrained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01245</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01245</id><created>2015-03-04</created><authors><author><keyname>Morales-Jimenez</keyname><forenames>David</forenames></author><author><keyname>Couillet</keyname><forenames>Romain</forenames></author><author><keyname>McKay</keyname><forenames>Matthew R.</forenames></author></authors><title>Large Dimensional Analysis of Robust M-Estimators of Covariance with
  Outliers</title><categories>math.ST cs.IT math.IT stat.ML stat.TH</categories><comments>Submitted to IEEE Transactions on Signal Processing</comments><doi>10.1109/TSP.2015.2460225</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A large dimensional characterization of robust M-estimators of covariance (or
scatter) is provided under the assumption that the dataset comprises
independent (essentially Gaussian) legitimate samples as well as arbitrary
deterministic samples, referred to as outliers. Building upon recent random
matrix advances in the area of robust statistics, we specifically show that the
so-called Maronna M-estimator of scatter asymptotically behaves similar to
well-known random matrices when the population and sample sizes grow together
to infinity. The introduction of outliers leads the robust estimator to behave
asymptotically as the weighted sum of the sample outer products, with a
constant weight for all legitimate samples and different weights for the
outliers. A fine analysis of this structure reveals importantly that the
propensity of the M-estimator to attenuate (or enhance) the impact of outliers
is mostly dictated by the alignment of the outliers with the inverse population
covariance matrix of the legitimate samples. Thus, robust M-estimators can
bring substantial benefits over more simplistic estimators such as the
per-sample normalized version of the sample covariance matrix, which is not
capable of differentiating the outlying samples. The analysis shows that,
within the class of Maronna's estimators of scatter, the Huber estimator is
most favorable for rejecting outliers. On the contrary, estimators more similar
to Tyler's scale invariant estimator (often preferred in the literature) run
the risk of inadvertently enhancing some outliers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01250</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01250</id><created>2015-03-04</created><authors><author><keyname>Mo</keyname><forenames>Qun</forenames></author></authors><title>A new method on deterministic construction of the measurement matrix in
  compressed sensing</title><categories>cs.IT math.IT</categories><comments>Version 1 is a very rough version. Suitable citations will be added
  soon</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Construction on the measurement matrix $A$ is a central problem in compressed
sensing. Although using random matrices is proven optimal and successful in
both theory and applications. A deterministic construction on the measurement
matrix is still very important and interesting. In fact, it is still an open
problem proposed by T. Tao. In this paper, we shall provide a new deterministic
construction method and prove it is optimal with regard to the mutual
incoherence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01258</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01258</id><created>2015-03-04</created><authors><author><keyname>Pavenkov</keyname><forenames>Oleg V.</forenames></author><author><keyname>Pavenkov</keyname><forenames>Vladimir G.</forenames></author><author><keyname>Rubtcova</keyname><forenames>Mariia V.</forenames></author></authors><title>The concept &quot;altruism&quot; for sociological research: from conceptualization
  to operationalization</title><categories>cs.CY cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article addresses the question of the relevant conceptualization of
{\guillemotleft}altruism{\guillemotright} in Russian from the perspective
sociological research operationalization. It investigates the spheres of social
application of the word {\guillemotleft}altruism{\guillemotright}, include
Russian equivalent {\guillemotleft}vzaimopomoshh`{\guillemotright} (mutual
help). The data for the study comes from Russian National Corpus (Russian). The
theoretical framework consists of Paul F. Lazarsfeld`s Theory of Sociological
Research Methodology and the Natural Semantic Metalanguage (NSM). Quantitative
analysis shows features in the representation of altruism in Russian that
sociologists need to know in the preparation of questionnaires, interview
guides and analysis of transcripts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01267</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01267</id><created>2015-03-04</created><authors><author><keyname>Lanna</keyname><forenames>Andrea</forenames></author><author><keyname>Liberati</keyname><forenames>Francesco</forenames></author><author><keyname>Zuccaro</keyname><forenames>Letterio</forenames></author><author><keyname>Di Giorgio</keyname><forenames>Alessandro</forenames></author></authors><title>Electric Vehicles Charging Control based on Future Internet Generic
  Enablers</title><categories>cs.NI</categories><comments>To appear in IEEE International Electric Vehicle Conference (IEEE
  IEVC 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a rationale for the deployment of Future Internet based
applications in the field of Electric Vehicles (EVs) smart charging is
presented. The focus is on the Connected Device Interface (CDI) Generic Enabler
(GE) and the Network Information and Controller (NetIC) GE, which are
recognized to have a potential impact on the charging control problem and the
configuration of communications networks within reconfigurable clusters of
charging points. The CDI GE can be used for capturing the driver feedback in
terms of Quality of Experience (QoE) in those situations where the charging
power is abruptly limited as a consequence of short term grid needs, like the
shedding action asked by the Transmission System Operator to the Distribution
System Operator aimed at clearing networks contingencies due to the loss of a
transmission line or large wind power fluctuations. The NetIC GE can be used
when a master Electric Vehicle Supply Equipment (EVSE) hosts the Load Area
Controller, responsible for managing simultaneous charging sessions within a
given Load Area (LA); the reconfiguration of distribution grid topology results
in shift of EVSEs among LAs, then reallocation of slave EVSEs is needed.
Involved actors, equipment, communications and processes are identified through
the standardized framework provided by the Smart Grid Architecture Model
(SGAM).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01269</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01269</id><created>2015-03-04</created><authors><author><keyname>Apers</keyname><forenames>Simon</forenames></author><author><keyname>Sarlette</keyname><forenames>Alain</forenames></author></authors><title>Accelerating Consensus by Spectral Clustering and Polynomial Filters</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is known that polynomial filtering can accelerate the convergence towards
average consensus on an undirected network. In this paper the gain of a
second-order filtering is investigated. A set of graphs is determined for which
consensus can be attained in finite time, and a preconditioner is proposed to
adapt the undirected weights of any given graph to achieve fastest convergence
with the polynomial filter. The corresponding cost function differs from the
traditional spectral gap, as it favors grouping the eigenvalues in two
clusters. A possible loss of robustness of the polynomial filter is also
highlighted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01288</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01288</id><created>2015-03-04</created><authors><author><keyname>Jord&#xe1;n</keyname><forenames>Jaume</forenames></author><author><keyname>Onaindia</keyname><forenames>Eva</forenames></author></authors><title>Game-theoretic Approach for Non-Cooperative Planning</title><categories>cs.AI cs.GT</categories><comments>Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI-15)
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When two or more self-interested agents put their plans to execution in the
same environment, conflicts may arise as a consequence, for instance, of a
common utilization of resources. In this case, an agent can postpone the
execution of a particular action, if this punctually solves the conflict, or it
can resort to execute a different plan if the agent's payoff significantly
diminishes due to the action deferral. In this paper, we present a
game-theoretic approach to non-cooperative planning that helps predict before
execution what plan schedules agents will adopt so that the set of strategies
of all agents constitute a Nash equilibrium. We perform some experiments and
discuss the solutions obtained with our game-theoretical approach, analyzing
how the conflicts between the plans determine the strategic behavior of the
agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01298</identifier>
 <datestamp>2015-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01298</id><created>2015-03-04</created><updated>2015-06-26</updated><authors><author><keyname>Kraker</keyname><forenames>Peter</forenames></author><author><keyname>Lex</keyname><forenames>Elisabeth</forenames></author><author><keyname>Gorraiz</keyname><forenames>Juan</forenames></author><author><keyname>Gumpenberger</keyname><forenames>Christian</forenames></author><author><keyname>Peters</keyname><forenames>Isabella</forenames></author></authors><title>Research Data Explored II: the Anatomy and Reception of figshare</title><categories>cs.DL</categories><comments>Accepted for publication at STI 2015. For the first paper in the
  series see: http://arxiv.org/abs/1501.03342</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the second paper in a series of bibliometric studies of research
data. In this paper, we present an analysis of figshare, one of the largest
multidisciplinary repositories for research materials to date. We analysed the
structure of items archived in figshare, their usage, and their reception in
two altmetrics sources (PlumX and ImpactStory). We found that figshare acts (1)
as a personal repository for yet unpublished materials, (2) as a platform for
newly published research materials, and (3) as an archive for PLOS. Depending
on the function, we found different bibliometric characteristics. Items
archived from PLOS tend to be coming from the natural sciences and are often
unviewed and non-downloaded. Self-archived items, however, come from a variety
of disciplines and exhibit some patterns of higher usage. In the altmetrics
analysis, we found that Twitter was the social media service where research
data gained most attention; generally, research data published in 2014 were
most popular across social media services. PlumX detects considerably more
items in social media and also finds higher altmetric scores than ImpactStory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01299</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01299</id><created>2015-03-04</created><authors><author><keyname>Shajarisales</keyname><forenames>Naji</forenames></author><author><keyname>Janzing</keyname><forenames>Dominik</forenames></author><author><keyname>Shoelkopf</keyname><forenames>Bernhard</forenames></author><author><keyname>Besserve</keyname><forenames>Michel</forenames></author></authors><title>Telling cause from effect in deterministic linear dynamical systems</title><categories>cs.AI</categories><comments>This article is under review for a peer-reviewed conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inferring a cause from its effect using observed time series data is a major
challenge in natural and social sciences. Assuming the effect is generated by
the cause trough a linear system, we propose a new approach based on the
hypothesis that nature chooses the &quot;cause&quot; and the &quot;mechanism that generates
the effect from the cause&quot; independent of each other. We therefore postulate
that the power spectrum of the time series being the cause is uncorrelated with
the square of the transfer function of the linear filter generating the effect.
While most causal discovery methods for time series mainly rely on the noise,
our method relies on asymmetries of the power spectral density properties that
can be exploited even in the context of deterministic systems. We describe
mathematical assumptions in a deterministic model under which the causal
direction is identifiable with this approach. We also discuss the method's
performance under the additive noise model and its relationship to Granger
causality. Experiments show encouraging results on synthetic as well as
real-world data. Overall, this suggests that the postulate of Independence of
Cause and Mechanism is a promising principle for causal inference on empirical
time series.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01313</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01313</id><created>2015-03-04</created><updated>2016-01-08</updated><authors><author><keyname>Kristan</keyname><forenames>Matej</forenames></author><author><keyname>Matas</keyname><forenames>Jiri</forenames></author><author><keyname>Leonardis</keyname><forenames>Ales</forenames></author><author><keyname>Vojir</keyname><forenames>Tomas</forenames></author><author><keyname>Pflugfelder</keyname><forenames>Roman</forenames></author><author><keyname>Fernandez</keyname><forenames>Gustavo</forenames></author><author><keyname>Nebehay</keyname><forenames>Georg</forenames></author><author><keyname>Porikli</keyname><forenames>Fatih</forenames></author><author><keyname>Cehovin</keyname><forenames>Luka</forenames></author></authors><title>A Novel Performance Evaluation Methodology for Single-Target Trackers</title><categories>cs.CV</categories><comments>Final version (Accepted), IEEE Pattern Analysis and Machine
  Intelligence, 2016</comments><doi>10.1109/TPAMI.2016.2516982</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of single-target tracker performance
evaluation. We consider the performance measures, the dataset and the
evaluation system to be the most important components of tracker evaluation and
propose requirements for each of them. The requirements are the basis of a new
evaluation methodology that aims at a simple and easily interpretable tracker
comparison. The ranking-based methodology addresses tracker equivalence in
terms of statistical significance and practical differences. A fully-annotated
dataset with per-frame annotations with several visual attributes is
introduced. The diversity of its visual properties is maximized in a novel way
by clustering a large number of videos according to their visual attributes.
This makes it the most sophistically constructed and annotated dataset to date.
A multi-platform evaluation system allowing easy integration of third-party
trackers is presented as well. The proposed evaluation methodology was tested
on the VOT2014 challenge on the new dataset and 38 trackers, making it the
largest benchmark to date. Most of the tested trackers are indeed
state-of-the-art since they outperform the standard baselines, resulting in a
highly-challenging benchmark. An exhaustive analysis of the dataset from the
perspective of tracking difficulty is carried out. To facilitate tracker
comparison a new performance visualization technique is proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01314</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01314</id><created>2015-03-04</created><authors><author><keyname>Choudhuri</keyname><forenames>Arka Rai</forenames></author><author><keyname>S</keyname><forenames>Kalyanasundaram</forenames></author><author><keyname>Sridhar</keyname><forenames>Shriyak</forenames></author><author><keyname>B</keyname><forenames>Annappa</forenames></author></authors><title>An Incentivized Approach for Fair Participation in Wireless Ad hoc
  Networks</title><categories>cs.NI cs.DC cs.GT</categories><comments>6 pages, 4 figures, published in the International Journal of Recent
  Development in Engineering and Technology</comments><journal-ref>IJRDET 2, no. 3 (2014): 117-121</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Wireless Ad hoc networks (WANETs), nodes separated by considerable
distance communicate with each other by relaying their messages through other
nodes. However, it might not be in the best interests of a node to forward the
message of another node due to power constraints. In addition, all nodes being
rational, some nodes may be selfish, i.e. they might not relay data from other
nodes so as to increase their lifetime. In this paper, we present a fair and
incentivized approach for participation in Ad hoc networks. Given the power
required for each transmission, we are able to determine the power saving
contributed by each intermediate hop. We propose the FAir Share incenTivizEd Ad
hoc paRticipation protocol (FASTER), which takes a selected route from a
routing protocol as input, to calculate the worth of each node using the
cooperative game theory concept of 'Shapley Value' applied on the power saved
by each node. This value can be used for allocation of Virtual Currency to the
nodes, which can be spent on subsequent message transmissions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01322</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01322</id><created>2015-03-04</created><updated>2015-09-09</updated><authors><author><keyname>Traag</keyname><forenames>V. A.</forenames></author></authors><title>Faster unfolding of communities: speeding up the Louvain algorithm</title><categories>physics.soc-ph cs.DS cs.SI</categories><journal-ref>Phys. Rev. E 92, 032801, (2015)</journal-ref><doi>10.1103/PhysRevE.92.032801</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many complex networks exhibit a modular structure of densely connected groups
of nodes. Usually, such a modular structure is uncovered by the optimization of
some quality function. Although flawed, modularity remains one of the most
popular quality functions. The Louvain algorithm was originally developed for
optimizing modularity, but has been applied to a variety of methods. As such,
speeding up the Louvain algorithm, enables the analysis of larger graphs in a
shorter time for various methods. We here suggest to consider moving nodes to a
random neighbor community, instead of the best neighbor community. Although
incredibly simple, it reduces the theoretical runtime complexity from
$\mathcal{O}(m)$ to $\mathcal{O}(n \log \langle k \rangle)$ in networks with a
clear community structure. In benchmark networks, it speeds up the algorithm
roughly 2-3 times, while in some real networks it even reaches 10 times faster
runtimes. This improvement is due to two factors: (1) a random neighbor is
likely to be in a &quot;good&quot; community; and (2) random neighbors are likely to be
hubs, helping the convergence. Finally, the performance gain only slightly
diminishes the quality, especially for modularity, thus providing a good
quality-performance ratio. However, these gains are less pronounced, or even
disappear, for some other measures such as significance or surprise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01327</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01327</id><created>2015-03-04</created><authors><author><keyname>Cohen</keyname><forenames>Liat</forenames></author><author><keyname>Shimony</keyname><forenames>Solomon Eyal</forenames></author><author><keyname>Weiss</keyname><forenames>Gera</forenames></author></authors><title>Estimating the Probability of Meeting a Deadline in Hierarchical Plans</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a hierarchical plan (or schedule) with uncertain task times, we may
need to determine the probability that a given plan will satisfy a given
deadline. This problem is shown to be NP-hard for series-parallel hierarchies.
We provide a polynomial-time approximation algorithm for it. Computing the
expected makespan of an hierarchical plan is also shown to be NP-hard. We
examine the approximation bounds empirically and demonstrate where our scheme
is superior to sampling and to exact computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01331</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01331</id><created>2015-03-04</created><updated>2015-04-21</updated><authors><author><keyname>Lazova</keyname><forenames>Verica</forenames></author><author><keyname>Basnarkov</keyname><forenames>Lasko</forenames></author></authors><title>PageRank Approach to Ranking National Football Teams</title><categories>cs.SI</categories><comments>4 pages, accepted for presentation at the CIIT 2015, 12th
  International Conference on Informatics and Information Technologies
  correspondence email: lazova992 at gmail.com</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Football World Cup as world's favorite sporting event is a source of both
entertainment and overwhelming amount of data about the games played. In this
paper we analyse the available data on football world championships since 1930
until today. Our goal is to rank the national teams based on all matches during
the championships. For this purpose, we apply the PageRank with restarts
algorithm to a graph built from the games played during the tournaments.
Several statistics such as matches won and goals scored are combined in
different metrics that assign weights to the links in the graph. Finally, our
results indicate that the Random walk approach with the use of right metrics
can indeed produce relevant rankings comparable to the FIFA official all-time
ranking board.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01334</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01334</id><created>2015-03-04</created><authors><author><keyname>Dunjko</keyname><forenames>Vedran</forenames></author><author><keyname>Briegel</keyname><forenames>Hans J.</forenames></author></authors><title>Sequential quantum mixing for slowly evolving sequences of Markov chains</title><categories>quant-ph cs.AI cs.DS</categories><comments>15 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we consider the problem of preparation of the stationary
distribution of irreducible, time-reversible Markov chains, which is a
fundamental task in algorithmic Markov chain theory. For the classical setting,
this task has a complexity lower bound of $\Omega(1/\delta)$, where $\delta$ is
the spectral gap of the Markov chain, and other dependencies contribute only
logarithmically. In the quantum case, the conjectured complexity is
$O(\sqrt{\delta^{-1}})$ (with other dependencies contributing only
logarithmically). However, this bound has only been achieved for a few special
classes of Markov chains.
  In this work, we provide a method for the sequential preparation of
stationary distributions for sequences of general time-reversible $N-$state
Markov chains, akin to the setting of simulated annealing methods.
  The complexity of preparation we achieve is $O(\sqrt{\delta^{-1}} N^{1/4})$,
neglecting logarithmic factors. While this result falls short of the
conjectured optimal time, it still provides at least a quadratic improvement
over other straightforward approaches for quantum mixing applied in this
setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01337</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01337</id><created>2015-03-03</created><authors><author><keyname>Feng</keyname><forenames>Yong</forenames></author><author><keyname>Zeng</keyname><forenames>Rui</forenames></author><author><keyname>Wu</keyname><forenames>Jiasong</forenames></author></authors><title>p Norm Constraint Leaky LMS Algorithm for Sparse System Identification</title><categories>cs.SY</categories><comments>5 pages, 1 table, 1 figure, 9 equations, 8 references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new leaky least mean square (leaky LMS, LLMS) algorithm
in which a norm penalty is introduced to force the solution to be sparse in the
application of system identification. The leaky LMS algorithm is derived
because the performance ofthe standard LMS algorithm deteriorates when the
input is highly correlated. However, both ofthem do not take the sparsity
information into account to yield better behaviors. As a modification ofthe
LLMS algorithm, the proposed algorithm, named Lp-LLMS, incorporates a p norm
penalty into the cost function ofthe LLMS to obtain a shrinkage in the weight
update equation, which then enhances the performance of the filter in system
identification settings, especially when the impulse response is sparse. The
simulation results verify that the proposed algorithm improves the performance
ofthe filter in sparse system settings in the presence ofnoisy input signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01348</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01348</id><created>2015-03-04</created><authors><author><keyname>Kissinger</keyname><forenames>Aleks</forenames></author><author><keyname>Quick</keyname><forenames>David</forenames></author></authors><title>Tensors, !-graphs, and non-commutative quantum structures (extended
  version)</title><categories>cs.LO math.CT quant-ph</categories><comments>extended version of arXiv:1412.8552 [cs.LO], adds additional examples
  and soundness proofs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  !-graphs provide a means of reasoning about infinite families of string
diagrams and have proven useful in manipulation of (co)algebraic structures
like Hopf algebras, Frobenius algebras, and compositions thereof. However, they
have previously been limited by an inability to express families of diagrams
involving non-commutative structures which play a central role in algebraic
quantum information and the theory of quantum groups. In this paper, we fix
this shortcoming by offering a new semantics for non-commutative !-graphs using
an enriched version of Penrose's abstract tensor notation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01363</identifier>
 <datestamp>2015-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01363</id><created>2015-03-04</created><updated>2015-03-06</updated><authors><author><keyname>Berman</keyname><forenames>Piotr</forenames></author><author><keyname>Murzabulatov</keyname><forenames>Meiram</forenames></author><author><keyname>Raskhodnikova</keyname><forenames>Sofya</forenames></author></authors><title>Constant-Time Testing and Learning of Image Properties</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We initiate a systematic study of sublinear-time algorithms for image
analysis that have access only to labeled random samples from the input. Most
previous sublinear-time algorithms for image analysis were query-based, that
is, they could query pixels of their choice. We consider algorithms with two
types of input access: sample-based algorithms that draw independent uniformly
random pixels, and block-sample-based algorithms that draw pixels from
independently random square blocks of the image. We investigate three basic
properties: being a half-plane, convexity, and connectedness. For the first
two, our algorithms are sample-based; for connectedness, they are
block-sample-based. All our algorithms have low sample complexity that depends
polynomially on the inverse of the error parameter and is independent of the
input size.
  We design algorithms that approximate the distance to the three properties
within a small additive error or, equivalently, tolerant testers for being a
half-plane, convexity and connectedness. Tolerant testers for these properties,
even with query access to the image, were not investigated previously.
Tolerance is important in image processing applications because it allows
algorithms to be robust to noise in the image. We also give (non-tolerant)
testers for convexity and connectedness with better complexity than our
distance approximation algorithms and previously known query-based testers.
  To obtain our algorithms for convexity, we design two fast proper PAC
learners of convex sets in two dimensions that work under the uniform
distribution: non-agnostic and agnostic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01375</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01375</id><created>2015-03-04</created><authors><author><keyname>Kolda</keyname><forenames>Tamara G.</forenames></author></authors><title>Symmetric Orthogonal Tensor Decomposition is Trivial</title><categories>math.NA cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of decomposing a real-valued symmetric tensor as the
sum of outer products of real-valued, pairwise orthogonal vectors. Such
decompositions do not generally exist, but we show that some symmetric tensor
decomposition problems can be converted to orthogonal problems following the
whitening procedure proposed by Anandkumar et al. (2012). If an orthogonal
decomposition of an $m$-way $n$-dimensional symmetric tensor exists, we propose
a novel method to compute it that reduces to an $n \times n$ symmetric matrix
eigenproblem. We provide numerical results demonstrating the effectiveness of
the method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01376</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01376</id><created>2015-03-04</created><authors><author><keyname>Consoli</keyname><forenames>Sergio</forenames></author><author><keyname>Mladenov&#xec;c</keyname><forenames>Nenad</forenames></author><author><keyname>Moreno-P&#xe8;rez</keyname><forenames>Jos&#xe8; A.</forenames></author></authors><title>BVNS para el problema del bosque generador k-etiquetado</title><categories>cs.DM</categories><comments>8 pages, in Spanish. X Congreso Espanol sobre Metaheur\`isticas,
  Algoritmos Evolutivos y Bioinspirados, MAEB 2015, M\`erida - Almendralejo 4-6
  Feb 2015; Proceedings of the X Congreso Espanol sobre Metaheur\`isticas,
  Algoritmos Evolutivos y Bioinspirados, MAEB 2015, Francisco Chavez de la O et
  al. (Eds.), ISBN: 978-84-697-2150-6, pages: 629-636, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose an efficient solution for the problem of generating
k-labeling forest VNS. This problem is an extension of the Minimum Spanning
Tree Problem Labelling problem with important applications in
telecommunications networks and multimodal transport. It is, given an
undirected graph whose links are labeled, and an integer positive number k,
find the spanning forest with the lowest number of connected components using
at most k different labels. To address the problem a Basic Variable
Neighbourhood Search is proposed where the maximum amplitude of the
neighbourhood space, n, is a key parameter. Different strategies are studied to
establish the value of n. BVNS with the best selected strategy is
experimentally compared with other metaheuristics that have appeared in the
literature applied to this type of problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01380</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01380</id><created>2015-03-04</created><authors><author><keyname>Saha</keyname><forenames>Snehanshu</forenames></author><author><keyname>Jangid</keyname><forenames>Neelam</forenames></author><author><keyname>MN</keyname><forenames>Anand</forenames></author><author><keyname>Gupta</keyname><forenames>Sidhant</forenames></author></authors><title>Journal rank in the Science and Technology domain: A lightweight
  quantitative approach for evaluation</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The evaluation of journals based on their influence is of interest for
numerous reasons. Various methods of computing a score have been proposed for
measuring the scientific influence of scholarly journals. Typically the
computation of any of these scores involves compiling the citation information
pertaining to the journal under consideration. This involves significant
overhead since the article citation information of not only the journal under
consideration but also that of other journals for the recent few years need to
be stored. Our work is motivated by the idea of developing a computationally
lightweight approach that does not require any data storage, yet yields a score
which is useful for measuring the importance of journals. In this paper, a
regression analysis based method is proposed to calculate Journal Influence
Score. Proposed model is validated using historical data from the SCImago
portal. The results show that the error is small between rankings obtained
using the proposed method and the SCImago Journal Rank, thus proving that the
proposed approach is a feasible and effective method of calculating scientific
impact of journals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01382</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01382</id><created>2015-03-04</created><updated>2015-04-30</updated><authors><author><keyname>Crampton</keyname><forenames>Jason</forenames></author><author><keyname>Farley</keyname><forenames>Naomi</forenames></author><author><keyname>Gutin</keyname><forenames>Gregory</forenames></author><author><keyname>Jones</keyname><forenames>Mark</forenames></author></authors><title>Optimal Constructions for Chain-based Cryptographic Enforcement of
  Information Flow Policies</title><categories>cs.CR cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The simple security property in an information flow policy can be enforced by
encrypting data objects and distributing an appropriate secret to each user. A
user derives a suitable decryption key from the secret and publicly available
information. A chain-based enforcement scheme provides an alternative method of
cryptographic enforcement that does not require any public information, the
trade-off being that a user may require more than one secret. For a given
information flow policy, there will be many different possible chain-based
enforcement schemes. In this paper, we provide a polynomial-time algorithm for
selecting a chain-based scheme which uses the minimum possible number of keys.
We also compute the number of secrets that will be required and establish an
upper bound on the number of secrets required by any user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01386</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01386</id><created>2015-03-04</created><updated>2015-03-05</updated><authors><author><keyname>Hajnal</keyname><forenames>P&#xe9;ter</forenames></author><author><keyname>Igamberdiev</keyname><forenames>Alexander</forenames></author><author><keyname>Rote</keyname><forenames>G&#xfc;nter</forenames></author><author><keyname>Schulz</keyname><forenames>Andr&#xe9;</forenames></author></authors><title>Saturated simple and 2-simple topological graphs with few edges</title><categories>cs.CG math.CO</categories><comments>18 pages, 22 figures</comments><msc-class>05C62</msc-class><acm-class>I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A simple topological graph is a topological graph in which any two edges have
at most one common point, which is either their common endpoint or a proper
crossing. More generally, in a k-simple topological graph, every pair of edges
has at most k common points of this kind. We construct saturated simple and
2-simple graphs with few edges. These are k-simple graphs in which no further
edge can be added. We improve the previous upper bounds of Kyn\v{c}l, Pach,
Radoi\v{c}i\'c, and T\'oth and show that there are saturated simple graphs on n
vertices with only 7n edges and saturated 2-simple graphs on n vertices with
14.5n edges. As a consequence, 14.5n edges is also a new upper bound for
k-simple graphs (considering all values of k). We also construct saturated
simple and 2-simple graphs that have some vertices with low degree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01393</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01393</id><created>2015-03-04</created><authors><author><keyname>Ozay</keyname><forenames>Mete</forenames></author><author><keyname>Walas</keyname><forenames>Krzysztof</forenames></author><author><keyname>Leonardis</keyname><forenames>Ales</forenames></author></authors><title>A Hierarchical Approach for Joint Multi-view Object Pose Estimation and
  Categorization</title><categories>cs.CV cs.RO</categories><comments>7 Figures</comments><journal-ref>Proceedings of IEEE International Conference on Robotics and
  Automation (ICRA), pp. 5480 - 5487, Hong Kong, 2014</journal-ref><doi>10.1109/ICRA.2014.6907665</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a joint object pose estimation and categorization approach which
extracts information about object poses and categories from the object parts
and compositions constructed at different layers of a hierarchical object
representation algorithm, namely Learned Hierarchy of Parts (LHOP). In the
proposed approach, we first employ the LHOP to learn hierarchical part
libraries which represent entity parts and compositions across different object
categories and views. Then, we extract statistical and geometric features from
the part realizations of the objects in the images in order to represent the
information about object pose and category at each different layer of the
hierarchy. Unlike the traditional approaches which consider specific layers of
the hierarchies in order to extract information to perform specific tasks, we
combine the information extracted at different layers to solve a joint object
pose estimation and categorization problem using distributed optimization
algorithms. We examine the proposed generative-discriminative learning approach
and the algorithms on two benchmark 2-D multi-view image datasets. The proposed
approach and the algorithms outperform state-of-the-art classification,
regression and feature extraction algorithms. In addition, the experimental
results shed light on the relationship between object categorization, pose
estimation and the part realizations observed at different layers of the
hierarchy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01397</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01397</id><created>2015-03-04</created><updated>2015-06-18</updated><authors><author><keyname>Vilnis</keyname><forenames>Luke</forenames></author><author><keyname>Belanger</keyname><forenames>David</forenames></author><author><keyname>Sheldon</keyname><forenames>Daniel</forenames></author><author><keyname>McCallum</keyname><forenames>Andrew</forenames></author></authors><title>Bethe Projections for Non-Local Inference</title><categories>stat.ML cs.CL cs.LG</categories><comments>18 pages, equal contribution by first and second author, accepted to
  UAI 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many inference problems in structured prediction are naturally solved by
augmenting a tractable dependency structure with complex, non-local auxiliary
objectives. This includes the mean field family of variational inference
algorithms, soft- or hard-constrained inference using Lagrangian relaxation or
linear programming, collective graphical models, and forms of semi-supervised
learning such as posterior regularization. We present a method to
discriminatively learn broad families of inference objectives, capturing
powerful non-local statistics of the latent variables, while maintaining
tractable and provably fast inference using non-Euclidean projected gradient
descent with a distance-generating function given by the Bethe entropy. We
demonstrate the performance and flexibility of our method by (1) extracting
structured citations from research papers by learning soft global constraints,
(2) achieving state-of-the-art results on a widely-used handwriting recognition
task using a novel learned non-convex inference procedure, and (3) providing a
fast and highly scalable algorithm for the challenging problem of inference in
a collective graphical model applied to bird migration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01398</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01398</id><created>2015-03-04</created><updated>2015-06-11</updated><authors><author><keyname>Fysarakis</keyname><forenames>Konstantinos</forenames><affiliation>Dept. of Electronic &amp; Computer Engineering, Technical University of Crete, Greece</affiliation></author><author><keyname>Mylonakis</keyname><forenames>Damianos</forenames><affiliation>Dept. of Computer Science, University of Crete, Greece</affiliation></author><author><keyname>Manifavas</keyname><forenames>Charalampos</forenames><affiliation>Dept. of Informatics Engineering, Technological Educational Institute of Crete, Greece</affiliation></author><author><keyname>Papaefstathiou</keyname><forenames>Ioannis</forenames><affiliation>Dept. of Electronic &amp; Computer Engineering, Technical University of Crete, Greece</affiliation></author></authors><title>Node.DPWS: High performance and scalable Web Services for the IoT</title><categories>cs.NI cs.PL cs.SE</categories><comments>12 pages, 1 table, 3 figures, for associated libraries, see
  https://github.com/danmilon/node-dpws and
  https://github.com/danmilon/node-ws-discovery. This work has been submitted
  to the IEEE for possible publication. Copyright may be transferred without
  notice, after which this version may no longer be accessible</comments><doi>10.1109/MS.2015.155</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interconnected computing systems, in various forms, are expected to permeate
our lives, realizing the vision of the Internet of Things (IoT) and allowing us
to enjoy novel, enhanced services that promise to improve our everyday lives.
Nevertheless, this new reality also introduces significant challenges in terms
of performance, scaling, usability and interoperability. Leveraging the
benefits of Service Oriented Architectures (SOAs) can help alleviate many of
the issues that developers, implementers and end-users have to face in the
context of the IoT. This work presents Node.DPWS, a novel implementation of the
Devices Profile for Web Services (DPWS) based on the Node.js platform.
Node.DPWS can be used to deploy lightweight, efficient and scalable Web
Services over heterogeneous nodes, including devices with limited resources.
The performance of the presented work is evaluated on typical embedded devices,
including comparisons with implementations created using alternative DPWS
toolkits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01402</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01402</id><created>2015-03-04</created><authors><author><keyname>Sasmal</keyname><forenames>Pradip</forenames></author><author><keyname>Naidu</keyname><forenames>R. Ramu</forenames></author><author><keyname>Sastry</keyname><forenames>C. S.</forenames></author><author><keyname>Jampana</keyname><forenames>P. V.</forenames></author></authors><title>Deterministic construction of sparse binary and ternary matrices from
  existing binary sensing matrices</title><categories>cs.IT math.IT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the present work, we discuss a procedure for constructing sparse binary
and ternary matrices from existing two binary sensing matrices. The matrices
that we construct have several attractive properties such as smaller density,
which supports algorithms with low computational complexity. As an application
of our method, we show that a CS matrix of general row size different from $p,
p^2, pq$ (for different primes $p,q$) can be constructed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01404</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01404</id><created>2015-03-04</created><authors><author><keyname>Tochimani</keyname><forenames>Azucena</forenames></author><author><keyname>Villarreal</keyname><forenames>Rafael H.</forenames></author></authors><title>Complete intersection vanishing ideals on sets of clutter type over
  finite fields</title><categories>math.AC cs.IT math.AG math.CO math.IT</categories><msc-class>14M10, 14G15, 13P25, 13P10, 11T71, 94B27, 94B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we give a classification of complete intersection vanishing
ideals on parameterized sets of clutter type over finite fields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01407</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01407</id><created>2015-03-04</created><authors><author><keyname>Barczyk</keyname><forenames>Martin</forenames></author><author><keyname>Bonnabel</keyname><forenames>Silv&#xe8;re</forenames></author><author><keyname>Deschaud</keyname><forenames>Jean-Emmanuel</forenames></author><author><keyname>Goulette</keyname><forenames>Fran&#xe7;ois</forenames></author></authors><title>Invariant EKF Design for Scan Matching-aided Localization</title><categories>cs.SY cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Localization in indoor environments is a technique which estimates the
robot's pose by fusing data from onboard motion sensors with readings of the
environment, in our case obtained by scan matching point clouds captured by a
low-cost Kinect depth camera. We develop both an Invariant Extended Kalman
Filter (IEKF)-based and a Multiplicative Extended Kalman Filter (MEKF)-based
solution to this problem. The two designs are successfully validated in
experiments and demonstrate the advantage of the IEKF design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01408</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01408</id><created>2015-03-04</created><updated>2015-03-11</updated><authors><author><keyname>Nakibly</keyname><forenames>Gabi</forenames></author><author><keyname>Shelef</keyname><forenames>Gilad</forenames></author><author><keyname>Yudilevich</keyname><forenames>Shiran</forenames></author></authors><title>Hardware Fingerprinting Using HTML5</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Device fingerprinting over the web has received much attention both by the
research community and the commercial market a like. Almost all the
fingerprinting features proposed to date depend on software run on the device.
All of these features can be changed by the user, thereby thwarting the
device's fingerprint. In this position paper we argue that the recent emergence
of the HTML5 standard gives rise to a new class of fingerprinting features that
are based on the \emph{hardware} of the device. Such features are much harder
to mask or change thus provide a higher degree of confidence in the
fingerprint. We propose several possible fingerprint methods that allow a HTML5
web application to identify a device's hardware. We also present an initial
experiment to fingerprint a device's GPU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01415</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01415</id><created>2015-03-04</created><authors><author><keyname>Hammadi</keyname><forenames>Ahmed Al</forenames></author><author><keyname>Alhussein</keyname><forenames>Omar</forenames></author><author><keyname>Muhaidat</keyname><forenames>Sami</forenames></author><author><keyname>Al-Qutayri</keyname><forenames>Mahmoud</forenames></author><author><keyname>Al-Araji</keyname><forenames>Saleh</forenames></author><author><keyname>Karagiannidis</keyname><forenames>George K.</forenames></author></authors><title>Unified Analysis of Cooperative Spectrum Sensing over Composite and
  Generalized Fading Channels</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Trans. Veh. Techn</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the performance of cooperative spectrum sensing
(CSS) with multiple antenna nodes over composite and generalized fading
channels. We model the probability density function (PDF) of the
signal-to-noise ratio (SNR) using the mixture gamma (MG) distribution. We then
derive a generalized closed-form expression for the probability of energy
detection, which can be used efficiently for generalized multipath as well as
composite (multipath and shadowing) fading channels. The composite effect of
fading and shadowing scenarios in CSS is mitigated by applying an optimal
fusion rule that minimizes the total error rate (TER), where the optimal number
of nodes is derived under the Bayesian criterion, assuming erroneous feedback
channels. For imperfect feedback channels, we demonstrate the existence of a
TER floor as the number of antennas of the CR nodes increases. Accordingly, we
derive the optimal rule for the number of antennas that minimizes the TER.
Numerical and Monte-Carlo simulations are presented to corroborate the
analytical results and to provide illustrative performance comparisons between
different composite fading channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01416</identifier>
 <datestamp>2015-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01416</id><created>2015-03-03</created><authors><author><keyname>Abali</keyname><forenames>Bulent</forenames></author><author><keyname>Eickemeyer</keyname><forenames>Richard J.</forenames></author><author><keyname>Franke</keyname><forenames>Hubertus</forenames></author><author><keyname>Li</keyname><forenames>Chung-Sheng</forenames></author><author><keyname>Taubenblatt</keyname><forenames>Marc A.</forenames></author></authors><title>Disaggregated and optically interconnected memory: when will it be cost
  effective?</title><categories>cs.DC cs.AR cs.PF</categories><comments>9 pages, 7 figures</comments><acm-class>B.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The &quot;Disaggregated Server&quot; concept has been proposed for datacenters where
the same type server resources are aggregated in their respective pools, for
example a compute pool, memory pool, network pool, and a storage pool. Each
server is constructed dynamically by allocating the right amount of resources
from these pools according to the workload's requirements. Modularity, higher
packaging and cooling efficiencies, and higher resource utilization are among
the suggested benefits. With the emergence of very large datacenters, &quot;clouds&quot;
containing tens of thousands of servers, datacenter efficiency has become an
important topic. Few computer chip and systems vendors are working on and
making frequent announcements on silicon photonics and disaggregated memory
systems.
  In this paper we study the trade-off between cost and performance of building
a disaggregated memory system where DRAM modules in the datacenter are pooled,
for example in memory-only chassis and racks. The compute pool and the memory
pool are interconnected by an optical interconnect to overcome the distance and
bandwidth issues of electrical fabrics. We construct a simple cost model that
includes the cost of latency, cost of bandwidth and the savings expected from a
disaggregated memory system. We then identify the level at which a
disaggregated memory system becomes cost competitive with a traditional direct
attached memory system.
  Our analysis shows that a rack-scale disaggregated memory system will have a
non-trivial performance penalty, and at the datacenter scale the penalty is
impractically high, and the optical interconnect costs are at least a factor of
10 more expensive than where they should be when compared to the traditional
direct attached memory systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01425</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01425</id><created>2015-03-03</created><updated>2015-09-20</updated><authors><author><keyname>Lam</keyname><forenames>Albert Y. S.</forenames></author></authors><title>Combinatorial Auction-Based Pricing for Multi-tenant Autonomous Vehicle
  Public Transportation System</title><categories>cs.GT cs.SY</categories><comments>12 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A smart city provides its people with high standard of living through
advanced technologies and transport is one of the major foci. With the advent
of autonomous vehicles (AVs), an AV-based public transportation system has been
proposed recently, which is capable of providing new forms of transportation
services with high efficiency, high flexibility, and low cost. For the benefit
of passengers, multitenancy can increase market competition leading to lower
service charge and higher quality of service. In this paper, we study the
pricing issue of the multi-tenant AV public transportation system and three
types of services are defined. The pricing process for each service type is
modeled as a combinatorial auction, in which the service providers, as bidders,
compete for offering transportation services. The winners of the auction are
determined through an integer linear program. To prevent the bidders from
raising their bids for higher returns, we propose a strategy-proof
Vickrey-Clarke-Groves-based charging mechanism, which can maximize the social
welfare, to settle the final charges for the customers. We perform extensive
simulations to verify the analytical results and evaluate the performance of
the charging mechanism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01427</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01427</id><created>2015-02-28</created><authors><author><keyname>Hacklin</keyname><forenames>Fredrik</forenames></author><author><keyname>Minato</keyname><forenames>Nobuaki</forenames></author><author><keyname>Kobayashi</keyname><forenames>Toma</forenames></author></authors><title>The business model bank: conceptualizing a database structure for
  large-sample study of an emerging management concept</title><categories>cs.CY</categories><comments>Proceedings of the 5th International Conference on Collaborative
  Innovation Networks COINs15, Tokyo, Japan March 12-14, 2015
  (arXiv:1502.01142)</comments><report-no>coins15/2015/03</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The business model represents an increasingly important management concept.
However, progress in research related to the concept is currently inhibited
from inconsistencies in terms of formalizing and therewith also empirically
measuring the business model concept. Taking this as a starting point, this
paper offers a conceptualization for building a scalable database to rigorously
capture large samples of business models. The following contributions are made:
First, we suggest a concept for dimensions to be modeled in the database.
Second, we discuss issues critical to the scalability of such an endeavor.
Third, we point to empirical and simulation-based studies enabled by the
population of such a database. Considerations for theory and practice are
offered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01428</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01428</id><created>2015-03-04</created><updated>2015-12-22</updated><authors><author><keyname>Ding</keyname><forenames>Nan</forenames></author><author><keyname>Deng</keyname><forenames>Jia</forenames></author><author><keyname>Murphy</keyname><forenames>Kevin</forenames></author><author><keyname>Neven</keyname><forenames>Hartmut</forenames></author></authors><title>Probabilistic Label Relation Graphs with Ising Models</title><categories>cs.LG</categories><comments>International Conference on Computer Vision (2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider classification problems in which the label space has structure. A
common example is hierarchical label spaces, corresponding to the case where
one label subsumes another (e.g., animal subsumes dog). But labels can also be
mutually exclusive (e.g., dog vs cat) or unrelated (e.g., furry, carnivore). To
jointly model hierarchy and exclusion relations, the notion of a HEX (hierarchy
and exclusion) graph was introduced in [7]. This combined a conditional random
field (CRF) with a deep neural network (DNN), resulting in state of the art
results when applied to visual object classification problems where the
training labels were drawn from different levels of the ImageNet hierarchy
(e.g., an image might be labeled with the basic level category &quot;dog&quot;, rather
than the more specific label &quot;husky&quot;). In this paper, we extend the HEX model
to allow for soft or probabilistic relations between labels, which is useful
when there is uncertainty about the relationship between two labels (e.g., an
antelope is &quot;sort of&quot; furry, but not to the same degree as a grizzly bear). We
call our new model pHEX, for probabilistic HEX. We show that the pHEX graph can
be converted to an Ising model, which allows us to use existing off-the-shelf
inference methods (in contrast to the HEX method, which needed specialized
inference algorithms). Experimental results show significant improvements in a
number of large-scale visual object classification tasks, outperforming the
previous HEX model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01436</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01436</id><created>2015-03-04</created><updated>2016-02-11</updated><authors><author><keyname>Bai</keyname><forenames>Qinxun</forenames></author><author><keyname>Rosenberg</keyname><forenames>Steven</forenames></author><author><keyname>Wu</keyname><forenames>Zheng</forenames></author><author><keyname>Sclaroff</keyname><forenames>Stan</forenames></author></authors><title>Class Probability Estimation via Differential Geometric Regularization</title><categories>cs.LG cs.CG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of supervised learning for both binary and multiclass
classification from a unified geometric perspective. In particular, we propose
a geometric regularization technique to find the submanifold corresponding to a
robust estimator of the class probability $P(y|\pmb{x})$. The regularization
term measures the volume of this submanifold, based on the intuition that
overfitting produces rapid local oscillations and hence large volume of the
estimator. This technique can be applied to regularize any classification
function that satisfies two requirements: firstly, an estimator of the class
probability can be obtained; secondly, first and second derivatives of the
class probability estimator can be calculated. In experiments, we apply our
regularization technique to standard loss functions for classification, our
RBF-based implementation compares favorably to widely used regularization
methods for both binary and multiclass classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01438</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01438</id><created>2015-03-04</created><updated>2015-03-05</updated><authors><author><keyname>Horel</keyname><forenames>Thibaut</forenames></author><author><keyname>Singer</keyname><forenames>Yaron</forenames></author></authors><title>Scalable Methods for Adaptively Seeding a Social Network</title><categories>cs.SI physics.soc-ph</categories><comments>Full version of the paper appearing in WWW 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, social networking platforms have developed into
extraordinary channels for spreading and consuming information. Along with the
rise of such infrastructure, there is continuous progress on techniques for
spreading information effectively through influential users. In many
applications, one is restricted to select influencers from a set of users who
engaged with the topic being promoted, and due to the structure of social
networks, these users often rank low in terms of their influence potential. An
alternative approach one can consider is an adaptive method which selects users
in a manner which targets their influential neighbors. The advantage of such an
approach is that it leverages the friendship paradox in social networks: while
users are often not influential, they often know someone who is.
  Despite the various complexities in such optimization problems, we show that
scalable adaptive seeding is achievable. In particular, we develop algorithms
for linear influence models with provable approximation guarantees that can be
gracefully parallelized. To show the effectiveness of our methods we collected
data from various verticals social network users follow. For each vertical, we
collected data on the users who responded to a certain post as well as their
neighbors, and applied our methods on this data. Our experiments show that
adaptive seeding is scalable, and importantly, that it obtains dramatic
improvements over standard approaches of information dissemination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01440</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01440</id><created>2015-03-04</created><authors><author><keyname>Lin</keyname><forenames>Jiun-Ren</forenames></author><author><keyname>Talty</keyname><forenames>Timothy</forenames></author><author><keyname>Tonguz</keyname><forenames>Ozan K.</forenames></author></authors><title>A Blind Zone Alert System based on Intra-vehicular Wireless Sensor
  Networks</title><categories>cs.NI</categories><comments>9 pages, 8 figures, 1 table, IEEE Transactions on Industrial
  Informatics, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the increasing number of sensors deployed in modern vehicles,
Intra-Vehicular Wireless Sensor Networks (IVWSNs) have recently received a lot
of attention in the automotive industry as they can reduce the amount of wiring
harness inside a vehicle. By removing the wires, car manufacturers can reduce
the weight of a vehicle and improve engine performance, fuel economy, and
reliability. In addition to these direct benefits, an IVWSN is a versatile
platform that can support other vehicular applications as well. An example
application, known as a Side Blind Zone Alert (SBZA) system, which monitors the
blind zone of the vehicle and alerts the driver in a timely manner to prevent
collisions, is discussed in this paper. The performance of the IVWSN-based SBZA
system is evaluated via real experiments conducted on two test vehicles. Our
results show that the proposed system can achieve approximately 95% to 99%
detection rate with less than 15% false alarm rate. Compared to commercial
systems using radars or cameras, the main benefit of the IVWSN-based SBZA is
substantially lower cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01444</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01444</id><created>2015-03-04</created><updated>2015-08-13</updated><authors><author><keyname>Oh</keyname><forenames>Tae-Hyun</forenames></author><author><keyname>Tai</keyname><forenames>Yu-Wing</forenames></author><author><keyname>Bazin</keyname><forenames>Jean-Charles</forenames></author><author><keyname>Kim</keyname><forenames>Hyeongwoo</forenames></author><author><keyname>Kweon</keyname><forenames>In So</forenames></author></authors><title>Partial Sum Minimization of Singular Values in Robust PCA: Algorithm and
  Applications</title><categories>cs.CV cs.AI</categories><comments>Accepted in Transactions on Pattern Analysis and Machine Intelligence
  (TPAMI). To appear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robust Principal Component Analysis (RPCA) via rank minimization is a
powerful tool for recovering underlying low-rank structure of clean data
corrupted with sparse noise/outliers. In many low-level vision problems, not
only it is known that the underlying structure of clean data is low-rank, but
the exact rank of clean data is also known. Yet, when applying conventional
rank minimization for those problems, the objective function is formulated in a
way that does not fully utilize a priori target rank information about the
problems. This observation motivates us to investigate whether there is a
better alternative solution when using rank minimization. In this paper,
instead of minimizing the nuclear norm, we propose to minimize the partial sum
of singular values, which implicitly encourages the target rank constraint. Our
experimental analyses show that, when the number of samples is deficient, our
approach leads to a higher success rate than conventional rank minimization,
while the solutions obtained by the two approaches are almost identical when
the number of samples is more than sufficient. We apply our approach to various
low-level vision problems, e.g. high dynamic range imaging, motion edge
detection, photometric stereo, image alignment and recovery, and show that our
results outperform those obtained by the conventional nuclear norm rank
minimization method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01445</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01445</id><created>2015-03-04</created><authors><author><keyname>Unterthiner</keyname><forenames>Thomas</forenames></author><author><keyname>Mayr</keyname><forenames>Andreas</forenames></author><author><keyname>Klambauer</keyname><forenames>G&#xfc;nter</forenames></author><author><keyname>Hochreiter</keyname><forenames>Sepp</forenames></author></authors><title>Toxicity Prediction using Deep Learning</title><categories>stat.ML cs.LG cs.NE q-bio.BM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Everyday we are exposed to various chemicals via food additives, cleaning and
cosmetic products and medicines -- and some of them might be toxic. However
testing the toxicity of all existing compounds by biological experiments is
neither financially nor logistically feasible. Therefore the government
agencies NIH, EPA and FDA launched the Tox21 Data Challenge within the
&quot;Toxicology in the 21st Century&quot; (Tox21) initiative. The goal of this challenge
was to assess the performance of computational methods in predicting the
toxicity of chemical compounds. State of the art toxicity prediction methods
build upon specifically-designed chemical descriptors developed over decades.
Though Deep Learning is new to the field and was never applied to toxicity
prediction before, it clearly outperformed all other participating methods. In
this application paper we show that deep nets automatically learn features
resembling well-established toxicophores. In total, our Deep Learning approach
won both of the panel-challenges (nuclear receptors and stress response) as
well as the overall Grand Challenge, and thereby sets a new standard in tox
prediction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01446</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01446</id><created>2015-03-04</created><authors><author><keyname>Baez</keyname><forenames>Selene</forenames></author></authors><title>Predicting opponent team activity in a RoboCup environment</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this project is to predict the opponent's configuration in a
RoboCup SSL environment. For simplicity, a Markov model assumption is made such
that the predicted formation of the opponent team only depends on its current
formation. The field is divided into a grid and a robot state per player is
created with information about its position and its velocity. To gather a more
general sense of what the opposing team is doing, the state also incorporates
the team's average position (centroid). All possible state transitions are
stored in a hash table that requires minimum storage space. The table is
populated with transition probabilities that are learned by reading vision
packages and counting the state transitions regardless of the specific robot
player. Therefore, the computation during the game is reduced to interpreting a
given vision package to assign each player to a state, and looking for the most
likely state it will transition to. The confidence of the predicted team's
formation is the product of each individual player's probability. The project
is noteworthy in that it minimizes the time and space complexity requirements
for opponent's moves prediction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01457</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01457</id><created>2015-03-03</created><authors><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author></authors><title>Time Averaged Consensus in a Direct Coupled Distributed Coherent Quantum
  Observer</title><categories>quant-ph cs.SY math.OC</categories><comments>To appear in the Proceedings of the 2015 American Control Conference.
  arXiv admin note: substantial text overlap with arXiv:1503.01179</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of constructing a distributed direct
coupling quantum observer for a closed linear quantum system. The proposed
distributed observer consists of a network of quantum harmonic oscillators and
it is shown that the distributed observer converges to a consensus in a time
averaged sense in which each component of the observer estimates the specified
output of the quantum plant. An example and simulations are included to
illustrate the properties of the distributed observer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01484</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01484</id><created>2015-03-03</created><authors><author><keyname>Feng</keyname><forenames>Yong</forenames></author><author><keyname>Zeng</keyname><forenames>Rui</forenames></author><author><keyname>Wu</keyname><forenames>Jiasong</forenames></author></authors><title>p-norm-like Constraint Leaky LMS Algorithm for Sparse System
  Identification</title><categories>cs.SY</categories><comments>3 pages, 1 table, 4 figures, 10 equations, 10 references. arXiv admin
  note: substantial text overlap with arXiv:1503.01337</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel leaky least mean square (leaky LMS, LLMS)
algorithm which employs a p-norm-like constraint to force the solution to be
sparse in the application of system identification. As an extension of the LMS
algorithm which is the most widely-used adaptive filtering technique, the LLMS
algorithm has been proposed for decades, due to the deteriorated performance of
the standard LMS algorithm with highly correlated input. However, both ofthem
do not consider the sparsity information to have better behaviors. As a
sparse-aware modification of the LLMS, our proposed Lplike-LLMS algorithm,
incorporates a p-norm-like penalty into the cost function of the LLMS to obtain
a shrinkage in the weight update, which then enhances the performance in sparse
system identification settings. The simulation results show that the proposed
algorithm improves the performance of the filter in sparse system settings in
the presence of noisy input signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01488</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01488</id><created>2015-03-04</created><authors><author><keyname>Hosseini</keyname><forenames>Hadi</forenames></author><author><keyname>Larson</keyname><forenames>Kate</forenames></author><author><keyname>Cohen</keyname><forenames>Robin</forenames></author></authors><title>Random Serial Dictatorship versus Probabilistic Serial Rule: A Tale of
  Two Random Mechanisms</title><categories>cs.GT cs.AI cs.MA</categories><comments>19 pages</comments><msc-class>91B68, 91A99</msc-class><acm-class>I.2.11; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For assignment problems where agents, specifying ordinal preferences, are
allocated indivisible objects, two widely studied randomized mechanisms are the
Random Serial Dictatorship (RSD) and Probabilistic Serial Rule (PS). These two
mechanisms both have desirable economic and computational properties, but the
outcomes they induce can be incomparable in many instances, thus creating
challenges in deciding which mechanism to adopt in practice. In this paper we
first look at the space of lexicographic preferences and show that, as opposed
to the general preference domain, RSD satisfies envyfreeness. Moreover, we show
that although under lexicographic preferences PS is strategyproof when the
number of objects is less than or equal agents, it is strictly manipulable when
there are more objects than agents. In the space of general preferences, we
provide empirical results on the (in)comparability of RSD and PS, analyze
economic properties, and provide further insights on the applicability of each
mechanism in different application domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01489</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01489</id><created>2015-03-04</created><updated>2015-06-26</updated><authors><author><keyname>Sitharam</keyname><forenames>Meera</forenames></author><author><keyname>Willoughby</keyname><forenames>Joel</forenames></author></authors><title>On Flattenability of Graphs</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a generalization of the concept of $d$-flattenability of graphs -
introduced for the $l_2$ norm by Belk and Connelly - to general $l_p$ norms,
with integer $P$, $1 \le p &lt; \infty$, though many of our results work for
$l_\infty$ as well. The following results are shown for graphs $G$, using
notions of genericity, rigidity, and generic $d$-dimensional rigidity matroid
introduced by Kitson for frameworks in general $l_p$ norms, as well as the
cones of vectors of pairwise $l_p^p$ distances of a finite point configuration
in $d$-dimensional, $l_p$ space: (i) $d$-flattenability of a graph $G$ is
equivalent to the convexity of $d$-dimensional, inherent Cayley configurations
spaces for $G$, a concept introduced by the first author; (ii)
$d$-flattenability and convexity of Cayley configuration spaces over specified
non-edges of a $d$-dimensional framework are not generic properties of
frameworks (in arbitrary dimension); (iii) $d$-flattenability of $G$ is
equivalent to all of $G$'s generic frameworks being $d$-flattenable; (iv)
existence of one generic $d$-flattenable framework for $G$ is equivalent to the
independence of the edges of $G$, a generic property of frameworks; (v) the
rank of $G$ equals the dimension of the projection of the $d$-dimensional
stratum of the $l_p^p$ distance cone. We give stronger results for specific
norms for $d = 2$: we show that (vi) 2-flattenable graphs for the $l_1$-norm
(and $l_\infty$-norm) are a larger class than 2-flattenable graphs for
Euclidean $l_2$-norm case and finally (vii) prove further results towards
characterizing 2-flattenability in the $l_1$-norm. A number of conjectures and
open problems are posed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01508</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01508</id><created>2015-03-04</created><authors><author><keyname>Zhu</keyname><forenames>Xiangxin</forenames></author><author><keyname>Vondrick</keyname><forenames>Carl</forenames></author><author><keyname>Fowlkes</keyname><forenames>Charless</forenames></author><author><keyname>Ramanan</keyname><forenames>Deva</forenames></author></authors><title>Do We Need More Training Data?</title><categories>cs.CV</categories><doi>10.1007/s11263-015-0812-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Datasets for training object recognition systems are steadily increasing in
size. This paper investigates the question of whether existing detectors will
continue to improve as data grows, or saturate in performance due to limited
model complexity and the Bayes risk associated with the feature spaces in which
they operate. We focus on the popular paradigm of discriminatively trained
templates defined on oriented gradient features. We investigate the performance
of mixtures of templates as the number of mixture components and the amount of
training data grows. Surprisingly, even with proper treatment of regularization
and &quot;outliers&quot;, the performance of classic mixture models appears to saturate
quickly ($\sim$10 templates and $\sim$100 positive training examples per
template). This is not a limitation of the feature space as compositional
mixtures that share template parameters via parts and that can synthesize new
templates not encountered during training yield significantly better
performance. Based on our analysis, we conjecture that the greatest gains in
detection performance will continue to derive from improved representations and
learning algorithms that can make efficient use of large datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01514</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01514</id><created>2015-03-04</created><updated>2016-01-29</updated><authors><author><keyname>Wang</keyname><forenames>Xin</forenames></author><author><keyname>Ma</keyname><forenames>Richard T. B.</forenames></author><author><keyname>Xu</keyname><forenames>Yinlong</forenames></author></authors><title>The Role of Data Cap in Optimal Two-part Network Pricing</title><categories>cs.NI cs.GT</categories><comments>Claim of a mistake</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internet services are traditionally priced at flat rates; however, many
Internet service providers (ISPs) have recently shifted towards two-part
tariffs where a data cap is imposed to restrain data demand from heavy users.
Although the two-part tariff could generally increase the revenue for ISPs and
has been supported by the US FCC, the role of data cap and its optimal pricing
structures are not well understood. In this paper, we study the impact of data
cap on the optimal two-part pricing schemes for congestion-prone service
markets. We model users' demand and preferences over pricing and congestion
alternatives and derive the market share and congestion of service providers
under a market equilibrium. Based on the equilibrium model, we characterize the
two-part structures of the revenue- and welfare-optimal pricing schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01521</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01521</id><created>2015-03-04</created><updated>2015-10-06</updated><authors><author><keyname>Zhang</keyname><forenames>Liwen</forenames></author><author><keyname>Maji</keyname><forenames>Subhransu</forenames></author><author><keyname>Tomioka</keyname><forenames>Ryota</forenames></author></authors><title>Jointly Learning Multiple Measures of Similarities from Triplet
  Comparisons</title><categories>stat.ML cs.AI cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Similarity between objects is multi-faceted and it can be easier for human
annotators to measure it when the focus is on a specific aspect. We consider
the problem of mapping objects into view-specific embeddings where the distance
between them is consistent with the similarity comparisons of the form &quot;from
the t-th view, object A is more similar to B than to C&quot;. Our framework jointly
learns view-specific embeddings exploiting correlations between views.
Experiments on a number of datasets, including one of multi-view crowdsourced
comparison on bird images, show the proposed method achieves lower triplet
generalization error when compared to both learning embeddings independently
for each view and all views pooled into one view. Our method can also be used
to learn multiple measures of similarity over input features taking class
labels into account and compares favorably to existing approaches for
multi-task metric learning on the ISOLET dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01524</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01524</id><created>2015-03-04</created><authors><author><keyname>Handmer</keyname><forenames>Casey J.</forenames></author></authors><title>Genetic optimization of the Hyperloop route through the Grapevine</title><categories>cs.NE</categories><comments>16 pages, 4 figures, 1 Mathematica notebook attachment</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We demonstrate a genetic algorithm that employs a versatile fitness function
to optimize route selection for the Hyperloop, a proposed high speed passenger
transportation system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01531</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01531</id><created>2015-03-04</created><authors><author><keyname>Mizutani</keyname><forenames>Tomohiko</forenames></author></authors><title>Spectral Clustering by Ellipsoid and Its Connection to Separable
  Nonnegative Matrix Factorization</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a variant of the normalized cut algorithm for spectral
clustering. Although the normalized cut algorithm applies the K-means algorithm
to the eigenvectors of a normalized graph Laplacian for finding clusters, our
algorithm instead uses a minimum volume enclosing ellipsoid for them. We show
that the algorithm shares similarity with the ellipsoidal rounding algorithm
for separable nonnegative matrix factorization. Our theoretical insight implies
that the algorithm can serve as a bridge between spectral clustering and
separable NMF. The K-means algorithm has the issues in that the choice of
initial points affects the construction of clusters and certain choices result
in poor clustering performance. The normalized cut algorithm inherits these
issues since K-means is incorporated in it, whereas the algorithm proposed here
does not. An empirical study is presented to examine the performance of the
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01532</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01532</id><created>2015-03-04</created><authors><author><keyname>Jung</keyname><forenames>Heechul</forenames></author><author><keyname>Lee</keyname><forenames>Sihaeng</forenames></author><author><keyname>Park</keyname><forenames>Sunjeong</forenames></author><author><keyname>Lee</keyname><forenames>Injae</forenames></author><author><keyname>Ahn</keyname><forenames>Chunghyun</forenames></author><author><keyname>Kim</keyname><forenames>Junmo</forenames></author></authors><title>Deep Temporal Appearance-Geometry Network for Facial Expression
  Recognition</title><categories>cs.CV</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Temporal information can provide useful features for recognizing facial
expressions. However, to manually design useful features requires a lot of
effort. In this paper, to reduce this effort, a deep learning technique which
is regarded as a tool to automatically extract useful features from raw data,
is adopted. Our deep network is based on two different models. The first deep
network extracts temporal geometry features from temporal facial landmark
points, while the other deep network extracts temporal appearance features from
image sequences . These two models are combined in order to boost the
performance of the facial expression recognition. Through several experiments,
we showed that the two models cooperate with each other. As a result, we
achieved superior performance to other state-of-the-art methods in CK+ and
Oulu-CASIA databases. Furthermore, one of the main contributions of this paper
is that our deep network catches the facial action points automatically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01535</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01535</id><created>2015-03-04</created><authors><author><keyname>Borjian</keyname><forenames>Setareh</forenames></author><author><keyname>Manshadi</keyname><forenames>Vahideh H.</forenames></author><author><keyname>Barnhart</keyname><forenames>Cynthia</forenames></author><author><keyname>Jaillet</keyname><forenames>Patrick</forenames></author></authors><title>Managing Relocation and Delay in Container Terminals with Flexible
  Service Policies</title><categories>cs.DS math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new model and mathematical formulation for planning crane
moves in the storage yard of container terminals. Our objective is to develop a
tool that captures customer centric elements, especially service time, and
helps operators to manage costly relocation moves. Our model incorporates
several practical details and provides port operators with expanded
capabilities including planning repositioning moves in off-peak hours,
controlling wait times of each customer as well as total service time,
optimizing the number of relocations and wait time jointly, and optimizing
simultaneously the container stacking and retrieval process. We also study a
class of flexible service policies which allow for out-of-order retrieval. We
show that under such flexible policies, we can decrease the number of
relocations and retrieval delays without creating inequities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01538</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01538</id><created>2015-03-04</created><authors><author><keyname>Bilenko</keyname><forenames>Natalia Y.</forenames></author><author><keyname>Gallant</keyname><forenames>Jack L.</forenames></author></authors><title>Pyrcca: regularized kernel canonical correlation analysis in Python and
  its applications to neuroimaging</title><categories>q-bio.QM cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Canonical correlation analysis (CCA) is a valuable method for interpreting
cross-covariance across related datasets of different dimensionality. There are
many potential applications of CCA to neuroimaging data analysis. For instance,
CCA can be used for finding functional similarities across fMRI datasets
collected from multiple subjects without resampling individual datasets to a
template anatomy. In this paper, we introduce Pyrcca, an open-source Python
module for executing CCA between two or more datasets. Pyrcca can be used to
implement CCA with or without regularization, and with or without linear or a
Gaussian kernelization of the datasets. We demonstrate an application of CCA
implemented with Pyrcca to neuroimaging data analysis. We use CCA to find a
data-driven set of functional response patterns that are similar across
individual subjects in a natural movie experiment. We then demonstrate how this
set of response patterns discovered by CCA can be used to accurately predict
subject responses to novel natural movie stimuli.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01539</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01539</id><created>2015-03-05</created><updated>2016-02-23</updated><authors><author><keyname>Ma</keyname><forenames>Qian</forenames></author><author><keyname>Gao</keyname><forenames>Lin</forenames></author><author><keyname>Liu</keyname><forenames>Ya-Feng</forenames></author><author><keyname>Huang</keyname><forenames>Jianwei</forenames></author></authors><title>A Game-Theoretic Analysis of User Behaviors in Crowdsourced Wireless
  Community Networks</title><categories>cs.GT cs.NI</categories><comments>This manuscript serves as the online technical report of the article
  published in the International Symposium on Modeling and Optimization in
  Mobile, Ad Hoc and Wireless Networks (WiOpt), 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A crowdsourced wireless community network can effectively alleviate the
limited coverage issue of Wi-Fi access points (APs), by encouraging individuals
(users) to share their private residential Wi-Fi APs with each other. This
paper presents the first study on the users' joint membership selection and
network access problem in such a network. Specifically, we formulate the
problem as a two-stage dynamic game: Stage I corresponds to a membership
selection game, in which each user chooses his membership type; Stage II
corresponds to a set of network access games, in each of which each user
decides his WiFi connection time on the AP at his current location. We analyze
the Subgame Perfect Equilibrium (SPE) systematically, and study whether and how
best response dynamics can reach the equilibrium. Through numerical studies, we
further explore how the equilibrium changes with the users' mobility patterns
and network access evaluations. We show that a user with a more popular home
location, a smaller travel time, or a smaller network access evaluation is more
likely to choose a specific type of membership called Bill. We further
demonstrate how the network operator can optimize its pricing and incentive
mechanism based on the game equilibrium analysis in this work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01543</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01543</id><created>2015-03-05</created><authors><author><keyname>Paisitkriangkrai</keyname><forenames>Sakrapee</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author></authors><title>Learning to rank in person re-identification with metric ensembles</title><categories>cs.CV</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an effective structured learning based approach to the problem of
person re-identification which outperforms the current state-of-the-art on most
benchmark data sets evaluated. Our framework is built on the basis of multiple
low-level hand-crafted and high-level visual features. We then formulate two
optimization algorithms, which directly optimize evaluation measures commonly
used in person re-identification, also known as the Cumulative Matching
Characteristic (CMC) curve. Our new approach is practical to many real-world
surveillance applications as the re-identification performance can be
concentrated in the range of most practical importance. The combination of
these factors leads to a person re-identification system which outperforms most
existing algorithms. More importantly, we advance state-of-the-art results on
person re-identification by improving the rank-$1$ recognition rates from
$40\%$ to $50\%$ on the iLIDS benchmark, $16\%$ to $18\%$ on the PRID2011
benchmark, $43\%$ to $46\%$ on the VIPeR benchmark, $34\%$ to $53\%$ on the
CUHK01 benchmark and $21\%$ to $62\%$ on the CUHK03 benchmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01546</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01546</id><created>2015-03-05</created><updated>2015-03-25</updated><authors><author><keyname>Mejova</keyname><forenames>Yelena</forenames></author><author><keyname>Haddadi</keyname><forenames>Hamed</forenames></author><author><keyname>Noulas</keyname><forenames>Anastasios</forenames></author><author><keyname>Weber</keyname><forenames>Ingmar</forenames></author></authors><title>#FoodPorn: Obesity Patterns in Culinary Interactions</title><categories>cs.CY</categories><acm-class>J.3; H.3.1; H.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a large-scale analysis of Instagram pictures taken at 164,753
restaurants by millions of users. Motivated by the obesity epidemic in the
United States, our aim is three-fold: (i) to assess the relationship between
fast food and chain restaurants and obesity, (ii) to better understand people's
thoughts on and perceptions of their daily dining experiences, and (iii) to
reveal the nature of social reinforcement and approval in the context of
dietary health on social media. When we correlate the prominence of fast food
restaurants in US counties with obesity, we find the Foursquare data to show a
greater correlation at 0.424 than official survey data from the County Health
Rankings would show. Our analysis further reveals a relationship between small
businesses and local foods with better dietary health, with such restaurants
getting more attention in areas of lower obesity. However, even in such areas,
social approval favors the unhealthy foods high in sugar, with donut shops
producing the most liked photos. Thus, the dietary landscape our study reveals
is a complex ecosystem, with fast food playing a role alongside social
interactions and personal perceptions, which often may be at odds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01547</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01547</id><created>2015-03-05</created><authors><author><keyname>Cox</keyname><forenames>Arlen</forenames></author></authors><title>Binary-Decision-Diagrams for Set Abstraction</title><categories>cs.LO cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Whether explicit or implicit, sets are a critical part of many pieces of
software. As a result, it is necessary to develop abstractions of sets for the
purposes of abstract interpretation, model checking, and deductive
verification. However, the construction of effective abstractions for sets is
challenging because they are a higher-order construct. It is necessary to
reason about contents of sets as well as relationships between sets. This paper
presents a new abstraction for sets that is based on binary decision diagrams.
It is optimized for precisely and efficiently representing relations between
sets while still providing limited support for content reasoning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01549</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01549</id><created>2015-03-05</created><authors><author><keyname>Hsu</keyname><forenames>William</forenames></author><author><keyname>Abduljabbar</keyname><forenames>Mohammed</forenames></author><author><keyname>Osuga</keyname><forenames>Ryuichi</forenames></author><author><keyname>Lu</keyname><forenames>Max</forenames></author><author><keyname>Elshamy</keyname><forenames>Wesam</forenames></author></authors><title>Visualization of Clandestine Labs from Seizure Reports: Thematic Mapping
  and Data Mining Research Directions</title><categories>cs.IR cs.CL</categories><comments>In Proceedings of The 2nd European Workshop on Human-Computer
  Interaction and Information Retrieval EuroHCIR2012, pages 43--46, Nijmegen,
  the Netherlands, 24th/25th August 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of spatiotemporal event visualization based on reports entails
subtasks ranging from named entity recognition to relationship extraction and
mapping of events. We present an approach to event extraction that is driven by
data mining and visualization goals, particularly thematic mapping and trend
analysis. This paper focuses on bridging the information extraction and
visualization tasks and investigates topic modeling approaches. We develop a
static, finite topic model and examine the potential benefits and feasibility
of extending this to dynamic topic modeling with a large number of topics and
continuous time. We describe an experimental test bed for event mapping that
uses this end-to-end information retrieval system, and report preliminary
results on a geoinformatics problem: tracking of methamphetamine lab seizure
events across time and space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01557</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01557</id><created>2015-03-05</created><updated>2015-04-18</updated><authors><author><keyname>Shen</keyname><forenames>Fumin</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Liu</keyname><forenames>Wei</forenames></author><author><keyname>Shen</keyname><forenames>Heng Tao</forenames></author></authors><title>Supervised Discrete Hashing</title><categories>cs.CV</categories><comments>This paper has been withdrawn by the authour since the algorithm is
  being used for patent application</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper has been withdrawn by the authour.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01558</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01558</id><created>2015-03-05</created><updated>2015-03-13</updated><authors><author><keyname>Malmaud</keyname><forenames>Jonathan</forenames></author><author><keyname>Huang</keyname><forenames>Jonathan</forenames></author><author><keyname>Rathod</keyname><forenames>Vivek</forenames></author><author><keyname>Johnston</keyname><forenames>Nick</forenames></author><author><keyname>Rabinovich</keyname><forenames>Andrew</forenames></author><author><keyname>Murphy</keyname><forenames>Kevin</forenames></author></authors><title>What's Cookin'? Interpreting Cooking Videos using Text, Speech and
  Vision</title><categories>cs.CL cs.CV cs.IR</categories><comments>To appear in NAACL 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel method for aligning a sequence of instructions to a video
of someone carrying out a task. In particular, we focus on the cooking domain,
where the instructions correspond to the recipe. Our technique relies on an HMM
to align the recipe steps to the (automatically generated) speech transcript.
We then refine this alignment using a state-of-the-art visual food detector,
based on a deep convolutional neural network. We show that our technique
outperforms simpler techniques based on keyword spotting. It also enables
interesting applications, such as automatically illustrating recipes with
keyframes, and searching within a video for events of interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01563</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01563</id><created>2015-03-05</created><authors><author><keyname>Kumar</keyname><forenames>K. S. Sesh</forenames><affiliation>LIENS,INRIA Paris - Rocquencourt</affiliation></author><author><keyname>Barbero</keyname><forenames>Alvaro</forenames><affiliation>MIT</affiliation></author><author><keyname>Jegelka</keyname><forenames>Stefanie</forenames><affiliation>MIT</affiliation></author><author><keyname>Sra</keyname><forenames>Suvrit</forenames><affiliation>MIT</affiliation></author><author><keyname>Bach</keyname><forenames>Francis</forenames><affiliation>LIENS,INRIA Paris - Rocquencourt</affiliation></author></authors><title>Convex Optimization for Parallel Energy Minimization</title><categories>cs.CV math.OC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy minimization has been an intensely studied core problem in computer
vision. With growing image sizes (2D and 3D), it is now highly desirable to run
energy minimization algorithms in parallel. But many existing algorithms, in
particular, some efficient combinatorial algorithms, are difficult to
par-allelize. By exploiting results from convex and submodular theory, we
reformulate the quadratic energy minimization problem as a total variation
denoising problem, which, when viewed geometrically, enables the use of
projection and reflection based convex methods. The resulting min-cut algorithm
(and code) is conceptually very simple, and solves a sequence of TV denoising
problems. We perform an extensive empirical evaluation comparing
state-of-the-art combinatorial algorithms and convex optimization techniques.
On small problems the iterative convex methods match the combinatorial max-flow
algorithms, while on larger problems they offer other flexibility and important
gains: (a) their memory footprint is small; (b) their straightforward
parallelizability fits multi-core platforms; (c) they can easily be
warm-started; and (d) they quickly reach approximately good solutions, thereby
enabling faster &quot;inexact&quot; solutions. A key consequence of our approach based on
submodularity and convexity is that it is allows to combine any arbitrary
combinatorial or convex methods as subroutines, which allows one to obtain
hybrid combinatorial and convex optimization algorithms that benefit from the
strengths of both.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="73000" completeListSize="102538">1122234|74001</resumptionToken>
</ListRecords>
</OAI-PMH>
