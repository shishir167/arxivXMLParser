<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T01:00:39Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|32001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5024</identifier>
 <datestamp>2012-05-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5024</id><created>2012-05-22</created><authors><author><keyname>Mishra</keyname><forenames>A. K.</forenames></author><author><keyname>Chandrasekharan</keyname><forenames>H.</forenames></author></authors><title>Analytical Study of Hexapod miRNAs using Phylogenetic Methods</title><categories>cs.CE q-bio.GN</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MicroRNAs (miRNAs) are a class of non-coding RNAs that regulate gene
expression. Identification of total number of miRNAs even in completely
sequenced organisms is still an open problem. However, researchers have been
using techniques that can predict limited number of miRNA in an organism. In
this paper, we have used homology based approach for comparative analysis of
miRNA of hexapoda group .We have used Apis mellifera, Bombyx mori, Anopholes
gambiae and Drosophila melanogaster miRNA datasets from miRBase repository. We
have done pair wise as well as multiple alignments for the available miRNAs in
the repository to identify and analyse conserved regions among related species.
Unfortunately, to the best of our knowledge, miRNA related literature does not
provide in depth analysis of hexapods. We have made an attempt to derive the
commonality among the miRNAs and to identify the conserved regions which are
still not available in miRNA repositories. The results are good approximation
with a small number of mismatches. However, they are encouraging and may
facilitate miRNA biogenesis for
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5025</identifier>
 <datestamp>2012-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5025</id><created>2012-05-22</created><updated>2012-08-02</updated><authors><author><keyname>Steinmann</keyname><forenames>Casper</forenames></author><author><keyname>Ibsen</keyname><forenames>Mikael W.</forenames></author><author><keyname>Hansen</keyname><forenames>Anne S.</forenames></author><author><keyname>Jensen</keyname><forenames>Jan H.</forenames></author></authors><title>FragIt: A Tool to Prepare Input Files for Fragment Based Quantum
  Chemical Calculations</title><categories>cs.CE physics.chem-ph</categories><comments>27 pages, 9 figures</comments><doi>10.1371/journal.pone.0044480</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Near linear scaling fragment based quantum chemical calculations are becoming
increasingly popular for treating large systems with high accuracy and is an
active field of research. However, it remains difficult to set up these
calculations without expert knowledge. To facilitate the use of such methods,
software tools need to be available to support these methods and help to set up
reasonable input files which will lower the barrier of entry for usage by
non-experts. Previous tools relies on specific annotations in structure files
for automatic and successful fragmentation such as residues in PDB files. We
present a general fragmentation methodology and accompanying tools called
FragIt to help setup these calculations. FragIt uses the SMARTS language to
locate chemically appropriate fragments in large structures and is applicable
to fragmentation of any molecular system given suitable SMARTS patterns. We
present SMARTS patterns of fragmentation for proteins, DNA and polysaccharides,
specifically for D-galactopyranose for use in cyclodextrins. FragIt is used to
prepare input files for the Fragment Molecular Orbital method in the GAMESS
program package, but can be extended to other computational methods easily.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5055</identifier>
 <datestamp>2012-05-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5055</id><created>2012-05-22</created><authors><author><keyname>Anderson</keyname><forenames>Matthew</forenames></author><author><keyname>Brodowicz</keyname><forenames>Maciej</forenames></author><author><keyname>Kaiser</keyname><forenames>Hartmut</forenames></author><author><keyname>Adelstein-Lelbach</keyname><forenames>Bryce</forenames></author><author><keyname>Sterling</keyname><forenames>Thomas</forenames></author></authors><title>Neutron Star Evolutions using Tabulated Equations of State with a New
  Execution Model</title><categories>cs.DC</categories><comments>9 pages, 8 figures. arXiv admin note: substantial text overlap with
  arXiv:1110.1131</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The addition of nuclear and neutrino physics to general relativistic fluid
codes allows for a more realistic description of hot nuclear matter in neutron
star and black hole systems. This additional microphysics requires that each
processor have access to large tables of data, such as equations of state, and
in large simulations the memory required to store these tables locally can
become excessive unless an alternative execution model is used. In this work we
present relativistic fluid evolutions of a neutron star obtained using a
message driven multi-threaded execution model known as ParalleX. These neutron
star simulations would require substantial memory overhead dedicated entirely
to the equation of state table if using a more traditional execution model. We
introduce a ParalleX component based on Futures for accessing large tables of
data, including out-of-core sized tables, which does not require substantial
memory overhead and effectively hides any increased network latency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5062</identifier>
 <datestamp>2012-05-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5062</id><created>2012-05-22</created><authors><author><keyname>Freibert</keyname><forenames>Finley</forenames></author></authors><title>The Classification of Complementary Information Set Codes of Lengths 14
  and 16</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the paper &quot;A new class of codes for Boolean masking of cryptographic
computations,&quot; Carlet, Gaborit, Kim, and Sol\'{e} defined a new class of rate
one-half binary codes called \emph{complementary information set} (or CIS)
codes. The authors then classified all CIS codes of length less than or equal
to 12. CIS codes have relations to classical Coding Theory as they are a
generalization of self-dual codes. As stated in the paper, CIS codes also have
important practical applications as they may improve the cost of masking
cryptographic algorithms against side channel attacks. In this paper, we give a
complete classification result for length 14 CIS codes using an equivalence
relation on $GL(n,\FF_2)$. We also give a new classification for all binary
$[16,8,3]$ and $[16,8,4]$ codes. We then complete the classification for length
16 CIS codes and give additional classifications for optimal CIS codes of
lengths 20 and 26.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5073</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5073</id><created>2012-05-22</created><authors><author><keyname>Fawzi</keyname><forenames>Hamza</forenames></author><author><keyname>Tabuada</keyname><forenames>Paulo</forenames></author><author><keyname>Diggavi</keyname><forenames>Suhas</forenames></author></authors><title>Secure estimation and control for cyber-physical systems under
  adversarial attacks</title><categories>math.OC cs.CR cs.IT cs.SY math.IT</categories><journal-ref>IEEE Transactions on Automatic Control, vol. 59, no. 6,
  pp.1454-1467, 2014</journal-ref><doi>10.1109/TAC.2014.2303233</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The vast majority of today's critical infrastructure is supported by numerous
feedback control loops and an attack on these control loops can have disastrous
consequences. This is a major concern since modern control systems are becoming
large and decentralized and thus more vulnerable to attacks. This paper is
concerned with the estimation and control of linear systems when some of the
sensors or actuators are corrupted by an attacker. In the first part we look at
the estimation problem where we characterize the resilience of a system to
attacks and study the possibility of increasing its resilience by a change of
parameters. We then propose an efficient algorithm to estimate the state
despite the attacks and we characterize its performance. Our approach is
inspired from the areas of error-correction over the reals and compressed
sensing. In the second part we consider the problem of designing
output-feedback controllers that stabilize the system despite attacks. We show
that a principle of separation between estimation and control holds and that
the design of resilient output feedback controllers can be reduced to the
design of resilient state estimators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5075</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5075</id><created>2012-05-22</created><updated>2013-01-18</updated><authors><author><keyname>Xiang</keyname><forenames>Shuo</forenames></author><author><keyname>Shen</keyname><forenames>Xiaotong</forenames></author><author><keyname>Ye</keyname><forenames>Jieping</forenames></author></authors><title>Efficient Sparse Group Feature Selection via Nonconvex Optimization</title><categories>cs.LG stat.ML</categories><comments>Accepted by the 30th International Conference on Machine Learning
  (ICML 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse feature selection has been demonstrated to be effective in handling
high-dimensional data. While promising, most of the existing works use convex
methods, which may be suboptimal in terms of the accuracy of feature selection
and parameter estimation. In this paper, we expand a nonconvex paradigm to
sparse group feature selection, which is motivated by applications that require
identifying the underlying group structure and performing feature selection
simultaneously. The main contributions of this article are twofold: (1)
statistically, we introduce a nonconvex sparse group feature selection model
which can reconstruct the oracle estimator. Therefore, consistent feature
selection and parameter estimation can be achieved; (2) computationally, we
propose an efficient algorithm that is applicable to large-scale problems.
Numerical results suggest that the proposed nonconvex method compares favorably
against its competitors on synthetic data and real-world applications, thus
achieving desired goal of delivering high performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5088</identifier>
 <datestamp>2012-05-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5088</id><created>2012-05-23</created><authors><author><keyname>Webb</keyname><forenames>Dustin J.</forenames></author><author><keyname>Berg</keyname><forenames>Jur van den</forenames></author></authors><title>Kinodynamic RRT*: Optimal Motion Planning for Systems with Linear
  Differential Constraints</title><categories>cs.RO cs.DS</categories><comments>8 pages, 7 figures, 1 table, slated for submission to ICRA 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present Kinodynamic RRT*, an incremental sampling-based approach for
asymptotically optimal motion planning for robots with linear differential
constraints. Our approach extends RRT*, which was introduced for holonomic
robots (Karaman et al. 2011), by using a fixed-final-state-free-final-time
controller that exactly and optimally connects any pair of states, where the
cost function is expressed as a trade-off between the duration of a trajectory
and the expended control effort. Our approach generalizes earlier work on
extending RRT* to kinodynamic systems, as it guarantees asymptotic optimality
for any system with controllable linear dynamics, in state spaces of any
dimension. Our approach can be applied to non-linear dynamics as well by using
their first-order Taylor approximations. In addition, we show that for the rich
subclass of systems with a nilpotent dynamics matrix, closed-form solutions for
optimal trajectories can be derived, which keeps the computational overhead of
our algorithm compared to traditional RRT* at a minimum. We demonstrate the
potential of our approach by computing asymptotically optimal trajectories in
three challenging motion planning scenarios: (i) a planar robot with a 4-D
state space and double integrator dynamics, (ii) an aerial vehicle with a 10-D
state space and linearized quadrotor dynamics, and (iii) a car-like robot with
a 5-D state space and non-linear dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5097</identifier>
 <datestamp>2012-05-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5097</id><created>2012-05-23</created><authors><author><keyname>Vijayalaxmi</keyname></author><author><keyname>Rao</keyname><forenames>P. Sudhakara</forenames></author><author><keyname>Sreehari</keyname><forenames>S.</forenames></author></authors><title>Neural Network Approach for Eye Detection</title><categories>cs.CV</categories><comments>12 PAGES, 8 FIGURES, CCSEA 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Driving support systems, such as car navigation systems are becoming common
and they support driver in several aspects. Non-intrusive method of detecting
Fatigue and drowsiness based on eye-blink count and eye directed instruction
controlhelps the driver to prevent from collision caused by drowsy driving. Eye
detection and tracking under various conditions such as illumination,
background, face alignment and facial expression makes the problem
complex.Neural Network based algorithm is proposed in this paper to detect the
eyes efficiently. In the proposed algorithm, first the neural Network is
trained to reject the non-eye regionbased on images with features of eyes and
the images with features of non-eye using Gabor filter and Support Vector
Machines to reduce the dimension and classify efficiently. In the algorithm,
first the face is segmented using L*a*btransform color space, then eyes are
detected using HSV and Neural Network approach. The algorithm is tested on
nearly 100 images of different persons under different conditions and the
results are satisfactory with success rate of 98%.The Neural Network is trained
with 50 non-eye images and 50 eye images with different angles using Gabor
filter. This paper is a part of research work on &quot;Development of Non-Intrusive
system for real-time Monitoring and Prediction of Driver Fatigue and
drowsiness&quot; project sponsored by Department of Science &amp; Technology, Govt. of
India, New Delhi at Vignan Institute of Technology and Sciences, Vignan Hills,
Hyderabad.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5098</identifier>
 <datestamp>2012-05-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5098</id><created>2012-05-23</created><authors><author><keyname>Sodhi</keyname><forenames>Balwinder</forenames></author><author><keyname>T.</keyname><forenames>Prabhakar</forenames><suffix>V</suffix></author></authors><title>A Simplified Description of Fuzzy TOPSIS</title><categories>cs.AI</categories><comments>3 pages</comments><acm-class>G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A simplified description of Fuzzy TOPSIS (Technique for Order Preference by
Similarity to Ideal Situation) is presented. We have adapted the TOPSIS
description from existing Fuzzy theory literature and distilled the bare
minimum concepts required for understanding and applying TOPSIS. An example has
been worked out to illustrate the application of TOPSIS for a multi-criteria
group decision making scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5106</identifier>
 <datestamp>2012-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5106</id><created>2012-05-23</created><updated>2012-08-09</updated><authors><author><keyname>Triantafyllou</keyname><forenames>Nikolaos</forenames></author><author><keyname>Stefaneas</keyname><forenames>Petros</forenames></author><author><keyname>Frangos</keyname><forenames>Panayiotis</forenames></author></authors><title>OTS/CafeOBJ2JML: An attempt to combine Design By Contract with
  Behavioral Specifications</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Design by Constract (DBC) has influenced the development of formal
specification languages that allow the mix of specification and implementation
code, like Eiffel, the Java Modeling Language (JML) and Spec#. Meanwhile
algebraic specification languages have been developing independently and offer
full support for specification and verification of design for large and complex
systems in a mathematical rigorous way. However there is no guarantee that the
final implementation will comply to the specification. In this paper we
proposed the use of the latter for the specification and verification of the
systems design and then by presenting a translation between the two, the use of
the former to ensure that the implementation respects the specification and
thus enjoy the verified properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5109</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5109</id><created>2012-05-23</created><updated>2013-07-06</updated><authors><author><keyname>Masuda</keyname><forenames>Naoki</forenames></author><author><keyname>Takaguchi</keyname><forenames>Taro</forenames></author><author><keyname>Sato</keyname><forenames>Nobuo</forenames></author><author><keyname>Yano</keyname><forenames>Kazuo</forenames></author></authors><title>Self-exciting point process modeling of conversation event sequences</title><categories>physics.soc-ph cs.SI</categories><comments>8 figures</comments><journal-ref>In: Temporal Networks, P. Holme and J. Saramaki (Eds.),
  Springer-Verlag, Berlin (2013), pp. 245-264</journal-ref><doi>10.1007/978-3-642-36461-7_12</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Self-exciting processes of Hawkes type have been used to model various
phenomena including earthquakes, neural activities, and views of online videos.
Studies of temporal networks have revealed that sequences of social interevent
times for individuals are highly bursty. We examine some basic properties of
event sequences generated by the Hawkes self-exciting process to show that it
generates bursty interevent times for a wide parameter range. Then, we fit the
model to the data of conversation sequences recorded in company offices in
Japan. In this way, we can estimate relative magnitudes of the self excitement,
its temporal decay, and the base event rate independent of the self excitation.
These variables highly depend on individuals. We also point out that the Hawkes
model has an important limitation that the correlation in the interevent times
and the burstiness cannot be independently modulated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5124</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5124</id><created>2012-05-23</created><authors><author><keyname>Tanbourgi</keyname><forenames>Ralph</forenames></author><author><keyname>J&#xe4;kel</keyname><forenames>Holger</forenames></author><author><keyname>Chaichenets</keyname><forenames>Leonid</forenames></author><author><keyname>Jondral</keyname><forenames>Friedrich K.</forenames></author></authors><title>Interference and Throughput in Aloha-based Ad Hoc Networks with
  Isotropic Node Distribution</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 figures. To appear in International Symposium on
  Information Theory (ISIT) 2012</comments><journal-ref>IEEE International Symposium on Information Theory, pp.616--620,
  1-6 July 2012</journal-ref><doi>10.1109/ISIT.2012.6284265</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the interference and outage statistics in a slotted Aloha ad hoc
network, where the spatial distribution of nodes is non-stationary and
isotropic. In such a network, outage probability and local throughput depend on
both the particular location in the network and the shape of the spatial
distribution. We derive in closed-form certain distributional properties of the
interference that are important for analyzing wireless networks as a function
of the location and the spatial shape. Our results focus on path loss exponents
2 and 4, the former case not being analyzable before due to the stationarity
assumption of the spatial node distribution. We propose two metrics for
measuring local throughput in non-stationary networks and discuss how our
findings can be applied to both analysis and optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5134</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5134</id><created>2012-05-23</created><updated>2013-03-11</updated><authors><author><keyname>Markin</keyname><forenames>Nadya</forenames></author><author><keyname>Oggier</keyname><forenames>Frederique</forenames></author></authors><title>Iterated Space-Time Code Constructions from Cyclic Algebras</title><categories>cs.IT math.IT math.RA</categories><comments>32 pages, 2 figures</comments><msc-class>16U99, 11T71</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a full-rate iterated space-time code construction, to design
2n-dimensional codes from n-dimensional cyclic algebra based codes. We give a
condition to determine whether the resulting codes satisfy the full-diversity
property, and study their maximum likelihood decoding complexity with respect
to sphere decoding. Particular emphasis is given to the cases n = 2, sometimes
referred to as MIDO (multiple input double output) codes, and n = 3. In the
process, we derive an interesting way of obtaining division algebras, and study
their center and maximal subfield.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5136</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5136</id><created>2012-05-23</created><updated>2014-05-08</updated><authors><author><keyname>Winkler</keyname><forenames>Severin</forenames></author><author><keyname>Wullschleger</keyname><forenames>J&#xfc;rg</forenames></author></authors><title>On the Efficiency of Classical and Quantum Secure Function Evaluation</title><categories>cs.CR quant-ph</categories><journal-ref>IEEE Trans. Inf. Th., vol. 60, no. 6, p. 1-21 (2014)</journal-ref><doi>10.1109/TIT.2014.2314467</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide bounds on the efficiency of secure one-sided output two-party
computation of arbitrary finite functions from trusted distributed randomness
in the statistical case. From these results we derive bounds on the efficiency
of protocols that use different variants of OT as a black-box. When applied to
implementations of OT, these bounds generalize most known results to the
statistical case. Our results hold in particular for transformations between a
finite number of primitives and for any error. In the second part we study the
efficiency of quantum protocols implementing OT. While most classical lower
bounds for perfectly secure reductions of OT to distributed randomness still
hold in the quantum setting, we present a statistically secure protocol that
violates these bounds by an arbitrarily large factor. We then prove a weaker
lower bound that does hold in the statistical quantum setting and implies that
even quantum protocols cannot extend OT. Finally, we present two lower bounds
for reductions of OT to commitments and a protocol based on string commitments
that is optimal with respect to both of these bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5141</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5141</id><created>2012-05-23</created><updated>2013-01-24</updated><authors><author><keyname>Araya</keyname><forenames>Makoto</forenames></author><author><keyname>Harada</keyname><forenames>Masaaki</forenames></author></authors><title>There is no [21, 5, 14] code over F5</title><categories>math.CO cs.IT math.IT</categories><comments>5 pages</comments><journal-ref>Discrete Math. 313 (2013) 2872-2874</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, we demonstrate that there is no [21, 5, 14] code over F5.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5147</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5147</id><created>2012-05-23</created><authors><author><keyname>Tekbiyik</keyname><forenames>Neyre</forenames></author><author><keyname>Girici</keyname><forenames>Tolga</forenames></author><author><keyname>Uysal-Biyikoglu</keyname><forenames>Elif</forenames></author><author><keyname>Leblebicioglu</keyname><forenames>Kemal</forenames></author></authors><title>Proportional Fair Resource Allocation on an Energy Harvesting Downlink -
  Part I: Structure</title><categories>cs.NI</categories><comments>29 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the allocation of time slots in a frame, as well as
power and rate to multiple receivers on an energy harvesting downlink. Energy
arrival times that will occur within the frame are known at the beginning of
the frame. The goal is to optimize throughput in a proportionally fair way,
taking into account the inherent differences of channel quality among users.
Analysis of structural characteristics of the problem reveals that it can be
formulated as a biconvex optimization problem, and that it has multiple optima.
Due to the biconvex nature of the problem, a Block Coordinate Descent (BCD)
based optimization algorithm that converges to an optimal solution is
presented. Numerical and simulation results show that the power-time
allocations found by BCD achieve a good balance between total throughput and
fairness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5148</identifier>
 <datestamp>2012-05-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5148</id><created>2012-05-23</created><authors><author><keyname>Fontein</keyname><forenames>Felix</forenames></author><author><keyname>Marshall</keyname><forenames>Kyle</forenames></author><author><keyname>Rosenthal</keyname><forenames>Joachim</forenames></author><author><keyname>Schipani</keyname><forenames>Davide</forenames></author><author><keyname>Trautmann</keyname><forenames>Anna-Lena</forenames></author></authors><title>On Burst Error Correction and Storage Security of Noisy Data</title><categories>cs.IT cs.CR math.IT</categories><comments>to be presented at MTNS 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Secure storage of noisy data for authentication purposes usually involves the
use of error correcting codes. We propose a new model scenario involving burst
errors and present for that several constructions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5153</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5153</id><created>2012-05-23</created><authors><author><keyname>Tekbiyik</keyname><forenames>Neyre</forenames></author><author><keyname>Girici</keyname><forenames>Tolga</forenames></author><author><keyname>Uysal-Biyikoglu</keyname><forenames>Elif</forenames></author><author><keyname>Leblebicioglu</keyname><forenames>Kemal</forenames></author></authors><title>Proportional Fair Resource Allocation on an Energy Harvesting Downlink -
  Part II: Algorithms</title><categories>cs.NI</categories><comments>25 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the proportionally fair allocation of time slots in a frame,
as well as power level to multiple receivers in an energy harvesting broadcast
system, is considered. Energy harvest times in a frame are assumed to be known
at the beginning of that frame. The goal is to solve an optimization problem
designed to maximize a throughput-based utility function that provides
proportional fairness among users. An optimal solution of the problem was
obtained by using a Block Coordinate Descent (BCD) method in earlier work
(presented in Part I of this study). However, finding the optimal allocation
entails a computational complexity that increases sharply in terms of the
number of users or slots. In this paper, certain structural characteristics of
the optimal power-time allocation policy are derived. Building on those, two
simple and computationally scalable heuristics, PTF and ProNTO are proposed.
Simulation results suggest that PTF and ProNTO can closely track the
performance of the BCD solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5162</identifier>
 <datestamp>2012-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5162</id><created>2012-05-23</created><updated>2012-06-06</updated><authors><author><keyname>Bose</keyname><forenames>Prosenjit</forenames></author><author><keyname>Cardinal</keyname><forenames>Jean</forenames></author><author><keyname>Collette</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Hurtado</keyname><forenames>Ferran</forenames></author><author><keyname>Korman</keyname><forenames>Matias</forenames></author><author><keyname>Langerman</keyname><forenames>Stefan</forenames></author><author><keyname>Taslakian</keyname><forenames>Perouz</forenames></author></authors><title>Coloring and Guarding Arrangements</title><categories>cs.CG</categories><comments>Abstract appeared in the proceedings of EuroCG 2012. Second version
  mentions the fact that the line arrangement must be simple (a fact that we
  unfortunately did not to mention in the first version)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an arrangement of lines in the plane, what is the minimum number $c$ of
colors required to color the lines so that no cell of the arrangement is
monochromatic? In this paper we give bounds on the number c both for the above
question, as well as some of its variations. We redefine these problems as
geometric hypergraph coloring problems. If we define $\Hlinecell$ as the
hypergraph where vertices are lines and edges represent cells of the
arrangement, the answer to the above question is equal to the chromatic number
of this hypergraph. We prove that this chromatic number is between $\Omega
(\log n / \log\log n)$. and $O(\sqrt{n})$.
  Similarly, we give bounds on the minimum size of a subset $S$ of the
intersections of the lines in $\mathcal{A}$ such that every cell is bounded by
at least one of the vertices in $S$. This may be seen as a problem on guarding
cells with vertices when the lines act as obstacles. The problem can also be
defined as the minimum vertex cover problem in the hypergraph $\Hvertexcell$,
the vertices of which are the line intersections, and the hyperedges are
vertices of a cell. Analogously, we consider the problem of touching the lines
with a minimum subset of the cells of the arrangement, which we identify as the
minimum vertex cover problem in the $\Hcellzone$ hypergraph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5164</identifier>
 <datestamp>2012-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5164</id><created>2012-05-23</created><updated>2012-10-16</updated><authors><author><keyname>Halldorsson</keyname><forenames>Magnus M.</forenames></author><author><keyname>Mitra</keyname><forenames>Pradipta</forenames></author></authors><title>Distributed Connectivity of Wireless Networks</title><categories>cs.NI cs.DS</categories><comments>17 pages, PODC'12, minor edits</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of constructing a communication infrastructure from
scratch, for a collection of identical wireless nodes. Combinatorially, this
means a) finding a set of links that form a strongly connected spanning graph
on a set of $n$ points in the plane, and b) scheduling it efficiently in the
SINR model of interference. The nodes must converge on a solution in a
distributed manner, having no means of communication beyond the sole wireless
channel.
  We give distributed connectivity algorithms that run in time $O(poly(\log
\Delta, \log n))$, where $\Delta$ is the ratio between the longest and shortest
distances among nodes. Given that algorithm without prior knowledge of the
instance are essentially limited to using uniform power, this is close to best
possible. Our primary aim, however, is to find efficient structures, measured
in the number of slots used in the final schedule of the links. Our main result
is algorithms that match the efficiency of centralized solutions. Specifically,
the networks can be scheduled in $O(\log n)$ slots using (arbitrary) power
control, and in $O(\log n (\log\log \Delta + \log n))$ slots using a simple
oblivious power scheme. Additionally, the networks have the desirable
properties that the latency of a converge-cast and of any node-to-node
communication is optimal $O(\log n)$ time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5177</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5177</id><created>2012-05-23</created><authors><author><keyname>Garc&#xed;a-Risue&#xf1;o</keyname><forenames>Pablo</forenames></author><author><keyname>Ib&#xe1;&#xf1;ez</keyname><forenames>Pablo E.</forenames></author></authors><title>A review of High Performance Computing foundations for scientists</title><categories>physics.comp-ph cs.DC</categories><comments>33 pages</comments><doi>10.1142/S0129183112300011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increase of existing computational capabilities has made simulation
emerge as a third discipline of Science, lying midway between experimental and
purely theoretical branches [1, 2]. Simulation enables the evaluation of
quantities which otherwise would not be accessible, helps to improve
experiments and provides new insights on systems which are analysed [3-6].
Knowing the fundamentals of computation can be very useful for scientists, for
it can help them to improve the performance of their theoretical models and
simulations. This review includes some technical essentials that can be useful
to this end, and it is devised as a complement for researchers whose education
is focused on scientific issues and not on technological respects. In this
document we attempt to discuss the fundamentals of High Performance Computing
(HPC) [7] in a way which is easy to understand without much previous
background. We sketch the way standard computers and supercomputers work, as
well as discuss distributed computing and discuss essential aspects to take
into account when running scientific calculations in computers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5190</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5190</id><created>2012-05-23</created><authors><author><keyname>Herzberg</keyname><forenames>Amir</forenames></author><author><keyname>Shulman</keyname><forenames>Haya</forenames></author></authors><title>Security of Patched DNS</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In spite of the availability of DNSSEC, which protects against cache
poisoning even by MitM attackers, many caching DNS resolvers still rely for
their security against poisoning on merely validating that DNS responses
contain some 'unpredictable' values, copied from the re- quest. These values
include the 16 bit identifier field, and other fields, randomised and validated
by different 'patches' to DNS. We investigate the prominent patches, and show
how attackers can circumvent all of them, namely: - We show how attackers can
circumvent source port randomisation, in the (common) case where the resolver
connects to the Internet via different NAT devices. - We show how attackers can
circumvent IP address randomisation, using some (standard-conforming)
resolvers. - We show how attackers can circumvent query randomisation,
including both randomisation by prepending a random nonce and case
randomisation (0x20 encoding). We present countermeasures preventing our
attacks; however, we believe that our attacks provide additional motivation for
adoption of DNSSEC (or other MitM-secure defenses).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5199</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5199</id><created>2012-05-23</created><updated>2013-06-23</updated><authors><author><keyname>Ganesan</keyname><forenames>Ashwin</forenames></author></authors><title>Automorphism groups of Cayley graphs generated by connected
  transposition sets</title><categories>cs.DM math.CO</categories><journal-ref>Discrete Mathematics, vol. 313, no. 21, pp. 2482-2485, November
  2013</journal-ref><doi>10.1016/j.disc.2013.07.013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $S$ be a set of transpositions that generates the symmetric group $S_n$,
where $n \ge 3$. The transposition graph $T(S)$ is defined to be the graph with
vertex set $\{1,\ldots,n\}$ and with vertices $i$ and $j$ being adjacent in
$T(S)$ whenever $(i,j) \in S$. We prove that if the girth of the transposition
graph $T(S)$ is at least 5, then the automorphism group of the Cayley graph
$\Cay(S_n,S)$ is the semidirect product $R(S_n) \rtimes \Aut(S_n,S)$, where
$\Aut(S_n,S)$ is the set of automorphisms of $S_n$ that fixes $S$. This
strengthens a result of Feng on transposition graphs that are trees. We also
prove that if the transposition graph $T(S)$ is a 4-cycle, then the set of
automorphisms of the Cayley graph $\Cay(S_4,S)$ that fixes a vertex and each of
its neighbors is isomorphic to the Klein 4-group and hence is nontrivial. We
thus identify the existence of 4-cycles in the transposition graph as being an
important factor in causing a potentially larger automorphism group of the
Cayley graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5204</identifier>
 <datestamp>2012-05-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5204</id><created>2012-05-23</created><authors><author><keyname>Jobard</keyname><forenames>Bruno</forenames></author><author><keyname>Ray</keyname><forenames>Nicolas</forenames></author><author><keyname>Sokolov</keyname><forenames>Dmitry</forenames></author></authors><title>Visualizing 2D Flows with Animated Arrow Plots</title><categories>cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Flow fields are often represented by a set of static arrows to illustrate
scientific vulgarization, documentary film, meteorology, etc. This simple
schematic representation lets an observer intuitively interpret the main
properties of a flow: its orientation and velocity magnitude. We propose to
generate dynamic versions of such representations for 2D unsteady flow fields.
Our algorithm smoothly animates arrows along the flow while controlling their
density in the domain over time. Several strategies have been combined to lower
the unavoidable popping artifacts arising when arrows appear and disappear and
to achieve visually pleasing animations. Disturbing arrow rotations in low
velocity regions are also handled by continuously morphing arrow glyphs to
semi-transparent discs. To substantiate our method, we provide results for
synthetic and real velocity field datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5224</identifier>
 <datestamp>2012-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5224</id><created>2012-05-23</created><updated>2012-05-31</updated><authors><author><keyname>D&#xf6;ttling</keyname><forenames>Nico</forenames></author><author><keyname>Dowsley</keyname><forenames>Rafael</forenames></author><author><keyname>M&#xfc;ller-Quade</keyname><forenames>J&#xf6;rn</forenames></author><author><keyname>Nascimento</keyname><forenames>Anderson C. A.</forenames></author></authors><title>A CCA2 Secure Variant of the McEliece Cryptosystem</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The McEliece public-key encryption scheme has become an interesting
alternative to cryptosystems based on number-theoretical problems. Differently
from RSA and ElGa- mal, McEliece PKC is not known to be broken by a quantum
computer. Moreover, even tough McEliece PKC has a relatively big key size,
encryption and decryption operations are rather efficient. In spite of all the
recent results in coding theory based cryptosystems, to the date, there are no
constructions secure against chosen ciphertext attacks in the standard model -
the de facto security notion for public-key cryptosystems. In this work, we
show the first construction of a McEliece based public-key cryptosystem secure
against chosen ciphertext attacks in the standard model. Our construction is
inspired by a recently proposed technique by Rosen and Segev.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5263</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5263</id><created>2012-05-23</created><updated>2014-07-31</updated><authors><author><keyname>Yu</keyname><forenames>Jingjin</forenames></author><author><keyname>Rus</keyname><forenames>Daniela</forenames></author></authors><title>Pebble Motion on Graphs with Rotations: Efficient Feasibility Tests and
  Planning Algorithms</title><categories>cs.DS cs.RO</categories><comments>WAFR submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of planning paths for $p$ distinguishable pebbles
(robots) residing on the vertices of an $n$-vertex connected graph with $p \le
n$. A pebble may move from a vertex to an adjacent one in a time step provided
that it does not collide with other pebbles. When $p = n$, the only collision
free moves are synchronous rotations of pebbles on disjoint cycles of the
graph. We show that the feasibility of such problems is intrinsically
determined by the diameter of a (unique) permutation group induced by the
underlying graph. Roughly speaking, the diameter of a group $\mathbf G$ is the
minimum length of the generator product required to reach an arbitrary element
of $\mathbf G$ from the identity element. Through bounding the diameter of this
associated permutation group, which assumes a maximum value of $O(n^2)$, we
establish a linear time algorithm for deciding the feasibility of such problems
and an $O(n^3)$ algorithm for planning complete paths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5282</identifier>
 <datestamp>2012-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5282</id><created>2012-05-23</created><authors><author><keyname>Ada</keyname><forenames>Anil</forenames></author><author><keyname>Fawzi</keyname><forenames>Omar</forenames></author><author><keyname>Hatami</keyname><forenames>Hamed</forenames></author></authors><title>Spectral Norm of Symmetric Functions</title><categories>cs.CC math.FA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The spectral norm of a Boolean function $f:\{0,1\}^n \to \{-1,1\}$ is the sum
of the absolute values of its Fourier coefficients. This quantity provides
useful upper and lower bounds on the complexity of a function in areas such as
learning theory, circuit complexity, and communication complexity. In this
paper, we give a combinatorial characterization for the spectral norm of
symmetric functions. We show that the logarithm of the spectral norm is of the
same order of magnitude as $r(f)\log(n/r(f))$ where $r(f) = \max\{r_0,r_1\}$,
and $r_0$ and $r_1$ are the smallest integers less than $n/2$ such that $f(x)$
or $f(x) \cdot parity(x)$ is constant for all $x$ with $\sum x_i \in [r_0,
n-r_1]$. We mention some applications to the decision tree and communication
complexity of symmetric functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5297</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5297</id><created>2012-05-23</created><updated>2012-07-06</updated><authors><author><keyname>Lima</keyname><forenames>F. W. S.</forenames></author><author><keyname>Moreira</keyname><forenames>Andr&#xe9; A.</forenames></author><author><keyname>Ara&#xfa;jo</keyname><forenames>Asc&#xe2;nio D.</forenames></author></authors><title>Non-nequilibrium model on Apollonian networks</title><categories>physics.soc-ph cs.SI</categories><comments>5 pages, 5 figures</comments><doi>10.1142/S0129183112500799</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the Majority-Vote Model with two states ($-1,+1$) and a noise
$q$ on Apollonian networks. The main result found here is the presence of the
phase transition as a function of the noise parameter $q$. We also studies de
effect of redirecting a fraction $p$ of the links of the network. By means of
Monte Carlo simulations, we obtained the exponent ratio $\gamma/\nu$,
$\beta/\nu$, and $1/\nu$ for several values of rewiring probability $p$. The
critical noise was determined $q_{c}$ and $U^{*}$ also was calculated. The
effective dimensionality of the system was observed to be independent on $p$,
and the value $D_{eff} \approx1.0$ is observed for these networks. Previous
results on the Ising model in Apollonian Networks have reported no presence of
a phase transition. Therefore, the results present here demonstrate that the
Majority-Vote Model belongs to a different universality class as the
equilibrium Ising Model on Apollonian Network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5324</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5324</id><created>2012-05-23</created><updated>2013-12-08</updated><authors><author><keyname>Sung</keyname><forenames>Chi Wan</forenames></author><author><keyname>Huang</keyname><forenames>Linyu</forenames></author><author><keyname>Kwan</keyname><forenames>Ho Yuet</forenames></author><author><keyname>Shum</keyname><forenames>Kenneth W.</forenames></author></authors><title>Linear Network Code for Erasure Broadcast Channel with Feedback:
  Complexity and Algorithms</title><categories>cs.IT cs.CC math.IT</categories><comments>18 pages, 11 figures, submitted to IEEE Trans. Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the construction of linear network codes for
broadcasting a set of data packets to a number of users. The links from the
source to the users are modeled as independent erasure channels. Users are
allowed to inform the source node whether a packet is received correctly via
feedback channels. In order to minimize the number of packet transmissions
until all users have received all packets successfully, it is necessary that a
data packet, if successfully received by a user, can increase the dimension of
the vector space spanned by the encoding vectors he or she has received by one.
Such an encoding vector is called innovative. We prove that innovative linear
network code is uniformly optimal in minimizing user download delay. When the
finite field size is strictly smaller than the number of users, the problem of
determining the existence of innovative vectors is proven to be NP-complete.
When the field size is larger than or equal to the number of users, innovative
vectors always exist and random linear network code (RLNC) is able to find an
innovative vector with high probability. While RLNC is optimal in terms of
completion time, it has high decoding complexity due to the need of solving a
system of linear equations. To reduce decoding time, we propose the use of
sparse linear network code, since the sparsity property of encoding vectors can
be exploited when solving systems of linear equations. Generating a sparsest
encoding vector with large finite field size, however, is shown to be NP-hard.
An approximation algorithm that guarantee the Hamming weight of a generated
encoding vector to be smaller than a certain factor of the optimal value is
constructed. Our simulation results show that our proposed methods have
excellent performance in completion time and outperforms RLNC in terms of
decoding time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5341</identifier>
 <datestamp>2012-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5341</id><created>2012-05-24</created><authors><author><keyname>Min</keyname><forenames>Rui</forenames></author><author><keyname>Wu</keyname><forenames>Yik-Chung</forenames></author></authors><title>Joint Channel Estimation and Data Detection for Multihop OFDM Relaying
  System under Unknown Channel Orders and Doppler Frequencies</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, channel estimation and data detection for multihop relaying
orthogonal frequency division multiplexing (OFDM) system is investigated under
time-varying channel. Different from previous works, which highly depend on the
statistical information of the doubly-selective channel (DSC) and noise to
deliver accurate channel estimation and data detection results, we focus on
more practical scenarios with unknown channel orders and Doppler frequencies.
Firstly, we integrate the multilink, multihop channel matrices into one
composite channel matrix. Then, we formulate the unknown channel using
generalized complex exponential basis expansion model (GCE-BEM) with a large
oversampling factor to introduce channel sparsity on delay-Doppler domain. To
enable the identification of nonzero entries, sparsity enhancing Gaussian
distributions with Gamma hyperpriors are adopted. An iterative algorithm is
developed under variational inference (VI) framework. The proposed algorithm
iteratively estimate the channel, recover the unknown data using Viterbi
algorithm and learn the channel and noise statistical information, using only
limited number of pilot subcarrier in one OFDM symbol. Simulation results show
that, without any statistical information, the performance of the proposed
algorithm is very close to that of the optimal channel estimation and data
detection algorithm, which requires specific information on system structure,
channel tap positions, channel lengths, Doppler shifts as well as noise powers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5344</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5344</id><created>2012-05-24</created><updated>2015-12-23</updated><authors><author><keyname>Gay</keyname><forenames>Simon J.</forenames><affiliation>University of Glasgow</affiliation></author><author><keyname>Gesbert</keyname><forenames>Nils</forenames><affiliation>Grenoble INP - Ensimag</affiliation></author><author><keyname>Ravara</keyname><forenames>Ant&#xf3;nio</forenames><affiliation>Universidade Nova de Lisboa</affiliation></author><author><keyname>Vasconcelos</keyname><forenames>Vasco T.</forenames><affiliation>Universidade de Lisboa</affiliation></author></authors><title>Modular session types for objects</title><categories>cs.PL</categories><comments>Logical Methods in Computer Science (LMCS), International Federation
  for Computational Logic, 2015</comments><proxy>LMCS</proxy><journal-ref>LMCS 11 (4:12) 2015</journal-ref><doi>10.2168/LMCS-11(4:12)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Session types allow communication protocols to be specified
type-theoretically so that protocol implementations can be verified by static
type checking. We extend previous work on session types for distributed
object-oriented languages in three ways. (1) We attach a session type to a
class definition, to specify the possible sequences of method calls. (2) We
allow a session type (protocol) implementation to be modularized, i.e.
partitioned into separately-callable methods. (3) We treat session-typed
communication channels as objects, integrating their session types with the
session types of classes. The result is an elegant unification of communication
channels and their session types, distributed object-oriented programming, and
a form of typestate supporting non-uniform objects, i.e. objects that
dynamically change the set of available methods. We define syntax, operational
se-mantics, a sound type system, and a sound and complete type checking
algorithm for a small distributed class-based object-oriented language with
structural subtyping. Static typing guarantees that both sequences of messages
on channels, and sequences of method calls on objects, conform to
type-theoretic specifications, thus ensuring type-safety. The language includes
expected features of session types, such as delegation, and expected features
of object-oriented programming, such as encapsulation of local state.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5351</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5351</id><created>2012-05-24</created><updated>2013-01-29</updated><authors><author><keyname>Ren</keyname><forenames>Xiang</forenames></author><author><keyname>Lin</keyname><forenames>Zhouchen</forenames></author></authors><title>Linearized Alternating Direction Method with Adaptive Penalty and Warm
  Starts for Fast Solving Transform Invariant Low-Rank Textures</title><categories>cs.CV</categories><comments>Accepted by International Journal of Computer Vision (IJCV)</comments><doi>10.1007/s11263-013-0611-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transform Invariant Low-rank Textures (TILT) is a novel and powerful tool
that can effectively rectify a rich class of low-rank textures in 3D scenes
from 2D images despite significant deformation and corruption. The existing
algorithm for solving TILT is based on the alternating direction method (ADM).
It suffers from high computational cost and is not theoretically guaranteed to
converge to a correct solution. In this paper, we propose a novel algorithm to
speed up solving TILT, with guaranteed convergence. Our method is based on the
recently proposed linearized alternating direction method with adaptive penalty
(LADMAP). To further reduce computation, warm starts are also introduced to
initialize the variables better and cut the cost on singular value
decomposition. Extensive experimental results on both synthetic and real data
demonstrate that this new algorithm works much more efficiently and robustly
than the existing algorithm. It could be at least five times faster than the
previous method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5353</identifier>
 <datestamp>2012-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5353</id><created>2012-05-24</created><authors><author><keyname>Jain</keyname><forenames>Ravindra</forenames></author></authors><title>A hybrid clustering algorithm for data mining</title><categories>cs.DB cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data clustering is a process of arranging similar data into groups. A
clustering algorithm partitions a data set into several groups such that the
similarity within a group is better than among groups. In this paper a hybrid
clustering algorithm based on K-mean and K-harmonic mean (KHM) is described.
The proposed algorithm is tested on five different datasets. The research is
focused on fast and accurate clustering. Its performance is compared with the
traditional K-means &amp; KHM algorithm. The result obtained from proposed hybrid
algorithm is much better than the traditional K-mean &amp; KHM algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5367</identifier>
 <datestamp>2012-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5367</id><created>2012-05-24</created><authors><author><keyname>Taranto</keyname><forenames>Claudio</forenames></author><author><keyname>Di Mauro</keyname><forenames>Nicola</forenames></author><author><keyname>Esposito</keyname><forenames>Floriana</forenames></author></authors><title>Language-Constraint Reachability Learning in Probabilistic Graphs</title><categories>cs.AI cs.LG</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The probabilistic graphs framework models the uncertainty inherent in
real-world domains by means of probabilistic edges whose value quantifies the
likelihood of the edge existence or the strength of the link it represents. The
goal of this paper is to provide a learning method to compute the most likely
relationship between two nodes in a framework based on probabilistic graphs. In
particular, given a probabilistic graph we adopted the language-constraint
reachability method to compute the probability of possible interconnections
that may exists between two nodes. Each of these connections may be viewed as
feature, or a factor, between the two nodes and the corresponding probability
as its weight. Each observed link is considered as a positive instance for its
corresponding link label. Given the training set of observed links a
L2-regularized Logistic Regression has been adopted to learn a model able to
predict unobserved link labels. The experiments on a real world collaborative
filtering problem proved that the proposed approach achieves better results
than that obtained adopting classical methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5373</identifier>
 <datestamp>2012-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5373</id><created>2012-05-24</created><updated>2012-07-23</updated><authors><author><keyname>Liao</keyname><forenames>Gang</forenames></author><author><keyname>Liu</keyname><forenames>Lei</forenames></author><author><keyname>Luo</keyname><forenames>Lian</forenames></author></authors><title>An Adaptive XP-based approach to Agile Development</title><categories>cs.SE</categories><comments>15 pages,6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software design is gradually becoming open, distributed, pervasive, and
connected. It is a sad statistical fact that software projects are
scientifically fragile and tend to fail more than other engineering fields.
Agile development is a philosophy. And agile methods are processes that support
the agile philosophy. XP places a strong emphasis on technical practices in
addition to the more common teamwork and structural practices. In this paper,
we elaborate how XP practices can be used to thinking, collaborating,
releasing, planning, developing. And the state that make your team and
organization more successful.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5375</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5375</id><created>2012-05-24</created><updated>2012-05-31</updated><authors><author><keyname>Wang</keyname><forenames>Kehao</forenames></author><author><keyname>Chen</keyname><forenames>Lin</forenames></author><author><keyname>Liu</keyname><forenames>Quan</forenames></author><author><keyname>Agha</keyname><forenames>Khaldoun Al</forenames></author></authors><title>On Optimality of Myopic Policy for Restless Multi-armed Bandit Problem
  with Non i.i.d. Arms and Imperfect Detection</title><categories>cs.SY cs.GT</categories><comments>Second version, 16 pages</comments><msc-class>94A05</msc-class><acm-class>C.2.1; G.1.6</acm-class><doi>10.1109/TSP.2011.2170684</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the channel access problem in a multi-channel opportunistic
communication system with imperfect channel sensing, where the state of each
channel evolves as a non independent and identically distributed Markov
process. This problem can be cast into a restless multi-armed bandit (RMAB)
problem that is intractable for its exponential computation complexity. A
natural alternative is to consider the easily implementable myopic policy that
maximizes the immediate reward but ignores the impact of the current strategy
on the future reward. In particular, we develop three axioms characterizing a
family of generic and practically important functions termed as $g$-regular
functions which includes a wide spectrum of utility functions in engineering.
By pursuing a mathematical analysis based on the axioms, we establish a set of
closed-form structural conditions for the optimality of myopic policy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5395</identifier>
 <datestamp>2012-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5395</id><created>2012-05-24</created><authors><author><keyname>Yakaryilmaz</keyname><forenames>Abuzer</forenames></author></authors><title>Turing-equivalent automata using a fixed-size quantum memory</title><categories>cs.CC quant-ph</categories><comments>28 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a new public quantum interactive proof system and
the first quantum alternating Turing machine: qAM proof system and qATM,
respectively. Both are obtained from their classical counterparts
(Arthur-Merlin proof system and alternating Turing machine, respectively,) by
augmenting them with a fixed-size quantum register. We focus on space-bounded
computation, and obtain the following surprising results: Both of them with
constant-space are Turing-equivalent. More specifically, we show that for any
Turing-recognizable language, there exists a constant-space weak-qAM system,
(the nonmembers do not need to be rejected with high probability), and we show
that any Turing-recognizable language can be recognized by a constant-space
qATM even with one-way input head.
  For strong proof systems, where the nonmembers must be rejected with high
probability, we show that the known space-bounded classical private protocols
can also be simulated by our public qAM system with the same space bound.
Besides, we introduce a strong version of qATM: The qATM that must halt in
every computation path. Then, we show that strong qATMs (similar to private
ATMs) can simulate deterministic space with exponentially less space. This
leads to shifting the deterministic space hierarchy exactly by one-level. The
method behind the main results is a new public protocol cleverly using its
fixed-size quantum register. Interestingly, the quantum part of this public
protocol cannot be simulated by any space-bounded classical protocol in some
cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5407</identifier>
 <datestamp>2012-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5407</id><created>2012-05-24</created><updated>2012-09-01</updated><authors><author><keyname>Yuret</keyname><forenames>Deniz</forenames></author></authors><title>FASTSUBS: An Efficient and Exact Procedure for Finding the Most Likely
  Lexical Substitutes Based on an N-gram Language Model</title><categories>cs.CL</categories><comments>4 pages, 1 figure, to appear in IEEE Signal Processing Letters</comments><doi>10.1109/LSP.2012.2215587</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lexical substitutes have found use in areas such as paraphrasing, text
simplification, machine translation, word sense disambiguation, and part of
speech induction. However the computational complexity of accurately
identifying the most likely substitutes for a word has made large scale
experiments difficult. In this paper I introduce a new search algorithm,
FASTSUBS, that is guaranteed to find the K most likely lexical substitutes for
a given word in a sentence based on an n-gram language model. The computation
is sub-linear in both K and the vocabulary size V. An implementation of the
algorithm and a dataset with the top 100 substitutes of each token in the WSJ
section of the Penn Treebank are available at http://goo.gl/jzKH0.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5425</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5425</id><created>2012-05-24</created><authors><author><keyname>Darkner</keyname><forenames>Sune</forenames></author><author><keyname>Sporring</keyname><forenames>Jon</forenames></author></authors><title>Locally Orderless Registration</title><categories>cs.CV</categories><comments>submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image registration is an important tool for medical image analysis and is
used to bring images into the same reference frame by warping the coordinate
field of one image, such that some similarity measure is minimized. We study
similarity in image registration in the context of Locally Orderless Images
(LOI), which is the natural way to study density estimates and reveals the 3
fundamental scales: the measurement scale, the intensity scale, and the
integration scale.
  This paper has three main contributions: Firstly, we rephrase a large set of
popular similarity measures into a common framework, which we refer to as
Locally Orderless Registration, and which makes full use of the features of
local histograms. Secondly, we extend the theoretical understanding of the
local histograms. Thirdly, we use our framework to compare two state-of-the-art
intensity density estimators for image registration: The Parzen Window (PW) and
the Generalized Partial Volume (GPV), and we demonstrate their differences on a
popular similarity measure, Normalized Mutual Information (NMI).
  We conclude, that complicated similarity measures such as NMI may be
evaluated almost as fast as simple measures such as Sum of Squared Distances
(SSD) regardless of the choice of PW and GPV. Also, GPV is an asymmetric
measure, and PW is our preferred choice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5434</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5434</id><created>2012-05-24</created><updated>2012-09-23</updated><authors><author><keyname>Hemmer</keyname><forenames>Michael</forenames></author><author><keyname>Kleinbort</keyname><forenames>Michal</forenames></author><author><keyname>Halperin</keyname><forenames>Dan</forenames></author></authors><title>Improved Implementation of Point Location in General Two-Dimensional
  Subdivisions</title><categories>cs.CG cs.DS</categories><comments>21 pages</comments><doi>10.1007/978-3-642-33090-2_53</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a major revamp of the point-location data structure for general
two-dimensional subdivisions via randomized incremental construction,
implemented in CGAL, the Computational Geometry Algorithms Library. We can now
guarantee that the constructed directed acyclic graph G is of linear size and
provides logarithmic query time. Via the construction of the Voronoi diagram
for a given point set S of size n, this also enables nearest-neighbor queries
in guaranteed O(log n) time. Another major innovation is the support of general
unbounded subdivisions as well as subdivisions of two-dimensional parametric
surfaces such as spheres, tori, cylinders. The implementation is exact,
complete, and general, i.e., it can also handle non-linear subdivisions. Like
the previous version, the data structure supports modifications of the
subdivision, such as insertions and deletions of edges, after the initial
preprocessing. A major challenge is to retain the expected O(n log n)
preprocessing time while providing the above (deterministic) space and
query-time guarantees. We describe an efficient preprocessing algorithm, which
explicitly verifies the length L of the longest query path in O(n log n) time.
However, instead of using L, our implementation is based on the depth D of G.
Although we prove that the worst case ratio of D and L is Theta(n/log n), we
conjecture, based on our experimental results, that this solution achieves
expected O(n log n) preprocessing time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5443</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5443</id><created>2012-05-24</created><updated>2013-05-16</updated><authors><author><keyname>Kim</keyname><forenames>Donggun</forenames></author><author><keyname>Seo</keyname><forenames>Junyeong</forenames></author><author><keyname>Sung</keyname><forenames>Youngchul</forenames></author></authors><title>Filter-and-Forward Transparent Relay Design for OFDM Systems</title><categories>cs.IT math.IT</categories><comments>Changes from the 1st version: Rate maximization and practical issues
  such as channel state information acquisition are considered in the new
  version. To appear in IEEE Trans. Vehicular Technology</comments><acm-class>E.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the filter-and-forward (FF) relay design for orthogonal
frequency-division multiplexing (OFDM) transmission systems is considered to
improve the system performance over simple amplify-and-forward (AF) relaying.
Unlike conventional OFDM relays performing OFDM demodulation and remodulation,
to reduce processing complexity, the proposed FF relay directly filters the
incoming signal in time domain with a finite impulse response (FIR) and
forwards the filtered signal to the destination. Three design criteria are
considered to optimize the relay filter. The first criterion is the
minimization of the relay transmit power subject to per-subcarrier
signal-to-noise ratio (SNR) constraints, the second is the maximization of the
worst subcarrier channel SNR subject to source and relay transmit power
constraints, and the third is the maximization of data rate subject to source
and relay transmit power constraints. It is shown that the first problem
reduces to a semi-definite programming (SDP) problem by semi-definite
relaxation and the solution to the relaxed SDP problem has rank one under a
mild condition. For the latter two problems, the problem of joint source power
allocation and relay filter design is considered and an efficient algorithm is
proposed for each problem based on alternating optimization and the projected
gradient method (PGM). Numerical results show that the proposed FF relay
significantly outperforms simple AF relays with insignificant increase in
complexity. Thus, the proposed FF relay provides a practical alternative to the
AF relaying scheme for OFDM transmission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5465</identifier>
 <datestamp>2014-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5465</id><created>2012-05-24</created><authors><author><keyname>Trautmann</keyname><forenames>Anna-Lena</forenames></author></authors><title>Isometry and Automorphisms of Constant Dimension Codes</title><categories>cs.IT math.IT</categories><journal-ref>Advances in Mathematics of Communications (AMC), volume 7 no. 2,
  pages 147--160, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define linear and semilinear isometry for general subspace codes, used for
random network coding. Furthermore, some results on isometry classes and
automorphism groups of known constant dimension code constructions are derived.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5487</identifier>
 <datestamp>2013-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5487</id><created>2012-05-24</created><updated>2013-01-09</updated><authors><author><keyname>Astsatryan</keyname><forenames>H. V.</forenames></author><author><keyname>Gevorgyan</keyname><forenames>T. V.</forenames></author><author><keyname>Shahinyan</keyname><forenames>A. R.</forenames></author></authors><title>Web Portal for Photonic Technologies Using Grid Infrastructures</title><categories>physics.comp-ph cs.DC quant-ph</categories><journal-ref>JSEA, Vol.5 No.11, PP. 864-869, 2012</journal-ref><doi>10.4236/jsea.2012.511100</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The modeling of physical processes is an integral part of scientific and
technical research. In this area, the Extendible C++ Application in Quantum
Technologies (ECAQT) package provides the numerical simulations and modeling of
complex quantum systems in the presence of decoherence with wide applications
in photonics. It allows creating models of interacting complex systems and
simulates their time evolution with a number of available time-evolution
drivers. Physical simulations involving massive amounts of calculations are
often executed on distributed computing infrastructures. It is often difficult
for non expert users to use such computational infrastructures or even to use
advanced libraries over the infrastructures, because they often require being
familiar with middleware and tools, parallel programming techniques and
packages. The P-RADE Grid Portal is a Grid portal solution that allows users to
manage the whole life-cycle for executing a parallel application on the
computing Grid infrastructures. The article describes the functionality and the
structure of the web portal based on ECAQT package.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5504</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5504</id><created>2012-05-24</created><updated>2015-12-31</updated><authors><author><keyname>Takahashi</keyname><forenames>Hayato</forenames></author></authors><title>Algorithmic randomness and stochastic selection function</title><categories>cs.IT math.IT</categories><comments>submitted to CCR2012 special issue. arXiv admin note: text overlap
  with arXiv:1106.3153</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show algorithmic randomness versions of the two classical theorems on
subsequences of normal numbers. One is Kamae-Weiss theorem (Kamae 1973) on
normal numbers, which characterize the selection function that preserves normal
numbers. Another one is the Steinhaus (1922) theorem on normal numbers, which
characterize the normality from their subsequences. In van Lambalgen (1987), an
algorithmic analogy to Kamae-Weiss theorem is conjectured in terms of
algorithmic randomness and complexity. In this paper we consider two types of
algorithmic random sequence; one is ML-random sequences and the other one is
the set of sequences that have maximal complexity rate. Then we show
algorithmic randomness versions of corresponding theorems to the above
classical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5509</identifier>
 <datestamp>2012-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5509</id><created>2012-05-24</created><authors><author><keyname>Boldi</keyname><forenames>Paolo</forenames></author><author><keyname>Vigna</keyname><forenames>Sebastiano</forenames></author></authors><title>Four Degrees of Separation, Really</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We recently measured the average distance of users in the Facebook graph,
spurring comments in the scientific community as well as in the general press
(&quot;Four Degrees of Separation&quot;). A number of interesting criticisms have been
made about the meaningfulness, methods and consequences of the experiment we
performed. In this paper we want to discuss some methodological aspects that we
deem important to underline in the form of answers to the questions we have
read in newspapers, magazines, blogs, or heard from colleagues. We indulge in
some reflections on the actual meaning of &quot;average distance&quot; and make a number
of side observations showing that, yes, 3.74 &quot;degrees of separation&quot; are really
few.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5522</identifier>
 <datestamp>2012-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5522</id><created>2012-05-24</created><authors><author><keyname>Koch</keyname><forenames>Tobias</forenames></author><author><keyname>Martinez</keyname><forenames>Alfonso</forenames></author><author><keyname>F&#xe0;bregas</keyname><forenames>Albert Guill&#xe9;n i</forenames></author></authors><title>The Capacity Loss of Dense Constellations</title><categories>cs.IT math.IT</categories><comments>To appear in the Proceedings of the 2012 IEEE International Symposium
  on Information Theory (ISIT). Corrected a minor error in Figure 1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We determine the loss in capacity incurred by using signal constellations
with a bounded support over general complex-valued additive-noise channels for
suitably high signal-to-noise ratio. Our expression for the capacity loss
recovers the power loss of 1.53dB for square signal constellations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5525</identifier>
 <datestamp>2012-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5525</id><created>2012-05-24</created><authors><author><keyname>Sarma</keyname><forenames>Atish Das</forenames></author><author><keyname>Molla</keyname><forenames>Anisur Rahaman</forenames></author><author><keyname>Pandurangan</keyname><forenames>Gopal</forenames></author></authors><title>Fast Distributed Computation in Dynamic Networks via Random Walks</title><categories>cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper investigates efficient distributed computation in dynamic networks
in which the network topology changes (arbitrarily) from round to round.
  Our first contribution is a rigorous framework for design and analysis of
distributed random walk algorithms in dynamic networks. We then develop a fast
distributed random walk based algorithm that runs in $\tilde{O}(\sqrt{\tau
\Phi})$ rounds (with high probability), where $\tau$ is the dynamic mixing time
and $\Phi$ is the dynamic diameter of the network respectively, and returns a
sample close to a suitably defined stationary distribution of the dynamic
network. We also apply our fast random walk algorithm to devise fast
distributed algorithms for two key problems, namely, information dissemination
and decentralized computation of spectral properties in a dynamic network.
  Our next contribution is a fast distributed algorithm for the fundamental
problem of information dissemination (also called as gossip) in a dynamic
network. In gossip, or more generally, $k$-gossip, there are $k$ pieces of
information (or tokens) that are initially present in some nodes and the
problem is to disseminate the $k$ tokens to all nodes. We present a random-walk
based algorithm that runs in $\tilde{O}(\min\{n^{1/3}k^{2/3}(\tau \Phi)^{1/3},
nk\})$ rounds (with high probability). To the best of our knowledge, this is
the first $o(nk)$-time fully-distributed token forwarding algorithm that
improves over the previous-best $O(nk)$ round distributed algorithm [Kuhn et
al., STOC 2010], although in an oblivious adversary model.
  Our final contribution is a simple and fast distributed algorithm for
estimating the dynamic mixing time and related spectral properties of the
underlying dynamic network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5569</identifier>
 <datestamp>2012-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5569</id><created>2012-05-24</created><updated>2012-06-01</updated><authors><author><keyname>Gorla</keyname><forenames>Jagadeesh</forenames></author><author><keyname>Robertson</keyname><forenames>Stephen</forenames></author><author><keyname>Wang</keyname><forenames>Jun</forenames></author><author><keyname>Jambor</keyname><forenames>Tamas</forenames></author></authors><title>A Theory of Information Matching</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose a theory for information matching. It is motivated
by the observation that retrieval is about the relevance matching between two
sets of properties (features), namely, the information need representation and
information item representation. However, many probabilistic retrieval models
rely on fixing one representation and optimizing the other (e.g. fixing the
single information need and tuning the document) but not both. Therefore, it is
difficult to use the available related information on both the document and the
query at the same time in calculating the probability of relevance. In this
paper, we address the problem by hypothesizing the relevance as a logical
relationship between the two sets of properties; the relationship is defined on
two separate mappings between these properties. By using the hypothesis we
develop a unified probabilistic relevance model which is capable of using all
the available information. We validate the proposed theory by formulating and
developing probabilistic relevance ranking functions for both ad-hoc text
retrieval and collaborative filtering. Our derivation in text retrieval
illustrates the use of the theory in the situation where no relevance
information is available. In collaborative filtering, we show that the
resulting recommender model unifies the user and item information into a
relevance ranking function without applying any dimensionality reduction
techniques or computing explicit similarity between two different users (or
items), in contrast to the state-of-the-art recommender models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5571</identifier>
 <datestamp>2012-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5571</id><created>2012-05-23</created><authors><author><keyname>Ratschan</keyname><forenames>Stefan</forenames></author></authors><title>Applications of Quantified Constraint Solving over the Reals -
  Bibliography</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantified constraints over the reals appear in numerous contexts. Usually
existential quantification occurs when some parameter can be chosen by the user
of a system, and univeral quantification when the exact value of a parameter is
either unknown, or when it occurs in infinitely many, similar versions. The
following is a list of application areas and publications that contain
applications for solving quantified constraints over the reals. The list is
certainly not complete, but grows as the author encounters new items.
Contributions are very welcome!
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5589</identifier>
 <datestamp>2012-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5589</id><created>2012-05-24</created><authors><author><keyname>Qi</keyname><forenames>Hanchao</forenames></author><author><keyname>Hughes</keyname><forenames>Shannon M.</forenames></author></authors><title>Technical report: Two observations on probability distribution
  symmetries for randomly-projected data</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this technical report, we will make two observations concerning symmetries
of the probability distribution resulting from projection of a piece of
p-dimensional data onto a random m-dimensional subspace of $\mathbb{R}^p$,
where m &lt; p. In particular, we shall observe that such distributions are
unchanged by reflection across the original data vector and by rotation about
the original data vector
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5602</identifier>
 <datestamp>2012-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5602</id><created>2012-05-24</created><authors><author><keyname>Ong</keyname><forenames>Lawrence</forenames></author><author><keyname>Johnson</keyname><forenames>Sarah J.</forenames></author></authors><title>The Capacity Region of Restricted Multi-Way Relay Channels with
  Deterministic Uplinks</title><categories>cs.IT math.IT</categories><comments>Author's final version (to be presented at ISIT 2012)</comments><journal-ref>Proceedings of the 2012 IEEE International Symposium on
  Information Theory (ISIT 2012), (Cambridge, USA, July 1-6, 2012) pp. 786-790</journal-ref><doi>10.1109/ISIT.2012.6284667</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the multi-way relay channel (MWRC) where multiple users
exchange messages via a single relay. The capacity region is derived for a
special class of MWRCs where (i) the uplink and the downlink are separated in
the sense that there is no direct user-to-user links, (ii) the channel is
restricted in the sense that each user's transmitted channel symbols can depend
on only its own message, but not on its received channel symbols, and (iii) the
uplink is any deterministic function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5603</identifier>
 <datestamp>2012-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5603</id><created>2012-05-24</created><authors><author><keyname>Ong</keyname><forenames>Lawrence</forenames></author><author><keyname>Timo</keyname><forenames>Roy</forenames></author><author><keyname>Johnson</keyname><forenames>Sarah J.</forenames></author></authors><title>The Finite Field Multi-Way Relay Channel with Correlated Sources: Beyond
  Three Users</title><categories>cs.IT math.IT</categories><comments>Author's final version (to be presented at ISIT 2012)</comments><journal-ref>Proceedings of the 2012 IEEE International Symposium on
  Information Theory (ISIT 2012), (Cambridge, USA, July 1-6, 2012) pp. 791-795</journal-ref><doi>10.1109/ISIT.2012.6284668</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The multi-way relay channel (MWRC) models cooperative communication networks
in which many users exchange messages via a relay. In this paper, we consider
the finite field MWRC with correlated messages. The problem is to find all
achievable rates, defined as the number of channel uses required per reliable
exchange of message tuple. For the case of three users, we have previously
established that for a special class of source distributions, the set of all
achievable rates can be found [Ong et al., ISIT 2010]. The class is specified
by an almost balanced conditional mutual information (ABCMI) condition. In this
paper, we first generalize the ABCMI condition to the case of more than three
users. We then show that if the sources satisfy the ABCMI condition, then the
set of all achievable rates is found and can be attained using a separate
source-channel coding architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5611</identifier>
 <datestamp>2012-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5611</id><created>2012-05-25</created><authors><author><keyname>Bar-Ilan</keyname><forenames>Judit</forenames><affiliation>Department of Information Science, Bar-Ilan University</affiliation></author><author><keyname>Haustein</keyname><forenames>Stefanie</forenames><affiliation>Central Library, Forschungszentrum J&#xfc;lich</affiliation></author><author><keyname>Peters</keyname><forenames>Isabella</forenames><affiliation>Department of Information Science, Heinrich-Heine-University</affiliation></author><author><keyname>Priem</keyname><forenames>Jason</forenames><affiliation>School of Information and Library Science, University of North Carolina-Chapel Hill</affiliation></author><author><keyname>Shema</keyname><forenames>Hadas</forenames><affiliation>Department of Information Science, Bar-Ilan University</affiliation></author><author><keyname>Terliesner</keyname><forenames>Jens</forenames><affiliation>Department of Information Science, Heinrich-Heine-University</affiliation></author></authors><title>Beyond citations: Scholars' visibility on the social Web</title><categories>cs.DL cs.SI physics.soc-ph</categories><comments>Accepted to 17th International Conference on Science and Technology
  Indicators, Montreal, Canada, 5-8 Sept. 2012. 14 pages, 3 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditionally, scholarly impact and visibility have been measured by counting
publications and citations in the scholarly literature. However, increasingly
scholars are also visible on the Web, establishing presences in a growing
variety of social ecosystems. But how wide and established is this presence,
and how do measures of social Web impact relate to their more traditional
counterparts? To answer this, we sampled 57 presenters from the 2010 Leiden STI
Conference, gathering publication and citations counts as well as data from the
presenters' Web &quot;footprints.&quot; We found Web presence widespread and diverse: 84%
of scholars had homepages, 70% were on LinkedIn, 23% had public Google Scholar
profiles, and 16% were on Twitter. For sampled scholars' publications, social
reference manager bookmarks were compared to Scopus and Web of Science
citations; we found that Mendeley covers more than 80% of sampled articles, and
that Mendeley bookmarks are significantly correlated (r=.45) to Scopus citation
counts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5613</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5613</id><created>2012-05-25</created><updated>2012-06-04</updated><authors><author><keyname>Babinkostova</keyname><forenames>Liljana</forenames></author><author><keyname>Bowden</keyname><forenames>Alyssa M.</forenames></author><author><keyname>Kimball</keyname><forenames>Andrew M.</forenames></author><author><keyname>Williams</keyname><forenames>Kameryn J.</forenames></author></authors><title>A simplified and generalized treatment of DES related ciphers</title><categories>math.GR cs.CR</categories><comments>24 pages, 2 figures</comments><msc-class>20B05, 20B30, 94A60, 11T71, 14G50</msc-class><acm-class>E.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work is a study of DES-like ciphers where the bitwise exclusive-or (XOR)
operation in the underlying Feistel network is replaced by an arbitrary group
operation. We construct a two round simplified version of DES that contains all
the DES components and show that its set of encryption permutations is not a
group under functional composition, it is not a pure cipher and its set of
encryption permutations does not generate the alternating group. We present a
non-computational proof that for n\leq6 the set of n-round Feistel permutations
over an arbitrary group do not constitute a group under functional composition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5614</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5614</id><created>2012-05-25</created><authors><author><keyname>Zhong</keyname><forenames>Caijun</forenames></author><author><keyname>Ratnarajah</keyname><forenames>Tharm</forenames></author><author><keyname>Jin</keyname><forenames>Shi</forenames></author><author><keyname>Wong</keyname><forenames>Kai Kit</forenames></author></authors><title>Performance Analysis of Optimal Single Stream Beamforming in MIMO
  Dual-Hop AF Systems</title><categories>cs.IT math.IT</categories><comments>to appear in IEEE Journal on Selected Areas in Communications special
  issue on Theories and Methods for Advanced Wireless Relays</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the performance of optimal single stream beamforming
schemes in multiple-input multiple-output (MIMO) dual-hop amplify-and-forward
(AF) systems. Assuming channel state information is not available at the source
and relay, the optimal transmit and receive beamforming vectors are computed at
the destination, and the transmit beamforming vector is sent to the transmitter
via a dedicated feedback link. Then, a set of new closed-form expressions for
the statistical properties of the maximum eigenvalue of the resultant channel
is derived, i.e., the cumulative density function (cdf), probability density
function (pdf) and general moments, as well as the first order asymptotic
expansion and asymptotic large dimension approximations. These analytical
expressions are then applied to study three important performance metrics of
the system, i.e., outage probability, average symbol error rate and ergodic
capacity. In addition, more detailed treatments are provided for some important
special cases, e.g., when the number of antennas at one of the nodes is one or
large, simple and insightful expressions for the key parameters such as
diversity order and array gain of the system are derived. With the analytical
results, the joint impact of source, relay and destination antenna numbers on
the system performance is addressed, and the performance of optimal beamforming
schemes and orthogonal space-time block-coding (OSTBC) schemes are compared.
Results reveal that the number of antennas at the relay has a great impact on
how the numbers of antennas at the source and destination contribute to the
system performance, and optimal beamforming not only achieves the same maximum
diversity order as OSTBC, but also provides significant power gains over OSTBC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5615</identifier>
 <datestamp>2012-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5615</id><created>2012-05-25</created><authors><author><keyname>Morshed</keyname><forenames>Md. Monzur</forenames></author><author><keyname>Rahman</keyname><forenames>Md. Arifur</forenames></author><author><keyname>Ahmed</keyname><forenames>Salah Uddin</forenames></author></authors><title>A Literature Review of Code Clone Analysis to Improve Software
  Maintenance Process</title><categories>cs.SE</categories><comments>This is a research collaboration of American International
  University-Bangladesh, Carleton University-Canada, SCICON &amp;
  TigerHATS-Bangladesh</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software systems are getting more complex as the system grows where
maintaining such system is a primary concern for the industry. Code clone is
one of the factors making software maintenance more difficult. It is a process
of replicating code blocks by copy-and-paste that is common in software
development. In the beginning stage of the project, developers find it easy and
time consuming though it has crucial drawbacks in the long run. There are two
types of researchers where some researchers think clones lead to additional
changes during maintenance phase, in later stage increase the overall
maintenance effort. On the other hand, some researchers think that cloned codes
are more stable than non cloned codes. In this study, we discussed Code Clones
and different ideas, methods, clone detection tools, related research on code
clone, case study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5632</identifier>
 <datestamp>2012-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5632</id><created>2012-05-25</created><authors><author><keyname>Zapatrin</keyname><forenames>Roman</forenames></author></authors><title>Quantum contextuality in classical information retrieval</title><categories>cs.IR</categories><comments>11 pages</comments><msc-class>81P13</msc-class><acm-class>H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Document ranking based on probabilistic evaluations of relevance is known to
exhibit non-classical correlations, which may be explained by admitting a
complex structure of the event space, namely, by assuming the events to emerge
from multiple sample spaces. The structure of event space formed by overlapping
sample spaces is known in quantum mechanics, they may exhibit some
counter-intuitive features, called quantum contextuality. In this Note I
observe that from the structural point of view quantum contextuality looks
similar to personalization of information retrieval scenarios. Along these
lines, Knowledge Revision is treated as operationalistic measurement and a way
to quantify the rate of personalization of Information Retrieval scenarios is
suggested.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5649</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5649</id><created>2012-05-25</created><authors><author><keyname>Vaze</keyname><forenames>Rahul</forenames></author></authors><title>Transmission Capacity of Wireless Ad Hoc Networks with Energy Harvesting
  Nodes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transmission capacity of an ad hoc wireless network is analyzed when each
node of the network harvests energy from nature, e.g. solar, wind, vibration
etc. Transmission capacity is the maximum allowable density of nodes,
satisfying a per transmitter-receiver rate, and an outage probability
constraint. Energy arrivals at each node are assumed to follow a Bernoulli
distribution, and each node stores energy using an energy buffer/battery. For
ALOHA medium access protocol (MAP), optimal transmission probability that
maximizes the transmission capacity is derived as a function of the energy
arrival distribution. Game theoretic analysis is also presented for ALOHA MAP,
where each transmitter tries to maximize its own throughput, and symmetric Nash
equilibrium is derived. For CSMA MAP, back-off probability and outage
probability are derived in terms of input energy distribution, thereby
characterizing the transmission capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5651</identifier>
 <datestamp>2012-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5651</id><created>2012-05-25</created><authors><author><keyname>Serr&#xe0;</keyname><forenames>Joan</forenames></author><author><keyname>Corral</keyname><forenames>&#xc1;lvaro</forenames></author><author><keyname>Bogu&#xf1;&#xe1;</keyname><forenames>Mari&#xe1;n</forenames></author><author><keyname>Haro</keyname><forenames>Mart&#xed;n</forenames></author><author><keyname>Arcos</keyname><forenames>Josep Lluis</forenames></author></authors><title>Measuring the evolution of contemporary western popular music</title><categories>cs.SD cs.IR cs.MM physics.soc-ph stat.AP</categories><comments>Supplementary materials not included. Please see the journal
  reference or contact the authors</comments><journal-ref>Scientific Reports 2, 521 (2012)</journal-ref><doi>10.1038/srep00521</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Popular music is a key cultural expression that has captured listeners'
attention for ages. Many of the structural regularities underlying musical
discourse are yet to be discovered and, accordingly, their historical evolution
remains formally unknown. Here we unveil a number of patterns and metrics
characterizing the generic usage of primary musical facets such as pitch,
timbre, and loudness in contemporary western popular music. Many of these
patterns and metrics have been consistently stable for a period of more than
fifty years, thus pointing towards a great degree of conventionalism.
Nonetheless, we prove important changes or trends related to the restriction of
pitch transitions, the homogenization of the timbral palette, and the growing
loudness levels. This suggests that our perception of the new would be rooted
on these changing characteristics. Hence, an old tune could perfectly sound
novel and fashionable, provided that it consisted of common harmonic
progressions, changed the instrumentation, and increased the average loudness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5653</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5653</id><created>2012-05-25</created><authors><author><keyname>Arora</keyname><forenames>Manuel</forenames></author><author><keyname>Ivanyos</keyname><forenames>G&#xe1;bor</forenames></author><author><keyname>Karpinski</keyname><forenames>Marek</forenames></author><author><keyname>Saxena</keyname><forenames>Nitin</forenames></author></authors><title>Deterministic Polynomial Factoring and Association Schemes</title><categories>cs.CC cs.DM cs.DS math.CO</categories><msc-class>12Y05 (Primary) 05E30, 05E10, 03D15, 68W30 (Secondary)</msc-class><doi>10.1112/S1461157013000296</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of finding a nontrivial factor of a polynomial f(x) over a finite
field F_q has many known efficient, but randomized, algorithms. The
deterministic complexity of this problem is a famous open question even
assuming the generalized Riemann hypothesis (GRH). In this work we improve the
state of the art by focusing on prime degree polynomials; let n be the degree.
If (n-1) has a `large' r-smooth divisor s, then we find a nontrivial factor of
f(x) in deterministic poly(n^r,log q) time; assuming GRH and that s &gt;
sqrt{n/(2^r)}. Thus, for r = O(1) our algorithm is polynomial time. Further,
for r &gt; loglog n there are infinitely many prime degrees n for which our
algorithm is applicable and better than the best known; assuming GRH.
  Our methods build on the algebraic-combinatorial framework of m-schemes
initiated by Ivanyos, Karpinski and Saxena (ISSAC 2009). We show that the
m-scheme on n points, implicitly appearing in our factoring algorithm, has an
exceptional structure; leading us to the improved time complexity. Our
structure theorem proves the existence of small intersection numbers in any
association scheme that has many relations, and roughly equal valencies and
indistinguishing numbers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5662</identifier>
 <datestamp>2013-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5662</id><created>2012-05-25</created><updated>2013-03-26</updated><authors><author><keyname>Gonzalez</keyname><forenames>Roberto</forenames></author><author><keyname>Cuevas</keyname><forenames>Ruben</forenames></author><author><keyname>Motamedi</keyname><forenames>Reza</forenames></author><author><keyname>Rejaie</keyname><forenames>Reza</forenames></author><author><keyname>Cuevas</keyname><forenames>Angel</forenames></author></authors><title>Google+ or Google-?: Dissecting the Evolution of the New OSN in its
  First Year</title><categories>cs.SI cs.NI</categories><comments>WWW 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the era when Facebook and Twitter dominate the market for social media,
Google has introduced Google+ (G+) and reported a significant growth in its
size while others called it a ghost town. This begs the question that &quot;whether
G+ can really attract a significant number of connected and active users
despite the dominance of Facebook and Twitter?&quot;.
  This paper tackles the above question by presenting a detailed
characterization of G+ based on large scale measurements. We identify the main
components of G+ structure, characterize the key features of their users and
their evolution over time. We then conduct detailed analysis on the evolution
of connectivity and activity among users in the largest connected component
(LCC) of G+ structure, and compare their characteristics with other major OSNs.
We show that despite the dramatic growth in the size of G+, the relative size
of LCC has been decreasing and its connectivity has become less clustered.
While the aggregate user activity has gradually increased, only a very small
fraction of users exhibit any type of activity. To our knowledge, our study
offers the most comprehensive characterization of G+ based on the largest
collected data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5672</identifier>
 <datestamp>2012-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5672</id><created>2012-05-25</created><authors><author><keyname>Cangiani</keyname><forenames>Andrea</forenames></author><author><keyname>Chapman</keyname><forenames>John</forenames></author><author><keyname>Georgoulis</keyname><forenames>Emmanuil H.</forenames></author><author><keyname>Jensen</keyname><forenames>Max</forenames></author></authors><title>On Local Super-Penalization of Interior Penalty Discontinuous Galerkin
  Methods</title><categories>math.NA cs.NA</categories><comments>Submitted to International Journal of Numerical Analysis and Modeling</comments><msc-class>65N30 (Primary) 65N55, 65M60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove in an abstract setting that standard (continuous) Galerkin finite
element approximations are the limit of interior penalty discontinuous Galerkin
approximations as the penalty parameter tends to infinity. We apply this result
to equations of non-negative characteristic form and the non-linear, time
dependent system of incompressible miscible displacement. Moreover, we
investigate varying the penalty parameter on only a subset of a triangulation
and the effects of local super-penalization on the stability of the method,
resulting in a partly continuous, partly discontinuous method in the limit. An
iterative automatic procedure is also proposed for the determination of the
continuous region of the domain without loss of stability of the method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5691</identifier>
 <datestamp>2012-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5691</id><created>2012-05-25</created><authors><author><keyname>Paul</keyname><forenames>Norbert</forenames></author></authors><title>Signed Simplicial Decomposition and Overlay of n-D Polytope Complexes</title><categories>cs.CG math.AT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polytope complexes are the generalisation of polygon meshes in
geo-information systems (GIS) to arbitrary dimension, and a natural concept for
accessing spatio-temporal information. Complexes of each dimension have a
straight-forward dimension-independent database representation called
&quot;Relational Complex&quot;. Accordingly, complex overlay is the corresponding
generalisation of map overlay in GIS to arbitrary dimension. Such overlay can
be computed by partitioning the cells into simplices, intersecting these and
finally combine their intersections into the resulting overlay complex. Simplex
partitioning, however, can expensive in dimension higher than 2. In the case of
polytope complex overlay /signed/ simplicial decomposition is an alternative.
This paper presents a purely combinatoric polytope complex decomposition which
ignores geometry. In particular, this method is also a decomposition method for
/non-convex/ polytopes. Geometric n-D-simplex intersection is then done by a
simplified active-set-method---a well-known numerical optimisation method.
&quot;Summing&quot; up the simplex intersections then yields the desired overlay complex.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5699</identifier>
 <datestamp>2012-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5699</id><created>2012-05-25</created><authors><author><keyname>Chalom</keyname><forenames>Gladys</forenames></author><author><keyname>Ferraz</keyname><forenames>Raul Ant&#xf4;nio</forenames></author><author><keyname>Guerreiro</keyname><forenames>Marin&#xea;s</forenames></author><author><keyname>Milies</keyname><forenames>C&#xe9;sar Polcino</forenames></author></authors><title>Minimal Binary Abelian Codes of length $p^m q^n$</title><categories>cs.IT math.IT</categories><comments>8 pages, contents partially presented at the Groups, Rings and Group
  Rings Conferences held in Ubatuba-SP (Brasil) and Edmonton-AL (Canada)</comments><msc-class>20K01, 11H71</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider binary abelian codes of length $p^m q^n$, where $p$ and $q$ are
prime rational integers under some restrictive hypotheses. In this case, we
determine the idempotents generating minimal codes and either the respective
weights or bounds of these weights. We give examples showing that these bounds
are attained in some cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5720</identifier>
 <datestamp>2012-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5720</id><created>2012-05-25</created><authors><author><keyname>Tapiador</keyname><forenames>Antonio</forenames></author><author><keyname>Carrera</keyname><forenames>Diego</forenames></author><author><keyname>Salvach&#xfa;a</keyname><forenames>Joaqu&#xed;n</forenames></author></authors><title>Tie-RBAC: An application of RBAC to Social Networks</title><categories>cs.SI cs.CR</categories><comments>Web 2.0 Security &amp; Privacy 2011</comments><acm-class>K.6.5; H.3.5</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper explores the application of role-based access control to social
networks, from the perspective of social network analysis. Each tie, composed
of a relation, a sender and a receiver, involves the sender's assignation of
the receiver to a role with permissions. The model is not constrained to
system-defined relations and lets users define them unilaterally. It benefits
of RBAC's advantages, such as policy neutrality, simplification of security
administration and permissions on other roles. Tie-RBAC has been implemented in
a core for building social network sites, Social Stream.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5729</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5729</id><created>2012-05-25</created><updated>2013-01-29</updated><authors><author><keyname>Martinez-Mateo</keyname><forenames>Jesus</forenames></author><author><keyname>Elkouss</keyname><forenames>David</forenames></author><author><keyname>Martin</keyname><forenames>Vicente</forenames></author></authors><title>Blind Reconciliation</title><categories>quant-ph cs.IT math.IT</categories><comments>22 pages, 8 figures</comments><journal-ref>Quantum Information and Computation, Vol. 12, No. 9&amp;10 (2012)
  0791-0812</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information reconciliation is a crucial procedure in the classical
post-processing of quantum key distribution (QKD). Poor reconciliation
efficiency, revealing more information than strictly needed, may compromise the
maximum attainable distance, while poor performance of the algorithm limits the
practical throughput in a QKD device. Historically, reconciliation has been
mainly done using close to minimal information disclosure but heavily
interactive procedures, like Cascade, or using less efficient but also less
interactive -just one message is exchanged- procedures, like the ones based in
low-density parity-check (LDPC) codes. The price to pay in the LDPC case is
that good efficiency is only attained for very long codes and in a very narrow
range centered around the quantum bit error rate (QBER) that the code was
designed to reconcile, thus forcing to have several codes if a broad range of
QBER needs to be catered for. Real world implementations of these methods are
thus very demanding, either on computational or communication resources or
both, to the extent that the last generation of GHz clocked QKD systems are
finding a bottleneck in the classical part. In order to produce compact, high
performance and reliable QKD systems it would be highly desirable to remove
these problems. Here we analyse the use of short-length LDPC codes in the
information reconciliation context using a low interactivity, blind, protocol
that avoids an a priori error rate estimation. We demonstrate that 2x10^3 bits
length LDPC codes are suitable for blind reconciliation. Such codes are of high
interest in practice, since they can be used for hardware implementations with
very high throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5742</identifier>
 <datestamp>2012-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5742</id><created>2012-05-25</created><authors><author><keyname>Qadir</keyname><forenames>Ashraf</forenames></author><author><keyname>Semke</keyname><forenames>William</forenames></author><author><keyname>Neubert</keyname><forenames>Jeremiah</forenames></author></authors><title>Implementation of an Onboard Visual Tracking System with Small Unmanned
  Aerial Vehicle (UAV)</title><categories>cs.RO</categories><comments>9 pages; 6 figures; International Journal of Innovative Technology
  and Creative Engineering (ISSN:2045-8711) VOl.1 No. 10 OCTOBER 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a visual tracking system that is capable or running real
time on-board a small UAV (Unmanned Aerial Vehicle). The tracking system is
computationally efficient and invariant to lighting changes and rotation of the
object or the camera. Detection and tracking is autonomously carried out on the
payload computer and there are two different methods for creation of the image
patches. The first method starts detecting and tracking using a stored image
patch created prior to flight with previous flight data. The second method
allows the operator on the ground to select the interest object for the UAV to
track. The tracking system is capable of re-detecting the object of interest in
the events of tracking failure. Performance of the tracking system was verified
both in the lab and during actual flights of the UAV. Results show that the
system can run on-board and track a diverse set of objects in real time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5745</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5745</id><created>2012-05-25</created><authors><author><keyname>Bova</keyname><forenames>Simone</forenames></author><author><keyname>Chen</keyname><forenames>Hubie</forenames></author><author><keyname>Valeriote</keyname><forenames>Matthew</forenames></author></authors><title>Generic Expression Hardness Results for Primitive Positive Formula
  Comparison</title><categories>cs.LO cs.CC cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the expression complexity of two basic problems involving the
comparison of primitive positive formulas: equivalence and containment. In
particular, we study the complexity of these problems relative to finite
relational structures. We present two generic hardness results for the studied
problems, and discuss evidence that they are optimal and yield, for each of the
problems, a complexity trichotomy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5757</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5757</id><created>2012-05-25</created><authors><author><keyname>Asim</keyname><forenames>Muhammad</forenames></author><author><keyname>Ignatenko</keyname><forenames>Tanya</forenames></author><author><keyname>Petkovic</keyname><forenames>Milan</forenames></author><author><keyname>Trivellato</keyname><forenames>Daniel</forenames></author><author><keyname>Zannone</keyname><forenames>Nicola</forenames></author></authors><title>Enforcing Access Control in Virtual Organizations Using Hierarchical
  Attribute-Based Encryption</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Virtual organizations are dynamic, inter-organizational collaborations that
involve systems and services belonging to different security domains. Several
solutions have been proposed to guarantee the enforcement of the access control
policies protecting the information exchanged in a distributed system, but none
of them addresses the dynamicity characterizing virtual organizations. In this
paper we propose a dynamic hierarchical attribute-based encryption (D-HABE)
scheme that allows the institutions in a virtual organization to encrypt
information according to an attribute-based policy in such a way that only
users with the appropriate attributes can decrypt it. In addition, we introduce
a key management scheme that determines which user is entitled to receive which
attribute key from which domain authority.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5770</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5770</id><created>2012-05-25</created><updated>2013-01-05</updated><authors><author><keyname>Zouzias</keyname><forenames>Anastasios</forenames></author><author><keyname>Freris</keyname><forenames>Nikolaos</forenames></author></authors><title>Randomized Extended Kaczmarz for Solving Least-Squares</title><categories>math.NA cs.DS</categories><comments>19 Pages, 5 figures; code is available at
  https://github.com/zouzias/REK</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a randomized iterative algorithm that exponentially converges in
expectation to the minimum Euclidean norm least squares solution of a given
linear system of equations. The expected number of arithmetic operations
required to obtain an estimate of given accuracy is proportional to the square
condition number of the system multiplied by the number of non-zeros entries of
the input matrix. The proposed algorithm is an extension of the randomized
Kaczmarz method that was analyzed by Strohmer and Vershynin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5779</identifier>
 <datestamp>2012-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5779</id><created>2012-05-25</created><authors><author><keyname>Shutters</keyname><forenames>Brad</forenames></author><author><keyname>Vakati</keyname><forenames>Sudheer</forenames></author><author><keyname>Fern&#xe1;ndez-Baca</keyname><forenames>David</forenames></author></authors><title>Improved Lower Bounds on the Compatibility of Multi-State Characters</title><categories>math.CO cs.DM cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a long standing conjecture on the necessary and sufficient
conditions for the compatibility of multi-state characters: There exists a
function $f(r)$ such that, for any set $C$ of $r$-state characters, $C$ is
compatible if and only if every subset of $f(r)$ characters of $C$ is
compatible. We show that for every $r \ge 2$, there exists an incompatible set
$C$ of $\lfloor\frac{r}{2}\rfloor\cdot\lceil\frac{r}{2}\rceil + 1$ $r$-state
characters such that every proper subset of $C$ is compatible. Thus, $f(r) \ge
\lfloor\frac{r}{2}\rfloor\cdot\lceil\frac{r}{2}\rceil + 1$ for every $r \ge 2$.
This improves the previous lower bound of $f(r) \ge r$ given by Meacham (1983),
and generalizes the construction showing that $f(4) \ge 5$ given by Habib and
To (2011). We prove our result via a result on quartet compatibility that may
be of independent interest: For every integer $n \ge 4$, there exists an
incompatible set $Q$ of
$\lfloor\frac{n-2}{2}\rfloor\cdot\lceil\frac{n-2}{2}\rceil + 1$ quartets over
$n$ labels such that every proper subset of $Q$ is compatible. We contrast this
with a result on the compatibility of triplets: For every $n \ge 3$, if $R$ is
an incompatible set of more than $n-1$ triplets over $n$ labels, then some
proper subset of $R$ is incompatible. We show this upper bound is tight by
exhibiting, for every $n \ge 3$, a set of $n-1$ triplets over $n$ taxa such
that $R$ is incompatible, but every proper subset of $R$ is compatible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5783</identifier>
 <datestamp>2012-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5783</id><created>2012-05-25</created><authors><author><keyname>Bartel</keyname><forenames>Alexandre</forenames><affiliation>SnT</affiliation></author><author><keyname>Baudry</keyname><forenames>Benoit</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Munoz</keyname><forenames>Freddy</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Klein</keyname><forenames>Jacques</forenames><affiliation>SnT</affiliation></author><author><keyname>Mouelhi</keyname><forenames>Tejeddine</forenames><affiliation>S'nT</affiliation></author><author><keyname>Traon</keyname><forenames>Yves Le</forenames><affiliation>SnT</affiliation></author></authors><title>Model Driven Mutation Applied to Adaptative Systems Testing</title><categories>cs.SE</categories><comments>IEEE International Conference on Software Testing, Verification and
  Validation, Mutation Analysis Workshop (Mutation 2011), Berlin : Allemagne
  (2011)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamically Adaptive Systems modify their behav- ior and structure in
response to changes in their surrounding environment and according to an
adaptation logic. Critical sys- tems increasingly incorporate dynamic
adaptation capabilities; examples include disaster relief and space exploration
systems. In this paper, we focus on mutation testing of the adaptation logic.
We propose a fault model for adaptation logics that classifies faults into
environmental completeness and adaptation correct- ness. Since there are
several adaptation logic languages relying on the same underlying concepts, the
fault model is expressed independently from specific adaptation languages.
Taking benefit from model-driven engineering technology, we express these
common concepts in a metamodel and define the operational semantics of mutation
operators at this level. Mutation is applied on model elements and model
transformations are used to propagate these changes to a given adaptation
policy in the chosen formalism. Preliminary results on an adaptive web server
highlight the difficulty of killing mutants for adaptive systems, and thus the
difficulty of generating efficient tests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5788</identifier>
 <datestamp>2012-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5788</id><created>2012-03-27</created><authors><author><keyname>Rinott</keyname><forenames>Yosef</forenames></author><author><keyname>Scarsini</keyname><forenames>Marco</forenames></author><author><keyname>Yu</keyname><forenames>Yaming</forenames></author></authors><title>A Colonel Blotto Gladiator Game</title><categories>cs.GT math.PR</categories><comments>32 pages, 8 figures</comments><msc-class>91A05, 60E15</msc-class><doi>10.1287/moor.1120.0550</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a stochastic version of the well-known Blotto game, called the
gladiator game. In this zero-sum allocation game two teams of gladiators engage
in a sequence of one-to-one fights in which the probability of winning is a
function of the gladiators' strengths. Each team's strategy consists of the
allocation of its total strength among its gladiators. We find the Nash
equilibria and the value of this class of games and show how they depend on the
total strength of teams and the number of gladiators in each team. To do this,
we study interesting majorization-type probability inequalities concerning
linear combinations of Gamma random variables. Similar inequalities have been
used in models of telecommunications and research and development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5819</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5819</id><created>2012-05-25</created><updated>2012-07-17</updated><authors><author><keyname>Kalajdzievski</keyname><forenames>Damjan</forenames></author></authors><title>Measurability Aspects of the Compactness Theorem for Sample Compression
  Schemes</title><categories>stat.ML cs.LG</categories><comments>Latex 2e, 64 pages, 1 figure. This is an M.Sc. thesis defended on
  July 4'th 2012 at the University of Ottawa, Canada, under the supervision of
  Dr. V. Pestov, and with examiners Dr. J. Levy and Dr. S. Zilles</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It was proved in 1998 by Ben-David and Litman that a concept space has a
sample compression scheme of size d if and only if every finite subspace has a
sample compression scheme of size d. In the compactness theorem, measurability
of the hypotheses of the created sample compression scheme is not guaranteed;
at the same time measurability of the hypotheses is a necessary condition for
learnability. In this thesis we discuss when a sample compression scheme,
created from com- pression schemes on finite subspaces via the compactness
theorem, have measurable hypotheses. We show that if X is a standard Borel
space with a d-maximum and universally separable concept class C, then (X,C)
has a sample compression scheme of size d with universally Borel measurable
hypotheses. Additionally we introduce a new variant of compression scheme
called a copy sample compression scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5821</identifier>
 <datestamp>2012-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5821</id><created>2012-03-06</created><authors><author><keyname>Wilkinson</keyname><forenames>Ian</forenames><affiliation>The University of Sydney</affiliation></author><author><keyname>Young</keyname><forenames>Louise</forenames><affiliation>University of Western Sydney</affiliation></author></authors><title>Toward A Normative Theory of Normative Marketing Theory</title><categories>q-fin.GN cs.CY physics.soc-ph</categories><comments>27 pages, 1 Figure</comments><journal-ref>Marketing Theory 5 (4) 2005, 363-396</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how different approaches to developing marketing strategies depending
on the type of environment a firm faces, where environments are distinguished
in terms of their systems properties not their context. Particular emphasis is
given to turbulent environments in which outcomes are not a priori predictable
and are not traceable to individual firm actions and we show that, in these
conditions, the relevant unit of competitive response and understanding is no
longer the individual firm but the network of relations comprising
interdependent, interacting firms. Networks of relations are complex adaptive
systems that are more 'intelligent' than the individual firms that comprise
them and are capable of comprehending and responding to more complex and
turbulent environments. Yet they are co-produced by the patterns of actions and
interactions of the firms involved. The creation and accessing of such
distributed intelligence cannot be centrally directed, as this necessarily
limits it. Instead managers and firms are involved in a kind of participatory
planning and adaptation process through which the network self-organises and
adapts. Drawing on research in systems theory, complexity, biology and
cognitive science, extensions to the resource-based theory of the firm are
proposed that include how resources are linked across relations and network in
a dynamic and evolutionary way. The concept of an extended firm and soft
assembled strategies are introduced to describe the nature of the strategy
development process. This results in a more theoretically grounded basis for
understanding the nature and role of relationship and network strategies in
marketing and management. We finish by considering the research implications of
our analysis and the role of agent based models as a means of sensitising and
informing management action.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5823</identifier>
 <datestamp>2012-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5823</id><created>2012-05-25</created><authors><author><keyname>Penrose</keyname><forenames>Roger</forenames></author></authors><title>Foreword: A Computable Universe, Understanding Computation and Exploring
  Nature As Computation</title><categories>cs.GL cs.AI cs.CC cs.IT math.IT physics.hist-ph physics.pop-ph</categories><comments>26 pages, foreword to the book A Computable Universe: Understanding
  Computation and Exploring Nature As Computation, World Scientific, 2012
  http://www.mathrix.org/experimentalAIT/ComputationNature.htm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I am most honoured to have the privilege to present the Foreword to this
fascinating and wonderfully varied collection of contributions, concerning the
nature of computation and of its deep connection with the operation of those
basic laws, known or yet unknown, governing the universe in which we live.
Fundamentally deep questions are indeed being grappled with here, and the fact
that we find so many different viewpoints is something to be expected, since,
in truth, we know little about the foundational nature and origins of these
basic laws, despite the immense precision that we so often find revealed in
them. Accordingly, it is not surprising that within the viewpoints expressed
here is some unabashed speculation, occasionally bordering on just partially
justified guesswork, while elsewhere we find a good deal of precise reasoning,
some in the form of rigorous mathematical theorems. Both of these are as should
be, for without some inspired guesswork we cannot have new ideas as to where
look in order to make genuinely new progress, and without precise mathematical
reasoning, no less than in precise observation, we cannot know when we are
right -- or, more usually, when we are wrong.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5824</identifier>
 <datestamp>2012-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5824</id><created>2012-05-25</created><updated>2012-11-27</updated><authors><author><keyname>Lemoine</keyname><forenames>Grady I.</forenames></author><author><keyname>Ou</keyname><forenames>M. Yvonne</forenames></author><author><keyname>LeVeque</keyname><forenames>Randall J.</forenames></author></authors><title>High-Resolution Finite Volume Modeling of Wave Propagation in
  Orthotropic Poroelastic Media</title><categories>math.NA cs.NA</categories><comments>26 pages, 7 figures, to appear in SIAM Journal on Scientific
  Computing Section 2, code provided at
  https://bitbucket.org/grady_lemoine/poro-2d-cartesian-archive</comments><msc-class>65M08 (Primary) 74S10, 74F10, 74J10, 74L05, 74L15, 86-08 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Poroelasticity theory models the dynamics of porous, fluid-saturated media.
It was pioneered by Maurice Biot in the 1930s through 1960s, and has
applications in several fields, including geophysics and modeling of in vivo
bone. A wide variety of methods have been used to model poroelasticity,
including finite difference, finite element, pseudospectral, and discontinuous
Galerkin methods. In this work we use a Cartesian-grid high-resolution finite
volume method to numerically solve Biot's equations in the time domain for
orthotropic materials, with the stiff relaxation source term in the equations
incorporated using operator splitting. This class of finite volume method has
several useful properties, including the ability to use wave limiters to reduce
numerical artifacts in the solution, ease of incorporating material
inhomogeneities, low memory overhead, and an explicit time-stepping approach.
To the authors' knowledge, this is the first use of high-resolution finite
volume methods to model poroelasticity. The solution code uses the CLAWPACK
finite volume method software, which also includes block-structured adaptive
mesh refinement in its AMRCLAW variant. We present convergence results for
known analytic plane wave solutions, achieving second-order convergence rates
outside of the stiff regime of the system. Our convergence rates are degraded
in the stiff regime, but we still achieve similar levels of error on the finest
grids examined. We also demonstrate good agreement against other numerical
results from the literature. To aid in reproducibility, we provide all of the
code used to generate the results of this paper, at
https://bitbucket.org/grady_lemoine/poro-2d-cartesian-archive .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5838</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5838</id><created>2012-05-25</created><updated>2013-12-31</updated><authors><author><keyname>Nguyen</keyname><forenames>Linh Anh</forenames></author></authors><title>ExpTime Tableaux for the Description Logic SHIQ Based on Global State
  Caching and Integer Linear Feasibility Checking</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give the first ExpTime (complexity-optimal) tableau decision procedure for
checking satisfiability of a knowledge base in the description logic SHIQ when
numbers are coded in unary. Our procedure is based on global state caching and
integer linear feasibility checking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5849</identifier>
 <datestamp>2013-05-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5849</id><created>2012-05-25</created><updated>2013-05-08</updated><authors><author><keyname>Nguyen</keyname><forenames>Hieu Duy</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author><author><keyname>Hui</keyname><forenames>Hon Tat</forenames></author></authors><title>Multi-Cell Random Beamforming: Achievable Rate and Degrees of Freedom
  Region</title><categories>cs.IT math.IT</categories><comments>28 pages, 6 figures, to appear in IEEE Transactions of Signal
  Processing. This work was presented in part at IEEE International Conference
  on Acoustics, Speech, and Signal Processing (ICASSP), Kyoto, Japan, March
  25-30, 2012. The authors are with the Department of Electrical and Computer
  Engineering, National University of Singapore (emails: {hieudn, elezhang,
  elehht}@nus.edu.sg)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random beamforming (RBF) is a practically favourable transmission scheme for
multiuser multi-antenna downlink systems since it requires only partial channel
state information (CSI) at the transmitter. Under the conventional single-cell
setup, RBF is known to achieve the optimal sum-capacity scaling law as the
number of users goes to infinity, thanks to the multiuser diversity enabled
transmission scheduling that virtually eliminates the intra-cell interference.
In this paper, we extend the study of RBF to a more practical multi-cell
downlink system with single-antenna receivers subject to the additional
inter-cell interference (ICI). First, we consider the case of finite
signal-to-noise ratio (SNR) at each receiver. We derive a closed-form
expression of the achievable sum-rate with the multi-cell RBF, based upon which
we show an asymptotic sum-rate scaling law as the number of users goes to
infinity. Next, we consider the high-SNR regime and for tractable analysis
assume that the number of users in each cell scales in a certain order with the
per-cell SNR. Under this setup, we characterize the achievable degrees of
freedom (DoF) for the single-cell case with RBF. Then we extend the analysis to
the multi-cell RBF case by characterizing the DoF region. It is shown that the
DoF region characterization provides useful guideline on how to design a
cooperative multi-cell RBF system to achieve optimal throughput tradeoffs among
different cells. Furthermore, our results reveal that the multi-cell RBF scheme
achieves the &quot;interference-free DoF&quot; region upper bound for the multi-cell
system, provided that the per-cell number of users has a sufficiently large
scaling order with the SNR. Our result thus confirms the optimality of
multi-cell RBF in this regime even without the complete CSI at the transmitter,
as compared to other full-CSI requiring transmission schemes such as
interference alignment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5856</identifier>
 <datestamp>2012-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5856</id><created>2012-05-26</created><authors><author><keyname>Timofeev</keyname><forenames>Evgeniy</forenames></author><author><keyname>Kaltchenko</keyname><forenames>Alexei</forenames></author></authors><title>Nearest-neighbor Entropy Estimators with Weak Metrics</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>9 pages, no figures</comments><msc-class>58F15, 58F17, 53C35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A problem of improving the accuracy of nonparametric entropy estimation for a
stationary ergodic process is considered. New weak metrics are introduced and
relations between metrics, measures, and entropy are discussed. Based on weak
metrics, a new nearest-neighbor entropy estimator is constructed and has a
parameter with which the estimator is optimized to reduce its bias. It is shown
that estimator's variance is upper-bounded by a nearly optimal Cramer-Rao lower
bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5863</identifier>
 <datestamp>2012-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5863</id><created>2012-05-26</created><authors><author><keyname>Mehri</keyname><forenames>Hassan</forenames></author><author><keyname>Sadeghi</keyname><forenames>Mohammad Reza</forenames></author></authors><title>Construction of LDGM lattices</title><categories>cs.IT cs.CR math.CO math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low density generator matrix (LDGM) codes have an acceptable performance
under iterative decoding algorithms. This idea is used to construct a class of
lattices with relatively good performance and low encoding and decoding
complexity. To construct such lattices, Construction D is applied to a set of
generator vectors of a class of LDGM codes. Bounds on the minimum distance and
the coding gain of the corresponding lattices and a corollary for the cross
sections and projections of these lattices are provided. The progressive edge
growth (PEG) algorithm is used to construct a class of binary codes to generate
the corresponding lattice. Simulation results confirm the acceptable
performance of these class of lattices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5865</identifier>
 <datestamp>2012-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5865</id><created>2012-05-26</created><authors><author><keyname>Fitelson</keyname><forenames>Branden</forenames></author><author><keyname>Osherson</keyname><forenames>Daniel</forenames></author></authors><title>Remarks on &quot;Random Sequences&quot;</title><categories>stat.ME cs.CC math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, we show that classical statistical tests for randomness are
language dependent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5866</identifier>
 <datestamp>2012-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5866</id><created>2012-05-26</created><authors><author><keyname>Tripathy</keyname><forenames>B. K.</forenames></author><author><keyname>Panda</keyname><forenames>G. K.</forenames></author></authors><title>Approximate Equalities on Rough Intuitionistic Fuzzy Sets and an
  Analysis of Approximate Equalities</title><categories>cs.AI</categories><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 2, No 3, March 2012 ISSN (Online): 1694-0814 www.IJCSI.org</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to involve user knowledge in determining equality of sets, which may
not be equal in the mathematical sense, three types of approximate (rough)
equalities were introduced by Novotny and Pawlak ([8, 9, 10]). These notions
were generalized by Tripathy, Mitra and Ojha ([13]), who introduced the
concepts of approximate (rough) equivalences of sets. Rough equivalences
capture equality of sets at a higher level than rough equalities. More
properties of these concepts were established in [14]. Combining the conditions
for the two types of approximate equalities, two more approximate equalities
were introduced by Tripathy [12] and a comparative analysis of their relative
efficiency was provided. In [15], the four types of approximate equalities were
extended by considering rough fuzzy sets instead of only rough sets. In fact
the concepts of leveled approximate equalities were introduced and properties
were studied. In this paper we proceed further by introducing and studying the
approximate equalities based on rough intuitionistic fuzzy sets instead of
rough fuzzy sets. That is we introduce the concepts of approximate
(rough)equalities of intuitionistic fuzzy sets and study their properties. We
provide some real life examples to show the applications of rough equalities of
fuzzy sets and rough equalities of intuitionistic fuzzy sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5871</identifier>
 <datestamp>2012-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5871</id><created>2012-05-26</created><updated>2012-07-18</updated><authors><author><keyname>Mazzucco</keyname><forenames>Michele</forenames></author><author><keyname>Vasar</keyname><forenames>Martti</forenames></author><author><keyname>Dumas</keyname><forenames>Marlon</forenames></author></authors><title>Squeezing out the Cloud via Profit-Maximizing Resource Allocation
  Policies</title><categories>cs.DC cs.PF</categories><comments>16 pages, 9 Figures. A 10 pages version of this manuscript will
  appear in IEEE MASCOTS 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of maximizing the average hourly profit earned by a
Software-as-a-Service (SaaS) provider who runs a software service on behalf of
a customer using servers rented from an Infrastructure-as-a-Service (IaaS)
provider. The SaaS provider earns a fee per successful transaction and incurs
costs proportional to the number of server-hours it uses. A number of resource
allocation policies for this or similar problems have been proposed in previous
work. However, to the best of our knowledge, these policies have not been
comparatively evaluated in a cloud environment. This paper reports on an
empirical evaluation of three policies using a replica of Wikipedia deployed on
the Amazon EC2 cloud. Experimental results show that a policy based on a
solution to an optimization problem derived from the SaaS provider's utility
function outperforms well-known heuristics that have been proposed for similar
problems. It is also shown that all three policies outperform a &quot;reactive&quot;
allocation approach based on Amazon's auto-scaling feature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5904</identifier>
 <datestamp>2012-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5904</id><created>2012-05-26</created><authors><author><keyname>Hern</keyname><forenames>Brett</forenames></author><author><keyname>Narayanan</keyname><forenames>Krishna</forenames></author></authors><title>Joint Compute and Forward for the Two Way Relay Channel with Spatially
  Coupled LDPC Codes</title><categories>cs.IT math.IT</categories><comments>This paper was submitted to IEEE Global Communications Conference
  2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the design and analysis of coding schemes for the binary input
two way relay channel with erasure noise. We are particularly interested in
reliable physical layer network coding in which the relay performs perfect
error correction prior to forwarding messages. The best known achievable rates
for this problem can be achieved through either decode and forward or compute
and forward relaying. We consider a decoding paradigm called joint compute and
forward which we numerically show can achieve the best of these rates with a
single encoder and decoder. This is accomplished by deriving the exact
performance of a message passing decoder based on joint compute and forward for
spatially coupled LDPC ensembles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5906</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5906</id><created>2012-05-26</created><updated>2012-09-09</updated><authors><author><keyname>Yilmaz</keyname><forenames>Yasin</forenames></author><author><keyname>Moustakides</keyname><forenames>George V.</forenames></author><author><keyname>Wang</keyname><forenames>Xiaodong</forenames></author></authors><title>Channel-aware Decentralized Detection via Level-triggered Sampling</title><categories>stat.AP cs.IT math.IT</categories><doi>10.1109/TSP.2012.2222401</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider decentralized detection through distributed sensors that perform
level-triggered sampling and communicate with a fusion center via noisy
channels. Each sensor computes its local log-likelihood ratio (LLR), samples it
using the level-triggered sampling, and upon sampling transmits a single bit to
the FC. Upon receiving a bit from a sensor, the FC updates the global LLR and
performs a sequential probability ratio test (SPRT) step. We derive the fusion
rules under various types of channels. We further provide an asymptotic
analysis on the average detection delay for the proposed channel-aware scheme,
and show that the asymptotic detection delay is characterized by a KL
information number. The delay analysis facilitates the choice of appropriate
signaling schemes under different channel types for sending the 1-bit
information from sensors to the FC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5911</identifier>
 <datestamp>2012-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5911</id><created>2012-05-26</created><authors><author><keyname>Grushko</keyname><forenames>Carmi</forenames></author></authors><title>A Hough Transform Approach to Solving Linear Min-Max Problems</title><categories>cs.NA cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several ways to accelerate the solution of 2D/3D linear min-max problems in
$n$ constraints are discussed. We also present an algorithm for solving such
problems in the 2D case, which is superior to CGAL's linear programming solver,
both in performance and in stability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5914</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5914</id><created>2012-05-26</created><authors><author><keyname>Torezzan</keyname><forenames>Cristiano</forenames></author><author><keyname>Costa</keyname><forenames>Sueli I. R.</forenames></author><author><keyname>Vaishampayan</keyname><forenames>Vinay A.</forenames></author></authors><title>Constructive spherical codes on layers of flat tori</title><categories>cs.IT math.IT</categories><comments>9 pages, 5 figures, submitted to IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new class of spherical codes is constructed by selecting a finite subset of
flat tori from a foliation of the unit sphere S^{2L-1} of R^{2L} and designing
a structured codebook on each torus layer. The resulting spherical code can be
the image of a lattice restricted to a specific hyperbox in R^L in each layer.
Group structure and homogeneity, useful for efficient storage and decoding, are
inherited from the underlying lattice codebook. A systematic method for
constructing such codes are presented and, as an example, the Leech lattice is
used to construct a spherical code in R^{48}. Upper and lower bounds on the
performance, the asymptotic packing density and a method for decoding are
derived.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5921</identifier>
 <datestamp>2012-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5921</id><created>2012-05-26</created><authors><author><keyname>GHERABI</keyname><forenames>Noreddine</forenames></author><author><keyname>BAHAJ</keyname><forenames>Mohamed</forenames></author></authors><title>Robust representation for conversion UML class into XML Document using
  DOM</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a Framework for converting a class diagram into an XML
structure and shows how to use Web files for the design of data warehouses
based on the classification UML. Extensible Markup Language (XML) has become a
standard for representing data over the Internet. We use XSD schema for define
the structure of XML documents and validate XML documents.
  A prototype has been developed, which migrates successfully UML Class into
XML document based on the formulation mathematics model. The experimental
results were very encouraging, demonstrating that the proposed approach is
feasible efficient and correct.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5922</identifier>
 <datestamp>2012-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5922</id><created>2012-05-26</created><authors><author><keyname>Gherabi</keyname><forenames>Noreddine</forenames></author><author><keyname>Addakiri</keyname><forenames>Khaoula</forenames></author><author><keyname>Bahaj</keyname><forenames>Mohamed</forenames></author></authors><title>Mapping relational database into OWL Structure with data semantic
  preservation</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a solution for migrating an RDB into Web semantic. The
solution takes an existing RDB as input, and extracts its metadata
representation (MTRDB). Based on the MTRDB, a Canonical Data Model (CDM) is
generated. Finally, the structure of the classification scheme in the CDM model
is converted into OWL ontology and the recordsets of database are stored in owl
document. A prototype has been implemented, which migrates a RDB into OWL
structure, for demonstrate the practical applicability of our approach by
showing how the results of reasoning of this technique can help improve the Web
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5923</identifier>
 <datestamp>2012-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5923</id><created>2012-05-26</created><authors><author><keyname>Gherabi</keyname><forenames>Noreddine</forenames></author><author><keyname>Bahaj</keyname><forenames>Mohamed</forenames></author></authors><title>Conversion database of the shapes into XML data for shape matching</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new approach to the matching of 2D shapes using XML language and
dynamic programming. Given a 2D shape, we extract its contour and which is
represented by set of points. The contour is divided into curves using corner
detection. After, each curve is described by local and global features; these
features are coded in a string of symbols and stored in a XML file. Finally,
using the dynamic programming, we find the optimal alignment between sequences
of symbols. Results are presented and compared with existing methods using
MATLAB for KIMIA-25 database and MPEG7 databases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5925</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5925</id><created>2012-05-26</created><authors><author><keyname>Ribeiro</keyname><forenames>Bruno</forenames></author><author><keyname>Basu</keyname><forenames>Prithwish</forenames></author><author><keyname>Towsley</keyname><forenames>Don</forenames></author></authors><title>Multiple Random Walks to Uncover Short Paths in Power Law Networks</title><categories>cs.SI physics.soc-ph</categories><report-no>UMass CMPSCI TechReport UM-CS-2011-049</report-no><journal-ref>IEEE INFOCOM NetSciCom 2012</journal-ref><doi>10.1109/INFCOMW.2012.6193500</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Consider the following routing problem in the context of a large scale
network $G$, with particular interest paid to power law networks, although our
results do not assume a particular degree distribution. A small number of nodes
want to exchange messages and are looking for short paths on $G$. These nodes
do not have access to the topology of $G$ but are allowed to crawl the network
within a limited budget. Only crawlers whose sample paths cross are allowed to
exchange topological information. In this work we study the use of random walks
(RWs) to crawl $G$. We show that the ability of RWs to find short paths bears
no relation to the paths that they take. Instead, it relies on two properties
of RWs on power law networks: 1) RW's ability observe a sizable fraction of the
network edges; and 2) an almost certainty that two distinct RW sample paths
cross after a small percentage of the nodes have been visited. We show
promising simulation results on several real world networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5927</identifier>
 <datestamp>2012-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5927</id><created>2012-05-26</created><authors><author><keyname>Lou</keyname><forenames>Youcheng</forenames></author><author><keyname>Shi</keyname><forenames>Guodong</forenames></author><author><keyname>Johansson</keyname><forenames>Karl Henrik</forenames></author><author><keyname>Hong</keyname><forenames>Yiguang</forenames></author></authors><title>An Approximate Projected Consensus Algorithm for Computing Intersection
  of Convex Sets</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an approximate projected consensus algorithm for a
network to cooperatively compute the intersection of convex sets. Instead of
assuming the exact convex projection proposed in the literature, we allow each
node to compute an approximate projection and communicate it to its neighbors.
The communication graph is directed and time-varying. Nodes update their states
by weighted averaging. Projection accuracy conditions are presented for the
considered algorithm. They indicate how much projection accuracy is required to
ensure global consensus to a point in the intersection set when the
communication graph is uniformly jointly strongly connected. We show that
$\pi/4$ is a critical angle error of the projection approximation to ensure a
bounded state. A numerical example indicates that this approximate projected
consensus algorithm may achieve better performance than the exact projected
consensus algorithm in some cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5928</identifier>
 <datestamp>2012-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5928</id><created>2012-05-26</created><authors><author><keyname>Meinke</keyname><forenames>Karl</forenames></author><author><keyname>Sindhu</keyname><forenames>Muddassar A.</forenames></author></authors><title>An n log n Alogrithm for Deterministic Kripke Structure Minimization</title><categories>cs.DS cs.FL cs.LO</categories><comments>9 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce an algorithm for the minimization of deterministic Kripke
structures with O(kn log2 n) time complexity. We prove the correctness and
complexity properties of this algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5938</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5938</id><created>2012-05-27</created><authors><author><keyname>Wongpiromsarn</keyname><forenames>Tichakorn</forenames></author><author><keyname>Uthaicharoenpong</keyname><forenames>Tawit</forenames></author><author><keyname>Wang</keyname><forenames>Yu</forenames></author><author><keyname>Frazzoli</keyname><forenames>Emilio</forenames></author><author><keyname>Wang</keyname><forenames>Danwei</forenames></author></authors><title>Distributed Traffic Signal Control for Maximum Network Throughput</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a distributed algorithm for controlling traffic signals. Our
algorithm is adapted from backpressure routing, which has been mainly applied
to communication and power networks. We formally prove that our algorithm
ensures global optimality as it leads to maximum network throughput even though
the controller is constructed and implemented in a completely distributed
manner. Simulation results show that our algorithm significantly outperforms
SCATS, an adaptive traffic signal control system that is being used in many
cities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5946</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5946</id><created>2012-05-27</created><authors><author><keyname>Bucci</keyname><forenames>Michelangelo</forenames></author><author><keyname>De Luca</keyname><forenames>Alessandro</forenames></author><author><keyname>Zamboni</keyname><forenames>Luca Q.</forenames></author></authors><title>Some characterizations of Sturmian words in terms of the lexicographic
  order</title><categories>math.CO cs.DM</categories><journal-ref>Fundamenta Informaticae 116 (2012) 25-33</journal-ref><doi>10.3233/FI-2012-665</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present three new characterizations of Sturmian words based
on the lexicographic ordering of their factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5959</identifier>
 <datestamp>2012-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5959</id><created>2012-05-27</created><authors><author><keyname>Choi</keyname><forenames>Sung-Tai</forenames></author><author><keyname>Kim</keyname><forenames>Ji-Youp</forenames></author><author><keyname>No</keyname><forenames>Jong-Seon</forenames></author></authors><title>On the Cross-Correlation of a $p$-ary m-Sequence and its Decimated
  Sequences by $d=\frac{p^n+1}{p^k+1}+\frac{p^n-1}{2}$</title><categories>cs.IT math.IT math.NT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, for an odd prime $p$ such that $p\equiv 3\bmod 4$, odd $n$,
and $d=(p^n+1)/(p^k+1)+(p^n-1)/2$ with $k|n$, the value distribution of the
exponential sum $S(a,b)$ is calculated as $a$ and $b$ run through
$\mathbb{F}_{p^n}$. The sequence family $\mathcal{G}$ in which each sequence
has the period of $N=p^n-1$ is also constructed. The family size of
$\mathcal{G}$ is $p^n$ and the correlation magnitude is roughly upper bounded
by $(p^k+1)\sqrt{N}/2$. The weight distribution of the relevant cyclic code
$\mathcal{C}$ over $\mathbb{F}_p$ with the length $N$ and the dimension ${\rm
dim}_{\mathbb{F}_p}\mathcal{C}=2n$ is also derived. Our result includes the
case in \cite{Xia} as a special case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5960</identifier>
 <datestamp>2012-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5960</id><created>2012-05-27</created><authors><author><keyname>Ouchetto</keyname><forenames>Hassania</forenames></author><author><keyname>Ouchetto</keyname><forenames>Ouail</forenames></author><author><keyname>Roudies</keyname><forenames>Ounsa</forenames></author></authors><title>Ontology-oriented e-gov services retrieval</title><categories>cs.CY</categories><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 2, No 3, March 2012 ISSN (Online): 1694-0814</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The semantic e-government is a new application field accompanying the
development of semantic web where the ontologies have become a fertile field of
investigation. This is due firstly to both the complexity and the size of
e-government systems and secondly to the importance of the issues. However,
permitting easy and personalized access to e-government services has become, at
this juncture, an arduous and not spontaneous process. Indeed, the provided
e-gov services to the user represent a critical contact point between
administrations and users. The encountered problems in the e-gov services
retrieving process are: the absence of an integrated one-stop government, the
difficulty of localizing the services' sources, the lack of mastery of search
terms and the deficiency of multilingualism of the online services. In order to
solve these problems, to facilitate access to e-gov services and to satisfy the
needs of potential users, we propose an original approach to this issue. This
approach incorporates a semantic layer as a crucial element in the retrieving
process. It consists in implementing a personalized search system that
integrates ontology of the e-gov domain in this process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5975</identifier>
 <datestamp>2012-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5975</id><created>2012-05-27</created><authors><author><keyname>Fabregat-Traver</keyname><forenames>Diego</forenames><affiliation>AICES, RWTH Aachen</affiliation></author><author><keyname>Bientinesi</keyname><forenames>Paolo</forenames><affiliation>AICES, RWTH Aachen</affiliation></author></authors><title>A Domain-Specific Compiler for Linear Algebra Operations</title><categories>cs.MS cs.PL cs.SC</categories><report-no>AICES-2012/01-2</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a prototypical linear algebra compiler that automatically exploits
domain-specific knowledge to generate high-performance algorithms. The input to
the compiler is a target equation together with knowledge of both the structure
of the problem and the properties of the operands. The output is a variety of
high-performance algorithms, and the corresponding source code, to solve the
target equation. Our approach consists in the decomposition of the input
equation into a sequence of library-supported kernels. Since in general such a
decomposition is not unique, our compiler returns not one but a number of
algorithms. The potential of the compiler is shown by means of its application
to a challenging equation arising within the genome-wide association study. As
a result, the compiler produces multiple &quot;best&quot; algorithms that outperform the
best existing libraries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5979</identifier>
 <datestamp>2012-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5979</id><created>2012-05-27</created><authors><author><keyname>Bahmani</keyname><forenames>Elham</forenames></author><author><keyname>Hodtani</keyname><forenames>Ghosheh Abed</forenames></author></authors><title>Achievable Rate Regions for the Dirty Multiple Access Channel with
  Partial Side Information at the Transmitters</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 figures, This paper was accepted at IEEE-ISIT2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we establish achievable rate regions for the multiple access
channel (MAC) with side information partially known (estimated or sensed
version) at the transmitters. Actually, we extend the lattice strategies used
by Philosof-Zamir for the MAC with full side information at the transmitters to
the partially known case. We show that the sensed or estimated side information
reduces the rate regions, the same as that occurs for Costa Gaussian channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5980</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5980</id><created>2012-05-27</created><authors><author><keyname>Dutton</keyname><forenames>Zachary</forenames></author><author><keyname>Guha</keyname><forenames>Saikat</forenames></author><author><keyname>Wilde</keyname><forenames>Mark M.</forenames></author></authors><title>Performance of polar codes for quantum and private classical
  communication</title><categories>quant-ph cs.IT math.IT</categories><comments>8 pages, 6 figures, submission to the 50th Annual Allerton Conference
  on Communication, Control, and Computing 2012</comments><journal-ref>Proceedings of the 50th Annual Allerton Conference on
  Communication, Control, and Computing, pages 572-579, October 2012</journal-ref><doi>10.1109/Allerton.2012.6483269</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the practical performance of quantum polar codes, by computing
rigorous bounds on block error probability and by numerically simulating them.
We evaluate our bounds for quantum erasure channels with coding block lengths
between 2^10 and 2^20, and we report the results of simulations for quantum
erasure channels, quantum depolarizing channels, and &quot;BB84&quot; channels with
coding block lengths up to N = 1024. For quantum erasure channels, we observe
that high quantum data rates can be achieved for block error rates less than
10^(-4) and that somewhat lower quantum data rates can be achieved for quantum
depolarizing and BB84 channels. Our results here also serve as bounds for and
simulations of private classical data transmission over these channels,
essentially due to Renes' duality bounds for privacy amplification and
classical data transmission of complementary observables. Future work might be
able to improve upon our numerical results for quantum depolarizing and BB84
channels by employing a polar coding rule other than the heuristic used here.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5982</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5982</id><created>2012-05-27</created><authors><author><keyname>Kuniavsky</keyname><forenames>Sergey</forenames></author></authors><title>Consumer Search with Chain Stores</title><categories>cs.GT</categories><comments>38 pages. version may be updated</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper explores a consumer search setting where the sellers have
asymmetries. The model is an extension of the popular Stahl Model, which is
widely used in the literature. The extension introduces sellers with
heterogeneous stores number, reflecting the typical market structure. The
market consists of several sellers heterogeneous in size consumers, some of
which face a cost when sequentially searching. The paper shows that no
symmetric model exist in the extension and asymmetric NE of the Stahl model are
found for comparison. Additional results suggest that smallest sellers will be
the ones offering lowest prices, in line with several real world examples
provided in the paper. However, profits remain in most cases fixed per store,
making a larger firm more profitable, yet with lower sold quantity. The
findings suggest that on some level price dispersion will still exist, together
with some level of price stickiness, both observed in reality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5994</identifier>
 <datestamp>2012-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5994</id><created>2012-05-27</created><authors><author><keyname>Ramezanian</keyname><forenames>Rasoul</forenames></author></authors><title>Computation Environments, An Interactive Semantics for Turing Machines
  (which P is not equal to NP considering it)</title><categories>cs.CC math.LO</categories><comments>33 pages, interactive computation, P vs NP</comments><acm-class>F.1.1; F.1.2; F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To scrutinize notions of computation and time complexity, we introduce and
formally define an interactive model for computation that we call it the
\emph{computation environment}. A computation environment consists of two main
parts: i) a universal processor and ii) a computist who uses the computability
power of the universal processor to perform effective procedures. The notion of
computation finds it meaning, for the computist, through his
\underline{interaction} with the universal processor.
  We are interested in those computation environments which can be considered
as alternative for the real computation environment that the human being is its
computist. These computation environments must have two properties: 1- being
physically plausible, and 2- being enough powerful.
  Based on Copeland' criteria for effective procedures, we define what a
\emph{physically plausible} computation environment is.
  We construct two \emph{physically plausible} and \emph{enough powerful}
computation environments: 1- the Turing computation environment, denoted by
$E_T$, and 2- a persistently evolutionary computation environment, denoted by
$E_e$, which persistently evolve in the course of executing the computations.
  We prove that the equality of complexity classes $\mathrm{P}$ and
$\mathrm{NP}$ in the computation environment $E_e$ conflicts with the
\underline{free will} of the computist.
  We provide an axiomatic system $\mathcal{T}$ for Turing computability and
prove that ignoring just one of the axiom of $\mathcal{T}$, it would not be
possible to derive $\mathrm{P=NP}$ from the rest of axioms.
  We prove that the computist who lives inside the environment $E_T$, can never
be confident that whether he lives in a static environment or a persistently
evolutionary one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6010</identifier>
 <datestamp>2012-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6010</id><created>2012-05-27</created><authors><author><keyname>Corona</keyname><forenames>Davide</forenames></author><author><keyname>Di Benedetto</keyname><forenames>Valeria</forenames></author><author><keyname>Giancarlo</keyname><forenames>Raffaele</forenames></author><author><keyname>Utro</keyname><forenames>Filippo</forenames></author></authors><title>The Chromatin Organization of an Eukaryotic Genome : Sequence Specific+
  Statistical=Combinatorial (Extended Abstract)</title><categories>q-bio.GN cs.CE</categories><comments>Work presented at the 8th SIBBM Seminar (Annual Conference Meeting of
  the Italian Biophysics and Molecular Biology Society)- May 24-26 2012,
  Palermo, Italy</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nucleosome organization in eukaryotic genomes has a deep impact on gene
function. Although progress has been recently made in the identification of
various concurring factors influencing nucleosome positioning, it is still
unclear whether nucleosome positions are sequence dictated or determined by a
random process. It has been postulated for a long time that,in the proximity of
TSS, a barrier determines the position of the +1 nucleosome and then geometric
constraints alter the random positioning process determining nucleosomal
phasing. Such a pattern fades out as one moves away from the barrier to become
again a random positioning process. Although this statistical model is widely
accepted,the molecular nature of the barrier is still unknown. Moreover,we are
far from the identification of a set of sequence rules able:to account for the
genome-wide nucleosome organization;to explain the nature of the barriers on
which the statistical mechanism hinges;to allow for a smooth transition from
sequence-dictated to statistical positioning and back. We show that sequence
complexity,quantified via various methods, can be the rule able to at least
partially account for all the above.In particular, we have conducted our
analyses on 4 high resolution nucleosomal maps of the model eukaryotes and
found that nucleosome depleted regions can be well distinguished from
nucleosome enriched regions by sequence complexity measures.In particular, (a)
the depleted regions are less complex than the enriched ones, (b) around TSS
complexity measures alone are in striking agreement with in vivo nucleosome
occupancy,in particular precisely indicating the positions of the +1 and -1
nucleosomes. Those findings indicate that the intrinsic richness of
subsequences within sequences plays a role in nucleosomal formation in genomes,
and that sequence complexity constitutes the molecular nature of nucleosome
barrier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6018</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6018</id><created>2012-05-27</created><authors><author><keyname>Nayyar</keyname><forenames>Ashutosh</forenames></author><author><keyname>Basar</keyname><forenames>Tamer</forenames></author><author><keyname>Teneketzis</keyname><forenames>Demosthenis</forenames></author><author><keyname>Veeravalli</keyname><forenames>Venugopal V.</forenames></author></authors><title>Optimal Strategies for Communication and Remote Estimation with an
  Energy Harvesting Sensor</title><categories>cs.SY math.OC</categories><comments>32 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a remote estimation problem with an energy harvesting sensor and
a remote estimator. The sensor observes the state of a discrete-time source
which may be a finite state Markov chain or a multi-dimensional linear Gaussian
system. It harvests energy from its environment (say, for example, through a
solar cell) and uses this energy for the purpose of communicating with the
estimator. Due to the randomness of energy available for communication, the
sensor may not be able to communicate all the time. The sensor may also want to
save its energy for future communications. The estimator relies on messages
communicated by the sensor to produce real-time estimates of the source state.
We consider the problem of finding a communication scheduling strategy for the
sensor and an estimation strategy for the estimator that jointly minimize an
expected sum of communication and distortion costs over a finite time horizon.
Our goal of joint optimization leads to a decentralized decision-making
problem. By viewing the problem from the estimator's perspective, we obtain a
dynamic programming characterization for the decentralized decision-making
problem that involves optimization over functions. Under some symmetry
assumptions on the source statistics and the distortion metric, we show that an
optimal communication strategy is described by easily computable thresholds and
that the optimal estimate is a simple function of the most recently received
sensor observation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6024</identifier>
 <datestamp>2012-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6024</id><created>2012-05-27</created><updated>2012-06-08</updated><authors><author><keyname>Xiang</keyname><forenames>Biao</forenames></author><author><keyname>Chen</keyname><forenames>Enhong</forenames></author><author><keyname>Liu</keyname><forenames>Qi</forenames></author><author><keyname>Xiong</keyname><forenames>Hui</forenames></author><author><keyname>Yang</keyname><forenames>Yu</forenames></author><author><keyname>Xie</keyname><forenames>Junyuan</forenames></author></authors><title>A Social Influence Model Based On Circuit Theory</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the behaviors of information propagation is essential for the
effective exploitation of social influence in social networks. However, few
existing influence models are tractable and efficient for describing the
information propagation process, especially when dealing with the difficulty of
incorporating the effects of combined influences from multiple nodes. To this
end, in this paper, we provide a social influence model that alleviates this
obstacle based on electrical circuit theory. This model vastly improves the
efficiency of measuring the influence strength between any pair of nodes, and
can be used to interpret the real-world influence propagation process in a
coherent way. In addition, this circuit theory model provides a natural
solution to the social influence maximization problem. When applied to
realworld data, the circuit theory model consistently outperforms the
state-of-the-art methods and can greatly alleviate the computation burden of
the influence maximization problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6031</identifier>
 <datestamp>2012-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6031</id><created>2012-05-28</created><updated>2012-06-25</updated><authors><author><keyname>Shen</keyname><forenames>Wen-Jun</forenames></author><author><keyname>Wong</keyname><forenames>Hau-San</forenames></author><author><keyname>Xiao</keyname><forenames>Quan-Wu</forenames></author><author><keyname>Guo</keyname><forenames>Xin</forenames></author><author><keyname>Smale</keyname><forenames>Stephen</forenames></author></authors><title>Towards a Mathematical Foundation of Immunology and Amino Acid Chains</title><categories>stat.ML cs.LG q-bio.GN</categories><comments>updated on June 25, 2012</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We attempt to set a mathematical foundation of immunology and amino acid
chains. To measure the similarities of these chains, a kernel on strings is
defined using only the sequence of the chains and a good amino acid
substitution matrix (e.g. BLOSUM62). The kernel is used in learning machines to
predict binding affinities of peptides to human leukocyte antigens DR (HLA-DR)
molecules. On both fixed allele (Nielsen and Lund 2009) and pan-allele (Nielsen
et.al. 2010) benchmark databases, our algorithm achieves the state-of-the-art
performance. The kernel is also used to define a distance on an HLA-DR allele
set based on which a clustering analysis precisely recovers the serotype
classifications assigned by WHO (Nielsen and Lund 2009, and Marsh et.al. 2010).
These results suggest that our kernel relates well the chain structure of both
peptides and HLA-DR molecules to their biological functions, and that it offers
a simple, powerful and promising methodology to immunology and amino acid chain
studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6033</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6033</id><created>2012-05-28</created><authors><author><keyname>Delestre</keyname><forenames>Olivier</forenames><affiliation>JAD</affiliation></author><author><keyname>Lagr&#xe9;e</keyname><forenames>Pierre-Yves</forenames><affiliation>IJLRA</affiliation></author></authors><title>A &quot;well-balanced&quot; finite volume scheme for blood flow simulation</title><categories>math.NA cs.CE cs.NA</categories><comments>36 pages</comments><proxy>ccsd</proxy><journal-ref>International Journal for Numerical Methods in Fluids 72, 2 (2013)
  177-205</journal-ref><doi>10.1002/fld.3736</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are interested in simulating blood flow in arteries with a one dimensional
model. Thanks to recent developments in the analysis of hyperbolic system of
conservation laws (in the Saint-Venant/ shallow water equations context) we
will perform a simple finite volume scheme. We focus on conservation properties
of this scheme which were not previously considered. To emphasize the necessity
of this scheme, we present how a too simple numerical scheme may induce
spurious flows when the basic static shape of the radius changes. On contrary,
the proposed scheme is &quot;well-balanced&quot;: it preserves equilibria of Q = 0. Then
examples of analytical or linearized solutions with and without viscous damping
are presented to validate the calculations. The influence of abrupt change of
basic radius is emphasized in the case of an aneurism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6071</identifier>
 <datestamp>2012-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6071</id><created>2012-05-28</created><authors><author><keyname>Erd&#x151;s</keyname><forenames>D&#xf3;ra</forenames></author><author><keyname>Frank</keyname><forenames>Andr&#xe1;s</forenames></author><author><keyname>Kun</keyname><forenames>Kriszti&#xe1;n</forenames></author></authors><title>Sink-Stable Sets of Digraphs</title><categories>math.CO cs.DM</categories><msc-class>90C27</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the notion of sink-stable sets of a digraph and prove a min-max
formula for the maximum cardinality of the union of k sink-stable sets. The
results imply a recent min-max theorem of Abeledo and Atkinson on the Clar
number of bipartite plane graphs and a sharpening of Minty's coloring theorem.
We also exhibit a link to min-max results of Bessy and Thomasse and of Sebo on
cyclic stable sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6082</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6082</id><created>2012-05-28</created><updated>2012-09-10</updated><authors><author><keyname>Tancer</keyname><forenames>Martin</forenames></author><author><keyname>Tonkonog</keyname><forenames>Dmitry</forenames></author></authors><title>Good covers are algorithmically unrecognizable</title><categories>cs.CG math.AT math.GT</categories><comments>22 pages, 5 figures; result extended also to acyclic covers in
  version 2</comments><msc-class>52A30, 05E45, 57Q40, 68Q17</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A good cover in R^d is a collection of open contractible sets in R^d such
that the intersection of any subcollection is either contractible or empty.
Motivated by an analogy with convex sets, intersection patterns of good covers
were studied intensively. Our main result is that intersection patterns of good
covers are algorithmically unrecognizable.
  More precisely, the intersection pattern of a good cover can be stored in a
simplicial complex called nerve which records which subfamilies of the good
cover intersect. A simplicial complex is topologically d-representable if it is
isomorphic to the nerve of a good cover in R^d. We prove that it is
algorithmically undecidable whether a given simplicial complex is topologically
d-representable for any fixed d \geq 5. The result remains also valid if we
replace good covers with acyclic covers or with covers by open d-balls.
  As an auxiliary result we prove that if a simplicial complex is PL embeddable
into R^d, then it is topologically d-representable. We also supply this result
with showing that if a &quot;sufficiently fine&quot; subdivision of a k-dimensional
complex is d-representable and k \leq (2d-3)/3, then the complex is PL
embeddable into R^d.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6114</identifier>
 <datestamp>2012-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6114</id><created>2012-04-25</created><authors><author><keyname>Aswani</keyname><forenames>Anil</forenames></author><author><keyname>Master</keyname><forenames>Neal</forenames></author><author><keyname>Taneja</keyname><forenames>Jay</forenames></author><author><keyname>Krioukov</keyname><forenames>Andrew</forenames></author><author><keyname>Culler</keyname><forenames>David</forenames></author><author><keyname>Tomlin</keyname><forenames>Claire</forenames></author></authors><title>Quantitative Methods for Comparing Different HVAC Control Schemes</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Experimentally comparing the energy usage and comfort characteristics of
different controllers in heating, ventilation, and air-conditioning (HVAC)
systems is difficult because variations in weather and occupancy conditions
preclude the possibility of establishing equivalent experimental conditions
across the order of hours, days, and weeks. This paper is concerned with
defining quantitative metrics of energy usage and occupant comfort, which can
be computed and compared in a rigorous manner that is capable of determining
whether differences between controllers are statistically significant in the
presence of such environmental fluctuations. Experimental case studies are
presented that compare two alternative controllers (a schedule controller and a
hybrid system learning-based model predictive controller) to the default
controller in a building-wide HVAC system. Lastly, we discuss how our proposed
methodology may also be able to quantify the efficiency of other building
automation systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6123</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6123</id><created>2012-04-29</created><authors><author><keyname>Akram</keyname><forenames>Muhammad</forenames></author><author><keyname>Dudek</keyname><forenames>Wieslaw A.</forenames></author></authors><title>Interval-valued fuzzy graphs</title><categories>cs.DM</categories><journal-ref>Computers and Mathematics with Applications 61 (2011), 289-299</journal-ref><doi>10.1016/j.camwa.2010.11.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define the Cartesian product, composition, union and join on
interval-valued fuzzy graphs and investigate some of their properties. We also
introduce the notion of interval-valued fuzzy complete graphs and present some
properties of self complementary and self weak complementary interval-valued
fuzzy complete graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6152</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6152</id><created>2012-05-28</created><updated>2015-12-28</updated><authors><author><keyname>Wei</keyname><forenames>Li</forenames></author><author><keyname>Xu</keyname><forenames>Youyun</forenames></author><author><keyname>Cai</keyname><forenames>Yueming</forenames></author><author><keyname>Xu</keyname><forenames>Xin</forenames></author></authors><title>Robust frequency offset estimator for OFDM over fast varying multipath
  channel</title><categories>cs.IT math.IT</categories><comments>5 pages; 2 figures</comments><journal-ref>Electron. Lett., vol. 43, No. 6, pp. 355-356 (2007)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a robust carrier frequency offset(CFO) estimation
algorithm suitable for fast varying multipath channels. The proposed algorithm
estimates CFO both in time-domain and frequency-domain using two carefully
designed sequences. This novel technique possesses high accuracy as well as
large estimation range and works well in fast varying channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6154</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6154</id><created>2012-05-28</created><authors><author><keyname>Shabat</keyname><forenames>Gil</forenames></author></authors><title>Potentials and Limits of Super-Resolution Algorithms and Signal
  Reconstruction from Sparse Data</title><categories>physics.optics cs.CV math-ph math.MP</categories><journal-ref>Journal of the Optical Society A 26(3), pp. 566-575, 2009</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A common distortion in videos is image instability in the form of chaotic
(global and local displacements). Those instabilities can be used to enhance
image resolution by using subpixel elastic registration. In this work, we
investigate the performance of such methods over the ability to improve the
resolution by accumulating several frames. The second part of this work deals
with reconstruction of discrete signals from a subset of samples under
different basis functions such as DFT, Haar, Walsh, Daubechies wavelets and CT
(Radon) projections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6162</identifier>
 <datestamp>2012-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6162</id><created>2012-05-28</created><authors><author><keyname>Ahmad</keyname><forenames>Faudziah</forenames></author><author><keyname>Baharom</keyname><forenames>Fauziah</forenames></author><author><keyname>Husni</keyname><forenames>Moath</forenames></author></authors><title>Current Web Application Development and Measurement Practices for Small
  Software Firms</title><categories>cs.SE</categories><comments>5 pages, 2 figures an 9 tables; IJCSI 2012</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper discusses issues on current development and measurement practices
that were identified from a pilot study conducted on Jordanian small software
firms. The study was to investigate whether developers follow development and
measurement best practices in web applications development. The analysis was
conducted in two stages: first, grouping the development and measurement
practices using variable clustering, and second, identifying the acceptance
degree. Mean interval was used to determine the degree of acceptance.
Hierarchal clustering was used to group the development and measurement
practices. The actual findings of this survey will be used for building a new
methodology for developing web applications in small software firms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6177</identifier>
 <datestamp>2012-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6177</id><created>2012-05-28</created><authors><author><keyname>Bergstra</keyname><forenames>Jan A.</forenames></author></authors><title>Decision Taking versus Action Determination</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decision taking is discussed in the context of the role it may play for
various types of agents, and it is contrasted with action determination. Some
remarks are made about the role of decision taking and action determination in
the ongoing debate concerning the reverse polder development of the hertogin
Hedwige polder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6179</identifier>
 <datestamp>2012-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6179</id><created>2012-05-28</created><authors><author><keyname>Mohammadi</keyname><forenames>Maryam</forenames></author><author><keyname>Tap</keyname><forenames>Masine Md.</forenames></author></authors><title>A Mixed Integer Programming Model Formulation for Solving the Lot-Sizing
  Problem</title><categories>math.OC cs.AI</categories><comments>9 pages, 1 figure, 13 tables; International Journal of Computer
  Science Issues, Vol. 9, Issue 2, No 1, March 2012</comments><report-no>IJCSI-9-2-1-28-36</report-no><journal-ref>International Journal of Computer Science Issues, Vol. 9, Issue 2,
  No 1, Pages 28-36, March 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses a mixed integer programming (MIP) formulation for the
multi-item uncapacitated lot-sizing problem that is inspired from the trailer
manufacturer. The proposed MIP model has been utilized to find out the optimum
order quantity, optimum order time, and the minimum total cost of purchasing,
ordering, and holding over the predefined planning horizon. This problem is
known as NP-hard problem. The model was presented in an optimal software form
using LINGO 13.0.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6184</identifier>
 <datestamp>2013-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6184</id><created>2012-05-28</created><updated>2013-09-03</updated><authors><author><keyname>Ballico</keyname><forenames>Edoardo</forenames></author><author><keyname>Ravagnani</keyname><forenames>Alberto</forenames></author></authors><title>On the duals of geometric Goppa codes from norm-trace curves</title><categories>math.AG cs.IT math.IT</categories><comments>Finite Fields and their Applications, vol. 20 (2013), pp. 30-39</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the dual codes of a wide family of evaluation codes on
norm-trace curves. We explicitly find out their minimum distance and give a
lower bound for the number of their minimum-weight codewords. A general
geometric approach is performed and applied to study in particular the dual
codes of one-point and two-point codes arising from norm-trace curves through
Goppa's construction, providing in many cases their minimum distance and some
bounds on the number of their minimum-weight codewords. The results are
obtained by showing that the supports of the minimum-weight codewords of the
studied codes obey some precise geometric laws as zero-dimensional subschemes
of the projective plane. Finally, the dimension of some classical two-point
Goppa codes on norm-trace curves is explicitly computed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6185</identifier>
 <datestamp>2012-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6185</id><created>2012-05-28</created><updated>2012-06-01</updated><authors><author><keyname>Odeh</keyname><forenames>Maha</forenames></author><author><keyname>Zorba</keyname><forenames>Nizar</forenames></author><author><keyname>Verikoukis</keyname><forenames>Christos</forenames></author></authors><title>Power Consumption in Spatial Cognition</title><categories>cs.IT math.IT</categories><comments>this paper has been withdrawn</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple Input Multiple Output (MIMO) adds a new dimension to be exploited in
Cognitive Radio (CR) by simultaneously serving several users. The spatial
domain that is added through MIMO is another system resource that has to be
optimized, and shared when possible. In this paper, we present a spatial
sharing that is carried out through Zero Forcing beamforming (ZFB). Power
consumption in such a scenario is discussed and compared to single user case,
to evaluate the feasibility of employing spatial cognition from the power
perspective. Closed form expressions are derived for the consumed power and
data rate at different transmission schemes. Finally, a joint power rate metric
is deduced to provide a comprehensive measure of the expediency of spatial
cognitive scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6186</identifier>
 <datestamp>2012-08-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6186</id><created>2012-05-28</created><updated>2012-08-08</updated><authors><author><keyname>Shomorony</keyname><forenames>Ilan</forenames></author><author><keyname>Etkin</keyname><forenames>Ra&#xfa;l</forenames></author><author><keyname>Parvaresh</keyname><forenames>Farzad</forenames></author><author><keyname>Avestimehr</keyname><forenames>A. Salman</forenames></author></authors><title>Diamond Networks with Bursty Traffic: Bounds on the Minimum
  Energy-Per-Bit</title><categories>cs.IT math.IT</categories><comments>Several proofs were updated</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When data traffic in a wireless network is bursty, small amounts of data
sporadically become available for transmission, at times that are unknown at
the receivers, and an extra amount of energy must be spent at the transmitters
to overcome this lack of synchronization between the network nodes. In
practice, pre-defined header sequences are used with the purpose of
synchronizing the different network nodes. However, in networks where relays
must be used for communication, the overhead required for synchronizing the
entire network may be very significant.
  In this work, we study the fundamental limits of energy-efficient
communication in an asynchronous diamond network with two relays. We formalize
the notion of relay synchronization by saying that a relay is synchronized if
the conditional entropy of the arrival time of the source message given the
received signals at the relay is small. We show that the minimum energy-per-bit
for bursty traffic in diamond networks is achieved with a coding scheme where
each relay is either synchronized or not used at all. A consequence of this
result is the derivation of a lower bound to the minimum energy-per-bit for
bursty communication in diamond networks. This bound allows us to show that
schemes that perform the tasks of synchronization and communication separately
(i.e., with synchronization signals preceding the communication block) can
achieve the minimum energy-per-bit to within a constant fraction that ranges
from 2 in the synchronous case to 1 in the highly asynchronous regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6192</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6192</id><created>2012-05-28</created><updated>2014-04-30</updated><authors><author><keyname>Schuster</keyname><forenames>Johann</forenames></author><author><keyname>Siegle</keyname><forenames>Markus</forenames></author></authors><title>Markov Automata: Deciding Weak Bisimulation by means of non-naively
  Vanishing States</title><categories>cs.LO cs.FL</categories><comments>Main results stay the same as in the previous versions, but the
  proofs have completely changed</comments><msc-class>03-02</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops a decision algorithm for weak bisimulation on Markov
Automata (MA). For that purpose, different notions of vanishing state (a
concept known from the area of Generalised Stochastic Petri Nets) are defined.
Vanishing states are shown to be essential for relating the concepts of
(state-based) naive weak bisimulation and (distribution-based) weak
bisimulation. The bisimulation algorithm presented here follows the
partition-refinement scheme and has exponential time complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6210</identifier>
 <datestamp>2012-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6210</id><created>2012-05-28</created><updated>2012-10-17</updated><authors><author><keyname>Sigg</keyname><forenames>Christian D.</forenames></author><author><keyname>Dikk</keyname><forenames>Tomas</forenames></author><author><keyname>Buhmann</keyname><forenames>Joachim M.</forenames></author></authors><title>Learning Dictionaries with Bounded Self-Coherence</title><categories>stat.ML cs.LG</categories><comments>4 pages, 2 figures; IEEE Signal Processing Letters, vol. 19, no. 12,
  2012</comments><doi>10.1109/LSP.2012.2223757</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse coding in learned dictionaries has been established as a successful
approach for signal denoising, source separation and solving inverse problems
in general. A dictionary learning method adapts an initial dictionary to a
particular signal class by iteratively computing an approximate factorization
of a training data matrix into a dictionary and a sparse coding matrix. The
learned dictionary is characterized by two properties: the coherence of the
dictionary to observations of the signal class, and the self-coherence of the
dictionary atoms. A high coherence to the signal class enables the sparse
coding of signal observations with a small approximation error, while a low
self-coherence of the atoms guarantees atom recovery and a more rapid residual
error decay rate for the sparse coding algorithm. The two goals of high signal
coherence and low self-coherence are typically in conflict, therefore one seeks
a trade-off between them, depending on the application. We present a dictionary
learning method with an effective control over the self-coherence of the
trained dictionary, enabling a trade-off between maximizing the sparsity of
codings and approximating an equiangular tight frame.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6218</identifier>
 <datestamp>2013-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6218</id><created>2012-05-28</created><updated>2013-04-17</updated><authors><author><keyname>Moore</keyname><forenames>Cristopher</forenames></author><author><keyname>Russell</keyname><forenames>Alexander</forenames></author></authors><title>Optimal epsilon-biased sets with just a little randomness</title><categories>cs.CC math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Subsets of F_2^n that are eps-biased, meaning that the parity of any set of
bits is even or odd with probability eps close to 1/2, are powerful tools for
derandomization. A simple randomized construction shows that such sets exist of
size O(n/eps^2), and known deterministic constructions achieve sets of size
O(n/eps^3), O(n^2/eps^2), and O((n/eps^2)^{5/4}). Rather than derandomizing
these sets completely in exchange for making them larger, we attempt a partial
derandomization while keeping them small, constructing sets of size O(n/eps^2)
with as few random bits as possible. The naive randomized construction requires
O(n^2/eps^2) random bits. We give two constructions. The first uses Nisan's
space-bounded pseudorandom generator to partly derandomize a folklore
probabilistic construction of an error-correcting code, and requires O(n log
(1/eps)) bits. Our second construction requires O(n log (n/eps)) bits, but is
more elementary; it adds randomness to a Legendre symbol construction on Alon,
Goldreich, H{\aa}stad, and Peralta, and uses Weil sums to bound high moments of
the bias.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6228</identifier>
 <datestamp>2012-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6228</id><created>2012-05-28</created><updated>2012-09-25</updated><authors><author><keyname>Yang</keyname><forenames>Jaewon</forenames></author><author><keyname>Leskovec</keyname><forenames>Jure</forenames></author></authors><title>Structure and Overlaps of Communities in Networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the main organizing principles in real-world social, information and
technological networks is that of network communities, where sets of nodes
organize into densely linked clusters. Even though detection of such
communities is of great interest, understanding the structure communities in
large networks remains relatively limited. Due to unavailability of labeled
ground-truth data it is practically impossible to evaluate and compare
different models and notions of communities on a large scale.
  In this paper we identify 6 large social, collaboration, and information
networks where nodes explicitly state their community memberships. We define
ground-truth communities by using these explicit memberships. We then
empirically study how such ground-truth communities emerge in networks and how
they overlap. We observe some surprising phenomena. First, ground-truth
communities contain high-degree hub nodes that reside in community overlaps and
link to most of the members of the community. Second, the overlaps of
communities are more densely connected than the non-overlapping parts of
communities, in contrast to the conventional wisdom that community overlaps are
more sparsely connected than the communities themselves.
  Existing models of network communities do not capture dense community
overlaps. We present the Community-Affiliation Graph Model (AGM), a conceptual
model of network community structure, which reliably captures the overall
structure of networks as well as the overlapping nature of network communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6229</identifier>
 <datestamp>2012-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6229</id><created>2012-05-28</created><authors><author><keyname>Reza</keyname><forenames>Md. Selim</forenames></author><author><keyname>Khan</keyname><forenames>Mohammed Shafiul Alam</forenames></author><author><keyname>Alam</keyname><forenames>Md. Golam Robiul</forenames></author><author><keyname>Islam</keyname><forenames>Serajul</forenames></author></authors><title>An Approach of Digital Image Copyright Protection by Using Watermarking
  Technology</title><categories>cs.CR</categories><comments>7 pages, 6 figures. arXiv admin note: text overlap with
  arXiv:1103.3802 by other authors</comments><journal-ref>International Journal of Computer Science Issues, Vol. 9, Issue 2,
  No 2, 2012, pp:280-286</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Digital watermarking system is a paramount for safeguarding valuable
resources and information. Digital watermarks are generally imperceptible to
the human eye and ear. Digital watermark can be used in video, audio and
digital images for a wide variety of applications such as copy prevention right
management, authentication and filtering of internet content. The proposed
system is able to protect copyright or owner identification of digital media,
such as audio, image, video, or text. The system permutated the watermark and
embed the permutated watermark into the wavelet coefficients of the original
image by using a key. The key is randomly generated and used to select the
locations in the wavelet domain in which to embed the permutated watermark.
Finally, the system combines the concept of cryptography and digital
watermarking techniques to implement a more secure digital watermarking system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6233</identifier>
 <datestamp>2012-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6233</id><created>2012-05-28</created><updated>2012-11-06</updated><authors><author><keyname>Yang</keyname><forenames>Jaewon</forenames></author><author><keyname>Leskovec</keyname><forenames>Jure</forenames></author></authors><title>Defining and Evaluating Network Communities based on Ground-truth</title><categories>cs.SI physics.soc-ph</categories><comments>Proceedings of 2012 IEEE International Conference on Data Mining
  (ICDM), 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nodes in real-world networks organize into densely linked communities where
edges appear with high concentration among the members of the community.
Identifying such communities of nodes has proven to be a challenging task
mainly due to a plethora of definitions of a community, intractability of
algorithms, issues with evaluation and the lack of a reliable gold-standard
ground-truth.
  In this paper we study a set of 230 large real-world social, collaboration
and information networks where nodes explicitly state their group memberships.
For example, in social networks nodes explicitly join various interest based
social groups. We use such groups to define a reliable and robust notion of
ground-truth communities. We then propose a methodology which allows us to
compare and quantitatively evaluate how different structural definitions of
network communities correspond to ground-truth communities. We choose 13
commonly used structural definitions of network communities and examine their
sensitivity, robustness and performance in identifying the ground-truth. We
show that the 13 structural definitions are heavily correlated and naturally
group into four classes. We find that two of these definitions, Conductance and
Triad-participation-ratio, consistently give the best performance in
identifying ground-truth communities. We also investigate a task of detecting
communities given a single seed node. We extend the local spectral clustering
algorithm into a heuristic parameter-free community detection method that
easily scales to networks with more than hundred million nodes. The proposed
method achieves 30% relative improvement over current local clustering methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6249</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6249</id><created>2012-05-28</created><authors><author><keyname>Dereniowski</keyname><forenames>Dariusz</forenames></author><author><keyname>Pelc</keyname><forenames>Andrzej</forenames></author></authors><title>Leader Election for Anonymous Asynchronous Agents in Arbitrary Networks</title><categories>cs.DC</categories><journal-ref>Distributed Computing 27 (2014) 21-38</journal-ref><doi>10.1007/s00446-013-0196-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of leader election among mobile agents operating in an
arbitrary network modeled as an undirected graph. Nodes of the network are
unlabeled and all agents are identical. Hence the only way to elect a leader
among agents is by exploiting asymmetries in their initial positions in the
graph. Agents do not know the graph or their positions in it, hence they must
gain this knowledge by navigating in the graph and share it with other agents
to accomplish leader election. This can be done using meetings of agents, which
is difficult because of their asynchronous nature: an adversary has total
control over the speed of agents. When can a leader be elected in this
adversarial scenario and how to do it? We give a complete answer to this
question by characterizing all initial configurations for which leader election
is possible and by constructing an algorithm that accomplishes leader election
for all configurations for which this can be done.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6252</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6252</id><created>2012-05-28</created><authors><author><keyname>Mehrabian</keyname><forenames>Abbas</forenames></author><author><keyname>Wormald</keyname><forenames>Nick</forenames></author></authors><title>On the Stretch Factor of Randomly Embedded Random Graphs</title><categories>cs.CG math.CO</categories><comments>12 pages</comments><journal-ref>Discrete &amp; Computational Geometry: Volume 49, Issue 3 (2013), Page
  647-658</journal-ref><doi>10.1007/s00454-012-9482-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a random graph G(n,p) whose vertex set V has been randomly
embedded in the unit square and whose edges are given weight equal to the
geometric distance between their end vertices. Then each pair {u,v} of vertices
have a distance in the weighted graph, and a Euclidean distance. The stretch
factor of the embedded graph is defined as the maximum ratio of these two
distances, over all u,v in V. We give upper and lower bounds on the stretch
factor (holding asymptotically almost surely), and show that for p not too
close to 0 or 1, these bounds are best possible in a certain sense. Our results
imply that the stretch factor is bounded with probability tending to 1 if and
only if n(1-p) tends to 0, answering a question of O'Rourke.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6256</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6256</id><created>2012-05-29</created><updated>2013-10-09</updated><authors><author><keyname>Van Pham</keyname><forenames>Trung</forenames></author><author><keyname>Phan</keyname><forenames>Thi Ha Duong</forenames></author></authors><title>Lattices generated by Chip Firing Game models: criteria and recognition
  algorithm</title><categories>cs.DM</categories><comments>Some informations about the term &quot;Abelian Sandpile model&quot; have been
  added to this version</comments><journal-ref>European Journal of Combinatorics 34 (2013) pp. 812-832</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well-known that the class of lattices generated by Chip Firing games
(CFGs) is strictly included in the class of upper locally distributive lattices
(ULD). However a necessary and sufficient criterion for this class is still an
open question. In this paper we settle this problem by giving such a criterion.
This criterion provides a polynomial-time algorithm for constructing a CFG
which generates a given lattice if such a CFG exists. Going further we solve
the same problem on two other classes of lattices which are generated by CFGs
on the classes of undirected graphs and directed acyclic graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6278</identifier>
 <datestamp>2012-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6278</id><created>2012-05-29</created><authors><author><keyname>&#x160;uvakov</keyname><forenames>Milovan</forenames></author><author><keyname>Garcia</keyname><forenames>David</forenames></author><author><keyname>Schweitzer</keyname><forenames>Frank</forenames></author><author><keyname>Tadi&#x107;</keyname><forenames>Bosiljka</forenames></author></authors><title>Agent-based simulations of emotion spreading in online social networks</title><categories>physics.soc-ph cs.SI</categories><comments>21 pages, 13 figures</comments><report-no>IJS-F1 preprint 12/08</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantitative analysis of empirical data from online social networks reveals
group dynamics in which emotions are involved (\v{S}uvakov et al). Full
understanding of the underlying mechanisms, however, remains a challenging
task. Using agent-based computer simulations, in this paper we study dynamics
of emotional communications in online social networks. The rules that guide how
the agents interact are motivated, and the realistic network structure and some
important parameters are inferred from the empirical dataset of
\texttt{MySpace} social network. Agent's emotional state is characterized by
two variables representing psychological arousal---reactivity to stimuli, and
valence---attractiveness or aversiveness, by which common emotions can be
defined. Agent's action is triggered by increased arousal. High-resolution
dynamics is implemented where each message carrying agent's emotion along the
network link is identified and its effect on the recipient agent is considered
as continuously aging in time. Our results demonstrate that (i) aggregated
group behaviors may arise from individual emotional actions of agents; (ii)
collective states characterized by temporal correlations and dominant positive
emotions emerge, similar to the empirical system; (iii) nature of the driving
signal---rate of user's stepping into online world, has profound effects on
building the coherent behaviors, which are observed for users in online social
networks. Further, our simulations suggest that spreading patterns differ for
the emotions, e.g., &quot;enthusiastic&quot; and &quot;ashamed&quot;, which have entirely different
emotional content. {\bf {All data used in this study are fully anonymized.}}
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6285</identifier>
 <datestamp>2012-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6285</id><created>2012-05-29</created><authors><author><keyname>Wilson</keyname><forenames>David J.</forenames></author><author><keyname>Bradford</keyname><forenames>Russell J.</forenames></author><author><keyname>Davenport</keyname><forenames>James H.</forenames></author></authors><title>Speeding up Cylindrical Algebraic Decomposition by Gr\&quot;obner Bases</title><categories>cs.SC math.AG</categories><comments>To appear in Proc. CICM 2012, LNCS 7362</comments><msc-class>68W30</msc-class><acm-class>G.4</acm-class><journal-ref>Proc. CICM 2012, Springer LNCS 7362, pp. 279-293</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gr\&quot;obner Bases and Cylindrical Algebraic Decomposition are generally thought
of as two, rather different, methods of looking at systems of equations and, in
the case of Cylindrical Algebraic Decomposition, inequalities. However, even
for a mixed system of equalities and inequalities, it is possible to apply
Gr\&quot;obner bases to the (conjoined) equalities before invoking CAD. We see that
this is, quite often but not always, a beneficial preconditioning of the CAD
problem.
  It is also possible to precondition the (conjoined) inequalities with respect
to the equalities, and this can also be useful in many cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6309</identifier>
 <datestamp>2012-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6309</id><created>2012-05-29</created><authors><author><keyname>Ho</keyname><forenames>Zuleita</forenames></author><author><keyname>Jorswieck</keyname><forenames>Eduard</forenames></author></authors><title>Improper Signaling on the Two-user SISO Interference Channel</title><categories>cs.IT math.IT</categories><comments>accepted, to appear in IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  On a single-input-single-out (SISO) interference channel (IC), conventional
non-cooperative strategies encourage players selfishly maximizing their
transmit data rates, neglecting the deficit of performance caused by and to
other players. In the case of proper complex Gaussian noise, the maximum
entropy theorem shows that the best-response strategy is to transmit with
proper signals (symmetric complex Gaussian symbols). However, such equilibrium
leads to degrees-of-freedom zero due to the saturation of interference.
  With improper signals (asymmetric complex Gaussian symbols), an extra freedom
of optimization is available. In this paper, we study the impact of improper
signaling on the 2-user SISO IC. We explore the achievable rate region with
non-cooperative strategies by computing a Nash equilibrium of a non-cooperative
game with improper signaling. Then, assuming cooperation between players, we
study the achievable rate region of improper signals. We propose the usage of
improper rank one signals for their simplicity and ease of implementation.
Despite their simplicity, rank one signals achieve close to optimal sum rate
compared to full rank improper signals. We characterize the Pareto boundary,
the outer-boundary of the achievable rate region, of improper rank one signals
with a single real-valued parameter; we compute the closed-form solution of the
Pareto boundary with the non-zero-forcing strategies, the maximum sum rate
point and the max-min fairness solution with zero-forcing strategies. Analysis
on the extreme SNR regimes shows that proper signals maximize the wide-band
slope of spectral efficiency whereas improper signals optimize the high-SNR
power offset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6326</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6326</id><created>2012-05-29</created><updated>2012-11-05</updated><authors><author><keyname>Chalupka</keyname><forenames>Krzysztof</forenames></author><author><keyname>Williams</keyname><forenames>Christopher K. I.</forenames></author><author><keyname>Murray</keyname><forenames>Iain</forenames></author></authors><title>A Framework for Evaluating Approximation Methods for Gaussian Process
  Regression</title><categories>stat.ML cs.LG stat.CO</categories><comments>19 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gaussian process (GP) predictors are an important component of many Bayesian
approaches to machine learning. However, even a straightforward implementation
of Gaussian process regression (GPR) requires O(n^2) space and O(n^3) time for
a dataset of n examples. Several approximation methods have been proposed, but
there is a lack of understanding of the relative merits of the different
approximations, and in what situations they are most useful. We recommend
assessing the quality of the predictions obtained as a function of the compute
time taken, and comparing to standard baselines (e.g., Subset of Data and
FITC). We empirically investigate four different approximation algorithms on
four different prediction problems, and make our code available to encourage
future comparisons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6343</identifier>
 <datestamp>2012-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6343</id><created>2012-05-29</created><authors><author><keyname>Frahm</keyname><forenames>K. M.</forenames></author><author><keyname>Chepelianskii</keyname><forenames>A. D.</forenames></author><author><keyname>Shepelyansky</keyname><forenames>D. L.</forenames></author></authors><title>PageRank of integers</title><categories>cs.IR cond-mat.stat-mech math.NT nlin.CD</categories><comments>Research at http://www.quantware.ups-tlse.fr/, 22 pages, 14 figures</comments><journal-ref>J. Phys. A: Math. Theor. 45, 405101(2012)</journal-ref><doi>10.1088/1751-8113/45/40/405101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We build up a directed network tracing links from a given integer to its
divisors and analyze the properties of the Google matrix of this network. The
PageRank vector of this matrix is computed numerically and it is shown that its
probability is inversely proportional to the PageRank index thus being similar
to the Zipf law and the dependence established for the World Wide Web. The
spectrum of the Google matrix of integers is characterized by a large gap and a
relatively small number of nonzero eigenvalues. A simple semi-analytical
expression for the PageRank of integers is derived that allows to find this
vector for matrices of billion size. This network provides a new PageRank order
of integers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6346</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6346</id><created>2012-05-29</created><updated>2013-02-26</updated><authors><author><keyname>Brihaye</keyname><forenames>Thomas</forenames></author><author><keyname>Bruy&#xe8;re</keyname><forenames>V&#xe9;ronique</forenames></author><author><keyname>De Pril</keyname><forenames>Julie</forenames></author><author><keyname>Gimbert</keyname><forenames>Hugo</forenames></author></authors><title>On (Subgame Perfect) Secure Equilibrium in Quantitative Reachability
  Games</title><categories>cs.GT cs.LO</categories><comments>32 pages. Full version of the FoSSaCS 2012 proceedings paper</comments><proxy>Logical Methods In Computer Science</proxy><acm-class>D.2.4</acm-class><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 1 (February
  28, 2013) lmcs:790</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study turn-based quantitative multiplayer non zero-sum games played on
finite graphs with reachability objectives. In such games, each player aims at
reaching his own goal set of states as soon as possible. A previous work on
this model showed that Nash equilibria (resp. secure equilibria) are guaranteed
to exist in the multiplayer (resp. two-player) case. The existence of secure
equilibria in the multiplayer case remained and is still an open problem. In
this paper, we focus our study on the concept of subgame perfect equilibrium, a
refinement of Nash equilibrium well-suited in the framework of games played on
graphs. We also introduce the new concept of subgame perfect secure
equilibrium. We prove the existence of subgame perfect equilibria (resp.
subgame perfect secure equilibria) in multiplayer (resp. two-player)
quantitative reachability games. Moreover, we provide an algorithm deciding the
existence of secure equilibria in the multiplayer case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6349</identifier>
 <datestamp>2012-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6349</id><created>2012-05-29</created><updated>2012-05-31</updated><authors><author><keyname>Wang</keyname><forenames>Wen Qiang</forenames></author><author><keyname>Anh</keyname><forenames>Dinh Tien Tuan</forenames></author><author><keyname>Lim</keyname><forenames>Hock Beng</forenames></author><author><keyname>Datta</keyname><forenames>Anwitaman</forenames></author></authors><title>Cloud and the City: Facilitating Flexible Access Control over Data
  Streams</title><categories>cs.CR cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The proliferation of sensing devices create plethora of data-streams, which
in turn can be harnessed to carry out sophisticated analytics to support
various real-time applications and services as well as long-term planning,
e.g., in the context of intelligent cities or smart homes to name a few
prominent ones. A mature cloud infrastructure brings such a vision closer to
reality than ever before. However, we believe that the ability for data-owners
to flexibly and easily to control the granularity at which they share their
data with other entities is very important - in making data owners feel
comfortable to share to start with, and also to leverage on such fine-grained
control to realize different business models or logics. In this paper, we
explore some basic operations to flexibly control the access on a data stream
and propose a framework eXACML+ that extends OASIS's XACML model to achieve the
same. We develop a prototype using the commercial StreamBase engine to
demonstrate a seamless combination of stream data processing with (a small but
important selected set of) fine-grained access control mechanisms, and study
the framework's efficacy based on experiments in cloud like environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6352</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6352</id><created>2012-05-29</created><updated>2012-09-13</updated><authors><author><keyname>Kolmogorov</keyname><forenames>Vladimir</forenames></author><author><keyname>Schoenemann</keyname><forenames>Thomas</forenames></author></authors><title>Generalized sequential tree-reweighted message passing</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of approximate MAP-MRF inference in general
graphical models. Following [36], we consider a family of linear programming
relaxations of the problem where each relaxation is specified by a set of
nested pairs of factors for which the marginalization constraint needs to be
enforced. We develop a generalization of the TRW-S algorithm [9] for this
problem, where we use a decomposition into junction chains, monotonic w.r.t.
some ordering on the nodes. This generalizes the monotonic chains in [9] in a
natural way. We also show how to deal with nested factors in an efficient way.
Experiments show an improvement over min-sum diffusion, MPLP and subgradient
ascent algorithms on a number of computer vision and natural language
processing problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6359</identifier>
 <datestamp>2012-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6359</id><created>2012-05-29</created><updated>2012-06-28</updated><authors><author><keyname>Deepak</keyname><forenames>Akshay</forenames></author><author><keyname>Fern&#xe1;ndez-Baca</keyname><forenames>David</forenames></author><author><keyname>McMahon</keyname><forenames>Michelle M.</forenames></author></authors><title>Extracting Conflict-free Information from Multi-labeled Trees</title><categories>cs.DS q-bio.PE</categories><comments>Submitted in Workshop on Algorithms in Bioinformatics 2012
  (http://algo12.fri.uni-lj.si/?file=wabi)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A multi-labeled tree, or MUL-tree, is a phylogenetic tree where two or more
leaves share a label, e.g., a species name. A MUL-tree can imply multiple
conflicting phylogenetic relationships for the same set of taxa, but can also
contain conflict-free information that is of interest and yet is not obvious.
We define the information content of a MUL-tree T as the set of all
conflict-free quartet topologies implied by T, and define the maximal reduced
form of T as the smallest tree that can be obtained from T by pruning leaves
and contracting edges while retaining the same information content. We show
that any two MUL-trees with the same information content exhibit the same
reduced form. This introduces an equivalence relation in MUL-trees with
potential applications to comparing MUL-trees. We present an efficient
algorithm to reduce a MUL-tree to its maximally reduced form and evaluate its
performance on empirical datasets in terms of both quality of the reduced tree
and the degree of data reduction achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6361</identifier>
 <datestamp>2012-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6361</id><created>2012-05-29</created><authors><author><keyname>Kimmig</keyname><forenames>Markus</forenames><affiliation>INRIA Lille - Nord Europe, LIFL</affiliation></author><author><keyname>Monperrus</keyname><forenames>Martin</forenames><affiliation>INRIA Lille - Nord Europe, LIFL</affiliation></author><author><keyname>Mezini</keyname><forenames>Mira</forenames></author></authors><title>Querying Source Code with Natural Language</title><categories>cs.SE</categories><proxy>ccsd</proxy><journal-ref>26th IEEE/ACM International Conference On Automated Software
  Engineering (2011) 376-379</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One common task of developing or maintaining software is searching the source
code for information like specific method calls or write accesses to certain
fields. This kind of information is required to correctly implement new
features and to solve bugs. This paper presents an approach for querying source
code with natural language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6363</identifier>
 <datestamp>2012-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6363</id><created>2012-05-29</created><authors><author><keyname>Monperrus</keyname><forenames>Martin</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Eichberg</keyname><forenames>Michael</forenames></author><author><keyname>Tekes</keyname><forenames>Elif</forenames></author><author><keyname>Mezini</keyname><forenames>Mira</forenames></author></authors><title>What Should Developers Be Aware Of? An Empirical Study on the Directives
  of API Documentation</title><categories>cs.SE</categories><comments>Empirical Software Engineering (2011)</comments><proxy>ccsd</proxy><doi>10.1007/s10664-011-9186-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Application Programming Interfaces (API) are exposed to developers in order
to reuse software libraries. API directives are natural-language statements in
API documentation that make developers aware of constraints and guidelines
related to the usage of an API. This paper presents the design and the results
of an empirical study on the directives of API documentation of object-oriented
libraries. Its main contribution is to propose and extensively discuss a
taxonomy of 23 kinds of API directives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6373</identifier>
 <datestamp>2012-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6373</id><created>2012-05-29</created><authors><author><keyname>Burnside</keyname><forenames>Gerard</forenames></author><author><keyname>Hong</keyname><forenames>Dohy</forenames></author><author><keyname>Nguyen-Kim</keyname><forenames>Son</forenames></author><author><keyname>Liu</keyname><forenames>Liang</forenames></author></authors><title>Publication Induced Research Analysis (PIRA) - Experiments on Real Data</title><categories>cs.DL cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the first results obtained by implementing a novel
approach to rank vertices in a heterogeneous graph, based on the PageRank
family of algorithms and applied here to the bipartite graph of papers and
authors as a first evaluation of its relevance on real data samples. With this
approach to evaluate research activities, the ranking of a paper/author depends
on that of the papers/authors citing it/him or her. We compare the results
against existing ranking methods (including methods which simply apply PageRank
to the graph of papers or the graph of authors) through the analysis of simple
scenarios based on a real dataset built from DBLP and CiteseerX. The results
show that in all examined cases the obtained result is most pertinent with our
method which allows to orient our future work to optimizing the execution of
this algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6376</identifier>
 <datestamp>2012-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6376</id><created>2012-05-29</created><authors><author><keyname>Granados</keyname><forenames>Ana</forenames></author></authors><title>Analysis and study on text representation to improve the accuracy of the
  Normalized Compression Distance</title><categories>cs.IT math.IT</categories><comments>PhD Thesis; 202 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The huge amount of information stored in text form makes methods that deal
with texts really interesting. This thesis focuses on dealing with texts using
compression distances. More specifically, the thesis takes a small step towards
understanding both the nature of texts and the nature of compression distances.
Broadly speaking, the way in which this is done is exploring the effects that
several distortion techniques have on one of the most successful distances in
the family of compression distances, the Normalized Compression Distance -NCD-.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6391</identifier>
 <datestamp>2012-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6391</id><created>2012-05-29</created><updated>2012-05-30</updated><authors><author><keyname>Shu</keyname><forenames>Kong</forenames></author><author><keyname>Donghui</keyname><forenames>Wang</forenames></author></authors><title>A Brief Summary of Dictionary Learning Based Approach for Classification</title><categories>cs.CV</categories><comments>Due to personal mistake, the authors' name are incorrectly written,
  thus we withdraw this submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note presents some representative methods which are based on dictionary
learning (DL) for classification. We do not review the sophisticated methods or
frameworks that involve DL for classification, such as online DL and spatial
pyramid matching (SPM), but rather, we concentrate on the direct DL-based
classification methods. Here, the &quot;so-called direct DL-based method&quot; is the
approach directly deals with DL framework by adding some meaningful penalty
terms. By listing some representative methods, we can roughly divide them into
two categories, i.e. (1) directly making the dictionary discriminative and (2)
forcing the sparse coefficients discriminative to push the discrimination power
of the dictionary. From this taxonomy, we can expect some extensions of them as
future researches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6396</identifier>
 <datestamp>2012-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6396</id><created>2012-05-29</created><authors><author><keyname>Choy</keyname><forenames>Murphy</forenames></author></authors><title>Effective Listings of Function Stop words for Twitter</title><categories>cs.IR cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many words in documents recur very frequently but are essentially meaningless
as they are used to join words together in a sentence. It is commonly
understood that stop words do not contribute to the context or content of
textual documents. Due to their high frequency of occurrence, their presence in
text mining presents an obstacle to the understanding of the content in the
documents. To eliminate the bias effects, most text mining software or
approaches make use of stop words list to identify and remove those words.
However, the development of such top words list is difficult and inconsistent
between textual sources. This problem is further aggravated by sources such as
Twitter which are highly repetitive or similar in nature. In this paper, we
will be examining the original work using term frequency, inverse document
frequency and term adjacency for developing a stop words list for the Twitter
data source. We propose a new technique using combinatorial values as an
alternative measure to effectively list out stop words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6399</identifier>
 <datestamp>2012-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6399</id><created>2012-05-29</created><authors><author><keyname>Arosemena-Trejos</keyname><forenames>Davis</forenames></author><author><keyname>Crespo</keyname><forenames>Sergio</forenames></author><author><keyname>Clunie</keyname><forenames>Clifton</forenames></author></authors><title>Forming Teams for Teaching Programming based on Static Code Analysis</title><categories>cs.SE</categories><comments>9 pages, 5 equations, 5 figures; IJCSI International Journal of
  Computer Science Issues, Vol. 9, Issue 2, No 3, March 2012. ISSN (Online):
  1694-0814</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of team for teaching programming can be effective in the classroom
because it helps students to generate and acquire new knowledge in less time,
but these groups to be formed without taking into account some respects, may
cause an adverse effect on the teaching-learning process. This paper proposes a
tool for the formation of team based on the semantics of source code (SOFORG).
This semantics is based on metrics extracted from the preferences, styles and
good programming practices. All this is achieved through a static analysis of
code that each student develops. In this way, you will have a record of
students with the information extracted; it evaluates the best formation of
teams in a given course. The team's formations are based on programming styles,
skills, pair programming or with leader.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6402</identifier>
 <datestamp>2012-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6402</id><created>2012-05-29</created><authors><author><keyname>Simmons</keyname><forenames>Robert J.</forenames></author><author><keyname>Toninho</keyname><forenames>Bernardo</forenames></author></authors><title>Constructive Provability Logic</title><categories>cs.LO cs.PL</categories><comments>Extended version of IMLA 2011 submission of the same title</comments><msc-class>03F05, 03B45, 03F45</msc-class><acm-class>F.4.1; I.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present constructive provability logic, an intuitionstic modal logic that
validates the L\&quot;ob rule of G\&quot;odel and L\&quot;ob's provability logic by permitting
logical reflection over provability. Two distinct variants of this logic, CPL
and CPL*, are presented in natural deduction and sequent calculus forms which
are then shown to be equivalent. In addition, we discuss the use of
constructive provability logic to justify stratified negation in logic
programming within an intuitionstic and structural proof theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6406</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6406</id><created>2012-05-29</created><updated>2013-04-24</updated><authors><author><keyname>Bachoc</keyname><forenames>Christine</forenames><affiliation>IMB</affiliation></author><author><keyname>Passuello</keyname><forenames>Alberto</forenames><affiliation>IMB</affiliation></author><author><keyname>Vallentin</keyname><forenames>Frank</forenames><affiliation>TWA</affiliation></author></authors><title>Bounds for projective codes from semidefinite programming</title><categories>cs.IT math.IT</categories><proxy>ccsd</proxy><journal-ref>Advances in mathematics of communications 7, 2 (2013) 127-145</journal-ref><doi>10.3934/amc.2013.7.127</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We apply the semidefinite programming method to derive bounds for projective
codes over a finite field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6412</identifier>
 <datestamp>2012-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6412</id><created>2012-05-03</created><authors><author><keyname>Ghosh</keyname><forenames>Arnab</forenames></author><author><keyname>Ghosh</keyname><forenames>Avishek</forenames></author><author><keyname>Chowdhury</keyname><forenames>Arkabandhu</forenames></author><author><keyname>Konar</keyname><forenames>Amit</forenames></author></authors><title>An Evolutionary Approach to Drug-Design Using a Novel Neighbourhood
  Based Genetic Algorithm</title><categories>cs.NE cs.CE</categories><comments>10 pages,13 figures (Communicated to journal)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present work provides a new approach to evolve ligand structures which
represent possible drug to be docked to the active site of the target protein.
The structure is represented as a tree where each non-empty node represents a
functional group. It is assumed that the active site configuration of the
target protein is known with position of the essential residues. In this paper
the interaction energy of the ligands with the protein target is minimized.
Moreover, the size of the tree is difficult to obtain and it will be different
for different active sites. To overcome the difficulty, a variable tree size
configuration is used for designing ligands. The optimization is done using a
novel Neighbourhood Based Genetic Algorithm (NBGA) which uses dynamic
neighbourhood topology. To get variable tree size, a variable-length version of
the above algorithm is devised. To judge the merit of the algorithm, it is
initially applied on the well known Travelling Salesman Problem (TSP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6420</identifier>
 <datestamp>2012-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6420</id><created>2012-05-29</created><authors><author><keyname>Nicodeme</keyname><forenames>Pierre</forenames></author></authors><title>Revisiting Waiting Times in DNA evolution</title><categories>cs.DM</categories><comments>19 pages, 3 Figures, 2 Tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transcription factors are short stretches of DNA (or $k$-mers) mainly located
in promoters sequences that enhance or repress gene expression. With respect to
an initial distribution of letters on the DNA alphabet, Behrens and Vingron
consider a random sequence of length $n$ that does not contain a given $k$-mer
or word of size $k$. Under an evolution model of the DNA, they compute the
probability $\mathfrak{p}_n$ that this $k$-mer appears after a unit time of 20
years. They prove that the waiting time for the first apparition of the $k$-mer
is well approximated by $T_n=1/\mathfrak{p}_n$. Their work relies on the
simplifying assumption that the $k$-mer is not self-overlapping. They observe
in particular that the waiting time is mostly driven by the initial
distribution of letters.
  Behrens et al. use an approach by automata that relaxes the assumption
related to words overlaps. Their numerical evaluations confirms the validity of
Behrens and Vingron approach for non self-overlapping words, but provides up to
44% corrections for highly self-overlapping words such as $\mathtt{AAAAA}$. We
devised an approach of the problem by clump analysis and generating functions;
this approach leads to prove a quasi-linear behaviour of $\mathfrak{p}_n$ for a
large range of values of $n$, an important result for DNA evolution. We present
here this clump analysis, first by language decomposition, and next by an
automaton construction; finally, we describe an equivalent approach by
construction of Markov automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6423</identifier>
 <datestamp>2012-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6423</id><created>2012-05-29</created><authors><author><keyname>Rath</keyname><forenames>Plawan Kumar</forenames></author><author><keyname>Anil</keyname><forenames>G. N.</forenames></author></authors><title>Proposed Challenges And Areas of Concern in Operating System Research
  and Development</title><categories>cs.OS</categories><comments>5 pages; International Journal for Computer Science Issues(IJCSI),
  Volume 9, Issue 2, March 2012</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Computers are a very important part of our lives and the major reason why
they have been such a success is because of the excellent graphical operating
systems that run on these powerful machines. As the computer hardware is
becoming more and more powerful, it is also vital to keep the software updated
in order to utilize the hardware of the system efficiently and make it faster
and smarter. This paper highlights some core issues that if dealt with in the
operating system level would make use of the full potential of the computer
hardware and provide an excellent user experience.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6432</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6432</id><created>2012-05-29</created><updated>2012-06-01</updated><authors><author><keyname>Daniely</keyname><forenames>Amit</forenames></author><author><keyname>Sabato</keyname><forenames>Sivan</forenames></author><author><keyname>Shwartz</keyname><forenames>Shai Shalev</forenames></author></authors><title>Multiclass Learning Approaches: A Theoretical Comparison with
  Implications</title><categories>cs.LG</categories><journal-ref>Advances in Neural Information Processing Systems, 2012, pages
  494-502</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We theoretically analyze and compare the following five popular multiclass
classification methods: One vs. All, All Pairs, Tree-based classifiers, Error
Correcting Output Codes (ECOC) with randomly generated code matrices, and
Multiclass SVM. In the first four methods, the classification is based on a
reduction to binary classification. We consider the case where the binary
classifier comes from a class of VC dimension $d$, and in particular from the
class of halfspaces over $\reals^d$. We analyze both the estimation error and
the approximation error of these methods. Our analysis reveals interesting
conclusions of practical relevance, regarding the success of the different
approaches under various conditions. Our proof technique employs tools from VC
theory to analyze the \emph{approximation error} of hypothesis classes. This is
in sharp contrast to most, if not all, previous uses of VC theory, which only
deal with estimation error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6433</identifier>
 <datestamp>2012-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6433</id><created>2012-05-29</created><authors><author><keyname>Ortiz-Ubarri</keyname><forenames>Jos&#xe9;</forenames></author><author><keyname>Moreno</keyname><forenames>Oscar</forenames></author><author><keyname>Tirkel</keyname><forenames>Andrew Z.</forenames></author><author><keyname>Arce-Nazario</keyname><forenames>Rafael</forenames></author><author><keyname>Golomb</keyname><forenames>Solomon W.</forenames></author></authors><title>Algebraic symmetries of generic $(m+1)$ dimensional periodic Costas
  arrays</title><categories>cs.IT math.IT</categories><doi>10.1109/TIT.2012.2221678</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we present two generators for the group of symmetries of the
generic $(m+1)$ dimensional periodic Costas arrays over elementary abelian
$(\mathbb{Z}_p)^m$ groups: one that is defined by multiplication on $m$
dimensions and the other by shear (addition) on $m$ dimensions. Through
exhaustive search we observe that these two generators characterize the group
of symmetries for the examples we were able to compute. Following the results,
we conjecture that these generators characterize the group of symmetries of the
generic $(m+1)$ dimensional periodic Costas arrays over elementary abelian
$(\mathbb{Z}_p)^m$ groups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6440</identifier>
 <datestamp>2012-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6440</id><created>2012-05-29</created><authors><author><keyname>Rao</keyname><forenames>Bandla Srinivasa</forenames></author><author><keyname>Prasad</keyname><forenames>R. Satya</forenames></author><author><keyname>Kantham</keyname><forenames>R. R. L.</forenames></author></authors><title>Monitoring Software Reliability using Statistical Process Control An
  Ordered Statistics Approach</title><categories>cs.SE</categories><comments>International Journal of Computer Applications; Published by
  Foundation of Computer Science</comments><journal-ref>International Journal of Computer Applications 32(7):28-33,
  October 2011</journal-ref><doi>10.5120/3917-5515</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The nature and complexity of software have changed significantly in the last
few decades. With the easy availability of computing power, deeper and broader
applications are made. It has been extremely necessary to produce good quality
software with high precession of reliability right in the first place. Olden
day's software errors and bugs were fixed at a later stage in the software
development. Today to produce high quality reliable software and to keep a
specific time schedule is a big challenge. To cope up the challenge many
concepts, methodology and practices of software engineering have been evolved
for developing reliable software. Better methods of controlling the process of
software production are underway. One of such methods to assess the software
reliability is using control charts. In this paper we proposed an NHPP based
control mechanism by using order statistics with cumulative quantity between
observations of failure data using mean value function of exponential
distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6445</identifier>
 <datestamp>2012-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6445</id><created>2012-02-22</created><authors><author><keyname>Zhao</keyname><forenames>Yunlong</forenames></author><author><keyname>Dong</keyname><forenames>Zhao</forenames></author><author><keyname>Iwai</keyname><forenames>Masayuki</forenames></author><author><keyname>Sezaki</keyname><forenames>Kaoru</forenames></author><author><keyname>Tobe</keyname><forenames>Yoshito</forenames></author></authors><title>An Extended Network Coding Opportunity Discovery Scheme in Wireless
  Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>15 pages and 7 figures</comments><journal-ref>International Journal of Computer Networks &amp; Communications
  (IJCNC) Vol.4, No.1, January (2012) 63-77</journal-ref><doi>10.5121/ijcnc.2012.4106</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Network coding is known as a promising approach to improve wireless network
performance. How to discover the coding opportunity in relay nodes is really
important for it. There are more coding chances, there are more times it can
improve network throughput by network coding operation. In this paper, an
extended network coding opportunity discovery scheme (ExCODE) is proposed,
which is realized by appending the current node ID and all its 1-hop neighbors'
IDs to the packet. ExCODE enables the next hop relay node to know which nodes
else have already overheard the packet, so it can discover the potential coding
opportunities as much as possible. ExCODE expands the region of discovering
coding chance to n-hops, and have more opportunities to execute network coding
operation in each relay node. At last, we implement ExCODE over the AODV
protocol, and efficiency of the proposed mechanism is demonstrated with NS2
simulations, compared to the existing coding opportunity discovery scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6460</identifier>
 <datestamp>2013-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6460</id><created>2012-05-29</created><updated>2013-05-27</updated><authors><author><keyname>Vince</keyname><forenames>Andrew</forenames></author></authors><title>A Combinatorial Approach to Positional Number Systems</title><categories>math.CO cs.DM math.NT</categories><comments>17 pages, 1 figure</comments><msc-class>11A63</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although the representation of the real numbers in terms of a base and a set
of digits has a long history, new questions arise even in simple situations.
This paper concerns binary radix systems, i.e., positional number systems with
digits 0 and 1. Our combinatorial approach is to construct infinitely many
binary radix systems, each one from a single pair of binary strings. Every
binary radix system that satisfies even a minimal set of conditions that would
be expected of a positional number system can be constructed in this way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6465</identifier>
 <datestamp>2012-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6465</id><created>2012-05-29</created><authors><author><keyname>Hernandez</keyname><forenames>Alejandro Mario</forenames></author></authors><title>Globally reasoning about localised security policies in distributed
  systems</title><categories>cs.CR cs.DC cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this report, we aim at establishing proper ways for model checking the
global security of distributed systems, which are designed consisting of set of
localised security policies that enforce specific issues about the security
expected.
  The systems are formally specified following a syntax, defined in detail in
this report, and their behaviour is clearly established by the Semantics, also
defined in detail in this report. The systems include the formal attachment of
security policies into their locations, whose intended interactions are trapped
by the policies, aiming at taking access control decisions of the system, and
the Semantics also takes care of this.
  Using the Semantics, a Labelled Transition System (LTS) can be induced for
every particular system, and over this LTS some model checking tasks could be
done. We identify how this LTS is indeed obtained, and propose an alternative
way of model checking the not-yet-induced LTS, by using the system design
directly. This may lead to over-approximation thereby producing imprecise,
though safe, results. We restrict ourselves to finite systems, in the sake of
being certain about the decidability of the proposed method.
  To illustrate the usefulness and validity of our proposal, we present 2 small
case-study-like examples, where we show how the system can be specified, which
policies could be added to it, and how to decide if the desired global security
property is met.
  Finally, an Appendix is given for digging deeply into how a tool for
automatically performing this task is being built, including some
implementation issues. The tool takes advantage of the proposed method, and
given some system and some desired global security property, it safely (i.e.
without false positives) ensures satisfaction of it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6515</identifier>
 <datestamp>2012-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6515</id><created>2012-05-29</created><authors><author><keyname>Debbal</keyname><forenames>Mohammed</forenames></author><author><keyname>Chikh-Bled</keyname><forenames>Mohamed</forenames></author></authors><title>Designing the Mode solving of the photonic crystal fiber via BPM and
  Exploring the Single-Mode Properties</title><categories>cs.OH physics.optics</categories><comments>8 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Microstructured optical fibers (MOFs) are one of the most exciting recent
developments in fiber optics. A MOF usually consists of a hexagonal arrangement
of air holes running down the length of a silica fiber surrounding a central
core of solid silica or, in some cases air. MOFs can exhibit a number of unique
properties, including zero dispersion at visible wavelengths and low or high
effective nonlinearity [3]-[17], By varying the size of the holes and their
number and position, one can also design MOFs with carefully controlled
dispersive and modal properties. In this paper, we analyze and modeling the
behavior of the photonic crystal fiber (PCF) by using in the first step a
propagator method based on the BPM method. With our BPM software, the electric
field contour of the fundamental mode of PCF was demonstrated. We also used it
to see the variation of the effective index; an effective index model confirms
that such a fiber can be single mode for any wavelength. It would make a study
of photonic crystal fibers, and a study of the numerical simulation methods
allow the simulation of optical properties and has modeled the propagation of
light in this fiber type. After that we use the V-parameter because it offers a
simple way to design a photonic crystal fiber (PCF), by basing in a recent
formulation of this parameter of a PCF, we provide numerically based empirical
expression for this quantity only dependent on the two structural parameters,
the air hole diameter and the hole-to-hole center spacing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6523</identifier>
 <datestamp>2012-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6523</id><created>2012-05-29</created><authors><author><keyname>Wang</keyname><forenames>Chamont</forenames></author><author><keyname>Gevertz</keyname><forenames>Jana</forenames></author><author><keyname>Chen</keyname><forenames>Chaur-Chin</forenames></author><author><keyname>Auslender</keyname><forenames>Leonardo</forenames></author></authors><title>Finding Important Genes from High-Dimensional Data: An Appraisal of
  Statistical Tests and Machine-Learning Approaches</title><categories>stat.ML cs.LG q-bio.QM</categories><comments>36 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past decades, statisticians and machine-learning researchers have
developed literally thousands of new tools for the reduction of
high-dimensional data in order to identify the variables most responsible for a
particular trait. These tools have applications in a plethora of settings,
including data analysis in the fields of business, education, forensics, and
biology (such as microarray, proteomics, brain imaging), to name a few.
  In the present work, we focus our investigation on the limitations and
potential misuses of certain tools in the analysis of the benchmark colon
cancer data (2,000 variables; Alon et al., 1999) and the prostate cancer data
(6,033 variables; Efron, 2010, 2008). Our analysis demonstrates that models
that produce 100% accuracy measures often select different sets of genes and
cannot stand the scrutiny of parameter estimates and model stability.
  Furthermore, we created a host of simulation datasets and &quot;artificial
diseases&quot; to evaluate the reliability of commonly used statistical and data
mining tools. We found that certain widely used models can classify the data
with 100% accuracy without using any of the variables responsible for the
disease. With moderate sample size and suitable pre-screening, stochastic
gradient boosting will be shown to be a superior model for gene selection and
variable screening from high-dimensional datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6527</identifier>
 <datestamp>2012-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6527</id><created>2012-05-29</created><authors><author><keyname>Christ</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Hoenicke</keyname><forenames>Jochen</forenames></author><author><keyname>Sch&#xe4;f</keyname><forenames>Martin</forenames></author></authors><title>Towards Bounded Infeasible Code Detection</title><categories>cs.PL cs.LO</categories><comments>24 pages</comments><msc-class>68N15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A first step towards more reliable software is to execute each statement and
each control-flow path in a method once. In this paper, we present a formal
method to automatically compute test cases for this purpose based on the idea
of a bounded infeasible code detection. The method first unwinds all loops in a
program finitely often and then encodes all feasible executions of the
loop-free programs in a logical formula. Helper variables are introduced such
that a theorem prover can reconstruct the control-flow path of a feasible
execution from a satisfying valuation of this formula. Based on this formula,
we present one algorithm that computes a feasible path cover and one algorithm
that computes a feasible statement cover. We show that the algorithms are
complete for loop-free programs and that they can be implemented efficiently.
We further provide a sound algorithm to compute procedure summaries which makes
the method scalable to larger programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6544</identifier>
 <datestamp>2012-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6544</id><created>2012-05-30</created><authors><author><keyname>Kong</keyname><forenames>Shu</forenames></author><author><keyname>Wang</keyname><forenames>Donghui</forenames></author></authors><title>A Brief Summary of Dictionary Learning Based Approach for Classification
  (revised)</title><categories>cs.CV cs.LG</categories><comments>a note revised from a withdrawn one</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note presents some representative methods which are based on dictionary
learning (DL) for classification. We do not review the sophisticated methods or
frameworks that involve DL for classification, such as online DL and spatial
pyramid matching (SPM), but rather, we concentrate on the direct DL-based
classification methods. Here, the &quot;so-called direct DL-based method&quot; is the
approach directly deals with DL framework by adding some meaningful penalty
terms. By listing some representative methods, we can roughly divide them into
two categories, i.e. (1) directly making the dictionary discriminative and (2)
forcing the sparse coefficients discriminative to push the discrimination power
of the dictionary. From this taxonomy, we can expect some extensions of them as
future researches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6548</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6548</id><created>2012-05-30</created><updated>2013-12-08</updated><authors><author><keyname>Zhou</keyname><forenames>Xiaojun</forenames></author><author><keyname>Yang</keyname><forenames>Chunhua</forenames></author><author><keyname>Gui</keyname><forenames>Weihua</forenames></author></authors><title>State Transition Algorithm</title><categories>math.OC cs.NE</categories><comments>18 pages, 28 figures</comments><msc-class>90C26, 90C30, 90C59</msc-class><journal-ref>Journal of Industrial and Management Optimization 8(4): 1039-1056,
  2012</journal-ref><doi>10.3934/jimo.2012.8.1039</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In terms of the concepts of state and state transition, a new heuristic
random search algorithm named state transition algorithm is proposed. For
continuous function optimization problems, four special transformation
operators called rotation, translation, expansion and axesion are designed.
Adjusting measures of the transformations are mainly studied to keep the
balance of exploration and exploitation. Convergence analysis is also discussed
about the algorithm based on random search theory. In the meanwhile, to
strengthen the search ability in high dimensional space, communication strategy
is introduced into the basic algorithm and intermittent exchange is presented
to prevent premature convergence. Finally, experiments are carried out for the
algorithms. With 10 common benchmark unconstrained continuous functions used to
test the performance, the results show that state transition algorithms are
promising algorithms due to their good global search capability and convergence
property when compared with some popular algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6557</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6557</id><created>2012-05-30</created><updated>2015-10-13</updated><authors><author><keyname>Seiller</keyname><forenames>Thomas</forenames></author></authors><title>Interaction Graphs: Additives</title><categories>cs.LO math.LO</categories><msc-class>03B70 (Primary), 03F52, 03B47 (Secondary)</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Geometry of Interaction (GoI) is a kind of semantics of linear logic proofs
that aims at accounting for the dynamical aspects of cut-elimination. We
present here a parametrized construction of a Geometry of Interaction for
Multiplicative Additive Linear Logic (MALL) in which proofs are represented by
families of directed weighted graphs. Contrarily to former constructions
dealing with additive connectives, we are able to solve the known issue of
obtaining a denotational semantics for MALL by introducing a notion of
observational equivalence. Moreover, our setting has the advantage of being the
first construction dealing with additives where proofs of MALL are interpreted
by finite objects. The fact that we obtain a denotational model of MALL relies
on a single geometric property, which we call the trefoil property, from which
we obtain, for each value of the parameter, adjunctions. We then proceed to
show how this setting is related to Girard's various constructions: particular
choices of the parameter respectively give a combinatorial version of his
latest GoI, a refined version of older Geometries of Interaction based on
nilpotency. This shows the importance of the trefoil property underlying our
constructions since all known GoI construction to this day rely on particular
cases of it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6558</identifier>
 <datestamp>2012-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6558</id><created>2012-05-30</created><authors><author><keyname>Seiller</keyname><forenames>Thomas</forenames></author></authors><title>Interaction Graphs: Multiplicatives</title><categories>cs.LO math.LO</categories><comments>To appear in Annals of Pure and Applied Logic</comments><msc-class>03B70 (Primary) 03F52, 03B47 (Secondary)</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a graph-theoretical representation of proofs of multiplicative
linear logic which yields both a denotational semantics and a notion of truth.
For this, we use a locative approach (in the sense of ludics) related to game
semantics and the Danos-Regnier interpretation of GoI operators as paths in
proof nets. We show how we can retrieve from this locative framework both a
categorical semantics for MLL with distinct units and a notion of truth.
Moreover, we show how a restricted version of our model can be reformulated in
the exact same terms as Girard's latest geometry of interaction. This shows
that this restriction of our framework gives a combinatorial approach to J.-Y.
Girard's geometry of interaction in the hyperfinite factor, while using only
graph-theoretical notions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6567</identifier>
 <datestamp>2012-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6567</id><created>2012-05-30</created><authors><author><keyname>Pollner</keyname><forenames>Peter</forenames></author><author><keyname>Palla</keyname><forenames>Gergely</forenames></author><author><keyname>Vicsek</keyname><forenames>Tamas</forenames></author></authors><title>Clustering of tag-induced sub-graphs in complex networks</title><categories>physics.soc-ph cs.SI</categories><journal-ref>PHYSICA A - STATISTICAL MECHANICS AND ITS APPLICATIONS 389: pp.
  5887-5894. (2010)</journal-ref><doi>10.1016/j.physa.2010.09.012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the behavior of the clustering coefficient in tagged networks. The
rich variety of tags associated with the nodes in the studied systems provide
additional information about the entities represented by the nodes which can be
important for practical applications like searching in the networks. Here we
examine how the clustering coefficient changes when narrowing the network to a
sub-graph marked by a given tag, and how does it correlate with various other
properties of the sub-graph. Another interesting question addressed in the
paper is how the clustering coefficient of the individual nodes is affected by
the tags on the node. We believe these sort of analysis help acquiring a more
complete description of the structure of large complex systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6568</identifier>
 <datestamp>2012-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6568</id><created>2012-05-30</created><authors><author><keyname>Su</keyname><forenames>Wei</forenames></author><author><keyname>Pott</keyname><forenames>Alexander</forenames></author><author><keyname>Tang</keyname><forenames>Xiaohu</forenames></author></authors><title>Characterization of Negabent Functions and Construction of Bent-Negabent
  Functions with Maximum Algebraic Degree</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present necessary and sufficient conditions for a Boolean function to be a
negabent function for both even and odd number of variables, which demonstrate
the relationship between negabent functions and bent functions. By using these
necessary and sufficient conditions for Boolean functions to be negabent, we
obtain that the nega spectrum of a negabent function has at most 4 values. We
determine the nega spectrum distribution of negabent functions. Further, we
provide a method to construct bent-negabent functions in $n$ variables ($n$
even) of algebraic degree ranging from 2 to $\frac{n}{2}$, which implies that
the maximum algebraic degree of an $n$-variable bent-negabent function is equal
to $\frac{n}{2}$. Thus, we answer two open problems proposed by Parker and Pott
and by St\v{a}nic\v{a} \textit{et al.} respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6572</identifier>
 <datestamp>2012-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6572</id><created>2012-05-30</created><authors><author><keyname>Halder</keyname><forenames>Amiya</forenames></author><author><keyname>Pramanik</keyname><forenames>Soumajit</forenames></author></authors><title>An Unsupervised Dynamic Image Segmentation using Fuzzy Hopfield Neural
  Network based Genetic Algorithm</title><categories>cs.CV</categories><comments>8 pages</comments><journal-ref>IJCSI March 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a Genetic Algorithm based segmentation method that can
automatically segment gray-scale images. The proposed method mainly consists of
spatial unsupervised grayscale image segmentation that divides an image into
regions. The aim of this algorithm is to produce precise segmentation of images
using intensity information along with neighborhood relationships. In this
paper, Fuzzy Hopfield Neural Network (FHNN) clustering helps in generating the
population of Genetic algorithm which there by automatically segments the
image. This technique is a powerful method for image segmentation and works for
both single and multiple-feature data with spatial information. Validity index
has been utilized for introducing a robust technique for finding the optimum
number of components in an image. Experimental results shown that the algorithm
generates good quality segmented image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6584</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6584</id><created>2012-05-30</created><updated>2013-02-08</updated><authors><author><keyname>Demri</keyname><forenames>St&#xe9;phane</forenames></author><author><keyname>Dhar</keyname><forenames>Amit Kumar</forenames></author><author><keyname>sangnier</keyname><forenames>Arnaud</forenames></author></authors><title>Taming Past LTL and Flat Counter Systems</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reachability and LTL model-checking problems for flat counter systems are
known to be decidable but whereas the reachability problem can be shown in NP,
the best known complexity upper bound for the latter problem is made of a tower
of several exponentials. Herein, we show that the problem is only NP-complete
even if LTL admits past-time operators and arithmetical constraints on
counters. Actually, the NP upper bound is shown by adequately combining a new
stuttering theorem for Past LTL and the property of small integer solutions for
quantifier-free Presburger formulae. Other complexity results are proved, for
instance for restricted classes of flat counter systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6586</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6586</id><created>2012-05-30</created><updated>2015-04-24</updated><authors><author><keyname>Linton</keyname><forenames>Steve</forenames></author><author><keyname>Niemeyer</keyname><forenames>Alice C.</forenames></author><author><keyname>Praeger</keyname><forenames>Cheryl E.</forenames></author></authors><title>Identifying long cycles in finite alternating and symmetric groups
  acting on subsets</title><categories>math.GR cs.DM</categories><comments>45 pages</comments><msc-class>20B30, 60C05, 20P05, 05A05</msc-class><journal-ref>Journal of Algebra Combinatorics Discrete Structures and
  Applications, Volume: 2, Issue: 2, pp:117-149, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $H$ be a permutation group on a set $\Lambda$, which is permutationally
isomorphic to a finite alternating or symmetric group $A_n$ or $S_n$ acting on
the $k$-element subsets of points from $\{1,\ldots,n\}$, for some arbitrary but
fixed $k$. Suppose moreover that no isomorphism with this action is known. We
show that key elements of $H$ needed to construct such an isomorphism
$\varphi$, such as those whose image under $\varphi$ is an $n$-cycle or
$(n-1)$-cycle, can be recognised with high probability by the lengths of just
four of their cycles in $\Lambda$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6593</identifier>
 <datestamp>2012-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6593</id><created>2012-05-30</created><authors><author><keyname>Zhang</keyname><forenames>Jun</forenames></author><author><keyname>Fu</keyname><forenames>Fang-Wei</forenames></author><author><keyname>Liao</keyname><forenames>Qun-Ying</forenames></author></authors><title>New Deep Holes of Generalized Reed-Solomon Codes</title><categories>cs.IT math.IT math.NT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep holes play an important role in the decoding of generalized Reed-Solomon
codes. Recently, Wu and Hong \cite{WH} found a new class of deep holes for
standard Reed-Solomon codes. In the present paper, we give a concise method to
obtain a new class of deep holes for generalized Reed-Solomon codes. In
particular, for standard Reed-Solomon codes, we get the new class of deep holes
given in \cite{WH}.
  Li and Wan \cite{L.W1} studied deep holes of generalized Reed-Solomon codes
$GRS_{k}(\f,D)$ and characterized deep holes defined by polynomials of degree
$k+1$. They showed that this problem is reduced to be a subset sum problem in
finite fields. Using the method of Li and Wan, we obtain some new deep holes
for special Reed-Solomon codes over finite fields with even characteristic.
Furthermore, we study deep holes of the extended Reed-Solomon code, i.e.,
$D=\f$ and show polynomials of degree $k+2$ can not define deep holes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6594</identifier>
 <datestamp>2012-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6594</id><created>2012-05-30</created><authors><author><keyname>Nath</keyname><forenames>Shimul Kumar</forenames></author><author><keyname>Merkel</keyname><forenames>Robert</forenames></author><author><keyname>Lau</keyname><forenames>Man Fai</forenames></author><author><keyname>Paul</keyname><forenames>Tanay Kanti</forenames></author></authors><title>Towards a better understanding of testing if conditionals</title><categories>cs.SE</categories><comments>10 pages, 1 figure, 9 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fault based testing is a technique in which test cases are chosen to reveal
certain classes of faults. At present, testing professionals use their personal
experience to select testing methods for fault classes considered the most
likely to be present. However, there is little empirical evidence available in
the open literature to support these intuitions. By examining the source code
changes when faults were fixed in seven open source software artifacts, we have
classified bug fix patterns into fault classes, and recorded the relative
frequencies of the identified fault classes. This paper reports our findings
related to &quot;if-conditional&quot; fixes. We have classified the &quot;if-conditional&quot;
fixes into fourteen fault classes and calculated their frequencies. We found
the most common fault class related to changes within a single &quot;atom&quot;. The next
most common fault was the omission of an &quot;atom&quot;. We analysed these results in
the context of Boolean specification testing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6595</identifier>
 <datestamp>2013-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6595</id><created>2012-05-30</created><updated>2013-03-13</updated><authors><author><keyname>Mouradian</keyname><forenames>Alexandre</forenames><affiliation>CITI, CITI Insa Lyon / Inria Grenoble Rh&#xf4;ne-Alpes</affiliation></author><author><keyname>Aug&#xe9;-Blum</keyname><forenames>Isabelle</forenames><affiliation>CITI Insa Lyon / Inria Grenoble Rh&#xf4;ne-Alpes</affiliation></author><author><keyname>Valois</keyname><forenames>Fabrice</forenames><affiliation>CITI, CITI Insa Lyon / Inria Grenoble Rh&#xf4;ne-Alpes</affiliation></author></authors><title>RTXP : A Localized Real-Time Mac-Routing Protocol for Wireless Sensor
  Networks</title><categories>cs.NI</categories><proxy>ccsd</proxy><report-no>RR-7978</report-no><journal-ref>N&amp;deg; RR-7978 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Protocols developed during the last years for Wireless Sensor Networks (WSNs)
are mainly focused on energy efficiency and autonomous mechanisms (e.g.
self-organization, self-configuration, etc). Nevertheless, with new WSN
applications, appear new QoS requirements such as time constraints. Real-time
applications require the packets to be delivered before a known time bound
which depends on the application requirements. We particularly focus on
applications which consist in alarms sent to the sink node. We propose
Real-Time X-layer Protocol (RTXP), a real-time communication protocol. To the
best of our knowledge, RTXP is the first MAC and routing real-time
communication protocol that is not centralized, but instead relies only on
local information. The solution is cross-layer (X-layer) because it allows to
control the delays due to MAC and Routing layers interactions. RTXP uses a
suited hop-count-based Virtual Coordinate System which allows deterministic
medium access and forwarder selection. In this paper we describe the protocol
mechanisms. We give theoretical bound on the end-to-end delay and the capacity
of the protocol. Intensive simulation results confirm the theoretical
predictions and allow to compare with a real-time centralized solution. RTXP is
also simulated under harsh radio channel, in this case the radio link
introduces probabilistic behavior. Nevertheless, we show that RTXP it performs
better than a non-deterministic solution. It thus advocates for the usefulness
of designing real-time (deterministic) protocols even for highly unreliable
networks such as WSNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6602</identifier>
 <datestamp>2012-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6602</id><created>2012-05-30</created><authors><author><keyname>Hu</keyname><forenames>Bao-Gang</forenames></author><author><keyname>Xing</keyname><forenames>Hong-Jie</forenames></author></authors><title>Analytical Bounds between Entropy and Error Probability in Binary
  Classifications</title><categories>cs.IT math.IT</categories><comments>7 Pages, 2 Figures, Maple code</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The existing upper and lower bounds between entropy and error probability are
mostly derived from the inequality of the entropy relations, which could
introduce approximations into the analysis. We derive analytical bounds based
on the closed-form solutions of conditional entropy without involving any
approximation. Two basic types of classification errors are investigated in the
context of binary classification problems, namely, Bayesian and non-Bayesian
errors. We theoretically confirm that Fano's lower bound is an exact lower
bound for any types of classifier in a relation diagram of &quot;error probability
vs. conditional entropy&quot;. The analytical upper bounds are achieved with respect
to the minimum prior probability, which are tighter than Kovalevskij's upper
bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6605</identifier>
 <datestamp>2012-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6605</id><created>2012-05-30</created><authors><author><keyname>Egger</keyname><forenames>Jan</forenames></author><author><keyname>Freisleben</keyname><forenames>Bernd</forenames></author><author><keyname>Nimsky</keyname><forenames>Christopher</forenames></author><author><keyname>Kapur</keyname><forenames>Tina</forenames></author></authors><title>Template-Cut: A Pattern-Based Segmentation Paradigm</title><categories>cs.CV</categories><comments>8 pages, 6 figures, 3 tables, 6 equations, 51 references</comments><journal-ref>J. Egger, B. Freisleben, C. Nimsky, T. Kapur. Template-Cut: A
  Pattern-Based Segmentation Paradigm. Nature - Scientific Reports, Nature
  Publishing Group (NPG), 2(420), 2012</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We present a scale-invariant, template-based segmentation paradigm that sets
up a graph and performs a graph cut to separate an object from the background.
Typically graph-based schemes distribute the nodes of the graph uniformly and
equidistantly on the image, and use a regularizer to bias the cut towards a
particular shape. The strategy of uniform and equidistant nodes does not allow
the cut to prefer more complex structures, especially when areas of the object
are indistinguishable from the background. We propose a solution by introducing
the concept of a &quot;template shape&quot; of the target object in which the nodes are
sampled non-uniformly and non-equidistantly on the image. We evaluate it on
2D-images where the object's textures and backgrounds are similar, and large
areas of the object have the same gray level appearance as the background. We
also evaluate it in 3D on 60 brain tumor datasets for neurosurgical planning
purposes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6654</identifier>
 <datestamp>2013-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6654</id><created>2012-05-30</created><authors><author><keyname>Yatawatta</keyname><forenames>Sarod</forenames></author></authors><title>Reduced Ambiguity Calibration for LOFAR</title><categories>astro-ph.IM cs.NA</categories><comments>Draft version. Final version published on 10 April 2012</comments><journal-ref>Experimental Astronomy, Volume 34, Issue 1, pp.89-103, 2012</journal-ref><doi>10.1007/s10686-012-9300-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interferometric calibration always yields non unique solutions. It is
therefore essential to remove these ambiguities before the solutions could be
used in any further modeling of the sky, the instrument or propagation effects
such as the ionosphere. We present a method for LOFAR calibration which does
not yield a unitary ambiguity, especially under ionospheric distortions. We
also present exact ambiguities we get in our solutions, in closed form. Casting
this as an optimization problem, we also present conditions for this approach
to work. The proposed method enables us to use the solutions obtained via
calibration for further modeling of instrumental and propagation effects. We
provide extensive simulation results on the performance of our method.
Moreover, we also give cases where due to degeneracy, this method fails to
perform as expected and in such cases, we suggest exploiting diversity in time,
space and frequency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6658</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6658</id><created>2012-05-30</created><updated>2014-11-13</updated><authors><author><keyname>Feldmann</keyname><forenames>Michel</forenames></author></authors><title>Solving satisfiability by statistical estimation</title><categories>cs.CC</categories><comments>Version refocused on technical computing problems. New title to
  reflect this change (Initial title:From classical versus quantum algorithms
  to P versus NP). Most comments on relations with quantum computing are moved
  to arXiv: 1312.7551. 11 pages, no figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose to solve any algorithm on discrete variables by a technique of
statistical estimation using deterministic convex analysis. In this framework,
the variables are represented by their probability and the distinction between
the complexity classes vanishes. The method is illustrated by solving the 3-SAT
problem in polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6664</identifier>
 <datestamp>2012-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6664</id><created>2012-05-30</created><authors><author><keyname>Y&#xfc;ksel</keyname><forenames>Ender</forenames></author><author><keyname>Nielson</keyname><forenames>Hanne Riis</forenames></author><author><keyname>Nielson</keyname><forenames>Flemming</forenames></author><author><keyname>Zhu</keyname><forenames>Huibiao</forenames></author><author><keyname>Huang</keyname><forenames>Heqing</forenames></author></authors><title>Modelling Chinese Smart Grid: A Stochastic Model Checking Case Study</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cyber-physical systems integrate information and communication technology
functions to the physical elements of a system for monitoring and controlling
purposes. The conversion of traditional power grid into a smart grid, a
fundamental example of a cyber-physical system, raises a number of issues that
require novel methods and applications. In this context, an important issue is
the verification of certain quantitative properties of the system. In this
technical report, we consider a specific Chinese Smart Grid implementation and
try to address the verification problem for certain quantitative properties
including performance and battery consumption. We employ stochastic model
checking approach and present our modelling and analysis study using PRISM
model checker.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6671</identifier>
 <datestamp>2012-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6671</id><created>2012-05-30</created><authors><author><keyname>Hashemi</keyname><forenames>Amir</forenames></author><author><keyname>Schweinfurter</keyname><forenames>Michael</forenames></author><author><keyname>Seiler</keyname><forenames>Werner M.</forenames></author></authors><title>Quasi-Stability versus Genericity</title><categories>cs.SC math.AC math.AG math.RA</categories><comments>13 pages, to appear in the proceedings of the CASC'12 conference
  (Maribor, Slovenia 2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quasi-stable ideals appear as leading ideals in the theory of Pommaret bases.
We show that quasi-stable leading ideals share many of the properties of the
generic initial ideal. In contrast to genericity, quasi-stability is a
characteristic independent property that can be effectively verified. We also
relate Pommaret bases to some invariants associated with local cohomology,
exhibit the existence of linear quotients in Pommaret bases and prove some
results on componentwise linear ideals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6675</identifier>
 <datestamp>2012-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6675</id><created>2012-05-30</created><authors><author><keyname>Y&#xfc;ksel</keyname><forenames>Ender</forenames></author><author><keyname>Nielson</keyname><forenames>Hanne Riis</forenames></author><author><keyname>Nielson</keyname><forenames>Flemming</forenames></author><author><keyname>Fruth</keyname><forenames>Matthias</forenames></author><author><keyname>Kwiatkowska</keyname><forenames>Marta</forenames></author></authors><title>Optimizing ZigBee Security using Stochastic Model Checking</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  ZigBee is a fairly new but promising wireless sensor network standard that
offers the advantages of simple and low resource communication. Nevertheless,
security is of great concern to ZigBee, and enhancements are prescribed in the
latest ZigBee specication: ZigBee-2007. In this technical report, we identify
an important gap in the specification on key updates, and present a methodology
for determining optimal key update policies and security parameters. We exploit
the stochastic model checking approach using the probabilistic model checker
PRISM, and assess the security needs for realistic application scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6677</identifier>
 <datestamp>2012-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6677</id><created>2012-05-30</created><authors><author><keyname>Kandil</keyname><forenames>Mostafa</forenames></author><author><keyname>Hassanein</keyname><forenames>Ehab</forenames></author><author><keyname>Mazen</keyname><forenames>Sherif</forenames></author></authors><title>Cross-View of Testing Techniques Toward Improving Web-Based Application
  Testing</title><categories>cs.SE</categories><comments>7 pages, 1 figure</comments><journal-ref>journal: http://www.ijcsi.org Publication year: 2012 Publication
  Volume: 9 Publication Issue: 2</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web Applications (WA's) failures may lead to collapse of the institutions,
therefore the importance of good quality WA's is increasing over the time.
Testing is one of the best quality metrics that decide whether WA's are
reliable or not. WA's testing approaches suffer from the lack of proper
coverage of WA's functional requirements testing. On the other hand some
approaches produce test cases that already cover WA's testing but they also
produce a great number of irrelevant test cases. This research analyzed the
main testing approaches for WA's and GUI applications. Also we have an overview
of Test-Driven Development and its effects on the current development. The
specification of good testing approach that satisfies the proper testing is
then presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6678</identifier>
 <datestamp>2012-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6678</id><created>2012-05-30</created><authors><author><keyname>Y&#xfc;ksel</keyname><forenames>Ender</forenames></author></authors><title>Analysing ZigBee Key Establishment Protocols</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this report, we present our approach for protocol analysis together with a
real example where we find an important flow in a contemporary wireless sensor
network security protocol. We start by modelling protocols using a specific
process algebraic formalism called LySa process calculus. We then apply an
analysis based on a special program analysis technique called control flow
analysis. We apply this technique to the ZigBee-2007 End-to-End Application Key
Establishment Protocol and with the help of the analysis discover an unknown
flaw. Finally we suggest a fix for the protocol, and verify that the fix works
by using the same technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6680</identifier>
 <datestamp>2012-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6680</id><created>2012-05-29</created><authors><author><keyname>Hirvola</keyname><forenames>Tommi</forenames></author></authors><title>MIPS code compression</title><categories>cs.OH</categories><comments>7 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MIPS machine code is very structured: registers used before are likely to be
used again, some instructions and registers are used more heavily than others,
some instructions often follow each other and so on. Standard file compression
utilities, such as gzip and bzip2, does not take full advantage of the
structure because they work on byte-boundaries and don't see the underlying
instruction fields. My idea is to filter opcodes, registers and immediates from
MIPS binary code into distinct streams and compress them individually to
achieve better compression ratios. Several different ways to split MIPS code
into streams are considered. The results presented in this paper shows that a
simple filter can reduce final compressed size by up to 10 % with gzip and
bzip2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6683</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6683</id><created>2012-05-30</created><updated>2012-12-01</updated><authors><author><keyname>Avis</keyname><forenames>David</forenames></author><author><keyname>Iwama</keyname><forenames>Kazuo</forenames></author><author><keyname>Paku</keyname><forenames>Daichi</forenames></author></authors><title>Reputation Games for Undirected Graphs</title><categories>cs.DM cs.DS cs.GT</categories><comments>19 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  J. Hopcroft and D. Sheldon originally introduced network reputation games to
investigate the self-interested behavior of web authors who want to maximize
their PageRank on a directed web graph by choosing their outlinks in a game
theoretic manner. They give best response strategies for each player and
characterize properties of web graphs which are Nash equilibria. In this paper
we consider three different models for PageRank games on undirected graphs such
as certain social networks. In undirected graphs players may delete links at
will, but typically cannot add links without the other player's permission. In
the deletion-model players are free to delete any of their bidirectional links
but may not add links. We study the problem of determining whether the given
graph represents a Nash equilibrium or not in this model. We give an $O(n^{2})$
time algorithm for a tree, and a parametric $O(2^{k}n^{4})$ time algorithm for
general graphs, where $k$ is the maximum vertex degree in any biconnected
component of the graph. In the request- delete-model players are free to delete
any bidirectional links and add any directed links, since these additions can
be done unilaterally and can be viewed as requests for bidirected links. For
this model we give an $O(n^3)$ time algorithm for verifying Nash equilibria in
trees. Finally, in the add-delete- model we allow a node to make arbitrary
deletions and the addition of a single bidirectional link if it would increase
the page rank of the other player also. In this model we give a parametric
algorithm for verifying Nash equilibria in general graphs and characterize so
called $\alpha$-insensitive Nash Equilibria. We also give a result showing a
large class of graphs where there is an edge addition that causes the PageRank
of both of its endpoints to increase, suggesting convergence towards complete
subgraphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6691</identifier>
 <datestamp>2012-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6691</id><created>2012-05-30</created><authors><author><keyname>Sun</keyname><forenames>Zhao</forenames></author><author><keyname>Wang</keyname><forenames>Hongzhi</forenames></author><author><keyname>Wang</keyname><forenames>Haixun</forenames></author><author><keyname>Shao</keyname><forenames>Bin</forenames></author><author><keyname>Li</keyname><forenames>Jianzhong</forenames></author></authors><title>Efficient Subgraph Matching on Billion Node Graphs</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 9, pp.
  788-799 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to handle large scale graph data is crucial to an increasing
number of applications. Much work has been dedicated to supporting basic graph
operations such as subgraph matching, reachability, regular expression
matching, etc. In many cases, graph indices are employed to speed up query
processing. Typically, most indices require either super-linear indexing time
or super-linear indexing space. Unfortunately, for very large graphs,
super-linear approaches are almost always infeasible. In this paper, we study
the problem of subgraph matching on billion-node graphs. We present a novel
algorithm that supports efficient subgraph matching for graphs deployed on a
distributed memory store. Instead of relying on super-linear indices, we use
efficient graph exploration and massive parallel computing for query
processing. Our experimental results demonstrate the feasibility of performing
subgraph matching on web-scale graph data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6692</identifier>
 <datestamp>2012-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6692</id><created>2012-05-30</created><authors><author><keyname>Yuan</keyname><forenames>Ye</forenames></author><author><keyname>Wang</keyname><forenames>Guoren</forenames></author><author><keyname>Chen</keyname><forenames>Lei</forenames></author><author><keyname>Wang</keyname><forenames>Haixun</forenames></author></authors><title>Efficient Subgraph Similarity Search on Large Probabilistic Graph
  Databases</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 9, pp.
  800-811 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many studies have been conducted on seeking the efficient solution for
subgraph similarity search over certain (deterministic) graphs due to its wide
application in many fields, including bioinformatics, social network analysis,
and Resource Description Framework (RDF) data management. All these works
assume that the underlying data are certain. However, in reality, graphs are
often noisy and uncertain due to various factors, such as errors in data
extraction, inconsistencies in data integration, and privacy preserving
purposes. Therefore, in this paper, we study subgraph similarity search on
large probabilistic graph databases. Different from previous works assuming
that edges in an uncertain graph are independent of each other, we study the
uncertain graphs where edges' occurrences are correlated. We formally prove
that subgraph similarity search over probabilistic graphs is #P-complete, thus,
we employ a filter-and-verify framework to speed up the search. In the
filtering phase,we develop tight lower and upper bounds of subgraph similarity
probability based on a probabilistic matrix index, PMI. PMI is composed of
discriminative subgraph features associated with tight lower and upper bounds
of subgraph isomorphism probability. Based on PMI, we can sort out a large
number of probabilistic graphs and maximize the pruning capability. During the
verification phase, we develop an efficient sampling algorithm to validate the
remaining candidates. The efficiency of our proposed solutions has been
verified through extensive experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6693</identifier>
 <datestamp>2012-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6693</id><created>2012-05-30</created><authors><author><keyname>Wang</keyname><forenames>Jia</forenames></author><author><keyname>Cheng</keyname><forenames>James</forenames></author></authors><title>Truss Decomposition in Massive Networks</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 9, pp.
  812-823 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The k-truss is a type of cohesive subgraphs proposed recently for the study
of networks. While the problem of computing most cohesive subgraphs is NP-hard,
there exists a polynomial time algorithm for computing k-truss. Compared with
k-core which is also efficient to compute, k-truss represents the &quot;core&quot; of a
k-core that keeps the key information of, while filtering out less important
information from, the k-core. However, existing algorithms for computing
k-truss are inefficient for handling today's massive networks. We first improve
the existing in-memory algorithm for computing k-truss in networks of moderate
size. Then, we propose two I/O-efficient algorithms to handle massive networks
that cannot fit in main memory. Our experiments on real datasets verify the
efficiency of our algorithms and the value of k-truss.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6694</identifier>
 <datestamp>2012-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6694</id><created>2012-05-30</created><authors><author><keyname>Fan</keyname><forenames>Ju</forenames></author><author><keyname>Li</keyname><forenames>Guoliang</forenames></author><author><keyname>Zhou</keyname><forenames>Lizhu</forenames></author><author><keyname>Chen</keyname><forenames>Shanshan</forenames></author><author><keyname>Hu</keyname><forenames>Jun</forenames></author></authors><title>SEAL: Spatio-Textual Similarity Search</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 9, pp.
  824-835 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Location-based services (LBS) have become more and more ubiquitous recently.
Existing methods focus on finding relevant points-of-interest (POIs) based on
users' locations and query keywords. Nowadays, modern LBS applications generate
a new kind of spatio-textual data, regions-of-interest (ROIs), containing
region-based spatial information and textual description, e.g., mobile user
profiles with active regions and interest tags. To satisfy search requirements
on ROIs, we study a new research problem, called spatio-textual similarity
search: Given a set of ROIs and a query ROI, we find the similar ROIs by
considering spatial overlap and textual similarity. Spatio-textual similarity
search has many important applications, e.g., social marketing in
location-aware social networks. It calls for an efficient search method to
support large scales of spatio-textual data in LBS systems. To this end, we
introduce a filter-and-verification framework to compute the answers. In the
filter step, we generate signatures for the ROIs and the query, and utilize the
signatures to generate candidates whose signatures are similar to that of the
query. In the verification step, we verify the candidates and identify the
final answers. To achieve high performance, we generate effective high-quality
signatures, and devise efficient filtering algorithms as well as pruning
techniques. Experimental results on real and synthetic datasets show that our
method achieves high performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6695</identifier>
 <datestamp>2012-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6695</id><created>2012-05-30</created><authors><author><keyname>Lappas</keyname><forenames>Theodoros</forenames></author><author><keyname>Vieira</keyname><forenames>Marcos R.</forenames></author><author><keyname>Gunopulos</keyname><forenames>Dimitrios</forenames></author><author><keyname>Tsotras</keyname><forenames>Vassilis J.</forenames></author></authors><title>On The Spatiotemporal Burstiness of Terms</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 9, pp.
  836-847 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Thousands of documents are made available to the users via the web on a daily
basis. One of the most extensively studied problems in the context of such
document streams is burst identification. Given a term t, a burst is generally
exhibited when an unusually high frequency is observed for t. While spatial and
temporal burstiness have been studied individually in the past, our work is the
first to simultaneously track and measure spatiotemporal term burstiness. In
addition, we use the mined burstiness information toward an efficient
document-search engine: given a user's query of terms, our engine returns a
ranked list of documents discussing influential events with a strong
spatiotemporal impact. We demonstrate the efficiency of our methods with an
extensive experimental evaluation on real and synthetic datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6696</identifier>
 <datestamp>2012-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6696</id><created>2012-05-30</created><authors><author><keyname>Shirani-Mehr</keyname><forenames>Houtan</forenames></author><author><keyname>Kashani</keyname><forenames>Farnoush Banaei</forenames></author><author><keyname>Shahabi</keyname><forenames>Cyrus</forenames></author></authors><title>Efficient Reachability Query Evaluation in Large Spatiotemporal Contact
  Datasets</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 9, pp.
  848-859 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the advent of reliable positioning technologies and prevalence of
location-based services, it is now feasible to accurately study the propagation
of items such as infectious viruses, sensitive information pieces, and malwares
through a population of moving objects, e.g., individuals, mobile devices, and
vehicles. In such application scenarios, an item passes between two objects
when the objects are sufficiently close (i.e., when they are, so-called, in
contact), and hence once an item is initiated, it can penetrate the object
population through the evolving network of contacts among objects, termed
contact network. In this paper, for the first time we define and study
reachability queries in large (i.e., disk-resident) contact datasets which
record the movement of a (potentially large) set of objects moving in a spatial
environment over an extended time period. A reachability query verifies whether
two objects are &quot;reachable&quot; through the evolving contact network represented by
such contact datasets. We propose two contact-dataset indexes that enable
efficient evaluation of such queries despite the potentially humongous size of
the contact datasets. With the first index, termed ReachGrid, at the query time
only a small necessary portion of the contact network which is required for
reachability evaluation is constructed and traversed. With the second approach,
termed ReachGraph, we precompute reachability at different scales and leverage
these precalculations at the query time for efficient query processing. We
optimize the placement of both indexes on disk to enable efficient index
traversal during query processing. We study the pros and cons of our proposed
approaches by performing extensive experiments with both real and synthetic
data. Based on our experimental results, our proposed approaches outperform
existing reachability query processing techniques in contact n...[truncated].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6697</identifier>
 <datestamp>2012-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6697</id><created>2012-05-30</created><authors><author><keyname>Nguyen</keyname><forenames>Thi</forenames></author><author><keyname>He</keyname><forenames>Zhen</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author><author><keyname>Ward</keyname><forenames>Phillip</forenames></author></authors><title>Boosting Moving Object Indexing through Velocity Partitioning</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 9, pp.
  860-871 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There have been intense research interests in moving object indexing in the
past decade. However, existing work did not exploit the important property of
skewed velocity distributions. In many real world scenarios, objects travel
predominantly along only a few directions. Examples include vehicles on road
networks, flights, people walking on the streets, etc. The search space for a
query is heavily dependent on the velocity distribution of the objects grouped
in the nodes of an index tree. Motivated by this observation, we propose the
velocity partitioning (VP) technique, which exploits the skew in velocity
distribution to speed up query processing using moving object indexes. The VP
technique first identifies the &quot;dominant velocity axes (DVAs)&quot; using a
combination of principal components analysis (PCA) and k-means clustering.
Then, a moving object index (e.g., a TPR-tree) is created based on each DVA,
using the DVA as an axis of the underlying coordinate system. An object is
maintained in the index whose DVA is closest to the object's current moving
direction. Thus, all the objects in an index are moving in a near 1-dimensional
space instead of a 2-dimensional space. As a result, the expansion of the
search space with time is greatly reduced, from a quadratic function of the
maximum speed (of the objects in the search range) to a near linear function of
the maximum speed. The VP technique can be applied to a wide range of moving
object index structures. We have implemented the VP technique on two
representative ones, the TPR*-tree and the Bx-tree. Extensive experiments
validate that the VP technique consistently improves the performance of those
index structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6698</identifier>
 <datestamp>2012-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6698</id><created>2012-05-30</created><authors><author><keyname>Bidoit-Tollu</keyname><forenames>Nicole</forenames></author><author><keyname>Colazzo</keyname><forenames>Dario</forenames></author><author><keyname>Ulliana</keyname><forenames>Federico</forenames></author></authors><title>Type-Based Detection of XML Query-Update Independence</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 9, pp.
  872-883 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel static analysis technique to detect XML
query-update independence, in the presence of a schema. Rather than types, our
system infers chains of types. Each chain represents a path that can be
traversed on a valid document during query/update evaluation. The resulting
independence analysis is precise, although it raises a challenging issue:
recursive schemas may lead to infer infinitely many chains. A sound and
complete approximation technique ensuring a finite analysis in any case is
presented, together with an efficient implementation performing the chain-based
analysis in polynomial space and time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6699</identifier>
 <datestamp>2012-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6699</id><created>2012-05-30</created><authors><author><keyname>Sowell</keyname><forenames>Benjamin</forenames></author><author><keyname>Golab</keyname><forenames>Wojciech</forenames></author><author><keyname>Shah</keyname><forenames>Mehul A.</forenames></author></authors><title>Minuet: A Scalable Distributed Multiversion B-Tree</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 9, pp.
  884-895 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data management systems have traditionally been designed to support either
long-running analytics queries or short-lived transactions, but an increasing
number of applications need both. For example, online games, socio-mobile apps,
and e-commerce sites need to not only maintain operational state, but also
analyze that data quickly to make predictions and recommendations that improve
user experience. In this paper, we present Minuet, a distributed, main-memory
B-tree that supports both transactions and copy-on-write snapshots for in-situ
analytics. Minuet uses main-memory storage to enable low-latency transactional
operations as well as analytics queries without compromising transaction
performance. In addition to supporting read-only analytics queries on
snapshots, Minuet supports writable clones, so that users can create branching
versions of the data. This feature can be quite useful, e.g. to support complex
&quot;what-if&quot; analysis or to facilitate wide-area replication. Our experiments show
that Minuet outperforms a commercial main-memory database in many ways. It
scales to hundreds of cores and TBs of memory, and can process hundreds of
thousands of B-tree operations per second while executing long-running scans.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6700</identifier>
 <datestamp>2012-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6700</id><created>2012-05-30</created><authors><author><keyname>Yin</keyname><forenames>Hongzhi</forenames></author><author><keyname>Cui</keyname><forenames>Bin</forenames></author><author><keyname>Li</keyname><forenames>Jing</forenames></author><author><keyname>Yao</keyname><forenames>Junjie</forenames></author><author><keyname>Chen</keyname><forenames>Chen</forenames></author></authors><title>Challenging the Long Tail Recommendation</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 9, pp.
  896-907 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The success of &quot;infinite-inventory&quot; retailers such as Amazon.com and Netflix
has been largely attributed to a &quot;long tail&quot; phenomenon. Although the majority
of their inventory is not in high demand, these niche products, unavailable at
limited-inventory competitors, generate a significant fraction of total revenue
in aggregate. In addition, tail product availability can boost head sales by
offering consumers the convenience of &quot;one-stop shopping&quot; for both their
mainstream and niche tastes. However, most of existing recommender systems,
especially collaborative filter based methods, can not recommend tail products
due to the data sparsity issue. It has been widely acknowledged that to
recommend popular products is easier yet more trivial while to recommend long
tail products adds more novelty yet it is also a more challenging task. In this
paper, we propose a novel suite of graph-based algorithms for the long tail
recommendation. We first represent user-item information with undirected
edge-weighted graph and investigate the theoretical foundation of applying
Hitting Time algorithm for long tail item recommendation. To improve
recommendation diversity and accuracy, we extend Hitting Time and propose
efficient Absorbing Time algorithm to help users find their favorite long tail
items. Finally, we refine the Absorbing Time algorithm and propose two
entropy-biased Absorbing Cost algorithms to distinguish the variation on
different user-item rating pairs, which further enhances the effectiveness of
long tail recommendation. Empirical experiments on two real life datasets show
that our proposed algorithms are effective to recommend long tail items and
outperform state-of-the-art recommendation techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6714</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6714</id><created>2012-05-30</created><updated>2012-08-13</updated><authors><author><keyname>Salo</keyname><forenames>Ville</forenames><affiliation>University of Turku, Finland</affiliation></author></authors><title>On Nilpotency and Asymptotic Nilpotency of Cellular Automata</title><categories>math.DS cs.FL nlin.CG</categories><comments>In Proceedings AUTOMATA&amp;JAC 2012, arXiv:1208.2498</comments><proxy>EPTCS</proxy><acm-class>F.1.1; F.1.2; F.1.3</acm-class><journal-ref>EPTCS 90, 2012, pp. 86-96</journal-ref><doi>10.4204/EPTCS.90.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove a conjecture of P. Guillon and G. Richard by showing that cellular
automata that eventually fix all cells to a fixed symbol 0 are nilpotent on
S^Z^d for all d. We also briefly discuss nilpotency on other subshifts, and
show that weak nilpotency implies nilpotency in all subshifts and all
dimensions, since we do not know a published reference for this.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6717</identifier>
 <datestamp>2012-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6717</id><created>2012-05-30</created><authors><author><keyname>Durocher</keyname><forenames>Stephane</forenames></author><author><keyname>Leblanc</keyname><forenames>Alexandre</forenames></author><author><keyname>Morrison</keyname><forenames>Jason</forenames></author><author><keyname>Skala</keyname><forenames>Matthew</forenames></author></authors><title>Robust Non-Parametric Data Approximation of Pointsets via Data Reduction</title><categories>cs.CG</categories><comments>13 pages, 6 figures</comments><acm-class>F.2.1; G.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a novel non-parametric method of simplifying
piecewise linear curves and we apply this method as a statistical approximation
of structure within sequential data in the plane. We consider the problem of
minimizing the average length of sequences of consecutive input points that lie
on any one side of the simplified curve. Specifically, given a sequence $P$ of
$n$ points in the plane that determine a simple polygonal chain consisting of
$n-1$ segments, we describe algorithms for selecting an ordered subset $Q
\subset P$ (including the first and last points of $P$) that determines a
second polygonal chain to approximate $P$, such that the number of crossings
between the two polygonal chains is maximized, and the cardinality of $Q$ is
minimized among all such maximizing subsets of $P$. Our algorithms have
respective running times $O(n^2\log n)$ when $P$ is monotonic and $O(n^2\log^2
n)$ when $P$ is an arbitrary simple polyline. Finally, we examine the
application of our algorithms iteratively in a bootstrapping technique to
define a smooth robust non-parametric approximation of the original sequence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6745</identifier>
 <datestamp>2012-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6745</id><created>2012-05-30</created><authors><author><keyname>Gnanasivam</keyname><forenames>P</forenames></author><author><keyname>Muttan</keyname><forenames>Dr. S</forenames></author></authors><title>Fingerprint Gender Classification using Wavelet Transform and Singular
  Value Decomposition</title><categories>cs.CV</categories><comments>12 figures and 6 tables</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 2, No 3, March 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel method of gender Classification from fingerprint is proposed based on
discrete wavelet transform (DWT) and singular value decomposition (SVD). The
classification is achieved by extracting the energy computed from all the
sub-bands of DWT combined with the spatial features of non-zero singular values
obtained from the SVD of fingerprint images. K nearest neighbor (KNN) used as a
classifier. This method is experimented with the internal database of 3570
fingerprints finger prints in which 1980 were male fingerprints and 1590 were
female fingerprints. Finger-wise gender classification is achieved which is
94.32% for the left hand little fingers of female persons and 95.46% for the
left hand index finger of male persons. Gender classification for any finger of
male persons tested is attained as 91.67% and 84.69% for female persons
respectively. Overall classification rate is 88.28% has been achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6752</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6752</id><created>2012-05-30</created><updated>2012-09-17</updated><authors><author><keyname>Ghavami</keyname><forenames>Siavash</forenames></author><author><keyname>Lahouti</keyname><forenames>Farshad</forenames></author><author><keyname>Masoudi-Nejad</keyname><forenames>Ali</forenames></author></authors><title>Modeling and Analysis of Abnormality Detection in Biomolecular
  Nano-Networks</title><categories>cs.IT math.IT q-bio.BM q-bio.MN</categories><comments>31 pages, 13 figures, Invited from IEEE MoNaCom 2012 to Journal of
  Nano Communication Networks</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A scheme for detection of abnormality in molecular nano-networks is proposed.
This is motivated by the fact that early diagnosis, classification and
detection of diseases such as cancer play a crucial role in their successful
treatment. The proposed nano-abnormality detection scheme (NADS) comprises of a
two-tier network of sensor nano-machines (SNMs) in the first tier and a data
gathering node (DGN) at the sink. The SNMs detect the presence of competitor
cells as abnormality that is captured by variations in parameters of a
nano-communications channel. In the second step, the SNMs transmit micro-scale
messages over a noisy micro communications channel (MCC) to the DGN, where a
decision is made upon fusing the received signals. The detection performance of
each SNM is analyzed by setting up a Neyman-Pearson test. Next, taking into
account the effect of the MCC, the overall performance of the proposed NADS is
quantified in terms of probabilities of misdetection and false alarm. A design
problem is formulated, when the optimized concentration of SNMs in a sample is
obtained for a high probability of detection and a limited probability of false
alarm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6773</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6773</id><created>2012-05-30</created><authors><author><keyname>Vinayakray-Jani</keyname><forenames>Preetida</forenames></author><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author></authors><title>Proactive TCP mechanism to improve Handover performance in Mobile
  Satellite and Terrestrial Networks</title><categories>cs.NI</categories><comments>5 pages, 2 figures</comments><doi>10.5120/7546-0643</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Emerging standardization of Geo Mobile Radio (GMR-1) for satellite system is
having strong resemblance to terrestrial GSM (Global System for Mobile
communications) at the upper protocol layers and TCP (Transmission Control
Protocol) is one of them. This space segment technology as well as terrestrial
technology, is characterized by periodic variations in communication properties
and coverage causing the termination of ongoing call as connections of Mobile
Nodes (MN) alter stochastically. Although provisions are made to provide
efficient communication infrastructure this hybrid space and terrestrial
networks must ensure the end-to-end network performance so that MN can move
seamlessly among these networks. However from connectivity point of view
current TCP performance has not been engineered for mobility events in
multi-radio MN. Traditionally, TCP has applied a set of congestion control
algorithms (slow-start, congestion avoidance, fast retransmit, fast recovery)
to probe the currently available bandwidth on the connection path. These
algorithms need several round-trip times to find the correct transmission rate
(i.e. congestion window), and adapt to sudden changes connectivity due to
handover. While there are protocols to maintain the connection continuity on
mobility events, such as Mobile IP (MIP) and Host Identity Protocol (HIP), TCP
performance engineering has had less attention. TCP is implemented as a
separate component in an operating system, and is therefore often unaware of
the mobility events or the nature of multi-radios' communication. This paper
aims to improve TCP communication performance in Mobile satellite and
terrestrial networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6775</identifier>
 <datestamp>2012-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6775</id><created>2012-05-30</created><authors><author><keyname>Mandal</keyname><forenames>J. K.</forenames></author><author><keyname>Das</keyname><forenames>Debashis</forenames></author></authors><title>Steganography Using Adaptive Pixel Value Differencing(APVD) of Gray
  Images Through Exclusion of Overflow/Underflow</title><categories>cs.CR</categories><comments>10 pages,3 figures,The Second International Conference on Computer
  Science, engineering and Applications (CCSEA-2012), Delhi, India. May 2012</comments><doi>10.5121/csit.20122201-10.5121/csit.2012.2243</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a gray scale image the pixel value ranges from 0 to 255. But when we use
pixel-value differencing (pvd) method as image steganographic scheme, the pixel
values in the stego-image may exceed gray scale range. An adaptive
steganography based on modified pixel-value differencing through management of
pixel values within the range of gray scale has been proposed in this paper.
PVD method is used and check whether the pixel value exceeds the range on
embedding. Positions where the pixel exceeds boundary has been marked and a
delicate handle is used to keep the value within the range. From the
experimental it is seen that the results obtained in proposed method provides
with identical payload and visual fidelity of stego-image compared to the pvd
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6785</identifier>
 <datestamp>2012-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6785</id><created>2012-05-30</created><authors><author><keyname>Ceccherini-Silberstein</keyname><forenames>Tullio</forenames></author><author><keyname>Coornaert</keyname><forenames>Michel</forenames></author><author><keyname>Fiorenzi</keyname><forenames>Francesca</forenames></author><author><keyname>Sunic</keyname><forenames>Zoran</forenames></author></authors><title>Cellular automata on regular rooted trees</title><categories>cs.FL cs.DM</categories><journal-ref>CIAA 2012, Lect. Notes in Comput. Sci. 7381 (2012), 101-112</journal-ref><doi>10.1007/978-3-642-31606-7_9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study cellular automata on regular rooted trees. This includes the
characterization of sofic tree shifts in terms of unrestricted Rabin automata
and the decidability of the surjectivity problem for cellular automata between
sofic tree shifts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6787</identifier>
 <datestamp>2012-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6787</id><created>2012-05-30</created><authors><author><keyname>Mucha</keyname><forenames>Marcin</forenames></author></authors><title>Lyndon Words and Short Superstrings</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Shortest-Superstring problem, we are given a set of strings S and want
to ?find a string that contains all strings in S as substrings and has minimum
length. This is a classical problem in approximation and the best known
approximation factor is 2 1/2, given by Sweedyk in 1999. Since then no
improvement has been made, howerever two other approaches yielding a 2
1/2-approximation algorithms have been proposed by Kaplan et al. and recently
by Paluch et al., both based on a reduction to maximum asymmetric TSP path
(Max-ATSP-Path) and structural results of Breslauer et al.
  In this paper we give an algorithm that achieves an approximation ratio of 2
11/23, breaking through the long-standing bound of 2 1/2.
  We use the standard reduction of Shortest-Superstring to Max-ATSP-Path. The
new, somewhat surprising, algorithmic idea is to take the better of the two
solutions obtained by using: (a) the currently best 2/3-approximation algorithm
for Max-ATSP-Path and (b) a naive cycle-cover based 1/2-approximation
algorithm. To prove that this indeed results in an improvement, we further
develop a theory of string overlaps, extending the results of Breslauer et al.
This theory is based on the novel use of Lyndon words, as a substitute for
generic unbordered rotations and critical factorizations, as used by Breslauer
et al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6791</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6791</id><created>2012-05-30</created><updated>2013-12-25</updated><authors><author><keyname>Sandomirskiy</keyname><forenames>Fedor</forenames></author></authors><title>Repeated games of incomplete information with large sets of states</title><categories>cs.GT cs.IT math.IT math.OC math.PR</categories><comments>21 pages; concluding remarks added; to appear in International
  Journal of Game Theory</comments><msc-class>91A20 (Primary), 94A17, 60G42, 62C10 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The famous theorem of R.Aumann and M.Maschler states that the sequence of
values of an N-stage zero-sum game G_N with incomplete information on one side
converges as N tends to infinity, and the error term is bounded by a constant
divided by square root of N if the set of states K is finite. The paper deals
with the case of infinite K. It turns out that for countably-supported prior
distribution p with heavy tails the error term can decrease arbitrarily slowly.
The slowest possible speed of the decreasing for a given p is determined in
terms of entropy-like family of functionals. Our approach is based on the
well-known connection between the behavior of the maximal variation of
measure-valued martingales and asymptotic properties of repeated games with
incomplete information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6822</identifier>
 <datestamp>2014-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6822</id><created>2012-05-30</created><authors><author><keyname>Ball</keyname><forenames>Brian</forenames></author><author><keyname>Newman</keyname><forenames>M. E. J.</forenames></author></authors><title>Friendship networks and social status</title><categories>cs.SI physics.soc-ph</categories><comments>7 pages, 6 figures</comments><journal-ref>Network Science 1, 16-30 (2013)</journal-ref><doi>10.1017/nws.2012.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In empirical studies of friendship networks participants are typically asked,
in interviews or questionnaires, to identify some or all of their close
friends, resulting in a directed network in which friendships can, and often
do, run in only one direction between a pair of individuals. Here we analyze a
large collection of such networks representing friendships among students at US
high and junior-high schools and show that the pattern of unreciprocated
friendships is far from random. In every network, without exception, we find
that there exists a ranking of participants, from low to high, such that almost
all unreciprocated friendships consist of a lower-ranked individual claiming
friendship with a higher-ranked one. We present a maximum-likelihood method for
deducing such rankings from observed network data and conjecture that the
rankings produced reflect a measure of social status. We note in particular
that reciprocated and unreciprocated friendships obey different statistics,
suggesting different formation processes, and that rankings are correlated with
other characteristics of the participants that are traditionally associated
with status, such as age and overall popularity as measured by total number of
friends.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6832</identifier>
 <datestamp>2012-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6832</id><created>2012-01-20</created><authors><author><keyname>Lortal</keyname><forenames>Ga&#xeb;lle</forenames><affiliation>TRT</affiliation></author><author><keyname>Grau</keyname><forenames>Brigitte</forenames><affiliation>LIMSI</affiliation></author><author><keyname>Zock</keyname><forenames>Michael</forenames><affiliation>LIF</affiliation></author></authors><title>Syst\`eme d'aide \`a l'acc\`es lexical : trouver le mot qu'on a sur le
  bout de la langue</title><categories>cs.CL</categories><comments>TALN, Fez : Maroc (2004)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study of the Tip of the Tongue phenomenon (TOT) provides valuable clues
and insights concerning the organisation of the mental lexicon (meaning, number
of syllables, relation with other words, etc.). This paper describes a tool
based on psycho-linguistic observations concerning the TOT phenomenon. We've
built it to enable a speaker/writer to find the word he is looking for, word he
may know, but which he is unable to access in time. We try to simulate the TOT
phenomenon by creating a situation where the system knows the target word, yet
is unable to access it. In order to find the target word we make use of the
paradigmatic and syntagmatic associations stored in the linguistic databases.
Our experiment allows the following conclusion: a tool like SVETLAN, capable to
structure (automatically) a dictionary by domains can be used sucessfully to
help the speaker/writer to find the word he is looking for, if it is combined
with a database rich in terms of paradigmatic links like EuroWordNet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6845</identifier>
 <datestamp>2012-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6845</id><created>2012-05-30</created><authors><author><keyname>Mansour</keyname><forenames>Hassan</forenames></author><author><keyname>Yilmaz</keyname><forenames>Ozgur</forenames></author></authors><title>Weighted-{$\ell_1$} minimization with multiple weighting sets</title><categories>cs.IT math.IT</categories><comments>Proceedings of the SPIE, Wavelets and Sparsity XIV, San Diego, August
  2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the support recovery conditions of weighted $\ell_1$
minimization for signal reconstruction from compressed sensing measurements
when multiple support estimate sets with different accuracy are available. We
identify a class of signals for which the recovered vector from $\ell_1$
minimization provides an accurate support estimate. We then derive stability
and robustness guarantees for the weighted $\ell_1$ minimization problem with
more than one support estimate. We show that applying a smaller weight to
support estimate that enjoy higher accuracy improves the recovery conditions
compared with the case of a single support estimate and the case with standard,
i.e., non-weighted, $\ell_1$ minimization. Our theoretical results are
supported by numerical simulations on synthetic signals and real audio signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6846</identifier>
 <datestamp>2012-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6846</id><created>2012-05-30</created><authors><author><keyname>Mansour</keyname><forenames>Hassan</forenames></author><author><keyname>Yilmaz</keyname><forenames>Ozgur</forenames></author></authors><title>Support driven reweighted $\ell_1$ minimization</title><categories>cs.IT math.IT</categories><comments>Proc. of the IEEE International Conference on Acoustics, Speech, and
  Signal Processing (ICASSP), March, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a support driven reweighted $\ell_1$ minimization
algorithm (SDRL1) that solves a sequence of weighted $\ell_1$ problems and
relies on the support estimate accuracy. Our SDRL1 algorithm is related to the
IRL1 algorithm proposed by Cand{\`e}s, Wakin, and Boyd. We demonstrate that it
is sufficient to find support estimates with \emph{good} accuracy and apply
constant weights instead of using the inverse coefficient magnitudes to achieve
gains similar to those of IRL1. We then prove that given a support estimate
with sufficient accuracy, if the signal decays according to a specific rate,
the solution to the weighted $\ell_1$ minimization problem results in a support
estimate with higher accuracy than the initial estimate. We also show that
under certain conditions, it is possible to achieve higher estimate accuracy
when the intersection of support estimates is considered. We demonstrate the
performance of SDRL1 through numerical simulations and compare it with that of
IRL1 and standard $\ell_1$ minimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6849</identifier>
 <datestamp>2012-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6849</id><created>2012-05-30</created><authors><author><keyname>Mansour</keyname><forenames>Hassan</forenames></author></authors><title>Beyond $\ell_1$-norm minimization for sparse signal recovery</title><categories>cs.IT cs.LG math.IT</categories><comments>IEEE Workshop on Statistical Signal Processing (SSP), August 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse signal recovery has been dominated by the basis pursuit denoise (BPDN)
problem formulation for over a decade. In this paper, we propose an algorithm
that outperforms BPDN in finding sparse solutions to underdetermined linear
systems of equations at no additional computational cost. Our algorithm, called
WSPGL1, is a modification of the spectral projected gradient for $\ell_1$
minimization (SPGL1) algorithm in which the sequence of LASSO subproblems are
replaced by a sequence of weighted LASSO subproblems with constant weights
applied to a support estimate. The support estimate is derived from the data
and is updated at every iteration. The algorithm also modifies the Pareto curve
at every iteration to reflect the new weighted $\ell_1$ minimization problem
that is being solved. We demonstrate through extensive simulations that the
sparse recovery performance of our algorithm is superior to that of $\ell_1$
minimization and approaches the recovery performance of iterative re-weighted
$\ell_1$ (IRWL1) minimization of Cand{\`e}s, Wakin, and Boyd, although it does
not match it in general. Moreover, our algorithm has the computational cost of
a single BPDN problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6852</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6852</id><created>2012-05-30</created><updated>2013-06-12</updated><authors><author><keyname>Awan</keyname><forenames>Zohaib Hassan</forenames></author><author><keyname>Zaidi</keyname><forenames>Abdellatif</forenames></author><author><keyname>Vandendorpe</keyname><forenames>Luc</forenames></author></authors><title>Multiaccess Channel with Partially Cooperating Encoders and Security
  Constraints</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Transactions on Information Forensics and Security</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a special case of Willems's two-user multi-access channel with
partially cooperating encoders from a security perspective. This model differs
from Willems's setup in that only one encoder, Encoder 1, is allowed to
conference; Encoder 2 does not transmit any message, and there is an additional
passive eavesdropper from whom the communication should be kept secret. For the
discrete memoryless (DM) case, we establish inner and outer bounds on the
capacity-equivocation region. The inner bound is based on a combination of
Willems's coding scheme, noise injection and additional binning that provides
randomization for security. For the memoryless Gaussian model, we establish
lower and upper bounds on the secrecy capacity. We also show that, under
certain conditions, these bounds agree in some extreme cases of cooperation
between the encoders. We illustrate our results through some numerical
examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6855</identifier>
 <datestamp>2012-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6855</id><created>2012-05-30</created><authors><author><keyname>Lin</keyname><forenames>Jimmy</forenames></author><author><keyname>Mishne</keyname><forenames>Gilad</forenames></author></authors><title>A Study of &quot;Churn&quot; in Tweets and Real-Time Search Queries (Extended
  Version)</title><categories>cs.IR cs.SI</categories><comments>This is an extended version of a similarly-titled paper at the 6th
  International AAAI Conference on Weblogs and Social Media (ICWSM 2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The real-time nature of Twitter means that term distributions in tweets and
in search queries change rapidly: the most frequent terms in one hour may look
very different from those in the next. Informally, we call this phenomenon
&quot;churn&quot;. Our interest in analyzing churn stems from the perspective of
real-time search. Nearly all ranking functions, machine-learned or otherwise,
depend on term statistics such as term frequency, document frequency, as well
as query frequencies. In the real-time context, how do we compute these
statistics, considering that the underlying distributions change rapidly? In
this paper, we present an analysis of tweet and query churn on Twitter, as a
first step to answering this question. Analyses reveal interesting insights on
the temporal dynamics of term distributions on Twitter and hold implications
for the design of search systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6862</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6862</id><created>2012-05-30</created><updated>2012-08-14</updated><authors><author><keyname>Balan</keyname><forenames>Horia Vlad</forenames></author><author><keyname>Rogalin</keyname><forenames>Ryan</forenames></author><author><keyname>Michaloliakos</keyname><forenames>Antonios</forenames></author><author><keyname>Psounis</keyname><forenames>Konstantinos</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>AirSync: Enabling Distributed Multiuser MIMO with Full Spatial
  Multiplexing</title><categories>cs.NI</categories><comments>Submitted to Transactions on Networking</comments><report-no>CENG-TR-2012-1</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The enormous success of advanced wireless devices is pushing the demand for
higher wireless data rates. Denser spectrum reuse through the deployment of
more access points per square mile has the potential to successfully meet the
increasing demand for more bandwidth. In theory, the best approach to density
increase is via distributed multiuser MIMO, where several access points are
connected to a central server and operate as a large distributed multi-antenna
access point, ensuring that all transmitted signal power serves the purpose of
data transmission, rather than creating &quot;interference.&quot; In practice, while
enterprise networks offer a natural setup in which distributed MIMO might be
possible, there are serious implementation difficulties, the primary one being
the need to eliminate phase and timing offsets between the jointly coordinated
access points.
  In this paper we propose AirSync, a novel scheme which provides not only time
but also phase synchronization, thus enabling distributed MIMO with full
spatial multiplexing gains. AirSync locks the phase of all access points using
a common reference broadcasted over the air in conjunction with a Kalman filter
which closely tracks the phase drift. We have implemented AirSync as a digital
circuit in the FPGA of the WARP radio platform. Our experimental testbed,
comprised of two access points and two clients, shows that AirSync is able to
achieve phase synchronization within a few degrees, and allows the system to
nearly achieve the theoretical optimal multiplexing gain. We also discuss MAC
and higher layer aspects of a practical deployment. To the best of our
knowledge, AirSync offers the first ever realization of the full multiuser MIMO
gain, namely the ability to increase the number of wireless clients linearly
with the number of jointly coordinated access points, without reducing the per
client rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6867</identifier>
 <datestamp>2012-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6867</id><created>2012-05-30</created><updated>2012-08-31</updated><authors><author><keyname>Matsen</keyname><forenames>Frederick A.</forenames></author><author><keyname>Gallagher</keyname><forenames>Aaron</forenames></author><author><keyname>McCoy</keyname><forenames>Connor</forenames></author></authors><title>Minimizing the average distance to a closest leaf in a phylogenetic tree</title><categories>q-bio.PE cs.DM</categories><comments>Please contact us with any comments or questions!</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When performing an analysis on a collection of molecular sequences, it can be
convenient to reduce the number of sequences under consideration while
maintaining some characteristic of a larger collection of sequences. For
example, one may wish to select a subset of high-quality sequences that
represent the diversity of a larger collection of sequences. One may also wish
to specialize a large database of characterized &quot;reference sequences&quot; to a
smaller subset that is as close as possible on average to a collection of
&quot;query sequences&quot; of interest. Such a representative subset can be useful
whenever one wishes to find a set of reference sequences that is appropriate to
use for comparative analysis of environmentally-derived sequences, such as for
selecting &quot;reference tree&quot; sequences for phylogenetic placement of metagenomic
reads. In this paper we formalize these problems in terms of the minimization
of the Average Distance to the Closest Leaf (ADCL) and investigate algorithms
to perform the relevant minimization. We show that the greedy algorithm is not
effective, show that a variant of the Partitioning Among Medoids (PAM)
heuristic gets stuck in local minima, and develop an exact dynamic programming
approach. Using this exact program we note that the performance of PAM appears
to be good for simulated trees, and is faster than the exact algorithm for
small trees. On the other hand, the exact program gives solutions for all
numbers of leaves less than or equal to the given desired number of leaves,
while PAM only gives a solution for the pre-specified number of leaves. Via
application to real data, we show that the ADCL criterion chooses chimeric
sequences less often than random subsets, while the maximization of
phylogenetic diversity chooses them more often than random. These algorithms
have been implemented in publicly available software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6903</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6903</id><created>2012-05-31</created><updated>2012-06-15</updated><authors><author><keyname>Kar</keyname><forenames>Swarnendu</forenames></author><author><keyname>Varshney</keyname><forenames>Pramod K.</forenames></author><author><keyname>Palaniswami</keyname><forenames>Marimuthu</forenames></author></authors><title>Cram\'er-Rao Bounds for Polynomial Signal Estimation using Sensors with
  AR(1) Drift</title><categories>cs.IT math.IT</categories><comments>14 pages, 6 figures, This paper will appear in the Oct/Nov 2012 issue
  of IEEE Transactions on Signal Processing</comments><doi>10.1109/TSP.2012.2204989</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We seek to characterize the estimation performance of a sensor network where
the individual sensors exhibit the phenomenon of drift, i.e., a gradual change
of the bias. Though estimation in the presence of random errors has been
extensively studied in the literature, the loss of estimation performance due
to systematic errors like drift have rarely been looked into. In this paper, we
derive closed-form Fisher Information matrix and subsequently Cram\'er-Rao
bounds (upto reasonable approximation) for the estimation accuracy of
drift-corrupted signals. We assume a polynomial time-series as the
representative signal and an autoregressive process model for the drift. When
the Markov parameter for drift \rho&lt;1, we show that the first-order effect of
drift is asymptotically equivalent to scaling the measurement noise by an
appropriate factor. For \rho=1, i.e., when the drift is non-stationary, we show
that the constant part of a signal can only be estimated inconsistently
(non-zero asymptotic variance). Practical usage of the results are demonstrated
through the analysis of 1) networks with multiple sensors and 2) bandwidth
limited networks communicating only quantized observations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6904</identifier>
 <datestamp>2012-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6904</id><created>2012-05-31</created><authors><author><keyname>Bassil</keyname><forenames>Youssef</forenames></author></authors><title>A Simulation Model for the Waterfall Software Development Life Cycle</title><categories>cs.SE</categories><comments>LACSC - Lebanese Association for Computational Sciences,
  http://www.lacsc.org; International Journal of Engineering &amp; Technology, Vol.
  2, No. 5, May 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software development life cycle or SDLC for short is a methodology for
designing, building, and maintaining information and industrial systems. So
far, there exist many SDLC models, one of which is the Waterfall model which
comprises five phases to be completed sequentially in order to develop a
software solution. However, SDLC of software systems has always encountered
problems and limitations that resulted in significant budget overruns, late or
suspended deliveries, and dissatisfied clients. The major reason for these
deficiencies is that project directors are not wisely assigning the required
number of workers and resources on the various activities of the SDLC.
Consequently, some SDLC phases with insufficient resources may be delayed;
while, others with excess resources may be idled, leading to a bottleneck
between the arrival and delivery of projects and to a failure in delivering an
operational product on time and within budget. This paper proposes a simulation
model for the Waterfall development process using the Simphony.NET simulation
tool whose role is to assist project managers in determining how to achieve the
maximum productivity with the minimum number of expenses, workers, and hours.
It helps maximizing the utilization of development processes by keeping all
employees and resources busy all the time to keep pace with the arrival of
projects and to decrease waste and idle time. As future work, other SDLC models
such as spiral and incremental are to be simulated, giving project executives
the choice to use a diversity of software development methodologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6907</identifier>
 <datestamp>2012-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6907</id><created>2012-05-31</created><authors><author><keyname>Kar</keyname><forenames>Swarnendu</forenames></author><author><keyname>Chen</keyname><forenames>Hao</forenames></author><author><keyname>Varshney</keyname><forenames>Pramod K.</forenames></author></authors><title>Optimal Identical Binary Quantizer Design for Distributed Estimation</title><categories>cs.IT math.IT</categories><comments>6 pages, 3 figures, This paper has been accepted for publication in
  IEEE Transactions in Signal Processing</comments><doi>10.1109/TSP.2012.2191777</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the design of identical one-bit probabilistic quantizers for
distributed estimation in sensor networks. We assume the parameter-range to be
finite and known and use the maximum Cram\'er-Rao Lower Bound (CRB) over the
parameter-range as our performance metric. We restrict our theoretical analysis
to the class of antisymmetric quantizers and determine a set of conditions for
which the probabilistic quantizer function is greatly simplified. We identify a
broad class of noise distributions, which includes Gaussian noise in the
low-SNR regime, for which the often used threshold-quantizer is found to be
minimax-optimal. Aided with theoretical results, we formulate an optimization
problem to obtain the optimum minimax-CRB quantizer. For a wide range of noise
distributions, we demonstrate the superior performance of the new quantizer -
particularly in the moderate to high-SNR regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6910</identifier>
 <datestamp>2012-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6910</id><created>2012-05-31</created><authors><author><keyname>Bourouis</keyname><forenames>Abderrahim</forenames></author><author><keyname>Feham</keyname><forenames>Mohamed</forenames></author><author><keyname>Bouchachia</keyname><forenames>Abdelhamid</forenames></author></authors><title>A New Architecture of a Ubiquitous Health Monitoring System: A Prototype
  Of Cloud Mobile Health Monitoring System</title><categories>cs.NI cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Body Area Sensor Networks (WBASN) is an emerging technology which
uses wireless sensors to implement real-time wearable health monitoring of
patients to enhance independent living. In this paper we propose a prototype of
cloud mobile health monitoring system. The system uses WBASN and Smartphone
application that uses cloud computing, location data and a neural network to
determine the state of patients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6917</identifier>
 <datestamp>2012-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6917</id><created>2012-05-31</created><authors><author><keyname>De Persis</keyname><forenames>Claudio</forenames></author><author><keyname>Frasca</keyname><forenames>Paolo</forenames></author></authors><title>Robust self-triggered coordination with ternary controllers</title><categories>cs.SY math.OC</categories><comments>13 pages (twocolumn format), 4 figures. Submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper regards coordination of networked systems, which is studied in the
framework of hybrid dynamical systems. We design a coordination scheme which
combines the use of ternary controllers with a self-triggered communication
policy. The communication policy requires the agents to collect, at each
sampling time, relative measurements of their neighbors' states: the collected
information is then used to update the control and determine the following
sampling time. We prove that the proposed scheme ensures finite-time
convergence to a neighborhood of a consensus state. We then study the
robustness of the proposed self-triggered coordination system with respect to
skews in the agents' local clocks, to delays, and to limited precision in
communication. Furthermore, we present two significant variations of our
scheme. First, we design a time-varying controller which asymptotically drives
the system to consensus. Second, we adapt our framework to a communication
model in which an agent does not poll all its neighbors simultaneously, but
single neighbors instead. This communication policy actually leads to a
self-triggered &quot;gossip&quot; coordination system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6919</identifier>
 <datestamp>2012-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6919</id><created>2012-05-31</created><authors><author><keyname>Kar</keyname><forenames>Swarnendu</forenames></author><author><keyname>Varshney</keyname><forenames>Pramod K.</forenames></author></authors><title>Accurate Estimation of Gaseous Strength using Transient Data</title><categories>cs.SY</categories><comments>10 pages, 9 figures</comments><journal-ref>IEEE Transactions on Instrumentation and Measurement, vol.60,
  no.4, pp.1197-1205, April 2011</journal-ref><doi>10.1109/TIM.2010.2084731</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information about the strength of gas sources in buildings has a number of
applications in the area of building automation and control, including
temperature and ventilation control, fire detection and security systems. Here,
we consider the problem of estimating the strength of a gas source in an
enclosure when some of the parameters of the gas transport process are unknown.
Traditionally, these problems are either solved by the Maximum-Likelihood (ML)
method which is accurate but computationally intense, or by Recursive Least
Squares (RLS, also Kalman) filtering which is simpler but less accurate. In
this paper, we suggest a different statistical estimation procedure based on
the concept of Method of Moments. We outline techniques that make this
procedure computationally efficient and amenable for recursive implementation.
We provide a comparative analysis of our proposed method based on experimental
results as well as Monte-Carlo simulations. When used with the building control
systems, these algorithms can estimate the gaseous strength in a room both
quickly and accurately, and can potentially provide improved indoor air quality
in an efficient manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6925</identifier>
 <datestamp>2012-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6925</id><created>2012-05-31</created><authors><author><keyname>Kar</keyname><forenames>Swarnendu</forenames></author><author><keyname>Varshney</keyname><forenames>Pramod K.</forenames></author><author><keyname>Chen</keyname><forenames>Hao</forenames></author></authors><title>Spatial Whitening Framework for Distributed Estimation</title><categories>cs.IT math.IT</categories><comments>4 pages, 2 figures, this paper has been presented at CAMSAP 2011;
  Proc. 4th Intl. Workshop on Computational Advances in Multi-Sensor Adaptive
  Processing (CAMSAP 2011), San Juan, Puerto Rico, Dec 13-16, 2011</comments><doi>10.1109/CAMSAP.2011.6136007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing resource allocation strategies for power constrained sensor network
in the presence of correlated data often gives rise to intractable problem
formulations. In such situations, applying well-known strategies derived from
conditional-independence assumption may turn out to be fairly suboptimal. In
this paper, we address this issue by proposing an adjacency-based spatial
whitening scheme, where each sensor exchanges its observation with their
neighbors prior to encoding their own private information and transmitting it
to the fusion center. We comment on the computational limitations for obtaining
the optimal whitening transformation, and propose an iterative optimization
scheme to achieve the same for large networks. We demonstrate the efficacy of
the whitening framework by considering the example of bit-allocation for
distributed estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6928</identifier>
 <datestamp>2012-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6928</id><created>2012-05-31</created><authors><author><keyname>Atig</keyname><forenames>Mohamed Faouzi</forenames></author><author><keyname>Bouajjani</keyname><forenames>Ahmed</forenames></author><author><keyname>Kumar</keyname><forenames>K. Narayan</forenames></author><author><keyname>Saivasan</keyname><forenames>Prakash</forenames></author></authors><title>Model checking Branching-Time Properties of Multi-Pushdown Systems is
  Hard</title><categories>cs.LO cs.FL cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the model checking problem for shared memory concurrent programs
modeled as multi-pushdown systems. We consider here boolean programs with a
finite number of threads and recursive procedures. It is well-known that the
model checking problem is undecidable for this class of programs. In this
paper, we investigate the decidability and the complexity of this problem under
the assumption of bounded context-switching defined by Qadeer and Rehof, and of
phase-boundedness proposed by La Torre et al. On the model checking of such
systems against temporal logics and in particular branching time logics such as
the modal $\mu$-calculus or CTL has received little attention. It is known that
parity games, which are closely related to the modal $\mu$-calculus, are
decidable for the class of bounded-phase systems (and hence for bounded-context
switching as well), but with non-elementary complexity (Seth). A natural
question is whether this high complexity is inevitable and what are the ways to
get around it. This paper addresses these questions and unfortunately, and
somewhat surprisingly, it shows that branching model checking for MPDSs is
inherently an hard problem with no easy solution. We show that parity games on
MPDS under phase-bounding restriction is non-elementary. Our main result shows
that model checking a $k$ context bounded MPDS against a simple fragment of
CTL, consisting of formulas that whose temporal operators come from the set
${\EF, \EX}$, has a non-elementary lower bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6935</identifier>
 <datestamp>2013-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6935</id><created>2012-05-31</created><updated>2013-01-16</updated><authors><author><keyname>Geiger</keyname><forenames>Bernhard C.</forenames></author><author><keyname>Kubin</keyname><forenames>Gernot</forenames></author></authors><title>Signal Enhancement as Minimization of Relevant Information Loss</title><categories>cs.IT math.IT</categories><comments>9 pages; 4 figures; accepted for presentation at a conference</comments><journal-ref>Proc. ITG Conf. on Systems, Communication and Coding, 2013, pp.
  1-6</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the notion of relevant information loss for the purpose of
casting the signal enhancement problem in information-theoretic terms. We show
that many algorithms from machine learning can be reformulated using relevant
information loss, which allows their application to the aforementioned problem.
As a particular example we analyze principle component analysis for
dimensionality reduction, discuss its optimality, and show that the relevant
information loss can indeed vanish if the relevant information is concentrated
on a lower-dimensional subspace of the input space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6960</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6960</id><created>2012-05-31</created><updated>2012-11-23</updated><authors><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>Hajiaghayi</keyname><forenames>MohammadTaghi</forenames></author><author><keyname>Marx</keyname><forenames>D&#xe1;niel</forenames></author></authors><title>Minimizing Movement: Fixed-Parameter Tractability</title><categories>cs.DS</categories><comments>A preliminary version of the paper appeared in ESA 2009</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study an extensive class of movement minimization problems which arise
from many practical scenarios but so far have little theoretical study. In
general, these problems involve planning the coordinated motion of a collection
of agents (representing robots, people, map labels, network messages, etc.) to
achieve a global property in the network while minimizing the maximum or
average movement (expended energy). The only previous theoretical results about
this class of problems are about approximation, and mainly negative: many
movement problems of interest have polynomial inapproximability. Given that the
number of mobile agents is typically much smaller than the complexity of the
environment, we turn to fixed-parameter tractability. We characterize the
boundary between tractable and intractable movement problems in a very general
set up: it turns out the complexity of the problem fundamentally depends on the
treewidth of the minimal configurations. Thus the complexity of a particular
problem can be determined by answering a purely combinatorial question. Using
our general tools, we determine the complexity of several concrete problems and
fortunately show that many movement problems of interest can be solved
efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6961</identifier>
 <datestamp>2012-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6961</id><created>2012-05-31</created><authors><author><keyname>Haeupler</keyname><forenames>Bernhard</forenames></author></authors><title>Tighter Worst-Case Bounds on Algebraic Gossip</title><categories>cs.DS cs.DC cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gossip and in particular network coded algebraic gossip have recently
attracted attention as a fast, bandwidth-efficient, reliable and distributed
way to broadcast or multicast multiple messages. While the algorithms are
simple, involved queuing approaches are used to study their performance. The
most recent result in this direction shows that uniform algebraic gossip
disseminates k messages in O({\Delta}(D + k + log n)) rounds where D is the
diameter, n the size of the network and {\Delta} the maximum degree.
  In this paper we give a simpler, short and self-contained proof for this
worst-case guarantee. Our approach also allows to reduce the quadratic
{\Delta}D term to min{3n, {\Delta}D}. We furthermore show that a simple round
robin routing scheme also achieves min{3n, {\Delta}D} + {\Delta}k rounds,
eliminating both randomization and coding. Lastly, we combine a recent
non-uniform gossip algorithm with a simple routing scheme to get a O(D + k +
log^{O(1)}) gossip information dissemination algorithm. This is order optimal
as long as D and k are not both polylogarithmically small.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6974</identifier>
 <datestamp>2012-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6974</id><created>2012-05-31</created><authors><author><keyname>Misra</keyname><forenames>Vinith</forenames></author><author><keyname>Weissman</keyname><forenames>Tsachy</forenames></author></authors><title>The Porosity of Additive Noise Sequences</title><categories>cs.IT math.IT</categories><comments>22 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a binary additive noise channel with noiseless feedback. When the
noise is a stationary and ergodic process $\mathbf{Z}$, the capacity is
$1-\mathbb{H}(\mathbf{Z})$ ($\mathbb{H}(\cdot)$ denoting the entropy rate). It
is shown analogously that when the noise is a deterministic sequence
$z^\infty$, the capacity under finite-state encoding and decoding is
$1-\bar{\rho}(z^\infty)$, where $\bar{\rho}(\cdot)$ is Lempel and Ziv's
finite-state compressibility. This quantity is termed the \emph{porosity}
$\underline{\sigma}(\cdot)$ of an individual noise sequence. A sequence of
schemes are presented that universally achieve porosity for any noise sequence.
These converse and achievability results may be interpreted both as a
channel-coding counterpart to Ziv and Lempel's work in universal source coding,
as well as an extension to the work by Lomnitz and Feder and Shayevitz and
Feder on communication across modulo-additive channels. Additionally, a
slightly more practical architecture is suggested that draws a connection with
finite-state predictability, as introduced by Feder, Gutman, and Merhav.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.6990</identifier>
 <datestamp>2012-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.6990</id><created>2012-05-27</created><authors><author><keyname>Gnang</keyname><forenames>Edinah K.</forenames></author><author><keyname>Nanda</keyname><forenames>Vidit</forenames></author></authors><title>The devil is in Asymmetries (Rough Version)</title><categories>math.AG cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We formally investigate some computational obstacles to tractability of
computing the variety determined by K complex polynomials in N boolean
variables. We show that using algebraic methods for solving combinatorial
problems, the obstacles to tractability lies in the order of magnitude of
asymmetries admitted by the given system of equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.7009</identifier>
 <datestamp>2012-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.7009</id><created>2012-05-31</created><authors><author><keyname>Zhu</keyname><forenames>Yaojia</forenames></author><author><keyname>Yan</keyname><forenames>Xiaoran</forenames></author><author><keyname>Moore</keyname><forenames>Cristopher</forenames></author></authors><title>Oriented and Degree-generated Block Models: Generating and Inferring
  Communities with Inhomogeneous Degree Distributions</title><categories>cs.SI cond-mat.stat-mech physics.soc-ph stat.ML</categories><comments>22 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The stochastic block model is a powerful tool for inferring community
structure from network topology. However, it predicts a Poisson degree
distribution within each community, while most real-world networks have a
heavy-tailed degree distribution. The degree-corrected block model can
accommodate arbitrary degree distributions within communities. But since it
takes the vertex degrees as parameters rather than generating them, it cannot
use them to help it classify the vertices, and its natural generalization to
directed graphs cannot even use the orientations of the edges. In this paper,
we present variants of the block model with the best of both worlds: they can
use vertex degrees and edge orientations in the classification process, while
tolerating heavy-tailed degree distributions within communities. We show that
for some networks, including synthetic networks and networks of word
adjacencies in English text, these new block models achieve a higher accuracy
than either standard or degree-corrected block models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.7014</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.7014</id><created>2012-05-31</created><updated>2014-08-28</updated><authors><author><keyname>Alon</keyname><forenames>Noga</forenames></author><author><keyname>Ghaffari</keyname><forenames>Mohsen</forenames></author><author><keyname>Haeupler</keyname><forenames>Bernhard</forenames></author><author><keyname>Khabbazian</keyname><forenames>Majid</forenames></author></authors><title>Broadcast Throughput in Radio Networks: Routing vs. Network Coding</title><categories>cs.DS cs.DC math.CO</categories><doi>10.1137/1.9781611973402.132</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The broadcast throughput in a network is defined as the average number of
messages that can be transmitted per unit time from a given source to all other
nodes when time goes to infinity.
  Classical broadcast algorithms treat messages as atomic tokens and route them
from the source to the receivers by making intermediate nodes store and forward
messages. The more recent network coding approach, in contrast, prompts
intermediate nodes to mix and code together messages. It has been shown that
certain wired networks have an asymptotic network coding gap, that is, they
have asymptotically higher broadcast throughput when using network coding
compared to routing. Whether such a gap exists for wireless networks has been
an open question of great interest. We approach this question by studying the
broadcast throughput of the radio network model which has been a standard
mathematical model to study wireless communication.
  We show that there is a family of radio networks with a tight $\Theta(\log
\log n)$ network coding gap, that is, networks in which the asymptotic
throughput achievable via routing messages is a $\Theta(\log \log n)$ factor
smaller than that of the optimal network coding algorithm. We also provide new
tight upper and lower bounds that show that the asymptotic worst-case broadcast
throughput over all networks with $n$ nodes is $\Theta(1 / \log n)$
messages-per-round for both routing and network coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.7016</identifier>
 <datestamp>2012-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.7016</id><created>2012-05-31</created><authors><author><keyname>Wu</keyname><forenames>Rongjun</forenames></author><author><keyname>Hong</keyname><forenames>Shaofang</forenames></author></authors><title>On deep holes of generalized Reed-Solomon codes</title><categories>math.NT cs.IT math.IT</categories><comments>5 pages. arXiv admin note: text overlap with arXiv:1108.3524</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Determining deep holes is an important topic in decoding Reed-Solomon codes.
In a previous paper \cite{[WH]}, we showed that the received word $u$ is a deep
hole of the standard Reed-Solomon codes $[q-1, k]_q$ if its Lagrange
interpolation polynomial is the sum of monomial of degree $q-2$ and a
polynomial of degree at most $k-1$. In this paper, we extend this result by
giving a new class of deep holes of the generalized Reed-Solomon codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.7025</identifier>
 <datestamp>2012-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.7025</id><created>2012-05-31</created><authors><author><keyname>Morvan</keyname><forenames>Gildas</forenames></author><author><keyname>Dupont</keyname><forenames>Daniel</forenames></author><author><keyname>Soyez</keyname><forenames>Jean-Baptiste</forenames></author><author><keyname>Merzouki</keyname><forenames>Rochdi</forenames></author></authors><title>Engineering hierarchical complex systems: an agent-based approach. The
  case of flexible manufacturing systems</title><categories>cs.MA</categories><journal-ref>Studies in Computational Intelligence, Volume 402, p. 49-60 2012</journal-ref><doi>10.1007/978-3-642-27449-7_4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article introduces a formal model to specify, model and validate
hierarchical complex systems described at different levels of analysis. It
relies on concepts that have been developed in the multi-agent-based simulation
(MABS) literature: level, influence and reaction. One application of such model
is the specification of hierarchical complex systems, in which decisional
capacities are dynamically adapted at each level with respect to the
emergences/constraints paradigm. In the conclusion, we discuss the main
perspective of this work: the definition of a generic meta-model for holonic
multi-agent systems (HMAS).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.7031</identifier>
 <datestamp>2012-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.7031</id><created>2012-05-31</created><updated>2012-08-01</updated><authors><author><keyname>Schuh</keyname><forenames>Fabian</forenames></author><author><keyname>Huber</keyname><forenames>Johannes B.</forenames></author></authors><title>Nonlinear Trellis Description for Convolutionally Encoded Transmission
  Over ISI-channels with Applications for CPM</title><categories>cs.IT math.IT</categories><comments>6 pages, 13 figures, submitted for IEEE-SCC-13</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper we propose a matched decoding scheme for convolutionally
encoded transmission over intersymbol interference (ISI) channels and devise a
nonlinear trellis description. As an application we show that for coded
continuous phase modulation (CPM) using a non-coherent receiver the number of
states of the super trellis can be significantly reduced by means of a matched
non-linear trellis encoder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.7036</identifier>
 <datestamp>2012-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.7036</id><created>2012-05-31</created><authors><author><keyname>Delfosse</keyname><forenames>Nicolas</forenames><affiliation>IMB</affiliation></author><author><keyname>Z&#xe9;mor</keyname><forenames>Gilles</forenames><affiliation>IMB</affiliation></author></authors><title>Upper Bounds on the Rate of Low Density Stabilizer Codes for the Quantum
  Erasure Channel</title><categories>quant-ph cs.IT math.CO math.IT</categories><comments>32 pages</comments><msc-class>94B99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using combinatorial arguments, we determine an upper bound on achievable
rates of stabilizer codes used over the quantum erasure channel. This allows us
to recover the no-cloning bound on the capacity of the quantum erasure channel,
R is below 1-2p, for stabilizer codes: we also derive an improved upper bound
of the form : R is below 1-2p-D(p) with a function D(p) that stays positive for
0 &lt; p &lt; 1/2 and for any family of stabilizer codes whose generators have
weights bounded from above by a constant - low density stabilizer codes.
  We obtain an application to percolation theory for a family of self-dual
tilings of the hyperbolic plane. We associate a family of low density
stabilizer codes with appropriate finite quotients of these tilings. We then
relate the probability of percolation to the probability of a decoding error
for these codes on the quantum erasure channel. The application of our upper
bound on achievable rates of low density stabilizer codes gives rise to an
upper bound on the critical probability for these tilings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.7041</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.7041</id><created>2012-05-31</created><updated>2012-12-03</updated><authors><author><keyname>Kiefer</keyname><forenames>Stefan</forenames></author></authors><title>BPA Bisimilarity is EXPTIME-hard</title><categories>cs.FL cs.CC</categories><comments>technical report for a an article that is to appear in Information
  Processing Letters. The present version takes into account improvements
  prompted by the journal's reviewers</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a basic process algebra (BPA) and two stack symbols, the BPA
bisimilarity problem asks whether the two stack symbols are bisimilar. We show
that this problem is EXPTIME-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.7044</identifier>
 <datestamp>2012-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.7044</id><created>2012-05-31</created><authors><author><keyname>Golrezaei</keyname><forenames>Negin</forenames></author><author><keyname>Dimakis</keyname><forenames>Alexandros G.</forenames></author><author><keyname>Molisch</keyname><forenames>Andreas F.</forenames></author></authors><title>Wireless Device-to-Device Communications with Distributed Caching</title><categories>cs.IT cs.NI math.IT</categories><comments>to appear in ISIT 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a novel wireless device-to-device (D2D) collaboration
architecture that exploits distributed storage of popular content to enable
frequency reuse. We identify a fundamental conflict between collaboration
distance and interference and show how to optimize the transmission power to
maximize frequency reuse. Our analysis depends on the user content request
statistics which are modeled by a Zipf distribution. Our main result is a
closed form expression of the optimal collaboration distance as a function of
the content reuse distribution parameters. We show that if the Zipf exponent of
the content reuse distribution is greater than 1, it is possible to have a
number of D2D interference-free collaboration pairs that scales linearly in the
number of nodes. If the Zipf exponent is smaller than 1, we identify the best
possible scaling in the number of D2D collaborating links. Surprisingly, a very
simple distributed caching policy achieves the optimal scaling behavior and
therefore there is no need to centrally coordinate what each node is caching.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.7074</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.7074</id><created>2012-05-31</created><updated>2013-08-09</updated><authors><author><keyname>Ayyer</keyname><forenames>Arvind</forenames></author><author><keyname>Klee</keyname><forenames>Steven</forenames></author><author><keyname>Schilling</keyname><forenames>Anne</forenames></author></authors><title>Combinatorial Markov chains on linear extensions</title><categories>math.CO cs.DS math.PR</categories><comments>35 pages, more examples of promotion, rephrased the main theorems in
  terms of discrete time Markov chains</comments><msc-class>06A07, 20M32, 20M30, 47D03, 60J27</msc-class><journal-ref>Journal of Algebraic Combinatorics, Volume 39, Issue 4 (2014),
  Page 853-881</journal-ref><doi>10.1007/s10801-013-0470-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider generalizations of Schuetzenberger's promotion operator on the
set L of linear extensions of a finite poset of size n. This gives rise to a
strongly connected graph on L. By assigning weights to the edges of the graph
in two different ways, we study two Markov chains, both of which are
irreducible. The stationary state of one gives rise to the uniform
distribution, whereas the weights of the stationary state of the other has a
nice product formula. This generalizes results by Hendricks on the Tsetlin
library, which corresponds to the case when the poset is the anti-chain and
hence L=S_n is the full symmetric group. We also provide explicit eigenvalues
of the transition matrix in general when the poset is a rooted forest. This is
shown by proving that the associated monoid is R-trivial and then using
Steinberg's extension of Brown's theory for Markov chains on left regular bands
to R-trivial monoids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0021</identifier>
 <datestamp>2012-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0021</id><created>2012-05-31</created><authors><author><keyname>Bennett</keyname><forenames>Casey C.</forenames></author></authors><title>Clinical Productivity System - A Decision Support Model</title><categories>cs.DB</categories><comments>Keywords: Decision support systems, clinical; Efficiency,
  organizational; Clinical productivity; Healthcare; Electronic Health Records</comments><journal-ref>International Journal of Productivity and Performance Management.
  60(3): 311-319 (2010)</journal-ref><doi>10.1108/17410401111112014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Purpose: This goal of this study was to evaluate the effects of a data-driven
clinical productivity system that leverages Electronic Health Record (EHR) data
to provide productivity decision support functionality in a real-world clinical
setting. The system was implemented for a large behavioral health care provider
seeing over 75,000 distinct clients a year. Design/methodology/approach: The
key metric in this system is a &quot;VPU&quot;, which simultaneously optimizes multiple
aspects of clinical care. The resulting mathematical value of clinical
productivity was hypothesized to tightly link the organization's performance to
its expectations and, through transparency and decision support tools at the
clinician level, affect significant changes in productivity, quality, and
consistency relative to traditional models of clinical productivity. Findings:
In only 3 months, every single variable integrated into the VPU system showed
significant improvement, including a 30% rise in revenue, 10% rise in clinical
percentage, a 25% rise in treatment plan completion, a 20% rise in case rate
eligibility, along with similar improvements in compliance/audit issues,
outcomes collection, access, etc. Practical implications: A data-driven
clinical productivity system employing decision support functionality is
effective because of the impact on clinician behavior relative to traditional
clinical productivity systems. Critically, the model is also extensible to
integration with outcomes-based productivity. Originality/Value: EHR's are only
a first step - the problem is turning that data into useful information.
Technology can leverage the data in order to produce actionable information
that can inform clinical practice and decision-making. Without additional
technology, EHR's are essentially just copies of paper-based records stored in
electronic form.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0038</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0038</id><created>2012-05-31</created><authors><author><keyname>Calafiore</keyname><forenames>Giuseppe C.</forenames></author><author><keyname>Fagiano</keyname><forenames>Lorenzo</forenames></author></authors><title>Robust Model Predictive Control via Scenario Optimization</title><categories>cs.SY math.OC</categories><comments>This manuscript is a preprint of a paper accepted for publication in
  the IEEE Transactions on Automatic Control, with DOI:
  10.1109/TAC.2012.2203054, and is subject to IEEE copyright. The copy of
  record will be available at http://ieeexplore.ieee.org</comments><journal-ref>IEEE Transactions on Automatic Control, vol. 58, n. 1, pp. 219-
  224, 2013</journal-ref><doi>10.1109/TAC.2012.2203054</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses a novel probabilistic approach for the design of robust
model predictive control (MPC) laws for discrete-time linear systems affected
by parametric uncertainty and additive disturbances. The proposed technique is
based on the iterated solution, at each step, of a finite-horizon optimal
control problem (FHOCP) that takes into account a suitable number of randomly
extracted scenarios of uncertainty and disturbances, followed by a specific
command selection rule implemented in a receding horizon fashion. The scenario
FHOCP is always convex, also when the uncertain parameters and disturbance
belong to non-convex sets, and irrespective of how the model uncertainty
influences the system's matrices. Moreover, the computational complexity of the
proposed approach does not depend on the uncertainty/disturbance dimensions,
and scales quadratically with the control horizon. The main result in this
paper is related to the analysis of the closed loop system under
receding-horizon implementation of the scenario FHOCP, and essentially states
that the devised control law guarantees constraint satisfaction at each step
with some a-priori assigned probability p, while the system's state reaches the
target set either asymptotically, or in finite time with probability at least
p. The proposed method may be a valid alternative when other existing
techniques, either deterministic or stochastic, are not directly usable due to
excessive conservatism or to numerical intractability caused by lack of
convexity of the robust or chance-constrained optimization problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0042</identifier>
 <datestamp>2012-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0042</id><created>2012-05-31</created><authors><author><keyname>Belzner</keyname><forenames>Megan</forenames></author><author><keyname>Colin-Ellerin</keyname><forenames>Sean</forenames></author><author><keyname>Roman</keyname><forenames>Jorge H.</forenames></author></authors><title>Language Acquisition in Computers</title><categories>cs.CL</categories><comments>39 pages, 10 figures and 6 tables</comments><acm-class>I.2.6; I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This project explores the nature of language acquisition in computers, guided
by techniques similar to those used in children. While existing natural
language processing methods are limited in scope and understanding, our system
aims to gain an understanding of language from first principles and hence
minimal initial input. The first portion of our system was implemented in Java
and is focused on understanding the morphology of language using bigrams. We
use frequency distributions and differences between them to define and
distinguish languages. English and French texts were analyzed to determine a
difference threshold of 55 before the texts are considered to be in different
languages, and this threshold was verified using Spanish texts. The second
portion of our system focuses on gaining an understanding of the syntax of a
language using a recursive method. The program uses one of two possible methods
to analyze given sentences based on either sentence patterns or surrounding
words. Both methods have been implemented in C++. The program is able to
understand the structure of simple sentences and learn new words. In addition,
we have provided some suggestions regarding future work and potential
extensions of the existing program.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0050</identifier>
 <datestamp>2012-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0050</id><created>2012-05-31</created><authors><author><keyname>Tal</keyname><forenames>Ido</forenames></author><author><keyname>Vardy</keyname><forenames>Alexander</forenames></author></authors><title>List Decoding of Polar Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a successive-cancellation \emph{list} decoder for polar codes,
which is a generalization of the classic successive-cancellation decoder of
Ar{\i}kan. In the proposed list decoder, up to $L$ decoding paths are
considered concurrently at each decoding stage. Then, a single codeword is
selected from the list as output. If the most likely codeword is selected,
simulation results show that the resulting performance is very close to that of
a maximum-likelihood decoder, even for moderate values of $L$. Alternatively,
if a &quot;genie&quot; is allowed to pick the codeword from the list, the results are
comparable to the current state of the art LDPC codes. Luckily, implementing
such a helpful genie is easy.
  Our list decoder doubles the number of decoding paths at each decoding step,
and then uses a pruning procedure to discard all but the $L$ &quot;best&quot; paths. %In
order to implement this algorithm, we introduce a natural pruning criterion
that can be easily evaluated. Nevertheless, a straightforward implementation
still requires $\Omega(L \cdot n^2)$ time, which is in stark contrast with the
$O(n \log n)$ complexity of the original successive-cancellation decoder. We
utilize the structure of polar codes to overcome this problem. Specifically, we
devise an efficient, numerically stable, implementation taking only $O(L \cdot
n \log n)$ time and $O(L \cdot n)$ space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0051</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0051</id><created>2012-05-31</created><updated>2013-02-20</updated><authors><author><keyname>Qin</keyname><forenames>Chengjie</forenames></author><author><keyname>Rusu</keyname><forenames>Florin</forenames></author></authors><title>PF-OLA: A High-Performance Framework for Parallel On-Line Aggregation</title><categories>cs.DB cs.DC</categories><comments>36 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online aggregation provides estimates to the final result of a computation
during the actual processing. The user can stop the computation as soon as the
estimate is accurate enough, typically early in the execution. This allows for
the interactive data exploration of the largest datasets. In this paper we
introduce the first framework for parallel online aggregation in which the
estimation virtually does not incur any overhead on top of the actual
execution. We define a generic interface to express any estimation model that
abstracts completely the execution details. We design a novel estimator
specifically targeted at parallel online aggregation. When executed by the
framework over a massive $8\text{TB}$ TPC-H instance, the estimator provides
accurate confidence bounds early in the execution even when the cardinality of
the final result is seven orders of magnitude smaller than the dataset size and
without incurring overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0068</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0068</id><created>2012-05-31</created><updated>2015-04-15</updated><authors><author><keyname>Nguyen</keyname><forenames>XuanLong</forenames></author></authors><title>Posterior contraction of the population polytope in finite admixture
  models</title><categories>math.ST cs.LG stat.TH</categories><comments>Published at http://dx.doi.org/10.3150/13-BEJ582 in the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ582</report-no><journal-ref>Bernoulli 2015, Vol. 21, No. 1, 618-646</journal-ref><doi>10.3150/13-BEJ582</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the posterior contraction behavior of the latent population
structure that arises in admixture models as the amount of data increases. We
adopt the geometric view of admixture models - alternatively known as topic
models - as a data generating mechanism for points randomly sampled from the
interior of a (convex) population polytope, whose extreme points correspond to
the population structure variables of interest. Rates of posterior contraction
are established with respect to Hausdorff metric and a minimum matching
Euclidean metric defined on polytopes. Tools developed include posterior
asymptotics of hierarchical models and arguments from convex geometry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0069</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0069</id><created>2012-05-31</created><updated>2014-08-28</updated><authors><author><keyname>Nuida</keyname><forenames>Koji</forenames></author><author><keyname>Abe</keyname><forenames>Takuro</forenames></author><author><keyname>Kaji</keyname><forenames>Shizuo</forenames></author><author><keyname>Maeno</keyname><forenames>Toshiaki</forenames></author><author><keyname>Numata</keyname><forenames>Yasuhide</forenames></author></authors><title>A mathematical problem for security analysis of hash functions and
  pseudorandom generators</title><categories>cs.CR math.CO</categories><comments>18 pages; (v2) 19 pages, to appear in International Journal of
  Foundations of Computer Science</comments><msc-class>94A60 (Primary), 68R05, 52C99 (Secondary)</msc-class><journal-ref>International Journal of Foundations of Computer Science, vol.26,
  no.2 (2015) 169--194</journal-ref><doi>10.1142/S0129054115500100</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we specify a class of mathematical problems, which we refer to
as &quot;Function Density Problems&quot; (FDPs, in short), and point out novel
connections of FDPs to the following two cryptographic topics; theoretical
security evaluations of keyless hash functions (such as SHA-1), and
constructions of provably secure pseudorandom generators (PRGs) with some
enhanced security property introduced by Dubrov and Ishai [STOC 2006]. Our
argument aims at proposing new theoretical frameworks for these topics
(especially for the former) based on FDPs, rather than providing some concrete
and practical results on the topics. We also give some examples of mathematical
discussions on FDPs, which would be of independent interest from mathematical
viewpoints. Finally, we discuss possible directions of future research on other
cryptographic applications of FDPs and on mathematical studies on FDPs
themselves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0087</identifier>
 <datestamp>2013-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0087</id><created>2012-06-01</created><updated>2013-09-20</updated><authors><author><keyname>Page</keyname><forenames>Aurel</forenames><affiliation>IMB, INRIA Bordeaux - Sud-Ouest</affiliation></author></authors><title>Computing arithmetic Kleinian groups</title><categories>math.NT cs.SC</categories><comments>Revisions according to the comments of the referee</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Arithmetic Kleinian groups are arithmetic lattices in PSL_2(C). We present an
algorithm which, given such a group Gamma, returns a fundamental domain and a
finite presentation for Gamma with a computable isomorphism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0089</identifier>
 <datestamp>2012-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0089</id><created>2012-06-01</created><authors><author><keyname>Li</keyname><forenames>Chuanyou</forenames><affiliation>INRIA - SUPELEC</affiliation></author><author><keyname>Hurfin</keyname><forenames>Michel</forenames><affiliation>INRIA - SUPELEC</affiliation></author><author><keyname>Wang</keyname><forenames>Yun</forenames></author></authors><title>Reaching Approximate Byzantine Consensus in Partially-Connected Mobile
  Networks</title><categories>cs.DC</categories><comments>No. RR-7985 (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of approximate consensus in mobile networks
containing Byzantine nodes. We assume that each correct node can communicate
only with its neighbors and has no knowledge of the global topology. As all
nodes have moving ability, the topology is dynamic. The number of Byzantine
nodes is bounded by f and known by all correct nodes. We first introduce an
approximate Byzantine consensus protocol which is based on the linear iteration
method. As nodes are allowed to collect information during several consecutive
rounds, moving gives them the opportunity to gather more values. We propose a
novel sufficient and necessary condition to guarantee the final convergence of
the consensus protocol. The requirement expressed by our condition is not
&quot;universal&quot;: in each phase it affects only a single correct node. More
precisely, at least one correct node among those that propose either the
minimum or the maximum value which is present in the network, has to receive
enough messages (quantity constraint) with either higher or lower values
(quality constraint). Of course, nodes' motion should not prevent this
requirement to be fulfilled. Our conclusion shows that the proposed condition
can be satisfied if the total number of nodes is greater than 3f+1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0103</identifier>
 <datestamp>2012-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0103</id><created>2012-06-01</created><authors><author><keyname>Munari</keyname><forenames>Andrea</forenames></author><author><keyname>Levorato</keyname><forenames>Marco</forenames></author><author><keyname>Zorzi</keyname><forenames>Michele</forenames></author></authors><title>Cooperation in Carrier Sense Based Wireless Ad Hoc Networks - Part I:
  Reactive Schemes</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cooperative techniques have been shown to significantly improve the
performance of wireless systems. Despite being a mature technology in single
communication link scenarios, their implementation in wider, and practical,
networks poses several challenges which have not been fully identified and
understood so far. In this two-part paper, the implementation of cooperative
communications in non-centralized ad hoc networks with sensing-based channel
access is extensively discussed. Both analysis and simulation are employed to
provide a clear understanding of the mutual influence between the link layer
contention mechanism and collaborative protocols. Part I of this work focuses
on reactive cooperation, in which relaying is triggered by packet delivery
failure events, while Part II addresses proactive approaches, preemptively
initiated by the source based on channel state information. Results show that
sensing-based channel access significantly hampers the effectiveness of
cooperation by biasing the spatial distribution of available relays, and by
inducing a level of spatial and temporal correlation of the interference that
diminishes the diversity improvement on which cooperative gains are founded.
Moreover, the efficiency reduction entailed by several practical protocol
issues related to carrier sense multiple access which are typically neglected
in the literature is thoroughly investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0104</identifier>
 <datestamp>2012-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0104</id><created>2012-06-01</created><authors><author><keyname>Pratiwi</keyname><forenames>Dian</forenames></author></authors><title>The Use of Self Organizing Map Method and Feature Selection in Image
  Database Classification System</title><categories>cs.IR cs.DB</categories><comments>5 pages, 5 figures, 2 tables. citation in IJCSI volume 9 issue 3</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a technique in classifying the images into a number of
classes or clusters desired by means of Self Organizing Map (SOM) Artificial
Neural Network method. A number of 250 color images to be classified as
previously done some processing, such as RGB to grayscale color conversion,
color histogram, feature vector selection, and then classifying by the SOM
Feature vector selection in this paper will use two methods, namely by PCA
(Principal Component Analysis) and LSA (Latent Semantic Analysis) in which each
of these methods would have taken the characteristic vector of 50, 100, and 150
from 256 initial feature vector into the process of color histogram. Then the
selection will be processed into the SOM network to be classified into five
classes using a learning rate of 0.5 and calculated accuracy. Classification of
some of the test results showed that the highest percentage of accuracy
obtained when using PCA and the selection of 100 feature vector that is equal
to 88%, compared to when using LSA selection that only 74%. Thus it can be
concluded that the method fits the PCA feature selection methods are applied in
conjunction with SOM and has an accuracy rate better than the LSA feature
selection methods. Keywords: Color Histogram, Feature Selection, LSA, PCA, SOM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0107</identifier>
 <datestamp>2012-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0107</id><created>2012-06-01</created><authors><author><keyname>Munari</keyname><forenames>Andrea</forenames></author><author><keyname>Levorato</keyname><forenames>Marco</forenames></author><author><keyname>Zorzi</keyname><forenames>Michele</forenames></author></authors><title>Cooperation in Carrier Sense Based Wireless Ad Hoc Networks - Part II:
  Proactive Schemes</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work is the second of a two-part series of papers on the effectiveness
of cooperative techniques in non-centralized carrier sense-based ad hoc
wireless networks. While Part I extensively discussed reactive cooperation,
characterized by relayed transmissions triggered by failure events at the
intended receiver, Part II investigates in depth proactive solutions, in which
the source of a packet exploits channel state information to preemptively
coordinate with relays in order to achieve the optimal overall rate to the
destination. In particular, this work shows by means of both analysis and
simulation that the performance of reactive cooperation is reduced by the
intrinsic nature of the considered medium access policy, which biases the
distribution of the available relays, locating them in unfavorable positions
for rate optimization. Moreover, the highly dynamic nature of interference that
characterizes non-infrastructured ad hoc networks is proved to hamper the
efficacy and the reliability of preemptively allocated cooperative links, as
unpredicted births and deaths of surrounding transmissions may force relays to
abort their support and/or change the maximum achievable rate at the intended
receiver. As a general conclusion, our work extensively suggests that
CSMA-based link layers are not apt to effectively support cooperative
strategies in large-scale non-centralized ad hoc networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0108</identifier>
 <datestamp>2012-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0108</id><created>2012-06-01</created><updated>2012-08-16</updated><authors><author><keyname>Pan</keyname><forenames>Raj Kumar</forenames></author><author><keyname>Sinha</keyname><forenames>Sitabhra</forenames></author><author><keyname>Kaski</keyname><forenames>Kimmo</forenames></author><author><keyname>Saram&#xe4;ki</keyname><forenames>Jari</forenames></author></authors><title>The evolution of interdisciplinarity in physics research</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>Published version, 10 pages, 8 figures + Supplementary Information</comments><journal-ref>Scientific Reports 2, 551 (2012)</journal-ref><doi>10.1038/srep00551</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Science, being a social enterprise, is subject to fragmentation into groups
that focus on specialized areas or topics. Often new advances occur through
cross-fertilization of ideas between sub-fields that otherwise have little
overlap as they study dissimilar phenomena using different techniques. Thus to
explore the nature and dynamics of scientific progress one needs to consider
the large-scale organization and interactions between different subject areas.
Here, we study the relationships between the sub-fields of Physics using the
Physics and Astronomy Classification Scheme (PACS) codes employed for
self-categorization of articles published over the past 25 years (1985-2009).
We observe a clear trend towards increasing interactions between the different
sub-fields. The network of sub-fields also exhibits core-periphery
organization, the nucleus being dominated by Condensed Matter and General
Physics. However, over time Interdisciplinary Physics is steadily increasing
its share in the network core, reflecting a shift in the overall trend of
Physics research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0111</identifier>
 <datestamp>2012-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0111</id><created>2012-06-01</created><authors><author><keyname>Andres</keyname><forenames>Bjoern</forenames></author><author><keyname>Beier</keyname><forenames>Thorsten</forenames></author><author><keyname>Kappes</keyname><forenames>Joerg H.</forenames></author></authors><title>OpenGM: A C++ Library for Discrete Graphical Models</title><categories>cs.AI cs.MS stat.ML</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  OpenGM is a C++ template library for defining discrete graphical models and
performing inference on these models, using a wide range of state-of-the-art
algorithms. No restrictions are imposed on the factor graph to allow for
higher-order factors and arbitrary neighborhood structures. Large models with
repetitive structure are handled efficiently because (i) functions that occur
repeatedly need to be stored only once, and (ii) distinct functions can be
implemented differently, using different encodings alongside each other in the
same model. Several parametric functions (e.g. metrics), sparse and dense value
tables are provided and so is an interface for custom C++ code. Algorithms are
separated by design from the representation of graphical models and are easily
exchangeable. OpenGM, its algorithms, HDF5 file format and command line tools
are modular and extendible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0115</identifier>
 <datestamp>2012-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0115</id><created>2012-06-01</created><authors><author><keyname>Agullo</keyname><forenames>Emmanuel</forenames><affiliation>INRIA Bordeaux - Sud-Ouest, LaBRI</affiliation></author><author><keyname>Bramas</keyname><forenames>B&#xe9;ranger</forenames><affiliation>INRIA Bordeaux - Sud-Ouest, LaBRI</affiliation></author><author><keyname>Coulaud</keyname><forenames>Olivier</forenames><affiliation>INRIA Bordeaux - Sud-Ouest, LaBRI</affiliation></author><author><keyname>Darve</keyname><forenames>Eric</forenames><affiliation>INRIA Bordeaux - Sud-Ouest, LaBRI</affiliation></author><author><keyname>Messner</keyname><forenames>Matthias</forenames><affiliation>INRIA Bordeaux - Sud-Ouest, LaBRI</affiliation></author><author><keyname>Toru</keyname><forenames>Takahashi</forenames></author></authors><title>Pipelining the Fast Multipole Method over a Runtime System</title><categories>cs.DC</categories><comments>No. RR-7981 (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fast Multipole Methods (FMM) are a fundamental operation for the simulation
of many physical problems. The high performance design of such methods usually
requires to carefully tune the algorithm for both the targeted physics and the
hardware. In this paper, we propose a new approach that achieves high
performance across architectures. Our method consists of expressing the FMM
algorithm as a task flow and employing a state-of-the-art runtime system,
StarPU, in order to process the tasks on the different processing units. We
carefully design the task flow, the mathematical operators, their Central
Processing Unit (CPU) and Graphics Processing Unit (GPU) implementations, as
well as scheduling schemes. We compute potentials and forces of 200 million
particles in 48.7 seconds on a homogeneous 160 cores SGI Altix UV 100 and of 38
million particles in 13.34 seconds on a heterogeneous 12 cores Intel Nehalem
processor enhanced with 3 Nvidia M2090 Fermi GPUs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0122</identifier>
 <datestamp>2012-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0122</id><created>2012-06-01</created><authors><author><keyname>M&#xe9;hus</keyname><forenames>Jean-Eudes</forenames><affiliation>CREC</affiliation></author><author><keyname>Batista</keyname><forenames>Thais</forenames><affiliation>DIMAP</affiliation></author><author><keyname>Buisson</keyname><forenames>J&#xe9;r&#xe9;my</forenames><affiliation>CREC, IRISA, UEB</affiliation></author></authors><title>ACME vs PDDL: support for dynamic reconfiguration of software
  architectures</title><categories>cs.SE</categories><comments>6\`eme \'edition de la Conf\'erence Francophone sur les Architectures
  Logicielles (CAL 2012), Montpellier : France (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  On the one hand, ACME is a language designed in the late 90s as an
interchange format for software architectures. The need for recon guration at
runtime has led to extend the language with speci c support in Plastik. On the
other hand, PDDL is a predicative language for the description of planning
problems. It has been designed in the AI community for the International
Planning Competition of the ICAPS conferences. Several related works have
already proposed to encode software architectures into PDDL. Existing planning
algorithms can then be used in order to generate automatically a plan that
updates an architecture to another one, i.e., the program of a recon guration.
In this paper, we improve the encoding in PDDL. Noticeably we propose how to
encode ADL types and constraints in the PDDL representation. That way, we can
statically check our design and express PDDL constraints in order to ensure
that the generated plan never goes through any bad or inconsistent
architecture, not even temporarily.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0129</identifier>
 <datestamp>2012-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0129</id><created>2012-06-01</created><authors><author><keyname>Censor</keyname><forenames>Yair</forenames></author><author><keyname>Zaslavski</keyname><forenames>Alexander J.</forenames></author></authors><title>Convergence and Perturbation Resilience of Dynamic String-Averaging
  Projection Methods</title><categories>math.OC cs.NA physics.med-ph</categories><comments>Computational Optimization and Applications, accepted for publication</comments><msc-class>65K10, 90C25</msc-class><doi>10.1007/s10589-012-9491-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the convex feasibility problem (CFP) in Hilbert space and
concentrate on the study of string-averaging projection (SAP) methods for the
CFP, analyzing their convergence and their perturbation resilience. In the
past, SAP methods were formulated with a single predetermined set of strings
and a single predetermined set of weights. Here we extend the scope of the
family of SAP methods to allow iteration-index-dependent variable strings and
weights and term such methods dynamic string-averaging projection (DSAP)
methods. The bounded perturbation resilience of DSAP methods is relevant and
important for their possible use in the framework of the recently developed
superiorization heuristic methodology for constrained minimization problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0130</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0130</id><created>2012-06-01</created><updated>2014-04-28</updated><authors><author><keyname>Apt</keyname><forenames>Krzysztof R.</forenames></author><author><keyname>Simon</keyname><forenames>Sunil</forenames></author></authors><title>A Classification of Weakly Acyclic Games</title><categories>cs.GT</categories><comments>25 pages. To appear in Theory and Decision. A preliminary version of
  this paper appeared in Proc. 5th International Symposium on Algorithmic Game
  Theory, (SAGT 2012). Lecture Notes in Computer Science 7615, Springer, pp.
  1-12</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Weakly acyclic games form a natural generalization of the class of games that
have the finite improvement property (FIP). In such games one stipulates that
from any initial joint strategy some finite improvement path exists. We
classify weakly acyclic games using the concept of a scheduler introduced in
arXiv:1202.2209. We also show that finite games that can be solved by the
iterated elimination of never best response strategies are weakly acyclic.
Finally, we explain how the schedulers allow us to improve the bounds on
finding a Nash equilibrium in a weakly acyclic game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0133</identifier>
 <datestamp>2012-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0133</id><created>2012-06-01</created><authors><author><keyname>Chaoub</keyname><forenames>Abdelaali</forenames></author><author><keyname>Ibn-Elhaj</keyname><forenames>Elhassane</forenames></author></authors><title>Comparison between Poissonian and Markovian Primary Traffics in
  Cognitive Radio Networks</title><categories>cs.NI stat.AP</categories><comments>IJCSI International Journal of Computer Science Issues, Vol. 9, Issue
  2, No 3, March 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cognitive Radio generates a big interest as a key cost-effective solution for
the underutilization of frequency spectrum in legacy communication networks.
The objective of this work lies in conducting a performance evaluation of the
end-to-end message delivery under both Markovian and Poissonian primary
traffics in lossy Cognitive Radio networks. We aim at inferring the most
appropriate conditions for an efficient secondary service provision according
to the Cognitive Radio network characteristics. Meanwhile, we have performed a
general analysis for many still open issues in Cognitive Radio, but at the end
only two critical aspects have been considered, namely, the unforeseen primary
reclaims in addition to the collided cognitive transmissions due to the
Opportunistic Spectrum Sharing. Some graphs, in view of the average Spectral
Efficiency, have been computed and plotted to report some comparative results
for a given video transmission under the Markovian and the Poissonian primary
interruptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0136</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0136</id><created>2012-06-01</created><updated>2012-06-19</updated><authors><author><keyname>Urban</keyname><forenames>Christian</forenames><affiliation>TU Munich</affiliation></author><author><keyname>Kaliszyk</keyname><forenames>Cezary</forenames><affiliation>University of Tsukuba</affiliation></author></authors><title>General Bindings and Alpha-Equivalence in Nominal Isabelle</title><categories>cs.LO</categories><comments>35 pages</comments><proxy>LMCS</proxy><acm-class>F.3.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 2 (June 20,
  2012) lmcs:813</journal-ref><doi>10.2168/LMCS-8(2:14)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nominal Isabelle is a definitional extension of the Isabelle/HOL theorem
prover. It provides a proving infrastructure for reasoning about programming
language calculi involving named bound variables (as opposed to de-Bruijn
indices). In this paper we present an extension of Nominal Isabelle for dealing
with general bindings, that means term constructors where multiple variables
are bound at once. Such general bindings are ubiquitous in programming language
research and only very poorly supported with single binders, such as
lambda-abstractions. Our extension includes new definitions of
alpha-equivalence and establishes automatically the reasoning infrastructure
for alpha-equated terms. We also prove strong induction principles that have
the usual variable convention already built in.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0141</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0141</id><created>2012-06-01</created><updated>2014-05-22</updated><authors><author><keyname>Urban</keyname><forenames>Josef</forenames></author></authors><title>Parallelizing Mizar</title><categories>cs.MS cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper surveys and describes the implementation of parallelization of the
Mizar proof checking and of related Mizar utilities. The implementation makes
use of Mizar's compiler-like division into several relatively independent
passes, with typically quite different processing speeds. The information
produced in earlier (typically much faster) passes can be used to parallelize
the later (typically much slower) passes. The parallelization now works by
splitting the formalization into a suitable number of pieces that are processed
in parallel, assembling from them together the required results. The
implementation is evaluated on examples from the Mizar library, and future
extensions are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0142</identifier>
 <datestamp>2012-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0142</id><created>2012-06-01</created><authors><author><keyname>Elasri</keyname><forenames>Hicham</forenames></author><author><keyname>Mehdi</keyname><forenames>Neknane</forenames></author><author><keyname>Jamila</keyname><forenames>Aatab</forenames></author><author><keyname>Karima</keyname><forenames>Ganoun</forenames></author></authors><title>Open source based cadastral information system : ANCFCC-MOROCCO</title><categories>cs.OH</categories><comments>International conference of GIS users, May 23-24, 2012, Fez (Morocco)</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This present project is developing a geographic information system to support
the cadastral business. This system based on open source solutions which
developed within the National Agency of Land Registry, Cadastre and Cartography
(ANCFCC) enabling monitoring and analysis of cadastral procedures as well as
offering consumable services by other information systems: consultation and
querying spatial data. The project will also assist the various user profiles
in the completion of production tasks and the possibility to eliminate the
deficiencies identified to ensure an optimum level of productivity
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0150</identifier>
 <datestamp>2012-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0150</id><created>2012-06-01</created><authors><author><keyname>Afek</keyname><forenames>Yehuda</forenames></author><author><keyname>Alon</keyname><forenames>Noga</forenames></author><author><keyname>Bar-Joseph</keyname><forenames>Ziv</forenames></author><author><keyname>Cornejo</keyname><forenames>Alejandro</forenames></author><author><keyname>Haeupler</keyname><forenames>Bernhard</forenames></author><author><keyname>Kuhn</keyname><forenames>Fabian</forenames></author></authors><title>Beeping a Maximal Independent Set</title><categories>cs.DS cs.DC</categories><comments>arXiv admin note: substantial text overlap with arXiv:1108.1926</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of computing a maximal independent set (MIS) in an
extremely harsh broadcast model that relies only on carrier sensing. The model
consists of an anonymous broadcast network in which nodes have no knowledge
about the topology of the network or even an upper bound on its size.
Furthermore, it is assumed that an adversary chooses at which time slot each
node wakes up. At each time slot a node can either beep, that is, emit a
signal, or be silent. At a particular time slot, beeping nodes receive no
feedback, while silent nodes can only differentiate between none of its
neighbors beeping, or at least one of its neighbors beeping.
  We start by proving a lower bound that shows that in this model, it is not
possible to locally converge to an MIS in sub-polynomial time. We then study
four different relaxations of the model which allow us to circumvent the lower
bound and find an MIS in polylogarithmic time. First, we show that if a
polynomial upper bound on the network size is known, it is possible to find an
MIS in O(log^3 n) time. Second, if we assume sleeping nodes are awoken by
neighboring beeps, then we can also find an MIS in O(log^3 n) time. Third, if
in addition to this wakeup assumption we allow sender-side collision detection,
that is, beeping nodes can distinguish whether at least one neighboring node is
beeping concurrently or not, we can find an MIS in O(log^2 n) time. Finally, if
instead we endow nodes with synchronous clocks, it is also possible to find an
MIS in O(log^2 n) time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0154</identifier>
 <datestamp>2012-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0154</id><created>2012-06-01</created><updated>2012-10-03</updated><authors><author><keyname>Ghaffari</keyname><forenames>Mohsen</forenames></author><author><keyname>Haeupler</keyname><forenames>Bernhard</forenames></author><author><keyname>Lynch</keyname><forenames>Nancy</forenames></author><author><keyname>Newport</keyname><forenames>Calvin</forenames></author></authors><title>Bounds on Contention Management in Radio Networks</title><categories>cs.DS cs.DC math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The local broadcast problem assumes that processes in a wireless network are
provided messages, one by one, that must be delivered to their neighbors. In
this paper, we prove tight bounds for this problem in two well-studied wireless
network models: the classical model, in which links are reliable and collisions
consistent, and the more recent dual graph model, which introduces unreliable
edges. Our results prove that the Decay strategy, commonly used for local
broadcast in the classical setting, is optimal. They also establish a
separation between the two models, proving that the dual graph setting is
strictly harder than the classical setting, with respect to this primitive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0169</identifier>
 <datestamp>2012-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0169</id><created>2012-06-01</created><authors><author><keyname>Singla</keyname><forenames>Pradeep</forenames></author><author><keyname>Dhingra</keyname><forenames>Kamya</forenames></author><author><keyname>Malik</keyname><forenames>Naveen Kr.</forenames></author></authors><title>DSTN (Distributed Sleep Transistor Network) for Low Power Programmable
  Logic array Design</title><categories>cs.OH</categories><comments>6 pages, 7 Figures</comments><journal-ref>IJCA 45(17):31-36, May 2012. Published by Foundation of Computer
  Science, New York, USA</journal-ref><doi>10.5120/7004-9563</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  With the high demand of the portable electronic products, Low- power design
of VLSI circuits &amp; Power dissipation has been recognized as a challenging
technology in the recent years. PLA (Programming logic array) is one of the
important off shelf part in the industrial application. This paper describes
the new design of PLA using power gating structure sleep transistor at circuit
level implementation for the low power applications. The important part of the
power gating design i.e. header and footer switch selection is also describes
in the paper. The simulating results of the proposed architecture of the new
PLA is shown and compared with the conventional PLA. This paper clearly shows
the optimization in the reduction of power dissipation in the new design
implementation of the PLA. The transient response of the power gates structure
of PLA is also illustrate in the paper by using TINA-PRO software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0181</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0181</id><created>2012-06-01</created><updated>2012-06-15</updated><authors><author><keyname>Gerdt</keyname><forenames>Vladimir</forenames></author><author><keyname>Hashemi</keyname><forenames>Amir</forenames></author></authors><title>Comprehensive Involutive Systems</title><categories>cs.SC math.AC math.RA</categories><comments>14 pages, to appear in Proc. of CASC 2012 (Maribor, Slovenia,
  September 3-5, 2012)</comments><msc-class>13F20</msc-class><acm-class>G.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider parametric ideals and introduce a notion of
comprehensive involutive system. This notion plays the same role in theory of
involutive bases as the notion of comprehensive Groebner system in theory of
Groebner bases. Given a parametric ideal, the space of parameters is decomposed
into a finite set of cells. Each cell yields the corresponding involutive basis
of the ideal for the values of parameters in that cell. Using the Gerdt-Blinkov
algorithm for computing involutive bases and also the Montes algorithm for
computing comprehensive Groebner systems, we present an algorithm for
construction of comprehensive involutive systems. The proposed algorithm has
been implemented in Maple, and we provide an illustrative example showing the
step-by-step construction of comprehensive involutive system by our algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0184</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0184</id><created>2012-06-01</created><updated>2012-06-15</updated><authors><author><keyname>Jarrahi</keyname><forenames>Ali</forenames></author><author><keyname>Kangavari</keyname><forenames>Mohammad Reza</forenames></author></authors><title>An Architecture for Context-Aware Knowledge Flow Management Systems</title><categories>cs.OH</categories><comments>12 pages; IJCSI International Journal of Computer Science Issues,
  Vol. 9, Issue 2, No 3, March 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The organizational knowledge is one of the most important and valuable assets
of organizations. In such environment, organizations with broad, specialized
and up-to-date knowledge, adequately using knowledge resources, will be more
successful than their competitors. For effective use of knowledge, dynamic
knowledge flow from the sources to destinations is essential. In this regard, a
novel complex concept in knowledge management is the analysis, design and
implementation of knowledge flow management systems. One of the major
challenges in such systems is to explore the knowledge flow from the source to
the recipient and control the flow for quality improvements concerning the
users' needs as possible. Therefore, the purpose of this paper is to provide an
architecture in order to solve this challenge. For this purpose, in addition to
the architecture for knowledge flow management systems, a new node selection
strategy is provided with higher success rate compared to previous strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0197</identifier>
 <datestamp>2014-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0197</id><created>2012-06-01</created><updated>2014-03-19</updated><authors><author><keyname>Ordentlich</keyname><forenames>Or</forenames></author><author><keyname>Erez</keyname><forenames>Uri</forenames></author><author><keyname>Nazer</keyname><forenames>Bobak</forenames></author></authors><title>The Approximate Sum Capacity of the Symmetric Gaussian K-User
  Interference Channel</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in the IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interference alignment has emerged as a powerful tool in the analysis of
multi-user networks. Despite considerable recent progress, the capacity region
of the Gaussian K-user interference channel is still unknown in general, in
part due to the challenges associated with alignment on the signal scale using
lattice codes. This paper develops a new framework for lattice interference
alignment, based on the compute-and-forward approach. Within this framework,
each receiver decodes by first recovering two or more linear combinations of
the transmitted codewords with integer-valued coefficients and then solving
these equations for its desired codeword. For the special case of symmetric
channel gains, this framework is used to derive the approximate sum capacity of
the Gaussian interference channel, up to an explicitly defined outage set of
the channel gains. The key contributions are the capacity lower bounds for the
weak through strong interference regimes, where each receiver should jointly
decode its own codeword along with part of the interfering codewords. As part
of the analysis, it is shown that decoding K linear combinations of the
codewords can approach the sum capacity of the K-user Gaussian multiple-access
channel up to a gap of no more than K log(K)/2 bits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0206</identifier>
 <datestamp>2012-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0206</id><created>2012-06-01</created><authors><author><keyname>Krebs</keyname><forenames>Andreas</forenames></author><author><keyname>Limaye</keyname><forenames>Nutan</forenames></author><author><keyname>Srinivasan</keyname><forenames>Srikanth</forenames></author></authors><title>Streaming algorithms for recognizing nearly well-parenthesized
  expressions</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the streaming complexity of the membership problem of 1-turn-Dyck2
and Dyck2 when there are a few errors in the input string.
  1-turn-Dyck2 with errors: We prove that there exists a randomized one-pass
algorithm that given x checks whether there exists a string x' in 1-turn-Dyck2
such that x is obtained by flipping at most $k$ locations of x' using:
  - O(k log n) space, O(k log n) randomness, and poly(k log n) time per item
and with error at most 1/poly(n). - O(k^{1+epsilon} + log n) space for every 0
&lt;= epsilon &lt;= 1, O(log n) randomness, O(polylog(n) + poly(k)) time per item,
with error at most 1/8.
  Here, we also prove that any randomized one-pass algorithm that makes error
at most k/n requires at least Omega(k log(n/k)) space to accept strings which
are exactly k-away from strings in 1-turn-Dyck2 and to reject strings which are
exactly (k+2)-away from strings in 1-turn-Dyck2. Since 1-turn-Dyck2 and the
Hamming Distance problem are closely related we also obtain new upper and lower
bounds for this problem.
  Dyck2 with errors: We prove that there exists a randomized one-pass algorithm
that given x checks whether there exists a string x' in Dyck2 such that x is
obtained from x' by changing (in some restricted manner) at most k positions
using:
  - O(k log n + sqrt(n log n)) space, O(k log n) randomness, poly(k log n) time
per element and with error at most 1/poly(n). - O(k^(1+epsilon)+ sqrt(n log n))
space for every 0 &lt;= epsilon &lt;= 1, O(log n) randomness, O(polylog(n) + poly(k))
time per element, with error at most 1/8.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0217</identifier>
 <datestamp>2012-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0217</id><created>2012-06-01</created><authors><author><keyname>El-Zawawy</keyname><forenames>Mohamed A.</forenames></author></authors><title>Efficient techniques for mining spatial databases</title><categories>cs.DB</categories><comments>112 pages; M.Sc. thesis, Department of Mathematics, Faculty of
  Science, Cairo University, 2002</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clustering is one of the major tasks in data mining. In the last few years,
Clustering of spatial data has received a lot of research attention. Spatial
databases are components of many advanced information systems like geographic
information systems VLSI design systems. In this thesis, we introduce several
efficient algorithms for clustering spatial data. First, we present a
grid-based clustering algorithm that has several advantages and comparable
performance to the well known efficient clustering algorithm. The algorithm has
several advantages. The algorithm does not require many input parameters. It
requires only three parameters, the number of the points in the data space, the
number of the cells in the grid and a percentage. The number of the cells in
the grid reflects the accuracy that should be achieved by the algorithm. The
algorithm is capable of discovering clusters of arbitrary shapes. The
computational complexity of the algorithm is comparable to the complexity of
the most efficient clustering algorithm. The algorithm has been implemented and
tested against different ranges of database sizes. The performance results show
that the running time of the algorithm is superior to the most well known
algorithms (CLARANS [23]). The results show also that the performance of the
algorithm do not degrade as the number of the data points increases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0224</identifier>
 <datestamp>2012-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0224</id><created>2012-06-01</created><authors><author><keyname>Li</keyname><forenames>Wei</forenames></author><author><keyname>Bashan</keyname><forenames>Amir</forenames></author><author><keyname>Buldyrev</keyname><forenames>Sergey V.</forenames></author><author><keyname>Stanley</keyname><forenames>H. Eugene</forenames></author><author><keyname>Havlin</keyname><forenames>Shlomo</forenames></author></authors><title>Cascading Failures in Interdependent Lattice Networks: The Critical Role
  of the Length of Dependency Links</title><categories>physics.data-an cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the cascading failures in a system composed of two interdependent
square lattice networks A and B placed on the same Cartesian plane, where each
node in network A depends on a node in network B randomly chosen within a
certain distance $r$ from the corresponding node in network A and vice versa.
Our results suggest that percolation for small $r$ below $r_{\rm max}\approx 8$
(lattice units) is a second-order transition, and for larger $r$ is a
first-order transition. For $r&lt;r_{\rm max}$, the critical threshold increases
linearly with $r$ from 0.593 at $r=0$ and reaches a maximum, 0.738 for
$r=r_{\rm max}$ and then gradually decreases to 0.683 for $r=\infty$. Our
analytical considerations are in good agreement with simulations. Our study
suggests that interdependent infrastructures embedded in Euclidean space become
most vulnerable when the distance between interdependent nodes is in the
intermediate range, which is much smaller than the size of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0232</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0232</id><created>2012-05-31</created><authors><author><keyname>Dai</keyname><forenames>Liyun</forenames></author><author><keyname>Xia</keyname><forenames>Bican</forenames></author></authors><title>Non-Termination Sets of Simple Linear Loops</title><categories>cs.LO</categories><comments>15 pages, 3 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  A simple linear loop is a simple while loop with linear assignments and
linear loop guards. If a simple linear loop has only two program variables, we
give a complete algorithm for computing the set of all the inputs on which the
loop does not terminate. For the case of more program variables, we show that
the non-termination set cannot be described by Tarski formulae in general
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0233</identifier>
 <datestamp>2012-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0233</id><created>2012-06-01</created><updated>2012-11-13</updated><authors><author><keyname>Leitert</keyname><forenames>Arne</forenames></author></authors><title>3-Colourability of Dually Chordal Graphs in Linear Time</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph G is dually chordal if there is a spanning tree T of G such that any
maximal clique of G induces a subtree in T. This paper investigates the
Colourability problem on dually chordal graphs. It will show that it is
NP-complete in case of four colours and solvable in linear time with a simple
algorithm in case of three colours. In addition, it will be shown that a dually
chordal graph is 3-colourable if and only if it is perfect and has no clique of
size four.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0238</identifier>
 <datestamp>2012-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0238</id><created>2012-06-01</created><authors><author><keyname>Hossain</keyname><forenames>M. Zahid</forenames></author><author><keyname>Amin</keyname><forenames>M. Ashraful</forenames></author><author><keyname>Yan</keyname><forenames>Hong</forenames></author></authors><title>Rapid Feature Extraction for Optical Character Recognition</title><categories>cs.CV</categories><comments>5 pages, 1 figure</comments><acm-class>I.5.2; I.7.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature extraction is one of the fundamental problems of character
recognition. The performance of character recognition system is depends on
proper feature extraction and correct classifier selection. In this article, a
rapid feature extraction method is proposed and named as Celled Projection (CP)
that compute the projection of each section formed through partitioning an
image. The recognition performance of the proposed method is compared with
other widely used feature extraction methods that are intensively studied for
many different scripts in literature. The experiments have been conducted using
Bangla handwritten numerals along with three different well known classifiers
which demonstrate comparable results including 94.12% recognition accuracy
using celled projection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0244</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0244</id><created>2012-06-01</created><updated>2012-11-19</updated><authors><author><keyname>Zhang</keyname><forenames>Zhenliang</forenames></author><author><keyname>Chong</keyname><forenames>Edwin K. P.</forenames></author><author><keyname>Pezeshki</keyname><forenames>Ali</forenames></author><author><keyname>Moran</keyname><forenames>William</forenames></author><author><keyname>Howard</keyname><forenames>Stephen D.</forenames></author></authors><title>Detection Performance in Balanced Binary Relay Trees with Node and Link
  Failures</title><categories>cs.IT math.IT</categories><doi>10.1109/TSP.2013.2246156</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the distributed detection problem in the context of a balanced
binary relay tree, where the leaves of the tree correspond to $N$ identical and
independent sensors generating binary messages. The root of the tree is a
fusion center making an overall decision. Every other node is a relay node that
aggregates the messages received from its child nodes into a new message and
sends it up toward the fusion center. We derive upper and lower bounds for the
total error probability $P_N$ as explicit functions of $N$ in the case where
nodes and links fail with certain probabilities. These characterize the
asymptotic decay rate of the total error probability as $N$ goes to infinity.
Naturally, this decay rate is not larger than that in the non-failure case,
which is $\sqrt N$. However, we derive an explicit necessary and sufficient
condition on the decay rate of the local failure probabilities $p_k$
(combination of node and link failure probabilities at each level) such that
the decay rate of the total error probability in the failure case is the same
as that of the non-failure case. More precisely, we show that $\log
P_N^{-1}=\Theta(\sqrt N)$ if and only if $\log p_k^{-1}=\Omega(2^{k/2})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0259</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0259</id><created>2012-02-25</created><updated>2012-06-03</updated><authors><author><keyname>Harnad</keyname><forenames>Stevan</forenames></author></authors><title>The Causal Topography of Cognition</title><categories>cs.AI</categories><comments>11 pages, 0 figures, 10 references, Journal of Cognitive Science 13
  2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The causal structure of cognition can be simulated but not implemented
computationally, just as the causal structure of a comet can be simulated but
not implemented computationally. The only thing that allows us even to imagine
otherwise is that cognition, unlike a comet, is invisible (to all but the
cognizer).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0260</identifier>
 <datestamp>2013-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0260</id><created>2012-06-01</created><updated>2013-02-12</updated><authors><author><keyname>Fujiwara</keyname><forenames>Yuichiro</forenames></author></authors><title>Block synchronization for quantum information</title><categories>quant-ph cs.IT math.IT</categories><comments>7 pages, no figures, final accepted version for publication in
  Physical Review A</comments><journal-ref>Physical Review A 87, 022344 (2013)</journal-ref><doi>10.1103/PhysRevA.87.022344</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Locating the boundaries of consecutive blocks of quantum information is a
fundamental building block for advanced quantum computation and quantum
communication systems. We develop a coding theoretic method for properly
locating boundaries of quantum information without relying on external
synchronization when block synchronization is lost. The method also protects
qubits from decoherence in a manner similar to conventional quantum
error-correcting codes, seamlessly achieving synchronization recovery and error
correction. A family of quantum codes that are simultaneously synchronizable
and error-correcting is given through this approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0277</identifier>
 <datestamp>2012-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0277</id><created>2012-06-01</created><authors><author><keyname>Achanta</keyname><forenames>Hema Kumari</forenames></author><author><keyname>Dasgupta</keyname><forenames>Soura</forenames></author><author><keyname>Xu</keyname><forenames>Weiyu</forenames></author></authors><title>Sensing with Optimal Matrices</title><categories>cs.IT cs.DM math.IT</categories><comments>Authors temporarily listed in alphabetical order</comments><msc-class>05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of designing optimal $M \times N$ ($M \leq N$)
sensing matrices which minimize the maximum condition number of all the
submatrices of $K$ columns. Such matrices minimize the worst-case estimation
errors when only $K$ sensors out of $N$ sensors are available for sensing at a
given time. For M=2 and matrices with unit-normed columns, this problem is
equivalent to the problem of maximizing the minimum singular value among all
the submatrices of $K$ columns. For M=2, we are able to give a closed form
formula for the condition number of the submatrices. When M=2 and K=3, for an
arbitrary $N\geq3$, we derive the optimal matrices which minimize the maximum
condition number of all the submatrices of $K$ columns. Surprisingly, a
uniformly distributed design is often \emph{not} the optimal design minimizing
the maximum condition number.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0285</identifier>
 <datestamp>2012-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0285</id><created>2012-02-19</created><authors><author><keyname>Mandal</keyname><forenames>J. K.</forenames></author><author><keyname>Mukhopadhyay</keyname><forenames>Somnath</forenames></author></authors><title>Image Filtering using All Neighbor Directional Weighted Pixels:
  Optimization using Particle Swarm Optimization</title><categories>cs.CV cs.NE</categories><comments>14 pages</comments><journal-ref>Signal &amp; Image Processing : An International Journal (SIPIJ)
  Vol.2, No.4 (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a novel approach for de noising images corrupted by random
valued impulses has been proposed. Noise suppression is done in two steps. The
detection of noisy pixels is done using all neighbor directional weighted
pixels (ANDWP) in the 5 x 5 window. The filtering scheme is based on minimum
variance of the four directional pixels. In this approach, relatively recent
category of stochastic global optimization technique i.e., particle swarm
optimization (PSO) has also been used for searching the parameters of detection
and filtering operators required for optimal performance. Results obtained
shows better de noising and preservation of fine details for highly corrupted
images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0302</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0302</id><created>2012-06-01</created><authors><author><keyname>Chakraborty</keyname><forenames>Joey</forenames></author></authors><title>Examining Motivations behind Paper Usage in Academia</title><categories>cs.HC</categories><comments>7 pages, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We carried out a qualitative study to identify the &quot;missing pieces&quot; in
current computing devices and technologies that are preventing people from
eliminating paper from their lives.
  Most of the existing literature has looked into the work practices of
businesses, while a few have researched how high school and college students
and teaching assistants at universities work with paper. We were speci?cally
interested in analysing paper use for people in the research side of academia,
and seeing how our results compare to existing work. We recruited and
interviewed participants from academia to understand what kind of tasks they
use paper for, what kind of tasks they use computing devices for and what
motivates them to use these two media.
  We found that, despite having access to at least one personal computing
device, the participants preferred to work with paper in many situations. This
appears to be attributed to certain intrinsic qualities that paper has, such as
open format, easy navigation, readability, and the aff?ordances these qualities
provide. In order to eventually replace paper with devices, designers of new
technology will have to successfully emulate these qualities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0303</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0303</id><created>2012-06-01</created><updated>2013-07-15</updated><authors><author><keyname>Bose</keyname><forenames>Prosenjit</forenames></author><author><keyname>Verdonschot</keyname><forenames>Sander</forenames></author></authors><title>A History of Flips in Combinatorial Triangulations</title><categories>cs.CG</categories><comments>Added a paragraph referencing earlier work in the vertex-labelled
  setting that has implications for the unlabeled setting</comments><doi>10.1007/978-3-642-34191-5_3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given two combinatorial triangulations, how many edge flips are necessary and
sufficient to convert one into the other? This question has occupied
researchers for over 75 years. We provide a comprehensive survey, including
full proofs, of the various attempts to answer it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0305</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0305</id><created>2012-06-01</created><authors><author><keyname>Samara</keyname><forenames>Ghassan</forenames></author></authors><title>Efficient Certificate Management in VANET</title><categories>cs.NI</categories><comments>5 pages. arXiv admin note: text overlap with arXiv:1006.5113, and
  with arXiv:1112.2257 by other authors</comments><journal-ref>2010 2nd International Conference on Future Computer and
  Communication</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vehicular Ad hoc Networks is one of the most challenging research area in the
field of Mobile Ad Hoc Networks, in this research We propose a flexible,
simple, and scalable design for VANET certificates, and new methods for
efficient certificate management, which will Reduce channel overhead by
eliminating the use of CRL, and make Better certificate Revocation Management.
Also it will increase the security of the network and helps in identifying the
adversary vehicle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0323</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0323</id><created>2012-06-01</created><authors><author><keyname>Nasiriani</keyname><forenames>Neda</forenames></author><author><keyname>Fallah</keyname><forenames>Yaser P.</forenames></author><author><keyname>Krishnan</keyname><forenames>Hariharan</forenames></author></authors><title>Fairness and Stability Analysis of Congestion Control Schemes in
  Vehicular Ad-hoc Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cooperative vehicle safety (CVS) systems operate based on broadcast of
vehicle position and safety information to neighboring cars. The communication
medium of CVS is a vehicular ad-hoc network. One of the main challenges in
large scale deployment of CVS systems is the issue of scalability. To address
the scalability problem, several congestion control methods have been proposed
and are currently under field study. These algorithms adapt transmission rate
and power based on network measures such as channel busy ratio. We examine two
such algorithms and study their dynamic behavior in time and space to evaluate
stability (in time) and fairness (in space) properties of these algorithms. We
present stability conditions and evaluate stability and fairness of the
algorithms through simulation experiments. Results show that there is a
trade-off between fast convergence, temporal stability and spatial fairness.
The proper ranges of parameters for achieving stability are presented for the
discussed algorithms. Stability is verified for all typical road density cases.
Fairness is shown to be naturally achieved for some algorithms, while under the
same conditions other algorithms may suffer from unfairness issues. A method
for resolving unfairness is introduced and evaluated through simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0333</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0333</id><created>2012-06-01</created><authors><author><keyname>Chen</keyname><forenames>Jianhui</forenames></author><author><keyname>Ye</keyname><forenames>Jieping</forenames></author></authors><title>Sparse Trace Norm Regularization</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of estimating multiple predictive functions from a
dictionary of basis functions in the nonparametric regression setting. Our
estimation scheme assumes that each predictive function can be estimated in the
form of a linear combination of the basis functions. By assuming that the
coefficient matrix admits a sparse low-rank structure, we formulate the
function estimation problem as a convex program regularized by the trace norm
and the $\ell_1$-norm simultaneously. We propose to solve the convex program
using the accelerated gradient (AG) method and the alternating direction method
of multipliers (ADMM) respectively; we also develop efficient algorithms to
solve the key components in both AG and ADMM. In addition, we conduct
theoretical analysis on the proposed function estimation scheme: we derive a
key property of the optimal solution to the convex program; based on an
assumption on the basis functions, we establish a performance bound of the
proposed function estimation scheme (via the composite regularization).
Simulation studies demonstrate the effectiveness and efficiency of the proposed
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0335</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0335</id><created>2012-06-01</created><authors><author><keyname>Hatami</keyname><forenames>Nima</forenames></author><author><keyname>Chira</keyname><forenames>Camelia</forenames></author><author><keyname>Armano</keyname><forenames>Giuliano</forenames></author></authors><title>A Route Confidence Evaluation Method for Reliable Hierarchical Text
  Categorization</title><categories>cs.IR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hierarchical Text Categorization (HTC) is becoming increasingly important
with the rapidly growing amount of text data available in the World Wide Web.
Among the different strategies proposed to cope with HTC, the Local Classifier
per Node (LCN) approach attains good performance by mirroring the underlying
class hierarchy while enforcing a top-down strategy in the testing step.
However, the problem of embedding hierarchical information (parent-child
relationship) to improve the performance of HTC systems still remains open. A
confidence evaluation method for a selected route in the hierarchy is proposed
to evaluate the reliability of the final candidate labels in an HTC system. In
order to take into account the information embedded in the hierarchy, weight
factors are used to take into account the importance of each level. An
acceptance/rejection strategy in the top-down decision making process is
proposed, which improves the overall categorization accuracy by rejecting a few
percentage of samples, i.e., those with low reliability score. Experimental
results on the Reuters benchmark dataset (RCV1- v2) confirm the effectiveness
of the proposed method, compared to other state-of-the art HTC methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0338</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0338</id><created>2012-06-01</created><updated>2014-04-28</updated><authors><author><keyname>Salmon</keyname><forenames>Joseph</forenames></author><author><keyname>Harmany</keyname><forenames>Zachary</forenames></author><author><keyname>Deledalle</keyname><forenames>Charles-Alban</forenames></author><author><keyname>Willett</keyname><forenames>Rebecca</forenames></author></authors><title>Poisson noise reduction with non-local PCA</title><categories>cs.CV cs.LG stat.CO</categories><comments>erratum: Image man is wrongly name pepper in the journal version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Photon-limited imaging arises when the number of photons collected by a
sensor array is small relative to the number of detector elements. Photon
limitations are an important concern for many applications such as spectral
imaging, night vision, nuclear medicine, and astronomy. Typically a Poisson
distribution is used to model these observations, and the inherent
heteroscedasticity of the data combined with standard noise removal methods
yields significant artifacts. This paper introduces a novel denoising algorithm
for photon-limited images which combines elements of dictionary learning and
sparse patch-based representations of images. The method employs both an
adaptation of Principal Component Analysis (PCA) for Poisson noise and recently
developed sparsity-regularized convex optimization algorithms for
photon-limited images. A comprehensive empirical evaluation of the proposed
method helps characterize the performance of this approach relative to other
state-of-the-art denoising methods. The results reveal that, despite its
conceptual simplicity, Poisson PCA-based denoising appears to be highly
competitive in very low light regimes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0357</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0357</id><created>2012-06-02</created><updated>2012-06-17</updated><authors><author><keyname>Ghani</keyname><forenames>Neil</forenames><affiliation>University of Strathclyde</affiliation></author><author><keyname>Johann</keyname><forenames>Patricia</forenames><affiliation>University of Strathclyde</affiliation></author><author><keyname>Fumex</keyname><forenames>Clement</forenames><affiliation>University of Strathclyde</affiliation></author></authors><title>Generic Fibrational Induction</title><categories>cs.PL</categories><comments>For Special Issue from CSL 2010</comments><proxy>LMCS</proxy><acm-class>F.3.2, D.3.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 2 (June 19,
  2012) lmcs:717</journal-ref><doi>10.2168/LMCS-8(2:12)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides an induction rule that can be used to prove properties of
data structures whose types are inductive, i.e., are carriers of initial
algebras of functors. Our results are semantic in nature and are inspired by
Hermida and Jacobs' elegant algebraic formulation of induction for polynomial
data types. Our contribution is to derive, under slightly different
assumptions, a sound induction rule that is generic over all inductive types,
polynomial or not. Our induction rule is generic over the kinds of properties
to be proved as well: like Hermida and Jacobs, we work in a general fibrational
setting and so can accommodate very general notions of properties on inductive
types rather than just those of a particular syntactic form. We establish the
soundness of our generic induction rule by reducing induction to iteration. We
then show how our generic induction rule can be instantiated to give induction
rules for the data types of rose trees, finite hereditary sets, and
hyperfunctions. The first of these lies outside the scope of Hermida and
Jacobs' work because it is not polynomial, and as far as we are aware, no
induction rules have been known to exist for the second and third in a general
fibrational framework. Our instantiation for hyperfunctions underscores the
value of working in the general fibrational setting since this data type cannot
be interpreted as a set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0361</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0361</id><created>2012-06-02</created><authors><author><keyname>Nair</keyname><forenames>T. R. Gopalakrishnan</forenames></author><author><keyname>Suma</keyname><forenames>V</forenames></author></authors><title>Defect Management Using Depth of Inspection and the Inspection
  Performance Metric</title><categories>cs.SE</categories><comments>6 pages, CrossTalk, 2011, pp no. 22-27</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advancement in fundamental engineering aspects of software development
enables IT enterprises to develop a more cost effective and better quality
product through aptly organized defect management strategies. Inspection
continues to be the most effective and efficient technique of defect
management. To have an appropriate measurement of the inspection process, the
process metric, Depth of Inspection (DI) and the people metric, Inspection
Performance Metric (IPM) are introduced. The introduction of these pair of
metrics can yield valuable information from a company in relation to the
inspection process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0373</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0373</id><created>2012-06-02</created><authors><author><keyname>Swain</keyname><forenames>Ranjita Kumari</forenames></author><author><keyname>Behera</keyname><forenames>Prafulla Kumar</forenames></author><author><keyname>Mohapatra</keyname><forenames>Durga Prasad</forenames></author></authors><title>Generation and Optimization of Test cases for Object-Oriented Software
  Using State Chart Diagram</title><categories>cs.SE</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The process of testing any software system is an enormous task which is time
consuming and costly. The time and required effort to do sufficient testing
grow, as the size and complexity of the software grows, which may cause overrun
of the project budget, delay in the development of software system or some test
cases may not be covered. During SDLC (software development life cycle),
generally the software testing phase takes around 40-70% of the time and cost.
State-based testing is frequently used in software testing. Test data
generation is one of the key issues in software testing. A properly generated
test suite may not only locate the errors in a software system, but also help
in reducing the high cost associated with software testing. It is often desired
that test data in the form of test sequences within a test suite can be
automatically generated to achieve required test coverage. This paper proposes
an optimization approach to test data generation for the state-based software
testing. In this paper, first state transition graph is derived from state
chart diagram. Then, all the required information are extracted from the state
chart diagram. Then, test cases are generated. Lastly, a set of test cases are
minimized by calculating the node coverage for each test case. It is also
determined that which test cases are covered by other test cases. The advantage
of our test generation technique is that it optimizes test coverage by
minimizing time and cost. The proposed test data generation scheme generates
test cases which satisfy transition path coverage criteria, path coverage
criteria and action coverage criteria. A case study on Automatic Ticket Machine
(ATM) has been presented to illustrate our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0375</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0375</id><created>2012-06-02</created><authors><author><keyname>Zenil</keyname><forenames>Hector</forenames></author><author><keyname>Marshall</keyname><forenames>James A. R.</forenames></author></authors><title>Some Computational Aspects of Essential Properties of Evolution and Life</title><categories>cs.CC cs.IT math.IT nlin.AO nlin.PS</categories><comments>Invited contribution to the ACM Ubiquity Symposium on Evolutionary
  Computation</comments><journal-ref>ACM Ubiquity, Symposium on Evolutionary Computation, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While evolution has inspired algorithmic methods of heuristic optimisation,
little has been done in the way of using concepts of computation to advance our
understanding of salient aspects of biological phenomena. We argue that under
reasonable assumptions, interesting conclusions can be drawn that are of
relevance to behavioural evolution. We will focus on two important features of
life--robustness and fitness--which, we will argue, are related to algorithmic
probability and to the thermodynamics of computation, disciplines that may be
capable of modelling key features of living organisms, and which can be used in
formulating new algorithms of evolutionary computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0376</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0376</id><created>2012-06-02</created><authors><author><keyname>Zenil</keyname><forenames>Hector</forenames></author></authors><title>Introducing the Computable Universe</title><categories>cs.IT cs.CC math.IT nlin.CG physics.hist-ph</categories><comments>Based in the introduction to the book A Computable Universe by World
  Scientific, 2012.
  http://www.mathrix.org/experimentalAIT/ComputationNature.htm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Some contemporary views of the universe assume information and computation to
be key in understanding and explaining the basic structure underpinning
physical reality. We introduce the Computable Universe exploring some of the
basic arguments giving foundation to these visions. We will focus on the
algorithmic and quantum aspects, and how these may fit and support the
computable universe hypothesis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0377</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0377</id><created>2012-06-02</created><authors><author><keyname>Pinter</keyname><forenames>Balazs</forenames></author><author><keyname>Voros</keyname><forenames>Gyula</forenames></author><author><keyname>Szabo</keyname><forenames>Zoltan</forenames></author><author><keyname>Lorincz</keyname><forenames>Andras</forenames></author></authors><title>Automated Word Puzzle Generation via Topic Dictionaries</title><categories>cs.CL math.CO</categories><comments>4 pages</comments><msc-class>68T50, 15A23</msc-class><acm-class>I.2.7; G.2.3; G.1.2</acm-class><journal-ref>International Conference on Machine Learning (ICML-2012) -
  Sparsity, Dictionaries and Projections in Machine Learning and Signal
  Processing Workshop, Edinburgh, Scotland, 30 June 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a general method for automated word puzzle generation. Contrary to
previous approaches in this novel field, the presented method does not rely on
highly structured datasets obtained with serious human annotation effort: it
only needs an unstructured and unannotated corpus (i.e., document collection)
as input. The method builds upon two additional pillars: (i) a topic model,
which induces a topic dictionary from the input corpus (examples include e.g.,
latent semantic analysis, group-structured dictionaries or latent Dirichlet
allocation), and (ii) a semantic similarity measure of word pairs. Our method
can (i) generate automatically a large number of proper word puzzles of
different types, including the odd one out, choose the related word and
separate the topics puzzle. (ii) It can easily create domain-specific puzzles
by replacing the corpus component. (iii) It is also capable of automatically
generating puzzles with parameterizable levels of difficulty suitable for,
e.g., beginners or intermediate learners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0379</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0379</id><created>2012-06-02</created><authors><author><keyname>Montakhab</keyname><forenames>Afshin</forenames></author><author><keyname>Manshour</keyname><forenames>Pouya</forenames></author></authors><title>Low prevalence, quasi-stationarity and power-law distribution in a model
  of spreading</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>7 pages, 8 figures. (Submitted)</comments><journal-ref>EPL 99 (2012) 58002</journal-ref><doi>10.1209/0295-5075/99/58002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding how contagions (information, infections, etc) are spread on
complex networks is important both from practical as well as theoretical point
of view. Considerable work has been done in this regard in the past decade or
so. However, most models are limited in their scope and as a result only
capture general features of spreading phenomena. Here, we propose and study a
model of spreading which takes into account the strength or quality of
contagions as well as the local (probabilistic) dynamics occurring at various
nodes. Transmission occurs only after the quality-based fitness of the
contagion has been evaluated by the local agent. The model exhibits
quality-dependent exponential time scales at early times leading to a slowly
evolving quasi-stationary state. Low prevalence is seen for a wide range of
contagion quality for arbitrary large networks. We also investigate the
activity of nodes and find a power-law distribution with a robust exponent
independent of network topology. Our results are consistent with recent
empirical observations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0381</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0381</id><created>2012-06-02</created><authors><author><keyname>Ali</keyname><forenames>Md. Nawab Yousuf</forenames></author><author><keyname>Ripon</keyname><forenames>Shamim</forenames></author><author><keyname>Allayear</keyname><forenames>Shaikh Muhammad</forenames></author></authors><title>UNL Based Bangla Natural Text Conversion - Predicate Preserving Parser
  Approach</title><categories>cs.CL</categories><comments>7 pages, International Journal of Computer Science Issues (IJCSI),
  Volume 9, Issue 3 May 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Universal Networking Language (UNL) is a declarative formal language that is
used to represent semantic data extracted from natural language texts. This
paper presents a novel approach to converting Bangla natural language text into
UNL using a method known as Predicate Preserving Parser (PPP) technique. PPP
performs morphological, syntactic and semantic, and lexical analysis of text
synchronously. This analysis produces a semantic-net like structure represented
using UNL. We demonstrate how Bangla texts are analyzed following the PPP
technique to produce UNL documents which can then be translated into any other
suitable natural language facilitating the opportunity to develop a universal
language translation method via UNL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0396</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0396</id><created>2012-06-02</created><authors><author><keyname>Saad</keyname><forenames>Elsayed</forenames></author><author><keyname>Awadalla</keyname><forenames>Medhat</forenames></author><author><keyname>Shalan</keyname><forenames>Mohamed</forenames></author><author><keyname>Elewi</keyname><forenames>Abdullah</forenames></author></authors><title>Energy-Aware Task Partitioning on Heterogeneous Multiprocessor Platforms</title><categories>cs.OS</categories><comments>8 pages, 9 figures</comments><journal-ref>the International Journal of Computer Science Issues (IJCSI), Vol.
  9, Issue 2, No. 1, 2012, pp. 176-183</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient task partitioning plays a crucial role in achieving high
performance at multiprocessor plat forms. This paper addresses the problem of
energy-aware static partitioning of periodic real-time tasks on heterogeneous
multiprocessor platforms. A Particle Swarm Optimization variant based on
Min-min technique for task partitioning is proposed. The proposed approach aims
to minimize the overall energy consumption, meanwhile avoid deadline
violations. An energy-aware cost function is proposed to be considered in the
proposed approach. Extensive simulations and comparisons are conducted in order
to validate the effectiveness of the proposed technique. The achieved results
demonstrate that the proposed partitioning scheme significantly surpasses
previous approaches in terms of both number of iterations and energy savings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0399</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0399</id><created>2012-06-02</created><updated>2012-08-20</updated><authors><author><keyname>Yilmaz</keyname><forenames>Ferkan</forenames></author><author><keyname>Tabassum</keyname><forenames>Hina</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>On the Computation of the Higher-Order Statistics of the Channel
  Capacity for Amplify-and-Forward Multihop Transmission</title><categories>cs.IT math.IT math.PR math.ST stat.TH</categories><comments>Two Figures, one table, ad submitted to a possible publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Higher-order statistics (HOS) of the channel capacity provide useful
information regarding the level of reliability of the signal transmission at a
particular rate. We propose in this letter a novel and unified analysis, which
is based on the moment-generating function (MGF) approach, to efficiently and
accurately compute the HOS of the channel capacity for amplify-and-forward
multihop transmission over generalized fading channels. More precisely, our
mathematical formulism is easy-to-use and tractable specifically requiring only
the reciprocal MGFs of the instantaneous signal-to-noise ratio distributions of
the transmission hops. Numerical and simulation results, performed to exemplify
the usefulness of the proposed MGF-based analysis, are shown to be in perfect
agreement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0411</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0411</id><created>2012-06-02</created><updated>2014-08-28</updated><authors><author><keyname>B&#xe4;&#xe4;rnhielm</keyname><forenames>Henrik</forenames></author></authors><title>Recognising the small Ree groups in their natural representations</title><categories>math.GR cs.DS</categories><msc-class>20-04, 20C40, 68W20, 68Q25</msc-class><journal-ref>J. Algebra 416, 139-166, 2014</journal-ref><doi>10.1016/j.jalgebra.2014.06.017</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present Las Vegas algorithms for constructive recognition and constructive
membership testing of the Ree groups 2G_2(q) = Ree(q), where q = 3^{2m + 1} for
some m &gt; 0, in their natural representations of degree 7. The input is a
generating set X.
  The constructive recognition algorithm is polynomial time given a discrete
logarithm oracle. The constructive membership testing consists of a
pre-processing step, that only needs to be executed once for a given X, and a
main step. The latter is polynomial time, and the former is polynomial time
given a discrete logarithm oracle.
  Implementations of the algorithms are available for the computer algebra
system MAGMA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0414</identifier>
 <datestamp>2012-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0414</id><created>2012-06-02</created><updated>2012-06-05</updated><authors><author><keyname>Formato</keyname><forenames>Richard A.</forenames></author></authors><title>Dynamic Threshold Optimization - A New Approach?</title><categories>cs.OH</categories><comments>Rev. 05 June 2012: Typos &amp; reference [1] corrected. Material added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic Threshold Optimization (DTO) adaptively &quot;compresses&quot; the decision
space (DS) in a global search and optimization problem by bounding the
objective function from below. This approach is different from &quot;shrinking&quot; DS
by reducing bounds on the decision variables. DTO is applied to Schwefel's
Problem 2.26 in 2 and 30 dimensions with good results. DTO is universally
applicable, and the author believes it may be a novel approach to global search
and optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0418</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0418</id><created>2012-06-02</created><authors><author><keyname>Balakrishnan</keyname><forenames>Hari</forenames></author><author><keyname>Iannucci</keyname><forenames>Peter</forenames></author><author><keyname>Perry</keyname><forenames>Jonathan</forenames></author><author><keyname>Shah</keyname><forenames>Devavrat</forenames></author></authors><title>De-randomizing Shannon: The Design and Analysis of a Capacity-Achieving
  Rateless Code</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an analysis of spinal codes, a class of rateless codes
proposed recently. We prove that spinal codes achieve Shannon capacity for the
binary symmetric channel (BSC) and the additive white Gaussian noise (AWGN)
channel with an efficient polynomial-time encoder and decoder. They are the
first rateless codes with proofs of these properties for BSC and AWGN. The key
idea in the spinal code is the sequential application of a hash function over
the message bits. The sequential structure of the code turns out to be crucial
for efficient decoding. Moreover, counter to the wisdom of having an expander
structure in good codes, we show that the spinal code, despite its sequential
structure, achieves capacity. The pseudo-randomness provided by a hash function
suffices for this purpose. Our proof introduces a variant of Gallager's result
characterizing the error exponent of random codes for any memoryless channel.
We present a novel application of these error-exponent results within the
framework of an efficient sequential code. The application of a hash function
over the message bits provides a methodical and effective way to de-randomize
Shannon's random codebook construction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0419</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0419</id><created>2012-06-03</created><authors><author><keyname>Nair</keyname><forenames>T. R. Gopalakrishnan</forenames></author><author><keyname>Jayarekha</keyname><forenames>P</forenames></author></authors><title>Pre-allocation Strategies of Computational Resources in Cloud Computing
  using Adaptive Resonance Theory-2</title><categories>cs.DC</categories><comments>pages 11, figures 3, International Journal on Cloud Computing:
  Services and Architecture(IJCCSA),Vol.1, No.2, August 2011</comments><doi>10.5121/ijccsa.2011.1203</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the major challenges of cloud computing is the management of
request-response coupling and optimal allocation strategies of computational
resources for the various types of service requests. In the normal situations
the intelligence required to classify the nature and order of the request using
standard methods is insufficient because the arrival of request is at a random
fashion and it is meant for multiple resources with different priority order
and variety. Hence, it becomes absolutely essential that we identify the trends
of different request streams in every category by auto classifications and
organize preallocation strategies in a predictive way. It calls for designs of
intelligent modes of interaction between the client request and cloud computing
resource manager. This paper discusses about the corresponding scheme using
Adaptive Resonance Theory-2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0420</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0420</id><created>2012-06-03</created><authors><author><keyname>Cherian</keyname><forenames>Mary</forenames></author><author><keyname>Nair</keyname><forenames>T. R. Gopalakrishnan</forenames></author></authors><title>Multipath Routing With Novel Packet Scheduling Approach In Wireless
  Sensor Networks</title><categories>cs.NI</categories><comments>pages 5, figures 5, ISSN: 1793-821X (Online Version); 1793-8201
  (Print Version), International Journal of Computer Theory and Engineering,
  Vol. 3, No. 5, October 2011,pp. 666-670</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless sensor networks sense and monitor real-time events. They supervise a
geographic area where a phenomenon is to be monitored. The data in sensor
networks have different levels of priority and hence their criticality differs.
In order to keep up the real time commitment, the applications need higher
transmission rates and reliability in information delivery. In this work we
propose a multipath routing algorithm which enables the reliable delivery of
data. By controlling the scheduling rate, it is possible to prevent congestion
and packet loss in the network. The algorithm provides an efficient way to
prevent the packet loss at each node. This results in congestion management in
the sensor networks. This protocol prevents packet clustering and provides
smoothness to the traffic. Through monitoring and controlling the scheduling
rate the flow control and congestion control are managed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0425</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0425</id><created>2012-06-03</created><authors><author><keyname>Nair</keyname><forenames>T. R. Gopalakrishnan</forenames></author><author><keyname>Subramaniam</keyname><forenames>Kumarashvari</forenames></author></authors><title>Transformation of Traditional Marketing Communications in to Paradigms
  of Social Media Networking</title><categories>cs.OH</categories><comments>Asia-Pacific Business Research Conference, 13th -14th February 2012,
  Kuala Lumpur, Malaysia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Effective Communication for marketing is a vital field in business
organizations, which is used to convey the details about their products and
services to the market segments and subsequently to build long lasting customer
relationships. This paper focuses on an emerging component of the integrated
marketing communication, ie. social media networking, as it is increasingly
becoming the trend. In 21st century, the marketing communication platforms show
a tendency to shift towards innovative technology bound people networking which
is becoming an acceptable domain of interaction. Though the traditional
channels like TV, print media etc. are still active and prominent in marketing
communication, the presences of the Internet and more specifically the Social
Media Networking, has started influencing the way individuals and business
enterprises communicate. It has become evident that more individuals and
business enterprises are engaging the social media networking sites either to
accelerate the sales of their products and services or to provide post-purchase
feedbacks. This shift in scenario has motivated this research which took six
months (June 2011 - December 2011), using empirical analysis which is carried
out based on several primary and secondary evidences. The research paper also
analyzes the factors that govern the social media networking sites to influence
consumers and subsequently enable their purchase decisions. The secondary data
presented for this research were those pertaining to the period between the
year 2005 and year 2011. The study revealed promising facts like the transition
to marketing through SMN gives visible advantages like bidirectional
communication, interactive product presentation, and a firm influence on
customer who has a rudimentary interest...
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0430</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0430</id><created>2012-06-03</created><authors><author><keyname>Southwell</keyname><forenames>Richard</forenames></author><author><keyname>Huang</keyname><forenames>Jianwei</forenames></author><author><keyname>Shou</keyname><forenames>Biying</forenames></author></authors><title>Congestion Games on Weighted Directed Graphs, with Applications to
  Spectrum Sharing</title><categories>cs.NI cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the advance of complex large-scale networks, it is becoming increasingly
important to understand how selfish and spatially distributed individuals will
share network resources without centralized coordinations. In this paper, we
introduce the graphical congestion game with weighted edges (GCGWE) as a
general theoretical model to study this problem. In GCGWE, we view the players
as vertices in a weighted graph. The amount of negative impact (e.g.
congestion) caused by two close-by players to each other is determined by the
weight of the edge linking them. The GCGWE unifies and significantly
generalizes several simpler models considered in the previous literature, and
is well suited for modeling a wide range of networking scenarios. One good
example is to use the GCGWE to model spectrum sharing in wireless networks,
where we can properly define the edge weights and payoff functions to capture
the rather complicated interference relationship between wireless nodes. By
identifying which GCGWEs possess pure Nash equilibria and the very desirable
finite improvement property, we gain insight into when spatially distributed
wireless nodes will be able to self-organize into a mutually acceptable
resource allocation. We also consider the efficiency of the pure Nash
equilibria, and the computational complexity of finding them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0447</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0447</id><created>2012-06-03</created><authors><author><keyname>Ganesh</keyname><forenames>K.</forenames></author><author><keyname>Thrivikraman</keyname><forenames>M.</forenames></author><author><keyname>Kuri</keyname><forenames>Joy</forenames></author><author><keyname>Dagale</keyname><forenames>Haresh</forenames></author><author><keyname>Sudhakar</keyname><forenames>G.</forenames></author><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author></authors><title>Implementation of a Real Time Passenger Information System</title><categories>cs.OH</categories><comments>14 pages, 11 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intelligent Transportation Systems (ITS) are gaining recognition in
developing countries like India. This paper describes the various components of
our prototype implementation of a Real-time Passenger Information System
(RTPIS) for a public transport system like a fleet of buses. Vehicle-mounted
units, bus station units and a server located at the transport company premises
comprise the system. The vehicle unit reports the current position of the
vehicle to a central server periodically via General Packet Radio Service
(GPRS). An Estimated Time of Arrival (ETA) algorithm running on the server
predicts the arrival times of buses at their stops based on real-time
observations of the buses' current Global Positioning System (GPS) coordinates.
This information is displayed and announced to passengers at stops using
station units, which periodically fetch the required ETA from the server via
GPRS. Novel features of our prototype include: (a) a route creator utility
which automatically creates new routes from scratch when a bus is driven along
the new route, and (b) voice tagging of stops and points of interest along any
route. Besides, the prototype provides: (i) web-based applications for
passengers, providing useful information like a snapshot of present bus
locations on the streets, and (ii) web-based analysis tools for the transport
authority, providing information useful for fleet management, like number of
trips undertaken by a specific bus. The prototype has been demonstrated in a
campus environment, with four-wheelers and two-wheelers emulating buses. The
automatic real-time passenger information system has the potential of making
the public transport system an attractive alternative for city-dwellers,
thereby contributing to fewer private vehicles on the road, leading to lower
congestion levels and less pollution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0448</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0448</id><created>2012-06-03</created><authors><author><keyname>Gaubert</keyname><forenames>St&#xe9;phane</forenames></author><author><keyname>Qu</keyname><forenames>Zheng</forenames></author></authors><title>The contraction rate in Thompson metric of order-preserving flows on a
  cone - application to generalized Riccati equations</title><categories>math.MG cs.SY math.OC</categories><journal-ref>Journal of Differential Equations, Volume 256, Issue 8, 15 April
  2014, Pages 2902-2948</journal-ref><doi>10.1016/j.jde.2014.01.024</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a formula for the Lipschitz constant in Thompson's part metric of any
order-preserving flow on the interior of a (possibly infinite dimensional)
closed convex pointed cone. This provides an explicit form of a
characterization of Nussbaum concerning non order-preserving flows. As an
application of this formula, we show that the flow of the generalized Riccati
equation arising in stochastic linear quadratic control is a local contraction
on the cone of positive definite matrices and characterize its Lipschitz
constant by a matrix inequality. We also show that the same flow is no longer a
contraction in other natural Finsler metrics on this cone, including the
standard invariant Riemannian metric. This is motivated by a series of
contraction properties concerning the standard Riccati equation, established by
Bougerol, Liverani, Wojtowski, Lawson, Lee and Lim: we show that some of these
properties do, and that some other do not, carry over to the generalized
Riccati equation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0469</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0469</id><created>2012-06-03</created><authors><author><keyname>Balakrishnan</keyname><forenames>Raju</forenames></author><author><keyname>Bhatt</keyname><forenames>Rushi P</forenames></author></authors><title>Real-Time Bid Optimization for Group-Buying Ads</title><categories>cs.GT cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Group-buying ads seeking a minimum number of customers before the deal expiry
are increasingly used by the daily-deal providers. Unlike the traditional web
ads, the advertiser's profits for group-buying ads depends on the time to
expiry and additional customers needed to satisfy the minimum group size. Since
both these quantities are time-dependent, optimal bid amounts to maximize
profits change with every impression. Consequently, traditional static bidding
strategies are far from optimal. Instead, bid values need to be optimized in
real-time to maximize expected bidder profits. This online optimization of deal
profits is made possible by the advent of ad exchanges offering real-time
(spot) bidding. To this end, we propose a real-time bidding strategy for
group-buying deals based on the online optimization of bid values. We derive
the expected bidder profit of deals as a function of the bid amounts, and
dynamically vary bids to maximize profits. Further, to satisfy time constraints
of the online bidding, we present methods of minimizing computation timings.
Subsequently, we derive the real time ad selection, admissibility, and real
time bidding of the traditional ads as the special cases of the proposed
method. We evaluate the proposed bidding, selection and admission strategies on
a multi-million click stream of 935 ads. The proposed real-time bidding,
selection and admissibility show significant profit increases over the existing
strategies. Further the experiments illustrate the robustness of the bidding
and acceptable computation timings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0489</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0489</id><created>2012-06-03</created><authors><author><keyname>Kontoyiannis</keyname><forenames>Ioannis</forenames></author><author><keyname>Madiman</keyname><forenames>Mokshay</forenames></author></authors><title>Sumset and Inverse Sumset Inequalities for Differential Entropy and
  Mutual Information</title><categories>cs.IT math.CO math.IT math.PR</categories><comments>23 pages</comments><journal-ref>IEEE Transactions on Information Theory, vol. 60, no. 8, pp.
  4503-4514, August 2014</journal-ref><doi>10.1109/TIT.2014.2322861</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The sumset and inverse sumset theories of Freiman, Pl\&quot;{u}nnecke and Ruzsa,
give bounds connecting the cardinality of the sumset $A+B=\{a+b\;;\;a\in
A,\,b\in B\}$ of two discrete sets $A,B$, to the cardinalities (or the finer
structure) of the original sets $A,B$. For example, the sum-difference bound of
Ruzsa states that, $|A+B|\,|A|\,|B|\leq|A-B|^3$, where the difference set $A-B=
\{a-b\;;\;a\in A,\,b\in B\}$. Interpreting the differential entropy $h(X)$ of a
continuous random variable $X$ as (the logarithm of) the size of the effective
support of $X$, the main contribution of this paper is a series of natural
information-theoretic analogs for these results. For example, the Ruzsa
sum-difference bound becomes the new inequality, $h(X+Y)+h(X)+h(Y)\leq
3h(X-Y)$, for any pair of independent continuous random variables $X$ and $Y$.
Our results include differential-entropy versions of Ruzsa's triangle
inequality, the Pl\&quot;{u}nnecke-Ruzsa inequality, and the
Balog-Szemer\'{e}di-Gowers lemma. Also we give a differential entropy version
of the Freiman-Green-Ruzsa inverse-sumset theorem, which can be seen as a
quantitative converse to the entropy power inequality. Versions of most of
these results for the discrete entropy $H(X)$ were recently proved by Tao,
relying heavily on a strong, functional form of the submodularity property of
$H(X)$. Since differential entropy is {\em not} functionally submodular, in the
continuous case many of the corresponding discrete proofs fail, in many cases
requiring substantially new proof strategies. We find that the basic property
that naturally replaces the discrete functional submodularity, is the data
processing property of mutual information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0514</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0514</id><created>2012-06-04</created><authors><author><keyname>Gordon</keyname><forenames>Taylor</forenames></author></authors><title>Simultaneous Embeddings with Vertices Mapping to Pre-Specified Points</title><categories>cs.CG math.CO</categories><comments>12 pages (plus appendix), 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss the problem of embedding graphs in the plane with restrictions on
the vertex mapping. In particular, we introduce a technique for drawing planar
graphs with a fixed vertex mapping that bounds the number of times edges bend.
An immediate consequence of this technique is that any planar graph can be
drawn with a fixed vertex mapping so that edges map to piecewise linear curves
with at most $3n + O(1)$ bends each. By considering uniformly random planar
graphs, we show that $2n + O(1)$ bends per edge is sufficient on average.
  To further utilize our technique, we consider simultaneous embeddings of $k$
uniformly random planar graphs with vertices mapping to a fixed, common point
set. We explain how to achieve such a drawing so that edges map to piecewise
linear curves with $O(n^{1-1/k})$ bends each, which holds with overwhelming
probability. This result improves upon the previously best known result of O(n)
bends per edge for the case where $k \geq 2$. Moreover, we give a lower bound
on the number of bends that matches our upper bound, proving our results are
optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0531</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0531</id><created>2012-06-04</created><authors><author><keyname>Hall</keyname><forenames>Joanne L.</forenames></author><author><keyname>Stovicek</keyname><forenames>Jan</forenames></author></authors><title>Mutually unbiased bases as submodules and subspaces</title><categories>math.CO cs.IT math.IT quant-ph</categories><comments>5 pages. Accepted to ISIT2012</comments><msc-class>81P45, 51C05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mutually unbiased bases (MUBs) have been used in several cryptographic and
communications applications. There has been much speculation regarding
connections between MUBs and finite geometries. Most of which has focused on a
connection with projective and affine planes. We propose a connection with
higher dimensional projective geometries and projective Hjelmslev geometries.
We show that this proposed geometric structure is present in several
constructions of MUBs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0549</identifier>
 <datestamp>2012-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0549</id><created>2012-06-04</created><updated>2012-06-12</updated><authors><author><keyname>Hekler</keyname><forenames>Achim</forenames></author><author><keyname>Fischer</keyname><forenames>J&#xf6;rg</forenames></author><author><keyname>Hanebeck</keyname><forenames>Uwe D.</forenames></author></authors><title>Sequence-Based Control for Networked Control Systems Based on Virtual
  Control Inputs</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the problem of controlling a system over an
unreliable connection that is affected by time-varying delays and randomly
occurring packet losses. A novel sequence-based approach is proposed that
extends a given controller designed without consideration of the
network-induced disturbances. Its key idea is to model the unknown future
control inputs by random variables, the so-called virtual control inputs, which
are characterized by discrete probability density functions. Subject to this
probabilistic description, the actual sequence of future control inputs is
determined and transmitted to the actuator. The high performance of the
proposed approach is demonstrated by means of Monte Carlo simulation runs with
an inverted pendulum on a cart and by a detailed comparison to standard NCS
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0555</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0555</id><created>2012-06-04</created><authors><author><keyname>Bianchi</keyname><forenames>Matteo</forenames></author><author><keyname>Salaris</keyname><forenames>Paolo</forenames></author><author><keyname>Bicchi</keyname><forenames>Antonio</forenames></author></authors><title>Synergy-based Hand Pose Sensing: Reconstruction Enhancement</title><categories>cs.RO</categories><comments>Submitted to International Journal of Robotics Research (2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low-cost sensing gloves for reconstruction posture provide measurements which
are limited under several regards. They are generated through an imperfectly
known model, are subject to noise, and may be less than the number of Degrees
of Freedom (DoFs) of the hand. Under these conditions, direct reconstruction of
the hand posture is an ill-posed problem, and performance can be very poor.
This paper examines the problem of estimating the posture of a human hand
using(low-cost) sensing gloves, and how to improve their performance by
exploiting the knowledge on how humans most frequently use their hands. To
increase the accuracy of pose reconstruction without modifying the glove
hardware - hence basically at no extra cost - we propose to collect, organize,
and exploit information on the probabilistic distribution of human hand poses
in common tasks. We discuss how a database of such an a priori information can
be built, represented in a hierarchy of correlation patterns or postural
synergies, and fused with glove data in a consistent way, so as to provide a
good hand pose reconstruction in spite of insufficient and inaccurate sensing
data. Simulations and experiments on a low-cost glove are reported which
demonstrate the effectiveness of the proposed techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0556</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0556</id><created>2012-06-04</created><authors><author><keyname>Bianchi</keyname><forenames>Matteo</forenames></author><author><keyname>Salaris</keyname><forenames>Paolo</forenames></author><author><keyname>Bicchi</keyname><forenames>Antonio</forenames></author></authors><title>Synergy-Based Hand Pose Sensing: Optimal Glove Design</title><categories>cs.RO</categories><comments>Submitted to International Journal of Robotics Research 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the problem of improving human hand pose sensing
device performance by exploiting the knowledge on how humans most frequently
use their hands in grasping tasks. In a companion paper we studied the problem
of maximizing the reconstruction accuracy of the hand pose from partial and
noisy data provided by any given pose sensing device (a sensorized &quot;glove&quot;)
taking into account statistical a priori information. In this paper we consider
the dual problem of how to design pose sensing devices, i.e. how and where to
place sensors on a glove, to get maximum information about the actual hand
posture. We study the continuous case, whereas individual sensing elements in
the glove measure a linear combination of joint angles, the discrete case,
whereas each measure corresponds to a single joint angle, and the most general
hybrid case, whereas both continuous and discrete sensing elements are
available. The objective is to provide, for given a priori information and
fixed number of measurements, the optimal design minimizing in average the
reconstruction error. Solutions relying on the geometrical synergy definition
as well as gradient flow-based techniques are provided. Simulations of
reconstruction performance show the effectiveness of the proposed optimal
design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0561</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0561</id><created>2012-06-04</created><authors><author><keyname>Bercher</keyname><forenames>J. -F.</forenames></author></authors><title>A simple probabilistic construction yielding generalized entropies and
  divergences, escort distributions and q-Gaussians</title><categories>cond-mat.stat-mech cs.IT math-ph math.IT math.MP</categories><doi>10.1016/j.physa.2012.04.024</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a simple probabilistic description of a transition between two states
which leads to a generalized escort distribution. When the parameter of the
distribution varies, it defines a parametric curve that we call an escort-path.
The R\'enyi divergence appears as a natural by-product of the setting. We study
the dynamics of the Fisher information on this path, and show in particular
that the thermodynamic divergence is proportional to Jeffreys' divergence.
Next, we consider the problem of inferring a distribution on the escort-path,
subject to generalized moments constraints. We show that our setting naturally
induces a rationale for the minimization of the R\'enyi information divergence.
Then, we derive the optimum distribution as a generalized q-Gaussian
distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0567</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0567</id><created>2012-06-04</created><authors><author><keyname>Bercher</keyname><forenames>J. -F.</forenames></author></authors><title>On generalized Cram\'er-Rao inequalities, generalized Fisher
  informations and characterizations of generalized q-Gaussian distributions</title><categories>math-ph cond-mat.stat-mech cs.IT math.IT math.MP</categories><journal-ref>J. Phys. A: Math. Theor. 45 255303 2012</journal-ref><doi>10.1088/1751-8113/45/25/255303</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with Cram\'er-Rao inequalities in the context of
nonextensive statistics and in estimation theory. It gives characterizations of
generalized q-Gaussian distributions, and introduces generalized versions of
Fisher information. The contributions of this paper are (i) the derivation of
new extended Cram\'er-Rao inequalities for the estimation of a parameter,
involving general q-moments of the estimation error, (ii) the derivation of
Cram\'er-Rao inequalities saturated by generalized q-Gaussian distributions,
(iii) the definition of generalized Fisher informations, (iv) the
identification and interpretation of some prior results, and finally, (v) the
suggestion of new estimation methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0570</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0570</id><created>2012-06-04</created><authors><author><keyname>Yahia</keyname><forenames>Nora</forenames></author><author><keyname>Mokhtar</keyname><forenames>Sahar A.</forenames></author><author><keyname>Ahmed</keyname><forenames>AbdelWahab</forenames></author></authors><title>Automatic Generation of OWL Ontology from XML Data Source</title><categories>cs.DL</categories><comments>International Journal of Computer Science Issues, Volume 9, Issue 2,
  March 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The eXtensible Markup Language (XML) can be used as data exchange format in
different domains. It allows different parties to exchange data by providing
common understanding of the basic concepts in the domain. XML covers the
syntactic level, but lacks support for reasoning. Ontology can provide a
semantic representation of domain knowledge which supports efficient reasoning
and expressive power. One of the most popular ontology languages is the Web
Ontology Language (OWL). It can represent domain knowledge using classes,
properties, axioms and instances for the use in a distributed environment such
as the World Wide Web. This paper presents a new method for automatic
generation of OWL ontology from XML data sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0580</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0580</id><created>2012-06-04</created><authors><author><keyname>Podolsky</keyname><forenames>Sergey</forenames></author><author><keyname>Zorin</keyname><forenames>Yuri</forenames></author></authors><title>O(1) Delta Component Computation Technique for the Quadratic Assignment
  Problem</title><categories>cs.DS</categories><comments>9 pages, 4 figures</comments><acm-class>F.2.2; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper describes a novel technique that allows to reduce by half the
number of delta values that were required to be computed with complexity O(N)
in most of the heuristics for the quadratic assignment problem. Using the
correlation between the old and new delta values, obtained in this work, a new
formula of complexity O(1) is proposed. Found result leads up to 25%
performance increase in such well-known algorithms as Robust Tabu Search and
others based on it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0585</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0585</id><created>2012-06-04</created><authors><author><keyname>Salo</keyname><forenames>Ville</forenames></author></authors><title>A Characterization of Cellular Automata Generated by Idempotents on the
  Full Shift</title><categories>math.DS cs.FL</categories><comments>will be presented in CSR 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we discuss the family of cellular automata generated by
so-called idempotent cellular automata (CA G such that G^2 = G) on the full
shift. We prove a characterization of products of idempotent CA, and show
examples of CA which are not easy to directly decompose into a product of
idempotents, but which are trivially seen to satisfy the conditions of the
characterization. Our proof uses ideas similar to those used in the well-known
Embedding Theorem and Lower Entropy Factor Theorem in symbolic dynamics. We
also consider some natural decidability questions for the class of products of
idempotent CA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0594</identifier>
 <datestamp>2012-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0594</id><created>2012-06-04</created><updated>2012-07-11</updated><authors><author><keyname>Liberty</keyname><forenames>Edo</forenames></author></authors><title>Simple and Deterministic Matrix Sketching</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We adapt a well known streaming algorithm for approximating item frequencies
to the matrix sketching setting. The algorithm receives the rows of a large
matrix $A \in \R^{n \times m}$ one after the other in a streaming fashion. It
maintains a sketch matrix $B \in \R^ {1/\eps \times m}$ such that for any unit
vector $x$ [\|Ax\|^2 \ge \|Bx\|^2 \ge \|Ax\|^2 - \eps \|A\|_{f}^2 \.] Sketch
updates per row in $A$ require $O(m/\eps^2)$ operations in the worst case. A
slight modification of the algorithm allows for an amortized update time of
$O(m/\eps)$ operations per row. The presented algorithm stands out in that it
is: deterministic, simple to implement, and elementary to prove. It also
experimentally produces more accurate sketches than widely used approaches
while still being computationally competitive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0595</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0595</id><created>2012-06-04</created><authors><author><keyname>Heydari-Malayeri</keyname><forenames>M.</forenames><affiliation>LERMA, Paris Observatory</affiliation></author><author><keyname>Moreau</keyname><forenames>N.</forenames><affiliation>LERMA, Paris Observatory</affiliation></author><author><keyname>Petit</keyname><forenames>F. Le</forenames><affiliation>LUTH, Paris Observatory</affiliation></author></authors><title>A Novel Semantic Software for Astronomical Concepts</title><categories>astro-ph.IM cs.DL</categories><comments>4 pages, submitted to the International Astonomical Union Information
  Bulletin</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have created a new semantic tool called AstroConcepts, providing
definitions of astronomical concepts present on Web pages. This tool is a
Google Chrome plug-in that interrogates the Etymological Dictionary of
Astronomy and Astrophysics, developed at Paris Observatory. Thanks to this
tool, if one selects an astronomical concept on a web page, a pop-up window
will display the definition of the available English or French terms. Another
expected use of this facility could be its implementation in Virtual
Observatory services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0603</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0603</id><created>2012-06-04</created><authors><author><keyname>Jansen</keyname><forenames>Nils</forenames></author><author><keyname>&#xc1;brah&#xe1;m</keyname><forenames>Erika</forenames></author><author><keyname>Scheffler</keyname><forenames>Maik</forenames></author><author><keyname>Volk</keyname><forenames>Matthias</forenames></author><author><keyname>Vorpahl</keyname><forenames>Andreas</forenames></author><author><keyname>Wimmer</keyname><forenames>Ralf</forenames></author><author><keyname>Katoen</keyname><forenames>Joost-Pieter</forenames></author><author><keyname>Becker</keyname><forenames>Bernd</forenames></author></authors><title>The COMICS Tool - Computing Minimal Counterexamples for Discrete-time
  Markov Chains</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report presents the tool COMICS, which performs model checking and
generates counterexamples for DTMCs. For an input DTMC, COMICS computes an
abstract system that carries the model checking information and uses this
result to compute a critical subsystem, which induces a counterexample. This
abstract subsystem can be refined and concretized hierarchically. The tool
comes with a command-line version as well as a graphical user interface that
allows the user to interactively influence the refinement process of the
counterexample.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0604</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0604</id><created>2012-06-04</created><authors><author><keyname>Glaubitz</keyname><forenames>John Paul Adrian</forenames></author></authors><title>Modern consumerism and the waste problem</title><categories>cs.CY</categories><comments>8 pages, 4 images</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the advance of industrial mass production, modern micro-electronics and
computers, the intervals between the release of new generations of consumer
products have been dramatically reduced and so have their lifetime cycles.
While it was very natural in the post-war era, that sophisticated consumer
products like television sets and stereo equipment would not be replaced with a
new product until they break, and usually beyond that point since it was very
common to have a broken television set serviced, the habits of consumers have
changed during the last quarter of the 20th century.
  A modern consumer product, like Apple's famous iPhone has a market life of
approximately one year until a successor is announced and subsequently pushed
into the market. Usually these new generations bring a bunch of new features,
have a higher performance while maintaining the price or becoming even cheaper,
thus the consumer greatly benefits from the reduced lifetime cycle of these
products.
  However, electronic devices not only require a lot of of Earth's limited
resources for their production, but their production processes are a major
source for harmful climate gases like carbon dioxide and toxic waste like heavy
metal alloys, acids and alkalis. And last but not least is every obsoleted
iPhone a candidate for waste facilities unless consumers are going to sell them
on the second hand market.
  While we can not expect consumers and manufacturers to go back to the early
days of consumer products where lifetime cycles reached up to 20 years, the
world record being the famous &quot;Centennial Lightbulb&quot; in Livermore, CA in the
US, which has been lit for over 100 years, it is certainly about time to
rethink modern consumerism with regard to responsibility to future generations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0609</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0609</id><created>2012-06-04</created><authors><author><keyname>Ismaeel</keyname><forenames>Ayad Ghany</forenames></author></authors><title>New Technique for Proposing Network's Topology using GPS and GIS</title><categories>cs.DC cs.NI</categories><comments>13 pages, 6 figures, 5 tables</comments><journal-ref>International Journal of Distributed and Parallel Systems (IJDPS)
  Vol.3, No.2, March 2012, Pages 53-65</journal-ref><doi>10.5121/ijdps.2012.3205</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of proposed topology for network comes when using Prim's
algorithm with default distance (unrealistic distances) between network's nodes
and don't care about the lakes, high hills, buildings, etc. This problem will
cause incorrect estimations for cost (budget) of requirements like the media
(optic fibre) and the number or type of Access-points, regenerator, Optic
Amplifier, etc. This paper proposed a new technique of implementing Prim's
algorithm to obtain realistic topology using realistic distances between
network's nodes via Global Positioning System GPS and Geographic Information
Systems GIS packages. Applying the new technique on academic institutes network
of Erbil city from view of media (optic fibre) shows that there is disability
in cost (budget) of the media which is needed (nearly) 4 times if implement
default Prim's algorithm (don't using GPS &amp; GIS) base on unrealistic distances
between the nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0629</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0629</id><created>2012-06-04</created><authors><author><keyname>Coscia</keyname><forenames>Michele</forenames></author><author><keyname>Rossetti</keyname><forenames>Giulio</forenames></author><author><keyname>Giannotti</keyname><forenames>Fosca</forenames></author><author><keyname>Pedreschi</keyname><forenames>Dino</forenames></author></authors><title>DEMON: a Local-First Discovery Method for Overlapping Communities</title><categories>cs.DS cs.SI physics.soc-ph</categories><comments>9 pages; Proceedings of the 18th ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining, Beijing, China, August 12-16, 2012</comments><acm-class>I.5.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community discovery in complex networks is an interesting problem with a
number of applications, especially in the knowledge extraction task in social
and information networks. However, many large networks often lack a particular
community organization at a global level. In these cases, traditional graph
partitioning algorithms fail to let the latent knowledge embedded in modular
structure emerge, because they impose a top-down global view of a network. We
propose here a simple local-first approach to community discovery, able to
unveil the modular organization of real complex networks. This is achieved by
democratically letting each node vote for the communities it sees surrounding
it in its limited view of the global system, i.e. its ego neighborhood, using a
label propagation algorithm; finally, the local communities are merged into a
global collection. We tested this intuition against the state-of-the-art
overlapping and non-overlapping community discovery methods, and found that our
new method clearly outperforms the others in the quality of the obtained
communities, evaluated by using the extracted communities to predict the
metadata about the nodes of several real world networks. We also show how our
method is deterministic, fully incremental, and has a limited time complexity,
so that it can be used on web-scale real networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0638</identifier>
 <datestamp>2012-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0638</id><created>2012-06-04</created><updated>2012-07-09</updated><authors><author><keyname>Tahat</keyname><forenames>Amani</forenames></author><author><keyname>Marti</keyname><forenames>Jordi</forenames></author><author><keyname>Tahat</keyname><forenames>Mohammad</forenames></author></authors><title>WM Program manual</title><categories>cs.CE</categories><comments>63 pages,19 figures,one appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This user manual has been written to describe the open source code WM to be
distributed associated with a research article submitted to the information
technology journal 45001-ITJ-ANSI, entitled: &quot;Maintenance and Reengineering of
software: Creating a Visual C++ Graphical User Interface to Perform Specific
Tasks Related to Soil Structure Interaction in Poroelastic Soil&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0641</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0641</id><created>2012-06-04</created><updated>2013-02-22</updated><authors><author><keyname>Bi</keyname><forenames>Suzhi</forenames></author><author><keyname>Zhang</keyname><forenames>Ying Jun</forenames></author></authors><title>The Cost of Mitigating Power Law Delay in Random Access Networks</title><categories>cs.NI</categories><comments>14 pages, 11 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Exponential backoff (EB) is a widely adopted collision resolution mechanism
in many popular random-access networks including Ethernet and wireless LAN
(WLAN). The prominence of EB is primarily attributed to its asymptotic
throughput stability, which ensures a non-zero throughput even when the number
of users in the network goes to infinity. Recent studies, however, show that EB
is fundamentally unsuitable for applications that are sensitive to large delay
and delay jitters, as it induces divergent second- and higher-order moments of
medium access delay. Essentially, the medium access delay follows a power law
distribution, a subclass of heavy-tailed distribution. To understand and
alleviate the issue, this paper systematically analyzes the tail delay
distribution of general backoff functions, with EB being a special case. In
particular, we establish a tradeoff between the tail decaying rate of medium
access delay distribution and the stability of throughput. To be more specific,
convergent delay moments are attainable only when the backoff functions $g(k)$
grows slower than exponential functions, i.e., when $g(k)\in o(r^k)$ for all
$r&gt;1$. On the other hand, non-zero asymptotic throughput is attainable only
when backoff functions grow at least as fast as an exponential function, i.e.,
$g(k)\in\Omega(r^k)$ for some $r&gt;1$. This implies that bounded delay moments
and stable throughput cannot be achieved at the same time. For practical
implementation, we show that polynomial backoff (PB), where $g(k)$ is a
polynomial that grows slower than exponential functions, obtains finite delay
moments and good throughput performance at the same time within a practical
range of user population. This makes PB a better alternative than EB for
multimedia applications with stringent delay requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0643</identifier>
 <datestamp>2012-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0643</id><created>2012-06-04</created><updated>2012-07-17</updated><authors><author><keyname>Stamatiou</keyname><forenames>Kostas</forenames></author><author><keyname>Chiarotto</keyname><forenames>Davide</forenames></author><author><keyname>Librino</keyname><forenames>Federico</forenames></author><author><keyname>Zorzi</keyname><forenames>Michele</forenames></author></authors><title>Performance analysis of an opportunistic relay selection protocol for
  multi-hop networks (Technical report)</title><categories>cs.NI</categories><comments>4 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this technical report, we analyze the performance of an interference-aware
opportunistic relay selection protocol for multi-hop line networks which is
based on the following simple rule: a node always transmits if it has a packet,
except when its successive node on the line is transmitting. We derive
analytically the saturation throughput and the end-to-end delay for two and
three hop networks, and present simulation results for higher numbers of hops.
In the case of three hops, we determine the throughput-optimal relay positions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0652</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0652</id><created>2012-05-30</created><updated>2012-11-21</updated><authors><author><keyname>Zhang</keyname><forenames>Zhenliang</forenames></author><author><keyname>Chong</keyname><forenames>Edwin K. P.</forenames></author><author><keyname>Pezeshki</keyname><forenames>Ali</forenames></author><author><keyname>Moran</keyname><forenames>William</forenames></author><author><keyname>Howard</keyname><forenames>Stephen D.</forenames></author></authors><title>Learning in Hierarchical Social Networks</title><categories>cs.SI cs.IT cs.LG math.IT</categories><doi>10.1109/JSTSP.2013.2245859</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a social network consisting of agents organized as a hierarchical
M-ary rooted tree, common in enterprise and military organizational structures.
The goal is to aggregate information to solve a binary hypothesis testing
problem. Each agent at a leaf of the tree, and only such an agent, makes a
direct measurement of the underlying true hypothesis. The leaf agent then makes
a decision and sends it to its supervising agent, at the next level of the
tree. Each supervising agent aggregates the decisions from the M members of its
group, produces a summary message, and sends it to its supervisor at the next
level, and so on. Ultimately, the agent at the root of the tree makes an
overall decision. We derive upper and lower bounds for the Type I and II error
probabilities associated with this decision with respect to the number of leaf
agents, which in turn characterize the converge rates of the Type I, Type II,
and total error probabilities. We also provide a message-passing scheme
involving non-binary message alphabets and characterize the exponent of the
error probability with respect to the message alphabet size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0663</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0663</id><created>2012-06-04</created><authors><author><keyname>Liu</keyname><forenames>Yipeng</forenames></author><author><keyname>Gligorijevic</keyname><forenames>Ivan</forenames></author><author><keyname>Matic</keyname><forenames>Vladimir</forenames></author><author><keyname>De Vos</keyname><forenames>Maarten</forenames></author><author><keyname>Van Huffel</keyname><forenames>Sabine</forenames></author></authors><title>Multi-Sparse Signal Recovery for Compressive Sensing</title><categories>cs.IT cs.SY math.IT math.OC stat.ML</categories><comments>4 pages, 7 figures; accepted by The 34th Annual International
  Conference of the Engineering in Medicine and Biology Society (IEEE EMBC
  2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Signal recovery is one of the key techniques of Compressive sensing (CS). It
reconstructs the original signal from the linear sub-Nyquist measurements.
Classical methods exploit the sparsity in one domain to formulate the L0 norm
optimization. Recent investigation shows that some signals are sparse in
multiple domains. To further improve the signal reconstruction performance, we
can exploit this multi-sparsity to generate a new convex programming model. The
latter is formulated with multiple sparsity constraints in multiple domains and
the linear measurement fitting constraint. It improves signal recovery
performance by additional a priori information. Since some EMG signals exhibit
sparsity both in time and frequency domains, we take them as example in
numerical experiments. Results show that the newly proposed method achieves
better performance for multi-sparse signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0681</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0681</id><created>2012-03-29</created><authors><author><keyname>Deka</keyname><forenames>Ganesh Ch</forenames></author><author><keyname>Zain</keyname><forenames>Jasni Mohamad</forenames></author><author><keyname>Mahanti</keyname><forenames>Prabhat</forenames></author></authors><title>ICT's role in e-Governance in India and Malaysia: A Review</title><categories>cs.OH</categories><comments>9 pages</comments><journal-ref>JNIT: Journal of Next Generation Information Technology, Vol. 3,
  No. 1, pp. 7 ~ 16, 2012</journal-ref><doi>10.4156/jnit.vol3.issue1.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information and Communication Technologies (ICTs) play a key role in
Development &amp; Economic growth of the Developing countries of the World.
Political, Cultural, Socio-economic Developmental &amp; Behavioral decisions today
rests on the ability to access, gather, analyze and utilize Information and
Knowledge. Government of India is having an ambitious objective of transforming
the citizen-government interaction at all levels to by the electronic mode by
2020.Similarly according to the Vision 2020-The Way Forward presented by His
Excellency YAB Dato' Seri Dr Mahathir Mohamad at the Malaysian Business Council
&quot;By the year 2020, Malaysia can be a united nation, with a confident Malaysian
society, infused by strong moral and ethical values, living in a society that
is democratic, liberal and tolerant, caring, economically just and equitable,
progressive and prosperous, and in full possession of an economy that is
competitive, dynamic, robust and resilient&quot;. This paper presents a comparative
study and review relating to e-Governance and application of ICT development
between India &amp; Malaysia.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0692</identifier>
 <datestamp>2012-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0692</id><created>2012-06-04</created><updated>2012-09-17</updated><authors><author><keyname>Davydov</keyname><forenames>Alexander Y.</forenames></author></authors><title>Signal and Image Processing with Sinlets</title><categories>cs.MM math-ph math.MP math.NA</categories><comments>26 pages, 21 figures</comments><msc-class>91A28</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new family of localized orthonormal bases - sinlets -
which are well suited for both signal and image processing and analysis.
One-dimensional sinlets are related to specific solutions of the time-dependent
harmonic oscillator equation. By construction, each sinlet is infinitely
differentiable and has a well-defined and smooth instantaneous frequency known
in analytical form. For square-integrable transient signals with infinite
support, one-dimensional sinlet basis provides an advantageous alternative to
the Fourier transform by rendering accurate signal representation via a
countable set of real-valued coefficients. The properties of sinlets make them
suitable for analyzing many real-world signals whose frequency content changes
with time including radar and sonar waveforms, music, speech, biological
echolocation sounds, biomedical signals, seismic acoustic waves, and signals
employed in wireless communication systems. One-dimensional sinlet bases can be
used to construct two- and higher-dimensional bases with variety of potential
applications including image analysis and representation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0701</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0701</id><created>2012-06-04</created><updated>2013-08-02</updated><authors><author><keyname>Nakshatrala</keyname><forenames>K. B.</forenames></author><author><keyname>Nagarajan</keyname><forenames>H.</forenames></author><author><keyname>Shabouei</keyname><forenames>M.</forenames></author></authors><title>A numerical methodology for enforcing maximum principles and the
  non-negative constraint for transient diffusion equations</title><categories>cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transient diffusion equations arise in many branches of engineering and
applied sciences (e.g., heat transfer and mass transfer), and are parabolic
partial differential equations. It is well-known that, under certain
assumptions on the input data, these equations satisfy important mathematical
properties like maximum principles and the non-negative constraint, which have
implications in mathematical modeling. However, existing numerical formulations
for these types of equations do not, in general, satisfy maximum principles and
the non-negative constraint. In this paper, we present a methodology for
enforcing maximum principles and the non-negative constraint for transient
anisotropic diffusion equation. The method of horizontal lines (also known as
the Rothe method) is applied in which the time is discretized first. This
results in solving steady anisotropic diffusion equation with decay equation at
every discrete time level. The proposed methodology for transient anisotropic
diffusion equation will satisfy maximum principles and the non-negative
constraint on general computational grids, and with no additional restrictions
on the time step. We illustrate the performance and accuracy of the proposed
formulation using representative numerical examples. We also perform numerical
convergence of the proposed methodology. For comparison, we also present the
results from the standard single-field semi-discrete formulation and the
results from a popular software package, which all will violate maximum
principles and the non-negative constraint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0720</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0720</id><created>2012-06-04</created><updated>2014-12-07</updated><authors><author><keyname>Honnappa</keyname><forenames>Harsha</forenames></author><author><keyname>Jain</keyname><forenames>Rahul</forenames></author><author><keyname>Ward</keyname><forenames>Amy R.</forenames></author></authors><title>A queueing model with independent arrivals, and its fluid and diffusion
  limits</title><categories>math.PR cs.SY</categories><comments>Accepted, Queueing Systems</comments><msc-class>60K25, 90B15 (Primary) 68M20, 90B22 (Secondary)</msc-class><journal-ref>Queueing Systems, 2014,0257-0130</journal-ref><doi>10.1007/s11134-014-9428-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the {\Delta}(i)/GI/1 queue, a new queueing model. In this model,
customers from a given population independently sample a time to arrive from
some given distribution F. Thus, the arrival times are an ordered statistics,
and the inter-arrival times are differences of consecutive ordered statistics.
They are served by a single server which provides service according to a
general distribution G, with independent service times. The exact model is
analytically intractable. Thus, we develop fluid and diffusion limits for the
various stochastic processes, and performance metrics. The fluid limit of the
queue length is observed to be a reflected process, while the diffusion limit
is observed to be a function of a Brownian motion and a Brownian bridge
process, and is given by a 'netput' process and a directional derivative of the
Skorokhod reflected fluid netput in the direction of a diffusion refinement of
the netput process. We also observe what may be interpreted as a transient
Little's law. Sample path analysis reveals various operating regimes where the
diffusion limit switches between a free diffusion, a reflected diffusion
process and the zero process, with possible discontinuities during regime
switches. The weak convergence is established in the M1 topology, and it is
also shown that this is not possible in the J1 topology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0729</identifier>
 <datestamp>2012-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0729</id><created>2012-06-04</created><authors><author><keyname>Miah</keyname><forenames>K. H.</forenames></author><author><keyname>Herrera</keyname><forenames>R. H.</forenames></author><author><keyname>van der Baan</keyname><forenames>M.</forenames></author><author><keyname>Sacchi</keyname><forenames>M. D.</forenames></author></authors><title>Application of Fractional Fourier Transform in Cepstrum Analysis</title><categories>cs.IT math.IT physics.geo-ph</categories><comments>4 pages, Conference: Recovery - 2011 CSPG CSEG CWLS Convention</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Source wavelet estimation is the key in seismic signal processing for
resolving subsurface structural properties. Homomorphic deconvolution using
cepstrum analysis has been an effective method for wavelet estimation for
decades. In general, the inverse of the Fourier transform of the logarithm of a
signal's Fourier transform is the cepstral domain representation of that
signal. The convolution operation of two signals in the time domain becomes an
addition in the cepstral domain. The fractional Fourier transform (FRFT) is the
generalization of the standard Fourier transform (FT). In an FRFT, the
transformation kernel is a set of linear chirps whereas the kernel is composed
of complex sinusoids for the FT. Depending on the fractional order, signals can
be represented in multiple domains. This gives FRFT an extra degree of freedom
in signal analysis over the standard FT. In this paper, we have taken advantage
of the multidomain nature of the FRFT and applied it to cepstral analysis. We
term this combination the Fractional-Cepstrum (FC). We derive the real FC
formulation, and give an example using wavelets to show the multidomain
representation of the traditional cepstrum with different fractional orders of
the FRFT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0730</identifier>
 <datestamp>2012-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0730</id><created>2012-06-04</created><authors><author><keyname>Akimoto</keyname><forenames>Youhei</forenames></author><author><keyname>Nagata</keyname><forenames>Yuichi</forenames></author><author><keyname>Ono</keyname><forenames>Isao</forenames></author><author><keyname>Kobayashi</keyname><forenames>Shigenobu</forenames></author></authors><title>Theoretical foundation for CMA-ES from information geometric perspective</title><categories>cs.NE</categories><comments>Algorithmica (special issue on evolutionary computation)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores the theoretical basis of the covariance matrix adaptation
evolution strategy (CMA-ES) from the information geometry viewpoint.
  To establish a theoretical foundation for the CMA-ES, we focus on a geometric
structure of a Riemannian manifold of probability distributions equipped with
the Fisher metric. We define a function on the manifold which is the
expectation of fitness over the sampling distribution, and regard the goal of
update of the parameters of sampling distribution in the CMA-ES as maximization
of the expected fitness. We investigate the steepest ascent learning for the
expected fitness maximization, where the steepest ascent direction is given by
the natural gradient, which is the product of the inverse of the Fisher
information matrix and the conventional gradient of the function.
  Our first result is that we can obtain under some types of parameterization
of multivariate normal distribution the natural gradient of the expected
fitness without the need for inversion of the Fisher information matrix. We
find that the update of the distribution parameters in the CMA-ES is the same
as natural gradient learning for expected fitness maximization. Our second
result is that we derive the range of learning rates such that a step in the
direction of the exact natural gradient improves the parameters in the expected
fitness. We see from the close relation between the CMA-ES and natural gradient
learning that the default setting of learning rates in the CMA-ES seems
suitable in terms of monotone improvement in expected fitness. Then, we discuss
the relation to the expectation-maximization framework and provide an
information geometric interpretation of the CMA-ES.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0758</identifier>
 <datestamp>2013-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0758</id><created>2012-06-04</created><updated>2013-01-25</updated><authors><author><keyname>Amy</keyname><forenames>Matthew</forenames></author><author><keyname>Maslov</keyname><forenames>Dmitri</forenames></author><author><keyname>Mosca</keyname><forenames>Michele</forenames></author><author><keyname>Roetteler</keyname><forenames>Martin</forenames></author></authors><title>A meet-in-the-middle algorithm for fast synthesis of depth-optimal
  quantum circuits</title><categories>quant-ph cs.ET</categories><comments>23 pages, 15 figures, 1 table; To appear in IEEE Transactions on
  Computer-Aided Design of Integrated Circuits and Systems</comments><journal-ref>IEEE Transactions on Computer-Aided Design of Integrated Circuits
  and Systems 32(6): 818-830, 2013</journal-ref><doi>10.1109/TCAD.2013.2244643</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm for computing depth-optimal decompositions of logical
operations, leveraging a meet-in-the-middle technique to provide a significant
speed-up over simple brute force algorithms. As an illustration of our method
we implemented this algorithm and found factorizations of the commonly used
quantum logical operations into elementary gates in the Clifford+T set. In
particular, we report a decomposition of the Toffoli gate over the set of
Clifford and T gates. Our decomposition achieves a total T-depth of 3, thereby
providing a 40% reduction over the previously best known decomposition for the
Toffoli gate. Due to the size of the search space the algorithm is only
practical for small parameters, such as the number of qubits, and the number of
gates in an optimal implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0771</identifier>
 <datestamp>2012-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0771</id><created>2012-06-04</created><authors><author><keyname>Johnson</keyname><forenames>Jesse</forenames></author></authors><title>Topological graph clustering with thin position</title><categories>math.GT cs.LG stat.ML</categories><comments>12 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A clustering algorithm partitions a set of data points into smaller sets
(clusters) such that each subset is more tightly packed than the whole. Many
approaches to clustering translate the vector data into a graph with edges
reflecting a distance or similarity metric on the points, then look for highly
connected subgraphs. We introduce such an algorithm based on ideas borrowed
from the topological notion of thin position for knots and 3-dimensional
manifolds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0773</identifier>
 <datestamp>2012-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0773</id><created>2012-06-04</created><authors><author><keyname>Sharpnack</keyname><forenames>James</forenames></author><author><keyname>Rinaldo</keyname><forenames>Alessandro</forenames></author><author><keyname>Singh</keyname><forenames>Aarti</forenames></author></authors><title>Changepoint Detection over Graphs with the Spectral Scan Statistic</title><categories>math.ST cs.IT math.IT stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the change-point detection problem of deciding, based on noisy
measurements, whether an unknown signal over a given graph is constant or is
instead piecewise constant over two connected induced subgraphs of relatively
low cut size. We analyze the corresponding generalized likelihood ratio (GLR)
statistics and relate it to the problem of finding a sparsest cut in a graph.
We develop a tractable relaxation of the GLR statistic based on the
combinatorial Laplacian of the graph, which we call the spectral scan
statistic, and analyze its properties. We show how its performance as a testing
procedure depends directly on the spectrum of the graph, and use this result to
explicitly derive its asymptotic properties on few significant graph
topologies. Finally, we demonstrate both theoretically and by simulations that
the spectral scan statistic can outperform naive testing procedures based on
edge thresholding and $\chi^2$ testing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0782</identifier>
 <datestamp>2012-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0782</id><created>2012-06-04</created><authors><author><keyname>Knill</keyname><forenames>Oliver</forenames></author></authors><title>A Brouwer fixed point theorem for graph endomorphisms</title><categories>math.DS cs.DM math.GN</categories><comments>24 pages, 6 figures</comments><msc-class>58J20, 47H10, 37C25, 05C80, 05C82, 05C10, 90B15, 57M15, 55M20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove a Lefschetz formula for general simple graphs which equates the
Lefschetz number L(T) of an endomorphism T with the sum of the degrees i(x) of
simplices in G which are fixed by T. The degree i(x) of x with respect to T is
defined as a graded sign of the permutation T induces on the simplex x
multiplied by -1 if the dimension of x is odd. The Lefschetz number is defined
as in the continuum as the super trace of T induced on cohomology. In the
special case where T is the identity, the formula becomes the Euler-Poincare
formula equating combinatorial and cohomological Euler characteristic. The
theorem assures in general that if L(T) is nonzero, then T has a fixed clique.
A special case is a discrete Brouwer fixed point theorem for graphs: if T is a
graph endomorphism of a connected graph G, which is star-shaped in the sense
that only the zeroth cohomology group is nontrivial, like for connected trees
or triangularizations of star shaped Euclidean domains, then there is clique x
which is fixed by T. Unlike in the continuum, the fixed point theorem proven
here looks for fixed cliques, complete subgraphs which play now the role of
&quot;points&quot; in the graph. Fixed points can so be vertices, edges, fixed triangles
etc. If A denotes the automorphism group of a graph, we also look at the
average Lefschetz number L(G) which is the average of L(T) over A. We prove
that this is the Euler characteristic of the graph G/A and especially an
integer. We also show that as a consequence of the Lefschetz formula, the zeta
function zeta(T,z) is a product of two dynamical zeta functions and therefore
has an analytic continuation as a rational function which is explicitly given
by a product formula involving only the dimension and the signature of prime
orbits of simplices in G.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0785</identifier>
 <datestamp>2013-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0785</id><created>2012-06-04</created><updated>2013-06-16</updated><authors><author><keyname>Fitzsimons</keyname><forenames>Joseph F.</forenames></author><author><keyname>Rieffel</keyname><forenames>Eleanor G.</forenames></author><author><keyname>Scarani</keyname><forenames>Valerio</forenames></author></authors><title>The Quantum Frontier</title><categories>quant-ph cs.GL</categories><comments>Invited book chapter for Computation for Humanity - Information
  Technology to Advance Society to be published by CRC Press. Concepts
  clarified and style made more uniform in version 2. Many thanks to the
  referees for their suggestions for improvements</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The success of the abstract model of computation, in terms of bits, logical
operations, programming language constructs, and the like, makes it easy to
forget that computation is a physical process. Our cherished notions of
computation and information are grounded in classical mechanics, but the
physics underlying our world is quantum. In the early 80s researchers began to
ask how computation would change if we adopted a quantum mechanical, instead of
a classical mechanical, view of computation. Slowly, a new picture of
computation arose, one that gave rise to a variety of faster algorithms, novel
cryptographic mechanisms, and alternative methods of communication. Small
quantum information processing devices have been built, and efforts are
underway to build larger ones. Even apart from the existence of these devices,
the quantum view on information processing has provided significant insight
into the nature of computation and information, and a deeper understanding of
the physics of our universe and its connections with computation.
  We start by describing aspects of quantum mechanics that are at the heart of
a quantum view of information processing. We give our own idiosyncratic view of
a number of these topics in the hopes of correcting common misconceptions and
highlighting aspects that are often overlooked. A number of the phenomena
described were initially viewed as oddities of quantum mechanics. It was
quantum information processing, first quantum cryptography and then, more
dramatically, quantum computing, that turned the tables and showed that these
oddities could be put to practical effect. It is these application we describe
next. We conclude with a section describing some of the many questions left for
future work, especially the mysteries surrounding where the power of quantum
information ultimately comes from.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0788</identifier>
 <datestamp>2012-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0788</id><created>2012-06-04</created><authors><author><keyname>Adjir</keyname><forenames>Noureddine</forenames></author><author><keyname>Sannes</keyname><forenames>Pierre de Saqui</forenames></author><author><keyname>Rahmouni</keyname><forenames>M. Kamel</forenames></author><author><keyname>Adla</keyname><forenames>Abdelkader</forenames></author></authors><title>Timed Test Case Generation Using Labeled Prioritized Time Petri Nets</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model-based testing of software and hardware systems uses behavioral and
formal models of the systems. The paper presents a technique for model-based
black-box conformance testing of real-time systems using Labeled Prioritized
Time Petri Nets (LPrTPN). The Timed Input/Output Conformance (tioco) relation,
which takes environment assumptions into account, serves as reference to decide
of implementation correctness. Test suites are derived automatically from a
LPrTPN made up of two concurrent sub-nets that respectively specify the system
under test and its environment. The result is optimal in the sense that test
cases have the shortest possible accumulated time to be executed. Test cases
selection combines test purposes and structural coverage criteria associated
with the model. A test purpose or a coverage criterion is specified in a SE-LTL
formula. The TIme Petri Net Analyzer TINA has been extended to support
concurrent composed subnets. Automatic generation of time-optimal test suites
with the Tina toolbox combines the model checker selt and the path analyzer
plan. selt outputs a sequence that satisfies the logic formula. plan computes
the fastest execution of this sequence which will be transformed in a test
cases suite.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0805</identifier>
 <datestamp>2012-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0805</id><created>2012-06-04</created><authors><author><keyname>Balogh</keyname><forenames>J&#xf3;zsef</forenames></author><author><keyname>Gonz&#xe1;lez-Aguilar</keyname><forenames>Hern&#xe1;n</forenames></author><author><keyname>Salazar</keyname><forenames>Gelasio</forenames></author></authors><title>Large convex holes in random point sets</title><categories>cs.CG math.PR</categories><msc-class>52C10, 60D05, 52A22, 52C05, 52A10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A {\em convex hole} (or {\em empty convex polygon)} of a point set $P$ in the
plane is a convex polygon with vertices in $P$, containing no points of $P$ in
its interior. Let $R$ be a bounded convex region in the plane. We show that the
expected number of vertices of the largest convex hole of a set of $n$ random
points chosen independently and uniformly over $R$ is
$\Theta(\log{n}/(\log{\log{n}}))$, regardless of the shape of $R$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0823</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0823</id><created>2012-06-05</created><updated>2015-03-30</updated><authors><author><keyname>Chen</keyname><forenames>Yudong</forenames></author><author><keyname>Caramanis</keyname><forenames>Constantine</forenames></author></authors><title>Orthogonal Matching Pursuit with Noisy and Missing Data: Low and High
  Dimensional Results</title><categories>math.ST cs.IT math.IT stat.ML stat.TH</categories><comments>Minor revision. Appeared at ICML 2013 under the title &quot;Noisy and
  Missing Data Regression: Distribution-Oblivious Support Recovery&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many models for sparse regression typically assume that the covariates are
known completely, and without noise. Particularly in high-dimensional
applications, this is often not the case. This paper develops efficient
OMP-like algorithms to deal with precisely this setting. Our algorithms are as
efficient as OMP, and improve on the best-known results for missing and noisy
data in regression, both in the high-dimensional setting where we seek to
recover a sparse vector from only a few measurements, and in the classical
low-dimensional setting where we recover an unstructured regressor. In the
high-dimensional setting, our support-recovery algorithm requires no knowledge
of even the statistics of the noise. Along the way, we also obtain improved
performance guarantees for OMP for the standard sparse regression problem with
Gaussian noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0834</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0834</id><created>2012-06-05</created><updated>2013-04-22</updated><authors><author><keyname>Skraba</keyname><forenames>Primoz</forenames></author><author><keyname>Wang</keyname><forenames>Bei</forenames></author></authors><title>Approximating Local Homology from Samples</title><categories>cs.CG math.AT</categories><comments>23 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, multi-scale notions of local homology (a variant of persistent
homology) have been used to study the local structure of spaces around a given
point from a point cloud sample. Current reconstruction guarantees rely on
constructing embedded complexes which become difficult in high dimensions. We
show that the persistence diagrams used for estimating local homology, can be
approximated using families of Vietoris-Rips complexes, whose simple
constructions are robust in any dimension. To the best of our knowledge, our
results, for the first time, make applications based on local homology, such as
stratification learning, feasible in high dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0848</identifier>
 <datestamp>2012-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0848</id><created>2012-06-05</created><authors><author><keyname>Ziyabar</keyname><forenames>Hashem Moradmand</forenames></author><author><keyname>Sinaie</keyname><forenames>Mahnaz</forenames></author><author><keyname>Payandeh</keyname><forenames>Ali</forenames></author><author><keyname>Vakili</keyname><forenames>Vahid Tabataba</forenames></author></authors><title>Secure FSM- based arithmetic codes</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, arithmetic coding has attracted the attention of many scholars
because of its high compression capability. Accordingly, in this paper a method
which adds secrecy to this well-known source code is proposed. Finite state
arithmetic code (FSAC) is used as source code to add security. Its finite state
machine (FSM) characteristic is exploited to insert some random jumps during
source coding process. In addition, a Huffman code is designed for each state
to make decoding possible even in jumps. Being Prefix free, Huffman codes are
useful in tracking correct states for an authorized user when s/he decodes with
correct symmetric pseudo random key. The robustness of our proposed scheme is
further reinforced by adding another extra uncertainty by swapping outputs of
Huffman codes in each state. Several test images are used for inspecting the
validity of the proposed Huffman Finite State Arithmetic Coding (HFSAC). The
results of several experimental, key space analyses, statistical analysis, key
sensitivity and plaintext sensitivity tests show that HFSAC with a little
effect on compression efficiency for image cryptosystem provides an efficient
and secure way for real-time image encryption and transmission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0855</identifier>
 <datestamp>2012-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0855</id><created>2012-06-05</created><authors><author><keyname>Fard</keyname><forenames>Pouyan Rafiei</forenames></author><author><keyname>Yahya</keyname><forenames>Keyvan</forenames></author></authors><title>A Mixed Observability Markov Decision Process Model for Musical Pitch</title><categories>cs.AI cs.LG</categories><comments>In 5th International Workshop on Machine Learning and Music,
  Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Partially observable Markov decision processes have been widely used to
provide models for real-world decision making problems. In this paper, we will
provide a method in which a slightly different version of them called Mixed
observability Markov decision process, MOMDP, is going to join with our
problem. Basically, we aim at offering a behavioural model for interaction of
intelligent agents with musical pitch environment and we will show that how
MOMDP can shed some light on building up a decision making model for musical
pitch conveniently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0883</identifier>
 <datestamp>2013-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0883</id><created>2012-06-05</created><updated>2013-07-19</updated><authors><author><keyname>Kikas</keyname><forenames>Riivo</forenames></author><author><keyname>Dumas</keyname><forenames>Marlon</forenames></author><author><keyname>Karsai</keyname><forenames>M&#xe1;rton</forenames></author></authors><title>Bursty egocentric network evolution in Skype</title><categories>physics.soc-ph cs.SI</categories><comments>7 pages, 6 figures. Social Network Analysis and Mining (2013)</comments><doi>10.1007/s13278-013-0123-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study we analyze the dynamics of the contact list evolution of
millions of users of the Skype communication network. We find that egocentric
networks evolve heterogeneously in time as events of edge additions and
deletions of individuals are grouped in long bursty clusters, which are
separated by long inactive periods. We classify users by their link creation
dynamics and show that bursty peaks of contact additions are likely to appear
shortly after user account creation. We also study possible relations between
bursty contact addition activity and other user-initiated actions like free and
paid service adoption events. We show that bursts of contact additions are
associated with increases in activity and adoption - an observation that can
inform the design of targeted marketing tactics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0886</identifier>
 <datestamp>2012-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0886</id><created>2012-06-05</created><authors><author><keyname>Hussein</keyname><forenames>Sari Haj</forenames></author></authors><title>Refining a Quantitative Information Flow Metric</title><categories>cs.CR</categories><comments>7 pages. 3 figures. Proceedings of the 5th IFIP International
  Conference on New Technologies, Mobility and Security (NTMS 2012), Istanbul,
  Turkey, Proceedings of the 5th IFIP International Conference on New
  Technologies, Mobility and Security (NTMS 2012), Istanbul, Turkey</comments><doi>10.1109/NTMS.2012.6208689</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new perspective into the field of quantitative information
flow (QIF) analysis that invites the community to bound the leakage, reported
by QIF quantifiers, by a range consistent with the size of a program's secret
input instead of by a mathematically sound (but counter-intuitive) upper bound
of that leakage. To substantiate our position, we present a refinement of a
recent QIF metric that appears in the literature. Our refinement is based on
slight changes we bring into the design of that metric. These changes do not
affect the theoretical premises onto which the original metric is laid.
However, they enable the natural association between flow results and the
exhaustive search effort needed to uncover a program's secret information (or
the residual secret part of that information) to be clearly established. The
refinement we discuss in this paper validates our perspective and demonstrates
its importance in the future design of QIF quantifiers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0893</identifier>
 <datestamp>2012-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0893</id><created>2012-06-05</created><authors><author><keyname>Ismaeel</keyname><forenames>Ayad Ghany</forenames></author><author><keyname>Abdul-Rahman</keyname><forenames>Suha Adham</forenames></author></authors><title>New Method of Measuring TCP Performance of IP Network using
  Bio-computing</title><categories>cs.NI</categories><comments>17 Pages,10 Figures,5 Tables</comments><journal-ref>International Journal of Distributed and Parallel Systems (IJDPS)
  Vol.3, No.3, May 2012, pages 167-183</journal-ref><doi>10.5121/ijdps.2012.3316</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The measurement of performance of Internet Protocol IP network can be done by
Transmission Control Protocol TCP because it guarantees send data from one end
of the connection actually gets to the other end and in the same order it was
send, otherwise an error is reported. There are several methods to measure the
performance of TCP among these methods genetic algorithms, neural network, data
mining etc, all these methods have weakness and can't reach to correct measure
of TCP performance. This paper proposed a new method of measuring TCP
performance for real time IP network using Biocomputing, especially molecular
calculation because it provides wisdom results and it can exploit all
facilities of phylogentic analysis. Applying the new method at real time on
Biological Kurdish Messenger BIOKM model designed to measure the TCP
performance in two types of protocols File Transfer Protocol FTP and Internet
Relay Chat Daemon IRCD. This application gives very close result of TCP
performance comparing with TCP performance which obtains from Little's law
using same model (BIOKM), i.e. the different percentage of utilization (Busy or
traffic industry) and the idle time which are obtained from a new method base
on Bio-computing comparing with Little's law was (nearly) 0.13%.
  KEYWORDS Bio-computing, TCP performance, Phylogenetic tree, Hybridized Model
(Normalized), FTP, IRCD
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0905</identifier>
 <datestamp>2012-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0905</id><created>2012-06-05</created><authors><author><keyname>Boughamoura</keyname><forenames>Radhouane</forenames></author><author><keyname>Omri</keyname><forenames>Mohamed Nazih</forenames></author><author><keyname>Youssef</keyname><forenames>Habib</forenames></author></authors><title>A Fuzzy Approach for Pertinent Information Extraction from Web Resources</title><categories>cs.IR</categories><comments>International Journal of Computational Science - 2008</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work in machine learning for information extraction has focused on two
distinct sub-problems: the conventional problem of filling template slots from
natural language text, and the problem of wrapper induction, learning simple
extraction procedures (&quot;wrappers&quot;) for highly structured text such as Web
pages. For suitable regular domains, existing wrapper induction algorithms can
efficiently learn wrappers that are simple and highly accurate, but the
regularity bias of these algorithms makes them unsuitable for most conventional
information extraction tasks. This paper describes a new approach for wrapping
semistructured Web pages. The wrapper is capable of learning how to extract
relevant information from Web resources on the basis of user supplied examples.
It is based on inductive learning techniques as well as fuzzy logic rules.
Experimental results show that our approach achieves noticeably better
precision and recall coefficient performance measures than SoftMealy, which is
one of the most recently reported wrappers capable of wrapping semi-structured
Web pages with missing attributes, multiple attributes, variant attribute
permutations, exceptions, and typos.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0908</identifier>
 <datestamp>2012-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0908</id><created>2012-06-05</created><authors><author><keyname>Xing</keyname><forenames>Bo</forenames></author><author><keyname>Gao</keyname><forenames>Wen-Jing</forenames></author><author><keyname>Nelwamondo</keyname><forenames>Fulufhelo V.</forenames></author><author><keyname>Battle</keyname><forenames>Kimberly</forenames></author><author><keyname>Marwala</keyname><forenames>Tshilidzi</forenames></author></authors><title>Soft Computing in Product Recovery: A Survey Focusing on Remanufacturing
  System</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on the application of soft computing in remanufacturing
system, in which end-of-life products are disassembled into basic components
and then remanufactured for both economic and environmental reasons. The
disassembly activities include disassembly sequencing and planning, while the
remanufacturing process is composed of product design, production planning &amp;
scheduling, and inventory management. This paper presents a review of the
related articles and suggests the corresponding further research directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0911</identifier>
 <datestamp>2012-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0911</id><created>2012-06-05</created><authors><author><keyname>Ferrucci</keyname><forenames>Luca</forenames></author><author><keyname>Mandrioli</keyname><forenames>Dino</forenames></author><author><keyname>Morzenti</keyname><forenames>Angelo</forenames></author><author><keyname>Rossi</keyname><forenames>Matteo</forenames></author></authors><title>Non-null Infinitesimal Micro-steps: a Metric Temporal Logic Approach</title><categories>cs.LO</categories><comments>20 pages, 2 figures, submitted to the conference &quot;FORMATS: Formal
  Modelling and Analysis of Timed Systems&quot; 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many systems include components interacting with each other that evolve with
possibly very different speeds. To deal with this situation many formal models
adopt the abstraction of &quot;zero-time transitions&quot;, which do not consume time.
These however have several drawbacks in terms of naturalness and logic
consistency, as a system is modeled to be in different states at the same time.
We propose a novel approach that exploits concepts from non-standard analysis
to introduce a notion of micro- and macro-steps in an extension of the TRIO
metric temporal logic, called X-TRIO. We use X-TRIO to provide a formal
semantics and an automated verification technique to Stateflow-like notations
used in the design of flexible manufacturing systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0918</identifier>
 <datestamp>2012-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0918</id><created>2012-06-05</created><authors><author><keyname>Heni</keyname><forenames>Abdelkader</forenames></author><author><keyname>Omri</keyname><forenames>Mohamed Nazih</forenames></author><author><keyname>Alimi</keyname><forenames>Adel</forenames></author></authors><title>Fuzzy Knowledge Representation Based on Possibilistic and Necessary
  Bayesian Networks</title><categories>cs.AI</categories><comments>ISSN: 1790-0832</comments><journal-ref>WSEAS Transactions on Information Science &amp; Applications Issue 2,
  Volume 3, February 2006, 224-231</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Within the framework proposed in this paper, we address the issue of
extending the certain networks to a fuzzy certain networks in order to cope
with a vagueness and limitations of existing models for decision under
imprecise and uncertain knowledge. This paper proposes a framework that
combines two disciplines to exploit their own advantages in uncertain and
imprecise knowledge representation problems. The framework proposed is a
possibilistic logic based one in which Bayesian nodes and their properties are
represented by local necessity-valued knowledge base. Data in properties are
interpreted as set of valuated formulas. In our contribution possibilistic
Bayesian networks have a qualitative part and a quantitative part, represented
by local knowledge bases. The general idea is to study how a fusion of these
two formalisms would permit representing compact way to solve efficiently
problems for knowledge representation. We show how to apply possibility and
necessity measures to the problem of knowledge representation with large scale
data. On the other hand fuzzification of crisp certainty degrees to fuzzy
variables improves the quality of the network and tends to bring smoothness and
robustness in the network performance. The general aim is to provide a new
approach for decision under uncertainty that combines three methodologies:
Bayesian networks certainty distribution and fuzzy logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0919</identifier>
 <datestamp>2012-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0919</id><created>2012-06-05</created><updated>2012-09-09</updated><authors><author><keyname>Gerhard</keyname><forenames>Jochen</forenames></author><author><keyname>Lindenstruth</keyname><forenames>Volker</forenames></author><author><keyname>Bleicher</keyname><forenames>Marcus</forenames></author></authors><title>Relativistic Hydrodynamics on Graphic Cards</title><categories>hep-ph cs.DC cs.PF hep-th</categories><comments>Details and discussions added, replaced with accepted version. (15
  pages, 8 figures, 2 listings)</comments><doi>10.1016/j.cpc.2012.09.013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how to accelerate relativistic hydrodynamics simulations using
graphic cards (graphic processing units, GPUs). These improvements are of
highest relevance e.g. to the field of high-energetic nucleus-nucleus
collisions at RHIC and LHC where (ideal and dissipative) relativistic
hydrodynamics is used to calculate the evolution of hot and dense QCD matter.
The results reported here are based on the Sharp And Smooth Transport Algorithm
(SHASTA), which is employed in many hydrodynamical models and hybrid simulation
packages, e.g. the Ultrarelativistic Quantum Molecular Dynamics model (UrQMD).
We have redesigned the SHASTA using the OpenCL computing framework to work on
accelerators like graphic processing units (GPUs) as well as on multi-core
processors. With the redesign of the algorithm the hydrodynamic calculations
have been accelerated by a factor 160 allowing for event-by-event calculations
and better statistics in hybrid calculations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0925</identifier>
 <datestamp>2012-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0925</id><created>2012-06-05</created><authors><author><keyname>Omri</keyname><forenames>Mohamed Nazih</forenames></author></authors><title>Possibilistic Pertinence Feedback and Semantic Networks for Goal's
  Extraction</title><categories>cs.AI cs.IR</categories><journal-ref>Asian Journal of Information Technology (4):258-265 - 2004</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pertinence Feedback is a technique that enables a user to interactively
express his information requirement by modifying his original query formulation
with further information. This information is provided by explicitly confirming
the pertinent of some indicating objects and/or goals extracted by the system.
Obviously the user cannot mark objects and/or goals as pertinent until some are
extracted, so the first search has to be initiated by a query and the initial
query specification has to be good enough to pick out some pertinent objects
and/or goals from the Semantic Network. In this paper we present a short survey
of fuzzy and Semantic approaches to Knowledge Extraction. The goal of such
approaches is to define flexible Knowledge Extraction Systems able to deal with
the inherent vagueness and uncertainty of the Extraction process. It has long
been recognised that interactivity improves the effectiveness of Knowledge
Extraction systems. Novice user's queries are the most natural and interactive
medium of communication and recent progress in recognition is making it
possible to build systems that interact with the user. However, given the
typical novice user's queries submitted to Knowledge Extraction Systems, it is
easy to imagine that the effects of goal recognition errors in novice user's
queries must be severely destructive on the system's effectiveness. The
experimental work reported in this paper shows that the use of possibility
theory in classical Knowledge Extraction techniques for novice user's query
processing is more robust than the use of the probability theory. Moreover,
both possibilistic and probabilistic pertinence feedback can be effectively
employed to improve the effectiveness of novice user's query processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0937</identifier>
 <datestamp>2012-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0937</id><created>2012-06-05</created><updated>2012-07-12</updated><authors><author><keyname>Sharpnack</keyname><forenames>James</forenames></author><author><keyname>Krishnamurthy</keyname><forenames>Akshay</forenames></author><author><keyname>Singh</keyname><forenames>Aarti</forenames></author></authors><title>Detecting Activations over Graphs using Spanning Tree Wavelet Bases</title><categories>stat.ML cs.IT math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the detection of activations over graphs under Gaussian noise,
where signals are piece-wise constant over the graph. Despite the wide
applicability of such a detection algorithm, there has been little success in
the development of computationally feasible methods with proveable theoretical
guarantees for general graph topologies. We cast this as a hypothesis testing
problem, and first provide a universal necessary condition for asymptotic
distinguishability of the null and alternative hypotheses. We then introduce
the spanning tree wavelet basis over graphs, a localized basis that reflects
the topology of the graph, and prove that for any spanning tree, this approach
can distinguish null from alternative in a low signal-to-noise regime. Lastly,
we improve on this result and show that using the uniform spanning tree in the
basis construction yields a randomized test with stronger theoretical
guarantees that in many cases matches our necessary conditions. Specifically,
we obtain near-optimal performance in edge transitive graphs, $k$-nearest
neighbor graphs, and $\epsilon$-graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0951</identifier>
 <datestamp>2012-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0951</id><created>2012-06-05</created><authors><author><keyname>Aguilar</keyname><forenames>Teck</forenames></author><author><keyname>Ghedira</keyname><forenames>Mohamed Chedly</forenames></author><author><keyname>Syue</keyname><forenames>Syue-Ju</forenames></author><author><keyname>Gauthier</keyname><forenames>Vincent</forenames></author><author><keyname>Afifi</keyname><forenames>Hossam</forenames></author><author><keyname>Wang</keyname><forenames>Chin-Liang</forenames></author></authors><title>A Cross-Layer Design Based on Geographic Information for Cooperative
  Wireless Networks</title><categories>cs.NI cs.PF</categories><comments>in 2010 IEEE 71st Vehicular Technology Conference, 2010</comments><doi>10.1109/VETECS.2010.5494169</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most of geographic routing approaches in wireless ad hoc and sensor networks
do not take into consideration the medium access control (MAC) and physical
layers when designing a routing protocol. In this paper, we focus on a
cross-layer framework design that exploits the synergies between network, MAC,
and physical layers. In the proposed CoopGeo, we use a beaconless forwarding
scheme where the next hop is selected through a contention process based on the
geographic position of nodes. We optimize this Network-MAC layer interaction
using a cooperative relaying technique with a relay selection scheme also based
on geographic information in order to improve the system performance in terms
of reliability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0956</identifier>
 <datestamp>2014-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0956</id><created>2012-06-05</created><updated>2014-06-03</updated><authors><author><keyname>Bitouz&#xe9;</keyname><forenames>Nicolas</forenames></author><author><keyname>Amat</keyname><forenames>Alexandre Graell i</forenames></author><author><keyname>Rosnes</keyname><forenames>Eirik</forenames></author></authors><title>Using Short Synchronous WOM Codes to Make WOM Codes Decodable</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Transactions on Communications. The material in
  this paper was presented in part at the 2012 IEEE International Symposium on
  Information Theory, Cambridge, MA, July 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the framework of write-once memory (WOM) codes, it is important to
distinguish between codes that can be decoded directly and those that require
that the decoder knows the current generation to successfully decode the state
of the memory. A widely used approach to construct WOM codes is to design first
nondecodable codes that approach the boundaries of the capacity region, and
then make them decodable by appending additional cells that store the current
generation, at an expense of a rate loss. In this paper, we propose an
alternative method to make nondecodable WOM codes decodable by appending cells
that also store some additional data. The key idea is to append to the original
(nondecodable) code a short synchronous WOM code and write generations of the
original code and of the synchronous code simultaneously. We consider both the
binary and the nonbinary case. Furthermore, we propose a construction of
synchronous WOM codes, which are then used to make nondecodable codes
decodable. For short-to-moderate block lengths, the proposed method
significantly reduces the rate loss as compared to the standard method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0968</identifier>
 <datestamp>2012-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0968</id><created>2012-06-05</created><authors><author><keyname>Garrouch</keyname><forenames>Kamel</forenames></author><author><keyname>Omri</keyname><forenames>Mohamed Nazih</forenames></author><author><keyname>Elayeb</keyname><forenames>Bachir</forenames></author></authors><title>Pertinent Information retrieval based on Possibilistic Bayesian network
  : origin and possibilistic perspective</title><categories>cs.IR</categories><comments>The International Conference on Computing &amp; e-Systems - 2008</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a synthesis of work performed on tow information
retrieval models: Bayesian network information retrieval model witch encode
(in) dependence relation between terms and possibilistic network information
retrieval model witch make use of necessity and possibility measures to
represent the fuzziness of pertinence measure. It is known that the use of a
general Bayesian network methodology as the basis for an IR system is difficult
to tackle. The problem mainly appears because of the large number of variables
involved and the computational efforts needed to both determine the
relationships between variables and perform the inference processes. To resolve
these problems, many models have been proposed such as BNR model. Generally,
Bayesian network models doesn't consider the fuzziness of natural language in
the relevance measure of a document to a given query and possibilistic models
doesn't undertake the dependence relations between terms used to index
documents. As a first solution we propose a hybridization of these two models
in one that will undertake both the relationship between terms and the
intrinsic fuzziness of natural language. We believe that the translation of
Bayesian network model from the probabilistic framework to possibilistic one
will allow a performance improvement of BNRM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0974</identifier>
 <datestamp>2012-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0974</id><created>2012-04-24</created><authors><author><keyname>Loshchilov</keyname><forenames>Ilya</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author><author><keyname>Schoenauer</keyname><forenames>Marc</forenames><affiliation>INRIA Saclay - Ile de France, MSR - INRIA</affiliation></author><author><keyname>Sebag</keyname><forenames>Mich&#xe8;le</forenames><affiliation>INRIA Saclay - Ile de France, LRI</affiliation></author></authors><title>Black-box optimization benchmarking of IPOP-saACM-ES on the BBOB-2012
  noisy testbed</title><categories>cs.NE</categories><comments>Genetic and Evolutionary Computation Conference (GECCO 2012) (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the performance of IPOP-saACM-ES, recently proposed
self-adaptive surrogate-assisted Covariance Matrix Adaptation Evolution
Strategy. The algorithm was tested using restarts till a total number of
function evaluations of $10^6D$ was reached, where $D$ is the dimension of the
function search space. The experiments show that the surrogate model control
allows IPOP-saACM-ES to be as robust as the original IPOP-aCMA-ES and
outperforms the latter by a factor from 2 to 3 on 6 benchmark problems with
moderate noise. On 15 out of 30 benchmark problems in dimension 20,
IPOP-saACM-ES exceeds the records observed during BBOB-2009 and BBOB-2010.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0976</identifier>
 <datestamp>2012-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0976</id><created>2012-06-05</created><authors><author><keyname>Ajroud</keyname><forenames>Amen</forenames></author><author><keyname>Omri</keyname><forenames>Mohamed Nazih</forenames></author><author><keyname>Youssef</keyname><forenames>Habib</forenames></author><author><keyname>Benferhat</keyname><forenames>Salem</forenames></author></authors><title>Loopy Belief Propagation in Bayesian Networks : origin and possibilistic
  perspectives</title><categories>cs.AI cs.IR</categories><comments>The International Conference on Computing &amp; e-Systems - 2007</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a synthesis of the work performed on two inference
algorithms: the Pearl's belief propagation (BP) algorithm applied to Bayesian
networks without loops (i.e. polytree) and the Loopy belief propagation (LBP)
algorithm (inspired from the BP) which is applied to networks containing
undirected cycles. It is known that the BP algorithm, applied to Bayesian
networks with loops, gives incorrect numerical results i.e. incorrect posterior
probabilities. Murphy and al. [7] find that the LBP algorithm converges on
several networks and when this occurs, LBP gives a good approximation of the
exact posterior probabilities. However this algorithm presents an oscillatory
behaviour when it is applied to QMR (Quick Medical Reference) network [15].
This phenomenon prevents the LBP algorithm from converging towards a good
approximation of posterior probabilities. We believe that the translation of
the inference computation problem from the probabilistic framework to the
possibilistic framework will allow performance improvement of LBP algorithm. We
hope that an adaptation of this algorithm to a possibilistic causal network
will show an improvement of the convergence of LBP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0978</identifier>
 <datestamp>2012-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0978</id><created>2012-06-05</created><authors><author><keyname>H.</keyname><forenames>Pradeep B.</forenames></author><author><keyname>Singh</keyname><forenames>Sanjay</forenames></author></authors><title>Privacy Preserving and Ownership Authentication in Ubiquitous Computing
  Devices using Secure Three Way Authentication</title><categories>cs.CR</categories><comments>13 pages, 4 figures, presented at IIT'12, Al Ain, UAE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In todays world of technology and gadgets almost every person is having a
portable device, be it a laptop or the smart phones. The user would like to
have all the services at his fingertips and access them through the portable
device he owns. Maybe he wants some data from the fellow user or from the
service provider or maybe he wants to control his smart devices at home from
wherever he is. In the present era of mobile environments, interactions between
the user device and the service provider must be secure enough regardless of
the type of device used to access or utilize the services. In this paper we
propose a &quot;Secure Three Way Authentication (STWA)&quot; technique intended to
preserve the user privacy and to accomplish ownership authentication in order
to securely deliver the services to the user devices. This technique will also
help the users or the service providers to check if the device is compromised
or not with the help of the encrypted pass-phrases that are being exchanged.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0981</identifier>
 <datestamp>2012-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0981</id><created>2012-06-05</created><authors><author><keyname>Squicciarini</keyname><forenames>Anna</forenames></author><author><keyname>Griffin</keyname><forenames>Christopher</forenames></author></authors><title>An Informed Model of Personal Information Release in Social Networking
  Sites</title><categories>cs.SI cs.GT physics.soc-ph</categories><comments>Submitted to 2012 ASE/IEEE International Conference on Privacy,
  Security, Risk and Trust</comments><msc-class>91D30, 91A22, 91A80</msc-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The emergence of online social networks and the growing popularity of digital
communication has resulted in an increasingly amount of information about
individuals available on the Internet. Social network users are given the
freedom to create complex digital identities, and enrich them with truthful or
even fake personal information. However, this freedom has led to serious
security and privacy incidents, due to the role users' identities play in
establishing social and privacy settings.
  In this paper, we take a step toward a better understanding of online
information exposure. Based on the detailed analysis of a sample of real-world
data, we develop a deception model for online users. The model uses a game
theoretic approach to characterizing a user's willingness to release, withhold
or lie about information depending on the behavior of individuals within the
user's circle of friends. In the model, we take into account both the
heterogeneous nature of users and their different attitudes, as well as the
different types of information they may expose online.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0983</identifier>
 <datestamp>2013-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0983</id><created>2012-06-05</created><updated>2013-01-22</updated><authors><author><keyname>Vitanyi</keyname><forenames>Paul M. B.</forenames><affiliation>CWI and University of Amsterdam</affiliation></author></authors><title>Conditional Kolmogorov Complexity and Universal Probability</title><categories>cs.IT math.IT</categories><comments>17 pages (LaTeX); Corrected previous version. arXiv admin note: text
  overlap with arXiv:cs/0204037</comments><msc-class>68Q30, 03D32</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Coding Theorem of L.A. Levin connects unconditional prefix Kolmogorov
complexity with the discrete universal distribution. There are conditional
versions referred to in several publications but as yet there exist no written
proofs in English. Here we provide those proofs. They use a different
definition than the standard one for the conditional version of the discrete
universal distribution. Under the classic definition of conditional
probability, there is no conditional version of the Coding Theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0985</identifier>
 <datestamp>2012-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0985</id><created>2012-06-05</created><authors><author><keyname>De</keyname><forenames>Anindya</forenames></author><author><keyname>Diakonikolas</keyname><forenames>Ilias</forenames></author><author><keyname>Feldman</keyname><forenames>Vitaly</forenames></author><author><keyname>Servedio</keyname><forenames>Rocco A.</forenames></author></authors><title>Nearly optimal solutions for the Chow Parameters Problem and low-weight
  approximation of halfspaces</title><categories>cs.CC cs.DS cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The \emph{Chow parameters} of a Boolean function $f: \{-1,1\}^n \to \{-1,1\}$
are its $n+1$ degree-0 and degree-1 Fourier coefficients. It has been known
since 1961 (Chow, Tannenbaum) that the (exact values of the) Chow parameters of
any linear threshold function $f$ uniquely specify $f$ within the space of all
Boolean functions, but until recently (O'Donnell and Servedio) nothing was
known about efficient algorithms for \emph{reconstructing} $f$ (exactly or
approximately) from exact or approximate values of its Chow parameters. We
refer to this reconstruction problem as the \emph{Chow Parameters Problem.}
  Our main result is a new algorithm for the Chow Parameters Problem which,
given (sufficiently accurate approximations to) the Chow parameters of any
linear threshold function $f$, runs in time $\tilde{O}(n^2)\cdot
(1/\eps)^{O(\log^2(1/\eps))}$ and with high probability outputs a
representation of an LTF $f'$ that is $\eps$-close to $f$. The only previous
algorithm (O'Donnell and Servedio) had running time $\poly(n) \cdot
2^{2^{\tilde{O}(1/\eps^2)}}.$
  As a byproduct of our approach, we show that for any linear threshold
function $f$ over $\{-1,1\}^n$, there is a linear threshold function $f'$ which
is $\eps$-close to $f$ and has all weights that are integers at most $\sqrt{n}
\cdot (1/\eps)^{O(\log^2(1/\eps))}$. This significantly improves the best
previous result of Diakonikolas and Servedio which gave a $\poly(n) \cdot
2^{\tilde{O}(1/\eps^{2/3})}$ weight bound, and is close to the known lower
bound of $\max\{\sqrt{n},$ $(1/\eps)^{\Omega(\log \log (1/\eps))}\}$ (Goldberg,
Servedio). Our techniques also yield improved algorithms for related problems
in learning theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0988</identifier>
 <datestamp>2012-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0988</id><created>2012-04-07</created><authors><author><keyname>Uddin</keyname><forenames>Mueen</forenames></author><author><keyname>Rahman</keyname><forenames>Azizah Abdul</forenames></author></authors><title>Virtualization Implementation Model for Cost Effective &amp; Efficient Data
  Centers</title><categories>cs.DC</categories><comments>6 pages, 1 Figure, Journal Paper. arXiv admin note: substantial text
  overlap with arXiv:1010.5037</comments><journal-ref>International Journal of Advanced Computer Science and
  Applications, (IJACSA) Vol. 2, No.1, January 2011 pg. 69-74</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data centers form a key part of the infrastructure upon which a variety of
information technology services are built. They provide the capabilities of
centralized repository for storage, management, networking and dissemination of
data. With the rapid increase in the capacity and size of data centers, there
is a continuous increase in the demand for energy consumption. These data
centers not only consume a tremendous amount of energy but are riddled with IT
inefficiencies. Data center are plagued with thousands of servers as major
components. These servers consume huge energy without performing useful work.
In an average server environment, 30% of the servers are &quot;dead&quot; only consuming
energy, without being properly utilized. This paper proposes a five step model
using an emerging technology called virtualization to achieve energy efficient
data centers. The proposed model helps Data Center managers to properly
implement virtualization technology in their data centers to make them green
and energy efficient so as to ensure that IT infrastructure contributes as
little as possible to the emission of greenhouse gases, and helps to regain
power and cooling capacity, recapture resilience and dramatically reducing
energy costs and total cost of ownership.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0992</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0992</id><created>2012-06-05</created><updated>2015-09-28</updated><authors><author><keyname>Shi</keyname><forenames>Guodong</forenames></author><author><keyname>Li</keyname><forenames>Bo</forenames></author><author><keyname>Johansson</keyname><forenames>Mikael</forenames></author><author><keyname>Johansson</keyname><forenames>Karl Henrik</forenames></author></authors><title>Finite-time Convergent Gossiping</title><categories>cs.DC</categories><comments>IEEE/ACM Transactions on Networking, In Press</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gossip algorithms are widely used in modern distributed systems, with
applications ranging from sensor networks and peer-to-peer networks to mobile
vehicle networks and social networks. A tremendous research effort has been
devoted to analyzing and improving the asymptotic rate of convergence for
gossip algorithms. In this work we study finite-time convergence of
deterministic gossiping. We show that there exists a symmetric gossip algorithm
that converges in finite time if and only if the number of network nodes is a
power of two, while there always exists an asymmetric gossip algorithm with
finite-time convergence, independent of the number of nodes. For $n=2^m$ nodes,
we prove that a fastest convergence can be reached in $nm=n\log_2 n$ node
updates via symmetric gossiping. On the other hand, under asymmetric gossip
among $n=2^m+r$ nodes with $0\leq r&lt;2^m$, it takes at least $mn+2r$ node
updates for achieving finite-time convergence. It is also shown that the
existence of finite-time convergent gossiping often imposes strong structural
requirements on the underlying interaction graph. Finally, we apply our results
to gossip algorithms in quantum networks, where the goal is to control the
state of a quantum system via pairwise interactions. We show that finite-time
convergence is never possible for such systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0994</identifier>
 <datestamp>2012-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0994</id><created>2012-04-19</created><authors><author><keyname>Acharya</keyname><forenames>Ayan</forenames></author><author><keyname>Hruschka</keyname><forenames>Eduardo R.</forenames></author><author><keyname>Ghosh</keyname><forenames>Joydeep</forenames></author><author><keyname>Acharyya</keyname><forenames>Sreangsu</forenames></author></authors><title>An Optimization Framework for Semi-Supervised and Transfer Learning
  using Multiple Classifiers and Clusterers</title><categories>cs.LG</categories><acm-class>I.5.2; I.5.3; I.5.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unsupervised models can provide supplementary soft constraints to help
classify new, &quot;target&quot; data since similar instances in the target set are more
likely to share the same class label. Such models can also help detect possible
differences between training and target distributions, which is useful in
applications where concept drift may take place, as in transfer learning
settings. This paper describes a general optimization framework that takes as
input class membership estimates from existing classifiers learnt on previously
encountered &quot;source&quot; data, as well as a similarity matrix from a cluster
ensemble operating solely on the target data to be classified, and yields a
consensus labeling of the target data. This framework admits a wide range of
loss functions and classification/clustering methods. It exploits properties of
Bregman divergences in conjunction with Legendre duality to yield a principled
and scalable approach. A variety of experiments show that the proposed
framework can yield results substantially superior to those provided by popular
transductive learning techniques or by naively applying classifiers learnt on
the original task to the target data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.0995</identifier>
 <datestamp>2012-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.0995</id><created>2012-03-19</created><authors><author><keyname>Doyen</keyname><forenames>Laurent</forenames></author><author><keyname>Massart</keyname><forenames>Thierry</forenames></author><author><keyname>Shirmohammadi</keyname><forenames>Mahsa</forenames></author></authors><title>Infinite Synchronizing Words for Probabilistic Automata (Erratum)</title><categories>cs.FL</categories><comments>5 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In [1], we introduced the weakly synchronizing languages for probabilistic
automata. In this report, we show that the emptiness problem of weakly
synchronizing languages for probabilistic automata is undecidable. This implies
that the decidability result of [1-3] for the emptiness problem of weakly
synchronizing language is incorrect.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1004</identifier>
 <datestamp>2012-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1004</id><created>2012-03-14</created><authors><author><keyname>Fu</keyname><forenames>Zhanghua</forenames></author><author><keyname>Huang</keyname><forenames>Wenqi</forenames></author><author><keyname>Lv</keyname><forenames>Zhipeng</forenames></author></authors><title>Iterated tabu search for the circular open dimension problem</title><categories>cs.OH</categories><comments>29 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper mainly investigates the circular open dimension problem (CODP),
which consists of packing a set of circles of known radii into a strip of fixed
width and unlimited length without overlapping. The objective is to minimize
the length of the strip. An iterated tabu search approach, named ITS, is
proposed. ITS starts from a randomly generated solution and attempts to gain
improvements by a tabu search procedure. After that, if the obtained solution
is not feasible, a perturbation operator is subsequently employed to
reconstruct the incumbent solution and an acceptance criterion is implemented
to determine whether or not accept the perturbed solution. This process is
repeated until a feasible solution has been found or the allowed computation
time has been elapsed. Computational experiments based on well-known benchmark
instances show that ITS produces quite competitive results with respect to the
best known results. For 18 representative CODP instances taken from the
literature, ITS succeeds in improving 13 best known results within reasonable
time. In addition, for another challenging related variant: the problem of
packing arbitrary sized circles into a circular container, ITS also succeeds in
improving many best known results. Supplementary experiments are also provided
to analyze the influence of the perturbation operator, as well as the
acceptance criterion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1009</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1009</id><created>2012-06-05</created><updated>2012-06-07</updated><authors><author><keyname>Grauwin</keyname><forenames>S&#xe9;bastian</forenames><affiliation>ENS / LIP Laboratoire de l'Informatique du Parall&#xe9;lisme / INRIA Grenoble Rh&#xf4;ne-Alpes, IXXI, Phys-ENS</affiliation></author><author><keyname>Jensen</keyname><forenames>Pablo</forenames><affiliation>IXXI, Phys-ENS</affiliation></author></authors><title>Opinion groups formation and dynamics : structures that last from non
  lasting entities</title><categories>physics.soc-ph cs.SI</categories><proxy>ccsd</proxy><doi>10.1103/PhysRevE.85.066113</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend simple opinion models to obtain stable but continuously evolving
communities. Our scope is to meet a challenge raised by sociologists of
generating &quot;structures that last from non lasting entities&quot;. We achieve this by
introducing two kinds of noise on a standard opinion model. First, agents may
interact with other agents even if their opinion difference is large. Second,
agents randomly change their opinion at a constant rate. We show that for a
large range of control parameters, our model yields stable and fluctuating
polarized states, where the composition and mean opinion of the emerging groups
is fluctuating over time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1011</identifier>
 <datestamp>2012-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1011</id><created>2012-04-06</created><authors><author><keyname>Elarnaoty</keyname><forenames>Mohamed</forenames></author><author><keyname>AbdelRahman</keyname><forenames>Samir</forenames></author><author><keyname>Fahmy</keyname><forenames>Aly</forenames></author></authors><title>A Machine Learning Approach For Opinion Holder Extraction In Arabic
  Language</title><categories>cs.IR cs.LG</categories><journal-ref>Mohamed Elarnaoty, Samir AbdelRahman and Aly Fahmy. &quot;A Machine
  Learning Approach for Opinion Holder Extraction in Arabic Language&quot;,
  ISSN:0976-2191, vol 3, March 2012</journal-ref><doi>10.5121/ijaia.2012.3205</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Opinion mining aims at extracting useful subjective information from reliable
amounts of text. Opinion mining holder recognition is a task that has not been
considered yet in Arabic Language. This task essentially requires deep
understanding of clauses structures. Unfortunately, the lack of a robust,
publicly available, Arabic parser further complicates the research. This paper
presents a leading research for the opinion holder extraction in Arabic news
independent from any lexical parsers. We investigate constructing a
comprehensive feature set to compensate the lack of parsing structural
outcomes. The proposed feature set is tuned from English previous works coupled
with our proposed semantic field and named entities features. Our feature
analysis is based on Conditional Random Fields (CRF) and semi-supervised
pattern recognition techniques. Different research models are evaluated via
cross-validation experiments achieving 54.03 F-measure. We publicly release our
own research outcome corpus and lexicon for opinion mining community to
encourage further research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1012</identifier>
 <datestamp>2012-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1012</id><created>2012-05-06</created><authors><author><keyname>Fister</keyname><forenames>Iztok</forenames><suffix>Jr.</suffix></author><author><keyname>Fister</keyname><forenames>Iztok</forenames></author><author><keyname>Brest</keyname><forenames>Janez</forenames></author></authors><title>A Hybrid Artificial Bee Colony Algorithm for Graph 3-Coloring</title><categories>cs.NE</categories><journal-ref>I. JR.,Fister, I., Fister and J., Brest. A Hybrid Artificial Bee
  Colony Algorithm for Graph 3-Coloring. In Swarm and Evolutionary Computation,
  Lecture Notes in Computer Science, 7269, Springer Berlin / Heidelberg, 66-74
  (2012)</journal-ref><doi>10.1007/978-3-642-29353-5_8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Artificial Bee Colony (ABC) is the name of an optimization algorithm that
was inspired by the intelligent behavior of a honey bee swarm. It is widely
recognized as a quick, reliable, and efficient methods for solving optimization
problems. This paper proposes a hybrid ABC (HABC) algorithm for graph
3-coloring, which is a well-known discrete optimization problem. The results of
HABC are compared with results of the well-known graph coloring algorithms of
today, i.e. the Tabucol and Hybrid Evolutionary algorithm (HEA) and results of
the traditional evolutionary algorithm with SAW method (EA-SAW). Extensive
experimentations has shown that the HABC matched the competitive results of the
best graph coloring algorithms, and did better than the traditional heuristics
EA-SAW when solving equi-partite, flat, and random generated medium-sized
graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1016</identifier>
 <datestamp>2012-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1016</id><created>2012-06-05</created><authors><author><keyname>DeMarco</keyname><forenames>Bobby</forenames></author><author><keyname>Kahn</keyname><forenames>Jeff</forenames></author></authors><title>Mantel's Theorem for random graphs</title><categories>math.PR cs.DM math.CO</categories><comments>15 pages</comments><msc-class>05D40, 05C35, 05C80</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a graph $G$, denote by $t(G)$ (resp. $b(G)$) the maximum size of a
triangle-free (resp. bipartite) subgraph of $G$. Of course $t(G) \geq b(G)$ for
any $G$, and a classic result of Mantel from 1907 (the first case of Tur\'an's
Theorem) says that equality holds for complete graphs. A natural question,
first considered by Babai, Simonovits and Spencer about 20 years ago is, when
(i.e. for what $p=p(n)$) is the &quot;Erd\H{o}s-R\'enyi&quot; random graph $G=G(n,p)$
likely to satisfy $t(G) = b(G)$? We show that this is true if $p&gt;C n^{-1/2}
\log^{1/2}n $ for a suitable constant $C$, which is best possible up to the
value of $C$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1032</identifier>
 <datestamp>2012-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1032</id><created>2012-06-05</created><authors><author><keyname>Zarrouk</keyname><forenames>Manel</forenames></author><author><keyname>Gouider</keyname><forenames>Med Salah</forenames></author></authors><title>Frequent Patterns mining in time-sensitive Data Stream</title><categories>cs.DB</categories><comments>8pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mining frequent itemsets through static Databases has been extensively
studied and used and is always considered a highly challenging task. For this
reason it is interesting to extend it to data streams field. In the streaming
case, the frequent patterns' mining has much more information to track and much
greater complexity to manage. Infrequent items can become frequent later on and
hence cannot be ignored. The output structure needs to be dynamically
incremented to reflect the evolution of itemset frequencies over time. In this
paper, we study this problem and specifically the methodology of mining
time-sensitive data streams. We tried to improve an existing algorithm by
increasing the temporal accuracy and discarding the out-of-date data by adding
a new concept called the &quot;Shaking Point&quot;. We presented as well some experiments
illustrating the time and space required.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1042</identifier>
 <datestamp>2013-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1042</id><created>2012-06-05</created><updated>2013-01-24</updated><authors><author><keyname>Omri</keyname><forenames>Mohamed Nazih</forenames></author></authors><title>Relevance Feedback for Goal's Extraction from Fuzzy Semantic Networks</title><categories>cs.IR</categories><comments>arXiv admin note: substantial text overlap with arXiv:1206.0925,
  arXiv:1206.1615</comments><journal-ref>Asian Journal of Information Technology(AJIT). Vol. 3, No. 4,
  258-265, (2004)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a short survey of fuzzy and Semantic approaches to
Knowledge Extraction. The goal of such approaches is to define flexible
Knowledge Extraction Systems able to deal with the inherent vagueness and
uncertainty of the Extraction process. It has long been recognised that
interactivity improves the effectiveness of Knowledge Extraction systems.
Novice user's queries is the most natural and interactive medium of
communication and recent progress in recognition is making it possible to build
systems that interact with the user. However, given the typical novice user's
queries submitted to Knowledge Extraction systems, it is easy to imagine that
the effects of goal recognition errors in novice user's queries must be
severely destructive on the system's effectiveness. The experimental work
reported in this paper shows that the use of classical Knowledge Extraction
techniques for novice user's query processing is robust to considerably high
levels of goal recognition errors. Moreover, both standard relevance feedback
and pseudo relevance feedback can be effectively employed to improve the
effectiveness of novice user's query processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1061</identifier>
 <datestamp>2012-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1061</id><created>2012-06-05</created><authors><author><keyname>Omri</keyname><forenames>Mohamed Nazih</forenames></author><author><keyname>Mahjoub</keyname><forenames>Mohamed Ali</forenames></author></authors><title>Use of Fuzzy Sets in Semantic Nets for Providing On-Line Assistance to
  User of Technological Systems</title><categories>cs.AI</categories><journal-ref>International Workshop on Intelligent Knowledge Management
  Techniques I-KOMAT'2002-KES'2002. p. 1444-1449. Podere d'Ombriano, Crema,
  Italy, (2002)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main objective of this paper is to develop a new semantic Network
structure, based on the fuzzy sets theory, used in Artificial Intelligent
system in order to provide effective on-line assistance to users of new
technological systems. This Semantic Networks is used to describe the knowledge
of an &quot;ideal&quot; expert while fuzzy sets are used both to describe the approximate
and uncertain knowledge of novice users who intervene to match fuzzy labels of
a query with categories from an &quot;ideal&quot; expert. The technical system we
consider is a word processor software, with Objects such as &quot;Word&quot; and Goals
such as &quot;Cut&quot; or &quot;Copy&quot;. We suggest to consider the set of the system's Goals
as a set of linguistic variables to which corresponds a set of possible
linguistic values based on the fuzzy set. We consider, therefore, a set of
interpretation's levels for these possible values to which corresponds a set of
membership functions. We also propose a method to measure the similarity degree
between different fuzzy linguistic variables for the partition of the semantic
network in class of similar objects to make easy the diagnosis of the user's
fuzzy queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1065</identifier>
 <datestamp>2012-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1065</id><created>2012-06-05</created><authors><author><keyname>Zeng</keyname><forenames>Shuqing</forenames></author></authors><title>An IMU-Aided Carrier-Phase Differential GPS Positioning System</title><categories>cs.RO</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We consider the problem of carrier-phase differential GPS positioning for an
land vehicle navigation system (LVNS), tightly coupled with an inertial
measurement unit (IMU) and a speedometer. The primary focus is to apply
Bayesian network to an IMU-aided GPS positioning system based on carrier-phase
differential GPS. We describe the implementation details of the positioning
system that integrates GPS measurements (i.e., pseudo-range, carrier-phase and
doppler), IMU measurements, and speedometer measurements. We derive the
linearized state process equation and the measurement equation for GPS and
speedometer. To account for constraints of land vehicle, we add two more pseudo
measurements to ensure the perpendicular velocities close to zero.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1066</identifier>
 <datestamp>2012-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1066</id><created>2012-06-05</created><authors><author><keyname>Choi</keyname><forenames>Eunsol</forenames></author><author><keyname>Tan</keyname><forenames>Chenhao</forenames></author><author><keyname>Lee</keyname><forenames>Lillian</forenames></author><author><keyname>Danescu-Niculescu-Mizil</keyname><forenames>Cristian</forenames></author><author><keyname>Spindel</keyname><forenames>Jennifer</forenames></author></authors><title>Hedge detection as a lens on framing in the GMO debates: A position
  paper</title><categories>cs.CL</categories><comments>10 pp; to appear in Proceedings of the ACL Workshop on
  Extra-Propositional Aspects of Meaning in Computational Linguistics, 2012.
  Data available at
  https://confluence.cornell.edu/display/llresearch/HedgingFramingGMOs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the ways in which participants in public discussions frame
their arguments is important in understanding how public opinion is formed. In
this paper, we adopt the position that it is time for more
computationally-oriented research on problems involving framing. In the
interests of furthering that goal, we propose the following specific,
interesting and, we believe, relatively accessible question: In the controversy
regarding the use of genetically-modified organisms (GMOs) in agriculture, do
pro- and anti-GMO articles differ in whether they choose to adopt a
&quot;scientific&quot; tone?
  Prior work on the rhetoric and sociology of science suggests that hedging may
distinguish popular-science text from text written by professional scientists
for their colleagues. We propose a detailed approach to studying whether hedge
detection can be used to understanding scientific framing in the GMO debates,
and provide corpora to facilitate this study. Some of our preliminary analyses
suggest that hedges occur less frequently in scientific discourse than in
popular text, a finding that contradicts prior assertions in the literature. We
hope that our initial work and data will encourage others to pursue this
promising line of inquiry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1069</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1069</id><created>2012-06-05</created><updated>2013-01-04</updated><authors><author><keyname>Aerts</keyname><forenames>Diederik</forenames></author><author><keyname>Gabora</keyname><forenames>Liane</forenames></author><author><keyname>Sozzo</keyname><forenames>Sandro</forenames></author></authors><title>Concepts and Their Dynamics: A Quantum-Theoretic Modeling of Human
  Thought</title><categories>cs.AI cs.CL quant-ph</categories><comments>31 pages, 5 figures</comments><journal-ref>Topics in Cognitive Science, 5, pp. 737-772, 2013</journal-ref><doi>10.1111/tops.12042</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze different aspects of our quantum modeling approach of human
concepts, and more specifically focus on the quantum effects of contextuality,
interference, entanglement and emergence, illustrating how each of them makes
its appearance in specific situations of the dynamics of human concepts and
their combinations. We point out the relation of our approach, which is based
on an ontology of a concept as an entity in a state changing under influence of
a context, with the main traditional concept theories, i.e. prototype theory,
exemplar theory and theory theory. We ponder about the question why quantum
theory performs so well in its modeling of human concepts, and shed light on
this question by analyzing the role of complex amplitudes, showing how they
allow to describe interference in the statistics of measurement outcomes, while
in the traditional theories statistics of outcomes originates in classical
probability weights, without the possibility of interference. The relevance of
complex numbers, the appearance of entanglement, and the role of Fock space in
explaining contextual emergence, all as unique features of the quantum
modeling, are explicitly revealed in this paper by analyzing human concepts and
their dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1074</identifier>
 <datestamp>2012-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1074</id><created>2012-06-05</created><authors><author><keyname>Fister</keyname><forenames>Iztok</forenames></author><author><keyname>Fister</keyname><forenames>Iztok</forenames><suffix>Jr.</suffix></author><author><keyname>Brest</keyname><forenames>Janez</forenames></author><author><keyname>&#x17d;umer</keyname><forenames>Viljem</forenames></author></authors><title>Memetic Artificial Bee Colony Algorithm for Large-Scale Global
  Optimization</title><categories>cs.NE cs.AI</categories><comments>CONFERENCE: IEEE Congress on Evolutionary Computation, Brisbane,
  Australia, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Memetic computation (MC) has emerged recently as a new paradigm of efficient
algorithms for solving the hardest optimization problems. On the other hand,
artificial bees colony (ABC) algorithms demonstrate good performances when
solving continuous and combinatorial optimization problems. This study tries to
use these technologies under the same roof. As a result, a memetic ABC (MABC)
algorithm has been developed that is hybridized with two local search
heuristics: the Nelder-Mead algorithm (NMA) and the random walk with direction
exploitation (RWDE). The former is attended more towards exploration, while the
latter more towards exploitation of the search space. The stochastic adaptation
rule was employed in order to control the balancing between exploration and
exploitation. This MABC algorithm was applied to a Special suite on Large Scale
Continuous Global Optimization at the 2012 IEEE Congress on Evolutionary
Computation. The obtained results the MABC are comparable with the results of
DECC-G, DECC-G*, and MLCC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1077</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1077</id><created>2012-06-05</created><updated>2012-09-08</updated><authors><author><keyname>Banin</keyname><forenames>Matan</forenames></author><author><keyname>Tsaban</keyname><forenames>Boaz</forenames></author></authors><title>The Discrete Logarithm Problem in Bergman's non-representable ring</title><categories>cs.CR math.GR</categories><comments>Improved exposition. To appear in the Journal of Mathematical
  Cryptology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bergman's Ring $E_p$, parameterized by a prime number $p$, is a ring with
$p^5$ elements that cannot be embedded in a ring of matrices over any
commutative ring. This ring was discovered in 1974. In 2011, Climent, Navarro
and Tortosa described an efficient implementation of $E_p$ using simple modular
arithmetic, and suggested that this ring may be a useful source for intractable
cryptographic problems.
  We present a deterministic polynomial time reduction of the Discrete
Logarithm Problem in $E_p$ to the classical Discrete Logarithm Problem in
$\Zp$, the $p$-element field. In particular, the Discrete Logarithm Problem in
$E_p$ can be solved, by conventional computers, in sub-exponential time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1078</identifier>
 <datestamp>2012-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1078</id><created>2012-06-05</created><authors><author><keyname>Anselme</keyname><forenames>Tueno</forenames></author></authors><title>Zwei Anwendungen des Paillier-Kryptosystems: Blinde Signatur und
  Three-Pass-Protocol</title><categories>cs.CR</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Englisch: In this paper we study the paillier cryptosystem and derive form it
to new schemes. First we transform the signature of paillier in a Blind
signature. Secondly we propose a three-pass protocol wich use the homomorphic
property instead of the commutativity as the Shamir protocol does.
  German: Basierend auf dem Kryptosystem von Paillier und dem damit
eingef\&quot;uhrten Problem der zusammengesetzten Residuenklasse werden in diesem
Artikel zwei kryptographische Verfahren vorgeschlagen. Zun\&quot;achst wird die
Signatur von Paillier in ein blindes Signaturverfahren umgewandelt. Des
Weiteren wird mit der homomorphen Eigenschaft des Kryptosystems von Paillier
ein sogenanntes Three-Pass-Protocol - auch No-Key-Protocol genannt -
entwickelt.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1088</identifier>
 <datestamp>2012-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1088</id><created>2012-06-05</created><updated>2012-06-22</updated><authors><author><keyname>Chen</keyname><forenames>Yutian</forenames></author><author><keyname>Welling</keyname><forenames>Max</forenames></author></authors><title>Bayesian Structure Learning for Markov Random Fields with a Spike and
  Slab Prior</title><categories>stat.ML cs.LG</categories><comments>Accepted in the Conference on Uncertainty in Artificial Intelligence
  (UAI), 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years a number of methods have been developed for automatically
learning the (sparse) connectivity structure of Markov Random Fields. These
methods are mostly based on L1-regularized optimization which has a number of
disadvantages such as the inability to assess model uncertainty and expensive
cross-validation to find the optimal regularization parameter. Moreover, the
model's predictive performance may degrade dramatically with a suboptimal value
of the regularization parameter (which is sometimes desirable to induce
sparseness). We propose a fully Bayesian approach based on a &quot;spike and slab&quot;
prior (similar to L0 regularization) that does not suffer from these
shortcomings. We develop an approximate MCMC method combining Langevin dynamics
and reversible jump MCMC to conduct inference in this model. Experiments show
that the proposed model learns a good combination of the structure and
parameter values without the need for separate hyper-parameter tuning.
Moreover, the model's predictive performance is much more robust than L1-based
methods with hyper-parameter settings that induce highly sparse model
structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1090</identifier>
 <datestamp>2012-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1090</id><created>2012-06-05</created><authors><author><keyname>El-Zawawy</keyname><forenames>Mohamed A.</forenames></author><author><keyname>Daoud</keyname><forenames>Nagwan M.</forenames></author></authors><title>Dynamic Verification for File Safety of Multithreaded Programs</title><categories>cs.LO cs.PL</categories><comments>7 pages, http://paper.ijcsns.org/07_book/201205/20120503.pdf</comments><journal-ref>IJCSNS International Journal of Computer Science and Network
  Security, VOL.12 No.5, pp 14-20, May 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a new semantics to check file safety of
multithreaded programs. A file-safe program is one that reaches a final
configuration under the proposed semantics. We extend the While language with
file operations and multi-threading commands, and call the new language whilef.
This paper shows that the file safety is an un-decidable property for whilef.
The file safety becomes a decidable property in a special case shown in this
paper. The case happens when users provide pointer information. If the file is
safe we call it a strongly safe file program. We modify the syntax and the
semantic of the language and called it SafeWhilef.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1099</identifier>
 <datestamp>2012-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1099</id><created>2012-06-05</created><authors><author><keyname>Bernstein</keyname><forenames>Andrey</forenames></author><author><keyname>Bienstock</keyname><forenames>Daniel</forenames></author><author><keyname>Hay</keyname><forenames>David</forenames></author><author><keyname>Uzunoglu</keyname><forenames>Meric</forenames></author><author><keyname>Zussman</keyname><forenames>Gil</forenames></author></authors><title>Power Grid Vulnerability to Geographically Correlated Failures -
  Analysis and Control Implications</title><categories>cs.SY cs.PF math.OC</categories><report-no>Columbia University, Electrical Engineering, Technical Report
  #2011-05-06, Nov. 2011</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider power line outages in the transmission system of the power grid,
and specifically those caused by a natural disaster or a large scale physical
attack. In the transmission system, an outage of a line may lead to overload on
other lines, thereby eventually leading to their outage. While such cascading
failures have been studied before, our focus is on cascading failures that
follow an outage of several lines in the same geographical area. We provide an
analytical model of such failures, investigate the model's properties, and show
that it differs from other models used to analyze cascades in the power grid
(e.g., epidemic/percolation-based models). We then show how to identify the
most vulnerable locations in the grid and perform extensive numerical
experiments with real grid data to investigate the various effects of
geographically correlated outages and the resulting cascades. These results
allow us to gain insights into the relationships between various parameters and
performance metrics, such as the size of the original event, the final number
of connected components, and the fraction of demand (load) satisfied after the
cascade. In particular, we focus on the timing and nature of optimal control
actions used to reduce the impact of a cascade, in real time. We also compare
results obtained by our model to the results of a real cascade that occurred
during a major blackout in the San Diego area on Sept. 2011. The analysis and
results presented in this paper will have implications both on the design of
new power grids and on identifying the locations for shielding, strengthening,
and monitoring efforts in grid upgrades.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1105</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1105</id><created>2012-06-05</created><updated>2012-09-10</updated><authors><author><keyname>Xiang</keyname><forenames>Biao</forenames></author><author><keyname>Chen</keyname><forenames>Enhong</forenames></author><author><keyname>Liu</keyname><forenames>Qi</forenames></author><author><keyname>Xiong</keyname><forenames>Hui</forenames></author></authors><title>A Linear Circuit Model For Social Influence Analysis</title><categories>cs.SI physics.soc-ph</categories><comments>arXiv admin note: substantial text overlap with arXiv:1205.6024</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the behaviors of information propagation is essential for the
effective exploitation of social influence in social networks. However, few
existing influence models are both tractable and efficient for describing the
information propagation process and quantitatively measuring social influence.
To this end, in this paper, we develop a linear social influence model, named
Circuit due to its close relation to the circuit network. Based on the
predefined four axioms of social influence, we first demonstrate that our model
can efficiently measure the influence strength between any pair of nodes. Along
this line, an upper bound of the node(s)' influence is identified for potential
use, e.g., reducing the search space. Furthermore, we provide the physical
implication of the Circuit model and also a deep analysis of its relationships
with the existing methods, such as PageRank. Then, we propose that the Circuit
model provides a natural solution to the problems of computing each single
node's authority and finding a set of nodes for social influence maximization.
At last, the effectiveness of the proposed model is evaluated on the real-world
data. The extensive experimental results demonstrate that Circuit model
consistently outperforms the state-of-the-art methods and can greatly alleviate
the computation burden of the influence maximization problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1106</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1106</id><created>2012-06-05</created><updated>2013-02-18</updated><authors><author><keyname>Schaul</keyname><forenames>Tom</forenames></author><author><keyname>Zhang</keyname><forenames>Sixin</forenames></author><author><keyname>LeCun</keyname><forenames>Yann</forenames></author></authors><title>No More Pesky Learning Rates</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance of stochastic gradient descent (SGD) depends critically on
how learning rates are tuned and decreased over time. We propose a method to
automatically adjust multiple learning rates so as to minimize the expected
error at any one time. The method relies on local gradient variations across
samples. In our approach, learning rates can increase as well as decrease,
making it suitable for non-stationary problems. Using a number of convex and
non-convex learning tasks, we show that the resulting algorithm matches the
performance of SGD or other adaptive approaches with their best settings
obtained through systematic search, and effectively removes the need for
learning rate tuning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1113</identifier>
 <datestamp>2012-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1113</id><created>2012-06-05</created><authors><author><keyname>Khan</keyname><forenames>Maleq</forenames></author><author><keyname>Kumar</keyname><forenames>V. S. Anil</forenames></author><author><keyname>Pandurangan</keyname><forenames>Gopal</forenames></author><author><keyname>Pei</keyname><forenames>Guanhong</forenames></author></authors><title>A Fast Distributed Approximation Algorithm for Minimum Spanning Trees in
  the SINR Model</title><categories>cs.DC cs.DS</categories><acm-class>C.2.4; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fundamental problem in wireless networks is the \emph{minimum spanning
tree} (MST) problem: given a set $V$ of wireless nodes, compute a spanning tree
$T$, so that the total cost of $T$ is minimized. In recent years, there has
been a lot of interest in the physical interference model based on SINR
constraints. Distributed algorithms are especially challenging in the SINR
model, because of the non-locality of the model.
  In this paper, we develop a fast distributed approximation algorithm for MST
construction in an SINR based distributed computing model. For an $n$-node
network, our algorithm's running time is $O(D\log{n}+\mu\log{n})$ and produces
a spanning tree whose cost is within $O(\log n)$ times the optimal (MST cost),
where $D$ denotes the diameter of the disk graph obtained by using the maximum
possible transmission range, and $\mu=\log{\frac{d_{max}}{d_{min}}}$ denotes
the &quot;distance diversity&quot; w.r.t. the largest and smallest distances between two
nodes. (When $\frac{d_{max}}{d_{min}}$ is $n$-polynomial, $\mu = O(\log n)$.)
Our algorithm's running time is essentially optimal (upto a logarithmic
factor), since computing {\em any} spanning tree takes $\Omega(D)$ time; thus
our algorithm produces a low cost spanning tree in time only a logarithmic
factor more than the time to compute a spanning tree. The distributed
scheduling complexity of the spanning tree resulted from our algorithm is
$O(\mu \log n)$. Our algorithmic design techniques can be useful in designing
efficient distributed algorithms for related &quot;global&quot; problems in wireless
networks in the SINR model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1116</identifier>
 <datestamp>2012-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1116</id><created>2012-06-05</created><authors><author><keyname>Sun</keyname><forenames>Can</forenames></author><author><keyname>Yang</keyname><forenames>Chenyang</forenames></author><author><keyname>Li</keyname><forenames>Yonghui</forenames></author><author><keyname>Vucetic</keyname><forenames>Branka</forenames></author></authors><title>Transceiver Design for Multi-user Multi-antenna Two-way Relay Cellular
  Systems</title><categories>cs.IT math.IT</categories><comments>11 pages, 9 figures, 2 tables, accepted by IEEE Trans. on Commun</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we design interference free transceivers for multi-user
two-way relay systems, where a multi-antenna base station (BS) simultaneously
exchanges information with multiple single-antenna users via a multi-antenna
amplify-and-forward relay station (RS). To offer a performance benchmark and
provide useful insight into the transceiver structure, we employ alternating
optimization to find optimal transceivers at the BS and RS that maximizes the
bidirectional sum rate. We then propose a low complexity scheme, where the BS
transceiver is the zero-forcing precoder and detector, and the RS transceiver
is designed to balance the uplink and downlink sum rates. Simulation results
demonstrate that the proposed scheme is superior to the existing zero forcing
and signal alignment schemes, and the performance gap between the proposed
scheme and the alternating optimization is minor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1118</identifier>
 <datestamp>2012-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1118</id><created>2012-06-05</created><authors><author><keyname>Qi</keyname><forenames>Han</forenames></author><author><keyname>Gani</keyname><forenames>Abdullah</forenames></author></authors><title>Research On Mobile Cloud Computing: Review, Trend, And Perspectives</title><categories>cs.DC</categories><comments>8 pages, 7 figures, The Second International Conference on Digital
  Information and Communication Technology and its Applications</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Mobile Cloud Computing (MCC) which combines mobile computing and cloud
computing, has become one of the industry buzz words and a major discussion
thread in the IT world since 2009. As MCC is still at the early stage of
development, it is necessary to grasp a thorough understanding of the
technology in order to point out the direction of future research. With the
latter aim, this paper presents a review on the background and principle of
MCC, characteristics, recent research work, and future research trends. A brief
account on the background of MCC: from mobile computing to cloud computing is
presented and then followed with a discussion on characteristics and recent
research work. It then analyses the features and infrastructure of mobile cloud
computing. The rest of the paper analyses the challenges of mobile cloud
computing, summary of some research projects related to this area, and points
out promising future research directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1120</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1120</id><created>2012-06-06</created><updated>2012-06-08</updated><authors><author><keyname>Bassett</keyname><forenames>Danielle S.</forenames></author><author><keyname>Alderson</keyname><forenames>David L.</forenames></author><author><keyname>Carlson</keyname><forenames>Jean M.</forenames></author></authors><title>Collective Decision Dynamics in the Presence of External Drivers</title><categories>physics.soc-ph cs.SI nlin.AO</categories><comments>14 pages, 7 figures</comments><doi>10.1103/PhysRevE.86.036105</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a sequence of models describing information transmission and
decision dynamics for a network of individual agents subject to multiple
sources of influence. Our general framework is set in the context of an
impending natural disaster, where individuals, represented by nodes on the
network, must decide whether or not to evacuate. Sources of influence include a
one-to-many externally driven global broadcast as well as pairwise
interactions, across links in the network, in which agents transmit either
continuous opinions or binary actions. We consider both uniform and variable
threshold rules on the individual opinion as baseline models for
decision-making. Our results indicate that 1) social networks lead to
clustering and cohesive action among individuals, 2) binary information
introduces high temporal variability and stagnation, and 3) information
transmission over the network can either facilitate or hinder action adoption,
depending on the influence of the global broadcast relative to the social
network. Our framework highlights the essential role of local interactions
between agents in predicting collective behavior of the population as a whole.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1121</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1121</id><created>2012-06-06</created><updated>2012-09-01</updated><authors><author><keyname>Dimitoglou</keyname><forenames>George</forenames></author><author><keyname>Adams</keyname><forenames>James A.</forenames></author><author><keyname>Jim</keyname><forenames>Carol M.</forenames></author></authors><title>Comparison of the C4.5 and a Naive Bayes Classifier for the Prediction
  of Lung Cancer Survivability</title><categories>cs.LG</categories><comments>9 pages, 3 figures, 9 tables</comments><journal-ref>Journal of Computing, Volume 4, Issue 8, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerous data mining techniques have been developed to extract information
and identify patterns and predict trends from large data sets. In this study,
two classification techniques, the J48 implementation of the C4.5 algorithm and
a Naive Bayes classifier are applied to predict lung cancer survivability from
an extensive data set with fifteen years of patient records. The purpose of the
project is to verify the predictive effectiveness of the two techniques on
real, historical data. Besides the performance outcome that renders J48
marginally better than the Naive Bayes technique, there is a detailed
description of the data and the required pre-processing activities. The
performance results confirm expectations while some of the issues that appeared
during experimentation, underscore the value of having domain-specific
understanding to leverage any domain-specific characteristics inherent in the
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1134</identifier>
 <datestamp>2012-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1134</id><created>2012-06-06</created><authors><author><keyname>Agarwal</keyname><forenames>Rachit</forenames></author><author><keyname>Caesar</keyname><forenames>Matthew</forenames></author><author><keyname>Godfrey</keyname><forenames>P. Brighten</forenames></author><author><keyname>Zhao</keyname><forenames>Ben Y.</forenames></author></authors><title>Shortest Paths in Less Than a Millisecond</title><categories>cs.SI cs.DB physics.soc-ph</categories><comments>6 pages; to appear in SIGCOMM WOSN 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of answering point-to-point shortest path queries on
massive social networks. The goal is to answer queries within tens of
milliseconds while minimizing the memory requirements. We present a technique
that achieves this goal for an extremely large fraction of path queries by
exploiting the structure of the social networks.
  Using evaluations on real-world datasets, we argue that our technique offers
a unique trade-off between latency, memory and accuracy. For instance, for the
LiveJournal social network (roughly 5 million nodes and 69 million edges), our
technique can answer 99.9% of the queries in less than a millisecond. In
comparison to storing all pair shortest paths, our technique requires at least
550x less memory; the average query time is roughly 365 microseconds --- 430x
faster than the state-of-the-art shortest path algorithm. Furthermore, the
relative performance of our technique improves with the size (and density) of
the network. For the Orkut social network (3 million nodes and 220 million
edges), for instance, our technique is roughly 2588x faster than the
state-of-the-art algorithm for computing shortest paths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1145</identifier>
 <datestamp>2012-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1145</id><created>2012-06-06</created><authors><author><keyname>de Nijs</keyname><forenames>Frits</forenames></author><author><keyname>Wilmer</keyname><forenames>Daan</forenames></author></authors><title>Evaluation and Improvement of Laruelle-Widgr\'en Inverse Banzhaf
  Approximation</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this paper is to critically evaluate a heuristic algorithm for
the Inverse Banzhaf Index problem by Laruelle and Widgr\'en. Few qualitative
results are known about the approximation quality of the heuristics for this
problem. The intuition behind the operation of this approximation algorithm is
analysed and evaluated. We found that the algorithm can not handle general
inputs well, and often fails to improve inputs. It is also shown to diverge
after only tens of iterations. We present three alternative extensions of the
algorithm that do not alter the complexity but can result in up to a factor 6.5
improvement in solution quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1147</identifier>
 <datestamp>2012-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1147</id><created>2012-06-06</created><updated>2012-06-08</updated><authors><author><keyname>Zeng</keyname><forenames>Jia</forenames></author><author><keyname>Liu</keyname><forenames>Zhi-Qiang</forenames></author><author><keyname>Cao</keyname><forenames>Xiao-Qin</forenames></author></authors><title>Memory-Efficient Topic Modeling</title><categories>cs.LG cs.IR</categories><comments>20 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As one of the simplest probabilistic topic modeling techniques, latent
Dirichlet allocation (LDA) has found many important applications in text
mining, computer vision and computational biology. Recent training algorithms
for LDA can be interpreted within a unified message passing framework. However,
message passing requires storing previous messages with a large amount of
memory space, increasing linearly with the number of documents or the number of
topics. Therefore, the high memory usage is often a major problem for topic
modeling of massive corpora containing a large number of topics. To reduce the
space complexity, we propose a novel algorithm without storing previous
messages for training LDA: tiny belief propagation (TBP). The basic idea of TBP
relates the message passing algorithms with the non-negative matrix
factorization (NMF) algorithms, which absorb the message updating into the
message passing process, and thus avoid storing previous messages. Experimental
results on four large data sets confirm that TBP performs comparably well or
even better than current state-of-the-art training algorithms for LDA but with
a much less memory consumption. TBP can do topic modeling when massive corpora
cannot fit in the computer memory, for example, extracting thematic topics from
7 GB PUBMED corpora on a common desktop computer with 2GB memory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1148</identifier>
 <datestamp>2012-08-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1148</id><created>2012-06-06</created><updated>2012-08-07</updated><authors><author><keyname>Botha</keyname><forenames>Charl P.</forenames></author><author><keyname>Preim</keyname><forenames>Bernhard</forenames></author><author><keyname>Kaufman</keyname><forenames>Arie</forenames></author><author><keyname>Takahashi</keyname><forenames>Shigeo</forenames></author><author><keyname>Ynnerman</keyname><forenames>Anders</forenames></author></authors><title>From individual to population: Challenges in Medical Visualization</title><categories>cs.GR physics.med-ph</categories><comments>Improvements based on comments by reviewers: Typos and layout issues
  fixed. Added two more multi-modal volume rendering references to 2.1. Added
  more detail on Virtual Colonoscopy to 2.2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we first give a high-level overview of medical visualization
development over the past 30 years, focusing on key developments and the trends
that they represent. During this discussion, we will refer to a number of key
papers that we have also arranged on the medical visualization research
timeline. Based on the overview and our observations of the field, we then
identify and discuss the medical visualization research challenges that we
foresee for the coming decade.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1187</identifier>
 <datestamp>2012-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1187</id><created>2012-06-06</created><updated>2012-06-22</updated><authors><author><keyname>Beliakov</keyname><forenames>Gleb</forenames></author><author><keyname>Johnstone</keyname><forenames>Michael</forenames></author><author><keyname>Creighton</keyname><forenames>Doug</forenames></author><author><keyname>Wilkin</keyname><forenames>Tim</forenames></author></authors><title>Parallel random variates generator for GPUs based on normal numbers</title><categories>cs.MS cs.DC cs.NA math.NA math.PR</categories><comments>preprint, 18 pages</comments><msc-class>11K45, 65C10, 65Y05, 68W10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pseudorandom number generators are required for many computational tasks,
such as stochastic modelling and simulation. This paper investigates the serial
CPU and parallel GPU implementation of a Linear Congruential Generator based on
the binary representation of the normal number $\alpha_{2,3}$. We adapted two
methods of modular reduction which allowed us to perform most operations in
64-bit integer arithmetic, improving on the original implementation based on
106-bit double-double operations. We found that our implementation is faster
than existing methods in literature, and our generation rate is close to the
limiting rate imposed by the efficiency of writing to a GPU's global memory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1188</identifier>
 <datestamp>2012-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1188</id><created>2012-06-06</created><authors><author><keyname>Wu</keyname><forenames>Haoyang</forenames></author></authors><title>Traditional sufficient conditions for Nash implementation may fail on
  Internet</title><categories>cs.GT</categories><comments>17 pages, 1 figure. arXiv admin note: substantial text overlap with
  arXiv:1004.5327</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Maskin's theorem is a fundamental work in the theory of mechanism design.
In this paper, we propose that if agents report messages to the designer
through channels (e.g., Internet), agents can construct a self-enforcing
agreement such that any Pareto-inefficient social choice rule satisfying
monotonicity and no-veto will not be Nash implementable when an additional
condition is satisfied. The key points are: 1) The agreement is unobservable to
the designer, and the designer cannot prevent the agents from constructing such
agreement; 2) The agents act non-cooperatively, and the Maskin mechanism remain
unchanged from the designer's perspective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1199</identifier>
 <datestamp>2012-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1199</id><created>2012-06-06</created><authors><author><keyname>Nakasato</keyname><forenames>Naohito</forenames></author><author><keyname>Ogiya</keyname><forenames>Go</forenames></author><author><keyname>Miki</keyname><forenames>Yohei</forenames></author><author><keyname>Mori</keyname><forenames>Masao</forenames></author><author><keyname>Nomoto</keyname><forenames>Ken'ichi</forenames></author></authors><title>Astrophysical Particle Simulations on Heterogeneous CPU-GPU Systems</title><categories>astro-ph.IM cs.PF physics.comp-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A heterogeneous CPU-GPU node is getting popular in HPC clusters. We need to
rethink algorithms and optimization techniques for such system depending on the
relative performance of CPU vs. GPU. In this paper, we report a performance
optimized particle simulation code &quot;OTOO&quot;, that is based on the octree method,
for heterogenous systems. Main applications of OTOO are astrophysical
simulations such as N-body models and the evolution of a violent merger of
stars. We propose optimal task split between CPU and GPU where GPU is only used
to compute the calculation of the particle force. Also, we describe
optimization techniques such as control of the force accuracy, vectorized tree
walk, and work partitioning among multiple GPUs. We used OTOO for modeling a
merger of two white dwarf stars and found that OTOO is powerful and practical
to simulate the fate of the process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1204</identifier>
 <datestamp>2012-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1204</id><created>2012-06-06</created><authors><author><keyname>Li</keyname><forenames>Yanfu</forenames><affiliation>SSEC, LGI</affiliation></author><author><keyname>Zio</keyname><forenames>Enrico</forenames><affiliation>SSEC</affiliation></author></authors><title>Uncertainty Analysis of the Adequacy Assessment Model of a Distributed
  Generation System</title><categories>cs.PF</categories><proxy>ccsd</proxy><journal-ref>Renewable Energy 41 (2012) 235-244</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the inherent aleatory uncertainties in renewable generators, the
reliability/adequacy assessments of distributed generation (DG) systems have
been particularly focused on the probabilistic modeling of random behaviors,
given sufficient informative data. However, another type of uncertainty
(epistemic uncertainty) must be accounted for in the modeling, due to
incomplete knowledge of the phenomena and imprecise evaluation of the related
characteristic parameters. In circumstances of few informative data, this type
of uncertainty calls for alternative methods of representation, propagation,
analysis and interpretation. In this study, we make a first attempt to
identify, model, and jointly propagate aleatory and epistemic uncertainties in
the context of DG systems modeling for adequacy assessment. Probability and
possibility distributions are used to model the aleatory and epistemic
uncertainties, respectively. Evidence theory is used to incorporate the two
uncertainties under a single framework. Based on the plausibility and belief
functions of evidence theory, the hybrid propagation approach is introduced. A
demonstration is given on a DG system adapted from the IEEE 34 nodes
distribution test feeder. Compared to the pure probabilistic approach, it is
shown that the hybrid propagation is capable of explicitly expressing the
imprecision in the knowledge on the DG parameters into the final adequacy
values assessed. It also effectively captures the growth of uncertainties with
higher DG penetration levels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1208</identifier>
 <datestamp>2012-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1208</id><created>2012-06-06</created><updated>2012-06-29</updated><authors><author><keyname>Chotard</keyname><forenames>Alexandre Adrien</forenames><affiliation>LRI, INRIA Saclay - Ile de France</affiliation></author><author><keyname>Auger</keyname><forenames>Anne</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author><author><keyname>Hansen</keyname><forenames>Nikolaus</forenames><affiliation>LRI, INRIA Saclay - Ile de France, MSR - INRIA</affiliation></author></authors><title>Cumulative Step-size Adaptation on Linear Functions: Technical Report</title><categories>cs.LG</categories><comments>Parallel Problem Solving From Nature (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The CSA-ES is an Evolution Strategy with Cumulative Step size Adaptation,
where the step size is adapted measuring the length of a so-called cumulative
path. The cumulative path is a combination of the previous steps realized by
the algorithm, where the importance of each step decreases with time. This
article studies the CSA-ES on composites of strictly increasing with affine
linear functions through the investigation of its underlying Markov chains.
Rigorous results on the change and the variation of the step size are derived
with and without cumulation. The step-size diverges geometrically fast in most
cases. Furthermore, the influence of the cumulation parameter is studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1216</identifier>
 <datestamp>2012-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1216</id><created>2012-06-06</created><authors><author><keyname>Enjalbert</keyname><forenames>Jean-Yves</forenames><affiliation>LIPN</affiliation></author><author><keyname>Minh</keyname><forenames>Hoang Ngoc</forenames><affiliation>LIPN</affiliation></author></authors><title>Combinatorial study of colored Hurwitz polyz\^etas</title><categories>math.CO cs.DM</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A combinatorial study discloses two surjective morphisms between generalized
shuffle algebras and algebras generated by the colored Hurwitz polyz\^etas. The
combinatorial aspects of the products and co-products involved in these
algebras will be examined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1247</identifier>
 <datestamp>2012-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1247</id><created>2012-06-05</created><authors><author><keyname>Abraham</keyname><forenames>Siby</forenames></author><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author><author><keyname>Sanglikar</keyname><forenames>Mukund</forenames></author></authors><title>Reciprocally induced coevolution: A computational metaphor in
  Mathematics</title><categories>cs.OH</categories><comments>11 pages, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Natural phenomenon of coevolution is the reciprocally induced evolutionary
change between two or more species or population. Though this biological
occurrence is a natural fact, there are only few attempts to use this as a
simile in computation. This paper is an attempt to introduce reciprocally
induced coevolution as a mechanism to counter problems faced by a typical
genetic algorithm applied as an optimization technique. The domain selected for
testing the efficacy of the procedure is the process of finding numerical
solutions of Diophantine equations. Diophantine equations are polynomial
equations in Mathematics where only integer solutions are sought. Such
equations and its solutions are significant in three aspects-(i) historically
they are important as Hilbert's tenth problem with a background of more than
twenty six centuries; (ii) there are many modern application areas of
Diophantine equations like public key cryptography and data dependency in super
computers (iii) it has been proved that there does not exist any general method
to find solutions of such equations. The proposed procedure has been tested
with Diophantine equations with different powers and different number of
variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1264</identifier>
 <datestamp>2012-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1264</id><created>2012-06-06</created><authors><author><keyname>Maguluri</keyname><forenames>Siva Theja</forenames></author><author><keyname>Srikant</keyname><forenames>R</forenames></author><author><keyname>Ying</keyname><forenames>Lei</forenames></author></authors><title>Heavy Traffic Optimal Resource Allocation Algorithms for Cloud Computing
  Clusters</title><categories>cs.PF cs.DC</categories><comments>Technical Report corresponding to the paper of same title to be
  presented at International Teletraffic Conference 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing is emerging as an important platform for business, personal
and mobile computing applications. In this paper, we study a stochastic model
of cloud computing, where jobs arrive according to a stochastic process and
request resources like CPU, memory and storage space. We consider a model where
the resource allocation problem can be separated into a routing or load
balancing problem and a scheduling problem. We study the
join-the-shortest-queue routing and power-of-two-choices routing algorithms
with MaxWeight scheduling algorithm. It was known that these algorithms are
throughput optimal. In this paper, we show that these algorithms are queue
length optimal in the heavy traffic limit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1270</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1270</id><created>2012-06-06</created><updated>2013-02-02</updated><authors><author><keyname>Bittorf</keyname><forenames>Victor</forenames></author><author><keyname>Recht</keyname><forenames>Benjamin</forenames></author><author><keyname>Re</keyname><forenames>Christopher</forenames></author><author><keyname>Tropp</keyname><forenames>Joel A.</forenames></author></authors><title>Factoring nonnegative matrices with linear programs</title><categories>math.OC cs.LG stat.ML</categories><comments>17 pages, 10 figures. Modified theorem statement for robust recovery
  conditions. Revised proof techniques to make arguments more elementary.
  Results on robustness when rows are duplicated have been superseded by
  arxiv.org/1211.6687</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a new approach, based on linear programming, for
computing nonnegative matrix factorizations (NMFs). The key idea is a
data-driven model for the factorization where the most salient features in the
data are used to express the remaining features. More precisely, given a data
matrix X, the algorithm identifies a matrix C such that X approximately equals
CX and some linear constraints. The constraints are chosen to ensure that the
matrix C selects features; these features can then be used to find a low-rank
NMF of X. A theoretical analysis demonstrates that this approach has guarantees
similar to those of the recent NMF algorithm of Arora et al. (2012). In
contrast with this earlier work, the proposed method extends to more general
noise models and leads to efficient, scalable algorithms. Experiments with
synthetic and real datasets provide evidence that the new approach is also
superior in practice. An optimized C++ implementation can factor a
multigigabyte matrix in a matter of minutes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1282</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1282</id><created>2012-06-06</created><updated>2014-03-25</updated><authors><author><keyname>Prabhakaran</keyname><forenames>Vinod M.</forenames></author><author><keyname>Prabhakaran</keyname><forenames>Manoj M.</forenames></author></authors><title>Assisted Common Information with an Application to Secure Two-Party
  Sampling</title><categories>cs.IT cs.CR math.IT</categories><comments>26 pages, 8 figures, to appear in IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we generalize the notion of common information of two dependent
variables introduced by G\'acs &amp; K\&quot;orner. They defined common information as
the largest entropy rate of a common random variable two parties observing one
of the sources each can agree upon. It is well-known that their common
information captures only a limited form of dependence between the random
variables and is zero in most cases of interest. Our generalization, which we
call the Assisted Common Information system, takes into account almost-common
information ignored by G\'acs-K\&quot;orner common information. In the assisted
common information system, a genie assists the parties in agreeing on a more
substantial common random variable; we characterize the trade-off between the
amount of communication from the genie and the quality of the common random
variable produced using a rate region we call the region of tension.
  We show that this region has an application in deriving upperbounds on the
efficiency of secure two-party sampling, which is a special case of secure
multi-party computation, a central problem in modern cryptography. Two parties
desire to produce samples of a pair of jointly distributed random variables
such that neither party learns more about the other's output than what its own
output reveals. They have access to a set up - correlated random variables
whose distribution is different from the desired distribution - and noiseless
communication. We present an upperbound on the rate at which a given set up can
be used to produce samples from a desired distribution by showing a
monotonicity property for the region of tension: a protocol between two parties
can only lower the tension between their views. Then, by calculating the bounds
on the region of tension of various pairs of correlated random variables, we
derive bounds on the rate of secure two-party sampling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1290</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1290</id><created>2012-06-06</created><authors><author><keyname>Michail</keyname><forenames>Othon</forenames></author><author><keyname>Chatzigiannakis</keyname><forenames>Ioannis</forenames></author><author><keyname>Spirakis</keyname><forenames>Paul G.</forenames></author></authors><title>Causality, Influence, and Computation in Possibly Disconnected Dynamic
  Networks</title><categories>cs.DC</categories><comments>14 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we study the propagation of influence and computation in
dynamic distributed systems. We focus on broadcasting models under a worst-case
dynamicity assumption which have received much attention recently. We drop for
the first time in worst-case dynamic networks the common instantaneous
connectivity assumption and require a minimal temporal connectivity. Our
temporal connectivity constraint only requires that another causal influence
occurs within every time-window of some given length. We establish that there
are dynamic graphs with always disconnected instances with equivalent temporal
connectivity to those with always connected instances. We present a termination
criterion and also establish the computational equivalence with instantaneous
connectivity networks. We then consider another model of dynamic networks in
which each node has an underlying communication neighborhood and the
requirement is that each node covers its local neighborhood within any
time-window of some given length. We discuss several properties and provide a
protocol for counting, that is for determining the number of nodes in the
network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1291</identifier>
 <datestamp>2012-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1291</id><created>2012-06-06</created><authors><author><keyname>Keyvanpour</keyname><forenames>Mohammadreza</forenames></author><author><keyname>Tavoli</keyname><forenames>Reza</forenames></author></authors><title>Feature Weighting for Improving Document Image Retrieval System
  Performance</title><categories>cs.AI</categories><journal-ref>International Journal of Computer Science Issues, Vol 9, Issue 3,
  No 3 (2012) 125-130</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature weighting is a technique used to approximate the optimal degree of
influence of individual features. This paper presents a feature weighting
method for Document Image Retrieval System (DIRS) based on keyword spotting. In
this method, we weight the feature using coefficient of multiple correlations.
Coefficient of multiple correlations can be used to describe the synthesized
effects and correlation of each feature. The aim of this paper is to show that
feature weighting increases the performance of DIRS. After applying the feature
weighting method to DIRS the average precision is 93.23% and average recall
become 98.66% respectively
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1299</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1299</id><created>2012-06-06</created><authors><author><keyname>Sun</keyname><forenames>John Z.</forenames></author><author><keyname>Misra</keyname><forenames>Vinith</forenames></author><author><keyname>Goyal</keyname><forenames>Vivek K</forenames></author></authors><title>Distributed Functional Scalar Quantization Simplified</title><categories>cs.IT math.IT</categories><journal-ref>IEEE Trans. on Signal Processing, vol. 61, no. 14, pp. 3495-3508,
  July 2013</journal-ref><doi>10.1109/TSP.2013.2259483</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed functional scalar quantization (DFSQ) theory provides optimality
conditions and predicts performance of data acquisition systems in which a
computation on acquired data is desired. We address two limitations of previous
works: prohibitively expensive decoder design and a restriction to sources with
bounded distributions. We rigorously show that a much simpler decoder has
equivalent asymptotic performance as the conditional expectation estimator
previously explored, thus reducing decoder design complexity. The simpler
decoder has the feature of decoupled communication and computation blocks.
Moreover, we extend the DFSQ framework with the simpler decoder to acquire
sources with infinite-support distributions such as Gaussian or exponential
distributions. Finally, through simulation results we demonstrate that
performance at moderate coding rates is well predicted by the asymptotic
analysis, and we give new insight on the rate of convergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1305</identifier>
 <datestamp>2012-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1305</id><created>2012-06-06</created><authors><author><keyname>Vasile</keyname><forenames>Massimiliano</forenames></author><author><keyname>Zuiani</keyname><forenames>Federico</forenames></author></authors><title>MACS: An Agent-Based Memetic Multiobjective Optimization Algorithm
  Applied to Space Trajectory Design</title><categories>cs.CE cs.NE math.OC</categories><journal-ref>Proceedings of the Institution of Mechanical Engineers, Part G:
  Journal of Aerospace Engineering September 5, 2011 0954410011410274</journal-ref><doi>10.1177/0954410011410274</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an algorithm for multiobjective optimization that blends
together a number of heuristics. A population of agents combines heuristics
that aim at exploring the search space both globally and in a neighborhood of
each agent. These heuristics are complemented with a combination of a local and
global archive. The novel agent- based algorithm is tested at first on a set of
standard problems and then on three specific problems in space trajectory
design. Its performance is compared against a number of state-of-the-art
multiobjective optimisation algorithms that use the Pareto dominance as
selection criterion: NSGA-II, PAES, MOPSO, MTS. The results demonstrate that
the agent-based search can identify parts of the Pareto set that the other
algorithms were not able to capture. Furthermore, convergence is statistically
better although the variance of the results is in some cases higher.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1307</identifier>
 <datestamp>2012-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1307</id><created>2012-06-06</created><authors><author><keyname>Chen</keyname><forenames>Jianxin</forenames></author><author><keyname>Winter</keyname><forenames>Andreas</forenames></author></authors><title>Non-Additivity of the Entanglement of Purification (Beyond Reasonable
  Doubt)</title><categories>quant-ph cs.IT math.IT</categories><comments>5 pages, 2 eps figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We demonstrate the convexity of the difference between the regularized
entanglement of purification and the entropy, as a function of the state. This
is proved by means of a new asymptotic protocol to prepare a state from
pre-shared entanglement and by local operations only. We go on to employ this
convexity property in an investigation of the additivity of the (single-copy)
entanglement of purification: using numerical results for two-qubit Werner
states we find strong evidence that the entanglement of purification is
different from its regularization, hence that entanglement of purification is
not additive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1309</identifier>
 <datestamp>2012-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1309</id><created>2012-06-06</created><authors><author><keyname>Zuiani</keyname><forenames>Federico</forenames></author><author><keyname>Vasile</keyname><forenames>Massimiliano</forenames></author><author><keyname>Gibbings</keyname><forenames>Alison</forenames></author></authors><title>Evidence-Based Robust Design of Deflection Actions for Near Earth
  Objects</title><categories>cs.CE cs.NE math.OC stat.AP</categories><comments>Celestial Mechanics and Dynamical Astronomy, 2012</comments><doi>10.1007/s10569-012-9423-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel approach to the robust design of deflection
actions for Near Earth Objects (NEO). In particular, the case of deflection by
means of Solar-pumped Laser ablation is studied here in detail. The basic idea
behind Laser ablation is that of inducing a sublimation of the NEO surface,
which produces a low thrust thereby slowly deviating the asteroid from its
initial Earth threatening trajectory. This work investigates the integrated
design of the Space-based Laser system and the deflection action generated by
laser ablation under uncertainty. The integrated design is formulated as a
multi-objective optimisation problem in which the deviation is maximised and
the total system mass is minimised. Both the model for the estimation of the
thrust produced by surface laser ablation and the spacecraft system model are
assumed to be affected by epistemic uncertainties (partial or complete lack of
knowledge). Evidence Theory is used to quantify these uncertainties and
introduce them in the optimisation process. The propagation of the trajectory
of the NEO under the laser-ablation action is performed with a novel approach
based on an approximated analytical solution of Gauss' Variational Equations.
An example of design of the deflection of asteroid Apophis with a swarm of
spacecraft is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1312</identifier>
 <datestamp>2012-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1312</id><created>2012-06-06</created><authors><author><keyname>Jakus</keyname><forenames>Stephanie</forenames></author><author><keyname>O'Rourke</keyname><forenames>Joseph</forenames></author></authors><title>From Pop-Up Cards to Coffee-Cup Caustics: The Knight's Visor</title><categories>cs.CG</categories><comments>12 pages, 13 figures</comments><msc-class>51N20, 53A04</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As a pedagogical exercise, we derive the shape of a particularly elegant
pop-up card design, and show that it connects to a classically studied plane
curve that is (among other interpretations) a caustic of a circle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1313</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1313</id><created>2012-06-06</created><authors><author><keyname>Beir&#xf3;</keyname><forenames>Mariano G.</forenames></author><author><keyname>Busch</keyname><forenames>Jorge R.</forenames></author><author><keyname>Grynberg</keyname><forenames>Sebastian P.</forenames></author><author><keyname>Alvarez-Hamelin</keyname><forenames>J. Ignacio</forenames></author></authors><title>Obtaining Communities with a Fitness Growth Process</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><doi>10.1016/j.physa.2013.01.031</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study of community structure has been a hot topic of research over the
last years. But, while successfully applied in several areas, the concept lacks
of a general and precise notion. Facts like the hierarchical structure and
heterogeneity of complex networks make it difficult to unify the idea of
community and its evaluation. The global functional known as modularity is
probably the most used technique in this area. Nevertheless, its limits have
been deeply studied. Local techniques as the ones by Lancichinetti et al. and
Palla et al. arose as an answer to the resolution limit and degeneracies that
modularity has.
  Here we start from the algorithm by Lancichinetti et al. and propose a unique
growth process for a fitness function that, while being local, finds a
community partition that covers the whole network, updating the scale parameter
dynamically. We test the quality of our results by using a set of benchmarks of
heterogeneous graphs. We discuss alternative measures for evaluating the
community structure and, in the light of them, infer possible explanations for
the better performance of local methods compared to global ones in these cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1317</identifier>
 <datestamp>2012-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1317</id><created>2012-06-06</created><authors><author><keyname>Chen</keyname><forenames>Taolue</forenames></author><author><keyname>Dr&#xe4;ger</keyname><forenames>Klaus</forenames></author><author><keyname>Kiefer</keyname><forenames>Stefan</forenames></author></authors><title>Model Checking Stochastic Branching Processes</title><categories>cs.LO cs.FL</categories><comments>This is a technical report accompanying an MFCS'12 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic branching processes are a classical model for describing random
trees, which have applications in numerous fields including biology, physics,
and natural language processing. In particular, they have recently been
proposed to describe parallel programs with stochastic process creation. In
this paper, we consider the problem of model checking stochastic branching
process. Given a branching process and a deterministic parity tree automaton,
we are interested in computing the probability that the generated random tree
is accepted by the automaton. We show that this probability can be compared
with any rational number in PSPACE, and with 0 and 1 in polynomial time. In a
second part, we suggest a tree extension of the logic PCTL, and develop a
PSPACE algorithm for model checking a branching process against a formula of
this logic. We also show that the qualitative fragment of this logic can be
model checked in polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1319</identifier>
 <datestamp>2012-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1319</id><created>2012-06-05</created><authors><author><keyname>Heni</keyname><forenames>Abdelkader</forenames></author><author><keyname>Omri</keyname><forenames>Mohamed Nazih</forenames></author><author><keyname>Alimi</keyname><forenames>Adel</forenames></author></authors><title>Certain Bayesian Network based on Fuzzy knowledge Bases</title><categories>cs.AI</categories><comments>arXiv admin note: substantial text overlap with 1206.0918</comments><journal-ref>International Conference on Internet &amp;, Information Technology in
  Modern Organizations (5th IBIMA), p. 826-832, Cairo, Egypt, December, 13-15
  (2005)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we are trying to examine trade offs between fuzzy logic and
certain Bayesian networks and we propose to combine their respective advantages
into fuzzy certain Bayesian networks (FCBN), a certain Bayesian networks of
fuzzy random variables. This paper deals with different definitions and
classifications of uncertainty, sources of uncertainty, and theories and
methodologies presented to deal with uncertainty. Fuzzification of crisp
certainty degrees to fuzzy variables improves the quality of the network and
tends to bring smoothness and robustness in the network performance. The aim is
to provide a new approach for decision under uncertainty that combines three
methodologies: Bayesian networks certainty distribution and fuzzy logic. Within
the framework proposed in this paper, we address the issue of extending the
certain networks to a fuzzy certain networks in order to cope with a vagueness
and limitations of existing models for decision under imprecise and uncertain
knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1331</identifier>
 <datestamp>2012-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1331</id><created>2012-06-06</created><authors><author><keyname>Myers</keyname><forenames>Seth A.</forenames></author><author><keyname>Zhu</keyname><forenames>Chenguang</forenames></author><author><keyname>Leskovec</keyname><forenames>Jure</forenames></author></authors><title>Information Diffusion and External Influence in Networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social networks play a fundamental role in the diffusion of information.
However, there are two different ways of how information reaches a person in a
network. Information reaches us through connections in our social networks, as
well as through the influence of external out-of-network sources, like the
mainstream media. While most present models of information adoption in networks
assume information only passes from a node to node via the edges of the
underlying network, the recent availability of massive online social media data
allows us to study this process in more detail. We present a model in which
information can reach a node via the links of the social network or through the
influence of external sources. We then develop an efficient model parameter
fitting technique and apply the model to the emergence of URL mentions in the
Twitter network. Using a complete one month trace of Twitter we study how
information reaches the nodes of the network. We quantify the external
influences over time and describe how these influences affect the information
adoption. We discover that the information tends to &quot;jump&quot; across the network,
which can only be explained as an effect of an unobservable external influence
on the network. We find that only about 71% of the information volume in
Twitter can be attributed to network diffusion, and the remaining 29% is due to
external events and factors outside the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1336</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1336</id><created>2012-06-06</created><updated>2012-06-29</updated><authors><author><keyname>Vasile</keyname><forenames>Massimiliano</forenames></author><author><keyname>Maddock</keyname><forenames>Chrisite</forenames></author></authors><title>Design of a Formation of Solar Pumped Lasers for Asteroid Deflection</title><categories>math.OC astro-ph.EP cs.CE physics.space-ph</categories><comments>Advances in Space Research, 2012</comments><doi>10.1016/j.asr.2012.06.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the design of a multi-spacecraft system for the
deflection of asteroids. Each spacecraft is equipped with a fibre laser and a
solar concentrator. The laser induces the sublimation of a portion of the
surface of the asteroid. The jet of gas and debris thrusts the asteroid off its
natural course. The main idea is to have a swarm of spacecraft flying in the
proximity of the asteroid with all the spacecraft beaming to the same location
to achieve the required deflection thrust. The paper presents the design of the
formation orbits and the multi-objective optimization of the swarm in order to
minimize the total mass in space and maximize the deflection of the asteroid.
The paper demonstrates how significant deflections can be obtained with
relatively small sized, easy-to-control spacecraft.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1337</identifier>
 <datestamp>2012-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1337</id><created>2012-06-06</created><authors><author><keyname>Samara</keyname><forenames>Ghassan</forenames></author></authors><title>Security Analysis of Vehicular Ad Hoc Networks (VANET)</title><categories>cs.CR</categories><comments>6 pages; 2010 Second International Conference on Network
  Applications, Protocols and Services</comments><doi>10.1109/NETAPPS.2010.17</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vehicular Ad Hoc Networks (VANET) has mostly gained the attention of today's
research efforts, while current solutions to achieve secure VANET, to protect
the network from adversary and attacks still not enough, trying to reach a
satisfactory level, for the driver and manufacturer to achieve safety of life
and infotainment. The need for a robust VANET networks is strongly dependent on
their security and privacy features, which will be discussed in this paper. In
this paper a various types of security problems and challenges of VANET been
analyzed and discussed; we also discuss a set of solutions presented to solve
these challenges and problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1339</identifier>
 <datestamp>2012-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1339</id><created>2012-06-06</created><authors><author><keyname>Mader</keyname><forenames>Christian</forenames></author><author><keyname>Haslhofer</keyname><forenames>Bernhard</forenames></author><author><keyname>Isaac</keyname><forenames>Antoine</forenames></author></authors><title>Finding Quality Issues in SKOS Vocabularies</title><categories>cs.DL cs.IR</categories><comments>12 pages, to be published in TPDL 2012 proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Simple Knowledge Organization System (SKOS) is a standard model for
controlled vocabularies on the Web. However, SKOS vocabularies often differ in
terms of quality, which reduces their applicability across system boundaries.
Here we investigate how we can support taxonomists in improving SKOS
vocabularies by pointing out quality issues that go beyond the integrity
constraints defined in the SKOS specification. We identified potential
quantifiable quality issues and formalized them into computable quality
checking functions that can find affected resources in a given SKOS vocabulary.
We implemented these functions in the qSKOS quality assessment tool, analyzed
15 existing vocabularies, and found possible quality issues in all of them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1355</identifier>
 <datestamp>2012-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1355</id><created>2012-06-06</created><authors><author><keyname>Gong</keyname><forenames>Xiaowen</forenames></author><author><keyname>Zhang</keyname><forenames>Junshan</forenames></author><author><keyname>Cochran</keyname><forenames>Douglas</forenames></author></authors><title>A Coverage Theory of Bistatic Radar Networks: Worst-Case Intrusion Path
  and Optimal Deployment</title><categories>cs.NI</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study optimal radar deployment for intrusion detection,
with focus on network coverage. In contrast to the disk-based sensing model in
a traditional sensor network, the detection range of a bistatic radar depends
on the locations of both the radar transmitter and radar receiver, and is
characterized by Cassini ovals. Furthermore, in a network with multiple radar
transmitters and receivers, since any pair of transmitter and receiver can
potentially form a bistatic radar, the detection ranges of different bistatic
radars are coupled and the corresponding network coverage is intimately related
to the locations of all transmitters and receivers, making the optimal
deployment design highly non-trivial. Clearly, the detectability of an intruder
depends on the highest SNR received by all possible bistatic radars. We focus
on the worst-case intrusion detectability, i.e., the minimum possible
detectability along all possible intrusion paths. Although it is plausible to
deploy radars on a shortest line segment across the field, it is not always
optimal in general, which we illustrate via counter-examples. We then present a
sufficient condition on the field geometry for the optimality of shortest line
deployment to hold. Further, we quantify the local structure of detectability
corresponding to a given deployment order and spacings of radar transmitters
and receivers, building on which we characterize the optimal deployment to
maximize the worst-case intrusion detectability. Our results show that the
optimal deployment locations exhibit a balanced structure. We also develop a
polynomial-time approximation algorithm for characterizing the worse-case
intrusion path for any given locations of radars under random deployment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1358</identifier>
 <datestamp>2012-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1358</id><created>2012-06-06</created><authors><author><keyname>Soua</keyname><forenames>Ahmed</forenames></author><author><keyname>Ameur</keyname><forenames>Walid Ben</forenames></author><author><keyname>Afifi</keyname><forenames>Hossam</forenames></author></authors><title>Broadcast-based Directional Routing in Vehicular Ad-Hoc Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An optimized broadcast protocol is proposed for VANETs. It is based on two
key information: the direction to the destination and the beamforming angle
{\theta}. The efficiency of this technique is demonstrated in terms of packet
delivery, bandwidth gain and probability of transmission success. An analytical
model is developed to calculate the transmission area. This model allows
capturing the propagation shape of the forwarding area. Comparisons with
simulations show that the analytical model is precise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1365</identifier>
 <datestamp>2012-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1365</id><created>2012-06-06</created><authors><author><keyname>Lesnick</keyname><forenames>Michael</forenames></author></authors><title>Multidimensional Interleavings and Applications to Topological Inference</title><categories>math.AT cs.CG math.ST stat.TH</categories><comments>Late stage draft of Ph.D. thesis. 176 pages. Expands upon content in
  arXiv:1106.5305</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work concerns the theoretical foundations of persistence-based
topological data analysis. We develop theory of topological inference in the
multidimensional persistence setting, and directly at the (topological) level
of filtrations rather than only at the (algebraic) level of persistent homology
modules.
  Our main mathematical objects of study are interleavings. These are tools for
quantifying the similarity between two multidimensional filtrations or
persistence modules. They were introduced for 1-D filtrations and persistence
modules by Chazal, Cohen-Steiner, Glisse, Guibas, and Oudot. We introduce
generalizations of the definitions of interleavings given by Chazal et al. and
use these to define pseudometrics, called interleaving distances, on
multidimensional filtrations and multidimensional persistence modules.
  We present an in-depth study of interleavings and interleaving distances. We
then use them to formulate and prove several multidimensional analogues of a
topological inference theorem of Chazal, Guibas, Oudot, and Skraba. These
results hold directly at the level of filtrations; they yield as corollaries
corresponding results at the module level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1389</identifier>
 <datestamp>2013-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1389</id><created>2012-06-06</created><updated>2013-07-18</updated><authors><author><keyname>Liu</keyname><forenames>Xi</forenames></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames></author><author><keyname>Erkip</keyname><forenames>Elza</forenames></author></authors><title>Lossy Computing of Correlated Sources with Fractional Sampling</title><categories>cs.IT math.IT</categories><comments>33 pages, 11 figures, to appear in journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of lossy compression for the computation of
a function of two correlated sources, both of which are observed at the
encoder. Due to presence of observation costs, the encoder is allowed to
observe only subsets of the samples from both sources, with a fraction of such
sample pairs possibly overlapping. The rate-distortion function is
characterized for memory-less sources, and then specialized to Gaussian and
binary sources for selected functions and with quadratic and Hamming distortion
metrics, respectively. The optimal measurement overlap fraction is shown to
depend on the function to be computed by the decoder, on the source statistics,
including the correlation, and on the link rate. Special cases are discussed in
which the optimal overlap fraction is the maximum or minimum possible value
given the sampling budget, illustrating non-trivial performance trade-offs in
the design of the sampling strategy. Finally, the analysis is extended to the
multi-hop set-up with jointly Gaussian sources, where each encoder can observe
only one of the sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1390</identifier>
 <datestamp>2012-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1390</id><created>2012-06-06</created><authors><author><keyname>Bridges</keyname><forenames>Patrick G.</forenames></author><author><keyname>Ferreira</keyname><forenames>Kurt B.</forenames></author><author><keyname>Heroux</keyname><forenames>Michael A.</forenames></author><author><keyname>Hoemmen</keyname><forenames>Mark</forenames></author></authors><title>Fault-tolerant linear solvers via selective reliability</title><categories>math.NA cs.DC cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy increasingly constrains modern computer hardware, yet protecting
computations and data against errors costs energy. This holds at all scales,
but especially for the largest parallel computers being built and planned
today. As processor counts continue to grow, the cost of ensuring reliability
consistently throughout an application will become unbearable. However, many
algorithms only need reliability for certain data and phases of computation.
This suggests an algorithm and system codesign approach. We show that if the
system lets applications apply reliability selectively, we can develop
algorithms that compute the right answer despite faults. These &quot;fault-tolerant&quot;
iterative methods either converge eventually, at a rate that degrades
gracefully with increased fault rate, or return a clear failure indication in
the rare case that they cannot converge. Furthermore, they store most of their
data unreliably, and spend most of their time in unreliable mode.
  We demonstrate this for the specific case of detected but uncorrectable
memory faults, which we argue are representative of all kinds of faults. We
developed a cross-layer application / operating system framework that
intercepts and reports uncorrectable memory faults to the application, rather
than killing the application, as current operating systems do. The application
in turn can mark memory allocations as subject to such faults. Using this
framework, we wrote a fault-tolerant iterative linear solver using components
from the Trilinos solvers library. Our solver exploits hybrid parallelism (MPI
and threads). It performs just as well as other solvers if no faults occur, and
converges where other solvers do not in the presence of faults. We show
convergence results for representative test problems. Near-term future work
will include performance tests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1402</identifier>
 <datestamp>2012-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1402</id><created>2012-06-07</created><authors><author><keyname>Jalali</keyname><forenames>Ali</forenames></author><author><keyname>Sanghavi</keyname><forenames>Sujay</forenames></author></authors><title>A New Greedy Algorithm for Multiple Sparse Regression</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new algorithm for multiple sparse regression in high
dimensions, where the task is to estimate the support and values of several
(typically related) sparse vectors from a few noisy linear measurements. Our
algorithm is a &quot;forward-backward&quot; greedy procedure that -- uniquely -- operates
on two distinct classes of objects. In particular, we organize our target
sparse vectors as a matrix; our algorithm involves iterative addition and
removal of both (a) individual elements, and (b) entire rows (corresponding to
shared features), of the matrix.
  Analytically, we establish that our algorithm manages to recover the supports
(exactly) and values (approximately) of the sparse vectors, under assumptions
similar to existing approaches based on convex optimization. However, our
algorithm has a much smaller computational complexity. Perhaps most
interestingly, it is seen empirically to require visibly fewer samples. Ours
represents the first attempt to extend greedy algorithms to the class of models
that can only/best be represented by a combination of component structural
assumptions (sparse and group-sparse, in our case).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1405</identifier>
 <datestamp>2012-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1405</id><created>2012-06-07</created><authors><author><keyname>Jaganathan</keyname><forenames>Kishore</forenames></author><author><keyname>Oymak</keyname><forenames>Samet</forenames></author><author><keyname>Hassibi</keyname><forenames>Babak</forenames></author></authors><title>Recovery of Sparse 1-D Signals from the Magnitudes of their Fourier
  Transform</title><categories>cs.IT math.IT math.OC</categories><comments>to appear in ISIT 2012, 12 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of signal recovery from the autocorrelation, or equivalently, the
magnitudes of the Fourier transform, is of paramount importance in various
fields of engineering. In this work, for one-dimensional signals, we give
conditions, which when satisfied, allow unique recovery from the
autocorrelation with very high probability. In particular, for sparse signals,
we develop two non-iterative recovery algorithms. One of them is based on
combinatorial analysis, which we prove can recover signals upto sparsity
$o(n^{1/3})$ with very high probability, and the other is developed using a
convex optimization based framework, which numerical simulations suggest can
recover signals upto sparsity $o(n^{1/2})$ with very high probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1406</identifier>
 <datestamp>2012-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1406</id><created>2012-06-07</created><authors><author><keyname>Walia</keyname><forenames>Arundhati</forenames></author><author><keyname>Ahson</keyname><forenames>Syed i.</forenames></author></authors><title>A Novel Advanced Heap Corruption and Security Method</title><categories>cs.CR</categories><comments>5 pages,9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heap security has been a major concern since the past two decades. Recently
many methods have been proposed to secure heap i.e. to avoid heap overrun and
attacks. The paper describes a method suggested to secure heap at the operating
system level. Major emphasis is given to Solaris operating system's dynamic
memory manager. When memory is required dynamically during runtime, the
SysVmalloc acts as a memory allocator.Vmalloc allocates the chunks of memory in
the form of splay tree structure. A self adjusting binary tree structure is
reviewed in the paper, moreover major security issue to secure heap area is
also suggested in the paper
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1409</identifier>
 <datestamp>2012-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1409</id><created>2012-06-07</created><authors><author><keyname>Zolfagharnasab</keyname><forenames>Hooshiar</forenames></author></authors><title>Reducing Packet Overhead in Mobile IPv6</title><categories>cs.NI</categories><doi>10.5121/ijdps.2012.3301</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Common Mobile IPv6 mechanisms, Bidirectional tunneling and Route
optimization, show inefficient packet overhead when both nodes are mobile.
Researchers have proposed methods to reduce packet overhead regarding to
maintain compatible with standard mechanisms. In this paper, three mechanisms
in Mobile IPv6 are discussed to show their efficiency and performance.
Following discussion, a new mechanism called Improved Tunneling-based Route
Optimization is proposed and due to performance analysis, it is shown that
proposed mechanism has less overhead comparing to common mechanisms. Analytical
results indicate that Improved Tunneling-based Route Optimization transmits
more payloads due to send packets with less overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1414</identifier>
 <datestamp>2012-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1414</id><created>2012-06-07</created><authors><author><keyname>Firouzi</keyname><forenames>Shahab</forenames><affiliation>Department of Computer engineering, Yazd Branch, Islamic Azad University, Yazd, Iran</affiliation></author><author><keyname>Nezarat</keyname><forenames>Amin</forenames><affiliation>Department of Computer engineering, Yazd Branch, Islamic Azad University, Yazd, Iran</affiliation></author></authors><title>An Intelligent Approach for Negotiating between chains in Supply Chain
  Management Systems</title><categories>cs.AI</categories><comments>10</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Holding commercial negotiations and selecting the best supplier in supply
chain management systems are among weaknesses of producers in production
process. Therefore, applying intelligent systems may have an effective role in
increased speed and improved quality in the selections .This paper introduces a
system which tries to trade using multi-agents systems and holding negotiations
between any agents. In this system, an intelligent agent is considered for each
segment of chains which it tries to send order and receive the response with
attendance in negotiation medium and communication with other agents .This
paper introduces how to communicate between agents, characteristics of
multi-agent and standard registration medium of each agent in the environment.
JADE (Java Application Development Environment) was used for implementation and
simulation of agents cooperation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1418</identifier>
 <datestamp>2012-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1418</id><created>2012-06-07</created><authors><author><keyname>Duong</keyname><forenames>Thuy Van T.</forenames></author><author><keyname>Tran</keyname><forenames>Dinh Que</forenames></author><author><keyname>Tran</keyname><forenames>Cong Hung</forenames></author></authors><title>A weighted combination similarity measure for mobility patterns in
  wireless networks</title><categories>cs.AI</categories><comments>15 pages, 2 figures; International Journal of Computer Networks &amp;
  Communications (IJCNC) http://airccse.org/journal/ijc2012.html</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The similarity between trajectory patterns in clustering has played an
important role in discovering movement behaviour of different groups of mobile
objects. Several approaches have been proposed to measure the similarity
between sequences in trajectory data. Most of these measures are based on
Euclidean space or on spatial network and some of them have been concerned with
temporal aspect or ordering types. However, they are not appropriate to
characteristics of spatiotemporal mobility patterns in wireless networks. In
this paper, we propose a new similarity measure for mobility patterns in
cellular space of wireless network. The framework for constructing our measure
is composed of two phases as follows. First, we present formal definitions to
capture mathematically two spatial and temporal similarity measures for
mobility patterns. And then, we define the total similarity measure by means of
a weighted combination of these similarities. The truth of the partial and
total similarity measures are proved in mathematics. Furthermore, instead of
the time interval or ordering, our work makes use of the timestamp at which two
mobility patterns share the same cell. A case study is also described to give a
comparison of the combination measure with other ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1419</identifier>
 <datestamp>2012-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1419</id><created>2012-06-07</created><authors><author><keyname>Khediri</keyname><forenames>Salim el</forenames></author><author><keyname>Nasri</keyname><forenames>Nejah</forenames></author><author><keyname>Samet</keyname><forenames>Mounir</forenames></author><author><keyname>Wei</keyname><forenames>Anne</forenames></author><author><keyname>Kachouri</keyname><forenames>Abdennaceur</forenames></author></authors><title>Analysis study of time synchronization protocols in wireless sensor
  networks</title><categories>cs.NI</categories><comments>15 pages, 1 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the main pervasive problems Wireless Sensor Networks (WSN) encounter
is to maintain flawless communication sharing and cooperative processing
between sensors via radio links to ensure a reliable treatment of information.
Many applications based on these WSNs consider local clocks at each sensor node
that need to be synchronized to a common notion of time. In this context, the
majority of previous researches were focused on the study of protocols, and
algorithms that address these issues in order to resolve synchronization
problems. Previous fforts and empirical studies in wireless sensor network
(WSN) proposed several solutions (algorithms). The focus of this this paper is
to examine and evaluate the most important synchronization algorithms based on
the positions of various quantitative and qualitative synchronization protocols
for energy-efficient information processing and routing in WSNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1422</identifier>
 <datestamp>2013-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1422</id><created>2012-06-07</created><updated>2013-09-14</updated><authors><author><keyname>Nivasch</keyname><forenames>Gabriel</forenames></author><author><keyname>Pach</keyname><forenames>J&#xe1;nos</forenames></author><author><keyname>Tardos</keyname><forenames>G&#xe1;bor</forenames></author></authors><title>The visible perimeter of an arrangement of disks</title><categories>cs.CG cs.DM</categories><comments>12 pages, 5 figures</comments><msc-class>68U05, 68R99</msc-class><journal-ref>Computational Geometry: Theory and Applications, 47:42-51, 2014</journal-ref><doi>10.1016/j.comgeo.2013.08.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a collection of n opaque unit disks in the plane, we want to find a
stacking order for them that maximizes their visible perimeter---the total
length of all pieces of their boundaries visible from above. We prove that if
the centers of the disks form a dense point set, i.e., the ratio of their
maximum to their minimum distance is O(n^1/2), then there is a stacking order
for which the visible perimeter is Omega(n^2/3). We also show that this bound
cannot be improved in the case of a sufficiently small n^1/2 by n^1/2 uniform
grid. On the other hand, if the set of centers is dense and the maximum
distance between them is small, then the visible perimeter is O(n^3/4) with
respect to any stacking order. This latter bound cannot be improved either.
Finally, we address the case where no more than c disks can have a point in
common. These results partially answer some questions of Cabello, Haverkort,
van Kreveld, and Speckmann.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1426</identifier>
 <datestamp>2012-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1426</id><created>2012-06-07</created><authors><author><keyname>Heni</keyname><forenames>Maher</forenames></author><author><keyname>Bouallegue</keyname><forenames>Ammar</forenames></author><author><keyname>Bouallegue</keyname><forenames>Ridha</forenames></author></authors><title>Energy Consumption Model in ad hoc Mobile Network</title><categories>cs.NI</categories><comments>11 pages 5 figures results found after publication of a paper in
  IJWMN; (2012),&quot;Power Control in reactive routing protocol for mobile ad hoc
  network&quot;, International Journal of Wireless &amp; Mobile Networks (IJWMN)</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The aim of this work is to model the nodes battery discharge in wireless ad
hoc networks. Many work focus on the energy consumption in such networks. Even,
the research in the highest layers of the ISO model, takes into account the
energy consumption and efficiency. Indeed, the nodes that form such network are
mobiles, so no instant recharge of battery. Also with special type of ad hoc
networks are wireless sensors networks using non-rechargeable batteries. All
nodes with an exhausted battery are considered death and left the network. To
consider the energy consumption, in this work we model using a Markov chain,
the discharge of the battery considering of instant activation and deactivation
distribution function of these nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1428</identifier>
 <datestamp>2012-08-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1428</id><created>2012-06-07</created><updated>2012-08-07</updated><authors><author><keyname>Pfister</keyname><forenames>Hanspeter</forenames></author><author><keyname>Kaynig</keyname><forenames>Verena</forenames></author><author><keyname>Botha</keyname><forenames>Charl P.</forenames></author><author><keyname>Bruckner</keyname><forenames>Stefan</forenames></author><author><keyname>Dercksen</keyname><forenames>Vincent J.</forenames></author><author><keyname>Hege</keyname><forenames>Hans-Christian</forenames></author><author><keyname>Roerdink</keyname><forenames>Jos B. T. M.</forenames></author></authors><title>Visualization in Connectomics</title><categories>cs.GR q-bio.NC</categories><comments>Improved definition of diffusion PDF. Integrated reviewer comments:
  Added figures showing DTI tractography and glyphs, fMRI connectivity vis, EM
  reconstruction of neuronal structures, Brainbow image. Typos and grammar
  errors fixed. Description of connectivity matrix added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Connectomics is a field of neuroscience that analyzes neuronal connections. A
connectome is a complete map of a neuronal system, comprising all neuronal
connections between its structures. The term &quot;connectome&quot; is close to the word
&quot;genome&quot; and implies completeness of all neuronal connections, in the same way
as a genome is a complete listing of all nucleotide sequences. The goal of
connectomics is to create a complete representation of the brain's wiring. Such
a representation is believed to increase our understanding of how functional
brain states emerge from their underlying anatomical structure. Furthermore, it
can provide important information for the cure of neuronal dysfunctions like
schizophrenia or autism. In this paper, we review the current state-of-the-art
of visualization and image processing techniques in the field of connectomics
and describe some remaining challenges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1430</identifier>
 <datestamp>2012-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1430</id><created>2012-06-07</created><authors><author><keyname>Khatri</keyname><forenames>Yogita</forenames></author></authors><title>Distance Based Asynchronous Recovery Approach in Mobile Computing
  Environment</title><categories>cs.DB cs.DC</categories><comments>7 pages, 1figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A mobile computing system is a distributed system in which at least one of
the processes is mobile. They are constrained by lack of stable storage, low
network bandwidth, mobility, frequent disconnection and limited battery life.
Checkpointing is one of the commonly used techniques to provide fault tolerance
in mobile computing environment. In order to suit the mobile environment a
distance based recovery scheme is proposed which is based on checkpointing and
message logging. After the system recovers from failures, only the failed
processes rollback and restart from their respective recent checkpoints,
independent of the others. The salient feature of this scheme is to reduce the
transfer and recovery cost. While the mobile host moves with in a specific
range, recovery information is not moved and thus only be transferred nearby if
the mobile host moves out of certain range.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1438</identifier>
 <datestamp>2012-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1438</id><created>2012-06-07</created><authors><author><keyname>Tajer</keyname><forenames>Ali</forenames></author><author><keyname>Castro</keyname><forenames>Rui M.</forenames></author><author><keyname>Wang</keyname><forenames>Xiaodong</forenames></author></authors><title>Adaptive Sensing of Congested Spectrum Bands</title><categories>cs.IT math.IT</categories><comments>16 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cognitive radios process their sensed information collectively in order to
opportunistically identify and access under-utilized spectrum segments
(spectrum holes). Due to the transient and rapidly-varying nature of the
spectrum occupancy, the cognitive radios (secondary users) must be agile in
identifying the spectrum holes in order to enhance their spectral efficiency.
We propose a novel {\em adaptive} procedure to reinforce the agility of the
secondary users for identifying {\em multiple} spectrum holes simultaneously
over a wide spectrum band. This is accomplished by successively {\em exploring}
the set of potential spectrum holes and {\em progressively} allocating the
sensing resources to the most promising areas of the spectrum. Such exploration
and resource allocation results in conservative spending of the sensing
resources and translates into very agile spectrum monitoring. The proposed
successive and adaptive sensing procedure is in contrast to the more
conventional approaches that distribute the sampling resources equally over the
entire spectrum. Besides improved agility, the adaptive procedure requires
less-stringent constraints on the power of the primary users to guarantee that
they remain distinguishable from the environment noise and renders more
reliable spectrum hole detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1443</identifier>
 <datestamp>2012-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1443</id><created>2012-06-07</created><authors><author><keyname>Perwej</keyname><forenames>Asif</forenames></author></authors><title>On applying Neuro - Computing in E-com Domain</title><categories>cs.NE</categories><comments>5 Pages, 1 Figure, ISSN No. 2250 - 2297</comments><journal-ref>International Journal of Arts Commerce &amp;
  Management(IJACM),November 2011, Volume 01, Number 01, Pages 1-5</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Prior studies have generally suggested that Artificial Neural Networks (ANNs)
are superior to conventional statistical models in predicting consumer buying
behavior. There are, however, contradicting findings which raise question over
usefulness of ANNs. This paper discusses development of three neural networks
for modeling consumer e-commerce behavior and compares the findings to
equivalent logistic regression models. The results showed that ANNs predict
e-commerce adoption slightly more accurately than logistic models but this is
hardly justifiable given the added complexity. Further, ANNs seem to be highly
adaptive, particularly when a small sample is coupled with a large number of
nodes in hidden layers which, in turn, limits the neural networks'
generalisability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1447</identifier>
 <datestamp>2012-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1447</id><created>2012-06-07</created><authors><author><keyname>Perwej</keyname><forenames>Asif</forenames></author></authors><title>Analytical Study for Seeking Relation Between Customer Relationship
  Management and Enterprise Resource Planning</title><categories>cs.OH</categories><comments>3 Pages, ISSN No. 2250 - 2297</comments><journal-ref>International Journal of Arts Commerce &amp; Management (IJACM), June
  2011, Volume 10, Number 01, pages 6-8</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Enterprise Resource Planning (ERP) is a integration of various resources of
any organization. It is computer software. All kinds of organization data that
is relating to each and every function of the organization are available in
ERP. So most of the big business organizations are implementing ERP and some of
the medium, small scale companies are also using ERP system. CRM in an
organization helps to retain their existing customers as well as capturing new
customers for their products. So it makes the organization to produce those
goods required by their consumers. This paper focuses mainly on the merging of
CRM and ERP through Neural Networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1450</identifier>
 <datestamp>2012-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1450</id><created>2012-06-07</created><authors><author><keyname>Ghosh</keyname><forenames>Debalina</forenames></author><author><keyname>Debnath</keyname><forenames>Depanwita Sarkar</forenames></author><author><keyname>Bose</keyname><forenames>Saikat</forenames></author></authors><title>A comparative study of performance of fpga based mel filter bank &amp; bark
  filter bank</title><categories>cs.SD</categories><comments>16 pages, 20 figures, 6 tables; International Journal of Artificial
  Intelligence &amp; Applications (IJAIA), Vol.3, No.3, May 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The sensitivity of human ear is dependent on frequency which is nonlinearly
resolved across the audio spectrum .Now to improve the recognition performance
in a similar non linear approach requires a front -end design, suggested by
empirical evidences. A popular alternative to linear prediction based analysis
is therefore filter bank analysis since this provides a much more
straightforward route to obtain the desired non-linear frequency resolution.
MEL filter bank and BARK filter bank are two popular filter bank analysis
techniques. This paper presents FPGA based implementation of MEL filter bank
and BARK filter bank with different bandwidths and different signal spectrum
ranges. The designs have been implemented using VHDL, simulated and verified
using Xilinx 11.1.For each filter bank, the basic building block is implemented
in Spartan 3E. A comparative study among these two mentioned filter banks is
also done in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1456</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1456</id><created>2012-06-07</created><updated>2012-06-11</updated><authors><author><keyname>Sendra</keyname><forenames>J. Rafael</forenames></author><author><keyname>Sevilla</keyname><forenames>David</forenames></author></authors><title>First Steps Towards Radical Parametrization of Algebraic Surfaces</title><categories>cs.SC math.AG</categories><comments>31 pages, 7 color figures. v2: added another case of genus 1</comments><journal-ref>Computer Aided Geometric Design 30 (2013) issue 4, 374-388</journal-ref><doi>10.1016/j.cagd.2012.12.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the notion of radical parametrization of a surface, and we
provide algorithms to compute such type of parametrizations for families of
surfaces, like: Fermat surfaces, surfaces with a high multiplicity (at least
the degree minus 4) singularity, all irreducible surfaces of degree at most 5,
all irreducible singular surfaces of degree 6, and surfaces containing a pencil
of low-genus curves. In addition, we prove that radical parametrizations are
preserved under certain type of geometric constructions that include offset and
conchoids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1458</identifier>
 <datestamp>2012-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1458</id><created>2012-06-07</created><authors><author><keyname>Ershad</keyname><forenames>Shervan Fekri</forenames></author><author><keyname>Hashemi</keyname><forenames>Sattar</forenames></author></authors><title>Dispelling Classes Gradually to Improve Quality of Feature Reduction
  Approaches</title><categories>cs.AI</categories><comments>11 Pages, 5 Figure, 7 Tables; Advanced Computing: An International
  Journal (ACIJ), Vol.3, No.3, May 2012</comments><doi>10.5121/acij.2012.3310</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature reduction is an important concept which is used for reducing
dimensions to decrease the computation complexity and time of classification.
Since now many approaches have been proposed for solving this problem, but
almost all of them just presented a fix output for each input dataset that some
of them aren't satisfied cases for classification. In this we proposed an
approach as processing input dataset to increase accuracy rate of each feature
extraction methods. First of all, a new concept called dispelling classes
gradually (DCG) is proposed to increase separability of classes based on their
labels. Next, this method is used to process input dataset of the feature
reduction approaches to decrease the misclassification error rate of their
outputs more than when output is achieved without any processing. In addition
our method has a good quality to collate with noise based on adapting dataset
with feature reduction approaches. In the result part, two conditions (With
process and without that) are compared to support our idea by using some of UCI
datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1469</identifier>
 <datestamp>2012-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1469</id><created>2012-06-07</created><authors><author><keyname>Ma</keyname><forenames>Liang</forenames><affiliation>IRCCyN, DIE</affiliation></author><author><keyname>Ma</keyname><forenames>Ruina</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Chablat</keyname><forenames>Damien</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Bennis</keyname><forenames>Fouad</forenames><affiliation>IRCCyN</affiliation></author></authors><title>Human Arm simulation for interactive constrained environment design</title><categories>cs.RO</categories><comments>International Journal on Interactive Design and Manufacturing
  (IJIDeM) (2012) 1-12. arXiv admin note: substantial text overlap with
  arXiv:1012.4327</comments><proxy>ccsd</proxy><doi>10.1007/s12008-012-0162-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During the conceptual and prototype design stage of an industrial product, it
is crucial to take assembly/disassembly and maintenance operations in advance.
A well-designed system should enable relatively easy access of operating
manipulators in the constrained environment and reduce musculoskeletal disorder
risks for those manual handling operations. Trajectory planning comes up as an
important issue for those assembly and maintenance operations under a
constrained environment, since it determines the accessibility and the other
ergonomics issues, such as muscle effort and its related fatigue. In this
paper, a customer-oriented interactive approach is proposed to partially solve
ergonomic related issues encountered during the design stage under a
constrained system for the operator's convenience. Based on a single objective
optimization method, trajectory planning for different operators could be
generated automatically. Meanwhile, a motion capture based method assists the
operator to guide the trajectory planning interactively when either a local
minimum is encountered within the single objective optimization or the operator
prefers guiding the virtual human manually. Besides that, a physical engine is
integrated into this approach to provide physically realistic simulation in
real time manner, so that collision free path and related dynamic information
could be computed to determine further muscle fatigue and accessibility of a
product design
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1471</identifier>
 <datestamp>2012-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1471</id><created>2012-06-07</created><authors><author><keyname>Ma</keyname><forenames>Ruina</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Chablat</keyname><forenames>Damien</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Bennis</keyname><forenames>Fouad</forenames><affiliation>IRCCyN</affiliation></author></authors><title>A new approach to muscle fatigue evaluation for Push/Pull task</title><categories>cs.RO</categories><comments>19th CISM-IFToMM Symposium on Robot Design, Dynamics, and Control,
  Paris : France (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pushing/Pulling tasks is an important part of work in many industries.
Usually, most researchers study the Push/Pull tasks by analyzing different
posture conditions, force requirements, velocity factors, etc. However few
studies have reported the effects of fatigue. Fatigue caused by physical
loading is one of the main reasons responsible for MusculoSkeletal Disorders
(MSD). In this paper, muscle groups of articulation is considered and from
joint level a new approach is proposed for muscle fatigue evaluation in the
arms Push/Pull operations. The objective of this work is to predict the muscle
fatigue situation in the Push/Pull tasks in order to reduce the probability of
MSD problems for workers. A case study is presented to use this new approach
for analyzing arm fatigue in Pushing/Pulling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1482</identifier>
 <datestamp>2012-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1482</id><created>2012-06-07</created><authors><author><keyname>Nasim</keyname><forenames>Robayet</forenames><affiliation>Assistant professor, Faculty of Science, Engineering &amp; Technology, University of Science &amp; Technology Chittagong</affiliation></author></authors><title>Security Threats Analysis in Bluetooth-Enabled Mobile Devices</title><categories>cs.CR</categories><comments>16 pages</comments><journal-ref>International Journal of Network Security &amp; Its Applications
  (IJNSA), Volume: 4, Number: 3, May 2012</journal-ref><doi>10.5121/ijnsa.2012.4303</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exponential growth of the volume of Bluetooth-enabled devices indicates that
it has become a popular way of wireless interconnections for exchanging
information. The main goal of this paper is to analyze the most critical
Bluetooth attacks in real scenarios. In order to find out the major
vulnerabilities in modern Bluetooth-enabled mobile devices several attacks have
performed successfully such as- Surveillance, Obfuscation, Sniffing,
Unauthorized Direct Data Access (UDDA) and Man-in-the-Middle Attack (MITM). To
perform the testbed, several devices are used such as mobile phones, laptops,
notebooks, wireless headsets, etc. and all the tests are carried out by
pen-testing software like hcittml, braudit, spoafiooph, hridump, bluesnarfer,
bluebugger and carwhisperer.
  KEYWORDS: Bluetooth, Security, Surveillance, Obfuscation, Sniffing, Denial of
service, Man-in-the-middle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1492</identifier>
 <datestamp>2012-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1492</id><created>2012-06-07</created><updated>2012-07-04</updated><authors><author><keyname>Singer</keyname><forenames>Georg</forenames></author><author><keyname>Norbisrath</keyname><forenames>Ulrich</forenames></author><author><keyname>Lewandowski</keyname><forenames>Dirk</forenames></author></authors><title>Ordinary Search Engine Users Carrying Out Complex Search Tasks</title><categories>cs.IR</categories><comments>60 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web search engines have become the dominant tools for finding information on
the Internet. Due to their popularity, users apply them to a wide range of
search needs, from simple look-ups to rather complex information tasks. This
paper presents the results of a study to investigate the characteristics of
these complex information needs in the context of Web search engines. The aim
of the study is to find out more about (1) what makes complex search tasks
distinct from simple tasks and if it is possible to find simple measures for
describing their complexity, (2) if search success for a task can be predicted
by means of unique measures, and (3) if successful searchers show a different
behavior than unsuccessful ones. The study includes 60 people who carried out a
set of 12 search tasks with current commercial search engines. Their behavior
was logged with the Search-Logger tool. The results confirm that complex tasks
show significantly different characteristics than simple tasks. Yet it seems to
be difficult to distinguish successful from unsuccessful search behaviors. Good
searchers can be differentiated from bad searchers by means of measurable
parameters. The implications of these findings for search engine vendors are
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1494</identifier>
 <datestamp>2012-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1494</id><created>2012-06-07</created><authors><author><keyname>Singer</keyname><forenames>Georg</forenames></author><author><keyname>Norbisrath</keyname><forenames>Ulrich</forenames></author><author><keyname>Lewandowski</keyname><forenames>Dirk</forenames></author></authors><title>Impact of Gender and Age on performing Search Tasks Online</title><categories>cs.IR cs.HC</categories><comments>10 pages</comments><doi>10.1524/9783486718782.23</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  More and more people use the Internet to work on duties of their daily work
routine. To find the right information online, Web search engines are the tools
of their choice. Apart from finding facts, people use Web search engines to
also execute rather complex and time consuming search tasks. So far search
engines follow the one-for-all approach to serve its users and little is known
about the impact of gender and age on people's Web search behavior. In this
article we present a study that examines (1) how female and male web users
carry out simple and complex search tasks and what are the differences between
the two user groups, and (2) how the age of the users impacts their search
performance. The laboratory study was done with 56 ordinary people each
carrying out 12 search tasks. Our findings confirm that age impacts behavior
and search performance significantly, while gender influences were smaller than
expected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1515</identifier>
 <datestamp>2012-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1515</id><created>2012-06-07</created><authors><author><keyname>Abdullah</keyname><forenames>Manal</forenames></author><author><keyname>Wazzan</keyname><forenames>Majda</forenames></author><author><keyname>Bo-saeed</keyname><forenames>Sahar</forenames></author></authors><title>Optimizing Face Recognition Using PCA</title><categories>cs.CV</categories><comments>9 pages</comments><journal-ref>International Journal of Artificial Intelligence &amp; Applications
  (IJAIA), Vol.3, No.2, March 2012, 23-31</journal-ref><doi>10.5121/ijaia.2012.3203</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Principle Component Analysis PCA is a classical feature extraction and data
representation technique widely used in pattern recognition. It is one of the
most successful techniques in face recognition. But it has drawback of high
computational especially for big size database. This paper conducts a study to
optimize the time complexity of PCA (eigenfaces) that does not affects the
recognition performance. The authors minimize the participated eigenvectors
which consequently decreases the computational time. A comparison is done to
compare the differences between the recognition time in the original algorithm
and in the enhanced algorithm. The performance of the original and the enhanced
proposed algorithm is tested on face94 face database. Experimental results show
that the recognition time is reduced by 35% by applying our proposed enhanced
algorithm. DET Curves are used to illustrate the experimental results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1518</identifier>
 <datestamp>2012-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1518</id><created>2012-06-07</created><authors><author><keyname>Abdullah</keyname><forenames>Manal A.</forenames></author><author><keyname>Al-Harigy</keyname><forenames>Lulwah M.</forenames></author><author><keyname>Al-Fraidi</keyname><forenames>Hanadi H.</forenames></author></authors><title>Off-Line Arabic Handwriting Character Recognition Using Word
  Segmentation</title><categories>cs.CV</categories><comments>5 pages; Journal of Computing, Volume 4, Issue 3, March 2012, ISSN
  2151-9617</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ultimate aim of handwriting recognition is to make computers able to read
and/or authenticate human written texts, with a performance comparable to or
even better than that of humans. Reading means that the computer is given a
piece of handwriting and it provides the electronic transcription of that (e.g.
in ASCII format). Two types of handwriting: on-line and offline. The most
important purpose of off-line handwriting recognition is in protection systems
and authentication. Arabic Handwriting scripts are much more complicated in
comparison to Latin scripts. This paper introduces a simple and novel
methodology to authenticate Arabic handwriting characters. Reaching our aim, we
built our own character database. The research methodology depends on two
stages: The first is character extraction where preprocessing the word and then
apply segmentation process to obtain the character. The second is the character
recognition by matching the characters comprising the word with the letters in
the database. Our results ensure character recognition with 81%. We eliminate
FAR by using similarity percent between 45-55%. Our research is coded using
MATLAB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1522</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1522</id><created>2012-06-07</created><updated>2013-10-21</updated><authors><author><keyname>Pandurangan</keyname><forenames>Gopal</forenames></author><author><keyname>Robinson</keyname><forenames>Peter</forenames></author><author><keyname>Trehan</keyname><forenames>Amitabh</forenames></author></authors><title>DEX: Self-healing Expanders</title><categories>cs.DC cs.DS</categories><msc-class>68W15</msc-class><acm-class>C.2.4; E.1; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a fully-distributed self-healing algorithm DEX, that maintains a
constant degree expander network in a dynamic setting. To the best of our
knowledge, our algorithm provides the first efficient distributed construction
of expanders --- whose expansion properties hold {\em deterministically} ---
that works even under an all-powerful adaptive adversary that controls the
dynamic changes to the network (the adversary has unlimited computational power
and knowledge of the entire network state, can decide which nodes join and
leave and at what time, and knows the past random choices made by the
algorithm). Previous distributed expander constructions typically provide only
{\em probabilistic} guarantees on the network expansion which {\em rapidly
degrade} in a dynamic setting; in particular, the expansion properties can
degrade even more rapidly under {\em adversarial} insertions and deletions.
  Our algorithm provides efficient maintenance and incurs a low overhead per
insertion/deletion by an adaptive adversary: only $O(\log n)$ rounds and
$O(\log n)$ messages are needed with high probability ($n$ is the number of
nodes currently in the network). The algorithm requires only a constant number
of topology changes. Moreover, our algorithm allows for an efficient
implementation and maintenance of a distributed hash table (DHT) on top of DEX,
with only a constant additional overhead.
  Our results are a step towards implementing efficient self-healing networks
that have \emph{guaranteed} properties (constant bounded degree and expansion)
despite dynamic changes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1529</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1529</id><created>2012-06-07</created><updated>2013-04-10</updated><authors><author><keyname>Kyrillidis</keyname><forenames>Anastasios</forenames></author><author><keyname>Becker</keyname><forenames>Stephen</forenames></author><author><keyname>and</keyname><forenames>Volkan Cevher</forenames></author><author><keyname>Koch</keyname><forenames>Christoph</forenames></author></authors><title>Sparse projections onto the simplex</title><categories>cs.LG stat.ML</categories><comments>9 Pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most learning methods with rank or sparsity constraints use convex
relaxations, which lead to optimization with the nuclear norm or the
$\ell_1$-norm. However, several important learning applications cannot benefit
from this approach as they feature these convex norms as constraints in
addition to the non-convex rank and sparsity constraints. In this setting, we
derive efficient sparse projections onto the simplex and its extension, and
illustrate how to use them to solve high-dimensional learning problems in
quantum tomography, sparse density estimation and portfolio selection with
non-convex constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1530</identifier>
 <datestamp>2013-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1530</id><created>2012-06-07</created><updated>2013-10-29</updated><authors><author><keyname>Landsberg</keyname><forenames>J. M.</forenames></author></authors><title>New lower bounds for the rank of matrix multiplication</title><categories>cs.CC math.AG math.RT</categories><comments>Completely rewritten, mistake in error term in previous version
  corrected. To appear in SICOMP</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rank of the matrix multiplication operator for nxn matrices is one of the
most studied quantities in algebraic complexity theory. I prove that the rank
is at least n^2-o(n^2). More precisely, for any integer p\leq n -1, the rank is
at least (3- 1/(p+1))n^2-(1+2p\binom{2p}{p-1})n. The previous lower bound, due
to Blaser, was 5n^2/2-3n (the case p=1).
  The new bounds improve Blaser's bound for all n&gt;84. I also prove lower bounds
for rectangular matrices significantly better than the the previous bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1531</identifier>
 <datestamp>2012-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1531</id><created>2012-06-07</created><authors><author><keyname>Zhao</keyname><forenames>Jun</forenames></author><author><keyname>Ya&#x11f;an</keyname><forenames>Osman</forenames></author><author><keyname>Gligor</keyname><forenames>Virgil</forenames></author></authors><title>k-Connectivity in Secure Wireless Sensor Networks with Physical Link
  Constraints - The On/Off Channel Model</title><categories>cs.IT math.CO math.IT math.PR</categories><comments>28 pages. Submitted to IEEE Transactions on Information Theory, May
  2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random key predistribution scheme of Eschenauer and Gligor (EG) is a typical
solution for ensuring secure communications in a wireless sensor network (WSN).
Connectivity of the WSNs under this scheme has received much interest over the
last decade, and most of the existing work is based on the assumption of
unconstrained sensor-to-sensor communications. In this paper, we study the
k-connectivity of WSNs under the EG scheme with physical link constraints;
k-connectivity is defined as the property that the network remains connected
despite the failure of any (k - 1) sensors. We use a simple communication
model, where unreliable wireless links are modeled as independent on/off
channels, and derive zero-one laws for the properties that i) the WSN is
k-connected, and ii) each sensor is connected to at least k other sensors.
These zero-one laws improve the previous results by Rybarczyk on the
k-connectivity under a fully connected communication model. Moreover, under the
on/off channel model, we provide a stronger form of the zero-one law for the
1-connectivity as compared to that given by Ya\u{g}an. We also discuss the
applicability of our results in a different network application, namely in a
large-scale, distributed publish-subscribe service for online social networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1534</identifier>
 <datestamp>2012-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1534</id><created>2012-06-07</created><authors><author><keyname>Sumathi</keyname><forenames>G.</forenames></author><author><keyname>Raju</keyname><forenames>R.</forenames></author></authors><title>Software Aging Analysis of Web Server Using Neural Networks</title><categories>cs.AI</categories><comments>11 pages, 8 figures, 1 table; International Journal of Artificial
  Intelligence &amp; Applications (IJAIA), Vol.3, No.3, May 2012</comments><msc-class>68</msc-class><doi>10.5121/ijaia.2012.3302</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software aging is a phenomenon that refers to progressive performance
degradation or transient failures or even crashes in long running software
systems such as web servers. It mainly occurs due to the deterioration of
operating system resource, fragmentation and numerical error accumulation. A
primitive method to fight against software aging is software rejuvenation.
Software rejuvenation is a proactive fault management technique aimed at
cleaning up the system internal state to prevent the occurrence of more severe
crash failures in the future. It involves occasionally stopping the running
software, cleaning its internal state and restarting it. An optimized schedule
for performing the software rejuvenation has to be derived in advance because a
long running application could not be put down now and then as it may lead to
waste of cost. This paper proposes a method to derive an accurate and optimized
schedule for rejuvenation of a web server (Apache) by using Radial Basis
Function (RBF) based Feed Forward Neural Network, a variant of Artificial
Neural Networks (ANN). Aging indicators are obtained through experimental setup
involving Apache web server and clients, which acts as input to the neural
network model. This method is better than existing ones because usage of RBF
leads to better accuracy and speed in convergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1552</identifier>
 <datestamp>2012-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1552</id><created>2012-06-07</created><authors><author><keyname>Vasanth</keyname><forenames>K.</forenames></author><author><keyname>Kumar</keyname><forenames>V. Jawahar Senthil</forenames></author></authors><title>Performance Analysis of Unsymmetrical trimmed median as detector on
  image noises and its Fpga implementation</title><categories>cs.CV</categories><comments>20 pages,17 images</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This Paper Analyze the performance of Unsymmetrical trimmed median, which is
used as detector for the detection of impulse noise, Gaussian noise and mixed
noise is proposed. The proposed algorithm uses a fixed 3x3 window for the
increasing noise densities. The pixels in the current window are arranged in
sorting order using a improved snake like sorting algorithm with reduced
comparator. The processed pixel is checked for the occurrence of outliers, if
the absolute difference between processed pixels is greater than fixed
threshold. Under high noise densities the processed pixel is also noisy hence
the median is checked using the above procedure. if found true then the pixel
is considered as noisy hence the corrupted pixel is replaced by the median of
the current processing window. If median is also noisy then replace the
corrupted pixel with unsymmetrical trimmed median else if the pixel is termed
uncorrupted and left unaltered. The proposed algorithm (PA) is tested on
varying detail images for various noises. The proposed algorithm effectively
removes the high density fixed value impulse noise, low density random valued
impulse noise, low density Gaussian noise and lower proportion of mixed noise.
The proposed algorithm is targeted on Xc3e5000-5fg900 FPGA using Xilinx 7.1
compiler version which requires less number of slices, optimum speed and low
power when compared to the other median finding architectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1553</identifier>
 <datestamp>2012-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1553</id><created>2012-06-07</created><authors><author><keyname>Lohr</keyname><forenames>Andrew</forenames></author></authors><title>Tight Lower Bounds for Unequal Division</title><categories>cs.GT</categories><msc-class>68Q17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Alice and Bob want to cut a cake; however, in contrast to the usual problems
of fair division, they want to cut it unfairly. More precisely, they want to
cut it in ratio $(a:b)$. (We can assume gcd(a,b)=1.) Let f(a,b) be the number
of cuts will this take (assuming both act in their own self interest). It is
known that f(a,b) \le \ceil{lg(a+b)}. We show that (1) for all a,b, f(a,b) \ge
lg(lg(a+b)) + (2) for an infinite number of (a,b), f(a,b) \le 1+lg(lg(a+b).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1557</identifier>
 <datestamp>2012-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1557</id><created>2012-06-07</created><authors><author><keyname>Gholap</keyname><forenames>Jay</forenames></author><author><keyname>Ingole</keyname><forenames>Anurag</forenames></author><author><keyname>Gohil</keyname><forenames>Jayesh</forenames></author><author><keyname>Gargade</keyname><forenames>Shailesh</forenames></author><author><keyname>Attar</keyname><forenames>Vahida</forenames></author></authors><title>Soil Data Analysis Using Classification Techniques and Soil Attribute
  Prediction</title><categories>cs.AI stat.AP stat.ML</categories><comments>4 pages, published in International Journal of Computer Science
  Issues, Volume 9, Issue 3</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Agricultural research has been profited by technical advances such as
automation, data mining. Today, data mining is used in a vast areas and many
off-the-shelf data mining system products and domain specific data mining
application soft wares are available, but data mining in agricultural soil
datasets is a relatively a young research field. The large amounts of data that
are nowadays virtually harvested along with the crops have to be analyzed and
should be used to their full extent. This research aims at analysis of soil
dataset using data mining techniques. It focuses on classification of soil
using various algorithms available. Another important purpose is to predict
untested attributes using regression technique, and implementation of automated
soil sample classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1566</identifier>
 <datestamp>2012-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1566</id><created>2012-06-07</created><authors><author><keyname>Santiago</keyname><forenames>Douglas F. G.</forenames></author><author><keyname>Portugal</keyname><forenames>Renato</forenames></author><author><keyname>Melo</keyname><forenames>Nolmar</forenames></author></authors><title>Non-Pauli Observables for CWS Codes</title><categories>quant-ph cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  It is known that nonadditive quantum codes are more optimal for error
correction when compared to stabilizer codes. The class of codeword stabilized
codes (CWS) provides tools to obtain new nonadditive quantum codes by reducing
the problem to finding nonlinear classical codes. In this work, we establish
some results on the kind of non-Pauli operators that can be used as decoding
observables for CWS codes and describe a procedure to obtain these observables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1567</identifier>
 <datestamp>2012-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1567</id><created>2012-06-07</created><authors><author><keyname>Paul</keyname><forenames>Rourab</forenames></author><author><keyname>Sau</keyname><forenames>Suman</forenames></author><author><keyname>Chakrabarti</keyname><forenames>Amlan</forenames></author></authors><title>Architecture for real time continuous sorting on large width data volume
  for fpga based applications</title><categories>cs.AR</categories><comments>5 pages,RASTM,2011 INDORE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In engineering applications sorting is an important and widely studied
problem where execution speed and resources used for computation are of extreme
importance, especially if we think about real time data processing. Most of the
traditional sorting techniques compute the process after receiving all of the
data and hence the process needs large amount of resources for data storage.
So, suitable design strategy needs to be adopted if we wish to sort a large
amount of data in real time, which essential means higher speed of process
execution and utilization of fewer resources in most of the cases. This paper
proposes a single chip scalable architecture based on Field Programmable Gate
Array(FPGA), for a modified counting sort algorithm where data acquisition and
sorting is being done in real time scenario. Our design promises to work
efficiently, where data can be accepted in the run time scenario without any
need of prior storage of data and also the execution speed of our algorithm is
invariant to the length of the data stream. The proposed design is implemented
and verified on Spartan 3E(XC3S500E-FG320) FPGA system. The results prove that
our design is better in terms of some of the design parameters compared to the
existing research works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1579</identifier>
 <datestamp>2012-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1579</id><created>2012-06-07</created><updated>2012-07-05</updated><authors><author><keyname>Reihaneh</keyname><forenames>Mohammad</forenames></author><author><keyname>Karapetyan</keyname><forenames>Daniel</forenames></author></authors><title>An Efficient Hybrid Ant Colony System for the Generalized Traveling
  Salesman Problem</title><categories>cs.AI math.CO math.OC</categories><comments>7 pages</comments><journal-ref>Algorithmic Operations Research Vol. 7 (2012) 21-28</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Generalized Traveling Salesman Problem (GTSP) is an extension of the
well-known Traveling Salesman Problem (TSP), where the node set is partitioned
into clusters, and the objective is to find the shortest cycle visiting each
cluster exactly once. In this paper, we present a new hybrid Ant Colony System
(ACS) algorithm for the symmetric GTSP. The proposed algorithm is a
modification of a simple ACS for the TSP improved by an efficient GTSP-specific
local search procedure. Our extensive computational experiments show that the
use of the local search procedure dramatically improves the performance of the
ACS algorithm, making it one of the most successful GTSP metaheuristics to
date.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1587</identifier>
 <datestamp>2012-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1587</id><created>2012-06-07</created><authors><author><keyname>Lahby</keyname><forenames>Mohamed</forenames></author><author><keyname>Cherkaoui</keyname><forenames>Leghris</forenames></author><author><keyname>Adib</keyname><forenames>Abdellah</forenames></author></authors><title>Network Selection Decision Based on Handover History in Heterogeneous
  Wireless Networks</title><categories>cs.NI</categories><comments>5 pages, 10 figures; ISSN 2047-3338. arXiv admin note: substantial
  text overlap with arXiv:1204.1383</comments><journal-ref>International Journal of Computer Science and Telecommunications
  Volume 3, Issue 2, pp. 21-25, February 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, the mobile devices are equipped with several wireless
interfaces in heterogeneous environments which integrate a multitude of radio
access technologies (RAT's). The evolution of these technologies will allow the
users to benefit simultaneously from these RAT's. However, the most important
issue is how to choose the most suitable access network for mobile's user which
can be used as long as possible for communication. To achieve this issue, this
paper proposes a new approach for network selection decision based on Saaty's
Fuzzy Analytical Hierarchy Process (FAHP) and the Technique for Order
Preference by Similarity to an Ideal Solution (TOPSIS). The FAHP method is used
to determine a weight for each criterion, and the TOPSIS method is applied to
rank the alternatives. Simulation results are presented to illustrate the
effectiveness of our new approach for network selection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1611</identifier>
 <datestamp>2012-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1611</id><created>2012-06-07</created><authors><author><keyname>Kora</keyname><forenames>Ahmed Dooguy</forenames></author><author><keyname>Soidridine</keyname><forenames>Moussa Moindze</forenames></author></authors><title>Nagios Based Enhanced IT Management System</title><categories>cs.NI</categories><comments>ISSN : 0975-5462; (IJEST), Vol. 4 No.03 March 2012</comments><journal-ref>International Journal of Engineering Science and Technology
  (IJEST), Vol. 4 No.03 March 2012, PP: 1199-1207</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The integrated management of multi-providers equipments is a key asset for
telecommunication operators or service providers when selecting the appropriate
network and service management platform for their network. In this paper, we
present an open and adaptable platform that support fault and configuration
management for next-generation networks. This platform Named Nagios based -
information technology management system (NB-ITMS) leverage of the well-known
Nagios platform to implement new capabilities. Offering additional features
applications and management functions enable easy and low cost management of
advanced services and networks technologies. The performance of our platform
has been compared to existing off-the-shell platforms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1615</identifier>
 <datestamp>2012-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1615</id><created>2012-06-07</created><authors><author><keyname>Omri</keyname><forenames>Mohamed Nazih</forenames></author></authors><title>Objects and Goals Extraction from Semantic Networks : Applications of
  Fuzzy SetS Theory</title><categories>cs.IR</categories><comments>arXiv admin note: text overlap with arXiv:1206.1042, arXiv:1206.0925</comments><journal-ref>Conf\'erence Internationale : Science \'Electroniques,
  Technologies de l'Information et des T\'el\'ecommunications(SETIT), p.
  275-282, Mahdia, Tunisie, 2003</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a short survey of fuzzy and Semantic approaches to
Knowledge Extraction. The goal of such approaches is to define flexible
Knowledge Extraction Systems able to deal with the inherent vagueness and
uncertainty of the Extraction process. In this survey we address if and how
some approaches met their goal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1618</identifier>
 <datestamp>2012-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1618</id><created>2012-06-07</created><authors><author><keyname>Kora</keyname><forenames>Ahmed Dooguy</forenames></author></authors><title>DWDM/OOC and large spectrum sources performance in broadband access
  network</title><categories>cs.NI</categories><comments>(IJDPS) Vol.3, No.3, May 2012, International Journal of Distributed
  and Parallel Systems (IJDPS) Vol.3, No.3, May 2012</comments><doi>10.5121/ijdps.2012.3317</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents a performance evaluation based on elaborated analytical
expressions of error probability for broadband access network in the case of a
combined technique of dense wavelength division multiplexing (DWDM) and one
dimensional optical orthogonal codes (1D-OOC). Optical sources with relatively
large spectrum has been considered and simulated. Besides the Multiple Access
Interference (MAI) at the receiver due to the access method which is optical
code division multiple access (OCDMA), the emitted radiation of these sources
in a dense WDM communication link introduces additional interference.
Conventional correlation receiver (CCR) and parallel interference cancellation
(PIC) receiver limitations are discussed. This paper has investigated the kind
of optical sources with large spectrum bandwidth which could be accepted for a
targeted bit error rate (BER) and given number of users in broadband access
network supporting DWDM with optical orthogonal codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1623</identifier>
 <datestamp>2014-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1623</id><created>2012-06-07</created><updated>2014-03-17</updated><authors><author><keyname>Lee</keyname><forenames>Jason D.</forenames></author><author><keyname>Sun</keyname><forenames>Yuekai</forenames></author><author><keyname>Saunders</keyname><forenames>Michael A.</forenames></author></authors><title>Proximal Newton-type methods for minimizing composite functions</title><categories>stat.ML cs.DS cs.LG cs.NA math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalize Newton-type methods for minimizing smooth functions to handle a
sum of two convex functions: a smooth function and a nonsmooth function with a
simple proximal mapping. We show that the resulting proximal Newton-type
methods inherit the desirable convergence behavior of Newton-type methods for
minimizing smooth functions, even when search directions are computed
inexactly. Many popular methods tailored to problems arising in bioinformatics,
signal processing, and statistical learning are special cases of proximal
Newton-type methods, and our analysis yields new convergence results for some
of these methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1624</identifier>
 <datestamp>2012-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1624</id><created>2012-06-07</created><authors><author><keyname>Omri</keyname><forenames>Mohamed nazih</forenames></author><author><keyname>Chouigui</keyname><forenames>Noureddine</forenames></author></authors><title>Measure of Similarity between Fuzzy Concepts for Optimization of Fuzzy
  Semantic Nets</title><categories>cs.IR</categories><comments>14th International Conference on Systems Science. Wroclaw, Poland,
  2001</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a method to measure the similarity between different
fuzzy concepts in order to optimize Semantic networks. The problem approached
is the minimization of the time of research and identification of user's
Objects and Goals. Indeed, it concerns to determine to each instant the
totality of Objects (respectively Goals) among which one can identify rapidly
the most satisfactory for the user's Object and Goal. Alone Objects and most
similar Goals to Objects and researched Goals of the viewpoint of attribute
values will be processed, what will avoid the analysis of all Objects and
system Goals far of needs of the user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1625</identifier>
 <datestamp>2012-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1625</id><created>2012-06-07</created><authors><author><keyname>Tai</keyname><forenames>Vin Cent</forenames></author><author><keyname>See</keyname><forenames>Phen Chiak</forenames></author><author><keyname>Molinas</keyname><forenames>Marta</forenames></author><author><keyname>Uhlen</keyname><forenames>Kjetil</forenames></author></authors><title>Performance assessment of two active power filter control strategies in
  the presence of non-stationary currents</title><categories>math.OC cs.SY</categories><comments>12 pages, 5 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper describes an active power filter (APF) control strategy, which
eliminates harmonics and compensates reactive power in a three-phase four-wire
power system supplying non-linear unbalanced loads in the presence of
non-linear non-stationary currents. Empirical Mode Decomposition (EMD)
technique developed as part of the Hilbert-Huang Transform (HHT) is used to
singulate the harmonics and non-linear non stationary disturbances from the
load currents. The control strategy for the APF is formulated by hybridizing
the so called modified p-q theory with the EMD algorithm. A four-leg
split-capacitor converter controlled by hysteresis band current controller is
used as an APF. The results obtained are compared with those obtained with the
conventional modified p-q theory, which does not possess current harmonics or
distortions separation strategy, to validate its performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1630</identifier>
 <datestamp>2012-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1630</id><created>2012-06-07</created><authors><author><keyname>Balas</keyname><forenames>Egon</forenames></author><author><keyname>Qualizza</keyname><forenames>Andrea</forenames></author></authors><title>Intersection cuts from multiple rows: a disjunctive programming approach</title><categories>math.CO cs.DM math.OC</categories><comments>38 pages, 6 figures</comments><msc-class>90C10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the issue of generating cutting planes for mixed integer programs
from multiple rows of the simplex tableau with the tools of disjunctive
programming. A cut from q rows of the simplex tableau is an intersection cuts
from a q-dimensional parametric cross-polytope, which can also be viewed as a
disjunctive cut from a 2q-term disjunction. We define the disjunctive hull of
the q-row problem, describe its relation to the integer hull, and show how to
generate its facets. For the case of binary basic variables, we derive cuts
from the stronger disjunctions whose terms are equations. We give cut
strengthening procedures using the integrality of the nonbasic variables for
both the integer and the binary case. Finally, we discuss some computational
experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1633</identifier>
 <datestamp>2012-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1633</id><created>2012-06-07</created><authors><author><keyname>Qualizza</keyname><forenames>Andrea</forenames></author><author><keyname>Belotti</keyname><forenames>Pietro</forenames></author><author><keyname>Margot</keyname><forenames>Francois</forenames></author></authors><title>Linear Programming Relaxations of Quadratically Constrained Quadratic
  Programs</title><categories>math.CO cs.DM math.OC</categories><comments>Published in IMA Volumes in Mathematics and its Applications, 2012,
  Volume 154</comments><msc-class>90C57</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the use of linear programming tools for solving semidefinite
programming relaxations of quadratically constrained quadratic problems.
Classes of valid linear inequalities are presented, including sparse PSD cuts,
and principal minors PSD cuts. Computational results based on instances from
the literature are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1634</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1634</id><created>2012-06-07</created><updated>2015-03-24</updated><authors><author><keyname>Ouyang</keyname><forenames>Wenzhuo</forenames></author><author><keyname>Eryilmaz</keyname><forenames>Atilla</forenames></author><author><keyname>Shroff</keyname><forenames>Ness B.</forenames></author></authors><title>Low-complexity Optimal Scheduling over Correlated Fading Channels with
  ARQ Feedback</title><categories>cs.NI</categories><comments>A shorter version of this paper appeared in IEEE WiOpt 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the downlink scheduling problem under Markovian ON/OFF fading
channels, where the instantaneous channel state information is not directly
accessible, but is revealed via ARQ-type feedback. The scheduler can exploit
the temporal correlation/channel memory inherent in the Markovian channels to
improve network performance. However, designing low-complexity and
throughput-optimal algorithms under temporal correlation is a challenging
problem. In this paper, we find that under an average number of transmissions
constraint, a low-complexity index policy is throughput-optimal. The policy
uses Whittle's index value, which was previously used to capture opportunistic
scheduling under temporally correlated channels. Our results build on the
interesting finding that, under the intricate queue length and channel memory
evolutions, the importance of scheduling a user is captured by a simple
multiplication of its queue length and Whittle's index value. The proposed
queue-based index policy has provably low complexity. Numerical results show
that significant throughput gains can be realized by exploiting the channel
memory using the proposed low-complexity policy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1653</identifier>
 <datestamp>2013-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1653</id><created>2012-06-07</created><updated>2013-06-05</updated><authors><author><keyname>Braghin</keyname><forenames>Stefano</forenames></author><author><keyname>Tan</keyname><forenames>Jackson</forenames></author><author><keyname>Sharma</keyname><forenames>Rajesh</forenames></author><author><keyname>Datta</keyname><forenames>Anwitaman</forenames></author></authors><title>PriSM: A Private Social Mesh for Leveraging Social Networking at
  Workplace</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we describe the PriSM framework for decentralized deployment of
a federation of autonomous social networks (ASN). The individual ASNs are
centrally managed by organizations according to their institutional needs,
while cross-ASN interactions are facilitated subject to security and
confidentiality requirements specified by administrators and users of the ASNs.
Such decentralized deployment, possibly either on private or public clouds,
provides control and ownership of information/flow to individual organizations.
Lack of such complete control (if third party online social networking services
were to be used) has so far been a great barrier in taking full advantage of
the novel communication mechanisms at workplace that have however become
commonplace for personal usage with the advent of Web 2.0 platforms and online
social networks. PriSM provides a practical solution for organizations to
harness the advantages of online social networking both in
intra/inter-organizational settings without sacrificing autonomy, security and
confidentiality needs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1665</identifier>
 <datestamp>2012-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1665</id><created>2012-06-08</created><authors><author><keyname>Sharma</keyname><forenames>Sarvesh Kumar</forenames></author></authors><title>An Approach In Optimization Of AD-Hoc Routing Algorithms</title><categories>cs.NI cs.IT math.IT</categories><comments>10 pages, 2 figures, International Journal of Distributed and
  Parallel Systems (IJDPS) Vol.3, No.3, May 2012, ISSN: 0976 - 9757 [Online],
  2229 - 3957 [Print]</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper different optimization of Ad-hoc routing algorithm is surveyed
and a new method using training based optimization algorithm for reducing the
complexity of routing algorithms is suggested. A binary matrix is assigned to
each node in the network and gets updated after each data transfer using the
protocols. The use of optimization algorithm in routing algorithm can reduce
the complexity of routing to the least amount possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1669</identifier>
 <datestamp>2012-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1669</id><created>2012-06-08</created><authors><author><keyname>Tabassum</keyname><forenames>Kahkashan</forenames></author><author><keyname>Sultana</keyname><forenames>Asia</forenames></author><author><keyname>Damodaram</keyname><forenames>A.</forenames></author></authors><title>Flexible Data Dissemination Strategy for Effective Cache Consistency in
  Mobile Wireless Communication Networks</title><categories>cs.NI</categories><comments>14 pages and 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In mobile wireless communication network, caching data items at the mobile
clients is important to reduce the data access delay. However, efficient cache
invalidation strategies are used to ensure the consistency between the data in
the cache of mobile clients and at the database server. Servers use
invalidation reports (IRs) to inform the mobile clients about data item
updates. This paper proposes and implements a multicast based strategy to
maintain cache consistency in mobile environment using AVI as the cache
invalidation scheme. The proposed algorithm is outlined as follows - To resolve
a query, the mobile client searches its cache to check if its data is valid. If
yes, then query is answered, otherwise the client queries the DTA (Dynamic
Transmitting Agent) for latest updates and the query is answered. If DTA
doesn't have the latest updates, it gets it from the server. So, the main idea
here is that DTA will be multicasting updates to the clients and hence the
clients need not uplink to the server individually, thus preserving the network
bandwidth. The scenario of simulation is developed in Java. The results
demonstrate that the traffic generated in the proposed multicast model is
simplified and it also retains cache consistency when compared to the existing
methods that used broadcast strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1672</identifier>
 <datestamp>2012-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1672</id><created>2012-06-08</created><authors><author><keyname>Singh</keyname><forenames>Vikas Vikram</forenames></author><author><keyname>Hemachandra</keyname><forenames>N.</forenames></author></authors><title>A mathematical programming based characterization of Nash equilibria of
  some constrained stochastic games</title><categories>math.OC cs.GT</categories><msc-class>91A10, 91A15, 90C05, 90C20, 90C26</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider two classes of constrained finite state-action stochastic games.
First, we consider a two player nonzero sum single controller constrained
stochastic game with both average and discounted cost criterion. We consider
the same type of constraints as in [1], i.e., player 1 has subscription based
constraints and player 2, who controls the transition probabilities, has
realization based constraints which can also depend on the strategies of player
1. Next, we consider a N -player nonzero sum constrained stochastic game with
independent state processes where each player has average cost criterion as
discussed in [2]. We show that the stationary Nash equilibria of both classes
of constrained games, which exists under strong Slater and irreducibility
conditions [3], [2], has one to one correspondence with global minima of
certain mathematical programs. In the single controller game if the constraints
of player 2 do not depend on the strategies of the player 1, then the
mathematical program reduces to the non-convex quadratic program. In two player
independent state processes stochastic game if the constraints of a player do
not depend on the strategies of another player, then the mathematical program
reduces to a non-convex quadratic program. Computational algorithms for finding
global minima of non-convex quadratic program exist [4], [5] and hence, one can
compute Nash equilibria of these constrained stochastic games. Our results
generalize some existing results for zero sum games [1], [6], [7].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1674</identifier>
 <datestamp>2012-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1674</id><created>2012-06-08</created><authors><author><keyname>Dumka</keyname><forenames>Ankur</forenames></author><author><keyname>Mandoria</keyname><forenames>Hardwari Lal</forenames></author></authors><title>Dynamic MPLS with Feedback</title><categories>cs.NI</categories><comments>6 pages and 3 figures</comments><journal-ref>International Journal of Computer Science, Engineering and
  Applications (IJCSEA) Vol.2, No.2, April 2012, 125-130</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiprotocol Label Switching (MPLS) fasten the speed of packet forwarding by
forwarding the packets based on labels and reduces the use of routing table
look up from all routers to label edge routers(LER), where as the label switch
routers (LSRs) uses Label Distribution Protocol (LDP) or RSVP (Resource
reservation Protocol) for label allocation and Label table for packet
forwarding. Dynamic protocol is implemented which carries a Updates packets for
the details of Label Switch Paths, along with this feedback mechanism is also
introduced which find the shortest path among MPLS network and also feedback is
provided which also help to overcome congestion, this feedback mechanism is on
a hop by hop basis rather than end to end thus providing a more reliable and
much faster and congestion free path for the packets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1678</identifier>
 <datestamp>2012-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1678</id><created>2012-06-08</created><authors><author><keyname>Mageshwari</keyname><forenames>G.</forenames></author><author><keyname>Kanaga</keyname><forenames>E. Grace Mary</forenames></author></authors><title>A Distributed Optimized Patient Scheduling using Partial Information</title><categories>cs.AI</categories><comments>11 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A software agent may be a member of a Multi-Agent System (MAS) which is
collectively performing a range of complex and intelligent tasks. In the
hospital, scheduling decisions are finding difficult to schedule because of the
dynamic changes and distribution. In order to face this problem with dynamic
changes in the hospital, a new method, Distributed Optimized Patient Scheduling
with Grouping (DOPSG) has been proposed. The goal of this method is that there
is no necessity for knowing patient agents information globally. With minimal
information this method works effectively. Scheduling problem can be solved for
multiple departments in the hospital. Patient agents have been scheduled to the
resource agent based on the patient priority to reduce the waiting time of
patient agent and to reduce idle time of resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1687</identifier>
 <datestamp>2012-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1687</id><created>2012-06-08</created><authors><author><keyname>Crafa</keyname><forenames>Silvia</forenames></author></authors><title>Behavioural Types for Actor Systems</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent mainstream programming languages such as Erlang or Scala have renewed
the interest on the Actor model of concurrency. However, the literature on the
static analysis of actor systems is still lacking of mature formal methods. In
this paper we present a minimal actor calculus that takes as primitive the
basic constructs of Scala's Actors API. More precisely, actors can send
asynchronous messages, process received messages according to a pattern
matching mechanism, and dynamically create new actors, whose scope can be
extruded by passing actor names as message parameters. Drawing inspiration from
the linear types and session type theories developed for process calculi, we
put forward a behavioural type system that addresses the key issues of an actor
calculus. We then study a safety property dealing with the determinism of
finite actor com- munication. More precisely, we show that well typed and
balanced actor systems are (i) deadlock-free and (ii) any message will
eventually be handled by the target actor, and dually no actor will
indefinitely wait for an expected message
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1698</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1698</id><created>2012-06-08</created><authors><author><keyname>K&#xe1;polnai</keyname><forenames>Rich&#xe1;rd</forenames></author><author><keyname>Domokos</keyname><forenames>G&#xe1;bor</forenames></author><author><keyname>Szab&#xf3;</keyname><forenames>T&#xed;mea</forenames></author></authors><title>Generating spherical multiquadrangulations by restricted vertex
  splittings and the reducibility of equilibrium classes</title><categories>cs.DM</categories><comments>21 pages, 11 figures and 3 tables</comments><journal-ref>Periodica Polytechnica Electrical Engineering and Computer Science
  56/1 (2012) 11-20</journal-ref><doi>10.3311/PPee.7074</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A quadrangulation is a graph embedded on the sphere such that each face is
bounded by a walk of length 4, parallel edges allowed. All quadrangulations can
be generated by a sequence of graph operations called vertex splitting,
starting from the path P_2 of length 2. We define the degree D of a splitting S
and consider restricted splittings S_{i,j} with i &lt;= D &lt;= j. It is known that
S_{2,3} generate all simple quadrangulations.
  Here we investigate the cases S_{1,2}, S_{1,3}, S_{1,1}, S_{2,2}, S_{3,3}.
First we show that the splittings S_{1,2} are exactly the monotone ones in the
sense that the resulting graph contains the original as a subgraph. Then we
show that they define a set of nontrivial ancestors beyond P_2 and each
quadrangulation has a unique ancestor.
  Our results have a direct geometric interpretation in the context of
mechanical equilibria of convex bodies. The topology of the equilibria
corresponds to a 2-coloured quadrangulation with independent set sizes s, u.
The numbers s, u identify the primary equilibrium class associated with the
body by V\'arkonyi and Domokos. We show that both S_{1,1} and S_{2,2} generate
all primary classes from a finite set of ancestors which is closely related to
their geometric results.
  If, beyond s and u, the full topology of the quadrangulation is considered,
we arrive at the more refined secondary equilibrium classes. As Domokos,
L\'angi and Szab\'o showed recently, one can create the geometric counterparts
of unrestricted splittings to generate all secondary classes. Our results show
that S_{1,2} can only generate a limited range of secondary classes from the
same ancestor. The geometric interpretation of the additional ancestors defined
by monotone splittings shows that minimal polyhedra play a key role in this
process. We also present computational results on the number of secondary
classes and multiquadrangulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1702</identifier>
 <datestamp>2012-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1702</id><created>2012-06-08</created><updated>2012-07-16</updated><authors><author><keyname>Comin</keyname><forenames>Carlo</forenames></author><author><keyname>Bianchi</keyname><forenames>Maria Paola</forenames></author></authors><title>Algebraic Characterization of the Class of Languages recognized by
  Measure Only Quantum Automata</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a model of one-way quantum automaton where only measurement
operations are allowed (MOn-1qfa). We give an algebraic characterization of
LMO, showing that the syntactic monoids of the languages in LMO are exactly the
literal pseudovariety of J-trivial literally idempotent monoids, where J is the
Green's relation determined by two-sided ideals. We also prove that LMO
coincides with the literal variety of literally idempotent piecewise testable
regular languages. This allows us to prove the existence of a polynomial time
algorithm for deciding whether a regular language belongs to LMO.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1714</identifier>
 <datestamp>2013-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1714</id><created>2012-06-08</created><updated>2013-05-30</updated><authors><author><keyname>Neunh&#xf6;ffer</keyname><forenames>Max</forenames></author><author><keyname>Pfeiffer</keyname><forenames>Markus</forenames></author><author><keyname>Ruskuc</keyname><forenames>Nik</forenames></author></authors><title>Deciding Word Problems of Semigroups using Finite State Automata</title><categories>cs.FL math.CO math.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore a natural class of semigroups that have word problem decidable by
finite state automata. Among the main results are invariance of this property
under change of generators, invariance under basic algebraic constructions and
algebraic properties of these semigroups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1722</identifier>
 <datestamp>2012-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1722</id><created>2012-06-08</created><authors><author><keyname>Surekha</keyname><forenames>T. P.</forenames></author><author><keyname>Ananthapadmanabha</keyname><forenames>T.</forenames></author><author><keyname>Puttamadappa</keyname><forenames>C.</forenames></author></authors><title>C-Band VSAT Data Communication System and RF Impairments</title><categories>cs.OH</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with modelling and simulation of VSAT (very small
aperture terminal) data messaging network operating in India at Karnataka with
extended C-band. VSATs in Karnataka of KPTCL use VSATS 6.875-6.9465G Hz uplinks
and 4.650- 4.7215 GHz downlinks. These frequencies are dedicated to fixed
services. The Satellite is Intelsat -3A, the hub has a 7.2 m diameter antenna
and uses 350W or 600W TWTA (Travelling wave Tube Amplifier). The VSAT's are 1.2
m with RF power of 1W or 2W depending on their position in the uplink beam with
data rate of 64 or 128 K bit/s. The performance of the system is analysed by
the error probability called BER (Bit Error Rate) and results are derived from
Earth station to hub and hub to Earth station using satellite Transponder as
the media of communication channel. The Link budgets are developed for a single
one-way satellite link.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1724</identifier>
 <datestamp>2012-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1724</id><created>2012-06-08</created><authors><author><keyname>Omri</keyname><forenames>Mohamed Nazih</forenames></author></authors><title>Softening Fuzzy Knowledge Representation Tool with the Learning of New
  Words in Natural Language</title><categories>cs.AI</categories><journal-ref>International Conference on Artificial and Computational
  Intelligence for Decision, Control and Automation in Engineering and
  Industrial Applications,(ACIDCA'2000). p. 190-194. Monastir, Tunisia,2000</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The approach described here allows using membership function to represent
imprecise and uncertain knowledge by learning in Fuzzy Semantic Networks. This
representation has a great practical interest due to the possibility to realize
on the one hand, the construction of this membership function from a simple
value expressing the degree of interpretation of an Object or a Goal as
compared to an other and on the other hand, the adjustment of the membership
function during the apprenticeship. We show, how to use these membership
functions to represent the interpretation of an Object (respectively of a Goal)
user as compared to an system Object (respectively to a Goal). We also show the
possibility to make decision for each representation of an user Object compared
to a system Object. This decision is taken by determining decision coefficient
calculates according to the nucleus of the membership function of the user
Object.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1728</identifier>
 <datestamp>2012-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1728</id><created>2012-06-08</created><updated>2012-07-02</updated><authors><author><keyname>Greene</keyname><forenames>Derek</forenames></author><author><keyname>Sheridan</keyname><forenames>Gavin</forenames></author><author><keyname>Smyth</keyname><forenames>Barry</forenames></author><author><keyname>Cunningham</keyname><forenames>P&#xe1;draig</forenames></author></authors><title>Aggregating Content and Network Information to Curate Twitter User Lists</title><categories>cs.SI cs.AI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Twitter introduced user lists in late 2009, allowing users to be grouped
according to meaningful topics or themes. Lists have since been adopted by
media outlets as a means of organising content around news stories. Thus the
curation of these lists is important - they should contain the key information
gatekeepers and present a balanced perspective on a story. Here we address this
list curation process from a recommender systems perspective. We propose a
variety of criteria for generating user list recommendations, based on content
analysis, network analysis, and the &quot;crowdsourcing&quot; of existing user lists. We
demonstrate that these types of criteria are often only successful for datasets
with certain characteristics. To resolve this issue, we propose the aggregation
of these different &quot;views&quot; of a news story on Twitter to produce more accurate
user recommendations to support the curation process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1741</identifier>
 <datestamp>2012-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1741</id><created>2012-06-08</created><authors><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author></authors><title>How to analyse percentile impact data meaningfully in bibliometrics: The
  statistical analysis of distributions, percentile rank classes and top-cited
  papers</title><categories>stat.AP cs.DL physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  According to current research in bibliometrics, percentiles (or percentile
rank classes) are the most suitable method for normalising the citation counts
of individual publications in terms of the subject area, the document type and
the publication year. Up to now, bibliometric research has concerned itself
primarily with the calculation of percentiles. This study suggests how
percentiles can be analysed meaningfully for an evaluation study. Publication
sets from four universities are compared with each other to provide sample
data. These suggestions take into account on the one hand the distribution of
percentiles over the publications in the sets (here: universities) and on the
other hand concentrate on the range of publications with the highest citation
impact - that is, the range which is usually of most interest in the evaluation
of scientific performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1748</identifier>
 <datestamp>2012-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1748</id><created>2012-06-08</created><authors><author><keyname>Shah</keyname><forenames>Kinjal</forenames></author><author><keyname>Ghrera</keyname><forenames>Satya Prakash</forenames></author><author><keyname>Thaker</keyname><forenames>Alok</forenames></author></authors><title>A novel approach for security issues in VoIP networks in Virtualization
  with IVR</title><categories>cs.NI</categories><comments>20 pages, 4 figures, 3 tables, 3 snap shots</comments><msc-class>68U35</msc-class><acm-class>D.4.0; C.2.1</acm-class><journal-ref>International Journal of Distributed and Parallel Systems (IJDPS),
  Vol.3, No.3, May 2012, 219-238</journal-ref><doi>10.5121/ijdps.2012.3319</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  VoIP (Voice over Internet Protocol) is a growing technology during last
decade. It provides the audio, video streaming facility on successful
implementation in the network. However, it provides the text transport facility
over the network. Due to implementation of it the cost effective solution, it
can be developed for the intercommunication among the employees of a
prestigious organization. The proposed idea has been implemented on the audio
streaming area of the VoIP technology. In the audio streaming, the security
vulnerabilities are possible on the VoIP server during communication between
two parties. In the proposed model, first the VoIP system has been implemented
with IVR (Interactive Voice Response) as a case study and with the
implementation of the security parameters provided to the asterisk server which
works as a VoIP service provider. The asterisk server has been configured with
different security parameters like VPN server, Firewall iptable rules,
Intrusion Detection and Intrusion Prevention System. Every parameter will be
monitored by the system administrator of the VoIP server along with the MySQL
database. The system admin will get every update related to the attacks on the
server through Mail server attached to the asterisk server. The main beauty of
the proposed system is VoIP server alone is configured as a VoIP server, IVR
provider, Mail Server with IDS and IPS, VPN server, connection with database
server in a single asterisk server inside virtualization environment. The VoIP
system is implemented for a Local Area Network inside the university system
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1751</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1751</id><created>2012-06-08</created><authors><author><keyname>Hansen</keyname><forenames>Kristoffer Arnsfelt</forenames></author><author><keyname>Ibsen-Jensen</keyname><forenames>Rasmus</forenames></author><author><keyname>Podolskii</keyname><forenames>Vladimir V.</forenames></author><author><keyname>Tsigaridas</keyname><forenames>Elias</forenames></author></authors><title>Patience of Matrix Games</title><categories>cs.DM cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For matrix games we study how small nonzero probability must be used in
optimal strategies. We show that for nxn win-lose-draw games (i.e. (-1,0,1)
matrix games) nonzero probabilities smaller than n^{-O(n)} are never needed. We
also construct an explicit nxn win-lose game such that the unique optimal
strategy uses a nonzero probability as small as n^{-Omega(n)}. This is done by
constructing an explicit (-1,1) nonsingular nxn matrix, for which the inverse
has only nonnegative entries and where some of the entries are of value
n^{Omega(n)}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1754</identifier>
 <datestamp>2012-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1754</id><created>2012-06-08</created><updated>2012-07-02</updated><authors><author><keyname>Yuan</keyname><forenames>Shuai</forenames></author><author><keyname>Abidin</keyname><forenames>Ahmad Zainal</forenames></author><author><keyname>Sloan</keyname><forenames>Marc</forenames></author><author><keyname>Wang</keyname><forenames>Jun</forenames></author></authors><title>Internet Advertising: An Interplay among Advertisers, Online Publishers,
  Ad Exchanges and Web Users</title><categories>cs.IR</categories><comments>44 pages, 7 figures, 6 tables. Submitted to Information Processing
  and Management</comments><acm-class>H.3.3; H.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internet advertising is a fast growing business which has proved to be
significantly important in digital economics. It is vitally important for both
web search engines and online content providers and publishers because web
advertising provides them with major sources of revenue. Its presence is
increasingly important for the whole media industry due to the influence of the
Web. For advertisers, it is a smarter alternative to traditional marketing
media such as TVs and newspapers. As the web evolves and data collection
continues, the design of methods for more targeted, interactive, and friendly
advertising may have a major impact on the way our digital economy evolves, and
to aid societal development.
  Towards this goal mathematically well-grounded Computational Advertising
methods are becoming necessary and will continue to develop as a fundamental
tool towards the Web. As a vibrant new discipline, Internet advertising
requires effort from different research domains including Information
Retrieval, Machine Learning, Data Mining and Analytic, Statistics, Economics,
and even Psychology to predict and understand user behaviours. In this paper,
we provide a comprehensive survey on Internet advertising, discussing and
classifying the research issues, identifying the recent technologies, and
suggesting its future directions. To have a comprehensive picture, we first
start with a brief history, introduction, and classification of the industry
and present a schematic view of the new advertising ecosystem. We then
introduce four major participants, namely advertisers, online publishers, ad
exchanges and web users; and through analysing and discussing the major
research problems and existing solutions from their perspectives respectively,
we discover and aggregate the fundamental problems that characterise the
newly-formed research field and capture its potential future prospects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1775</identifier>
 <datestamp>2012-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1775</id><created>2012-06-08</created><authors><author><keyname>Dell</keyname><forenames>Holger</forenames></author><author><keyname>Husfeldt</keyname><forenames>Thore</forenames></author><author><keyname>Marx</keyname><forenames>D&#xe1;niel</forenames></author><author><keyname>Taslaman</keyname><forenames>Nina</forenames></author><author><keyname>W&#xe1;hlen</keyname><forenames>Martin</forenames></author></authors><title>Exponential Time Complexity of the Permanent and the Tutte Polynomial</title><categories>cs.CC cs.DS math.CO</categories><acm-class>F.2.1; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show conditional lower bounds for well-studied #P-hard problems:
  (a) The number of satisfying assignments of a 2-CNF formula with n variables
cannot be counted in time exp(o(n)), and the same is true for computing the
number of all independent sets in an n-vertex graph.
  (b) The permanent of an n x n matrix with entries 0 and 1 cannot be computed
in time exp(o(n)).
  (c) The Tutte polynomial of an n-vertex multigraph cannot be computed in time
exp(o(n)) at most evaluation points (x,y) in the case of multigraphs, and it
cannot be computed in time exp(o(n/polylog n)) in the case of simple graphs.
  Our lower bounds are relative to (variants of) the Exponential Time
Hypothesis (ETH), which says that the satisfiability of n-variable 3-CNF
formulas cannot be decided in time exp(o(n)). We relax this hypothesis by
introducing its counting version #ETH, namely that the satisfying assignments
cannot be counted in time exp(o(n)). In order to use #ETH for our lower bounds,
we transfer the sparsification lemma for d-CNF formulas to the counting
setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1783</identifier>
 <datestamp>2012-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1783</id><created>2012-06-08</created><authors><author><keyname>Barzanti</keyname><forenames>Luca</forenames></author><author><keyname>Mastroleo</keyname><forenames>Marcello</forenames></author></authors><title>An Improved Two-Party Negotiation Over Continues Issues Method Secure
  Against Manipulatory Behavior</title><categories>math.OC cs.GT math.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This contribution focuses on two-party negotiation over continuous issues. We
firstly prove two drawbacks of the jointly Improving Direction Method (IDM),
namely that IDM is not a Strategy-Proof (SP) nor an Information Concealing (IC)
method. Thus we prove that the concurrent lack of these two properties implies
the actual non-efficiency of IDM. Finally we propose a probabilistic method
which is both IC and stochastically SP thus leading to efficient settlements
without being affected by manipulatory behaviors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1794</identifier>
 <datestamp>2012-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1794</id><created>2012-06-08</created><authors><author><keyname>Omri</keyname><forenames>Mohamed Nazih</forenames></author></authors><title>Fuzzy Knowledge Representation, Learning and Optimization with Bayesian
  Analysis in Fuzzy Semantic Networks</title><categories>cs.AI</categories><journal-ref>The 6th International Conference of Neural Information Processing.
  ICONIP'99. p. 345-351. Perth. Austria, 1999</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a method of optimization, based on both Bayesian Analysis
technical and Gallois Lattice, of a Fuzzy Semantic Networks. The technical
System we use learn by interpreting an unknown word using the links created
between this new word and known words. The main link is provided by the context
of the query. When novice's query is confused with an unknown verb (goal)
applied to a known noun denoting either an object in the ideal user's Network
or an object in the user's Network, the system infer that this new verb
corresponds to one of the known goal. With the learning of new words in natural
language as the interpretation, which was produced in agreement with the user,
the system improves its representation scheme at each experiment with a new
user and, in addition, takes advantage of previous discussions with users. The
semantic Net of user objects thus obtained by these kinds of learning is not
always optimal because some relationships between couple of user objects can be
generalized and others suppressed according to values of forces that
characterize them. Indeed, to simplify the obtained Net, we propose to proceed
to an inductive Bayesian analysis, on the Net obtained from Gallois lattice.
The objective of this analysis can be seen as an operation of filtering of the
obtained descriptive graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1800</identifier>
 <datestamp>2012-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1800</id><created>2012-06-08</created><authors><author><keyname>Pitkow</keyname><forenames>Xaq</forenames></author></authors><title>Compressive neural representation of sparse, high-dimensional
  probabilities</title><categories>q-bio.NC cs.IT math.IT</categories><comments>9 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows how sparse, high-dimensional probability distributions could
be represented by neurons with exponential compression. The representation is a
novel application of compressive sensing to sparse probability distributions
rather than to the usual sparse signals. The compressive measurements
correspond to expected values of nonlinear functions of the probabilistically
distributed variables. When these expected values are estimated by sampling,
the quality of the compressed representation is limited only by the quality of
sampling. Since the compression preserves the geometric structure of the space
of sparse probability distributions, probabilistic computation can be performed
in the compressed domain. Interestingly, functions satisfying the requirements
of compressive sensing can be implemented as simple perceptrons. If we use
perceptrons as a simple model of feedforward computation by neurons, these
results show that the mean activity of a relatively small number of neurons can
accurately represent a high-dimensional joint distribution implicitly, even
without accounting for any noise correlations. This comprises a novel
hypothesis for how neurons could encode probabilities in the brain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1803</identifier>
 <datestamp>2012-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1803</id><created>2012-06-08</created><authors><author><keyname>Cannon</keyname><forenames>Sarah</forenames></author><author><keyname>Souvaine</keyname><forenames>Diane L.</forenames></author><author><keyname>Winslow</keyname><forenames>Andrew</forenames></author></authors><title>Hidden Mobile Guards in Simple Polygons</title><categories>cs.CG</categories><comments>An abstract (6-page) version of this paper is to appear in the
  proceedings of CCCG 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider guarding classes of simple polygons using mobile guards (polygon
edges and diagonals) under the constraint that no two guards may see each
other. In contrast to most other art gallery problems, existence is the primary
question: does a specific type of polygon admit some guard set? Types include
simple polygons and the subclasses of orthogonal, monotone, and starshaped
polygons. Additionally, guards may either exclude or include the endpoints
(so-called open and closed guards). We provide a nearly complete set of answers
to existence questions of open and closed edge, diagonal, and mobile guards in
simple, orthogonal, monotone, and starshaped polygons, with some surprising
results. For instance, every monotone or starshaped polygon can be guarded
using hidden open mobile (edge or diagonal) guards, but not necessarily with
hidden open edge or hidden open diagonal guards.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1815</identifier>
 <datestamp>2012-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1815</id><created>2012-06-08</created><authors><author><keyname>Weinsberg</keyname><forenames>Udi</forenames></author><author><keyname>Balachandran</keyname><forenames>Athula</forenames></author><author><keyname>Taft</keyname><forenames>Nina</forenames></author><author><keyname>Iannaccone</keyname><forenames>Gianluca</forenames></author><author><keyname>Sekar</keyname><forenames>Vyas</forenames></author><author><keyname>Seshan</keyname><forenames>Srinivasan</forenames></author></authors><title>CARE: Content Aware Redundancy Elimination for Disaster Communications
  on Damaged Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During a disaster scenario, situational awareness information, such as
location, physical status and images of the surrounding area, is essential for
minimizing loss of life, injury, and property damage. Today's handhelds make it
easy for people to gather data from within the disaster area in many formats,
including text, images and video. Studies show that the extreme anxiety induced
by disasters causes humans to create a substantial amount of repetitive and
redundant content. Transporting this content outside the disaster zone can be
problematic when the network infrastructure is disrupted by the disaster.
  This paper presents the design of a novel architecture called CARE
(Content-Aware Redundancy Elimination) for better utilizing network resources
in disaster-affected regions. Motivated by measurement-driven insights on
redundancy patterns found in real-world disaster area photos, we demonstrate
that CARE can detect the semantic similarity between photos in the networking
layer, thus reducing redundant transfers and improving buffer utilization.
Using DTN simulations, we explore the boundaries of the usefulness of deploying
CARE on a damaged network, and show that CARE can reduce packet delivery times
and drops, and enables 20-40% more unique information to reach the rescue teams
outside the disaster area than when CARE is not deployed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1833</identifier>
 <datestamp>2012-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1833</id><created>2012-06-08</created><authors><author><keyname>van de Stadt</keyname><forenames>Richard</forenames></author></authors><title>CyberChair: A Web-Based Groupware Application to Facilitate the Paper
  Reviewing Process</title><categories>cs.DL</categories><comments>7 pages, 7 figures, created 29 May 2000, when I still worked at the
  University of Twente, The Netherlands. The paper was submitted to a Python
  conference and accepted, but due to a misunderstanding with my employer, I
  had to withdraw the paper from the conference. The paper describes the public
  version, called CyberChair, and not the commercial version called
  CyberChairPRO</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe CyberChair, a web-based groupware application that
supports the review process for technical contributions to conferences.
CyberChair deals with most administrative tasks that are involved in the review
process, such as storing author information, abstracts, (camera-ready) papers
and reviews. It generates several overviews based on the reviews which support
the Program Committee (PC) in selecting the best papers. CyberChair points out
conflicting reviews and offers the reviewers means to easily communicate to
solve these conflicts. In his paper Identify the Champion, O. Nierstrasz
describes this review process in terms of a pattern language. CyberChair
supports PCs by using these patterns in its implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1837</identifier>
 <datestamp>2012-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1837</id><created>2012-06-08</created><updated>2012-06-30</updated><authors><author><keyname>Chauve</keyname><forenames>Cedric</forenames></author><author><keyname>Stephen</keyname><forenames>Tamon</forenames></author><author><keyname>Tamayo</keyname><forenames>Maria</forenames></author></authors><title>Efficient Algorithms for Finding Tucker Patterns</title><categories>cs.DS cs.DM</categories><comments>15 pages. Preliminary version This paper had been withdrawn due to
  some missing cases in Algorithms 2 and 3</comments><msc-class>05C85</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Consecutive Ones Property is an important notion for binary matrices,
both from a theoretical and applied point of view. Tucker gave in 1972 a
characterization of matrices that do not satisfy the Consecutive Ones Property
in terms of forbidden submatrices, the Tucker patterns. We describe here a
linear time algorithm to find a Tucker pattern in a non-C1P binary matrix,
which allows to extract in linear time a certificate for the non-C1P. We also
describe an output-sensitive algorithm to enumerate all Tucker patterns of a
non-C1P binary matrix.
  This paper had been withdrawn due to some missing cases in Algorithms 2 and
3.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1848</identifier>
 <datestamp>2012-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1848</id><created>2012-06-08</created><authors><author><keyname>Lahby</keyname><forenames>Mohamed</forenames></author><author><keyname>Cherkaoui</keyname><forenames>Leghris</forenames></author><author><keyname>Adib</keyname><forenames>Abdellah</forenames></author></authors><title>An Enhanced Evaluation Model For Vertical Handover Algorithm In
  Heterogeneous Networks</title><categories>cs.NI</categories><comments>6 pages; SSN (Online): 1694-0814</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 3, No 2, pp. 254-259, May 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The vertical handover decision is considered an NP-Hard problem. For that
reason, a large variety of vertical handoff algorithms (VHA) have been proposed
to help the user to select dynamically the best access network in terms of
quality of service (QoS). The objective of this paper is to provide a new
approach for evaluating of the vertical handoff algorithms in order to choose
the most appropriate algorithm which should be used to select the best access
network. Simulation results are presented to evaluate and to test our new
evaluation model
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1851</identifier>
 <datestamp>2012-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1851</id><created>2012-06-08</created><authors><author><keyname>Fister</keyname><forenames>Iztok</forenames></author><author><keyname>Fister</keyname><forenames>Iztok</forenames><suffix>Jr</suffix></author></authors><title>Concept of drafting detection system in Ironmans</title><categories>cs.SY</categories><journal-ref>Electrotechnical Review, vol. 78, no. 4, pp. 217-222, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the biggest challenges for the Computer Science of today can be summed
up by the paradigm &quot;access to information from $everywhere$ at $anytime$&quot;. This
is especially true for pervasive computing. With the growth of mobile devices
(e.g., smart-phones), on the one hand, and the quick development of the
Internet (this has become the really pervasive network of today), on the other
hand, the development of real-time pervasive applications has broadened. This
paper focuses on the problem of drafting detection in the Ironman triathlons
which causes serious problems for the majority of organizers regarding such
competitions. A concept of drafting detection system in Ironman is based on the
paradigm of pervasive computing. Results of performing a test system show that
this concept can along with further development of computer technologies become
a reality in the near future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1852</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1852</id><created>2012-06-07</created><authors><author><keyname>Omri</keyname><forenames>Mohamed Nazih</forenames></author></authors><title>Optimization of Fuzzy Semantic Networks Based on Galois Lattice and
  Bayesian Formalism</title><categories>cs.IR</categories><comments>arXiv admin note: text overlap with arXiv:1206.1794</comments><journal-ref>Eighth international Conference on Information Processing and
  Management of Uncertainty in Knowledge-Based Systems. P. 1844-1850. Madrid,
  Espagne, 2000</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a method of optimization, based on both Bayesian Analysis
technical and Galois Lattice of Fuzzy Semantic Network. The technical System we
use learns by interpreting an unknown word using the links created between this
new word and known words. The main link is provided by the context of the
query. When novice's query is confused with an unknown verb (goal) applied to a
known noun denoting either an object in the ideal user's Network or an object
in the user's Network, the system infer that this new verb corresponds to one
of the known goal. With the learning of new words in natural language as the
interpretation, which was produced in agreement with the user, the system
improves its representation scheme at each experiment with a new user and, in
addition, takes advantage of previous discussions with users. The semantic Net
of user objects thus obtained by learning is not always optimal because some
relationships between couple of user objects can be generalized and others
suppressed according to values of forces that characterize them. Indeed, to
simplify the obtained Net, we propose to proceed to an Inductive Bayesian
Analysis, on the Net obtained from Galois lattice. The objective of this
analysis can be seen as an operation of filtering of the obtained descriptive
graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1866</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1866</id><created>2012-06-08</created><authors><author><keyname>Xue</keyname><forenames>Yuanyi</forenames></author><author><keyname>Wang</keyname><forenames>Yao</forenames></author></authors><title>Perceptual quality comparison between single-layer and scalable videos
  at the same spatial, temporal and amplitude resolutions</title><categories>cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the perceptual quality difference between scalable and
single-layer videos coded at the same spatial, temporal and amplitude
resolution (STAR) is investigated through a subjective test using a mobile
platform. Three source videos are considered and for each source video
single-layer and scalable video are compared at 9 different STARs. We utilize
paired comparison methods with and without tie option. Results collected from
10 subjects in the without &quot;tie&quot; option and 6 subjects in the with &quot;tie&quot; option
show that there is no significant quality difference between scalable and
singlelayer video when coded at the same STAR. An analysis of variance (ANOVA)
test is also performed to further confirm the finding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1877</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1877</id><created>2012-06-08</created><authors><author><keyname>Dondi</keyname><forenames>Riccardo</forenames></author><author><keyname>El-Mabrouk</keyname><forenames>Nadia</forenames></author></authors><title>On the Complexity of Minimum Labeling Alignment of Two Genomes</title><categories>cs.CC cs.DS q-bio.GN</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note we investigate the complexity of the Minimum Label Alignment
problem and we show that such a problem is APX-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1880</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1880</id><created>2012-06-08</created><authors><author><keyname>Bruns</keyname><forenames>Bryan</forenames></author></authors><title>Escaping Prisoner's Dilemmas: From Discord to Harmony in the Landscape
  of 2x2 Games</title><categories>cs.GT</categories><comments>41 pages including 9 plates at end. This extends and largely
  supersedes my October 2010 paper &quot;Navigating the Topology of 2x2 Games: An
  Introductory Note on Payoff Families, Normalization, and Natural Order.&quot;</comments><msc-class>91A05, 91A70</msc-class><acm-class>J.4</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Changes in payoffs can transform Prisoner's Dilemma and other social dilemmas
into harmonious win-win games. Using the Robinson-Goforth topology of 2x2
games, this paper analyzes how payoff swaps turn Prisoner's Dilemma into other
games, compares Prisoner's Dilemmas with other families of games, traces paths
that affect the difficulty of transforming Prisoner's Dilemma and other social
dilemmas into win-win games, and shows how ties connect simpler and more
complex games. Charts illustrate the relationships between the 144 strict
ordinal 2x2 games, the 38 symmetric 2x2 ordinal games with and without ties,
and the complete set of 1,413 2x2 ordinal games. Payoffs from the symmetric
ordinal 2x2 games combine to form asymmetric games, generating coordinates for
a simple labeling scheme to uniquely identify and locate all asymmetric ordinal
2x2 games. The expanded topology elegantly maps relationships between 2x2 games
with and without ties, enables a systematic understanding of the potential for
transformations in social dilemmas and other strategic interactions, offers a
tool for institutional analysis and design, and locates a variety of
interesting games for further research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1891</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1891</id><created>2012-06-08</created><authors><author><keyname>Shin</keyname><forenames>Donghyuk</forenames></author><author><keyname>Si</keyname><forenames>Si</forenames></author><author><keyname>Dhillon</keyname><forenames>Inderjit S.</forenames></author></authors><title>Multi-Scale Link Prediction</title><categories>cs.SI physics.soc-ph</categories><comments>20 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The automated analysis of social networks has become an important problem due
to the proliferation of social networks, such as LiveJournal, Flickr and
Facebook. The scale of these social networks is massive and continues to grow
rapidly. An important problem in social network analysis is proximity
estimation that infers the closeness of different users. Link prediction, in
turn, is an important application of proximity estimation. However, many
methods for computing proximity measures have high computational complexity and
are thus prohibitive for large-scale link prediction problems. One way to
address this problem is to estimate proximity measures via low-rank
approximation. However, a single low-rank approximation may not be sufficient
to represent the behavior of the entire network. In this paper, we propose
Multi-Scale Link Prediction (MSLP), a framework for link prediction, which can
handle massive networks. The basis idea of MSLP is to construct low rank
approximations of the network at multiple scales in an efficient manner. Based
on this approach, MSLP combines predictions at multiple scales to make robust
and accurate predictions. Experimental results on real-life datasets with more
than a million nodes show the superior performance and scalability of our
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1892</identifier>
 <datestamp>2014-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1892</id><created>2012-06-08</created><updated>2014-01-04</updated><authors><author><keyname>Lopez</keyname><forenames>Hiram H.</forenames></author><author><keyname>Villarreal</keyname><forenames>Rafael H.</forenames></author></authors><title>Computing the degree of a lattice ideal of dimension one</title><categories>math.AC cs.IT math.AG math.IT</categories><comments>J. Symbolic Comput., to appear</comments><msc-class>13F20, 13P25, 13H15, 11T71</msc-class><journal-ref>J. Symbolic Comput. 65 (2014), 15--28</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the degree of a graded lattice ideal of dimension 1 is the order
of the torsion subgroup of the quotient group of the lattice. This gives an
efficient method to compute the degree of this type of lattice ideals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1897</identifier>
 <datestamp>2012-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1897</id><created>2012-06-08</created><updated>2012-07-23</updated><authors><author><keyname>Galeana-S&#xe1;nchez</keyname><forenames>Hortensia</forenames></author><author><keyname>Hern&#xe1;ndez-Cruz</keyname><forenames>C&#xe9;sar</forenames></author><author><keyname>Ju&#xe1;rez-Camacho</keyname><forenames>Manuel Alejandro</forenames></author></authors><title>On the existence and number of $(k+1)$-kings in $k$-quasi-transitive
  digraphs</title><categories>math.CO cs.DM</categories><comments>17 pages</comments><msc-class>05C20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $D=(V(D), A(D))$ be a digraph and $k \ge 2$ an integer. We say that $D$
is $k$-quasi-transitive if for every directed path $(v_0, v_1,..., v_k)$ in
$D$, then $(v_0, v_k) \in A(D)$ or $(v_k, v_0) \in A(D)$. Clearly, a
2-quasi-transitive digraph is a quasi-transitive digraph in the usual sense.
  Bang-Jensen and Gutin proved that a quasi-transitive digraph $D$ has a 3-king
if and only if $D$ has a unique initial strong component and, if $D$ has a
3-king and the unique initial strong component of $D$ has at least three
vertices, then $D$ has at least three 3-kings. In this paper we prove the
following generalization: A $k$-quasi-transitive digraph $D$ has a $(k+1)$-king
if and only if $D$ has a unique initial strong component, and if $D$ has a
$(k+1)$-king then, either all the vertices of the unique initial strong
components are $(k+1)$-kings or the number of $(k+1)$-kings in $D$ is at least
$(k+2)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1898</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1898</id><created>2012-06-08</created><updated>2012-11-10</updated><authors><author><keyname>Ortega</keyname><forenames>Pedro A.</forenames></author><author><keyname>Grau-Moya</keyname><forenames>Jordi</forenames></author><author><keyname>Genewein</keyname><forenames>Tim</forenames></author><author><keyname>Balduzzi</keyname><forenames>David</forenames></author><author><keyname>Braun</keyname><forenames>Daniel A.</forenames></author></authors><title>A Nonparametric Conjugate Prior Distribution for the Maximizing Argument
  of a Noisy Function</title><categories>stat.ML cs.AI math.ST stat.TH</categories><comments>9 pages, 5 figures</comments><journal-ref>Neural Information Processing Systems (NIPS) 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel Bayesian approach to solve stochastic optimization
problems that involve finding extrema of noisy, nonlinear functions. Previous
work has focused on representing possible functions explicitly, which leads to
a two-step procedure of first, doing inference over the function space and
second, finding the extrema of these functions. Here we skip the representation
step and directly model the distribution over extrema. To this end, we devise a
non-parametric conjugate prior based on a kernel regressor. The resulting
posterior distribution directly captures the uncertainty over the maximum of
the unknown function. We illustrate the effectiveness of our model by
optimizing a noisy, high-dimensional, non-convex objective function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1899</identifier>
 <datestamp>2013-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1899</id><created>2012-06-08</created><updated>2012-08-09</updated><authors><author><keyname>Shakya</keyname><forenames>Rajeev K.</forenames></author></authors><title>TTMA: Traffic-adaptive Time-division Multiple Access Protocol Wireless
  Sensor Networks</title><categories>cs.DC</categories><comments>This paper has been withdrawn by arXiv. arXiv admin note: author list
  truncated due to disputed authorship and content. This submission repeats
  large portions of text from
  http://www.cse.msu.edu/~lxiao/publications/TATD_MAC.pdf by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper has been withdrawn by arXiv. arXiv admin note: author list
truncated due to disputed authorship and content. This submission repeats large
portions of text from this http URL by other authors. Duty cycle mode in WSN
improves energy-efficiency, but also introduces packet delivery latency.
Several duty-cycle based MAC schemes have been proposed to reduce latency, but
throughput is limited by duty-cycled scheduling performance. In this paper, a
Traffic-adaptive Time-division Multiple Access (TTMA), a distributed TDMA-based
MAC protocol is introduced to improves the throughput by traffic-adaptive
time-slot scheduling that increases the channel utilisation efficiency. The
proposed time-slot scheduling method first avoids time-slots assigned to nodes
with no traffic through fast traffic notification. It then achieves better
channel utilisation among nodes having traffic through an ordered schedule
negotiation scheme. By decomposing traffic notification and data transmission
scheduling into two phases leads each phase to be simple and efficient. The
performance evaluation shows that the two-phase design significantly improves
the throughput and outperforms the time division multiple access (TDMA) control
with slot stealing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1903</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1903</id><created>2012-06-08</created><authors><author><keyname>Tang</keyname><forenames>Wenyuan</forenames></author><author><keyname>Jain</keyname><forenames>Rahul</forenames></author></authors><title>Mechanism Designs for Stochastic Resources for Renewable Energy
  Integration</title><categories>cs.GT cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Among the many challenges of integrating renewable energy sources into the
existing power grid, is the challenge of integrating renewable energy
generators into the power systems economy. Electricity markets currently are
run in a way that participating generators must supply contracted amounts. And
yet, renewable energy generators such as wind power generators cannot supply
contracted amounts with certainty. Thus, alternative market architectures must
be considered where there are aggregator entities who participate in the
electricity market by buying power from the renewable energy generators, and
assuming risk of any shortfall from contracted amounts. In this paper, we
propose auction mechanisms that can be used by the aggregators for procuring
stochastic resources, such as wind power. The nature of stochastic resources is
different from classical resources in that such a resource is only available
stochastically. The distribution of the generation is private information, and
the system objective is to truthfully elicit such information. We introduce a
variant of the VCG mechanism for this problem. We also propose a non-VCG
mechanism with a contracted-payment-plus-penalty payoff structure. We
generalize the basic mechanisms in various ways. We then consider the setting
where there are two classes of players to demonstrate the difficulty of auction
design in such scenarios. We also consider an alternative architecture where
the generators need to fulfill any shortfall from the contracted amount by
buying from the spot market.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1918</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1918</id><created>2012-06-09</created><authors><author><keyname>Vinayakray-Jani</keyname><forenames>Preetida</forenames></author><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author></authors><title>Routing Protocols for Mobile and Vehicular Ad-Hoc Networks: A
  Comparative Analysis</title><categories>cs.NI</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present comparative analysis of MANET (Mobile Ad-Hoc Network) and VANET
(Vehicular Ad-Hoc Network) routing protocols, in this paper. The analysis is
based on various design factors. The traditional routing protocols of AODV (Ad
hoc On-Demand Distance Vector), DSR (Dynamic Source Routing), and DSDV
(Destination-Sequenced Distance-Vector) of MANET are utilizing node centric
routing which leads to frequent breaking of routes, causing instability in
routing. Usage of these protocols in high mobility environment like VANET may
eventually cause many packets to drop. Route repairs and failures notification
overheads increase significantly leading to low throughput and long delays.
Such phenomenon is not suitable for Vehicular Ad hoc Networks (VANET) due to
high mobility of nodes where network can be dense or sparse. Researchers have
proposed various routing algorithms or mechanism for MANET and VANET. This
paper describes the relevant protocols, associated algorithm and the strength
and weakness of these routing protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1919</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1919</id><created>2012-06-09</created><updated>2012-09-15</updated><authors><author><keyname>Aleardi</keyname><forenames>Luca Castelli</forenames></author><author><keyname>Devillers</keyname><forenames>Olivier</forenames></author><author><keyname>Fusy</keyname><forenames>Eric</forenames></author></authors><title>Canonical Ordering for Triangulations on the Cylinder, with Applications
  to Periodic Straight-line Drawings</title><categories>math.CO cs.DM</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend the notion of canonical orderings to cylindric triangulations. This
allows us to extend the incremental straight-line drawing algorithm of de
Fraysseix et al. to this setting. Our algorithm yields in linear time a
crossing-free straight-line drawing of a cylindric triangulation $T$ with $n$
vertices on a regular grid $Z/wZ\times[0,h]$, with $w\leq 2n$ and $h\leq
n(2d+1)$, where $d$ is the (graph-) distance between the two boundaries. As a
by-product, we can also obtain in linear time a crossing-free straight-line
drawing of a toroidal triangulation with $n$ vertices on a periodic regular
grid $Z/wZ\times Z/hZ$, with $w\leq 2n$ and $h\leq 1+n(2c+1)$, where $c$ is the
length of a shortest non-contractible cycle. Since $c\leq\sqrt{2n}$, the grid
area is $O(n^{5/2})$. Our algorithms apply to any triangulation (whether on the
cylinder or on the torus) with no loops nor multiple edges in the periodic
representation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1926</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1926</id><created>2012-06-09</created><authors><author><keyname>Novozhilov</keyname><forenames>Nikolay</forenames></author></authors><title>The hardest logic puzzle ever becomes even tougher</title><categories>math.LO cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  &quot;The hardest logic puzzle ever&quot; presented by George Boolos became a target
for philosophers and logicians who tried to modify it and make it even tougher.
I propose further modification of the original puzzle where part of the
available information is eliminated but the solution is still possible. The
solution also gives interesting ideas on logic behind discovery of unknown
language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1932</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1932</id><created>2012-06-09</created><authors><author><keyname>Fronczak</keyname><forenames>Piotr</forenames></author></authors><title>Theoretical approach and impact of correlations on the critical packet
  generation rate in traffic dynamics on complex networks</title><categories>physics.soc-ph cs.SI</categories><comments>6 pages, 5 figures</comments><doi>10.1140/epjb/e2012-30062-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using the formalism of the biased random walk in random uncorrelated networks
with arbitrary degree distributions, we develop theoretical approach to the
critical packet generation rate in traffic based on routing strategy with local
information. We explain microscopic origins of the transition from the flow to
the jammed phase and discuss how the node neighbourhood topology affects the
transport capacity in uncorrelated and correlated networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1935</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1935</id><created>2012-06-09</created><authors><author><keyname>Yu</keyname><forenames>Nengkun</forenames></author><author><keyname>Ying</keyname><forenames>Mingsheng</forenames></author></authors><title>Reachability and Termination Analysis of Concurrent Quantum Programs</title><categories>cs.LO quant-ph</categories><comments>Accepted by Concur'12. Comments are welcome</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a Markov chain model of concurrent quantum programs. This model
is a quantum generalization of Hart, Sharir and Pnueli's probabilistic
concurrent programs. Some characterizations of the reachable space, uniformly
repeatedly reachable space and termination of a concurrent quantum program are
derived by the analysis of their mathematical structures. Based on these
characterizations, algorithms for computing the reachable space and uniformly
repeatedly reachable space and for deciding the termination are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1936</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1936</id><created>2012-06-09</created><authors><author><keyname>Staudt</keyname><forenames>D. J. C.</forenames></author></authors><title>Completeness for Two Left-Sequential Logics</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Left-sequential logics provide a means for reasoning about (closed)
propositional terms with atomic propositions that may have side effects and
that are evaluated sequentially from left to right. Such propositional terms
are commonly used in programming languages to direct the flow of a program. In
this thesis we explore two such left-sequential logics. First we discuss Fully
Evaluated Left-Sequential Logic, which employs a full evaluation strategy,
i.e., to evaluate a term every one of its atomic propositions is evaluated
causing its possible side effects to occur. We then turn to Short-Circuit
(Left-Sequential) Logic as presented in [BP10b], where the evaluation may be
'short-circuited', thus preventing some, if not all, of the atomic propositions
in a term being evaluated. We propose evaluation trees as a natural semantics
for both logics and provide axiomatizations for the least identifying variant
of each. From this, we define a logic with connectives that prescribe a full
evaluation strategy as well as connectives that prescribe a short-circuit
evaluation strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1943</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1943</id><created>2012-06-09</created><updated>2012-06-17</updated><authors><author><keyname>Georgakopoulos</keyname><forenames>Dimitrios</forenames></author><author><keyname>Ranjan</keyname><forenames>Rajiv</forenames></author><author><keyname>Mitra</keyname><forenames>Karan</forenames></author><author><keyname>Zhou</keyname><forenames>Xiangmin</forenames></author></authors><title>MediaWise - Designing a Smart Media Cloud</title><categories>cs.DC cs.MM cs.NI</categories><comments>This paper appears as the invited keynote paper in the proceedings of
  the International Conference on Advances in Cloud Computing (ACC-2012), July
  26-28, 2012, Bangalore, India</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The MediaWise project aims to expand the scope of existing media delivery
systems with novel cloud, personalization and collaboration capabilities that
can serve the needs of more users, communities, and businesses. The project
develops a MediaWise Cloud platform that supports do-it-yourself creation,
search, management, and consumption of multimedia content. The MediaWise Cloud
supports pay-as-you-go models and elasticity that are similar to those offered
by commercially available cloud services. However, unlike existing commercial
CDN services providers such as Limelight Networks and Akamai the MediaWise
Cloud require no ownerships of computing infrastructure and instead rely on the
public Internet and public cloud services (e.g., commercial cloud storage to
store its content). In addition to integrating such public cloud services into
a public cloud-based Content Delivery Network, the MediaWise Cloud also
provides advanced Quality of Service (QoS) management as required for the
delivery of streamed and interactive high resolution multimedia content. In
this paper, we give a brief overview of MediaWise Cloud architecture and
present a comprehensive discussion on research objectives related to its
service components. Finally, we also compare the features supported by the
existing CDN services against the envisioned objectives of MediaWise Cloud.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1948</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1948</id><created>2012-06-09</created><updated>2012-10-07</updated><authors><author><keyname>Vaezi</keyname><forenames>Mojtaba</forenames></author></authors><title>The Capacity of Less Noisy Cognitive Interference Channels</title><categories>cs.IT math.IT</categories><comments>6 pages, one figure, one table. Appeared at the 50th Annual Allerton
  Conference on Communications, Control, and Computing (Allerton 2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fundamental limits of the cognitive interference channel (CIC) with two pairs
of transmitter-receiver has been under exploration for several years. In this
paper, we study the discrete memoryless cognitive interference channel (DM-CIC)
in which the cognitive transmitter non-causally knows the message of the
primary transmitter. The capacity of this channel is not known in general; it
is only known in some special cases. Inspired by the concept of less noisy
broadcast channel (BC), in this work we introduce the notion of less noisy
cognitive interference channel. Unlike BC, due to the inherent asymmetry of the
cognitive channel, two different less noisy channels are distinguishable; these
are named the primary-less-noisy and cognitive-less-noisy channels. We derive
capacity region for the latter case, by introducing inner and outer bounds on
the capacity of the DM-CIC and showing that these bounds coincide for the
cognitive-less-noisy channel. Having established the capacity region, we prove
that superposition coding is the optimal encoding technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1953</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1953</id><created>2012-06-09</created><authors><author><keyname>Nouri</keyname><forenames>Mojtaba</forenames></author><author><keyname>Mokhtari</keyname><forenames>Mahdi Bayat</forenames></author><author><keyname>Mirsaeidi</keyname><forenames>Sohrab</forenames></author><author><keyname>Miveh</keyname><forenames>Mohammad Reza</forenames></author></authors><title>Improvement of Loadability in Distribution System Using Genetic
  Algorithm</title><categories>cs.SY cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generally during recent decades due to development of power systems, the
methods for delivering electrical energy to consumers, and because of voltage
variations is a very important problem, the power plants follow this criteria.
The good solution for improving transfer and distribution of electrical power
the majority of consumers prefer to use energy near the loads .So small units
that are connected to distribution system named &quot;Decentralized Generation&quot; or
&quot;Dispersed Generation&quot;. Deregulated in power industry and development of
renewable energies are the most important factors in developing this type of
electricity generation. Today DG has a key role in electrical distribution
systems. For example we can refer to improving reliability indices, improvement
of stability and reduction of losses in power system. One of the key problems
in using DG's, is allocation of these sources in distribution networks. Load
ability in distribution systems and its improvement has an effective role in
the operation of power systems. However, placement of distributed generation
sources in order to improve the distribution system load ability index was not
considered, we show DG placement and allocation with genetic algorithm
optimization method maximize load ability of power systems .This method
implemented on the IEEE Standard bench marks. The results show the
effectiveness of the proposed algorithm .Another benefits of DG in selected
positions are also studied and compared.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1957</identifier>
 <datestamp>2012-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1957</id><created>2012-06-09</created><authors><author><keyname>Kayarkar</keyname><forenames>Harshavardhan</forenames></author><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author></authors><title>A Survey on Various Data Hiding Techniques and their Comparative
  Analysis</title><categories>cs.CR</categories><comments>9 pages, 1 table</comments><journal-ref>ACTA Technica Corviniensis, Vol. 5, Issue 3, July-September 2012,
  pp. 35-40</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the explosive growth of internet and the fast communication techniques
in recent years the security and the confidentiality of the sensitive data has
become of prime and supreme importance and concern. To protect this data from
unauthorized access and tampering various methods for data hiding like
cryptography, hashing, authentication have been developed and are in practice
today. In this paper we will be discussing one such data hiding technique
called Steganography. Steganography is the process of concealing sensitive
information in any media to transfer it securely over the underlying unreliable
and unsecured communication network. Our paper presents a survey on various
data hiding techniques in Steganography that are in practice today along with
the comparative analysis of these techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1968</identifier>
 <datestamp>2012-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1968</id><created>2012-06-09</created><updated>2012-10-02</updated><authors><author><keyname>Sarkar</keyname><forenames>Saurabh</forenames></author></authors><title>A novel 2.5D approach for interfacing with web applications</title><categories>cs.HC cs.GR</categories><comments>An approach offering a new idea for Human Computer Interaction,
  including 3 pages and 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web applications need better user interface to be interactive and attractive.
A new approach/concept of dimensional enhancement - 2.5D &quot;a 2D display of a
virtual 3D environment&quot;, which can be implemented in social networking sites
and further in other system applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1969</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1969</id><created>2012-06-09</created><authors><author><keyname>Fister</keyname><forenames>Iztok</forenames><suffix>Jr.</suffix></author><author><keyname>Mernik</keyname><forenames>Marjan</forenames></author><author><keyname>Fister</keyname><forenames>Iztok</forenames></author><author><keyname>Hrn&#x10d;i&#x10d;</keyname><forenames>Dejan</forenames></author></authors><title>Implementation of EasyTime Formal Semantics using a LISA Compiler
  Generator</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A manual measuring time tool in mass sporting competitions would not be
imaginable nowadays, because many modern disciplines, such as IRONMAN, last a
long-time and, therefore, demand additional reliability. Moreover, automatic
timing-devices based on RFID technology, have become cheaper. However, these
devices cannot operate as stand-alone because they need a computer measuring
system that is capable of processing incoming events, encoding the results,
assigning them to the correct competitor, sorting the results according to the
achieved times, and then providing a printout of the results. This article
presents the domain-specific language EasyTime, which enables the controlling
of an agent by writing the events within a database. It focuses, in particular,
on the implementation of EasyTime with a LISA tool that enables the automatic
construction of compilers from language specifications, using Attribute
Grammars.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1971</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1971</id><created>2012-06-09</created><updated>2012-10-08</updated><authors><author><keyname>Abraham</keyname><forenames>Siby</forenames></author><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author><author><keyname>Sanglikar</keyname><forenames>Mukund</forenames></author></authors><title>A Connectionist Network Approach to Find Numerical Solutions of
  Diophantine Equations</title><categories>cs.NE</categories><comments>7 pages, 2 tables, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper introduces a connectionist network approach to find numerical
solutions of Diophantine equations as an attempt to address the famous
Hilbert's tenth problem. The proposed methodology uses a three layer feed
forward neural network with back propagation as sequential learning procedure
to find numerical solutions of a class of Diophantine equations. It uses a
dynamically constructed network architecture where number of nodes in the input
layer is chosen based on the number of variables in the equation. The powers of
the given Diophantine equation are taken as input to the input layer. The
training of the network starts with initial random integral weights. The
weights are updated based on the back propagation of the error values at the
output layer. The optimization of weights is augmented by adding a momentum
factor into the network. The optimized weights of the connection between the
input layer and the hidden layer are taken as numerical solution of the given
Diophantine equation. The procedure is validated using different Diophantine
Equations of different number of variables and different powers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1973</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1973</id><created>2012-06-09</created><authors><author><keyname>Carson</keyname><forenames>William R.</forenames></author><author><keyname>Chen</keyname><forenames>Minhua</forenames></author><author><keyname>Rodrigues</keyname><forenames>Miguel R. D.</forenames></author><author><keyname>Calderbank</keyname><forenames>Robert</forenames></author><author><keyname>Carin</keyname><forenames>Lawrence</forenames></author></authors><title>Communications-Inspired Projection Design with Application to
  Compressive Sensing</title><categories>cs.IT math.IT</categories><comments>25 pages, 7 figures, parts of material published in IEEE ICASSP 2012,
  submitted to SIIMS</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the recovery of an underlying signal x \in C^m based on
projection measurements of the form y=Mx+w, where y \in C^l and w is
measurement noise; we are interested in the case l &lt; m. It is assumed that the
signal model p(x) is known, and w CN(w;0,S_w), for known S_W. The objective is
to design a projection matrix M \in C^(l x m) to maximize key
information-theoretic quantities with operational significance, including the
mutual information between the signal and the projections I(x;y) or the Renyi
entropy of the projections h_a(y) (Shannon entropy is a special case). By
capitalizing on explicit characterizations of the gradients of the information
measures with respect to the projections matrix, where we also partially extend
the well-known results of Palomar and Verdu from the mutual information to the
Renyi entropy domain, we unveil the key operations carried out by the optimal
projections designs: mode exposure and mode alignment. Experiments are
considered for the case of compressive sensing (CS) applied to imagery. In this
context, we provide a demonstration of the performance improvement possible
through the application of the novel projection designs in relation to
conventional ones, as well as justification for a fast online projections
design method with which state-of-the-art adaptive CS signal recovery is
achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1982</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1982</id><created>2012-06-09</created><authors><author><keyname>Bose</keyname><forenames>Prosenjit</forenames></author><author><keyname>Dujmovi&#x107;</keyname><forenames>Vida</forenames></author><author><keyname>Hoda</keyname><forenames>Nima</forenames></author><author><keyname>Morin</keyname><forenames>Pat</forenames></author></authors><title>Visibility-Monotonic Polygon Deflation</title><categories>cs.CG</categories><comments>19 pages, 139 figures, abridged version submitted to CCCG 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A deflated polygon is a polygon with no visibility crossings. We answer a
question posed by Devadoss et al. (2012) by presenting a polygon that cannot be
deformed via continuous visibility-decreasing motion into a deflated polygon.
We show that the least n for which there exists such an n-gon is seven. In
order to demonstrate non-deflatability, we use a new combinatorial structure
for polygons, the directed dual, which encodes the visibility properties of
deflated polygons. We also show that any two deflated polygons with the same
directed dual can be deformed, one into the other, through a
visibility-preserving deformation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1984</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1984</id><created>2012-06-09</created><authors><author><keyname>Emami</keyname><forenames>Masnida</forenames></author><author><keyname>Ghiasi</keyname><forenames>Yashar</forenames></author><author><keyname>Jaberi</keyname><forenames>Nasrin</forenames></author></authors><title>Energy-Aware Scheduling using Dynamic Voltage-Frequency Scaling</title><categories>cs.DC</categories><comments>arXiv admin note: text overlap with arXiv:1203.5160</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The energy consumption issue in distributed computing systems has become
quite critical due to environmental concerns. In response to this, many
energy-aware scheduling algorithms have been developed primarily by using the
dynamic voltage-frequency scaling (DVFS) capability incorporated in recent
commodity processors. The majority of these algorithms involve two passes:
schedule generation and slack reclamation. The latter is typically achieved by
lowering processor frequency for tasks with slacks. In this article, we study
the latest papers in this area and develop them. This study has been evaluated
based on results obtained from experiments with 1,500 randomly generated task
graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1993</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1993</id><created>2012-06-10</created><authors><author><keyname>Chang</keyname><forenames>Maw-Shang</forenames></author><author><keyname>Kloks</keyname><forenames>Ton</forenames></author><author><keyname>Liu</keyname><forenames>Ching-Hao</forenames></author></authors><title>Independent sets in edge-clique graphs</title><categories>math.CO cs.DM</categories><comments>arXiv admin note: incorporates arXiv:1205.2483</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the edge-clique graphs of cocktail party graphs have unbounded
rankwidth. This, and other observations lead us to conjecture that the
edge-clique cover problem is NP-complete for cographs. We show that the
independent set problem on edge-clique graphs of cographs and of
distance-hereditary graphs can be solved in O(n^4) time. We show that the
independent set problem on edge-clique graphs of graphs without odd wheels
remains NP-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1996</identifier>
 <datestamp>2012-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1996</id><created>2012-06-10</created><updated>2012-10-04</updated><authors><author><keyname>Balachandran</keyname><forenames>Niranjan</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Srimanta</forenames></author></authors><title>On an Extremal Hypergraph Problem Related to Combinatorial Batch Codes</title><categories>cs.DM math.CO</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $n, r, k$ be positive integers such that $3\leq k &lt; n$ and $2\leq r \leq
k-1$. Let $m(n, r, k)$ denote the maximum number of edges an $r$-uniform
hypergraph on $n$ vertices can have under the condition that any collection of
$i$ edges, span at least $i$ vertices for all $1 \leq i \leq k$. We are
interested in the asymptotic nature of $m(n, r, k)$ for fixed $r$ and $k$ as $n
\rightarrow \infty$. This problem is related to the forbidden hypergraph
problem introduced by Brown, Erd\H{o}s, and S\'os and very recently discussed
in the context of combinatorial batch codes. In this short paper we obtain the
following results. {enumerate}[(i)] Using a result due to Erd\H{o}s we are able
to show $m(n, k, r) = o(n^r)$ for $7\leq k$, and $3 \leq r \leq k-1-\lceil\log
k \rceil$. This result is best possible with respect to the upper bound on $r$
as we subsequently show through explicit construction that for $6 \leq k$, and
$k-\lceil \log k \rceil \leq r \leq k-1, m(n, r, k) = \Theta(n^r)$.
  This explicit construction improves on the non-constructive general lower
bound obtained by Brown, Erd\H{o}s, and S\'os for the considered parameter
values. For 2-uniform CBCs we obtain the following results. {enumerate} We
provide exact value of $m(n, 2, 5)$ for $n \geq 5$. Using a result of
Lazebnik,et al. regarding maximum size of graphs with large girth, we improve
the existing lower bound on $m(n, 2, k)$ ($\Omega(n^{\frac{k+1}{k-1}})$) for
all $k \geq 8$ and infinitely many values of $n$. We show $m(n, 2, k) =
O(n^{1+\frac{1}{\lfloor\frac{k}{4}\rfloor}})$ by using a result due to Bondy
and Simonovits, and also show $m(n, 2, k) = \Theta(n^{3/2})$ for $k = 6, 7, 8$
by using a result of K\&quot;{o}vari, S\'os, and Tur\'{a}n.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.1999</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.1999</id><created>2012-06-10</created><authors><author><keyname>Golosovsky</keyname><forenames>Michael</forenames></author><author><keyname>Solomon</keyname><forenames>Sorin</forenames></author></authors><title>Runaway Events Dominate the Heavy Tail of Citation Distributions</title><categories>physics.soc-ph cond-mat.stat-mech cs.DL</categories><comments>6 pages, 5 Figures</comments><journal-ref>Eur. Phys. J. Special Topics 205, 303--311 (2012)</journal-ref><doi>10.1140/epjst/e2012-01576-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistical distributions with heavy tails are ubiquitous in natural and
social phenomena. Since the entries in heavy tail have disproportional
significance, the knowledge of its exact shape is very important. Citations of
scientific papers form one of the best-known heavy tail distributions. Even in
this case there is a considerable debate whether citation distribution follows
the log-normal or power-law fit. The goal of our study is to solve this debate
by measuring citation distribution for a very large and homogeneous data. We
measured citation distribution for 418,438 Physics papers published in
1980-1989 and cited by 2008. While the log-normal fit deviates too strong from
the data, the discrete power-law function with the exponent $\gamma=3.15$ does
better and fits 99.955% of the data. However, the extreme tail of the
distribution deviates upward even from the power-law fit and exhibits a
dramatic &quot;runaway&quot; behavior. The onset of the runaway regime is revealed
macroscopically as the paper garners 1000-1500 citations, however the
microscopic measurements of autocorrelation in citation rates are able to
predict this behavior in advance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2005</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2005</id><created>2012-06-10</created><authors><author><keyname>Hoang</keyname><forenames>Doan B.</forenames></author><author><keyname>Kamyabpour</keyname><forenames>Najmeh</forenames></author></authors><title>An Energy Driven Architecture for Wireless Sensor Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most wireless sensor networks operate with very limited energy sources-their
batteries, and hence their usefulness in real life applications is severely
constrained. The challenging issues are how to optimize the use of their energy
or to harvest their own energy in order to lengthen their lives for wider
classes of application. Tackling these important issues requires a robust
architecture that takes into account the energy consumption level of functional
constituents and their interdependency. Without such architecture, it would be
difficult to formulate and optimize the overall energy consumption of a
wireless sensor network. Unlike most current researches that focus on a single
energy constituent of WSNs independent from and regardless of other
constituents, this paper presents an Energy Driven Architecture (EDA) as a new
architecture and indicates a novel approach for minimising the total energy
consumption of a WSN
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2009</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2009</id><created>2012-06-10</created><authors><author><keyname>Boudhief</keyname><forenames>Asma</forenames></author><author><keyname>Maraoui</keyname><forenames>Mohsen</forenames></author><author><keyname>Zrigui</keyname><forenames>Mounir</forenames></author></authors><title>Developing a model for a text database indexed pedagogically for
  teaching the Arabic language</title><categories>cs.CL</categories><comments>43 pages,27 figures,12 tables</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this memory we made the design of an indexing model for Arabic language
and adapting standards for describing learning resources used (the LOM and
their application profiles) with learning conditions such as levels education
of students, their levels of understanding...the pedagogical context with
taking into account the repre-sentative elements of the text, text's
length,...in particular, we highlight the specificity of the Arabic language
which is a complex language, characterized by its flexion, its voyellation and
its agglutination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2010</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2010</id><created>2012-06-10</created><authors><author><keyname>Filannino</keyname><forenames>Michele</forenames></author></authors><title>Temporal expression normalisation in natural language texts</title><categories>cs.CL cs.IR</categories><comments>7 pages, 1 figure, 5 tables</comments><msc-class>68U02</msc-class><acm-class>D.3.2</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Automatic annotation of temporal expressions is a research challenge of great
interest in the field of information extraction. In this report, I describe a
novel rule-based architecture, built on top of a pre-existing system, which is
able to normalise temporal expressions detected in English texts. Gold standard
temporally-annotated resources are limited in size and this makes research
difficult. The proposed system outperforms the state-of-the-art systems with
respect to TempEval-2 Shared Task (value attribute) and achieves substantially
better results with respect to the pre-existing system on top of which it has
been developed. I will also introduce a new free corpus consisting of 2822
unique annotated temporal expressions. Both the corpus and the system are
freely available on-line.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2016</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2016</id><created>2012-06-10</created><updated>2012-07-26</updated><authors><author><keyname>Rizvandi</keyname><forenames>Nikzad Babaii</forenames></author><author><keyname>Taheri</keyname><forenames>Javid</forenames></author><author><keyname>Moraveji</keyname><forenames>Reza</forenames></author><author><keyname>Zomaya</keyname><forenames>Albert Y.</forenames></author></authors><title>Network Load Analysis and Provisioning of MapReduce Applications</title><categories>cs.DC cs.PF</categories><comments>6 pages-submitted to The Thirteenth International Conference on
  Parallel and Distributed Computing, Applications and Technologies(PDCAT-12),
  Beijing, China</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the dependency between configuration parameters and
network load of fixed-size MapReduce applications in shuffle phase and then
propose an analytical method to model this dependency. Our approach consists of
three key phases: profiling, modeling, and prediction. In the first stage, an
application is run several times with different sets of MapReduce configuration
parameters (here number of mappers and number of reducers) to profile the
network load of the application in the shuffle phase on a given cluster. Then,
the relation between these parameters and the network load is modeled by
multivariate linear regression. For evaluation, three applications (WordCount,
Exim Mainlog parsing, and TeraSort) are utilized to evaluate our technique on a
4-node MapReduce private cluster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2027</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2027</id><created>2012-06-10</created><authors><author><keyname>Delavari</keyname><forenames>H.</forenames></author><author><keyname>Ghaderi</keyname><forenames>R.</forenames></author><author><keyname>Ranjbar</keyname><forenames>N. A.</forenames></author><author><keyname>HosseinNia</keyname><forenames>S. H.</forenames></author><author><keyname>Momani</keyname><forenames>S.</forenames></author></authors><title>Adaptive Fractional PID Controller for Robot Manipulator</title><categories>nlin.AO cs.RO</categories><comments>Proceedings of FDA'10. The 4th IFAC Workshop Fractional
  Differentiation and its Applications. Article no. FDA10-038 Badajoz, Spain,
  October 18-20, 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Fractional adaptive PID (FPID) controller for a robot manipulator will be
proposed. The PID parameters have been optimized by Genetic algorithm. The
proposed controller is found robust by means of simulation in a tracking job.
The validity of the proposed controller is shown by simulation of two-link
robot manipulator. The result then is compared with integer type adaptive PID
controller. It is found that when error signals in the learning stage are
bounded, the trajectory of the robot converges to the desired one
asymptotically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2032</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2032</id><created>2012-06-10</created><authors><author><keyname>Gonczarowski</keyname><forenames>Yannai A.</forenames></author></authors><title>Timely Coordination in a Multi-Agent System</title><categories>cs.MA cs.GT cs.LO</categories><comments>Master's thesis. Advisors: Prof. Gil Kalai and Prof. Yoram Moses</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a distributed algorithm, multiple processes, or agents, work toward a
common goal. More often than not, the actions of some agents are dependent on
the previous execution (if not also on the outcome) of the actions of other
agents. The resulting interdependencies between the timings of the actions of
the various agents give rise to the study of methods for timely coordination of
these actions.
  In this work, we formulate and mathematically analyze &quot;Timely-Coordinated
Response&quot; - a novel multi-agent coordination problem in which the time
difference between each pair of actions may be constrained by upper and/or
lower bounds. This problem generalizes coordination problems previously studied
by Halpern and Moses and by Ben-Zvi and Moses.
  We optimally solve timely-coordinated response in two ways: using a
generalization of the fixed-point approach of Halpern and Moses, and using a
generalization of the &quot;syncausality&quot; approach of Ben-Zvi and Moses. We
constructively show the equivalence of the solutions yielded by both
approaches, and by combining them, derive strengthened versions of known
results for some previously-defined special cases of this problem.
  Our analysis is conducted under minimal assumptions: we work in a
continuous-time model with possibly infinitely many agents. The general results
we obtain for this model reduce to stronger ones for discrete-time models with
only finitely many agents. In order to distill the properties of such models
that are significant to this reduction, we define several classes of
naturally-occurring models, which in a sense separate the different results. We
present both a more practical optimal solution, as well as a surprisingly
simple condition for solvability, for timely coordinated response under these
models.
  Finally, we show how our results generalize the results known for
previously-studied special cases of this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2037</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2037</id><created>2012-06-10</created><updated>2014-12-27</updated><authors><author><keyname>van Emden</keyname><forenames>M. H.</forenames></author></authors><title>Programming in logic without Prolog</title><categories>cs.LO</categories><comments>This paper has been withdrawn because it is superseded by
  arXiv:1412.3480, &quot;Logic Programming Beyond Prolog&quot;</comments><report-no>DCS-346-IR</report-no><acm-class>D.1.6; D.3.1; F.3.1; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Logic can be made useful for programming and for databases independently of
logic programming. To be useful in this way, logic has to provide a mechanism
for the definition of new functions and new relations on the basis of those
given in the interpretation of a logical theory. We provide this mechanism by
creating a compositional semantics on top of the classical semantics. In this
approach verification of computational results relies on a correspondence
between logic interpretations and a class definition in languages like Java or
C++. The advantage of this approach is the combination of an expressive medium
for the programmer with, in the case of C++, optimal use of computer resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2038</identifier>
 <datestamp>2013-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2038</id><created>2012-06-10</created><updated>2013-10-01</updated><authors><author><keyname>Anh</keyname><forenames>Dinh Tien Tuan</forenames></author><author><keyname>Thanh</keyname><forenames>Quach Vinh</forenames></author><author><keyname>Datta</keyname><forenames>Anwitaman</forenames></author></authors><title>CloudMine: Multi-Party Privacy-Preserving Data Analytics Service</title><categories>cs.CR cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An increasing number of businesses are replacing their data storage and
computation infrastructure with cloud services. Likewise, there is an increased
emphasis on performing analytics based on multiple datasets obtained from
different data sources. While ensuring security of data and computation
outsourced to a third party cloud is in itself challenging, supporting
analytics using data distributed across multiple, independent clouds is even
further from trivial. In this paper we present CloudMine, a cloud-based service
which allows multiple data owners to perform privacy-preserved computation over
the joint data using their clouds as delegates. CloudMine protects data privacy
with respect to semi-honest data owners and semi-honest clouds. It furthermore
ensures the privacy of the computation outputs from the curious clouds. It
allows data owners to reliably detect if their cloud delegates have been lazy
when carrying out the delegated computation. CloudMine can run as a centralized
service on a single cloud, or as a distributed service over multiple,
independent clouds. CloudMine supports a set of basic computations that can be
used to construct a variety of highly complex, distributed privacy-preserving
data analytics. We demonstrate how a simple instance of CloudMine (secure sum
service) is used to implement three classical data mining tasks
(classification, association rule mining and clustering) in a cloud
environment. We experiment with a prototype of the service, the results of
which suggest its practicality for supporting privacy-preserving data analytics
as a (multi) cloud-based service.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2057</identifier>
 <datestamp>2012-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2057</id><created>2012-06-10</created><updated>2012-06-12</updated><authors><author><keyname>Hong</keyname><forenames>Chi-Yao</forenames></author><author><keyname>Caesar</keyname><forenames>Matthew</forenames></author><author><keyname>Godfrey</keyname><forenames>P. Brighten</forenames></author></authors><title>Finishing Flows Quickly with Preemptive Scheduling</title><categories>cs.NI</categories><comments>The conference version was published in SIGCOMM 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today's data centers face extreme challenges in providing low latency.
However, fair sharing, a principle commonly adopted in current congestion
control protocols, is far from optimal for satisfying latency requirements.
  We propose Preemptive Distributed Quick (PDQ) flow scheduling, a protocol
designed to complete flows quickly and meet flow deadlines. PDQ enables flow
preemption to approximate a range of scheduling disciplines. For example, PDQ
can emulate a shortest job first algorithm to give priority to the short flows
by pausing the contending flows. PDQ borrows ideas from centralized scheduling
disciplines and implements them in a fully distributed manner, making it
scalable to today's data centers. Further, we develop a multipath version of
PDQ to exploit path diversity.
  Through extensive packet-level and flow-level simulation, we demonstrate that
PDQ significantly outperforms TCP, RCP and D3 in data center environments. We
further show that PDQ is stable, resilient to packet loss, and preserves nearly
all its performance gains even given inaccurate flow information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2058</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2058</id><created>2012-06-10</created><authors><author><keyname>Shadvar</keyname><forenames>Ali</forenames></author></authors><title>Dimension Reduction by Mutual Information Discriminant Analysis</title><categories>cs.CV cs.IT cs.LG math.IT</categories><comments>13pages, 3 tables, International Journal of Artificial Intelligence &amp;
  Applications</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  In the past few decades, researchers have proposed many discriminant analysis
(DA) algorithms for the study of high-dimensional data in a variety of
problems. Most DA algorithms for feature extraction are based on
transformations that simultaneously maximize the between-class scatter and
minimize the withinclass scatter matrices. This paper presents a novel DA
algorithm for feature extraction using mutual information (MI). However, it is
not always easy to obtain an accurate estimation for high-dimensional MI. In
this paper, we propose an efficient method for feature extraction that is based
on one-dimensional MI estimations. We will refer to this algorithm as mutual
information discriminant analysis (MIDA). The performance of this proposed
method was evaluated using UCI databases. The results indicate that MIDA
provides robust performance over different data sets with different
characteristics and that MIDA always performs better than, or at least
comparable to, the best performing algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2059</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2059</id><created>2012-06-10</created><authors><author><keyname>Vlassis</keyname><forenames>Nikos</forenames></author></authors><title>NP-hardness of polytope M-matrix testing and related problems</title><categories>math.OC cs.CC cs.SY math.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note we prove NP-hardness of the following problem: Given a set of
matrices, is there a convex combination of those that is a nonsingular
M-matrix? Via known characterizations of M-matrices, our result establishes
NP-hardness of several fundamental problems in systems analysis and control,
such as testing the instability of an uncertain dynamical system, and
minimizing the spectral radius of an affine matrix function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2060</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2060</id><created>2012-06-10</created><authors><author><keyname>Miller</keyname><forenames>Daniel B.</forenames></author><author><keyname>Fredkin</keyname><forenames>Edward</forenames></author></authors><title>Circular Motion of Strings in Cellular Automata, and Other Surprises</title><categories>nlin.CG cs.DM</categories><comments>Companion website: http://busyboxes.org</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A two-state, three-dimensional, deterministic, reversible cellular automaton
is shown to be capable of approximately circular orbits, wavelike undulations,
and particle-like configurations that decay in accordance with a half-life law.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2061</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2061</id><created>2012-06-10</created><authors><author><keyname>Celebi</keyname><forenames>M. Emre</forenames></author><author><keyname>Kingravi</keyname><forenames>Hassan A.</forenames></author><author><keyname>Celiker</keyname><forenames>Fatih</forenames></author></authors><title>Comments on &quot;On Approximating Euclidean Metrics by Weighted t-Cost
  Distances in Arbitrary Dimension&quot;</title><categories>cs.NA cs.CV</categories><comments>7 pages, 1 figure, 3 tables. arXiv admin note: substantial text
  overlap with arXiv:1008.4870</comments><acm-class>G.1.2; I.5</acm-class><journal-ref>Pattern Recognition Letters 33 (2012) 1422--1425</journal-ref><doi>10.1016/j.patrec.2012.03.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mukherjee (Pattern Recognition Letters, vol. 32, pp. 824-831, 2011) recently
introduced a class of distance functions called weighted t-cost distances that
generalize m-neighbor, octagonal, and t-cost distances. He proved that weighted
t-cost distances form a family of metrics and derived an approximation for the
Euclidean norm in $\mathbb{Z}^n$. In this note we compare this approximation to
two previously proposed Euclidean norm approximations and demonstrate that the
empirical average errors given by Mukherjee are significantly optimistic in
$\mathbb{R}^n$. We also propose a simple normalization scheme that improves the
accuracy of his approximation substantially with respect to both average and
maximum relative errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2068</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2068</id><created>2012-06-10</created><updated>2013-07-14</updated><authors><author><keyname>Fong</keyname><forenames>Chamberlain</forenames></author></authors><title>Revolvable Indoor Panoramas Using a Rectified Azimuthal Projection</title><categories>cs.CV math.DG</categories><comments>appendix; blended cylindrical projection; included inverse equations</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm for converting an indoor spherical panorama into a
photograph with a simulated overhead view. The resulting image will have an
extremely wide field of view covering up to 4*pi steradians of the spherical
panorama. We argue that our method complements the stereographic projection
commonly used in the &quot;little planet&quot; effect. The stereographic projection works
well in creating little planets of outdoor scenes; whereas our method is a
well-suited counterpart for indoor scenes. The main innovation of our method is
the introduction of a novel azimuthal map projection that can smoothly blend
between the stereographic projection and the Lambert azimuthal equal-area
projection. Our projection has an adjustable parameter that allows one to
control and compromise between distortions in shape and distortions in size
within the projected panorama. This extra control parameter gives our
projection the ability to produce superior results over the stereographic
projection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2082</identifier>
 <datestamp>2013-05-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2082</id><created>2012-06-10</created><updated>2013-05-23</updated><authors><author><keyname>Zadeh</keyname><forenames>Reza Bosagh</forenames></author><author><keyname>Goel</keyname><forenames>Ashish</forenames></author></authors><title>Dimension Independent Similarity Computation</title><categories>cs.DS cs.AI cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a suite of algorithms for Dimension Independent Similarity
Computation (DISCO) to compute all pairwise similarities between very high
dimensional sparse vectors. All of our results are provably independent of
dimension, meaning apart from the initial cost of trivially reading in the
data, all subsequent operations are independent of the dimension, thus the
dimension can be very large. We study Cosine, Dice, Overlap, and the Jaccard
similarity measures. For Jaccard similiarity we include an improved version of
MinHash. Our results are geared toward the MapReduce framework. We empirically
validate our theorems at large scale using data from the social networking site
Twitter. At time of writing, our algorithms are live in production at
twitter.com.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2097</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2097</id><created>2012-06-11</created><authors><author><keyname>Takaguchi</keyname><forenames>Taro</forenames></author><author><keyname>Masuda</keyname><forenames>Naoki</forenames></author><author><keyname>Holme</keyname><forenames>Petter</forenames></author></authors><title>Bursty communication patterns facilitate spreading in a threshold-based
  epidemic dynamics</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>15 pages, 4 figures, 1 table</comments><doi>10.1371/journal.pone.0068629</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Records of social interactions provide us with new sources of data for
understanding how interaction patterns affect collective dynamics. Such human
activity patterns are often bursty, i.e., they consist of short periods of
intense activity followed by long periods of silence. This burstiness has been
shown to affect spreading phenomena; it accelerates epidemic spreading in some
cases and slows it down in other cases. We investigate a model of
history-dependent contagion. In our model, repeated interactions between
susceptible and infected individuals in a short period of time is needed for a
susceptible individual to contract infection. We carry out numerical
simulations on real temporal network data to find that bursty activity patterns
facilitate epidemic spreading in our model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2111</identifier>
 <datestamp>2013-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2111</id><created>2012-06-11</created><updated>2013-05-24</updated><authors><author><keyname>Menton</keyname><forenames>Curtis</forenames></author><author><keyname>Singh</keyname><forenames>Preetjot</forenames></author></authors><title>Manipulation and Control Complexity of Schulze Voting</title><categories>cs.GT</categories><report-no>TR-2012-977</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Schulze voting is a recently introduced voting system enjoying unusual
popularity and a high degree of real-world use, with users including the
Wikimedia foundation, several branches of the Pirate Party, and MTV. It is a
Condorcet voting system that determines the winners of an election using
information about paths in a graph representation of the election. We resolve
the complexity of many electoral control cases for Schulze voting. We find that
it falls short of the best known voting systems in terms of control resistance,
demonstrating vulnerabilities of concern to some prospective users of the
system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2123</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2123</id><created>2012-06-11</created><authors><author><keyname>Schaer</keyname><forenames>Philipp</forenames></author><author><keyname>Mayr</keyname><forenames>Philipp</forenames></author><author><keyname>L&#xfc;ke</keyname><forenames>Thomas</forenames></author></authors><title>Extending Term Suggestion with Author Names</title><categories>cs.IR cs.DL</categories><comments>6 pages; to be published in Proceedings of Theory and Practice of
  Digital Libraries 2012 (TPDL 2012)</comments><doi>10.1007/978-3-642-33290-6_34</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Term suggestion or recommendation modules can help users to formulate their
queries by mapping their personal vocabularies onto the specialized vocabulary
of a digital library. While we examined actual user queries of the social
sciences digital library Sowiport we could see that nearly one third of the
users were explicitly looking for author names rather than terms. Common term
recommenders neglect this fact. By picking up the idea of polyrepresentation we
could show that in a standardized IR evaluation setting we can significantly
increase the retrieval performances by adding topical-related author names to
the query. This positive effect only appears when the query is additionally
expanded with thesaurus terms. By just adding the author names to a query we
often observe a query drift which results in worse results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2126</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2126</id><created>2012-06-11</created><authors><author><keyname>L&#xfc;ke</keyname><forenames>Thomas</forenames></author><author><keyname>Schaer</keyname><forenames>Philipp</forenames></author><author><keyname>Mayr</keyname><forenames>Philipp</forenames></author></authors><title>Improving Retrieval Results with discipline-specific Query Expansion</title><categories>cs.IR cs.DL</categories><comments>6 pages; to be published in Proceedings of Theory and Practice of
  Digital Libraries 2012 (TPDL 2012)</comments><doi>10.1007/978-3-642-33290-6_44</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Choosing the right terms to describe an information need is becoming more
difficult as the amount of available information increases.
Search-Term-Recommendation (STR) systems can help to overcome these problems.
This paper evaluates the benefits that may be gained from the use of STRs in
Query Expansion (QE). We create 17 STRs, 16 based on specific disciplines and
one giving general recommendations, and compare the retrieval performance of
these STRs. The main findings are: (1) QE with specific STRs leads to
significantly better results than QE with a general STR, (2) QE with specific
STRs selected by a heuristic mechanism of topic classification leads to better
results than the general STR, however (3) selecting the best matching specific
STR in an automatic way is a major challenge of this process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2129</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2129</id><created>2012-06-11</created><authors><author><keyname>Zordan</keyname><forenames>Davide</forenames></author><author><keyname>Martinez</keyname><forenames>Borja</forenames></author><author><keyname>Vilajosana</keyname><forenames>Ignasi</forenames></author><author><keyname>Rossi</keyname><forenames>Michele</forenames></author></authors><title>To Compress or Not To Compress: Processing vs Transmission Tradeoffs for
  Energy Constrained Sensor Networking</title><categories>cs.NI cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the past few years, lossy compression has been widely applied in the field
of wireless sensor networks (WSN), where energy efficiency is a crucial concern
due to the constrained nature of the transmission devices. Often, the common
thinking among researchers and implementers is that compression is always a
good choice, because the major source of energy consumption in a sensor node
comes from the transmission of the data. Lossy compression is deemed a viable
solution as the imperfect reconstruction of the signal is often acceptable in
WSN. In this paper, we thoroughly review a number of lossy compression methods
from the literature, and analyze their performance in terms of compression
efficiency, computational complexity and energy consumption. We consider two
different scenarios, namely, wireless and underwater communications, and show
that signal compression may or may not help in the reduction of the overall
energy consumption, depending on factors such as the compression algorithm, the
signal statistics and the hardware characteristics, i.e., micro-controller and
transmission technology. The lesson that we have learned, is that signal
compression may in fact provide some energy savings. However, its usage should
be carefully evaluated, as in quite a few cases processing and transmission
costs are of the same order of magnitude, whereas, in some other cases, the
former may even dominate the latter. In this paper, we show quantitative
comparisons to assess these tradeoffs in the above mentioned scenarios.
Finally, we provide formulas, obtained through numerical fittings, to gauge
computational complexity, overall energy consumption and signal representation
accuracy for the best performing algorithms as a function of the most relevant
system parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2130</identifier>
 <datestamp>2012-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2130</id><created>2012-06-11</created><updated>2012-07-12</updated><authors><author><keyname>Toscani</keyname><forenames>Giuseppe</forenames></author></authors><title>An information-theoretic proof of Nash's inequality</title><categories>cs.IT math-ph math.FA math.IT math.MP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that an information-theoretic property of Shannon's entropy power,
known as concavity of entropy power, can be fruitfully employed to prove
inequalities in sharp form. In particular, the concavity of entropy power
implies the logarithmic Sobolev inequality, and Nash's inequality with the
sharp constant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2131</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2131</id><created>2012-06-11</created><updated>2014-03-27</updated><authors><author><keyname>Li</keyname><forenames>Lvzhou</forenames></author><author><keyname>Feng</keyname><forenames>Yuan</forenames></author></authors><title>On hybrid models of quantum finite automata</title><categories>cs.FL quant-ph</categories><comments>The paper has been revised such that it is more concise and has a
  better structure. All comments are welcome</comments><doi>10.1016/j.jcss.2015.01.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the literature, there exist several quantum finite automata (QFA) models
with both quantum and classical states. These models are of particular
interest,as they show praiseworthy advantages over the fully quantum models in
some nontrivial aspects. This paper characterizes these models in a uniform
framework by proposing a general hybrid model consisting of a quantum component
and a classical one which can interact with each other. The existing hybrid QFA
can be naturally regarded as the general model with specific communication
patterns (classical-quantum, quantum-classical, and two-way, respectively). We
further clarify the relationship between these hybrid QFA and some other
quantum models. In particular, it is shown that hybrid QFA can be simulated
exactly by QFA with quantum operations, which in turn has a close relationship
with two early proposed models: {\it ancialla QFA} and {\it quantum sequential
machines}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2132</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2132</id><created>2012-06-11</created><authors><author><keyname>Li</keyname><forenames>Lei</forenames></author><author><keyname>Chen</keyname><forenames>Tianshi</forenames></author><author><keyname>Chen</keyname><forenames>Yunji</forenames></author><author><keyname>Li</keyname><forenames>Ling</forenames></author><author><keyname>Wu</keyname><forenames>Ruiyang</forenames></author></authors><title>RepTFD: Replay Based Transient Fault Detection</title><categories>cs.AR</categories><comments>22 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The advances in IC process make future chip multiprocessors (CMPs) more and
more vulnerable to transient faults. To detect transient faults, previous
core-level schemes provide redundancy for each core separately. As a result,
they may leave transient faults in the uncore parts, which consume over 50%
area of a modern CMP, escaped from detection. This paper proposes RepTFD, the
first core-level transient fault detection scheme with 100% coverage. Instead
of providing redundancy for each core separately, RepTFD provides redundancy
for a group of cores as a whole. To be specific, it replays the execution of
the checked group of cores on a redundant group of cores. Through comparing the
execution results between the two groups of cores, all malignant transient
faults can be caught. Moreover, RepTFD adopts a novel pending period based
record-replay approach, which can greatly reduce the number of execution orders
that need to be enforced in the replay-run. Hence, RepTFD brings only 4.76%
performance overhead in comparison to the normal execution without
fault-tolerance according to our experiments on the RTL design of an industrial
CMP named Godson-3. In addition, RepTFD only consumes about 0.83% area of
Godson-3, while needing only trivial modifications to existing components of
Godson-3.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2138</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2138</id><created>2012-06-11</created><authors><author><keyname>Kedia</keyname><forenames>Deepak</forenames></author></authors><title>Comparative Analysis of Peak Correlation Characteristics of
  Non-Orthogonal Spreading Codes for Wireless Systems</title><categories>cs.IT math.IT</categories><comments>12 Pages, 8 Figures, 3 Tables</comments><journal-ref>International Journal of Distributed &amp; Parallel Systems (IJDPS)
  Vol. 3, No. 3, May 2012, 63-74</journal-ref><doi>10.5121/ijdps.2012.3307</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance of a CDMA based wireless system is largely dependent on the
characteristics of pseudo-random spreading codes. The spreading codes should be
carefully chosen to ensure highest possible peak value of auto-correlation
function and lower correlation peaks (side-lobes) at non-zero time-shifts.
Simultaneously, zero cross-correlation value at all time shifts is required in
order to eliminate the effect of multiple access interference at the receiver.
But no such code family exists which possess both characteristics
simultaneously. That's why an exhaustive effort has been made in this paper to
evaluate the peak correlation characteristics of various non-orthogonal
spreading codes and suggest a suitable solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2145</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2145</id><created>2012-06-11</created><updated>2012-06-15</updated><authors><author><keyname>Small</keyname><forenames>Michael</forenames></author><author><keyname>Judd</keyname><forenames>Kevin</forenames></author><author><keyname>Stemler</keyname><forenames>Thomas</forenames></author></authors><title>The stability of networks --- towards a structural dynamical systems
  theory</title><categories>nlin.CD cs.SI physics.soc-ph</categories><comments>Draft - based on an extension to NOLTA 2012 submission (22 pages 11
  figures)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The need to build a link between the structure of a complex network and the
dynamical properties of the corresponding complex system (comprised of multiple
low dimensional systems) has recently become apparent. Several attempts to
tackle this problem have been made and all focus on either the controllability
or synchronisability of the network --- usually analyzed by way of the master
stability function, or the graph Laplacian. We take a different approach. Using
the basic tools from dynamical systems theory we show that the dynamical
stability of a network can easily be defined in terms of the eigenvalues of an
homologue of the network adjacency matrix. This allows us to compute the
stability of a network (a quantity derived from the eigenspectrum of the
adjacency matrix). Numerical experiments show that this quantity is very
closely related too, and can even be predicted from, the standard structural
network properties. Following from this we show that the stability of large
network systems can be understood via an analytic study of the eigenvalues of
their fixed points --- even for a very large number of fixed points.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2187</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2187</id><created>2012-06-11</created><authors><author><keyname>Pamies-Juarez</keyname><forenames>Lluis</forenames></author><author><keyname>Oggier</keyname><forenames>Fr&#xe9;d&#xe9;rique</forenames></author><author><keyname>Datta</keyname><forenames>Anwitaman</forenames></author></authors><title>An Empirical Study of the Repair Performance of Novel Coding Schemes for
  Networked Distributed Storage Systems</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Erasure coding techniques are getting integrated in networked distributed
storage systems as a way to provide fault-tolerance at the cost of less storage
overhead than traditional replication. Redundancy is maintained over time
through repair mechanisms, which may entail large network resource overheads.
In recent years, several novel codes tailor-made for distributed storage have
been proposed to optimize storage overhead and repair, such as Regenerating
Codes that minimize the per repair traffic, or Self-Repairing Codes which
minimize the number of nodes contacted per repair. Existing studies of these
coding techniques are however predominantly theoretical, under the simplifying
assumption that only one object is stored. They ignore many practical issues
that real systems must address, such as data placement, de/correlation of
multiple stored objects, or the competition for limited network resources when
multiple objects are repaired simultaneously. This paper empirically studies
the repair performance of these novel storage centric codes with respect to
classical erasure codes by simulating realistic scenarios and exploring the
interplay of code parameters, failure characteristics and data placement with
respect to the trade-offs of bandwidth usage and speed of repairs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2188</identifier>
 <datestamp>2013-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2188</id><created>2012-06-11</created><updated>2013-02-13</updated><authors><author><keyname>Genaim</keyname><forenames>Samir</forenames></author><author><keyname>Zanardini</keyname><forenames>Damiano</forenames></author></authors><title>Reachability-based Acyclicity Analysis by Abstract Interpretation</title><categories>cs.PL</categories><comments>38 pages (included proofs)</comments><msc-class>68Qxx</msc-class><acm-class>D.1.5; D.2.4</acm-class><journal-ref>Theoretical Computer Science, 474(0), pages 60-79, 2013. Elsevier</journal-ref><doi>10.1016/j.tcs.2012.12.018</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In programming languages with dynamic use of memory, such as Java, knowing
that a reference variable x points to an acyclic data structure is valuable for
the analysis of termination and resource usage (e.g., execution time or memory
consumption). For instance, this information guarantees that the depth of the
data structure to which x points is greater than the depth of the data
structure pointed to by x.f for any field f of x. This, in turn, allows
bounding the number of iterations of a loop which traverses the structure by
its depth, which is essential in order to prove the termination or infer the
resource usage of the loop. The present paper provides an
Abstract-Interpretation-based formalization of a static analysis for inferring
acyclicity, which works on the reduced product of two abstract domains:
reachability, which models the property that the location pointed to by a
variable w can be reached by dereferencing another variable v (in this case, v
is said to reach w); and cyclicity, modeling the property that v can point to a
cyclic data structure. The analysis is proven to be sound and optimal with
respect to the chosen abstraction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2190</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2190</id><created>2012-06-11</created><authors><author><keyname>Yan</keyname><forenames>Jian-feng</forenames></author><author><keyname>Liu</keyname><forenames>Zhi-Qiang</forenames></author><author><keyname>Gao</keyname><forenames>Yang</forenames></author><author><keyname>Zeng</keyname><forenames>Jia</forenames></author></authors><title>Communication-Efficient Parallel Belief Propagation for Latent Dirichlet
  Allocation</title><categories>cs.LG</categories><comments>9 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel communication-efficient parallel belief
propagation (CE-PBP) algorithm for training latent Dirichlet allocation (LDA).
Based on the synchronous belief propagation (BP) algorithm, we first develop a
parallel belief propagation (PBP) algorithm on the parallel architecture.
Because the extensive communication delay often causes a low efficiency of
parallel topic modeling, we further use Zipf's law to reduce the total
communication cost in PBP. Extensive experiments on different data sets
demonstrate that CE-PBP achieves a higher topic modeling accuracy and reduces
more than 80% communication cost than the state-of-the-art parallel Gibbs
sampling (PGS) algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2197</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2197</id><created>2012-06-11</created><authors><author><keyname>Fan</keyname><forenames>Rong</forenames></author><author><keyname>Wan</keyname><forenames>Qun</forenames></author><author><keyname>Liu</keyname><forenames>Yipeng</forenames></author><author><keyname>Chen</keyname><forenames>Hui</forenames></author><author><keyname>Zhang</keyname><forenames>Xiao</forenames></author></authors><title>Complex Orthogonal Matching Pursuit and Its Exact Recovery Conditions</title><categories>cs.IT math.IT math.NA stat.ML</categories><comments>18 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present new results on using orthogonal matching pursuit
(OMP), to solve the sparse approximation problem over redundant dictionaries
for complex cases (i.e., complex measurement vector, complex dictionary and
complex additive white Gaussian noise (CAWGN)). A sufficient condition that OMP
can recover the optimal representation of an exactly sparse signal in the
complex cases is proposed both in noiseless and bound Gaussian noise settings.
Similar to exact recovery condition (ERC) results in real cases, we extend them
to complex case and derivate the corresponding ERC in the paper. It leverages
this theory to show that OMP succeed for k-sparse signal from a class of
complex dictionary. Besides, an application with geometrical theory of
diffraction (GTD) model is presented for complex cases. Finally, simulation
experiments illustrate the validity of the theoretical analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2199</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2199</id><created>2012-06-11</created><updated>2012-06-28</updated><authors><author><keyname>Guo</keyname><forenames>Fangjian</forenames></author><author><keyname>Yang</keyname><forenames>Zimo</forenames></author><author><keyname>Zhou</keyname><forenames>Tao</forenames></author></authors><title>Predicting link directions via a recursive subgraph-based ranking</title><categories>physics.soc-ph cs.SI</categories><comments>6 pages, 5 figures; revised arguments for methods section; figures
  replotted; minor revisions</comments><journal-ref>Physica A. Volume 392, Issue 16, 15 August 2013</journal-ref><doi>10.1016/j.physa.2013.03.025</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Link directions are essential to the functionality of networks and their
prediction is helpful towards a better knowledge of directed networks from
incomplete real-world data. We study the problem of predicting the directions
of some links by using the existence and directions of the rest of links. We
propose a solution by first ranking nodes in a specific order and then
predicting each link as stemming from a lower-ranked node towards a
higher-ranked one. The proposed ranking method works recursively by utilizing
local indicators on multiple scales, each corresponding to a subgraph extracted
from the original network. Experiments on real networks show that the
directions of a substantial fraction of links can be correctly recovered by our
method, which outperforms either purely local or global methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2216</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2216</id><created>2012-06-11</created><authors><author><keyname>Grauwin</keyname><forenames>Sebastian</forenames><affiliation>ENS / LIP Laboratoire de l'Informatique du Parall&#xe9;lisme / INRIA Grenoble Rh&#xf4;ne-Alpes, IXXI</affiliation></author><author><keyname>Beslon</keyname><forenames>Guillaume</forenames><affiliation>Insa Lyon / INRIA Grenoble Rh&#xf4;ne-Alpes / UCBL</affiliation></author><author><keyname>Fleury</keyname><forenames>Eric</forenames><affiliation>ENS / LIP Laboratoire de l'Informatique du Parall&#xe9;lisme / INRIA Grenoble Rh&#xf4;ne-Alpes, IXXI, LIP</affiliation></author><author><keyname>Franceschelli</keyname><forenames>Sara</forenames><affiliation>RNSC</affiliation></author><author><keyname>Robardet</keyname><forenames>C&#xe9;line</forenames><affiliation>LIRIS</affiliation></author><author><keyname>Rouquier</keyname><forenames>Jean-Baptiste</forenames><affiliation>ISC-PIF</affiliation></author><author><keyname>Jensen</keyname><forenames>Pablo</forenames><affiliation>IXXI, Phys-ENS</affiliation></author></authors><title>Complex Systems Science: Dreams of Universality, Reality of
  Interdisciplinarity</title><categories>physics.soc-ph cs.DL</categories><comments>Journal of the American Society for Information Science and
  Technology (2012) 10.1002/asi.22644</comments><proxy>ccsd</proxy><doi>10.1002/asi.22644</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using a large database (~ 215 000 records) of relevant articles, we
empirically study the &quot;complex systems&quot; field and its claims to find universal
principles applying to systems in general. The study of references shared by
the papers allows us to obtain a global point of view on the structure of this
highly interdisciplinary field. We show that its overall coherence does not
arise from a universal theory but instead from computational techniques and
fruitful adaptations of the idea of self-organization to specific systems. We
also find that communication between different disciplines goes through
specific &quot;trading zones&quot;, ie sub-communities that create an interface around
specific tools (a DNA microchip) or concepts (a network).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2220</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2220</id><created>2012-06-11</created><authors><author><keyname>Taneja</keyname><forenames>Inder Jeet</forenames></author></authors><title>Representations of Genetic Tables, Bimagic Squares, Hamming Distances
  and Shannon Entropy</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we have established relations of the genetic tables with magic
and bimagic squares. Connections with Hamming distances, binomial coefficients
are established. The idea of Gray code is applied. Shannon entropy of magic
squares of order 4x4, 8x8 and 16x16 are also calculated. Some comparison is
also made. Symmetry among restriction enzymes having four letters is also
studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2248</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2248</id><created>2012-06-11</created><updated>2016-02-03</updated><authors><author><keyname>Krueger</keyname><forenames>Tammo</forenames></author><author><keyname>Panknin</keyname><forenames>Danny</forenames></author><author><keyname>Braun</keyname><forenames>Mikio</forenames></author></authors><title>Fast Cross-Validation via Sequential Testing</title><categories>cs.LG stat.ML</categories><acm-class>I.2.6; I.2.8</acm-class><journal-ref>Journal of Machine Learning Research, 16:1103-1155, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the increasing size of today's data sets, finding the right parameter
configuration in model selection via cross-validation can be an extremely
time-consuming task. In this paper we propose an improved cross-validation
procedure which uses nonparametric testing coupled with sequential analysis to
determine the best parameter set on linearly increasing subsets of the data. By
eliminating underperforming candidates quickly and keeping promising candidates
as long as possible, the method speeds up the computation while preserving the
capability of the full cross-validation. Theoretical considerations underline
the statistical power of our procedure. The experimental evaluation shows that
our method reduces the computation time by a factor of up to 120 compared to a
full cross-validation with a negligible impact on the accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2254</identifier>
 <datestamp>2013-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2254</id><created>2012-06-11</created><updated>2013-05-30</updated><authors><author><keyname>Borradaile</keyname><forenames>Glencora</forenames></author><author><keyname>Eppstein</keyname><forenames>David</forenames></author></authors><title>Near-Linear-Time Deterministic Plane Steiner Spanners and TSP
  Approximation for Well-Spaced Point Sets</title><categories>cs.CG</categories><comments>Appear at the 24th Canadian Conference on Computational Geometry. To
  appear in CGTA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe an algorithm that takes as input n points in the plane and a
parameter {\epsilon}, and produces as output an embedded planar graph having
the given points as a subset of its vertices in which the graph distances are a
(1 + {\epsilon})-approximation to the geometric distances between the given
points. For point sets in which the Delaunay triangulation has bounded sharpest
angle, our algorithm's output has O(n) vertices, its weight is O(1) times the
minimum spanning tree weight, and the algorithm's running time is bounded by
O(n \sqrt{log log n}). We use this result in a similarly fast deterministic
approximation scheme for the traveling salesperson problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2262</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2262</id><created>2012-06-11</created><authors><author><keyname>Bagnoli</keyname><forenames>Franco</forenames></author><author><keyname>Guazzini</keyname><forenames>Andrea</forenames></author><author><keyname>Massaro</keyname><forenames>Emanuele</forenames></author></authors><title>Community-detection cellular automata with local and long-range
  connectivity</title><categories>nlin.CG cs.SI physics.soc-ph</categories><comments>arXiv admin note: text overlap with arXiv:1112.1224</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore a community-detection cellular automata algorithm inspired by
human heuristics, based on information diffusion and a non-linear processing
phase with a dynamics inspired by human heuristics. The main point of the
methods is that of furnishing different &quot;views&quot; of the clustering levels from
an individual point of view. We apply the method to networks with local
connectivity and long-range rewiring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2269</identifier>
 <datestamp>2012-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2269</id><created>2012-06-11</created><updated>2012-07-26</updated><authors><author><keyname>Kapralov</keyname><forenames>Michael</forenames></author></authors><title>Better bounds for matchings in the streaming model</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present improved bounds for approximating maximum matchings
in bipartite graphs in the streaming model. First, we consider the question of
how well maximum matching can be approximated in a single pass over the input
using \tilde O(n)$ space, where $n$ is the number of vertices in the input
graph. Two natural variants of this problem have been considered in the
literature: (1) the edge arrival setting, where edges arrive in the stream and
(2) the vertex arrival setting, where vertices on one side of the graph arrive
in the stream together with all their incident edges. The latter setting has
also been studied extensively in the context of {\em online algorithms}, where
each arriving vertex has to either be matched irrevocably or discarded upon
arrival. In the online setting, the celebrated algorithm of
Karp-Vazirani-Vazirani achieves a $1-1/e$ approximation. Despite the fact that
the streaming model is less restrictive in that the algorithm is not
constrained to match vertices irrevocably upon arrival, the best known
approximation in the streaming model with vertex arrivals and $\tilde O(n)$
space is the same factor of $1-1/e$.
  We show that no single pass streaming algorithm that uses $\tilde O(n)$ space
can achieve a better than $1-1/e$ approximation to maximum matching, even in
the vertex arrival setting. This leads to the striking conclusion that no
single pass streaming algorithm can do better than online algorithms unless it
uses significantly more than $\tilde O(n)$ space. Additionally, our bound
yields the best known impossibility result for approximating matchings in the
{\em edge arrival} model.
  We also give a simple algorithm that achieves approximation ratio
$1-e^{-k}k^{k-1}/(k-1)!=1-\frac1{\sqrt{2\pi k}}+o(1/k)$ in $k$ passes in the
vertex arrival model using linear space, improving upon previously best known
convergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2276</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2276</id><created>2012-06-11</created><authors><author><keyname>Alipour</keyname><forenames>Masoud</forenames></author><author><keyname>Etesami</keyname><forenames>Omid</forenames></author><author><keyname>Maatouk</keyname><forenames>Ghid</forenames></author><author><keyname>Shokrollahi</keyname><forenames>Amin</forenames></author></authors><title>Irregular Product Codes</title><categories>cs.IT math.IT</categories><comments>First draft</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider irregular product codes.In this class of codes, each codeword is
represented by a matrix. The entries in each row (column) of the matrix should
come from a component row (column) code. As opposed to (standard) product
codes, we do not require that all component row codes nor all component column
codes be the same. As we will see, relaxing this requirement can provide some
additional attractive features including 1) allowing some regions of the
codeword be more error-resilient 2) allowing a more refined spectrum of rates
for finite-lengths and improved performance in some of these rates 3) more
interaction between row and column codes during decoding. We study these codes
over erasure channels. We find that for any $0 &lt; \epsilon &lt; 1$, for many rate
distributions on component row codes, there is a matching rate distribution on
component column codes such that an irregular product code based on MDS codes
with those rate distributions on the component codes has asymptotic rate $1 -
\epsilon$ and can decode on erasure channels (of alphabet size equal the
alphabet size of the component MDS codes) with erasure probability $&lt;
\epsilon$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2292</identifier>
 <datestamp>2012-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2292</id><created>2012-06-11</created><updated>2012-06-13</updated><authors><author><keyname>Tabassum</keyname><forenames>Hina</forenames></author><author><keyname>Yilmaz</keyname><forenames>Ferkan</forenames></author><author><keyname>Dawy</keyname><forenames>Zaher</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>An Intercell Interference Model based on Scheduling for Future
  Generation Wireless Networks (Part 1 and Part 2)</title><categories>cs.IT math.IT math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This technical report is divided into two parts. The first part of the
technical report presents a novel framework for modeling the uplink and
downlink intercell interference (ICI) in a multiuser cellular network. The
proposed framework assists in quantifying the impact of various fading channel
models and multiuser scheduling schemes on the uplink and downlink ICI.
Firstly, we derive a semi-analytical expression for the distribution of the
location of the scheduled user in a given cell considering a wide range of
scheduling schemes. Based on this, we derive the distribution and moment
generating function (MGF) of the ICI considering a single interfering cell.
Consequently, we determine the MGF of the cumulative ICI observed from all
interfering cells and derive explicit MGF expressions for three typical fading
models. Finally, we utilize the obtained expressions to evaluate important
network performance metrics such as the outage probability, ergodic capacity
and average fairness numerically. Monte-Carlo simulation results are provided
to demonstrate the efficacy of the derived analytical expressions {\bf The
first part of the technical report is currently submitted to IEEE Transactions
on Wireless Communications}. The second part of the technical report deals with
the statistical modeling of uplink inter-cell interference (ICI) considering
greedy scheduling with power adaptation based on channel conditions. The
derived model is utilized to evaluate important network performance metrics
such as ergodic capacity, average fairness and average power preservation
numerically. In parallel to the literature, we have shown that greedy
scheduling with power adaptation reduces the ICI, average power consumption of
users, and enhances the average fairness among users, compared to the case
without power adaptation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2297</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2297</id><created>2012-06-11</created><authors><author><keyname>Zarrazvand</keyname><forenames>Hamid</forenames></author><author><keyname>Shojafar</keyname><forenames>Mohammad</forenames></author></authors><title>The Use of Fuzzy Cognitive Maps in Analyzing and Implementation of ITIL
  Processes</title><categories>cs.OH</categories><comments>18 Figures, 11 references; IJCSI International Journal of Computer
  Science Issues, Vol. 9, Issue 3, No 3, May 2012, ISSN (Online): 1694-0814</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information Technology Infrastructure Library (ITIL) is series of best
practices that helps Information technology Organizations to provide
Information technology (IT) services for their customers with better
performances and quality. This article is looking for a way to implement ITIL
in an organization and also using Fuzzy Cognitive Maps (FCM) to model the
problem for better understanding of environment. ITIL helps to improve the
performance of IT services in order to gain business objectives and Fuzzy
Cognitive Maps will help to model the problem of needing ITIL processes for
those objectives. First, it defines the concept of FCM and ITIL in two separate
sections and then, it will describe the relationship and the way that FCM helps
to implement ITIL. The paper will measure the cost of service support that is
depended on the metrics like changes Authorization Degree, Process Oriented
activities degree, Response time and Interrupt time. This paper will be used as
a part of gap analyzes step in implementing ITIL in each organizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2307</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2307</id><created>2012-06-11</created><authors><author><keyname>Thakur</keyname><forenames>Manoj Rameshchandra</forenames></author><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author></authors><title>A PAXOS based State Machine Replication System for Anomaly Detection</title><categories>cs.CR</categories><comments>9 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A number of systems in recent times suffer from attacks like DDoS and Ping of
Death. Such attacks result in loss of critical system resources and CPU cycles,
as these compromised systems behave in an abnormal manner. The effect of such
abnormalities is worse in case of compromised systems handling financial
transaction, since it leads to severe monetary losses. In this paper we propose
a system that uses the Replicated State Machine approach to detect abnormality
in system usage. The suggested system is based on PAXOS algorithm, an algorithm
for solving the consensus problem in a network of unreliable processors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2320</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2320</id><created>2012-06-11</created><authors><author><keyname>Ou</keyname><forenames>Yen-Fu</forenames></author><author><keyname>Xue</keyname><forenames>Yuanyi</forenames></author><author><keyname>Wang</keyname><forenames>Yao</forenames></author></authors><title>Q-STAR:A Perceptual Video Quality Model Considering Impact of Spatial,
  Temporal, and Amplitude Resolutions</title><categories>cs.MM</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the impact of spatial, temporal and amplitude
resolution (STAR) on the perceptual quality of a compressed video. Subjective
quality tests were carried out on a mobile device. Seven source sequences are
included in the tests and for each source sequence we have 27 test
configurations generated by JSVM encoder (3 QP levels, 3 spatial resolutions,
and 3 temporal resolutions), resulting a total of 189 processed video sequences
(PVSs). Videos coded at different spatial resolutions are displayed at the full
screen size of the mobile platform. Subjective data reveal that the impact of
spatial resolution (SR), temporal resolution (TR) and quantization stepsize
(QS) can each be captured by a function with a single content-dependent
parameter. The joint impact of SR, TR and QS can be accurately modeled by the
product of these three functions with only three parameters. We further find
that the quality decay rates with SR and QS, respectively are independent of
TR, and likewise, the decay rate with TR is independent of SR and QS,
respectively. However, there is a significant interaction between the effects
of SR and QS. The overall quality model is further validated on five other
datasets with very high accuracy. The complete model correlates well with the
subjective ratings with a Pearson Correlation Coefficient (PCC) of 0.991.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2322</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2322</id><created>2012-06-11</created><authors><author><keyname>Fan</keyname><forenames>Rong</forenames></author><author><keyname>Wan</keyname><forenames>Qun</forenames></author><author><keyname>Zhang</keyname><forenames>Xiao</forenames></author><author><keyname>Chen</keyname><forenames>Hui</forenames></author><author><keyname>Liu</keyname><forenames>Yipeng</forenames></author></authors><title>A Fast HRRP Synthesis Algorithm with Sensing Dictionary in GTD Model</title><categories>cs.IT math.IT</categories><comments>16 pages, 8 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To achieve high range resolution profile (HRRP), the geometric theory of
diffraction (GTD) parametric model is widely used in stepped-frequency radar
system. In the paper, a fast synthetic range profile algorithm, called
orthogonal matching pursuit with sensing dictionary (OMP-SD), is proposed. It
formulates the traditional HRRP synthetic to be a sparse approximation problem
over redundant dictionary. As it employs a priori information that targets are
sparsely distributed in the range space, the synthetic range profile (SRP) can
be accomplished even in presence of data lost. Besides, the computational
complexity is reduced by introducing sensing dictionary (SD) and it mitigates
the model mismatch at the same time. The computation complexity decreases from
O(MNDK) flops for OMP to O(M(N +D)K) flops for OMP-SD. Simulation experiments
illustrate its advantages both in additive white Gaussian noise (AWGN) and
noiseless situation, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2346</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2346</id><created>2012-06-10</created><updated>2015-03-23</updated><authors><author><keyname>Lopez-Sandoval</keyname><forenames>E.</forenames></author><author><keyname>Mello</keyname><forenames>A.</forenames></author><author><keyname>Nava</keyname><forenames>J. J. Godina</forenames></author></authors><title>Power Series Solution to Non-Linear Partial Differential equations of
  Mathematical Physics</title><categories>cs.SC</categories><comments>17 pages, 0 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Power Series Solution method has been used traditionally for to solve Linear
Differential Equations, in Ordinary and Partial form. But this method has been
limited to this kind of problems. We present the solution of problems of Non
Linear Partial Differential equations of Physical Mathematical using power
series.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2347</identifier>
 <datestamp>2012-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2347</id><created>2012-06-11</created><authors><author><keyname>Omri</keyname><forenames>Mohamed Nazih</forenames></author></authors><title>Uncertain and Approximative Knowledge Representation to Reasoning on
  Classification with a Fuzzy Networks Based System</title><categories>cs.AI</categories><comments>arXiv admin note: text overlap with arXiv:1206.1794</comments><journal-ref>The 8th IEEE International Conference on Fuzzy Systems,
  FUZZ-IEEE'99. p. 1632-1637. Seoul. Korea,1999</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The approach described here allows to use the fuzzy Object Based
Representation of imprecise and uncertain knowledge. This representation has a
great practical interest due to the possibility to realize reasoning on
classification with a fuzzy semantic network based system. For instance, the
distinction between necessary, possible and user classes allows to take into
account exceptions that may appear on fuzzy knowledge-base and facilitates
integration of user's Objects in the base. This approach describes the
theoretical aspects of the architecture of the whole experimental A.I. system
we built in order to provide effective on-line assistance to users of new
technological systems: the understanding of &quot;how it works&quot; and &quot;how to complete
tasks&quot; from queries in quite natural languages. In our model, procedural
semantic networks are used to describe the knowledge of an &quot;ideal&quot; expert while
fuzzy sets are used both to describe the approximative and uncertain knowledge
of novice users in fuzzy semantic networks which intervene to match fuzzy
labels of a query with categories from our &quot;ideal&quot; expert.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2362</identifier>
 <datestamp>2012-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2362</id><created>2012-05-28</created><authors><author><keyname>Hirki</keyname><forenames>Mikael</forenames></author></authors><title>Applying Compression to a Game's Network Protocol</title><categories>cs.IT math.IT</categories><comments>7 pages, 2 figures, technical report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report presents the results of applying different compression algorithms
to the network protocol of an online game. The algorithm implementations
compared are zlib, liblzma and my own implementation based on LZ77 and a
variation of adaptive Huffman coding. The comparison data was collected from
the game TomeNET. The results show that adaptive coding is especially useful
for compressing large amounts of very small packets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2369</identifier>
 <datestamp>2012-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2369</id><created>2012-06-11</created><authors><author><keyname>Motter</keyname><forenames>Adilson E.</forenames></author><author><keyname>Albert</keyname><forenames>Reka</forenames></author></authors><title>Networks in Motion</title><categories>physics.soc-ph cond-mat.dis-nn cs.SI nlin.AO q-bio.MN</categories><comments>Review of current research on network dynamics</comments><journal-ref>Physics Today 65(4), 43-48 (2012)</journal-ref><doi>10.1063/PT.3.1518</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature article on how networks that govern communication, growth, herd
behavior, and other key processes in nature and society are becoming
increasingly amenable to modeling, forecast, and control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2372</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2372</id><created>2012-06-11</created><updated>2012-11-18</updated><authors><author><keyname>Orabona</keyname><forenames>Francesco</forenames></author><author><keyname>Argyriou</keyname><forenames>Andreas</forenames></author><author><keyname>Srebro</keyname><forenames>Nathan</forenames></author></authors><title>PRISMA: PRoximal Iterative SMoothing Algorithm</title><categories>math.OC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by learning problems including max-norm regularized matrix
completion and clustering, robust PCA and sparse inverse covariance selection,
we propose a novel optimization algorithm for minimizing a convex objective
which decomposes into three parts: a smooth part, a simple non-smooth Lipschitz
part, and a simple non-smooth non-Lipschitz part. We use a time variant
smoothing strategy that allows us to obtain a guarantee that does not depend on
knowing in advance the total number of iterations nor a bound on the domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2384</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2384</id><created>2012-06-11</created><updated>2013-03-30</updated><authors><author><keyname>Edwards</keyname><forenames>Katherine</forenames></author><author><keyname>King</keyname><forenames>Andrew D.</forenames></author></authors><title>Bounding the fractional chromatic number of $K_\Delta$-free graphs</title><categories>cs.DM math.CO</categories><comments>30 pages, revised edition</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  King, Lu, and Peng recently proved that for $\Delta\geq 4$, any
$K_\Delta$-free graph with maximum degree $\Delta$ has fractional chromatic
number at most $\Delta-\tfrac{2}{67}$ unless it is isomorphic to $C_5\boxtimes
K_2$ or $C_8^2$. Using a different approach we give improved bounds for
$\Delta\geq 6$ and pose several related conjectures. Our proof relies on a
weighted local generalization of the fractional relaxation of Reed's $\omega$,
$\Delta$, $\chi$ conjecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2436</identifier>
 <datestamp>2012-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2436</id><created>2012-06-12</created><updated>2012-07-27</updated><authors><author><keyname>Mathieson</keyname><forenames>Luke</forenames></author></authors><title>A Proof Checking View of Parameterized Complexity</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The PCP Theorem is one of the most stunning results in computational
complexity theory, a culmination of a series of results regarding proof
checking it exposes some deep structure of computational problems. As a
surprising side-effect, it also gives strong non-approximability results. In
this paper we initiate the study of proof checking within the scope of
Parameterized Complexity. In particular we adapt and extend the PCP[n log log
n, n log log n] result of Feige et al. to several parameterized classes, and
discuss some corollaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2437</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2437</id><created>2012-06-12</created><authors><author><keyname>Sahidullah</keyname><forenames>Md.</forenames></author><author><keyname>Saha</keyname><forenames>Goutam</forenames></author></authors><title>A Novel Windowing Technique for Efficient Computation of MFCC for
  Speaker Recognition</title><categories>cs.CV</categories><doi>10.1109/LSP.2012.2235067</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel family of windowing technique to compute
Mel Frequency Cepstral Coefficient (MFCC) for automatic speaker recognition
from speech. The proposed method is based on fundamental property of discrete
time Fourier transform (DTFT) related to differentiation in frequency domain.
Classical windowing scheme such as Hamming window is modified to obtain
derivatives of discrete time Fourier transform coefficients. It has been
mathematically shown that the slope and phase of power spectrum are inherently
incorporated in newly computed cepstrum. Speaker recognition systems based on
our proposed family of window functions are shown to attain substantial and
consistent performance improvement over baseline single tapered Hamming window
as well as recently proposed multitaper windowing technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2445</identifier>
 <datestamp>2012-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2445</id><created>2012-06-12</created><authors><author><keyname>Thiyagarajan</keyname><forenames>P.</forenames></author><author><keyname>Aghila</keyname><forenames>G.</forenames></author><author><keyname>Venkatesan</keyname><forenames>V. Prasanna</forenames></author></authors><title>Pixastic: Steganography based Anti-Phihsing Browser Plug-in</title><categories>cs.CR</categories><comments>Thiyagarajan P, Aghila G, Prasanna Venkatesan V, &quot;Pixastic:
  Steganography based Anti-Phishing Browser Plug-in&quot;, Journal of Internet
  Banking and Commerce, April 2012, Vol. 17, no. 1, ISSN: 1204-5357</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In spite of existence of many standard security mechanisms for ensuring
secure e-Commerce business, users still fall prey for online attacks. One such
simple but powerful attack is 'Phishing'. Phishing is the most alarming threat
in the e-Commerce world and effective anti-phishing technique is the need of
the hour. This paper focuses on a novel anti-phishing browser plug-in which
uses information hiding technique - Steganography. A Robust Message based Image
Steganography (RMIS) algorithm has been proposed. The same has been
incorporated in the form of a browser plug-in (safari) called Pixastic.
Pixastic is tested in an online banking scenario and it is compared with other
well-known anti-phishing plug-in methods in practice. Various parameters such
as robustness, usability and its behavior on various attacks have been
analysed. From experimental results, it is evident that our method Pixastic
performs well compared to other anti-phishing plug-ins.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2448</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2448</id><created>2012-06-12</created><updated>2013-07-22</updated><authors><author><keyname>Gcasior</keyname><forenames>Dariusz</forenames></author><author><keyname>Drwal</keyname><forenames>Maciej</forenames></author></authors><title>Pareto-optimal Nash equilibrium in capacity allocation game for
  self-managed networks</title><categories>cs.GT cs.NI</categories><comments>Computer Networks, 2013</comments><doi>10.1016/j.comnet.2013.06.012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce a capacity allocation game which models the
problem of maximizing network utility from the perspective of distributed
noncooperative agents. Motivated by the idea of self-managed networks, in the
developed framework decision-making entities are associated with individual
transmission links, deciding on the way they split capacity among concurrent
flows. An efficient decentralized algorithm is given for computing strongly
Pareto-optimal strategies, constituting a pure Nash equilibrium. Subsequently,
we discuss the properties of the introduced game related to the Price of
Anarchy and Price of Stability. The paper is concluded with an experimental
study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2459</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2459</id><created>2012-06-12</created><updated>2014-04-24</updated><authors><author><keyname>van Erven</keyname><forenames>Tim</forenames></author><author><keyname>Harremo&#xeb;s</keyname><forenames>Peter</forenames></author></authors><title>R\'enyi Divergence and Kullback-Leibler Divergence</title><categories>cs.IT math.IT math.ST stat.ML stat.TH</categories><comments>To appear in IEEE Transactions on Information Theory</comments><doi>10.1109/TIT.2014.2320500</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  R\'enyi divergence is related to R\'enyi entropy much like Kullback-Leibler
divergence is related to Shannon's entropy, and comes up in many settings. It
was introduced by R\'enyi as a measure of information that satisfies almost the
same axioms as Kullback-Leibler divergence, and depends on a parameter that is
called its order. In particular, the R\'enyi divergence of order 1 equals the
Kullback-Leibler divergence.
  We review and extend the most important properties of R\'enyi divergence and
Kullback-Leibler divergence, including convexity, continuity, limits of
$\sigma$-algebras and the relation of the special order 0 to the Gaussian
dichotomy and contiguity. We also show how to generalize the Pythagorean
inequality to orders different from 1, and we extend the known equivalence
between channel capacity and minimax redundancy to continuous channel inputs
(for all orders) and present several other minimax results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2465</identifier>
 <datestamp>2012-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2465</id><created>2012-06-12</created><updated>2012-06-26</updated><authors><author><keyname>Singer</keyname><forenames>Kristiina</forenames></author><author><keyname>Singer</keyname><forenames>Georg</forenames></author><author><keyname>Lepik</keyname><forenames>Krista</forenames></author><author><keyname>Norbisrath</keyname><forenames>Ulrich</forenames></author><author><keyname>Pruulmann-Vengerfeldt</keyname><forenames>Pille</forenames></author></authors><title>Search Strategies of Library Search Experts</title><categories>cs.IR cs.DL</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Search engines like Google, Yahoo or Bing are an excellent support for
finding documents, but this strength also imposes a limitation. As they are
optimized for document retrieval tasks, they perform less well when it comes to
more complex search needs. Complex search tasks are usually described as
open-ended, abstract and poorly defined information needs with a multifaceted
character. In this paper we will present the results of an experiment carried
out with information professionals from libraries and museums in the course of
a search contest. The aim of the experiment was to analyze the search
strategies of experienced information workers trying to tackle search tasks of
varying complexity and get qualitative results on the impact of time pressure
on such an experiment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2478</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2478</id><created>2012-06-12</created><authors><author><keyname>Ivanov</keyname><forenames>Mikhail</forenames></author><author><keyname>Brannstrom</keyname><forenames>Fredrik</forenames></author><author><keyname>Alvarado</keyname><forenames>Alex</forenames></author><author><keyname>Agrell</keyname><forenames>Erik</forenames></author></authors><title>On the Exact BER of Bit-Wise Demodulators for One-Dimensional
  Constellations</title><categories>cs.IT math.IT</categories><journal-ref>IEEE Transactions on Communications, vol. 61, no. 4, pp.
  1450-1459, Apr. 2013</journal-ref><doi>10.1109/TCOMM.2013.13.120401</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The optimal bit-wise demodulator for M-ary pulse amplitude modulation (PAM)
over the additive white Gaussian noise channel is analyzed in terms of uncoded
bit-error rate (BER). New closed-form BER expressions for 4-PAM with any
labeling are developed. Moreover, closed-form BER expressions for 11 out of 23
possible bit patterns for 8-PAM are presented, which enable us to obtain the
BER for 8-PAM with some of the most popular labelings, including the binary
reflected Gray code and the natural binary code. Numerical results show that,
regardless of the labeling, there is no difference between the optimal
demodulator and the symbol-wise demodulator for any BER of practical interest
(below 0.1).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2484</identifier>
 <datestamp>2012-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2484</id><created>2012-06-12</created><authors><author><keyname>Singh</keyname><forenames>Puneet</forenames></author><author><keyname>Kapoor</keyname><forenames>Ashutosh</forenames></author><author><keyname>Kaushik</keyname><forenames>Vishal</forenames></author><author><keyname>Maringanti</keyname><forenames>Hima Bindu</forenames></author></authors><title>Architecture for Automated Tagging and Clustering of Song Files
  According to Mood</title><categories>cs.IR cs.MM</categories><comments>7 pages</comments><journal-ref>IJCSI Volume 7, Issue 4, No 2, pp 11-17, July 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Music is one of the basic human needs for recreation and entertainment. As
song files are digitalized now a days, and digital libraries are expanding
continuously, which makes it difficult to recall a song. Thus need of a new
classification system other than genre is very obvious and mood based
classification system serves the purpose very well. In this paper we will
present a well-defined architecture to classify songs into different mood-based
categories, using audio content analysis, affective value of song lyrics to map
a song onto a psychological-based emotion space and information from online
sources. In audio content analysis we will use music features such as
intensity, timbre and rhythm including their subfeatures to map music in a
2-Dimensional emotional space. In lyric based classification 1-Dimensional
emotional space is used. Both the results are merged onto a 2-Dimensional
emotional space, which will classify song into a particular mood category.
Finally clusters of mood based song files are formed and arranged according to
data acquired from various Internet sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2491</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2491</id><created>2012-06-12</created><updated>2013-06-03</updated><authors><author><keyname>Venkataramanan</keyname><forenames>Ramji</forenames></author><author><keyname>Tatikonda</keyname><forenames>Sekhar</forenames></author><author><keyname>Lastras</keyname><forenames>Luis</forenames></author><author><keyname>Franceschini</keyname><forenames>Michele</forenames></author></authors><title>Rewritable storage channels with hidden state</title><categories>cs.IT math.IT</categories><comments>10 pages. Part of the paper appeared in the proceedings of the 2012
  IEEE International Symposium on Information Theory</comments><journal-ref>IEEE Journal on Selected Areas in Communications, vol. 32, no. 5,
  pp. 815-824, May 2014</journal-ref><doi>10.1109/JSAC.2014.140502</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many storage channels admit reading and rewriting of the content at a given
cost. We consider rewritable channels with a hidden state which models the
unknown characteristics of the memory cell. In addition to mitigating the
effect of the write noise, rewrites can help the write controller obtain a
better estimate of the hidden state. The paper has two contributions. The first
is a lower bound on the capacity of a general rewritable channel with hidden
state. The lower bound is obtained using a coding scheme that combines
Gelfand-Pinsker coding with superposition coding. The rewritable AWGN channel
is discussed as an example. The second contribution is a simple coding scheme
for a rewritable channel where the write noise and hidden state are both
uniformly distributed. It is shown that this scheme is asymptotically optimal
as the number of rewrites gets large.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2497</identifier>
 <datestamp>2012-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2497</id><created>2012-06-12</created><authors><author><keyname>Lampis</keyname><forenames>Michael</forenames></author></authors><title>Improved Inapproximability for TSP</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Traveling Salesman Problem is one of the most studied problems in
computational complexity and its approximability has been a long standing open
question. Currently, the best known inapproximability threshold known is
220/219 due to Papadimitriou and Vempala. Here, using an essentially different
construction and also relying on the work of Berman and Karpinski on bounded
occurrence CSPs, we give an alternative and simpler inapproximability proof
which improves the bound to 185/184.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2500</identifier>
 <datestamp>2012-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2500</id><created>2012-06-12</created><authors><author><keyname>Sundar</keyname><forenames>R. Shyam</forenames></author><author><keyname>Kumar</keyname><forenames>S. Nanda</forenames></author></authors><title>Performance Improvement of Heterogeneous Wireless Networks using
  Modified Newton Method</title><categories>cs.NI</categories><doi>10.5121/ijsea.2012.330</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The heterogeneous wireless networks where coexistence of different Radio
access technology (RAT) are widely deployed for various services and support
various traffic demand, channel allocation. Under heterogeneous wireless
networks, a user can send data through a single or multi RATs simultaneous. The
objective of this paper is to choose the optimal bandwidth for the services and
power allocation to that bandwidth. The proposed distributed joint allocation
algorithm using modified Newton method is adopted to maximize the total system
capacity. We validate the performance of the proposed algorithm through
numerical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2510</identifier>
 <datestamp>2012-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2510</id><created>2012-06-12</created><authors><author><keyname>Novak</keyname><forenames>David</forenames></author><author><keyname>Volny</keyname><forenames>Petr</forenames></author><author><keyname>Zezula</keyname><forenames>Pavel</forenames></author></authors><title>Generic Subsequence Matching Framework: Modularity, Flexibility,
  Efficiency</title><categories>cs.MM cs.DS cs.IR</categories><comments>This is an extended version of a paper published on DEXA 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Subsequence matching has appeared to be an ideal approach for solving many
problems related to the fields of data mining and similarity retrieval. It has
been shown that almost any data class (audio, image, biometrics, signals) is or
can be represented by some kind of time series or string of symbols, which can
be seen as an input for various subsequence matching approaches. The variety of
data types, specific tasks and their partial or full solutions is so wide that
the choice, implementation and parametrization of a suitable solution for a
given task might be complicated and time-consuming; a possibly fruitful
combination of fragments from different research areas may not be obvious nor
easy to realize. The leading authors of this field also mention the
implementation bias that makes difficult a proper comparison of competing
approaches. Therefore we present a new generic Subsequence Matching Framework
(SMF) that tries to overcome the aforementioned problems by a uniform frame
that simplifies and speeds up the design, development and evaluation of
subsequence matching related systems. We identify several relatively separate
subtasks solved differently over the literature and SMF enables to combine them
in straightforward manner achieving new quality and efficiency. This framework
can be used in many application domains and its components can be reused
effectively. Its strictly modular architecture and openness enables also
involvement of efficient solutions from different fields, for instance
efficient metric-based indexes. This is an extended version of a paper
published on DEXA 2012.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2517</identifier>
 <datestamp>2013-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2517</id><created>2012-06-12</created><updated>2013-10-24</updated><authors><author><keyname>Qin</keyname><forenames>Xiangju</forenames></author><author><keyname>Cunningham</keyname><forenames>P&#xe1;draig</forenames></author></authors><title>Assessing the Quality of Wikipedia Pages Using Edit Longevity and
  Contributor Centrality</title><categories>cs.SI cs.CY</categories><comments>9 pages</comments><journal-ref>The 23rd Irish Conference on Artificial Intelligence and Cognitive
  Science (AICS 2012), p.p. 3--11</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address the challenge of assessing the quality of Wikipedia
pages using scores derived from edit contribution and contributor
authoritativeness measures. The hypothesis is that pages with significant
contributions from authoritative contributors are likely to be high-quality
pages. Contributions are quantified using edit longevity measures and
contributor authoritativeness is scored using centrality metrics in either the
Wikipedia talk or co-author networks. The results suggest that it is useful to
take into account the contributor authoritativeness when assessing the
information quality of Wikipedia content. The percentile visualization of the
quality scores provides some insights about the anomalous articles, and can be
used to help Wikipedia editors to identify Start and Stub articles that are of
relatively good quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2520</identifier>
 <datestamp>2012-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2520</id><created>2012-06-12</created><authors><author><keyname>Salinesi</keyname><forenames>Camille</forenames><affiliation>CRI</affiliation></author><author><keyname>Triki</keyname><forenames>Raouia</forenames><affiliation>CRI</affiliation></author><author><keyname>Mazo</keyname><forenames>Raul</forenames><affiliation>CRI</affiliation></author></authors><title>Combining configuration and recommendation to define an interactive
  product line configuration approach</title><categories>cs.OH</categories><comments>arXiv admin note: text overlap with arXiv:1108.5586 by other authors</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is interested in e-commerce for complex configurable
products/systems. In e-commerce, satisfying the customer needs is a vital
concern. One particular way to achieve this is to offer customers a panel of
options among which they can select their preferred ones. While solution
exists, they are not adapted for highly complex configurable systems such as
product lines. This paper proposes an approach that combines two complementary
forms of guidance: configuration and recommendation, to help customers define
their own products out of a product line specification. The proposed approach,
called interactive configuration supports the combination by organizing the
configuration process in a series of partial configurations where decisions are
made by the recommendation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2523</identifier>
 <datestamp>2013-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2523</id><created>2012-06-12</created><updated>2013-05-31</updated><authors><author><keyname>Badkobeh</keyname><forenames>Golnaz</forenames></author><author><keyname>Fici</keyname><forenames>Gabriele</forenames></author><author><keyname>Kroon</keyname><forenames>Steve</forenames></author><author><keyname>Lipt&#xe1;k</keyname><forenames>Zsuzsanna</forenames></author></authors><title>Binary Jumbled String Matching for Highly Run-Length Compressible Texts</title><categories>cs.DS cs.IR</categories><comments>v2: only small cosmetic changes; v3: new title, weakened conjectures
  on size of Corner Index (we no longer conjecture it to be always linear in
  size of RLE); removed experimental part on random strings (these are valid
  but limited in their predictive power w.r.t. general strings); v3 published
  in IPL</comments><msc-class>68W32, 68P05, 68P20</msc-class><acm-class>G.2.1</acm-class><journal-ref>Information Processing Letters, 113: 604-608 (2013)</journal-ref><doi>10.1016/j.ipl.2013.05.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Binary Jumbled String Matching problem is defined as: Given a string $s$
over $\{a,b\}$ of length $n$ and a query $(x,y)$, with $x,y$ non-negative
integers, decide whether $s$ has a substring $t$ with exactly $x$ $a$'s and $y$
$b$'s. Previous solutions created an index of size O(n) in a pre-processing
step, which was then used to answer queries in constant time. The fastest
algorithms for construction of this index have running time $O(n^2/\log n)$
[Burcsi et al., FUN 2010; Moosa and Rahman, IPL 2010], or $O(n^2/\log^2 n)$ in
the word-RAM model [Moosa and Rahman, JDA 2012]. We propose an index
constructed directly from the run-length encoding of $s$. The construction time
of our index is $O(n+\rho^2\log \rho)$, where O(n) is the time for computing
the run-length encoding of $s$ and $\rho$ is the length of this encoding---this
is no worse than previous solutions if $\rho = O(n/\log n)$ and better if $\rho
= o(n/\log n)$. Our index $L$ can be queried in $O(\log \rho)$ time. While
$|L|= O(\min(n, \rho^{2}))$ in the worst case, preliminary investigations have
indicated that $|L|$ may often be close to $\rho$. Furthermore, the algorithm
for constructing the index is conceptually simple and easy to implement. In an
attempt to shed light on the structure and size of our index, we characterize
it in terms of the prefix normal forms of $s$ introduced in [Fici and Lipt\'ak,
DLT 2011].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2526</identifier>
 <datestamp>2012-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2526</id><created>2012-06-12</created><updated>2012-11-28</updated><authors><author><keyname>King</keyname><forenames>Emily J.</forenames></author><author><keyname>Kutyniok</keyname><forenames>Gitta</forenames></author><author><keyname>Zhuang</keyname><forenames>Xiaosheng</forenames></author></authors><title>Analysis of Inpainting via Clustered Sparsity and Microlocal Analysis</title><categories>math.FA cs.IT math.IT math.NA</categories><comments>49 pages, 9 Figures</comments><msc-class>41A65, 68P30, 68U10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, compressed sensing techniques in combination with both wavelet and
directional representation systems have been very effectively applied to the
problem of image inpainting. However, a mathematical analysis of these
techniques which reveals the underlying geometrical content is completely
missing. In this paper, we provide the first comprehensive analysis in the
continuum domain utilizing the novel concept of clustered sparsity, which
besides leading to asymptotic error bounds also makes the superior behavior of
directional representation systems over wavelets precise. First, we propose an
abstract model for problems of data recovery and derive error bounds for two
different recovery schemes, namely l_1 minimization and thresholding. Second,
we set up a particular microlocal model for an image governed by edges inspired
by seismic data as well as a particular mask to model the missing data, namely
a linear singularity masked by a horizontal strip. Applying the abstract
estimate in the case of wavelets and of shearlets we prove that -- provided the
size of the missing part is asymptotically to the size of the analyzing
functions -- asymptotically precise inpainting can be obtained for this model.
Finally, we show that shearlets can fill strictly larger gaps than wavelets in
this model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2528</identifier>
 <datestamp>2012-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2528</id><created>2012-06-12</created><authors><author><keyname>Singer</keyname><forenames>Georg</forenames></author><author><keyname>Norbisrath</keyname><forenames>Ulrich</forenames></author><author><keyname>Lewandowski</keyname><forenames>Dirk</forenames></author></authors><title>Ordinary Search Engine Users assessing Difficulty, Effort, and Outcome
  for Simple and Complex Search Tasks</title><categories>cs.IR</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Search engines are the preferred tools for finding information on the Web.
They are advancing to be the common helpers to answer any of our search needs.
We use them to carry out simple look-up tasks and also to work on rather time
consuming and more complex search tasks. Yet, we do not know very much about
the user performance while carrying out those tasks -- especially not for
ordinary users. The aim of this study was to get more insight into whether Web
users manage to assess difficulty, time effort, query effort, and task outcome
of search tasks, and if their judging performance relates to task complexity.
Our study was conducted with a systematically selected sample of 56 people with
a wide demographic background. They carried out a set of 12 search tasks with
commercial Web search engines in a laboratory environment. The results confirm
that it is hard for normal Web users to judge the difficulty and effort to
carry out complex search tasks. The judgments are more reliable for simple
tasks than for complex ones. Task complexity is an indicator for judging
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2534</identifier>
 <datestamp>2013-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2534</id><created>2012-06-11</created><updated>2012-06-16</updated><authors><author><keyname>Mingesz</keyname><forenames>R.</forenames></author><author><keyname>Kish</keyname><forenames>L. B.</forenames></author><author><keyname>Gingl</keyname><forenames>Z.</forenames></author><author><keyname>Granqvist</keyname><forenames>C. G.</forenames></author><author><keyname>Wen</keyname><forenames>H.</forenames></author><author><keyname>Peper</keyname><forenames>F.</forenames></author><author><keyname>Eubank</keyname><forenames>T.</forenames></author><author><keyname>Schmera</keyname><forenames>G.</forenames></author></authors><title>Information theoretic security by the laws of classical physics</title><categories>cs.ET physics.class-ph</categories><comments>Featured in MIT Technology Review
  http://www.technologyreview.com/view/428202/quantum-cryptography-outperformed-by-classical/
  ; Plenary talk at the 5th IEEE Workshop on Soft Computing Applications,
  August 22-24, 2012, (SOFA 2012). Typos corrected</comments><journal-ref>in: V.E. Balas et al. (Eds.): Soft Computing Applications, AISC
  195, pp. 11-25. (Springer, 2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been shown recently that the use of two pairs of resistors with
enhanced Johnson-noise and a Kirchhoff-loop-i.e., a Kirchhoff-Law-Johnson-Noise
(KLJN) protocol-for secure key distribution leads to information theoretic
security levels superior to those of a quantum key distribution, including a
natural immunity against a man-in-the-middle attack. This issue is becoming
particularly timely because of the recent full cracks of practical quantum
communicators, as shown in numerous peer-reviewed publications. This
presentation first briefly surveys the KLJN system and then discusses related,
essential questions such as: what are perfect and imperfect security
characteristics of key distribution, and how can these two types of securities
be unconditional (or information theoretical)? Finally the presentation
contains a live demonstration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2542</identifier>
 <datestamp>2012-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2542</id><created>2012-06-12</created><authors><author><keyname>Fister</keyname><forenames>Iztok</forenames><suffix>Jr</suffix></author><author><keyname>Mernik</keyname><forenames>Marjan</forenames></author><author><keyname>Fister</keyname><forenames>Iztok</forenames></author><author><keyname>Hrn&#x10d;i&#x10d;</keyname><forenames>Dejan</forenames></author></authors><title>Implementation of the Domain-Specific Language EasyTime using a LISA
  Compiler Generator</title><categories>cs.PL</categories><comments>arXiv admin note: substantial text overlap with arXiv:1206.1969</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A manually time-measuring tool in mass sporting competitions cannot be
imagined nowadays because many modern disciplines, such as IronMan, take a long
time and, therefore, demand additional reliability. Moreover, automatic timing
devices, based on RFID technology, have become cheaper. However, these devices
cannot operate stand-alone because they need a computer measuring system that
is capable of processing the incoming events, encoding the results, assigning
them to the correct competitor, sorting the results according to the achieved
times, and then providing a printout of the results. In this article, the
domain-specific language EasyTime is presented, which enables the controlling
of an agent by writing the events in a database. In particular, we are focused
on the implementation of EasyTime with a LISA tool that enables the automatic
construction of compilers from language specifications using Attribute
Grammars. By using of EasyTime, we can also decrease the number of measuring
devices. Furthermore, EasyTime is universal and can be applied to many
different sporting competitions in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2544</identifier>
 <datestamp>2012-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2544</id><created>2012-06-12</created><authors><author><keyname>Mahmood</keyname><forenames>Shah</forenames></author><author><keyname>Nazar</keyname><forenames>Ismatullah</forenames></author></authors><title>Education in Conflict Zones: a Web and Mobility Approach</title><categories>cs.CY cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new framework for education in conflict zones, considering the
explosive growth of social media, web services, and mobile Internet over the
past decade. Moreover, we focus on one conflict zone, Afghanistan, as a case
study, because of its alarmingly high illiteracy rate, lack of qualified
teachers, rough terrain, and relatively high mobile penetration of over 50%. In
several of Afghanistan's provinces, it is hard to currently sustain the
traditional bricks-and-mortar school model, due to numerous incidents of
schools, teachers, and students being attacked because of the ongoing
insurgency and political instability. Our model improves the virtual school
model, by addressing most of its disadvantages, to provide students in
Afghanistan with an opportunity to achieve standardised education, even when
the security situation does not allow them to attend traditional schools. One
of the biggest advantages of this model is that it is sufficiently robust to
deal with gender discrimination, imposed by culture or politics of the region.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2550</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2550</id><created>2012-06-12</created><updated>2012-06-15</updated><authors><author><keyname>Liu</keyname><forenames>Yang-Yu</forenames></author><author><keyname>Cs&#xf3;ka</keyname><forenames>Endre</forenames></author><author><keyname>Zhou</keyname><forenames>Haijun</forenames></author><author><keyname>P&#xf3;sfai</keyname><forenames>M&#xe1;rton</forenames></author></authors><title>Core percolation on complex networks</title><categories>cond-mat.stat-mech cs.SI physics.soc-ph</categories><comments>17 pages, 6 figures</comments><journal-ref>Phys. Rev. Lett. 109, 205703 (2012)</journal-ref><doi>10.1103/PhysRevLett.109.205703</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As a fundamental structural transition in complex networks, core percolation
is related to a wide range of important problems. Yet, previous theoretical
studies of core percolation have been focusing on the classical
Erd\H{o}s-R\'enyi random networks with Poisson degree distribution, which are
quite unlike many real-world networks with scale-free or fat-tailed degree
distributions. Here we show that core percolation can be analytically studied
for complex networks with arbitrary degree distributions. We derive the
condition for core percolation and find that purely scale-free networks have no
core for any degree exponents. We show that for undirected networks if core
percolation occurs then it is always continuous while for directed networks it
becomes discontinuous when the in- and out-degree distributions are different.
We also apply our theory to real-world directed networks and find,
surprisingly, that they often have much larger core sizes as compared to random
models. These findings would help us better understand the interesting
interplay between the structural and dynamical properties of complex networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2568</identifier>
 <datestamp>2012-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2568</id><created>2012-06-12</created><authors><author><keyname>Viderman</keyname><forenames>Michael</forenames></author></authors><title>LP decoding of expander codes: a simpler proof</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A code $C \subseteq \F_2^n$ is a $(c,\epsilon,\delta)$-expander code if it
has a Tanner graph, where every variable node has degree $c$, and every subset
of variable nodes $L_0$ such that $|L_0|\leq \delta n$ has at least $\epsilon c
|L_0|$ neighbors. Feldman et al. (IEEE IT, 2007) proved that LP decoding
corrects $\frac{3\epsilon-2}{2\epsilon-1} \cdot (\delta n-1)$ errors of
$(c,\epsilon,\delta)$-expander code, where $\epsilon &gt; 2/3+\frac{1}{3c}$. In
this paper, we provide a simpler proof of their result and show that this
result holds for every expansion parameter $\epsilon &gt; 2/3$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2583</identifier>
 <datestamp>2012-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2583</id><created>2012-06-12</created><authors><author><keyname>Thiyagarajan</keyname><forenames>P.</forenames></author><author><keyname>Aghila</keyname><forenames>G.</forenames></author><author><keyname>Venkatesan</keyname><forenames>V. Prasanna</forenames></author></authors><title>Dynamic Pattern Based Image Steganography</title><categories>cs.CR</categories><comments>Journal of Computing ISSN 2151-9617, Volume 3, Issue 2 February 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Steganography is the art of hiding secret information in media such as image,
audio and video. The purpose of steganography is to conceal the existence of
the secret information in any given medium. This work aims at strengthening the
security in steganography algorithm by generating dynamic pattern in selection
of indicator sequence. In addition to this dynamicity is also encompassed in
number of bits embedded in data channel. This technique has been implemented
and the results have been compared and evaluated with existing similar
techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2586</identifier>
 <datestamp>2012-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2586</id><created>2012-06-12</created><authors><author><keyname>Thiyagarajan</keyname><forenames>P.</forenames></author><author><keyname>Aghila</keyname><forenames>G.</forenames></author><author><keyname>Venkatesan</keyname><forenames>V. Prasanna</forenames></author></authors><title>Stego-Image Generator (SIG) - Building Steganography Image Database</title><categories>cs.CR</categories><comments>First International Conference on Digital Image Processing and
  Pattern Recognition, Springer (LNCS) in Communications in Computer and
  Information Science (CCIS) Series, ISSN: 1865:0929, September 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Any Universal Steganalysis algorithm developed should be tested with various
stego-images to prove its efficiency. This work is aimed to build the
stego-image database which is obtained by implementing various RGB Least
Significant Bit Steganographic algorithms. Though there are many stego-images
sources available on the internet it lacks in the information such as how many
rows has been infected by the steganography algorithms, how many bits have been
modified and which channel has been affected. These parameters are important
for Steganalysis algorithms and it helps to rate its efficiency. Images are
chosen from board categories such as animals, nature, person to produce variety
of Stego-Image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2587</identifier>
 <datestamp>2012-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2587</id><created>2012-04-03</created><authors><author><keyname>Fliss</keyname><forenames>Imtiez</forenames></author><author><keyname>Tagina</keyname><forenames>Moncef</forenames></author></authors><title>Exploiting Particle Swarm Optimization in Multiple Faults Fuzzy
  Detection</title><categories>cs.NE cs.SY</categories><comments>Extended version of : Fliss I. and Tagina M., Multiple faults fuzzy
  detection approach improved by Particle Swarm Optimization, published in The
  8th International Conference of Modelling and Simulation - MOSIM'10,
  Hammamet, Tunisia, May 10-12, 2010; Journal of Computing, Volume 4, Issue 2,
  February 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper an on-line multiple faults detection approach is first of all
proposed. For efficiency, an optimal design of membership functions is
required. Thus, the proposed approach is improved using Particle Swarm
Optimization (PSO) technique. The inputs of the proposed approaches are
residuals representing the numerical evaluation of Analytical Redundancy
Relations. These residuals are generated due to the use of bond graph modeling.
The results of the fuzzy detection modules are displayed as a colored causal
graph. A comparison between the results obtained by using PSO and those given
by the use of Genetic Algorithms (GA) is finally made. The experiments focus on
a simulation of the three-tank hydraulic system, a benchmark in the diagnosis
domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2597</identifier>
 <datestamp>2012-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2597</id><created>2012-06-12</created><authors><author><keyname>Susanto</keyname><forenames>Heru</forenames></author><author><keyname>Almunawar</keyname><forenames>Mohammad Nabil</forenames></author></authors><title>Information Security Awareness Within Business Environment: An IT Review</title><categories>cs.OH</categories><comments>FBEPS- AGBEP PhD Colloquium, 5-6 June 2012 ASEAN University Network</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The beauty of Information Technology (IT) is with its multifunction nature;
it is a support system, a networking system, a storage system, as well as an
information facilitator. Aided with their broad line of services, an IT system
aims to support or even drive organizations towards desired paths. Trends of IT
and information security awareness (ISA) in society today, particularly within
the business environment is quite interesting phenomenon. The overviews of the
role of IT in the modern world as well as the perception towards ISA are
initially introduced. A series of scope are outlined, and also further
examination on matter of IT and ISA in the business environment-emphasis on
revolution of business with ISA, security threats such as identity thefts,
hacking and web harassment, and the different mode of protections that are
applied in different business environments. Unfortunately, the advancement of
IT is not followed by the awareness of its security issues properly, especially
in the context of the business settings and functions. This research and review
is expected to influence the awareness of information security issues in
business processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2599</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2599</id><created>2012-06-12</created><authors><author><keyname>von Ferber</keyname><forenames>C.</forenames></author><author><keyname>Berche</keyname><forenames>B.</forenames></author><author><keyname>Holovatch</keyname><forenames>T.</forenames></author><author><keyname>Holovatch</keyname><forenames>Yu.</forenames></author></authors><title>A tale of two cities. Vulnerabilities of the London and Paris transit
  networks</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>16 pages, 6 figures, style files included</comments><journal-ref>J. Transp. Secur. 5 (2012) 199-216</journal-ref><doi>10.1007/s12198-012-0092-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper analyses the impact of random failure or attack on the public
transit networks of London and Paris in a comparative study. In particular we
analyze how the dysfunction or removal of sets of stations or links (rails,
roads, etc.) affects the connectivity properties within these networks. We show
how accumulating dysfunction leads to emergent phenomena that cause the
transportation system to break down as a whole. Simulating different directed
attack strategies, we find minimal strategies with high impact and identify
a-priory criteria that correlate with the resilience of these networks. To
demonstrate our approach, we choose the London and Paris public transit
networks. Our quantitative analysis is performed in the frames of the complex
network theory - a methodological tool that has emerged recently as an
interdisciplinary approach joining methods and concepts of the theory of random
graphs, percolation, and statistical physics. In conclusion we demonstrate that
taking into account cascading effects the network integrity is controlled for
both networks by less than 0.5 % of the stations i.e. 19 for Paris and 34 for
London.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2610</identifier>
 <datestamp>2012-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2610</id><created>2012-06-12</created><updated>2012-09-21</updated><authors><author><keyname>Ciss</keyname><forenames>Abdoul Aziz</forenames></author><author><keyname>Cheikh</keyname><forenames>Ahmed Youssef Ould</forenames></author></authors><title>An Efficient Signature Scheme based on Factoring and Discrete Logarithm</title><categories>cs.CR math.NT</categories><comments>There are some mistakes in the paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new signature scheme based on two hard problems : the
cube root extraction modulo a composite moduli (which is equivalent to the
factorisation of the moduli, IFP) and the discrete logarithm problem(DLP). By
combining these two cryptographic assumptions, we introduce an efficient and
strongly secure signature scheme. We show that if an adversary can break the
new scheme with an algorithm $\mathcal{A},$ then $\mathcal{A}$ can be used to
sove both the DLP and the IFP. The key generation is a simple operation based
on the discrete logarithm modulo a composite moduli. The signature phase is
based both on the cube root computation and the DLP. These operations are
computationally efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2625</identifier>
 <datestamp>2012-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2625</id><created>2012-06-12</created><authors><author><keyname>Ma</keyname><forenames>Zhan</forenames></author><author><keyname>Hu</keyname><forenames>Hao</forenames></author><author><keyname>Xu</keyname><forenames>Meng</forenames></author><author><keyname>Wang</keyname><forenames>Yao</forenames></author></authors><title>Rate Model for Compressed Video Considering Impacts Of Spatial, Temporal
  and Amplitude Resolutions and Its Applications for Video Coding and
  Adaptation</title><categories>cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the impacts of spatial, temporal and amplitude
resolution (STAR) on the bit rate of a compressed video. We propose an
analytical rate model in terms of the quantization stepsize, frame size and
frame rate. Experimental results reveal that the increase of the video rate as
the individual resolution increases follows a power function. Hence, the
proposed model expresses the rate as the product of power functions of the
quantization stepsize, frame size and frame rate, respectively. The proposed
rate model is analytically tractable, requiring only four content dependent
parameters. We also propose methods for predicting the model parameters from
content features that can be computed from original video. Simulation results
show that model predicted rates fit the measured data very well with high
Pearson correlation (PC) and small relative root mean square error (RRMSE). The
same model function works for different coding scenarios (including scalable
and non-scalable video, temporal prediction using either hierarchical B or IPPP
structure, etc.) with very high accuracy (average PC $&gt;$ 0.99), but the values
of model parameters differ. Using the proposed rate model and the quality model
introduced in a separate work, we show how to optimize the STAR for a given
rate constraint, which is important for both encoder rate control and scalable
video adaptation. Furthermore, we demonstrate how to order the spatial,
temporal and amplitude layers of a scalable video in a rate-quality optimized
way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2627</identifier>
 <datestamp>2013-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2627</id><created>2012-06-12</created><updated>2013-05-07</updated><authors><author><keyname>Guha</keyname><forenames>Tanaya</forenames></author><author><keyname>Ward</keyname><forenames>Rabab K.</forenames></author></authors><title>Image Similarity Using Sparse Representation and Compression Distance</title><categories>cs.CV</categories><comments>submitted journal draft</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new line of research uses compression methods to measure the similarity
between signals. Two signals are considered similar if one can be compressed
significantly when the information of the other is known. The existing
compression-based similarity methods, although successful in the discrete one
dimensional domain, do not work well in the context of images. This paper
proposes a sparse representation-based approach to encode the information
content of an image using information from the other image, and uses the
compactness (sparsity) of the representation as a measure of its
compressibility (how much can the image be compressed) with respect to the
other image. The more sparse the representation of an image, the better it can
be compressed and the more it is similar to the other image. The efficacy of
the proposed measure is demonstrated through the high accuracies achieved in
image clustering, retrieval and classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2656</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2656</id><created>2012-06-12</created><updated>2013-12-17</updated><authors><author><keyname>Couvreur</keyname><forenames>Alain</forenames></author><author><keyname>Delfosse</keyname><forenames>Nicolas</forenames></author><author><keyname>Z&#xe9;mor</keyname><forenames>Gilles</forenames></author></authors><title>A Construction of Quantum LDPC Codes from Cayley Graphs</title><categories>cs.IT math.CO math.IT</categories><comments>The material in this paper was presented in part at ISIT 2011. This
  article is published in IEEE Transactions on Information Theory. We point out
  that the second step of the proof of Proposition VI.2 in the published
  version (Proposition 25 in the present version and Proposition 18 in the ISIT
  extended abstract) is not strictly correct. This issue is addressed in the
  present version</comments><msc-class>94C15, 05C99, 94B99</msc-class><journal-ref>IEEE Trans. Inform. Theory. 59(9). 6087-6098. 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a construction of Quantum LDPC codes proposed by MacKay, Mitchison
and Shokrollahi. It is based on the Cayley graph of Fn together with a set of
generators regarded as the columns of the parity-check matrix of a classical
code. We give a general lower bound on the minimum distance of the Quantum code
in $\mathcal{O}(dn^2)$ where d is the minimum distance of the classical code.
When the classical code is the $[n, 1, n]$ repetition code, we are able to
compute the exact parameters of the associated Quantum code which are $[[2^n,
2^{\frac{n+1}{2}}, 2^{\frac{n-1}{2}}]]$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2657</identifier>
 <datestamp>2015-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2657</id><created>2012-06-12</created><updated>2013-04-11</updated><authors><author><keyname>Jung</keyname><forenames>Taeho</forenames></author><author><keyname>Li</keyname><forenames>Xiang-Yang</forenames></author><author><keyname>Wan</keyname><forenames>Zhiguo</forenames></author><author><keyname>Wan</keyname><forenames>Meng</forenames></author></authors><title>AnonyControl: Control Cloud Data Anonymously with Multi-Authority
  Attribute-Based Encryption</title><categories>cs.CR</categories><comments>9 pages, 6 figures, 3 tables, conference, IEEE INFOCOM 2013</comments><journal-ref>INFOCOM, 2013 Proceedings IEEE (pp. 2625-2633). IEEE</journal-ref><doi>10.1109/INFCOM.2013.6567070</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Cloud computing is a revolutionary computing paradigm which enables flexible,
on-demand and low-cost usage of computing resources. However, those advantages,
ironically, are the causes of security and privacy problems, which emerge
because the data owned by different users are stored in some cloud servers
instead of under their own control. To deal with security problems, various
schemes based on the Attribute- Based Encryption (ABE) have been proposed
recently. However, the privacy problem of cloud computing is yet to be solved.
This paper presents an anonymous privilege control scheme AnonyControl to
address the user and data privacy problem in a cloud. By using multiple
authorities in cloud computing system, our proposed scheme achieves anonymous
cloud data access, finegrained privilege control, and more importantly,
tolerance to up to (N -2) authority compromise. Our security and performance
analysis show that AnonyControl is both secure and efficient for cloud
computing environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2660</identifier>
 <datestamp>2015-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2660</id><created>2012-06-12</created><updated>2013-04-11</updated><authors><author><keyname>Jung</keyname><forenames>Taeho</forenames></author><author><keyname>Mao</keyname><forenames>XuFei</forenames></author><author><keyname>Li</keyname><forenames>Xiang-Yang</forenames></author><author><keyname>Tang</keyname><forenames>Shaojie</forenames></author><author><keyname>Gong</keyname><forenames>Wei</forenames></author><author><keyname>Zhang</keyname><forenames>Lan</forenames></author></authors><title>Data Aggregation without Secure Channel: How to Evaluate a Multivariate
  Polynomial Securely</title><categories>cs.CR</categories><comments>9 pages, 3 figures, 5 tables, conference, IEEE INFOCOM 2013</comments><journal-ref>INFOCOM, 2013 Proceedings IEEE, pp. 2634-2642. IEEE, 2013</journal-ref><doi>10.1109/INFCOM.2013.6567071</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Much research has been conducted to securely outsource multiple parties' data
aggregation to an untrusted aggregator without disclosing each individual's
data, or to enable multiple parties to jointly aggregate their data while
preserving privacy. However, those works either assume to have a secure channel
or suffer from high complexity. Here we consider how an external aggregator or
multiple parties learn some algebraic statistics (e.g., summation, product)
over participants' data while any individual's input data is kept secret to
others (the aggregator and other participants). We assume channels in our
construction are insecure. That is, all channels are subject to eavesdropping
attacks, and all the communications throughout the aggregation are open to
others. We successfully guarantee data confidentiality under this weak
assumption while limiting both the communication and computation complexity to
at most linear.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2669</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2669</id><created>2012-06-12</created><updated>2013-02-04</updated><authors><author><keyname>Wang</keyname><forenames>Ye</forenames></author><author><keyname>Ishwar</keyname><forenames>Prakash</forenames></author><author><keyname>Rane</keyname><forenames>Shantanu</forenames></author></authors><title>Information-Theoretically Secure Three-Party Computation with One
  Corrupted Party</title><categories>cs.CR cs.IT math.IT</categories><comments>7 pages, 1 figure, submitted to ISIT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem in which one of three pairwise interacting parties is required to
securely compute a function of the inputs held by the other two, when one party
may arbitrarily deviate from the computation protocol (active behavioral
model), is studied. An information-theoretic characterization of
unconditionally secure computation protocols under the active behavioral model
is provided. A protocol for Hamming distance computation is provided and shown
to be unconditionally secure under both active and passive behavioral models
using the information-theoretic characterization. The difference between the
notions of security under the active and passive behavioral models is
illustrated through the BGW protocol for computing quadratic and Hamming
distances; this protocol is secure under the passive model, but is shown to be
not secure under the active model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2691</identifier>
 <datestamp>2012-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2691</id><created>2012-06-12</created><authors><author><keyname>Sindhu</keyname><forenames>Muddassar A.</forenames></author><author><keyname>Meinke</keyname><forenames>Karl</forenames></author></authors><title>IDS: An Incremental Learning Algorithm for Finite Automata</title><categories>cs.LG cs.DS cs.FL</categories><comments>8 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new algorithm IDS for incremental learning of deterministic
finite automata (DFA). This algorithm is based on the concept of distinguishing
sequences introduced in (Angluin81). We give a rigorous proof that two versions
of this learning algorithm correctly learn in the limit. Finally we present an
empirical performance analysis that compares these two algorithms, focussing on
learning times and different types of learning queries. We conclude that IDS is
an efficient algorithm for software engineering applications of automata
learning, such as testing and model inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2720</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2720</id><created>2012-06-13</created><updated>2012-09-23</updated><authors><author><keyname>Valdez</keyname><forenames>L. D.</forenames></author><author><keyname>Macri</keyname><forenames>P. A.</forenames></author><author><keyname>Braunstein</keyname><forenames>L. A.</forenames></author></authors><title>Temporal percolation of the susceptible network in an epidemic spreading</title><categories>physics.soc-ph cs.SI</categories><comments>Published in PLoS ONE</comments><journal-ref>PLoS ONE 7(9): e44188 (2012)</journal-ref><doi>10.1371/journal.pone.0044188</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we study the evolution of the susceptible individuals during
the spread of an epidemic modeled by the susceptible-infected-recovered (SIR)
process spreading on the top of complex networks. Using an edge-based
compartmental approach and percolation tools, we find that a time-dependent
quantity $\Phi_S(t)$, namely, the probability that a given neighbor of a node
is susceptible at time $t$, is the control parameter of a node void percolation
process involving those nodes on the network not-reached by the disease. We
show that there exists a critical time $t_c$ above which the giant susceptible
component is destroyed. As a consequence, in order to preserve a macroscopic
connected fraction of the network composed by healthy individuals which
guarantee its functionality, any mitigation strategy should be implemented
before this critical time $t_c$. Our theoretical results are confirmed by
extensive simulations of the SIR process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2728</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2728</id><created>2012-06-13</created><updated>2012-06-14</updated><authors><author><keyname>Toyota</keyname><forenames>Norihito</forenames></author></authors><title>Effect of Closed Paths in Complex networks on Six Degrees of Separation
  and Disorder</title><categories>physics.soc-ph cs.SI</categories><comments>14pages, 12 figures, Fig 4 is added correctly</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Milgram Condition proposed by Aoyama et al. plays an important role on the
analysis of &quot;six degrees of separation&quot;. We have shown that the relations
between Milgram condition and the generalized clustering coefficient, which was
introduced as an index for measuring the number of closed paths by us, are
absolutely different in scale free networks (Barabasi and Albert) and small
world networks (Watts and Strogatz, Watts). This fact implies that the effect
of closed paths on information propagation is different in both networks. In
this article, we first investigate the difference and pursuit what is a crucial
mathematical quantity for information propagation. As a result we find that a
sort of &quot;disorder&quot; plays more important role for information propagation than
partially closed paths included in a network. Next we inquired into it in more
detail by introducing two types of intermediate networks. Then we find that the
average of the local clustering coefficient and the generalized clustering
coefficients $C_{(q)}$ have some different functions and important meanings,
respectively. We also find that $C_{(q)}$ is close to the propagation of
information on networks. Lastly, we show that realizability of six degrees of
separation in networks can be understood in a unified way by disorder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2733</identifier>
 <datestamp>2012-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2733</id><created>2012-06-13</created><authors><author><keyname>Toyota</keyname><forenames>Norihito</forenames></author></authors><title>Parrondo Paradox in Scale Free Networks</title><categories>physics.soc-ph cs.SI</categories><comments>4 pages, 4 figures, appear in Proceedings of ITC-CSCC2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parrondo's paradox occurs in sequences of games in which a winning
expectation may be obtained by playing the games in a random order, even though
each game in the sequence may be lost when played individually. Several
variations of Parrondo's games with paradoxical property have been introduced.
In this paper, I examine whether Parrondo's paradox occurs or not in scale free
networks. Two models are discussed by some theoretical analyses and computer
simulations. As a result, I prove that Parrondo's paradox occurs only in the
second model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2742</identifier>
 <datestamp>2012-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2742</id><created>2012-06-13</created><authors><author><keyname>Nielsen</keyname><forenames>Finn &#xc5;rup</forenames></author><author><keyname>Kempton</keyname><forenames>Matthew J.</forenames></author><author><keyname>Williams</keyname><forenames>Steven C. R.</forenames></author></authors><title>Online open neuroimaging mass meta-analysis</title><categories>cs.DL cs.AI stat.AP</categories><comments>5 pages, 4 figures SePublica 2012, ESWC 2012 Workshop, 28 May 2012,
  Heraklion, Greece</comments><msc-class>68U35</msc-class><acm-class>H.5.4; J.3; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a system for meta-analysis where a wiki stores numerical data in
a simple format and a web service performs the numerical computation.
  We initially apply the system on multiple meta-analyses of structural
neuroimaging data results. The described system allows for mass meta-analysis,
e.g., meta-analysis across multiple brain regions and multiple mental
disorders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2772</identifier>
 <datestamp>2014-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2772</id><created>2012-06-13</created><updated>2014-07-29</updated><authors><author><keyname>D'Angelo</keyname><forenames>Gabriele</forenames></author><author><keyname>Ferretti</keyname><forenames>Stefano</forenames></author><author><keyname>Marzolla</keyname><forenames>Moreno</forenames></author></authors><title>Time Warp on the Go (Updated Version)</title><categories>cs.DC</categories><comments>Proceedings of 3nd ICST/CREATE-NET Workshop on DIstributed SImulation
  and Online gaming (DISIO 2012). In conjunction with SIMUTools 2012.
  Desenzano, Italy, March 2012. ISBN: 978-1-936968-47-3</comments><doi>10.4108/icst.simutools.2012.247736</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we deal with the impact of multi and many-core processor
architectures on simulation. Despite the fact that modern CPUs have an
increasingly large number of cores, most softwares are still unable to take
advantage of them. In the last years, many tools, programming languages and
general methodologies have been proposed to help building scalable applications
for multi-core architectures, but those solutions are somewhat limited.
Parallel and distributed simulation is an interesting application area in which
efficient and scalable multi-core implementations would be desirable. In this
paper we investigate the use of the Go Programming Language to implement
optimistic parallel simulations based on the Time Warp mechanism. Specifically,
we describe the design, implementation and evaluation of a new parallel
simulator. The scalability of the simulator is studied when in presence of a
modern multi-core CPU and the effects of the Hyper-Threading technology on
optimistic simulation are analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2774</identifier>
 <datestamp>2014-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2774</id><created>2012-06-13</created><updated>2014-07-30</updated><authors><author><keyname>Ferretti</keyname><forenames>Stefano</forenames></author><author><keyname>D'Angelo</keyname><forenames>Gabriele</forenames></author></authors><title>Mobile Online Gaming via Resource Sharing</title><categories>cs.NI</categories><comments>Proceedings of 3nd ICST/CREATE-NET Workshop on DIstributed SImulation
  and Online gaming (DISIO 2012). In conjunction with SIMUTools 2012.
  Desenzano, Italy, March 2012. ISBN: 978-1-936968-47-3</comments><doi>10.4108/icst.simutools.2012.247720</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile gaming presents a number of main issues which remain open. These are
concerned mainly with connectivity, computational capacities, memory and
battery constraints. In this paper, we discuss the design of a fully
distributed approach for the support of mobile Multiplayer Online Games (MOGs).
In mobile environments, several features might be exploited to enable resource
sharing among multiple devices / game consoles owned by different mobile users.
We show the advantages of trading computing / networking facilities among
mobile players. This operation mode opens a wide number of interesting sharing
scenarios, thus promoting the deployment of novel mobile online games. In
particular, once mobile nodes make their resource available for the community,
it becomes possible to distribute the software modules that compose the game
engine. This allows to distribute the workload for the game advancement
management. We claim that resource sharing is in unison with the idea of ludic
activity that is behind MOGs. Hence, such schemes can be profitably employed in
these contexts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2775</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2775</id><created>2012-06-13</created><updated>2014-07-24</updated><authors><author><keyname>Toscano</keyname><forenames>Luca</forenames></author><author><keyname>D'Angelo</keyname><forenames>Gabriele</forenames></author><author><keyname>Marzolla</keyname><forenames>Moreno</forenames></author></authors><title>Parallel Discrete Event Simulation with Erlang</title><categories>cs.DC</categories><comments>Proceedings of ACM SIGPLAN Workshop on Functional High-Performance
  Computing (FHPC 2012) in conjunction with ICFP 2012. ISBN: 978-1-4503-1577-7</comments><acm-class>D.1.3; I.6.8</acm-class><doi>10.1145/2364474.2364487</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discrete Event Simulation (DES) is a widely used technique in which the state
of the simulator is updated by events happening at discrete points in time
(hence the name). DES is used to model and analyze many kinds of systems,
including computer architectures, communication networks, street traffic, and
others. Parallel and Distributed Simulation (PADS) aims at improving the
efficiency of DES by partitioning the simulation model across multiple
processing elements, in order to enabling larger and/or more detailed studies
to be carried out. The interest on PADS is increasing since the widespread
availability of multicore processors and affordable high performance computing
clusters. However, designing parallel simulation models requires considerable
expertise, the result being that PADS techniques are not as widespread as they
could be. In this paper we describe ErlangTW, a parallel simulation middleware
based on the Time Warp synchronization protocol. ErlangTW is entirely written
in Erlang, a concurrent, functional programming language specifically targeted
at building distributed systems. We argue that writing parallel simulation
models in Erlang is considerably easier than using conventional programming
languages. Moreover, ErlangTW allows simulation models to be executed either on
single-core, multicore and distributed computing architectures. We describe the
design and prototype implementation of ErlangTW, and report some preliminary
performance results on multicore and distributed architectures using the well
known PHOLD benchmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2802</identifier>
 <datestamp>2012-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2802</id><created>2012-06-13</created><authors><author><keyname>Tilles</keyname><forenames>P. F. C.</forenames></author><author><keyname>Fontanari</keyname><forenames>J. F.</forenames></author></authors><title>Critical behavior in a cross-situational lexicon learning scenario</title><categories>physics.soc-ph cond-mat.stat-mech cs.AI</categories><journal-ref>EPL, 99 (2012) 60001</journal-ref><doi>10.1209/0295-5075/99/60001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The associationist account for early word-learning is based on the
co-occurrence between objects and words. Here we examine the performance of a
simple associative learning algorithm for acquiring the referents of words in a
cross-situational scenario affected by noise produced by out-of-context words.
We find a critical value of the noise parameter $\gamma_c$ above which learning
is impossible. We use finite-size scaling to show that the sharpness of the
transition persists across a region of order $\tau^{-1/2}$ about $\gamma_c$,
where $\tau$ is the number of learning trials, as well as to obtain the
learning error (scaling function) in the critical region. In addition, we show
that the distribution of durations of periods when the learning error is zero
is a power law with exponent -3/2 at the critical point.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2807</identifier>
 <datestamp>2012-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2807</id><created>2012-06-13</created><authors><author><keyname>Guimar&#xe3;es</keyname><forenames>Silvio Jamil F.</forenames></author><author><keyname>Cousty</keyname><forenames>Jean</forenames></author><author><keyname>Kenmochi</keyname><forenames>Yukiko</forenames></author><author><keyname>Najman</keyname><forenames>Laurent</forenames></author></authors><title>An efficient hierarchical graph based image segmentation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hierarchical image segmentation provides region-oriented scalespace, i.e., a
set of image segmentations at different detail levels in which the
segmentations at finer levels are nested with respect to those at coarser
levels. Most image segmentation algorithms, such as region merging algorithms,
rely on a criterion for merging that does not lead to a hierarchy, and for
which the tuning of the parameters can be difficult. In this work, we propose a
hierarchical graph based image segmentation relying on a criterion popularized
by Felzenzwalb and Huttenlocher. We illustrate with both real and synthetic
images, showing efficiency, ease of use, and robustness of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2812</identifier>
 <datestamp>2012-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2812</id><created>2012-06-13</created><authors><author><keyname>Kreeft</keyname><forenames>Jasper</forenames></author><author><keyname>Gerritsma</keyname><forenames>Marc</forenames></author></authors><title>A priori error estimates for compatible spectral discretization of the
  Stokes problem for all admissible boundary conditions</title><categories>cs.NA math.NA physics.flu-dyn</categories><msc-class>76D07, 65N30, 65M70, 12Y05, 13P20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the recently developed mixed mimetic spectral element
method for the Stokes problem in the vorticity-velocity-pressure formulation.
This compatible discretization method relies on the construction of a
conforming discrete Hodge decomposition, that is based on a bounded projection
operator that commutes with the exterior derivative. The projection operator is
the composition of a reduction and a reconstruction step. The reconstruction in
terms of mimetic spectral element basis-functions are tensor-based
constructions and therefore hold for curvilinear quadrilateral and hexahedral
meshes. For compatible discretization methods that contain a conforming
discrete Hodge decomposition, we derive optimal a priori error estimates which
are valid for all admissible boundary conditions on both Cartesian and
curvilinear meshes. These theoretical results are confirmed by numerical
experiments. These clearly show that the mimetic spectral elements outperform
the commonly used H(div)-compatible Raviart-Thomas elements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2847</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2847</id><created>2012-06-13</created><updated>2013-07-07</updated><authors><author><keyname>Friot</keyname><forenames>Nicolas</forenames></author><author><keyname>Guyeux</keyname><forenames>Christophe</forenames></author><author><keyname>Bahi</keyname><forenames>Jacques M.</forenames></author></authors><title>Topological study and Lyapunov exponent of a secure steganographic
  scheme</title><categories>cs.CR cs.MM math.DS math.GN</categories><comments>This paper has been withdrawn because of it's acceptation in the
  conference SECRYPT'2013. (See my profile on Research Gate or the conference
  web site). ******* http://www.researchgate.net/profile/Nicolas_Friot/
  ******** http://www.secrypt.icete.org/ ********</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  CIS2 is a steganographic scheme proposed in the information hiding
literature, belonging into the small category of algorithms being both stego
and topologically secure. Due to its stego-security, this scheme is able to
face attacks that take place into the &quot;watermark only attack&quot; framework. Its
topological security reinforce its capability to face attacks in other
frameworks as &quot;known message attack&quot; or &quot;known original attack&quot;, in the
Simmons' prisoner problem. In this research work, the study of topological
properties of C I S 2 is enlarged by describing this scheme as iterations over
the real line, and investigating other security properties of topological
nature as the Lyapunov exponent. Results show that this scheme is able to
withdraw a malicious attacker in the &quot;estimated original attack&quot; context too.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2866</identifier>
 <datestamp>2012-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2866</id><created>2012-06-03</created><authors><author><keyname>Yamaguchi</keyname><forenames>Osamu</forenames></author><author><keyname>Roy</keyname><forenames>Soumen</forenames></author><author><keyname>D'Souza</keyname><forenames>Raissa M.</forenames></author></authors><title>Efficient scheduling using complex networks</title><categories>physics.soc-ph cs.CE cs.SY</categories><comments>4 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of efficiently scheduling the production of goods for
a model steel manufacturing company. We propose a new approach for solving this
classic problem, using techniques from the statistical physics of complex
networks in conjunction with depth-first search to generate a successful,
flexible, schedule. The schedule generated by our algorithm is more efficient
and outperforms schedules selected at random from those observed in real steel
manufacturing processes. Finally, we explore whether the proposed approach
could be beneficial for long term planning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2878</identifier>
 <datestamp>2012-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2878</id><created>2012-06-13</created><authors><author><keyname>Benthall</keyname><forenames>Sebastian</forenames></author><author><keyname>Chuang</keyname><forenames>John</forenames></author></authors><title>Computational Asymmetry in Strategic Bayesian Networks</title><categories>cs.GT cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Among the strategic choices made by today's economic actors are choices about
algorithms and computational resources. Different access to computational
resources may result in a kind of economic asymmetry analogous to information
asymmetry. In order to represent strategic computational choices within a game
theoretic framework, we propose a new game specification, Strategic Bayesian
Networks (SBN). In an SBN, random variables are represented as nodes in a
graph, with edges indicating probabilistic dependence. For some nodes, players
can choose conditional probability distributions as a strategic choice. Using
SBN, we present two games that demonstrate computational asymmetry. These games
are symmetric except for the computational limitations of the actors. We show
that the better computationally endowed player receives greater payoff.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2893</identifier>
 <datestamp>2012-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2893</id><created>2012-06-13</created><authors><author><keyname>Shayda</keyname><forenames>Dara O</forenames></author></authors><title>Decomposition of Kolmogorov Complexity And Link To Geometry</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A link between Kolmogorov Complexity and geometry is uncovered. A similar
concept of projection and vector decomposition is described for Kolmogorov
Complexity. By using a simple approximation to the Kolmogorov Complexity, coded
in Mathematica, the derived formulas are tested and used to study the geometry
of Light Cone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2898</identifier>
 <datestamp>2012-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2898</id><created>2012-06-13</created><updated>2012-11-20</updated><authors><author><keyname>Riascos</keyname><forenames>A. P.</forenames></author><author><keyname>Mateos</keyname><forenames>Jos&#xe9; L.</forenames></author></authors><title>Long-Range Navigation on Complex Networks using L\'evy Random Walks</title><categories>cond-mat.stat-mech cs.SI physics.soc-ph</categories><comments>6 pages, 3 figures</comments><journal-ref>Phys. Rev. E 86, 056110 (2012)</journal-ref><doi>10.1103/PhysRevE.86.056110</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a strategy of navigation in undirected networks, including
regular, random, and complex networks, that is inspired by L\'evy random walks,
generalizing previous navigation rules. We obtained exact expressions for the
stationary probability distribution, the occupation probability, the mean first
passage time, and the average time to reach a node on the network. We found
that the long-range navigation using the L\'evy random walk strategy, compared
with the normal random walk strategy, is more efficient at reducing the time to
cover the network. The dynamical effect of using the L\'evy walk strategy is to
transform a large-world network into a small world. Our exact results provide a
general framework that connects two important fields: L\'evy navigation
strategies and dynamics on complex networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2914</identifier>
 <datestamp>2012-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2914</id><created>2012-06-12</created><authors><author><keyname>Thiyagarajan</keyname><forenames>P.</forenames></author><author><keyname>Aghila</keyname><forenames>G.</forenames></author><author><keyname>Venkatesan</keyname><forenames>V. Prasanna</forenames></author></authors><title>Steganalysis Using Color Model Conversion</title><categories>cs.CR</categories><comments>arXiv admin note: substantial text overlap with arXiv:1206.2586</comments><journal-ref>Thiyagarajan P, Aghila G, Prasanna Venkatesan V, &quot;Steganalysis
  using Color Model Conversion&quot;, Signal and Image Processing: An International
  Journal (SIPIJ) ISSN 0976-710x Volume 2, No 4 December 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The major threat in cyber crime for digital forensic examiner is to identify,
analyze and interpret the concealed information inside digital medium such as
image, audio and video. There are strong indications that hiding information
inside digital medium has been used for planning criminal activities. In this
way, it is important to develop a steganalysis technique which detects the
existence of hidden messages inside digital medium. This paper focuses on
universal image steganalysis method which uses RGB to HSI colour model
conversion. Any Universal Steganalysis algorithm developed should be tested
with various stego-images to prove its efficiency. The developed Universal
Steganalysis algorithm is tested in stego-image database which is obtained by
implementing various RGB Least Significant Bit Steganographic algorithms.
Though there are many stego-image sources available on the internet it lacks in
the information such as how many rows has been infected by the steganography
algorithms, how many bits have been modified and which channel has been
affected. These parameters are important for Steganalysis algorithms and it
helps to rate its efficiency. Proposed Steganalysis using Colour Model has been
tested with our Image Database and the results were affirmative.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2944</identifier>
 <datestamp>2012-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2944</id><created>2012-06-13</created><updated>2012-08-29</updated><authors><author><keyname>Snoek</keyname><forenames>Jasper</forenames></author><author><keyname>Larochelle</keyname><forenames>Hugo</forenames></author><author><keyname>Adams</keyname><forenames>Ryan P.</forenames></author></authors><title>Practical Bayesian Optimization of Machine Learning Algorithms</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine learning algorithms frequently require careful tuning of model
hyperparameters, regularization terms, and optimization parameters.
Unfortunately, this tuning is often a &quot;black art&quot; that requires expert
experience, unwritten rules of thumb, or sometimes brute-force search. Much
more appealing is the idea of developing automatic approaches which can
optimize the performance of a given learning algorithm to the task at hand. In
this work, we consider the automatic tuning problem within the framework of
Bayesian optimization, in which a learning algorithm's generalization
performance is modeled as a sample from a Gaussian process (GP). The tractable
posterior distribution induced by the GP leads to efficient use of the
information gathered by previous experiments, enabling optimal choices about
what parameters to try next. Here we show how the effects of the Gaussian
process prior and the associated inference procedure can have a large impact on
the success or failure of Bayesian optimization. We show that thoughtful
choices can lead to results that exceed expert-level performance in tuning
machine learning algorithms. We also describe new algorithms that take into
account the variable cost (duration) of learning experiments and that can
leverage the presence of multiple cores for parallel experimentation. We show
that these proposed algorithms improve on previous automatic procedures and can
reach or surpass human expert-level optimization on a diverse set of
contemporary algorithms including latent Dirichlet allocation, structured SVMs
and convolutional neural networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2948</identifier>
 <datestamp>2012-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2948</id><created>2012-06-13</created><authors><author><keyname>Collier</keyname><forenames>Nathan</forenames></author><author><keyname>Dalcin</keyname><forenames>Lisandro</forenames></author><author><keyname>Pardo</keyname><forenames>David</forenames></author><author><keyname>Calo</keyname><forenames>V. M.</forenames></author></authors><title>The cost of continuity: performance of iterative solvers on isogeometric
  finite elements</title><categories>cs.NA math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study how the use of a more continuous set of basis
functions affects the cost of solving systems of linear equations resulting
from a discretized Galerkin weak form. Specifically, we compare performance of
linear solvers when discretizing using $C^0$ B-splines, which span traditional
finite element spaces, and $C^{p-1}$ B-splines, which represent maximum
continuity. We provide theoretical estimates for the increase in cost of the
matrix-vector product as well as for the construction and application of
black-box preconditioners. We accompany these estimates with numerical results
and study their sensitivity to various grid parameters such as element size $h$
and polynomial order of approximation $p$. Finally, we present timing results
for a range of preconditioning options for the Laplace problem. We conclude
that the matrix-vector product operation is at most $\slfrac{33p^2}{8}$ times
more expensive for the more continuous space, although for moderately low $p$,
this number is significantly reduced. Moreover, if static condensation is not
employed, this number further reduces to at most a value of 8, even for high
$p$. Preconditioning options can be up to $p^3$ times more expensive to setup,
although this difference significantly decreases for some popular
preconditioners such as Incomplete LU factorization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2955</identifier>
 <datestamp>2013-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2955</id><created>2012-06-13</created><updated>2013-07-09</updated><authors><author><keyname>Suk</keyname><forenames>Andrew</forenames></author></authors><title>Density theorems for intersection graphs of t-monotone curves</title><categories>math.CO cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A curve \gamma in the plane is t-monotone if its interior has at most t-1
vertical tangent points. A family of t-monotone curves F is \emph{simple} if
any two members intersect at most once. It is shown that if F is a simple
family of n t-monotone curves with at least \epsilon n^2 intersecting pairs
(disjoint pairs), then there exists two subfamilies F_1,F_2 \subset F of size
\delta n each, such that every curve in F_1 intersects (is disjoint to) every
curve in F_2, where \delta depends only on \epsilon. We apply these results to
find pairwise disjoint edges in simple topological graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2957</identifier>
 <datestamp>2012-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2957</id><created>2012-06-13</created><authors><author><keyname>Dughmi</keyname><forenames>Shaddin</forenames></author><author><keyname>Peres</keyname><forenames>Yuval</forenames></author></authors><title>Mechanisms for Risk Averse Agents, Without Loss</title><categories>cs.GT</categories><comments>Presented at the workshop on risk aversion in algorithmic game theory
  and mechanism design, held in conjunction with EC 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Auctions in which agents' payoffs are random variables have received
increased attention in recent years. In particular, recent work in algorithmic
mechanism design has produced mechanisms employing internal randomization,
partly in response to limitations on deterministic mechanisms imposed by
computational complexity. For many of these mechanisms, which are often
referred to as truthful-in-expectation, incentive compatibility is contingent
on the assumption that agents are risk-neutral. These mechanisms have been
criticized on the grounds that this assumption is too strong, because &quot;real&quot;
agents are typically risk averse, and moreover their precise attitude towards
risk is typically unknown a-priori. In response, researchers in algorithmic
mechanism design have sought the design of universally-truthful mechanisms ---
mechanisms for which incentive-compatibility makes no assumptions regarding
agents' attitudes towards risk.
  We show that any truthful-in-expectation mechanism can be generically
transformed into a mechanism that is incentive compatible even when agents are
risk averse, without modifying the mechanism's allocation rule. The transformed
mechanism does not require reporting of agents' risk profiles. Equivalently,
our result can be stated as follows: Every (randomized) allocation rule that is
implementable in dominant strategies when players are risk neutral is also
implementable when players are endowed with an arbitrary and unknown concave
utility function for money.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2959</identifier>
 <datestamp>2012-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2959</id><created>2012-06-13</created><updated>2012-11-07</updated><authors><author><keyname>Ekambaram</keyname><forenames>Venkatesan. N.</forenames></author><author><keyname>Ramchandran</keyname><forenames>Kannan</forenames></author><author><keyname>Sengupta</keyname><forenames>Raja</forenames></author></authors><title>Collaborative High Accuracy Localization in Mobile Multipath
  Environments</title><categories>cs.NI cs.IT cs.RO math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of high accuracy localization of mobile nodes in a
multipath-rich environment where sub-meter accuracies are required. We employ a
peer-to-peer framework where the vehicles/nodes can get pairwise
multipath-degraded ranging estimates in local neighborhoods together with a
fixed number of anchor nodes. The challenge is to overcome the
multipath-barrier with redundancy in order to provide the desired accuracies
especially under severe multipath conditions when the fraction of received
signals corrupted by multipath is dominating. We invoke a analytical graphical
model framework based on particle filtering and reveal its high accuracy
localization promise through simulations. We also address design questions such
as &quot;How many anchors and what fraction of line-of-sight (LOS) measurements are
needed to achieve a specified target accuracy?&quot;, by analytically characterizing
the performance improvement in localization accuracy as a function of the
number of nodes in the network and the fraction of LOS measurements. In
particular, for a static node placement, we show that the Cramer-Rao Lower
Bound (CRLB), a fundamental lower bound on the localization accuracy, can be
expressed as a product of two factors - a scalar function that depends only on
the parameters of the noise distribution and a matrix that depends only on the
geometry of node locations and the underlying connectivity graph. Further, a
simplified expression is obtained for the CRLB that helps deduce the scaling
behavior of the estimation error as a function of the number of agents and
anchors in the network. The bound suggests that even a small fraction of LOS
measurements can provide significant improvements. Conversely, a small fraction
of NLOS measurements can significantly degrade the performance. The analysis is
extended to the mobile setting and the performance is compared with the derived
CRLB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2960</identifier>
 <datestamp>2012-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2960</id><created>2012-06-13</created><authors><author><keyname>Patriarca</keyname><forenames>M.</forenames></author><author><keyname>Castell&#xf3;</keyname><forenames>X.</forenames></author><author><keyname>Uriarte</keyname><forenames>J. R.</forenames></author><author><keyname>Egu&#xed;luz</keyname><forenames>V. M.</forenames></author><author><keyname>Miguel</keyname><forenames>M. San</forenames></author></authors><title>Modeling two-language competition dynamics</title><categories>physics.soc-ph cs.SI</categories><comments>15 pages, 5 figures; published in the Special Issue of Advances in
  Complex Systems &quot;Language Dynamics&quot;</comments><journal-ref>Advances in Complex Systems, Vol. 15, Nos. 3 &amp; 4 (2012) 1250048</journal-ref><doi>10.1142/S0219525912500488</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During the last decade, much attention has been paid to language competition
in the complex systems community, that is, how the fractions of speakers of
several competing languages evolve in time. In this paper we review recent
advances in this direction and focus on three aspects. First we consider the
shift from two-state models to three state models that include the possibility
of bilingual individuals. The understanding of the role played by bilingualism
is essential in sociolinguistics. In particular, the question addressed is
whether bilingualism facilitates the coexistence of languages. Second, we will
analyze the effect of social interaction networks and physical barriers.
Finally, we will show how to analyze the issue of bilingualism from a game
theoretical perspective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2961</identifier>
 <datestamp>2012-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2961</id><created>2012-06-13</created><authors><author><keyname>Montina</keyname><forenames>Alberto</forenames></author></authors><title>Epistemic view of quantum states and communication complexity of quantum
  channels</title><categories>quant-ph cs.IT math.IT</categories><journal-ref>Phys. Rev. Lett. 109, 110501 (2012)</journal-ref><doi>10.1103/PhysRevLett.109.110501</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The communication complexity of a quantum channel is the minimal amount of
classical communication required for classically simulating a process of state
preparation, transmission through the channel and subsequent measurement. It
establishes a limit on the power of quantum communication in terms of classical
resources. We show that classical simulations employing a finite amount of
communication can be derived from a special class of hidden variable theories
where quantum states represent statistical knowledge about the classical state
and not an element of reality. This special class has attracted strong interest
very recently. The communication cost of each derived simulation is given by
the mutual information between the quantum state and the classical state of the
parent hidden variable theory. Finally, we find that the communication
complexity for single qubits is smaller than 1.28 bits. The previous known
upper bound was 1.85 bits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2968</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2968</id><created>2012-06-13</created><updated>2013-12-31</updated><authors><author><keyname>Kulkarni</keyname><forenames>Ankur A.</forenames></author><author><keyname>Shanbhag</keyname><forenames>Uday V.</forenames></author></authors><title>A Shared-Constraint Approach to Multi-leader Multi-follower Games</title><categories>math.OC cs.GT</categories><comments>The earlier manuscript was rejected. We felt it had too many themes
  crowding it and decided to make a separate paper from each theme. This
  submission draws some parts from the earlier manuscript and adds new results.
  Another parts is under review with the IEEE TAC (on arxiv) and another was
  published in Proc IEEE CDC, 2013. This submission is under review with
  Set-valued and Variational Analysis</comments><msc-class>90C33, 91A65, 91A20, 90C26</msc-class><doi>10.1007/s11228-014-0292-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-leader multi-follower games are a class of hierarchical games in which
a collection of leaders compete in a Nash game constrained by the equilibrium
conditions of another Nash game amongst the followers. The resulting
equilibrium problem with equilibrium constraints is complicated by nonconvex
agent problems and therefore providing tractable conditions for existence of
global or even local equilibria for it has proved challenging. Consequently,
much of the extant research on this topic is either model specific or relies on
weaker notions of equilibria. We consider a modified formulation in which every
leader is cognizant of the equilibrium constraints of all leaders. Equilibria
of this modified game contain the equilibria, if any, of the original game. The
new formulation has a constraint structure called shared constraints, and our
main result shows that if the leader objectives admit a potential function, the
global minimizers of the potential function over the shared constraint are
equilibria of the modified formulation. We provide another existence result
using fixed point theory that does not require potentiality. Additionally,
local minima, B-stationary, and strong-stationary points of this minimization
are shown to be local Nash equilibria, Nash B-stationary, and Nash
strong-stationary points of the corresponding multi-leader multi-follower game.
We demonstrate the relationship between variational equilibria associated with
this modified shared-constraint game and equilibria of the original game from
the standpoint of the multiplier sets and show how equilibria of the original
formulation may be recovered. We note through several examples that such
potential multi-leader multi-follower games capture a breadth of application
problems of interest and demonstrate our findings on a multi-leader
multi-follower Cournot game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2973</identifier>
 <datestamp>2012-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2973</id><created>2012-06-13</created><authors><author><keyname>Minevich</keyname><forenames>Igor</forenames></author></authors><title>Symmetric Matrices over F_2 and the Lights Out Problem</title><categories>math.RA cs.DM</categories><comments>4 pages</comments><msc-class>05C50</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the range of a symmetric matrix over F_2 contains the vector of
its diagonal elements. We apply the theorem to a generalization of the &quot;Lights
Out&quot; problem on graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2974</identifier>
 <datestamp>2012-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2974</id><created>2012-06-13</created><authors><author><keyname>Akyol</keyname><forenames>Emrah</forenames></author><author><keyname>Rose</keyname><forenames>Kenneth</forenames></author></authors><title>On Constrained Randomized Quantization</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Signal Processing, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Randomized (dithered) quantization is a method capable of achieving white
reconstruction error independent of the source. Dithered quantizers have
traditionally been considered within their natural setting of uniform
quantization. In this paper we extend conventional dithered quantization to
nonuniform quantization, via a subterfage: dithering is performed in the
companded domain. Closed form necessary conditions for optimality of the
compressor and expander mappings are derived for both fixed and variable rate
randomized quantization. Numerically, mappings are optimized by iteratively
imposing these necessary conditions. The framework is extended to include an
explicit constraint that deterministic or randomized quantizers yield
reconstruction error that is uncorrelated with the source. Surprising
theoretical results show direct and simple connection between the optimal
constrained quantizers and their unconstrained counterparts. Numerical results
for the Gaussian source provide strong evidence that the proposed constrained
randomized quantizer outperforms the conventional dithered quantizer, as well
as the constrained deterministic quantizer. Moreover, the proposed constrained
quantizer renders the reconstruction error nearly white. In the second part of
the paper, we investigate whether uncorrelated reconstruction error requires
random coding to achieve asymptotic optimality. We show that for a Gaussian
source, the optimal vector quantizer of asymptotically high dimension whose
quantization error is uncorrelated with the source, is indeed random. Thus,
random encoding in this setting of rate-distortion theory, is not merely a tool
to characterize performance bounds, but a required property of quantizers that
approach such bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.2994</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.2994</id><created>2012-06-14</created><updated>2012-06-18</updated><authors><author><keyname>Akyol</keyname><forenames>Emrah</forenames></author><author><keyname>Rose</keyname><forenames>Kenneth</forenames></author></authors><title>Towards Optimality in Transform Coding</title><categories>cs.IT math.IT</categories><comments>discovered a related prior work on the subject</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well-known for transform coding of multivariate Gaussian sources, that
the Karhunen-Lo\`eve transform (KLT) minimizes the mean square error
distortion. However, finding the optimal transform for general non-Gaussian
sources has been an open problem for decades, despite several important
advances that provide some partial answers regarding KLT optimality. In this
paper, we present a necessary and sufficient condition for optimality of a
transform when high resolution, variable rate quantizers are employed. We hence
present not only a complete characterization of when KLT is optimal, but also a
determining condition for optimality of a general (non-KLT) transform. This
necessary and sufficient condition is shown to have direct connections to the
well studied source separation problem. This observation can impact source
separation itself, as illustrated with a new optimality result. We combine the
transform optimality condition with algorithmic tools from source separation,
to derive a practical numerical method to search for the optimal transform in
source coding. Then, we focus on multiterminal settings, for which {\it
conditional} KLT was shown to possess certain optimality properties for
Gaussian sources. We derive the optimal orthogonal transform for the setting
where side information is only available to the decoder, along with new
specialized results specific to the conditions for optimality of conditional
KLT. Finally, we consider distributed source coding where two correlated
sources are to be transform coded separately but decoded jointly. We derive the
necessary and sufficient condition of optimality of the orthogonal transforms.
We specialize to find the optimal orthogonal transforms, in this setting, for
specific source densities, including jointly Gaussian sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3001</identifier>
 <datestamp>2012-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3001</id><created>2012-06-14</created><authors><author><keyname>Jost</keyname><forenames>C&#xe9;line</forenames><affiliation>Lab-STICC</affiliation></author><author><keyname>P&#xe9;v&#xe9;dic</keyname><forenames>Brigitte Le</forenames><affiliation>Lab-STICC</affiliation></author><author><keyname>Duhaut</keyname><forenames>Dominique</forenames><affiliation>Lab-STICC</affiliation></author></authors><title>Creating Interaction Scenarios With a New Graphical User Interface</title><categories>cs.HC</categories><comments>5th International Workshop on Intelligent Interfaces for
  Human-Computer Interaction, Palerme : Italy (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The field of human-centered computing has known a major progress these past
few years. It is admitted that this field is multidisciplinary and that the
human is the core of the system. It shows two matters of concern:
multidisciplinary and human. The first one reveals that each discipline plays
an important role in the global research and that the collaboration between
everyone is needed. The second one explains that a growing number of researches
aims at making the human commitment degree increase by giving him/her a
decisive role in the human-machine interaction. This paper focuses on these
both concerns and presents MICE (Machines Interaction Control in their
Environment) which is a system where the human is the one who makes the
decisions to manage the interaction with the machines. In an ambient context,
the human can decide of objects actions by creating interaction scenarios with
a new visual programming language: scenL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3002</identifier>
 <datestamp>2012-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3002</id><created>2012-06-14</created><authors><author><keyname>Jost</keyname><forenames>C&#xe9;line</forenames><affiliation>Lab-STICC</affiliation></author><author><keyname>P&#xe9;v&#xe9;dic</keyname><forenames>Brigitte Le</forenames><affiliation>Lab-STICC</affiliation></author><author><keyname>Duhaut</keyname><forenames>Dominique</forenames><affiliation>Lab-STICC</affiliation></author></authors><title>Study of the Importance of Adequacy to Robot Verbal and Non Verbal
  Communication in Human-Robot interaction</title><categories>cs.RO cs.HC</categories><comments>the 43rd Symposium on Robotics - ISR 2012, Taipei : Taiwan, Province
  Of China (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Robadom project aims at creating a homecare robot that help and assist
people in their daily life, either in doing task for the human or in managing
day organization. A robot could have this kind of role only if it is accepted
by humans. Before thinking about the robot appearance, we decided to evaluate
the importance of the relation between verbal and nonverbal communication
during a human-robot interaction in order to determine the situation where the
robot is accepted. We realized two experiments in order to study this
acceptance. The first experiment studied the importance of having robot
nonverbal behavior in relation of its verbal behavior. The second experiment
studied the capability of a robot to provide a correct human-robot interaction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3014</identifier>
 <datestamp>2012-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3014</id><created>2012-06-14</created><authors><author><keyname>Li</keyname><forenames>Yao</forenames></author><author><keyname>Vingelmann</keyname><forenames>P&#xe9;ter</forenames></author><author><keyname>Pedersen</keyname><forenames>Morten Videb&#xe6;k</forenames></author><author><keyname>Soljanin</keyname><forenames>Emina</forenames></author></authors><title>Round-Robin Streaming with Generations</title><categories>cs.IT math.IT</categories><comments>NetCod'12</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider three types of application layer coding for streaming over lossy
links: random linear coding, systematic random linear coding, and structured
coding. The file being streamed is divided into sub-blocks (generations). Code
symbols are formed by combining data belonging to the same generation, and
transmitted in a round-robin fashion. We compare the schemes based on delivery
packet count, net throughput, and energy consumption for a range of generation
sizes. We determine these performance measures both analytically and in an
experimental configuration. We find our analytical predictions to match the
experimental results. We show that coding at the application layer brings about
a significant increase in net data throughput, and thereby reduction in energy
consumption due to reduced communication time. On the other hand, on devices
with constrained computing resources, heavy coding operations cause packet
drops in higher layers and negatively affect the net throughput. We find from
our experimental results that low-rate MDS codes are best for small generation
sizes, whereas systematic random linear coding has the best net throughput and
lowest energy consumption for larger generation sizes due to its low decoding
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3026</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3026</id><created>2012-06-14</created><authors><author><keyname>Stevens</keyname><forenames>William M.</forenames></author></authors><title>Using transition systems to describe and predict the behaviour of
  structured excitable media</title><categories>nlin.PS cs.FL</categories><doi>10.1007/s11047-012-9355-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I show how transition systems can be applied to the naturally concurrent
behaviour of excitable media. I consider structured excitable media, in which
excitations are constrained to propagate only in defined narrow channels, and
cannot propagate elsewhere. I define a type of transition system that can be
used to describe the complete set of behaviours exhibited by simple structures.
The composition rules that result from this definition can be used to
automatically deduce the behaviour of more complex structures composed from
simpler structures. Several examples illustrate the method, and a software
implementation is provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3027</identifier>
 <datestamp>2012-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3027</id><created>2012-06-14</created><authors><author><keyname>Pohle</keyname><forenames>J&#xf6;rg</forenames></author></authors><title>Social Networks, Functional Differentiation of Society, and Data
  Protection</title><categories>cs.CY cs.SI physics.soc-ph</categories><comments>8 pages</comments><acm-class>K.4.1; K.5.2</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Most scholars, politicians, and activists are following individualistic
theories of privacy and data protection. In contrast, some of the pioneers of
the data protection legislation in Germany like Adalbert Podlech, Paul J.
M\&quot;uller, and Ulrich Dammann used a systems theory approach. Following Niklas
Luhmann, the aim of data protection is (1) maintaining the functional
differentiation of society against the threats posed by the possibilities of
modern information processing, and (2) countering undue information power by
organized social players. It could be, therefore, no surprise that the first
data protection law in the German state of Hesse contained rules to protect the
individual as well as the balance of power between the legislative and the
executive body of the state. Social networks like Facebook or Google+ do not
only endanger their users by exposing them to other users or the public. They
constitute, first and foremost, a threat to society as a whole by collecting
information about individuals, groups, and organizations from different social
systems and combining them in a centralized data bank. They transgress the
boundaries between social systems that act as a shield against total visibility
and transparency of the individual and protect the freedom and the autonomy of
the people. Without enforcing structural limitations on the organizational use
of collected data by the social network itself or the company behind it, social
networks pose the worst totalitarian peril for western societies since the fall
of the Soviet Union.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3029</identifier>
 <datestamp>2012-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3029</id><created>2012-06-14</created><authors><author><keyname>Coon</keyname><forenames>Justin P.</forenames></author><author><keyname>Wang</keyname><forenames>Yue</forenames></author><author><keyname>Huang</keyname><forenames>Gillian</forenames></author></authors><title>Asymptotic Outage Probability Analysis for General Fixed-Gain
  Amplify-and-Forward Multihop Relay Systems</title><categories>cs.IT math.IT</categories><comments>29 pages, 9 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an analysis of the outage probability for
fixed-gain amplify-and-forward (AF) multihop relay links operating in the high
SNR regime. Our analysis exploits properties of Mellin transforms to derive an
asymptotic approximation that is accurate even when the per-hop channel gains
adhere to completely different fading models. The main result contained in the
paper is a general expression for the outage probability, which is a functional
of the Mellin transforms of the per-hop channel gains. Furthermore, we
explicitly calculate the asymptotic outage probability for four different
systems, whereby in each system the per-hop channels adhere to either a
Nakagami-m, Weibull, Rician, or Hoyt fading profile, but where the
distributional parameters may differ from hop to hop. This analysis leads to
our second main result, which is a semi-general closed-form formula for the
outage probability of general fixed-gain AF multihop systems. We exploit this
formula to analyze an example scenario for a four-hop system where the per-hop
channels follow the four aforementioned fading models, i.e., the first channel
is Nakagami-m fading, the second is Weibull fading, and so on. Finally, we
provide simulation results to corroborate our analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3037</identifier>
 <datestamp>2012-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3037</id><created>2012-06-14</created><authors><author><keyname>Moretti</keyname><forenames>Paolo</forenames></author><author><keyname>Baronchelli</keyname><forenames>Andrea</forenames></author><author><keyname>Starnini</keyname><forenames>Michele</forenames></author><author><keyname>Pastor-Satorras</keyname><forenames>Romualdo</forenames></author></authors><title>Generalized voter-like models on heterogeneous networks</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a generalization of the voter model on complex networks that
encompasses different sources of degree-related heterogeneity and that is
amenable to direct analytical solution by applying the standard methods of
heterogeneous mean-field theory. Our formalism allows for a compact description
of previously proposed heterogeneous voter-like models, and represents a basic
framework within which we can rationalize the effects of heterogeneity in
voter-like models, as well as implement novel sources of heterogeneity, not
previously considered in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3038</identifier>
 <datestamp>2012-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3038</id><created>2012-06-14</created><updated>2012-06-25</updated><authors><author><keyname>Gupta</keyname><forenames>Manish. K.</forenames></author><author><keyname>Durairajan</keyname><forenames>C.</forenames></author></authors><title>On the Covering Radius of Some Modular Codes</title><categories>cs.IT math.IT math.RA</categories><comments>revised</comments><msc-class>94B25 (Primary) 11H31 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper gives lower and upper bounds on the covering radius of codes over
$\Z_{2^s}$ with respect to homogenous distance. We also determine the covering
radius of various Repetition codes, Simplex codes (Type $\alpha$ and Type
$\beta$) and their dual and give bounds on the covering radii for MacDonald
codes of both types over $\Z_4$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3043</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3043</id><created>2012-06-14</created><updated>2012-11-22</updated><authors><author><keyname>Moulay</keyname><forenames>Djamila</forenames></author><author><keyname>Pign&#xe9;</keyname><forenames>Yoann</forenames></author></authors><title>A Metapopulation Model for Chikungunya Including Populations Mobility on
  a Large-Scale Network</title><categories>cs.SI math.DS physics.soc-ph</categories><comments>Accepted in Journal of Theoretical biology</comments><doi>10.1016/j.jtbi.2012.11.008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we study the influence of populations mobility on the spread of
a vector-borne disease. We focus on the chikungunya epidemic event that
occurred in 2005-2006 on the R\'eunion Island, Indian Ocean, France, and
validate our models with real epidemic data from the event. We propose a
metapopulation model to represent both a high-resolution patch model of the
island with realistic population densities and also mobility models for humans
(based on real-motion data) and mosquitoes. In this metapopulation network, two
models are coupled: one for the dynamics of the mosquito population and one for
the transmission of the disease. A high-resolution numerical model is created
out from real geographical, demographical and mobility data. The Island is
modeled with an 18 000-nodes metapopulation network. Numerical results show the
impact of the geographical environment and populations' mobility on the spread
of the disease. The model is finally validated against real epidemic data from
the R\'eunion event.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3061</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3061</id><created>2012-06-14</created><updated>2012-06-15</updated><authors><author><keyname>Alagu</keyname><forenames>Siva</forenames></author><author><keyname>Meyyappan</keyname><forenames>T.</forenames></author></authors><title>A Novel Adaptive Channel Allocation Scheme to Handle Handoffs</title><categories>cs.NI</categories><comments>9 Pages; in International Journal of Distributed and Parallel systems</comments><journal-ref>International Journal of Distributed and Parallel Systems (IJDPS)
  Vol.3, No.3, May 2012, 145-153</journal-ref><doi>10.5121/ijdps.2012.3314</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless networking is becoming an increasingly important and popular way of
providing global information access to users on the move. One of the main
challenges for seamless mobility is the availability of simple and robust
handoff algorithms, which allow a mobile node to roam among heterogeneous
wireless networks. In this paper, the authors devise a scheme, A Novel Adaptive
Channel Allocation Scheme (ACAS) where the number of guard channel(s) is
adjusted automatically based on the average handoff blocking rate measured in
the past certain period of time. The handoff blocking rate is controlled under
the designated threshold and the new call blocking rate is minimized. The
performance evaluation of the ACAS is done through simulation of nodes. The
result shows that the ACAS scheme outperforms the Static Channel Allocation
Scheme by controlling a hard constraint on the handoff rejection probability.
The proposed scheme achieves the optimal performance by maximizing the resource
utilization and adapts itself to changing traffic conditions automatically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3065</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3065</id><created>2012-06-14</created><authors><author><keyname>Ouyang</keyname><forenames>Ruiyue</forenames></author><author><keyname>Jayawardhana</keyname><forenames>Bayu</forenames></author></authors><title>Stability Analysis and Controller Design for a Linear System with Duhem
  Hysteresis Nonlinearity</title><categories>math.OC cs.SY</categories><comments>30 pages</comments><msc-class>34H05, 34C55, 37N35, 70Q05, 47J40</msc-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, we investigate the stability of a feedback interconnection
between a linear system and a Duhem hysteresis operator, where the linear
system and the Duhem hysteresis operator satisfy either the counter-clockwise
(CCW) or clockwise (CW) input-output dynamics. More precisely, we present
sufficient conditions for the stability of the interconnected system that
depend on the CW or CCW properties of the linear system and the Duhem operator.
Based on these results we introduce a control design methodology for
stabilizing a linear plant with a hysteretic actuator or sensor without
requiring precise information on the hysteresis operator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3072</identifier>
 <datestamp>2012-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3072</id><created>2012-06-14</created><authors><author><keyname>Telgarsky</keyname><forenames>Matus</forenames></author></authors><title>Statistical Consistency of Finite-dimensional Unregularized Linear
  Classification</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This manuscript studies statistical properties of linear classifiers obtained
through minimization of an unregularized convex risk over a finite sample.
Although the results are explicitly finite-dimensional, inputs may be passed
through feature maps; in this way, in addition to treating the consistency of
logistic regression, this analysis also handles boosting over a finite weak
learning class with, for instance, the exponential, logistic, and hinge losses.
In this finite-dimensional setting, it is still possible to fit arbitrary
decision boundaries: scaling the complexity of the weak learning class with the
sample size leads to the optimal classification risk almost surely.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3075</identifier>
 <datestamp>2013-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3075</id><created>2012-06-14</created><updated>2013-06-04</updated><authors><author><keyname>Hackett</keyname><forenames>Adam</forenames></author><author><keyname>Gleeson</keyname><forenames>James P.</forenames></author></authors><title>Cascades on clique-based graphs</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>11 pages, 4 figures</comments><journal-ref>Phys. Rev. E 87, 062801 (2013)</journal-ref><doi>10.1103/PhysRevE.87.062801</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an analytical approach to determining the expected cascade size in
a broad range of dynamical models on the class of highly-clustered random
graphs introduced by Gleeson [J. P. Gleeson, Phys. Rev. E 80, 036107 (2009)]. A
condition for the existence of global cascades is also derived. Applications of
this approach include analyses of percolation, and Watts's model. We show how
our techniques can be used to study the effects of in-group bias in cascades on
social networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3078</identifier>
 <datestamp>2012-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3078</id><created>2012-06-14</created><authors><author><keyname>Pal</keyname><forenames>Saurabh</forenames></author></authors><title>Mining Educational Data Using Classification to Decrease Dropout Rate of
  Students</title><categories>cs.IR</categories><comments>5 pages. arXiv admin note: substantial text overlap with
  arXiv:1203.2987, arXiv:1203.3832, arXiv:1202.4815, arXiv:1201.3418,
  arXiv:1201.3417, and with arXiv:1002.1144 by other authors</comments><journal-ref>International journal of multidisciplinary sciences and
  engineering, vol. 3, no. 5, May 2012, 35-39</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last two decades, number of Higher Education Institutions (HEI) grows
rapidly in India. Since most of the institutions are opened in private mode
therefore, a cut throat competition rises among these institutions while
attracting the student to got admission. This is the reason for institutions to
focus on the strength of students not on the quality of education. This paper
presents a data mining application to generate predictive models for
engineering student's dropout management. Given new records of incoming
students, the predictive model can produce short accurate prediction list
identifying students who tend to need the support from the student dropout
program most. The results show that the machine learning algorithm is able to
establish effective predictive model from the existing student dropout data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3099</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3099</id><created>2012-06-14</created><updated>2012-11-12</updated><authors><author><keyname>Di Lorenzo</keyname><forenames>Paolo</forenames></author><author><keyname>Sayed</keyname><forenames>Ali H.</forenames></author></authors><title>Sparse Distributed Learning Based on Diffusion Adaptation</title><categories>cs.LG cs.DC</categories><comments>to appear in IEEE Trans. on Signal Processing, 2013</comments><journal-ref>IEEE Transactions on Signal Processing, Vol. 61, no. 6 , 15 March
  2013</journal-ref><doi>10.1109/TSP.2012.2232663</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article proposes diffusion LMS strategies for distributed estimation
over adaptive networks that are able to exploit sparsity in the underlying
system model. The approach relies on convex regularization, common in
compressive sensing, to enhance the detection of sparsity via a diffusive
process over the network. The resulting algorithms endow networks with learning
abilities and allow them to learn the sparse structure from the incoming data
in real-time, and also to track variations in the sparsity of the model. We
provide convergence and mean-square performance analysis of the proposed method
and show under what conditions it outperforms the unregularized diffusion
version. We also show how to adaptively select the regularization parameter.
Simulation results illustrate the advantage of the proposed filters for sparse
data recovery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3111</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3111</id><created>2012-06-14</created><authors><author><keyname>Calimeri</keyname><forenames>Francesco</forenames></author><author><keyname>Ianni</keyname><forenames>Giovambattista</forenames></author><author><keyname>Ricca</keyname><forenames>Francesco</forenames></author></authors><title>The third open Answer Set Programming competition</title><categories>cs.AI</categories><comments>37 pages, 12 figures, 1 table - To appear in Theory and Practice of
  Logic Programming (TPLP)</comments><doi>10.1017/S1471068412000105</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Answer Set Programming (ASP) is a well-established paradigm of declarative
programming in close relationship with other declarative formalisms such as SAT
Modulo Theories, Constraint Handling Rules, FO(.), PDDL and many others. Since
its first informal editions, ASP systems have been compared in the now
well-established ASP Competition. The Third (Open) ASP Competition, as the
sequel to the ASP Competitions Series held at the University of Potsdam in
Germany (2006-2007) and at the University of Leuven in Belgium in 2009, took
place at the University of Calabria (Italy) in the first half of 2011.
Participants competed on a pre-selected collection of benchmark problems, taken
from a variety of domains as well as real world applications. The Competition
ran on two tracks: the Model and Solve (M&amp;S) Track, based on an open problem
encoding, and open language, and open to any kind of system based on a
declarative specification paradigm; and the System Track, run on the basis of
fixed, public problem encodings, written in a standard ASP language. This paper
discusses the format of the Competition and the rationale behind it, then
reports the results for both tracks. Comparison with the second ASP competition
and state-of-the-art solutions for some of the benchmark domains is eventually
discussed.
  To appear in Theory and Practice of Logic Programming (TPLP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3120</identifier>
 <datestamp>2012-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3120</id><created>2012-06-14</created><authors><author><keyname>Subramanian</keyname><forenames>Vijay G.</forenames></author><author><keyname>Leith</keyname><forenames>Douglas J.</forenames></author></authors><title>Convexity Conditions for 802.11 WLANs</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we characterise the maximal convex subsets of the (non-convex)
rate region in 802.11 WLANs. In addition to being of intrinsic interest as a
fundamental property of 802.11 WLANs, this characterisation can be exploited to
allow the wealth of convex optimisation approaches to be applied to 802.11
WLANs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3133</identifier>
 <datestamp>2012-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3133</id><created>2012-06-14</created><authors><author><keyname>Siavoshani</keyname><forenames>Mahdi Jafari</forenames></author><author><keyname>Fragouli</keyname><forenames>Christina</forenames></author></authors><title>Multi-terminal Secrecy in a Linear Non-coherent Packetized Networks</title><categories>cs.IT cs.CR math.IT</categories><comments>This paper is going to be presented in NetCod 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a group of m+1 trusted nodes that aim to create a shared secret
key K over a network in the presence of a passive eavesdropper, Eve. We assume
a linear non-coherent network coding broadcast channel (over a finite field
F_q) from one of the honest nodes (i.e., Alice) to the rest of them including
Eve. All of the trusted nodes can also discuss over a cost-free public channel
which is also overheard by Eve.
  For this setup, we propose upper and lower bounds for the secret key
generation capacity assuming that the field size q is very large. For the case
of two trusted terminals (m = 1) our upper and lower bounds match and we have
complete characterization for the secrecy capacity in the large field size
regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3135</identifier>
 <datestamp>2012-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3135</id><created>2012-06-14</created><authors><author><keyname>Kim</keyname><forenames>Ilhee</forenames></author><author><keyname>Seymour</keyname><forenames>Paul</forenames></author></authors><title>Tournament Minors</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We say a digraph $G$ is a {\em minor} of a digraph $H$ if $G$ can be obtained
from a subdigraph of $H$ by repeatedly contracting a strongly-connected
subdigraph to a vertex. Here, we show the class of all tournaments is a
well-quasi-order under minor containment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3136</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3136</id><created>2012-06-14</created><authors><author><keyname>Prisacariu</keyname><forenames>Cristian</forenames></author></authors><title>The Glory of the Past and Geometrical Concurrency</title><categories>cs.LO</categories><comments>17 pages, 7 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper contributes to the general understanding of the geometrical model
of concurrency that was named higher dimensional automata (HDAs) by Pratt. In
particular we investigate modal logics for such models and their expressive
power in terms of the bisimulation that can be captured. The geometric model of
concurrency is interesting from two main reasons: its generality and
expressiveness, and the natural way in which autoconcurrency and action
refinement are captured. Logics for this model, though, are not well
investigated, where a simple, yet adequate, modal logic over HDAs was only
recently introduced. As this modal logic, with two existential modalities,
during and after, captures only split bisimulation, which is rather low in the
spectrum of van Glabbeek and Vaandrager, the immediate question was what small
extension of this logic could capture the more fine-grained hereditary history
preserving bisimulation (hh)? In response, the work in this paper provides
several insights. One is the fact that the geometrical aspect of HDAs makes it
possible to use for capturing the hh-bisimulation, a standard modal logic that
does not employ event variables, opposed to the two logics (over less
expressive models) that we compare with. The logic that we investigate here
uses standard past modalities and extends the previously introduced logic
(called HDML) that had only forward, action-labelled, modalities. Besides, we
try to understand better the above issues by introducing a related model that
we call ST-configuration structures, which extend the configuration structures
of van Glabbeek and Plotkin. We relate this model to HDAs, and redefine and
prove the earlier results in the light of this new model. These offer a
different view on why the past modalities and geometrical concurrency capture
the hereditary history preserving bisimulation. Additional correlating insights
are also gained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3137</identifier>
 <datestamp>2012-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3137</id><created>2012-06-14</created><authors><author><keyname>Hsu</keyname><forenames>Daniel</forenames></author><author><keyname>Kakade</keyname><forenames>Sham M.</forenames></author><author><keyname>Liang</keyname><forenames>Percy</forenames></author></authors><title>Identifiability and Unmixing of Latent Parse Trees</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores unsupervised learning of parsing models along two
directions. First, which models are identifiable from infinite data? We use a
general technique for numerically checking identifiability based on the rank of
a Jacobian matrix, and apply it to several standard constituency and dependency
parsing models. Second, for identifiable models, how do we estimate the
parameters efficiently? EM suffers from local optima, while recent work using
spectral methods cannot be directly applied since the topology of the parse
tree varies across sentences. We develop a strategy, unmixing, which deals with
this additional complexity for restricted classes of parsing models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3138</identifier>
 <datestamp>2012-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3138</id><created>2012-06-14</created><authors><author><keyname>Khisti</keyname><forenames>Ashish</forenames></author><author><keyname>Hern</keyname><forenames>Brett</forenames></author><author><keyname>Narayanan</keyname><forenames>Krishna</forenames></author></authors><title>On Modulo-Sum Computation over an Erasure Multiple Access Channel</title><categories>cs.IT math.IT</categories><comments>Shorter Version will Appear at ISIT 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study computation of a modulo-sum of two binary source sequences over a
two-user erasure multiple access channel. The channel is modeled as a
binary-input, erasure multiple access channel, which can be in one of three
states - either the channel output is a modulo-sum of the two input symbols, or
the channel output equals the input symbol on the first link and an erasure on
the second link, or vice versa. The associated state sequence is independent
and identically distributed. We develop a new upper bound on the sum-rate by
revealing only part of the state sequence to the transmitters. Our coding
scheme is based on the compute and forward and the decode and forward
techniques. When a (strictly) causal feedback of the channel state is available
to the encoders, we show that the modulo-sum capacity is increased. Extensions
to the case of lossy reconstruction of the modulo-sum and to channels involving
additional states are also treated briefly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3165</identifier>
 <datestamp>2012-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3165</id><created>2012-06-14</created><authors><author><keyname>Galvin</keyname><forenames>David</forenames></author><author><keyname>Tetali</keyname><forenames>Prasad</forenames></author></authors><title>Slow mixing of Glauber Dynamics for the hard-core model on regular
  bipartite graphs</title><categories>math.CO cs.DM</categories><comments>18 pages. This paper appeared in Random Structures and Algorithms in
  2006, and is an expanded version of the abstract Slow mixing of Glauber
  dynamics for the hard-core model on the hypercube that appeared in the
  proceedings of SODA 2004</comments><msc-class>68R10, 68W25, 05C69</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\gS=(V,E)$ be a finite, $d$-regular bipartite graph. For any $\lambda&gt;0$
let $\pi_\lambda$ be the probability measure on the independent sets of $\gS$
in which the set $I$ is chosen with probability proportional to $\lambda^{|I|}$
($\pi_\lambda$ is the {\em hard-core measure with activity $\lambda$ on
$\gS$}). We study the Glauber dynamics, or single-site update Markov chain,
whose stationary distribution is $\pi_\lambda$. We show that when $\lambda$ is
large enough (as a function of $d$ and the expansion of subsets of
single-parity of $V$) then the convergence to stationarity is exponentially
slow in $|V(\gS)|$. In particular, if $\gS$ is the $d$-dimensional hypercube
$\{0,1\}^d$ we show that for values of $\lambda$ tending to 0 as $d$ grows, the
convergence to stationarity is exponentially slow in the volume of the cube.
The proof combines a conductance argument with combinatorial enumeration
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3177</identifier>
 <datestamp>2012-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3177</id><created>2012-06-14</created><authors><author><keyname>Hong</keyname><forenames>Dohy</forenames></author><author><keyname>Jacquet</keyname><forenames>Philippe</forenames></author></authors><title>Optimizing the eigenvector computation algorithm with diffusion approach</title><categories>cs.NA math.NA</categories><comments>4 pages</comments><acm-class>G.1.3; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we apply the ideas of the matrix column based diffusion
approach to define a new eigenvector computation algorithm of a stationary
probability of a Markov chain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3179</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3179</id><created>2012-06-14</created><updated>2015-05-12</updated><authors><author><keyname>Pilz</keyname><forenames>Alexander</forenames></author></authors><title>Flip Distance Between Triangulations of a Planar Point Set is APX-Hard</title><categories>cs.CG</categories><comments>A previous version only showed NP-completeness of the corresponding
  decision problem. The current version is the one of the accepted manuscript</comments><journal-ref>Comput. Geom., 47(5):589-604, 2014</journal-ref><doi>10.1016/j.comgeo.2014.01.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we consider triangulations of point sets in the Euclidean plane,
i.e., maximal straight-line crossing-free graphs on a finite set of points.
Given a triangulation of a point set, an edge flip is the operation of removing
one edge and adding another one, such that the resulting graph is again a
triangulation. Flips are a major way of locally transforming triangular meshes.
We show that, given a point set $S$ in the Euclidean plane and two
triangulations $T_1$ and $T_2$ of $S$, it is an APX-hard problem to minimize
the number of edge flips to transform $T_1$ to $T_2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3180</identifier>
 <datestamp>2012-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3180</id><created>2012-06-14</created><authors><author><keyname>Barletta</keyname><forenames>Michele</forenames></author><author><keyname>Ranise</keyname><forenames>Silvio</forenames></author><author><keyname>Vigan&#xf2;</keyname><forenames>Luca</forenames></author></authors><title>Automated Analysis of Scenario-based Specifications of Distributed
  Access Control Policies with Non-Mechanizable Activities (Extended Version)</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The advance of web services technologies promises to have far-reaching
effects on the Internet and enterprise networks allowing for greater
accessibility of data. The security challenges presented by the web services
approach are formidable. In particular, access control solutions should be
revised to address new challenges, such as the need of using certificates for
the identification of users and their attributes, human intervention in the
creation or selection of the certificates, and (chains of) certificates for
trust management. With all these features, it is not surprising that analyzing
policies to guarantee that a sensitive resource can be accessed only by
authorized users becomes very difficult. In this paper, we present an automated
technique to analyze scenario-based specifications of access control policies
in open and distributed systems. We illustrate our ideas on a case study
arising in the e-government area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3182</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3182</id><created>2012-06-14</created><updated>2012-07-31</updated><authors><author><keyname>Cimatti</keyname><forenames>Alessandro</forenames><affiliation>Fondazione Bruno Kessler</affiliation></author><author><keyname>Narasamdya</keyname><forenames>Iman</forenames><affiliation>Fondazione Bruno Kessler</affiliation></author><author><keyname>Roveri</keyname><forenames>Marco</forenames><affiliation>Fondazione Bruno Kessler</affiliation></author></authors><title>Software Model Checking with Explicit Scheduler and Symbolic Threads</title><categories>cs.LO cs.PL</categories><comments>40 pages, 10 figures, accepted for publication in journal of logical
  methods in computer science</comments><proxy>LMCS</proxy><acm-class>D.2.4</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 2 (August 5,
  2012) lmcs:1032</journal-ref><doi>10.2168/LMCS-8(2:18)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many practical application domains, the software is organized into a set
of threads, whose activation is exclusive and controlled by a cooperative
scheduling policy: threads execute, without any interruption, until they either
terminate or yield the control explicitly to the scheduler. The formal
verification of such software poses significant challenges. On the one side,
each thread may have infinite state space, and might call for abstraction. On
the other side, the scheduling policy is often important for correctness, and
an approach based on abstracting the scheduler may result in loss of precision
and false positives. Unfortunately, the translation of the problem into a
purely sequential software model checking problem turns out to be highly
inefficient for the available technologies. We propose a software model
checking technique that exploits the intrinsic structure of these programs.
Each thread is translated into a separate sequential program and explored
symbolically with lazy abstraction, while the overall verification is
orchestrated by the direct execution of the scheduler. The approach is
optimized by filtering the exploration of the scheduler with the integration of
partial-order reduction. The technique, called ESST (Explicit Scheduler,
Symbolic Threads) has been implemented and experimentally evaluated on a
significant set of benchmarks. The results demonstrate that ESST technique is
way more effective than software model checking applied to the sequentialized
programs, and that partial-order reduction can lead to further performance
improvements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3189</identifier>
 <datestamp>2013-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3189</id><created>2012-06-14</created><updated>2013-01-17</updated><authors><author><keyname>Rajan</keyname><forenames>Adithya</forenames></author><author><keyname>Tepedelenlioglu</keyname><forenames>Cihan</forenames></author></authors><title>A New Representation for the Symbol Error Rate</title><categories>cs.IT math.IT</categories><comments>26 pages, 3 figures. Parts of this work have been presented in the
  Asilomar Conference on Signals, Systems, and Computers, 2012, and the IEEE
  International Symposium on Information Theory, 2012. This work was supported
  in part by the National Science Foundation under Grant CCF 1117041</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The symbol error rate of the minimum distance detector for an arbitrary
multi-dimensional constellation impaired by additive white Gaussian noise is
characterized as the product of a completely monotone function with a
non-negative power of the signal to noise ratio. This representation is also
shown to apply to cases when the impairing noise is compound Gaussian. Using
this general result, it is proved that the symbol error rate is completely
monotone if the rank of its constellation matrix is either one or two. Further,
a necessary and sufficient condition for the complete monotonicity of the
symbol error rate of a constellation of any dimension is also obtained.
Applications to stochastic ordering of wireless system performance are also
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3193</identifier>
 <datestamp>2012-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3193</id><created>2012-06-14</created><authors><author><keyname>Galvin</keyname><forenames>David</forenames></author><author><keyname>Randall</keyname><forenames>Dana</forenames></author></authors><title>Torpid Mixing of Local Markov Chains on 3-Colorings of the Discrete
  Torus</title><categories>math.CO cs.DM</categories><comments>9 pages. Originally appeared in the proceedings of SODA 2007</comments><msc-class>68R10, 68W25, 05C15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study local Markov chains for sampling 3-colorings of the discrete torus
$T_{L,d}={0,..., L-1}^d$. We show that there is a constant $\rho \approx .22$
such that for all even $L \geq 4$ and $d$ sufficiently large, certain local
Markov chains require exponential time to converge to equilibrium. More
precisely, if $\cM$ is a Markov chain on the set of proper 3-colorings of
$T_{L,d}$ that updates the color of at most $\rho L^d$ vertices at each step
and whose stationary distribution is uniform, then the convergence to
stationarity of $\cM$ is exponential in $L^{d-1}$. Our proof is based on a
conductance argument that builds on sensitive new combinatorial enumeration
techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3202</identifier>
 <datestamp>2012-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3202</id><created>2012-06-14</created><authors><author><keyname>Galvin</keyname><forenames>David</forenames></author></authors><title>Sampling 3-colourings of regular bipartite graphs</title><categories>math.CO cs.DM</categories><comments>19 pages. Appeared in Electronic Journal of Probability in 2007</comments><msc-class>05C15, 82B20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that if $\gS=(V,E)$ is a regular bipartite graph for which the
expansion of subsets of a single parity of $V$ is reasonably good and which
satisfies a certain local condition (that the union of the neighbourhoods of
adjacent vertices does not contain too many pairwise non-adjacent vertices),
and if $\cM$ is a Markov chain on the set of proper 3-colourings of $\gS$ which
updates the colour of at most $\rho|V|$ vertices at each step and whose
stationary distribution is uniform, then for $\rho \approx .22$ and $d$
sufficiently large the convergence to stationarity of $\cM$ is (essentially)
exponential in $|V|$. In particular, if $\gS$ is the $d$-dimensional hypercube
$Q_d$ (the graph on vertex set $\{0,1\}^d$ in which two strings are adjacent if
they differ on exactly one coordinate) then the convergence to stationarity of
the well-known Glauber (single-site update) dynamics is exponentially slow in
$2^d/(\sqrt{d}\log d)$. A combinatorial corollary of our main result is that in
a uniform 3-colouring of $Q_d$ there is an exponentially small probability (in
$2^d$) that there is a colour $i$ such the proportion of vertices of the even
subcube coloured $i$ differs from the proportion of the odd subcube coloured
$i$ by at most $.22$. Our proof combines a conductance argument with
combinatorial enumeration methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3204</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3204</id><created>2012-06-14</created><updated>2012-06-15</updated><authors><author><keyname>Awasthi</keyname><forenames>Pranjal</forenames></author><author><keyname>Sheffet</keyname><forenames>Or</forenames></author></authors><title>Improved Spectral-Norm Bounds for Clustering</title><categories>cs.LG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Aiming to unify known results about clustering mixtures of distributions
under separation conditions, Kumar and Kannan[2010] introduced a deterministic
condition for clustering datasets. They showed that this single deterministic
condition encompasses many previously studied clustering assumptions. More
specifically, their proximity condition requires that in the target
$k$-clustering, the projection of a point $x$ onto the line joining its cluster
center $\mu$ and some other center $\mu'$, is a large additive factor closer to
$\mu$ than to $\mu'$. This additive factor can be roughly described as $k$
times the spectral norm of the matrix representing the differences between the
given (known) dataset and the means of the (unknown) target clustering.
Clearly, the proximity condition implies center separation -- the distance
between any two centers must be as large as the above mentioned bound.
  In this paper we improve upon the work of Kumar and Kannan along several
axes. First, we weaken the center separation bound by a factor of $\sqrt{k}$,
and secondly we weaken the proximity condition by a factor of $k$. Using these
weaker bounds we still achieve the same guarantees when all points satisfy the
proximity condition. We also achieve better guarantees when only
$(1-\epsilon)$-fraction of the points satisfy the weaker proximity condition.
The bulk of our analysis relies only on center separation under which one can
produce a clustering which (i) has low error, (ii) has low $k$-means cost, and
(iii) has centers very close to the target centers.
  Our improved separation condition allows us to match the results of the
Planted Partition Model of McSherry[2001], improve upon the results of
Ostrovsky et al[2006], and improve separation results for mixture of Gaussian
models in a particular setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3215</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3215</id><created>2012-06-14</created><updated>2012-08-12</updated><authors><author><keyname>Cloutier</keyname><forenames>B.</forenames></author><author><keyname>Muite</keyname><forenames>B. K.</forenames></author><author><keyname>Rigge</keyname><forenames>P.</forenames></author></authors><title>Performance of FORTRAN and C GPU Extensions for a Benchmark Suite of
  Fourier Pseudospectral Algorithms</title><categories>physics.comp-ph cs.MS</categories><comments>Revised version. New title. Forthcoming in proceedings of the
  Symposium on Application Accelerators in High-Performance computing (SAAHPC
  2012). Related programs are available for download</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A comparison of PGI OpenACC, FORTRAN CUDA, and Nvidia CUDA pseudospectral
methods on a single GPU and GCC FORTRAN on single and multiple CPU cores is
reported. The GPU implementations use CuFFT and the CPU implementations use
FFTW. Porting pre-existing FORTRAN codes to utilize a GPUs is efficient and
easy to implement with OpenACC and CUDA FORTRAN. Example programs are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3227</identifier>
 <datestamp>2012-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3227</id><created>2012-06-14</created><updated>2012-07-17</updated><authors><author><keyname>Sharad</keyname><forenames>Mrigank</forenames></author><author><keyname>Augustine</keyname><forenames>Charles</forenames></author><author><keyname>Panagopoulos</keyname><forenames>Georgios</forenames></author><author><keyname>Roy</keyname><forenames>Kaushik</forenames></author></authors><title>Proposal For Neuromorphic Hardware Using Spin Devices</title><categories>cond-mat.dis-nn cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a design-scheme for ultra-low power neuromorphic hardware using
emerging spin-devices. We propose device models for 'neuron', based on lateral
spin valves and domain wall magnets that can operate at ultra-low terminal
voltage of ~20 mV, resulting in small computation energy. Magnetic tunnel
junctions are employed for interfacing the spin-neurons with charge-based
devices like CMOS, for large-scale networks. Device-circuit
co-simulation-framework is used for simulating such hybrid designs, in order to
evaluate system-level performance. We present the design of different classes
of neuromorphic architectures using the proposed scheme that can be suitable
for different applications like, analog-data-sensing, data-conversion,
cognitive-computing, associative memory, programmable-logic and analog and
digital signal processing. We show that the spin-based neuromorphic designs can
achieve 15X-300X lower computation energy for these applications; as compared
to state of art CMOS designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3231</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3231</id><created>2012-06-13</created><authors><author><keyname>Brunskill</keyname><forenames>Emma</forenames></author><author><keyname>Leffler</keyname><forenames>Bethany</forenames></author><author><keyname>Li</keyname><forenames>Lihong</forenames></author><author><keyname>Littman</keyname><forenames>Michael L.</forenames></author><author><keyname>Roy</keyname><forenames>Nicholas</forenames></author></authors><title>CORL: A Continuous-state Offset-dynamics Reinforcement Learner</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-53-61</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Continuous state spaces and stochastic, switching dynamics characterize a
number of rich, realworld domains, such as robot navigation across varying
terrain. We describe a reinforcementlearning algorithm for learning in these
domains and prove for certain environments the algorithm is probably
approximately correct with a sample complexity that scales polynomially with
the state-space dimension. Unfortunately, no optimal planning techniques exist
in general for such problems; instead we use fitted value iteration to solve
the learned MDP, and include the error due to approximate planning in our
bounds. Finally, we report an experiment using a robotic car driving over
varying terrain to demonstrate that these dynamics representations adequately
capture real-world dynamics and that our algorithm can be used to efficiently
solve such problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3232</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3232</id><created>2012-06-13</created><authors><author><keyname>Gogate</keyname><forenames>Vibhav</forenames></author><author><keyname>Dechter</keyname><forenames>Rina</forenames></author></authors><title>AND/OR Importance Sampling</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-212-219</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper introduces AND/OR importance sampling for probabilistic graphical
models. In contrast to importance sampling, AND/OR importance sampling caches
samples in the AND/OR space and then extracts a new sample mean from the stored
samples. We prove that AND/OR importance sampling may have lower variance than
importance sampling; thereby providing a theoretical justification for
preferring it over importance sampling. Our empirical evaluation demonstrates
that AND/OR importance sampling is far more accurate than importance sampling
in many cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3233</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3233</id><created>2012-06-13</created><authors><author><keyname>Isaza</keyname><forenames>Alejandro</forenames></author><author><keyname>Szepesvari</keyname><forenames>Csaba</forenames></author><author><keyname>Bulitko</keyname><forenames>Vadim</forenames></author><author><keyname>Greiner</keyname><forenames>Russell</forenames></author></authors><title>Speeding Up Planning in Markov Decision Processes via Automatically
  Constructed Abstractions</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-306-314</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider planning in stochastic shortest path (SSP)
problems, a subclass of Markov Decision Problems (MDP). We focus on medium-size
problems whose state space can be fully enumerated. This problem has numerous
important applications, such as navigation and planning under uncertainty. We
propose a new approach for constructing a multi-level hierarchy of
progressively simpler abstractions of the original problem. Once computed, the
hierarchy can be used to speed up planning by first finding a policy for the
most abstract level and then recursively refining it into a solution to the
original problem. This approach is fully automated and delivers a speed-up of
two orders of magnitude over a state-of-the-art MDP solver on sample problems
while returning near-optimal solutions. We also prove theoretical bounds on the
loss of solution optimality resulting from the use of abstractions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3234</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3234</id><created>2012-06-13</created><authors><author><keyname>Acar</keyname><forenames>Umut A.</forenames></author><author><keyname>Ihler</keyname><forenames>Alexander T.</forenames></author><author><keyname>Mettu</keyname><forenames>Ramgopal</forenames></author><author><keyname>Sumer</keyname><forenames>Ozgur</forenames></author></authors><title>Adaptive Inference on General Graphical Models</title><categories>cs.DS cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-1-8</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many algorithms and applications involve repeatedly solving variations of the
same inference problem; for example we may want to introduce new evidence to
the model or perform updates to conditional dependencies. The goal of adaptive
inference is to take advantage of what is preserved in the model and perform
inference more rapidly than from scratch. In this paper, we describe techniques
for adaptive inference on general graphs that support marginal computation and
updates to the conditional probabilities and dependencies in logarithmic time.
We give experimental results for an implementation of our algorithm, and
demonstrate its potential performance benefit in the study of protein
structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3235</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3235</id><created>2012-06-13</created><authors><author><keyname>Antos</keyname><forenames>Dimitrios</forenames></author><author><keyname>Pfeffer</keyname><forenames>Avi</forenames></author></authors><title>Identifying reasoning patterns in games</title><categories>cs.GT cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-9-17</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm that identifies the reasoning patterns of agents in a
game, by iteratively examining the graph structure of its Multi-Agent Influence
Diagram (MAID) representation. If the decision of an agent participates in no
reasoning patterns, then we can effectively ignore that decision for the
purpose of calculating a Nash equilibrium for the game. In some cases, this can
lead to exponential time savings in the process of equilibrium calculation.
Moreover, our algorithm can be used to enumerate the reasoning patterns in a
game, which can be useful for constructing more effective computerized agents
interacting with humans.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3236</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3236</id><created>2012-06-13</created><authors><author><keyname>Auvray</keyname><forenames>Vincent</forenames></author><author><keyname>Wehenkel</keyname><forenames>Louis</forenames></author></authors><title>Learning Inclusion-Optimal Chordal Graphs</title><categories>cs.LG cs.DS stat.ML</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-18-25</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Chordal graphs can be used to encode dependency models that are representable
by both directed acyclic and undirected graphs. This paper discusses a very
simple and efficient algorithm to learn the chordal structure of a
probabilistic model from data. The algorithm is a greedy hill-climbing search
algorithm that uses the inclusion boundary neighborhood over chordal graphs. In
the limit of a large sample size and under appropriate hypotheses on the
scoring criterion, we prove that the algorithm will find a structure that is
inclusion-optimal when the dependency model of the data-generating distribution
can be represented exactly by an undirected graph. The algorithm is evaluated
on simulated datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3237</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3237</id><created>2012-06-13</created><authors><author><keyname>Barber</keyname><forenames>David</forenames></author></authors><title>Clique Matrices for Statistical Graph Decomposition and Parameterising
  Restricted Positive Definite Matrices</title><categories>cs.DM cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-26-33</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce Clique Matrices as an alternative representation of undirected
graphs, being a generalisation of the incidence matrix representation. Here we
use clique matrices to decompose a graph into a set of possibly overlapping
clusters, de ned as well-connected subsets of vertices. The decomposition is
based on a statistical description which encourages clusters to be well
connected and few in number. Inference is carried out using a variational
approximation. Clique matrices also play a natural role in parameterising
positive de nite matrices under zero constraints on elements of the matrix. We
show that clique matrices can parameterise all positive de nite matrices
restricted according to a decomposable graph and form a structured Factor
Analysis approximation in the non-decomposable case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3238</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3238</id><created>2012-06-13</created><authors><author><keyname>Bo</keyname><forenames>Liefeng</forenames></author><author><keyname>Sminchisescu</keyname><forenames>Cristian</forenames></author></authors><title>Greedy Block Coordinate Descent for Large Scale Gaussian Process
  Regression</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-43-52</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a variable decomposition algorithm -greedy block coordinate
descent (GBCD)- in order to make dense Gaussian process regression practical
for large scale problems. GBCD breaks a large scale optimization into a series
of small sub-problems. The challenge in variable decomposition algorithms is
the identification of a subproblem (the active set of variables) that yields
the largest improvement. We analyze the limitations of existing methods and
cast the active set selection into a zero-norm constrained optimization problem
that we solve using greedy methods. By directly estimating the decrease in the
objective function, we obtain not only efficient approximate solutions for
GBCD, but we are also able to demonstrate that the method is globally
convergent. Empirical comparisons against competing dense methods like
Conjugate Gradient or SMO show that GBCD is an order of magnitude faster.
Comparisons against sparse GP methods show that GBCD is both accurate and
capable of handling datasets of 100,000 samples or more.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3239</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3239</id><created>2012-06-13</created><authors><author><keyname>Cai</keyname><forenames>Zhihong</forenames></author><author><keyname>Kuroki</keyname><forenames>Manabu</forenames></author></authors><title>On Identifying Total Effects in the Presence of Latent Variables and
  Selection bias</title><categories>stat.ME cs.AI stat.AP</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-62-69</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Assume that cause-effect relationships between variables can be described as
a directed acyclic graph and the corresponding linear structural equation
model.We consider the identification problem of total effects in the presence
of latent variables and selection bias between a treatment variable and a
response variable. Pearl and his colleagues provided the back door criterion,
the front door criterion (Pearl, 2000) and the conditional instrumental
variable method (Brito and Pearl, 2002) as identifiability criteria for total
effects in the presence of latent variables, but not in the presence of
selection bias. In order to solve this problem, we propose new graphical
identifiability criteria for total effects based on the identifiable factor
models. The results of this paper are useful to identify total effects in
observational studies and provide a new viewpoint to the identification
conditions of factor models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3240</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3240</id><created>2012-06-13</created><authors><author><keyname>Chandrasekaran</keyname><forenames>Venkat</forenames></author><author><keyname>Srebro</keyname><forenames>Nathan</forenames></author><author><keyname>Harsha</keyname><forenames>Prahladh</forenames></author></authors><title>Complexity of Inference in Graphical Models</title><categories>cs.DS cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-70-78</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well-known that inference in graphical models is hard in the worst
case, but tractable for models with bounded treewidth. We ask whether treewidth
is the only structural criterion of the underlying graph that enables tractable
inference. In other words, is there some class of structures with unbounded
treewidth in which inference is tractable? Subject to a combinatorial
hypothesis due to Robertson et al. (1994), we show that low treewidth is indeed
the only structural restriction that can ensure tractability. Thus, even for
the ?best case? graph structure, there is no inference algorithm with
complexity polynomial in the treewidth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3241</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3241</id><created>2012-06-13</created><authors><author><keyname>Choi</keyname><forenames>Arthur</forenames></author><author><keyname>Darwiche</keyname><forenames>Adnan</forenames></author></authors><title>Approximating the Partition Function by Deleting and then Correcting for
  Model Edges</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-79-87</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an approach for approximating the partition function which is
based on two steps: (1) computing the partition function of a simplified model
which is obtained by deleting model edges, and (2) rectifying the result by
applying an edge-by-edge correction. The approach leads to an intuitive
framework in which one can trade-off the quality of an approximation with the
complexity of computing it. It also includes the Bethe free energy
approximation as a degenerate case. We develop the approach theoretically in
this paper and provide a number of empirical results that reveal its practical
utility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3242</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3242</id><created>2012-06-13</created><authors><author><keyname>Christoudias</keyname><forenames>C.</forenames></author><author><keyname>Urtasun</keyname><forenames>Raquel</forenames></author><author><keyname>Darrell</keyname><forenames>Trevor</forenames></author></authors><title>Multi-View Learning in the Presence of View Disagreement</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-88-96</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional multi-view learning approaches suffer in the presence of view
disagreement,i.e., when samples in each view do not belong to the same class
due to view corruption, occlusion or other noise processes. In this paper we
present a multi-view learning approach that uses a conditional entropy
criterion to detect view disagreement. Once detected, samples with view
disagreement are filtered and standard multi-view learning methods can be
successfully applied to the remaining samples. Experimental evaluation on
synthetic and audio-visual databases demonstrates that the detection and
filtering of view disagreement considerably increases the performance of
traditional multi-view learning approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3243</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3243</id><created>2012-06-13</created><authors><author><keyname>Cseke</keyname><forenames>Botond</forenames></author><author><keyname>Heskes</keyname><forenames>Tom</forenames></author></authors><title>Bounds on the Bethe Free Energy for Gaussian Networks</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-97-104</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of computing approximate marginals in Gaussian
probabilistic models by using mean field and fractional Bethe approximations.
As an extension of Welling and Teh (2001), we define the Gaussian fractional
Bethe free energy in terms of the moment parameters of the approximate
marginals and derive an upper and lower bound for it. We give necessary
conditions for the Gaussian fractional Bethe free energies to be bounded from
below. It turns out that the bounding condition is the same as the pairwise
normalizability condition derived by Malioutov et al. (2006) as a sufficient
condition for the convergence of the message passing algorithm. By giving a
counterexample, we disprove the conjecture in Welling and Teh (2001): even when
the Bethe free energy is not bounded from below, it can possess a local minimum
to which the minimization algorithms can converge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3244</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3244</id><created>2012-06-13</created><authors><author><keyname>Cussens</keyname><forenames>James</forenames></author></authors><title>Bayesian network learning by compiling to weighted MAX-SAT</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-105-112</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of learning discrete Bayesian networks from data is encoded as a
weighted MAX-SAT problem and the MaxWalkSat local search algorithm is used to
address it. For each dataset, the per-variable summands of the (BDeu) marginal
likelihood for different choices of parents ('family scores') are computed
prior to applying MaxWalkSat. Each permissible choice of parents for each
variable is encoded as a distinct propositional atom and the associated family
score encoded as a 'soft' weighted single-literal clause. Two approaches to
enforcing acyclicity are considered: either by encoding the ancestor relation
or by attaching a total order to each graph and encoding that. The latter
approach gives better results. Learning experiments have been conducted on 21
synthetic datasets sampled from 7 BNs. The largest dataset has 10,000
datapoints and 60 variables producing (for the 'ancestor' encoding) a weighted
CNF input file with 19,932 atoms and 269,367 clauses. For most datasets,
MaxWalkSat quickly finds BNs with higher BDeu score than the 'true' BN. The
effect of adding prior information is assessed. It is further shown that
Bayesian model averaging can be effected by collecting BNs generated during the
search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3245</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3245</id><created>2012-06-13</created><authors><author><keyname>Dawid</keyname><forenames>Philip</forenames></author><author><keyname>Didelez</keyname><forenames>Vanessa</forenames></author></authors><title>Identifying Optimal Sequential Decisions</title><categories>cs.AI math.ST stat.ME stat.TH</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-113-120</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider conditions that allow us to find an optimal strategy for
sequential decisions from a given data situation. For the case where all
interventions are unconditional (atomic), identifiability has been discussed by
Pearl &amp; Robins (1995). We argue here that an optimal strategy must be
conditional, i.e. take the information available at each decision point into
account. We show that the identification of an optimal sequential decision
strategy is more restrictive, in the sense that conditional interventions might
not always be identified when atomic interventions are. We further demonstrate
that a simple graphical criterion for the identifiability of an optimal
strategy can be given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3246</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3246</id><created>2012-06-13</created><authors><author><keyname>de Campos</keyname><forenames>Cassio Polpo</forenames></author><author><keyname>Ji</keyname><forenames>Qiang</forenames></author></authors><title>Strategy Selection in Influence Diagrams using Imprecise Probabilities</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-121-128</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a new algorithm to solve the decision making problem in
Influence Diagrams based on algorithms for credal networks. Decision nodes are
associated to imprecise probability distributions and a reformulation is
introduced that finds the global maximum strategy with respect to the expected
utility. We work with Limited Memory Influence Diagrams, which generalize most
Influence Diagram proposals and handle simultaneous decisions. Besides the
global optimum method, we explore an anytime approximate solution with a
guaranteed maximum error and show that imprecise probabilities are handled in a
straightforward way. Complexity issues and experiments with random diagrams and
an effects-based military planning problem are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3247</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3247</id><created>2012-06-13</created><authors><author><keyname>Domke</keyname><forenames>Justin</forenames></author></authors><title>Learning Convex Inference of Marginals</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-137-144</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graphical models trained using maximum likelihood are a common tool for
probabilistic inference of marginal distributions. However, this approach
suffers difficulties when either the inference process or the model is
approximate. In this paper, the inference process is first defined to be the
minimization of a convex function, inspired by free energy approximations.
Learning is then done directly in terms of the performance of the inference
process at univariate marginal prediction. The main novelty is that this is a
direct minimization of emperical risk, where the risk measures the accuracy of
predicted marginals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3248</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3248</id><created>2012-06-13</created><authors><author><keyname>Duong</keyname><forenames>Quang</forenames></author><author><keyname>Wellman</keyname><forenames>Michael P.</forenames></author><author><keyname>Singh</keyname><forenames>Satinder</forenames></author></authors><title>Knowledge Combination in Graphical Multiagent Model</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-145-152</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graphical multiagent model (GMM) represents a joint distribution over the
behavior of a set of agents. One source of knowledge about agents' behavior may
come from gametheoretic analysis, as captured by several graphical game
representations developed in recent years. GMMs generalize this approach to
express arbitrary distributions, based on game descriptions or other sources of
knowledge bearing on beliefs about agent behavior. To illustrate the
flexibility of GMMs, we exhibit game-derived models that allow probabilistic
deviation from equilibrium, as well as models based on heuristic action choice.
We investigate three different methods of integrating these models into a
single model representing the combined knowledge sources. To evaluate the
predictive performance of the combined model, we treat as actual outcome the
behavior produced by a reinforcement learning process. We find that combining
the two knowledge sources, using any of the methods, provides better
predictions than either source alone. Among the combination methods, mixing
data outperforms the opinion pool and direct update methods investigated in
this empirical trial.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3249</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3249</id><created>2012-06-13</created><authors><author><keyname>Duchi</keyname><forenames>John</forenames></author><author><keyname>Gould</keyname><forenames>Stephen</forenames></author><author><keyname>Koller</keyname><forenames>Daphne</forenames></author></authors><title>Projected Subgradient Methods for Learning Sparse Gaussians</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-153-160</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gaussian Markov random fields (GMRFs) are useful in a broad range of
applications. In this paper we tackle the problem of learning a sparse GMRF in
a high-dimensional space. Our approach uses the l1-norm as a regularization on
the inverse covariance matrix. We utilize a novel projected gradient method,
which is faster than previous methods in practice and equal to the best
performing of these in asymptotic complexity. We also extend the l1-regularized
objective to the problem of sparsifying entire blocks within the inverse
covariance matrix. Our methods generalize fairly easily to this case, while
other methods do not. We demonstrate that our extensions give better
generalization performance on two real domains--biological network analysis and
a 2D-shape modeling image task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3250</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3250</id><created>2012-06-13</created><authors><author><keyname>Eberhardt</keyname><forenames>Frederick</forenames></author></authors><title>Almost Optimal Intervention Sets for Causal Discovery</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-161-168</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We conjecture that the worst case number of experiments necessary and
sufficient to discover a causal graph uniquely given its observational Markov
equivalence class can be specified as a function of the largest clique in the
Markov equivalence class. We provide an algorithm that computes intervention
sets that we believe are optimal for the above task. The algorithm builds on
insights gained from the worst case analysis in Eberhardt et al. (2005) for
sequences of experiments when all possible directed acyclic graphs over N
variables are considered. A simulation suggests that our conjecture is correct.
We also show that a generalization of our conjecture to other classes of
possible graph hypotheses cannot be given easily, and in what sense the
algorithm is then no longer optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3251</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3251</id><created>2012-06-13</created><authors><author><keyname>El-Hay</keyname><forenames>Tal</forenames></author><author><keyname>Friedman</keyname><forenames>Nir</forenames></author><author><keyname>Kupferman</keyname><forenames>Raz</forenames></author></authors><title>Gibbs Sampling in Factorized Continuous-Time Markov Processes</title><categories>cs.AI stat.CO</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-169-178</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A central task in many applications is reasoning about processes that change
over continuous time. Continuous-Time Bayesian Networks is a general compact
representation language for multi-component continuous-time processes. However,
exact inference in such processes is exponential in the number of components,
and thus infeasible for most models of interest. Here we develop a novel Gibbs
sampling procedure for multi-component processes. This procedure iteratively
samples a trajectory for one of the components given the remaining ones. We
show how to perform exact sampling that adapts to the natural time scale of the
sampled process. Moreover, we show that this sampling procedure naturally
exploits the structure of the network to reduce the computational cost of each
step. This procedure is the first that can provide asymptotically unbiased
approximation in such processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3252</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3252</id><created>2012-06-13</created><authors><author><keyname>Elidan</keyname><forenames>Gal</forenames></author><author><keyname>Packer</keyname><forenames>Ben</forenames></author><author><keyname>Heitz</keyname><forenames>Geremy</forenames></author><author><keyname>Koller</keyname><forenames>Daphne</forenames></author></authors><title>Convex Point Estimation using Undirected Bayesian Transfer Hierarchies</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-179-187</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When related learning tasks are naturally arranged in a hierarchy, an
appealing approach for coping with scarcity of instances is that of transfer
learning using a hierarchical Bayes framework. As fully Bayesian computations
can be difficult and computationally demanding, it is often desirable to use
posterior point estimates that facilitate (relatively) efficient prediction.
However, the hierarchical Bayes framework does not always lend itself naturally
to this maximum aposteriori goal. In this work we propose an undirected
reformulation of hierarchical Bayes that relies on priors in the form of
similarity measures. We introduce the notion of &quot;degree of transfer&quot; weights on
components of these similarity measures, and show how they can be automatically
learned within a joint probabilistic framework. Importantly, our reformulation
results in a convex objective for many learning problems, thus facilitating
optimal posterior point estimation using standard optimization techniques. In
addition, we no longer require proper priors, allowing for flexible and
straightforward specification of joint distributions over transfer hierarchies.
We show that our framework is effective for learning models that are part of
transfer hierarchies for two real-life tasks: object shape modeling using
Gaussian density estimation and document classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3253</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3253</id><created>2012-06-13</created><authors><author><keyname>Ficici</keyname><forenames>Sevan G.</forenames></author><author><keyname>Parkes</keyname><forenames>David C.</forenames></author><author><keyname>Pfeffer</keyname><forenames>Avi</forenames></author></authors><title>Learning and Solving Many-Player Games through a Cluster-Based
  Representation</title><categories>cs.GT cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-188-195</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In addressing the challenge of exponential scaling with the number of agents
we adopt a cluster-based representation to approximately solve asymmetric games
of very many players. A cluster groups together agents with a similar
&quot;strategic view&quot; of the game. We learn the clustered approximation from data
consisting of strategy profiles and payoffs, which may be obtained from
observations of play or access to a simulator. Using our clustering we
construct a reduced &quot;twins&quot; game in which each cluster is associated with two
players of the reduced game. This allows our representation to be individually-
responsive because we align the interests of every individual agent with the
strategy of its cluster. Our approach provides agents with higher payoffs and
lower regret on average than model-free methods as well as previous
cluster-based methods, and requires only few observations for learning to be
successful. The &quot;twins&quot; approach is shown to be an important component of
providing these low regret approximations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3254</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3254</id><created>2012-06-13</created><authors><author><keyname>Gruber</keyname><forenames>Amit</forenames></author><author><keyname>Rosen-Zvi</keyname><forenames>Michal</forenames></author><author><keyname>Weiss</keyname><forenames>Yair</forenames></author></authors><title>Latent Topic Models for Hypertext</title><categories>cs.IR cs.CL cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-230-239</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Latent topic models have been successfully applied as an unsupervised topic
discovery technique in large document collections. With the proliferation of
hypertext document collection such as the Internet, there has also been great
interest in extending these approaches to hypertext [6, 9]. These approaches
typically model links in an analogous fashion to how they model words - the
document-link co-occurrence matrix is modeled in the same way that the
document-word co-occurrence matrix is modeled in standard topic models. In this
paper we present a probabilistic generative model for hypertext document
collections that explicitly models the generation of links. Specifically, links
from a word w to a document d depend directly on how frequent the topic of w is
in d, in addition to the in-degree of d. We show how to perform EM learning on
this model efficiently. By not modeling links as analogous to words, we end up
using far fewer free parameters and obtain better link prediction results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3255</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3255</id><created>2012-06-13</created><updated>2014-07-15</updated><authors><author><keyname>Goodman</keyname><forenames>Noah</forenames></author><author><keyname>Mansinghka</keyname><forenames>Vikash</forenames></author><author><keyname>Roy</keyname><forenames>Daniel M.</forenames></author><author><keyname>Bonawitz</keyname><forenames>Keith</forenames></author><author><keyname>Tenenbaum</keyname><forenames>Joshua B.</forenames></author></authors><title>Church: a language for generative models</title><categories>cs.PL cs.AI cs.LO</categories><comments>Minor revisions. Fixed errors in author list</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-220-229</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce Church, a universal language for describing stochastic
generative processes. Church is based on the Lisp model of lambda calculus,
containing a pure Lisp as its deterministic subset. The semantics of Church is
defined in terms of evaluation histories and conditional distributions on such
histories. Church also includes a novel language construct, the stochastic
memoizer, which enables simple description of many complex non-parametric
models. We illustrate language features through several examples, including: a
generalized Bayes net in which parameters cluster over trials, infinite PCFGs,
planning by inference, and various non-parametric clustering models. Finally,
we show how to implement query on any Church program, exactly and
approximately, using Monte Carlo techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3256</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3256</id><created>2012-06-13</created><authors><author><keyname>Ganchev</keyname><forenames>Kuzman</forenames></author><author><keyname>Graca</keyname><forenames>Joao</forenames></author><author><keyname>Blitzer</keyname><forenames>John</forenames></author><author><keyname>Taskar</keyname><forenames>Ben</forenames></author></authors><title>Multi-View Learning over Structured and Non-Identical Outputs</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-204-211</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many machine learning problems, labeled training data is limited but
unlabeled data is ample. Some of these problems have instances that can be
factored into multiple views, each of which is nearly sufficent in determining
the correct labels. In this paper we present a new algorithm for probabilistic
multi-view learning which uses the idea of stochastic agreement between views
as regularization. Our algorithm works on structured and unstructured problems
and easily generalizes to partial agreement scenarios. For the full agreement
case, our algorithm minimizes the Bhattacharyya distance between the models of
each view, and performs better than CoBoosting and two-view Perceptron on
several flat and structured classification problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3257</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3257</id><created>2012-06-13</created><authors><author><keyname>Ganapathi</keyname><forenames>Varun</forenames></author><author><keyname>Vickrey</keyname><forenames>David</forenames></author><author><keyname>Duchi</keyname><forenames>John</forenames></author><author><keyname>Koller</keyname><forenames>Daphne</forenames></author></authors><title>Constrained Approximate Maximum Entropy Learning of Markov Random Fields</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-196-203</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parameter estimation in Markov random fields (MRFs) is a difficult task, in
which inference over the network is run in the inner loop of a gradient descent
procedure. Replacing exact inference with approximate methods such as loopy
belief propagation (LBP) can suffer from poor convergence. In this paper, we
provide a different approach for combining MRF learning and Bethe
approximation. We consider the dual of maximum likelihood Markov network
learning - maximizing entropy with moment matching constraints - and then
approximate both the objective and the constraints in the resulting
optimization problem. Unlike previous work along these lines (Teh &amp; Welling,
2003), our formulation allows parameter sharing between features in a general
log-linear model, parameter regularization and conditional training. We show
that piecewise training (Sutton &amp; McCallum, 2005) is a very restricted special
case of this formulation. We study two optimization strategies: one based on a
single convex approximation and one that uses repeated convex approximations.
We show results on several real-world networks that demonstrate that these
algorithms can significantly outperform learning with loopy and piecewise. Our
results also provide a framework for analyzing the trade-offs of different
relaxations of the entropy objective and of the constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3258</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3258</id><created>2012-06-13</created><authors><author><keyname>Hui</keyname><forenames>Bowen</forenames></author><author><keyname>Boutilier</keyname><forenames>Craig</forenames></author></authors><title>Toward Experiential Utility Elicitation for Interface Customization</title><categories>cs.AI cs.HC</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-298-305</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  User preferences for automated assistance often vary widely, depending on the
situation, and quality or presentation of help. Developing effectivemodels to
learn individual preferences online requires domain models that associate
observations of user behavior with their utility functions, which in turn can
be constructed using utility elicitation techniques. However, most elicitation
methods ask for users' predicted utilities based on hypothetical scenarios
rather than more realistic experienced utilities. This is especially true in
interface customization, where users are asked to assess novel interface
designs. We propose experiential utility elicitation methods for customization
and compare these to predictivemethods. As experienced utilities have been
argued to better reflect true preferences in behavioral decision making, the
purpose here is to investigate accurate and efficient procedures that are
suitable for software domains. Unlike conventional elicitation, our results
indicate that an experiential approach helps people understand stochastic
outcomes, as well as better appreciate the sequential utility of intelligent
assistance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3259</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3259</id><created>2012-06-13</created><authors><author><keyname>Huang</keyname><forenames>Jim</forenames></author><author><keyname>Frey</keyname><forenames>Brendan J.</forenames></author></authors><title>Cumulative distribution networks and the derivative-sum-product
  algorithm</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-290-297</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new type of graphical model called a &quot;cumulative distribution
network&quot; (CDN), which expresses a joint cumulative distribution as a product of
local functions. Each local function can be viewed as providing evidence about
possible orderings, or rankings, of variables. Interestingly, we find that the
conditional independence properties of CDNs are quite different from other
graphical models. We also describe a messagepassing algorithm that efficiently
computes conditional cumulative distributions. Due to the unique independence
properties of the CDN, these messages do not in general have a one-to-one
correspondence with messages exchanged in standard algorithms, such as belief
propagation. We demonstrate the application of CDNs for structured ranking
learning using a previously-studied multi-player gaming dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3260</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3260</id><created>2012-06-13</created><authors><author><keyname>Hoyer</keyname><forenames>Patrik O.</forenames></author><author><keyname>Hyvarinen</keyname><forenames>Aapo</forenames></author><author><keyname>Scheines</keyname><forenames>Richard</forenames></author><author><keyname>Spirtes</keyname><forenames>Peter L.</forenames></author><author><keyname>Ramsey</keyname><forenames>Joseph</forenames></author><author><keyname>Lacerda</keyname><forenames>Gustavo</forenames></author><author><keyname>Shimizu</keyname><forenames>Shohei</forenames></author></authors><title>Causal discovery of linear acyclic models with arbitrary distributions</title><categories>stat.ML cs.AI cs.LG</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-282-289</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important task in data analysis is the discovery of causal relationships
between observed variables. For continuous-valued data, linear acyclic causal
models are commonly used to model the data-generating process, and the
inference of such models is a well-studied problem. However, existing methods
have significant limitations. Methods based on conditional independencies
(Spirtes et al. 1993; Pearl 2000) cannot distinguish between
independence-equivalent models, whereas approaches purely based on Independent
Component Analysis (Shimizu et al. 2006) are inapplicable to data which is
partially Gaussian. In this paper, we generalize and combine the two
approaches, to yield a method able to learn the model structure in many cases
for which the previous methods provide answers that are either incorrect or are
not as informative as possible. We give exact graphical conditions for when two
distinct models represent the same family of distributions, and empirically
demonstrate the power of our method through thorough simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3261</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3261</id><created>2012-06-13</created><authors><author><keyname>Hines</keyname><forenames>Greg</forenames></author><author><keyname>Larson</keyname><forenames>Kate</forenames></author></authors><title>Learning When to Take Advice: A Statistical Test for Achieving A
  Correlated Equilibrium</title><categories>cs.GT cs.AI cs.MA</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-274-281</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a multiagent learning problem where agents can either learn via
repeated interactions, or can follow the advice of a mediator who suggests
possible actions to take. We present an algorithmthat each agent can use so
that, with high probability, they can verify whether or not the mediator's
advice is useful. In particular, if the mediator's advice is useful then agents
will reach a correlated equilibrium, but if the mediator's advice is not
useful, then agents are not harmed by using our test, and can fall back to
their original learning algorithm. We then generalize our algorithm and show
that in the limit it always correctly verifies the mediator's advice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3262</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3262</id><created>2012-06-13</created><authors><author><keyname>Hazan</keyname><forenames>Tamir</forenames></author><author><keyname>Shashua</keyname><forenames>Amnon</forenames></author></authors><title>Convergent Message-Passing Algorithms for Inference over General Graphs
  with Convex Free Energies</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-264-273</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inference problems in graphical models can be represented as a constrained
optimization of a free energy function. It is known that when the Bethe free
energy is used, the fixedpoints of the belief propagation (BP) algorithm
correspond to the local minima of the free energy. However BP fails to converge
in many cases of interest. Moreover, the Bethe free energy is non-convex for
graphical models with cycles thus introducing great difficulty in deriving
efficient algorithms for finding local minima of the free energy for general
graphs. In this paper we introduce two efficient BP-like algorithms, one
sequential and the other parallel, that are guaranteed to converge to the
global minimum, for any graph, over the class of energies known as &quot;convex free
energies&quot;. In addition, we propose an efficient heuristic for setting the
parameters of the convex free energy based on the structure of the graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3263</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3263</id><created>2012-06-13</created><authors><author><keyname>Hansen</keyname><forenames>Eric A.</forenames></author></authors><title>Sparse Stochastic Finite-State Controllers for POMDPs</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-256-263</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bounded policy iteration is an approach to solving infinite-horizon POMDPs
that represents policies as stochastic finite-state controllers and iteratively
improves a controller by adjusting the parameters of each node using linear
programming. In the original algorithm, the size of the linear programs, and
thus the complexity of policy improvement, depends on the number of parameters
of each node, which grows with the size of the controller. But in practice, the
number of parameters of a node with non-zero values is often very small, and
does not grow with the size of the controller. Based on this observation, we
develop a version of bounded policy iteration that leverages the sparse
structure of a stochastic finite-state controller. In each iteration, it
improves a policy by the same amount as the original algorithm, but with much
better scalability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3264</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3264</id><created>2012-06-13</created><authors><author><keyname>Hajishirzi</keyname><forenames>Hannaneh</forenames></author><author><keyname>Amir</keyname><forenames>Eyal</forenames></author></authors><title>Sampling First Order Logical Particles</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-248-255</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Approximate inference in dynamic systems is the problem of estimating the
state of the system given a sequence of actions and partial observations. High
precision estimation is fundamental in many applications like diagnosis,
natural language processing, tracking, planning, and robotics. In this paper we
present an algorithm that samples possible deterministic executions of a
probabilistic sequence. The algorithm takes advantage of a compact
representation (using first order logic) for actions and world states to
improve the precision of its estimation. Theoretical and empirical results show
that the algorithm's expected error is smaller than propositional sampling and
Sequential Monte Carlo (SMC) sampling techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3265</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3265</id><created>2012-06-13</created><authors><author><keyname>Kwisthout</keyname><forenames>Johan</forenames></author><author><keyname>van der Gaag</keyname><forenames>Linda C.</forenames></author></authors><title>The Computational Complexity of Sensitivity Analysis and Parameter
  Tuning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-349-356</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While known algorithms for sensitivity analysis and parameter tuning in
probabilistic networks have a running time that is exponential in the size of
the network, the exact computational complexity of these problems has not been
established as yet. In this paper we study several variants of the tuning
problem and show that these problems are NPPP-complete in general. We further
show that the problems remain NP-complete or PP-complete, for a number of
restricted variants. These complexity results provide insight in whether or not
recent achievements in sensitivity analysis and tuning can be extended to more
general, practicable methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3266</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3266</id><created>2012-06-13</created><authors><author><keyname>Kveton</keyname><forenames>Branislav</forenames></author><author><keyname>Hauskrecht</keyname><forenames>Milos</forenames></author></authors><title>Partitioned Linear Programming Approximations for MDPs</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-341-348</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Approximate linear programming (ALP) is an efficient approach to solving
large factored Markov decision processes (MDPs). The main idea of the method is
to approximate the optimal value function by a set of basis functions and
optimize their weights by linear programming (LP). This paper proposes a new
ALP approximation. Comparing to the standard ALP formulation, we decompose the
constraint space into a set of low-dimensional spaces. This structure allows
for solving the new LP efficiently. In particular, the constraints of the LP
can be satisfied in a compact form without an exponential dependence on the
treewidth of ALP constraints. We study both practical and theoretical aspects
of the proposed approach. Moreover, we demonstrate its scale-up potential on an
MDP with more than 2^100 states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3267</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3267</id><created>2012-06-13</created><authors><author><keyname>Kuroki</keyname><forenames>Manabu</forenames></author><author><keyname>Cai</keyname><forenames>Zhihong</forenames></author></authors><title>The Evaluation of Causal Effects in Studies with an Unobserved
  Exposure/Outcome Variable: Bounds and Identification</title><categories>stat.ME cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-333-340</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the problem of evaluating the causal effect using
observational data in the presence of an unobserved exposure/ outcome variable,
when cause-effect relationships between variables can be described as a
directed acyclic graph and the corresponding recursive factorization of a joint
distribution. First, we propose identifiability criteria for causal effects
when an unobserved exposure/outcome variable is considered to contain more than
two categories. Next, when unmeasured variables exist between an unobserved
outcome variable and its proxy variables, we provide the tightest bounds based
on the potential outcome approach. The results of this paper are helpful to
evaluate causal effects in the case where it is difficult or expensive to
observe an exposure/ outcome variable in many practical fields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3269</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3269</id><created>2012-06-13</created><authors><author><keyname>Jebara</keyname><forenames>Tony S.</forenames></author></authors><title>Bayesian Out-Trees</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-315-324</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Bayesian treatment of latent directed graph structure for non-iid data is
provided where each child datum is sampled with a directed conditional
dependence on a single unknown parent datum. The latent graph structure is
assumed to lie in the family of directed out-tree graphs which leads to
efficient Bayesian inference. The latent likelihood of the data and its
gradients are computable in closed form via Tutte's directed matrix tree
theorem using determinants and inverses of the out-Laplacian. This novel
likelihood subsumes iid likelihood, is exchangeable and yields efficient
unsupervised and semi-supervised learning algorithms. In addition to handling
taxonomy and phylogenetic datasets the out-tree assumption performs
surprisingly well as a semi-parametric density estimator on standard iid
datasets. Experiments with unsupervised and semisupervised learning are shown
on various UCI and taxonomy datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3270</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3270</id><created>2012-06-13</created><authors><author><keyname>Meila</keyname><forenames>Marina</forenames></author><author><keyname>Bao</keyname><forenames>Le</forenames></author></authors><title>Estimation and Clustering with Infinite Rankings</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-393-402</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a natural extension of stagewise ranking to the the case
of infinitely many items. We introduce the infinite generalized Mallows model
(IGM), describe its properties and give procedures to estimate it from data.
For estimation of multimodal distributions we introduce the
Exponential-Blurring-Mean-Shift nonparametric clustering algorithm. The
experiments highlight the properties of the new model and demonstrate that
infinite models can be simple, elegant and practical.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3271</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3271</id><created>2012-06-13</created><authors><author><keyname>Lowd</keyname><forenames>Daniel</forenames></author><author><keyname>Domingos</keyname><forenames>Pedro</forenames></author></authors><title>Learning Arithmetic Circuits</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-383-392</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graphical models are usually learned without regard to the cost of doing
inference with them. As a result, even if a good model is learned, it may
perform poorly at prediction, because it requires approximate inference. We
propose an alternative: learning models with a score function that directly
penalizes the cost of inference. Specifically, we learn arithmetic circuits
with a penalty on the number of edges in the circuit (in which the cost of
inference is linear). Our algorithm is equivalent to learning a Bayesian
network with context-specific independence by greedily splitting conditional
distributions, at each step scoring the candidates by compiling the resulting
network into an arithmetic circuit, and using its size as the penalty. We show
how this can be done efficiently, without compiling a circuit from scratch for
each candidate. Experiments on several real-world domains show that our
algorithm is able to learn tractable models with very large treewidth, and
yields more accurate predictions than a standard context-specific Bayesian
network learner, in far less time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3272</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3272</id><created>2012-06-13</created><authors><author><keyname>Lawrence</keyname><forenames>Gregory</forenames></author><author><keyname>Russell</keyname><forenames>Stuart</forenames></author></authors><title>Improving Gradient Estimation by Incorporating Sensor Data</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-375-382</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An efficient policy search algorithm should estimate the local gradient of
the objective function, with respect to the policy parameters, from as few
trials as possible. Whereas most policy search methods estimate this gradient
by observing the rewards obtained during policy trials, we show, both
theoretically and empirically, that taking into account the sensor data as well
gives better gradient estimates and hence faster learning. The reason is that
rewards obtained during policy execution vary from trial to trial due to noise
in the environment; sensor data, which correlates with the noise, can be used
to partially correct for this variation, resulting in an estimatorwith lower
variance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3273</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3273</id><created>2012-06-13</created><authors><author><keyname>Lacerda</keyname><forenames>Gustavo</forenames></author><author><keyname>Spirtes</keyname><forenames>Peter L.</forenames></author><author><keyname>Ramsey</keyname><forenames>Joseph</forenames></author><author><keyname>Hoyer</keyname><forenames>Patrik O.</forenames></author></authors><title>Discovering Cyclic Causal Models by Independent Components Analysis</title><categories>cs.AI stat.ME</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-366-374</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalize Shimizu et al's (2006) ICA-based approach for discovering
linear non-Gaussian acyclic (LiNGAM) Structural Equation Models (SEMs) from
causally sufficient, continuous-valued observational data. By relaxing the
assumption that the generating SEM's graph is acyclic, we solve the more
general problem of linear non-Gaussian (LiNG) SEM discovery. LiNG discovery
algorithms output the distribution equivalence class of SEMs which, in the
large sample limit, represents the population distribution. We apply a LiNG
discovery algorithm to simulated data. Finally, we give sufficient conditions
under which only one of the SEMs in the output class is 'stable'.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3274</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3274</id><created>2012-06-13</created><authors><author><keyname>Laber</keyname><forenames>Eric B.</forenames></author><author><keyname>Murphy</keyname><forenames>Susan A.</forenames></author></authors><title>Small Sample Inference for Generalization Error in Classification Using
  the CUD Bound</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-357-365</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Confidence measures for the generalization error are crucial when small
training samples are used to construct classifiers. A common approach is to
estimate the generalization error by resampling and then assume the resampled
estimator follows a known distribution to form a confidence set [Kohavi 1995,
Martin 1996,Yang 2006]. Alternatively, one might bootstrap the resampled
estimator of the generalization error to form a confidence set. Unfortunately,
these methods do not reliably provide sets of the desired confidence. The poor
performance appears to be due to the lack of smoothness of the generalization
error as a function of the learned classifier. This results in a non-normal
distribution of the estimated generalization error. We construct a confidence
set for the generalization error by use of a smooth upper bound on the
deviation between the resampled estimate and generalization error. The
confidence set is formed by bootstrapping this upper bound. In cases in which
the approximation class for the classifier can be represented as a parametric
additive model, we provide a computationally efficient algorithm. This method
exhibits superior performance across a series of test and simulated data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3275</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3275</id><created>2012-06-13</created><authors><author><keyname>Noto</keyname><forenames>Keith</forenames></author><author><keyname>Craven</keyname><forenames>Mark</forenames></author></authors><title>Learning Hidden Markov Models for Regression using Path Aggregation</title><categories>cs.LG cs.CE q-bio.QM</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-444-451</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the task of learning mappings from sequential data to real-valued
responses. We present and evaluate an approach to learning a type of hidden
Markov model (HMM) for regression. The learning process involves inferring the
structure and parameters of a conventional HMM, while simultaneously learning a
regression model that maps features that characterize paths through the model
to continuous responses. Our results, in both synthetic and biological domains,
demonstrate the value of jointly learning the two components of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3276</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3276</id><created>2012-06-13</created><authors><author><keyname>Nielsen</keyname><forenames>Ulf</forenames></author><author><keyname>Pellet</keyname><forenames>Jean-Philippe</forenames></author><author><keyname>Elisseeff</keyname><forenames>Andr&#xe9;</forenames></author></authors><title>Explanation Trees for Causal Bayesian Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-427-434</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian networks can be used to extract explanations about the observed
state of a subset of variables. In this paper, we explicate the desiderata of
an explanation and confront them with the concept of explanation proposed by
existing methods. The necessity of taking into account causal approaches when a
causal graph is available is discussed. We then introduce causal explanation
trees, based on the construction of explanation trees using the measure of
causal information ow (Ay and Polani, 2006). This approach is compared to
several other methods on known networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3277</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3277</id><created>2012-06-13</created><authors><author><keyname>de Cote</keyname><forenames>Enrique Munoz</forenames></author><author><keyname>Littman</keyname><forenames>Michael L.</forenames></author></authors><title>A Polynomial-time Nash Equilibrium Algorithm for Repeated Stochastic
  Games</title><categories>cs.GT</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-419-426</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a polynomial-time algorithm that always finds an (approximate)
Nash equilibrium for repeated two-player stochastic games. The algorithm
exploits the folk theorem to derive a strategy profile that forms an
equilibrium by buttressing mutually beneficial behavior with threats, where
possible. One component of our algorithm efficiently searches for an
approximation of the egalitarian point, the fairest pareto-efficient solution.
The paper concludes by applying the algorithm to a set of grid games to
illustrate typical solutions the algorithm finds. These solutions compare very
favorably to those found by competing algorithms, resulting in strategies with
higher social welfare, as well as guaranteed computational efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3278</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3278</id><created>2012-06-13</created><authors><author><keyname>Mimno</keyname><forenames>David</forenames></author><author><keyname>McCallum</keyname><forenames>Andrew</forenames></author></authors><title>Topic Models Conditioned on Arbitrary Features with
  Dirichlet-multinomial Regression</title><categories>cs.IR stat.ME</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-411-418</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although fully generative models have been successfully used to model the
contents of text documents, they are often awkward to apply to combinations of
text data and document metadata. In this paper we propose a
Dirichlet-multinomial regression (DMR) topic model that includes a log-linear
prior on document-topic distributions that is a function of observed features
of the document, such as author, publication venue, references, and dates. We
show that by selecting appropriate features, DMR topic models can meet or
exceed the performance of several previously published topic models designed
for specific data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3279</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3279</id><created>2012-06-13</created><authors><author><keyname>Miller</keyname><forenames>Kurt T.</forenames></author><author><keyname>Griffiths</keyname><forenames>Thomas</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author></authors><title>The Phylogenetic Indian Buffet Process: A Non-Exchangeable Nonparametric
  Prior for Latent Features</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-403-410</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nonparametric Bayesian models are often based on the assumption that the
objects being modeled are exchangeable. While appropriate in some applications
(e.g., bag-of-words models for documents), exchangeability is sometimes assumed
simply for computational reasons; non-exchangeable models might be a better
choice for applications based on subject matter. Drawing on ideas from
graphical models and phylogenetics, we describe a non-exchangeable prior for a
class of nonparametric latent feature models that is nearly as efficient
computationally as its exchangeable counterpart. Our model is applicable to the
general setting in which the dependencies between objects can be expressed
using a tree, where edge lengths indicate the strength of relationships. We
demonstrate an application to modeling probabilistic choice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3280</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3280</id><created>2012-06-13</created><authors><author><keyname>Simma</keyname><forenames>Aleksandr</forenames></author><author><keyname>Goldszmidt</keyname><forenames>Moises</forenames></author><author><keyname>MacCormick</keyname><forenames>John</forenames></author><author><keyname>Barham</keyname><forenames>Paul</forenames></author><author><keyname>Black</keyname><forenames>Richard</forenames></author><author><keyname>Isaacs</keyname><forenames>Rebecca</forenames></author><author><keyname>Mortier</keyname><forenames>Richard</forenames></author></authors><title>CT-NOR: Representing and Reasoning About Events in Continuous Time</title><categories>cs.AI stat.AP</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-484-493</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a generative model for representing and reasoning about the
relationships among events in continuous time. We apply the model to the domain
of networked and distributed computing environments where we fit the parameters
of the model from timestamp observations, and then use hypothesis testing to
discover dependencies between the events and changes in behavior for monitoring
and diagnosis. After introducing the model, we present an EM algorithm for
fitting the parameters and then present the hypothesis testing approach for
both dependence discovery and change-point detection. We validate the approach
for both tasks using real data from a trace of network events at Microsoft
Research Cambridge. Finally, we formalize the relationship between the proposed
model and the noisy-or gate for cases when time can be discretized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3281</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3281</id><created>2012-06-13</created><authors><author><keyname>Ross</keyname><forenames>Stephane</forenames></author><author><keyname>Pineau</keyname><forenames>Joelle</forenames></author></authors><title>Model-Based Bayesian Reinforcement Learning in Large Structured Domains</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-476-483</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model-based Bayesian reinforcement learning has generated significant
interest in the AI community as it provides an elegant solution to the optimal
exploration-exploitation tradeoff in classical reinforcement learning.
Unfortunately, the applicability of this type of approach has been limited to
small domains due to the high complexity of reasoning about the joint posterior
over model parameters. In this paper, we consider the use of factored
representations combined with online planning techniques, to improve
scalability of these methods. The main contribution of this paper is a Bayesian
framework for learning the structure and parameters of a dynamical system,
while also simultaneously planning a (near-)optimal sequence of actions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3282</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3282</id><created>2012-06-13</created><authors><author><keyname>Riedel</keyname><forenames>Sebastian</forenames></author></authors><title>Improving the Accuracy and Efficiency of MAP Inference for Markov Logic</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-468-475</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we present Cutting Plane Inference (CPI), a Maximum A Posteriori
(MAP) inference method for Statistical Relational Learning. Framed in terms of
Markov Logic and inspired by the Cutting Plane Method, it can be seen as a meta
algorithm that instantiates small parts of a large and complex Markov Network
and then solves these using a conventional MAP method. We evaluate CPI on two
tasks, Semantic Role Labelling and Joint Entity Resolution, while plugging in
two different MAP inference methods: the current method of choice for MAP
inference in Markov Logic, MaxWalkSAT, and Integer Linear Programming. We
observe that when used with CPI both methods are significantly faster than when
used alone. In addition, CPI improves the accuracy of MaxWalkSAT and maintains
the exactness of Integer Linear Programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3283</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3283</id><created>2012-06-13</created><authors><author><keyname>Radovilsky</keyname><forenames>Yan</forenames></author><author><keyname>Shimony</keyname><forenames>Solomon Eyal</forenames></author></authors><title>Observation Subset Selection as Local Compilation of Performance
  Profiles</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-460-467</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deciding what to sense is a crucial task, made harder by dependencies and by
a nonadditive utility function. We develop approximation algorithms for
selecting an optimal set of measurements, under a dependency structure modeled
by a tree-shaped Bayesian network (BN). Our approach is a generalization of
composing anytime algorithm represented by conditional performance profiles.
This is done by relaxing the input monotonicity assumption, and extending the
local compilation technique to more general classes of performance profiles
(PPs). We apply the extended scheme to selecting a subset of measurements for
choosing a maximum expectation variable in a binary valued BN, and for
minimizing the worst variance in a Gaussian BN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3284</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3284</id><created>2012-06-13</created><authors><author><keyname>Otten</keyname><forenames>Lars</forenames></author><author><keyname>Dechter</keyname><forenames>Rina</forenames></author></authors><title>Bounding Search Space Size via (Hyper)tree Decompositions</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-452-459</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops a measure for bounding the performance of AND/OR search
algorithms for solving a variety of queries over graphical models. We show how
drawing a connection to the recent notion of hypertree decompositions allows to
exploit determinism in the problem specification and produce tighter bounds. We
demonstrate on a variety of practical problem instances that we are often able
to improve upon existing bounds by several orders of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3285</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3285</id><created>2012-06-13</created><authors><author><keyname>Sutton</keyname><forenames>Richard S.</forenames></author><author><keyname>Szepesvari</keyname><forenames>Csaba</forenames></author><author><keyname>Geramifard</keyname><forenames>Alborz</forenames></author><author><keyname>Bowling</keyname><forenames>Michael P.</forenames></author></authors><title>Dyna-Style Planning with Linear Function Approximation and Prioritized
  Sweeping</title><categories>cs.AI cs.LG cs.SY</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-528-536</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of efficiently learning optimal control policies and
value functions over large state spaces in an online setting in which estimates
must be available after each interaction with the world. This paper develops an
explicitly model-based approach extending the Dyna architecture to linear
function approximation. Dynastyle planning proceeds by generating imaginary
experience from the world model and then applying model-free reinforcement
learning algorithms to the imagined state transitions. Our main results are to
prove that linear Dyna-style planning converges to a unique solution
independent of the generating distribution, under natural conditions. In the
policy evaluation setting, we prove that the limit point is the least-squares
(LSTD) solution. An implication of our results is that prioritized-sweeping can
be soundly extended to the linear approximation case, backing up to preceding
features rather than to preceding states. We introduce two versions of
prioritized sweeping with linear Dyna and briefly illustrate their performance
empirically on the Mountain Car and Boyan Chain problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3286</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3286</id><created>2012-06-13</created><authors><author><keyname>Streeter</keyname><forenames>Matthew</forenames></author><author><keyname>Smith</keyname><forenames>Stephen F.</forenames></author></authors><title>New Techniques for Algorithm Portfolio Design</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-519-527</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present and evaluate new techniques for designing algorithm portfolios. In
our view, the problem has both a scheduling aspect and a machine learning
aspect. Prior work has largely addressed one of the two aspects in isolation.
Building on recent work on the scheduling aspect of the problem, we present a
technique that addresses both aspects simultaneously and has attractive
theoretical guarantees. Experimentally, we show that this technique can be used
to improve the performance of state-of-the-art algorithms for Boolean
satisfiability, zero-one integer programming, and A.I. planning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3287</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3287</id><created>2012-06-13</created><authors><author><keyname>Steck</keyname><forenames>Harald</forenames></author></authors><title>Learning the Bayesian Network Structure: Dirichlet Prior versus Data</title><categories>cs.LG stat.ME stat.ML</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-511-518</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Bayesian approach to structure learning of graphical models, the
equivalent sample size (ESS) in the Dirichlet prior over the model parameters
was recently shown to have an important effect on the maximum-a-posteriori
estimate of the Bayesian network structure. In our first contribution, we
theoretically analyze the case of large ESS-values, which complements previous
work: among other results, we find that the presence of an edge in a Bayesian
network is favoured over its absence even if both the Dirichlet prior and the
data imply independence, as long as the conditional empirical distribution is
notably different from uniform. In our second contribution, we focus on
realistic ESS-values, and provide an analytical approximation to the &quot;optimal&quot;
ESS-value in a predictive sense (its accuracy is also validated
experimentally): this approximation provides an understanding as to which
properties of the data have the main effect determining the &quot;optimal&quot;
ESS-value.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3288</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3288</id><created>2012-06-13</created><authors><author><keyname>Sontag</keyname><forenames>David</forenames></author><author><keyname>Meltzer</keyname><forenames>Talya</forenames></author><author><keyname>Globerson</keyname><forenames>Amir</forenames></author><author><keyname>Jaakkola</keyname><forenames>Tommi S.</forenames></author><author><keyname>Weiss</keyname><forenames>Yair</forenames></author></authors><title>Tightening LP Relaxations for MAP using Message Passing</title><categories>cs.DS cs.AI cs.CE</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-503-510</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear Programming (LP) relaxations have become powerful tools for finding
the most probable (MAP) configuration in graphical models. These relaxations
can be solved efficiently using message-passing algorithms such as belief
propagation and, when the relaxation is tight, provably find the MAP
configuration. The standard LP relaxation is not tight enough in many
real-world problems, however, and this has lead to the use of higher order
cluster-based LP relaxations. The computational cost increases exponentially
with the size of the clusters and limits the number and type of clusters we can
use. We propose to solve the cluster selection problem monotonically in the
dual LP, iteratively selecting clusters with guaranteed improvement, and
quickly re-solving with the added clusters by reusing the existing solution.
Our dual message-passing algorithm finds the MAP configuration in protein
sidechain placement, protein design, and stereo problems, in cases where the
standard LP relaxation fails.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3289</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3289</id><created>2012-06-13</created><authors><author><keyname>Singliar</keyname><forenames>Tomas</forenames></author><author><keyname>Dash</keyname><forenames>Denver</forenames></author></authors><title>Efficient inference in persistent Dynamic Bayesian Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-494-502</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerous temporal inference tasks such as fault monitoring and anomaly
detection exhibit a persistence property: for example, if something breaks, it
stays broken until an intervention. When modeled as a Dynamic Bayesian Network,
persistence adds dependencies between adjacent time slices, often making exact
inference over time intractable using standard inference algorithms. However,
we show that persistence implies a regular structure that can be exploited for
efficient inference. We present three successively more general classes of
models: persistent causal chains (PCCs), persistent causal trees (PCTs) and
persistent polytrees (PPTs), and the corresponding exact inference algorithms
that exploit persistence. We show that analytic asymptotic bounds for our
algorithms compare favorably to junction tree inference; and we demonstrate
empirically that we can perform exact smoothing on the order of 100 times
faster than the approximate Boyen-Koller method on randomly generated instances
of persistent tree models. We also show how to handle non-persistent variables
and how persistence can be exploited effectively for approximate filtering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3290</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3290</id><created>2012-06-13</created><authors><author><keyname>Vanhatalo</keyname><forenames>Jarno</forenames></author><author><keyname>Vehtari</keyname><forenames>Aki</forenames></author></authors><title>Modelling local and global phenomena with sparse Gaussian processes</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-571-578</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Much recent work has concerned sparse approximations to speed up the Gaussian
process regression from the unfavorable O(n3) scaling in computational time to
O(nm2). Thus far, work has concentrated on models with one covariance function.
However, in many practical situations additive models with multiple covariance
functions may perform better, since the data may contain both long and short
length-scale phenomena. The long length-scales can be captured with global
sparse approximations, such as fully independent conditional (FIC), and the
short length-scales can be modeled naturally by covariance functions with
compact support (CS). CS covariance functions lead to naturally sparse
covariance matrices, which are computationally cheaper to handle than full
covariance matrices. In this paper, we propose a new sparse Gaussian process
model with two additive components: FIC for the long length-scales and CS
covariance function for the short length-scales. We give theoretical and
experimental results and show that under certain conditions the proposed model
has the same computational complexity as FIC. We also compare the model
performance of the proposed model to additive models approximated by fully and
partially independent conditional (PIC). We use real data sets and show that
our model outperforms FIC and PIC approximations for data sets with two
additive phenomena.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3291</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3291</id><created>2012-06-13</created><authors><author><keyname>Toussaint</keyname><forenames>Marc</forenames></author><author><keyname>Charlin</keyname><forenames>Laurent</forenames></author><author><keyname>Poupart</keyname><forenames>Pascal</forenames></author></authors><title>Hierarchical POMDP Controller Optimization by Likelihood Maximization</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-562-570</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Planning can often be simpli ed by decomposing the task into smaller tasks
arranged hierarchically. Charlin et al. [4] recently showed that the hierarchy
discovery problem can be framed as a non-convex optimization problem. However,
the inherent computational di culty of solving such an optimization problem
makes it hard to scale to realworld problems. In another line of research,
Toussaint et al. [18] developed a method to solve planning problems by
maximumlikelihood estimation. In this paper, we show how the hierarchy
discovery problem in partially observable domains can be tackled using a
similar maximum likelihood approach. Our technique rst transforms the problem
into a dynamic Bayesian network through which a hierarchical structure can
naturally be discovered while optimizing the policy. Experimental results
demonstrate that this approach scales better than previous techniques based on
non-convex optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3292</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3292</id><created>2012-06-13</created><authors><author><keyname>Tian</keyname><forenames>Jin</forenames></author></authors><title>Identifying Dynamic Sequential Plans</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-554-561</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of identifying dynamic sequential plans in the
framework of causal Bayesian networks, and show that the problem is reduced to
identifying causal effects, for which there are complete identi cation
algorithms available in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3293</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3293</id><created>2012-06-13</created><authors><author><keyname>Thwaites</keyname><forenames>Peter</forenames></author><author><keyname>Smith</keyname><forenames>Jim Q.</forenames></author><author><keyname>Cowell</keyname><forenames>Robert G.</forenames></author></authors><title>Propagation using Chain Event Graphs</title><categories>cs.AI cs.CL</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-546-553</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Chain Event Graph (CEG) is a graphial model which designed to embody
conditional independencies in problems whose state spaces are highly asymmetric
and do not admit a natural product structure. In this paer we present a
probability propagation algorithm which uses the topology of the CEG to build a
transporter CEG. Intriungly,the transporter CEG is directly analogous to the
triangulated Bayesian Network (BN) in the more conventional junction tree
propagation algorithms used with BNs. The propagation method uses factorization
formulae also analogous to (but different from) the ones using potentials on
cliques and separators of the BN. It appears that the methods will be typically
more efficient than the BN algorithms when applied to contexts where there is
significant asymmetry present.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3294</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3294</id><created>2012-06-13</created><authors><author><keyname>Tarlow</keyname><forenames>Daniel</forenames></author><author><keyname>Zemel</keyname><forenames>Richard S.</forenames></author><author><keyname>Frey</keyname><forenames>Brendan J.</forenames></author></authors><title>Flexible Priors for Exemplar-based Clustering</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-537-545</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exemplar-based clustering methods have been shown to produce state-of-the-art
results on a number of synthetic and real-world clustering problems. They are
appealing because they offer computational benefits over latent-mean models and
can handle arbitrary pairwise similarity measures between data points. However,
when trying to recover underlying structure in clustering problems, tailored
similarity measures are often not enough; we also desire control over the
distribution of cluster sizes. Priors such as Dirichlet process priors allow
the number of clusters to be unspecified while expressing priors over data
partitions. To our knowledge, they have not been applied to exemplar-based
models. We show how to incorporate priors, including Dirichlet process priors,
into the recently introduced affinity propagation algorithm. We develop an
efficient maxproduct belief propagation algorithm for our new model and
demonstrate experimentally how the expanded range of clustering priors allows
us to better recover true clusterings in situations where we have some
information about the generating process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3295</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3295</id><created>2012-06-13</created><authors><author><keyname>Yu</keyname><forenames>Haohai</forenames></author><author><keyname>van Engelen</keyname><forenames>Robert A.</forenames></author></authors><title>Refractor Importance Sampling</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-603-609</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce Refractor Importance Sampling (RIS), an
improvement to reduce error variance in Bayesian network importance sampling
propagation under evidential reasoning. We prove the existence of a collection
of importance functions that are close to the optimal importance function under
evidential reasoning. Based on this theoretic result we derive the RIS
algorithm. RIS approaches the optimal importance function by applying localized
arc changes to minimize the divergence between the evidence-adjusted importance
function and the optimal importance function. The validity and performance of
RIS is empirically tested with a large setof synthetic Bayesian networks and
two real-world networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3296</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3296</id><created>2012-06-13</created><authors><author><keyname>Wexler</keyname><forenames>Ydo</forenames></author><author><keyname>Meek</keyname><forenames>Christopher</forenames></author></authors><title>Inference for Multiplicative Models</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-595-602</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper introduces a generalization for known probabilistic models such as
log-linear and graphical models, called here multiplicative models. These
models, that express probabilities via product of parameters are shown to
capture multiple forms of contextual independence between variables, including
decision graphs and noisy-OR functions. An inference algorithm for
multiplicative models is provided and its correctness is proved. The complexity
analysis of the inference algorithm uses a more refined parameter than the
tree-width of the underlying graph, and shows the computational cost does not
exceed that of the variable elimination algorithm in graphical models. The
paper ends with examples where using the new models and algorithm is
computationally beneficial.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3297</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3297</id><created>2012-06-13</created><authors><author><keyname>Welling</keyname><forenames>Max</forenames></author><author><keyname>Teh</keyname><forenames>Yee Whye</forenames></author><author><keyname>Kappen</keyname><forenames>Hilbert</forenames></author></authors><title>Hybrid Variational/Gibbs Collapsed Inference in Topic Models</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-587-594</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Variational Bayesian inference and (collapsed) Gibbs sampling are the two
important classes of inference algorithms for Bayesian networks. Both have
their advantages and disadvantages: collapsed Gibbs sampling is unbiased but is
also inefficient for large count values and requires averaging over many
samples to reduce variance. On the other hand, variational Bayesian inference
is efficient and accurate for large count values but suffers from bias for
small counts. We propose a hybrid algorithm that combines the best of both
worlds: it samples very small counts and applies variational updates to large
counts. This hybridization is shown to significantly improve testset perplexity
relative to variational inference at no computational cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3298</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3298</id><created>2012-06-13</created><updated>2015-05-16</updated><authors><author><keyname>Wang</keyname><forenames>Chong</forenames></author><author><keyname>Blei</keyname><forenames>David</forenames></author><author><keyname>Heckerman</keyname><forenames>David</forenames></author></authors><title>Continuous Time Dynamic Topic Models</title><categories>cs.IR cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>Martijn de Jongh</proxy><report-no>UAI-P-2008-PG-579-586</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we develop the continuous time dynamic topic model (cDTM). The
cDTM is a dynamic topic model that uses Brownian motion to model the latent
topics through a sequential collection of documents, where a &quot;topic&quot; is a
pattern of word use that we expect to evolve over the course of the collection.
We derive an efficient variational approximate inference algorithm that takes
advantage of the sparsity of observations in text, a property that lets us
easily handle many time points. In contrast to the cDTM, the original
discrete-time dynamic topic model (dDTM) requires that time be discretized.
Moreover, the complexity of variational inference for the dDTM grows quickly as
time granularity increases, a drawback which limits fine-grained
discretization. We demonstrate the cDTM on two news corpora, reporting both
predictive perplexity and the novel task of time stamp prediction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3318</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3318</id><created>2012-06-14</created><authors><author><keyname>Bowling</keyname><forenames>Michael</forenames></author><author><keyname>Zinkevich</keyname><forenames>Martin</forenames></author></authors><title>On Local Regret</title><categories>cs.AI</categories><comments>This is the longer version of the same-titled paper appearing in the
  Proceedings of the Twenty-Ninth International Conference on Machine Learning
  (ICML), 2012</comments><report-no>TR12-04</report-no><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Online learning aims to perform nearly as well as the best hypothesis in
hindsight. For some hypothesis classes, though, even finding the best
hypothesis offline is challenging. In such offline cases, local search
techniques are often employed and only local optimality guaranteed. For online
decision-making with such hypothesis classes, we introduce local regret, a
generalization of regret that aims to perform nearly as well as only nearby
hypotheses. We then present a general algorithm to minimize local regret with
arbitrary locality graphs. We also show how the graph structure can be
exploited to drastically speed learning. These algorithms are then demonstrated
on a diverse set of online problems: online disjunct learning, online Max-SAT,
and online decision tree learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3320</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3320</id><created>2012-06-14</created><authors><author><keyname>Liu</keyname><forenames>Jinhu</forenames></author><author><keyname>Yang</keyname><forenames>Chengcheng</forenames></author><author><keyname>Zhang</keyname><forenames>Zi-Ke</forenames></author></authors><title>A two-step Recommendation Algorithm via Iterative Local Least Squares</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recommender systems can change our life a lot and help us select suitable and
favorite items much more conveniently and easily. As a consequence, various
kinds of algorithms have been proposed in last few years to improve the
performance. However, all of them face one critical problem: data sparsity. In
this paper, we proposed a two-step recommendation algorithm via iterative local
least squares (ILLS). Firstly, we obtain the ratings matrix which is
constructed via users' behavioral records, and it is normally very sparse.
Secondly, we preprocess the &quot;ratings&quot; matrix through ProbS which can convert
the sparse data to a dense one. Then we use ILLS to estimate those missing
values. Finally, the recommendation list is generated. Experimental results on
the three datasets: MovieLens, Netflix, RYM, suggest that the proposed method
can enhance the algorithmic accuracy of AUC. Especially, it performs much
better in dense datasets. Furthermore, since this methods can improve those
missing value more accurately via iteration which might show light in
discovering those inactive users' purchasing intention and eventually solving
cold-start problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3334</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3334</id><created>2012-06-14</created><authors><author><keyname>Awasthi</keyname><forenames>Pranjal</forenames></author><author><keyname>Blum</keyname><forenames>Avrim</forenames></author><author><keyname>Morgenstern</keyname><forenames>Jamie</forenames></author><author><keyname>Sheffet</keyname><forenames>Or</forenames></author></authors><title>Additive Approximation for Near-Perfect Phylogeny Construction</title><categories>cs.DS cs.CE q-bio.PE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of constructing phylogenetic trees for a given set of
species. The problem is formulated as that of finding a minimum Steiner tree on
$n$ points over the Boolean hypercube of dimension $d$. It is known that an
optimal tree can be found in linear time if the given dataset has a perfect
phylogeny, i.e. cost of the optimal phylogeny is exactly $d$. Moreover, if the
data has a near-perfect phylogeny, i.e. the cost of the optimal Steiner tree is
$d+q$, it is known that an exact solution can be found in running time which is
polynomial in the number of species and $d$, yet exponential in $q$. In this
work, we give a polynomial-time algorithm (in both $d$ and $q$) that finds a
phylogenetic tree of cost $d+O(q^2)$. This provides the best guarantees known -
namely, a $(1+o(1))$-approximation - for the case $\log(d) \ll q \ll \sqrt{d}$,
broadening the range of settings for which near-optimal solutions can be
efficiently found. We also discuss the motivation and reasoning for studying
such additive approximations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3350</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3350</id><created>2012-06-14</created><updated>2012-10-11</updated><authors><author><keyname>Yerramalli</keyname><forenames>Srinivas</forenames></author><author><keyname>Jain</keyname><forenames>Rahul</forenames></author><author><keyname>Mitra</keyname><forenames>Urbashi</forenames></author></authors><title>Coalitional Games for Transmitter Cooperation in MIMO Multiple Access
  Channels</title><categories>cs.IT cs.GT math.IT</categories><comments>in review for publication in IEEE Transactions on Signal Processing</comments><acm-class>C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cooperation between nodes sharing a wireless channel is becoming increasingly
necessary to achieve performance goals in a wireless network. The problem of
determining the feasibility and stability of cooperation between rational nodes
in a wireless network is of great importance in understanding cooperative
behavior. This paper addresses the stability of the grand coalition of
transmitters signaling over a multiple access channel using the framework of
cooperative game theory. The external interference experienced by each TX is
represented accurately by modeling the cooperation game between the TXs in
\emph{partition form}. Single user decoding and successive interference
cancelling strategies are examined at the receiver. In the absence of
coordination costs, the grand coalition is shown to be \emph{sum-rate optimal}
for both strategies. Transmitter cooperation is \emph{stable}, if and only if
the core of the game (the set of all divisions of grand coalition utility such
that no coalition deviates) is nonempty. Determining the stability of
cooperation is a co-NP-complete problem in general. For a single user decoding
receiver, transmitter cooperation is shown to be \emph{stable} at both high and
low SNRs, while for an interference cancelling receiver with a fixed decoding
order, cooperation is stable only at low SNRs and unstable at high SNR. When
time sharing is allowed between decoding orders, it is shown using an
approximate lower bound to the utility function that TX cooperation is also
stable at high SNRs. Thus, this paper demonstrates that ideal zero cost TX
cooperation over a MAC is stable and improves achievable rates for each
individual user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3357</identifier>
 <datestamp>2015-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3357</id><created>2012-06-14</created><updated>2012-11-22</updated><authors><author><keyname>Platzer</keyname><forenames>Andre</forenames><affiliation>Carnegie Mellon University</affiliation></author></authors><title>A Complete Axiomatization of Quantified Differential Dynamic Logic for
  Distributed Hybrid Systems</title><categories>cs.LO cs.PL math.DS</categories><comments>arXiv admin note: text overlap with arXiv:1205.4788</comments><proxy>LMCS</proxy><acm-class>F.3.1, F.4.1, D.2.4, C.1.m, C.2.4, D.4.7</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 4 (November
  26, 2012) lmcs:720</journal-ref><doi>10.2168/LMCS-8(4:17)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address a fundamental mismatch between the combinations of dynamics that
occur in cyber-physical systems and the limited kinds of dynamics supported in
analysis. Modern applications combine communication, computation, and control.
They may even form dynamic distributed networks, where neither structure nor
dimension stay the same while the system follows hybrid dynamics, i.e., mixed
discrete and continuous dynamics. We provide the logical foundations for
closing this analytic gap. We develop a formal model for distributed hybrid
systems. It combines quantified differential equations with quantified
assignments and dynamic dimensionality-changes. We introduce a dynamic logic
for verifying distributed hybrid systems and present a proof calculus for this
logic. This is the first formal verification approach for distributed hybrid
systems. We prove that our calculus is a sound and complete axiomatization of
the behavior of distributed hybrid systems relative to quantified differential
equations. In our calculus we have proven collision freedom in distributed car
control even when an unbounded number of new cars may appear dynamically on the
road.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3362</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3362</id><created>2012-06-14</created><authors><author><keyname>Ma</keyname><forenames>Kexiang</forenames></author><author><keyname>Li</keyname><forenames>Yongzhao</forenames></author><author><keyname>Zhu</keyname><forenames>Caizhi</forenames></author><author><keyname>Zhang</keyname><forenames>Hailin</forenames></author><author><keyname>Qi</keyname><forenames>Feng</forenames></author></authors><title>An Improved WBF Algorithm for Higher-Speed Decoding of LDPC Codes</title><categories>cs.IT math.IT</categories><comments>This paper was submitted to China Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the speed limitation of the conventional bit-chosen strategy in the
existing weighted bit flipping algorithms, a high-speed LDPC decoder cannot be
realized. To solve this problem, we propose a fast weighted bit flipping (FWBF)
algorithm. Specifically, based on the stochastic error bitmap of the received
vector, a partially parallel bit-choose strategy is adopted to lower the delay
of choosing the bit flipped. Because of its partially parallel structure, the
novel strategy can be well incorporated into the LDPC decoder [1]. The analysis
of the decoding delay demonstrates that, the decoding speed can be greatly
improved by adopting the proposed FWBF algorithm. Further, simulation results
verify the validity of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3365</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3365</id><created>2012-06-14</created><authors><author><keyname>Wang</keyname><forenames>Qike</forenames></author><author><keyname>Tse</keyname><forenames>Kam-Hon</forenames></author><author><keyname>Chen</keyname><forenames>Lian-Kuan</forenames></author><author><keyname>Liew</keyname><forenames>Soung-Chang</forenames></author></authors><title>Physical-Layer Network Coding for VPN in TDM-PON</title><categories>cs.NI</categories><doi>10.1109/LPT.2012.2224103</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We experimentally demonstrate a novel optical physical-layer network coding
(PNC) scheme over time-division multiplexing (TDM) passive optical network
(PON). Full-duplex error-free communications between optical network units
(ONUs) at 2.5 Gb/s are shown for all-optical virtual private network (VPN)
applications. Compared to the conventional half-duplex communications set-up,
our scheme can increase the capacity by 100% with power penalty smaller than 3
dB. Synchronization of two ONUs is not required for the proposed VPN scheme
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3375</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3375</id><created>2012-06-15</created><authors><author><keyname>Alagu</keyname><forenames>S.</forenames></author><author><keyname>Meyyappan</keyname><forenames>T.</forenames></author></authors><title>Efficient Utilization of Channels Using Dynamic Guard Channel Allocation
  with Channel Borrowing Strategy in Handoffs</title><categories>cs.NI</categories><comments>International Conference CCSEA 2012 at New Delhi May 27. arXiv admin
  note: substantial text overlap with arXiv:1206.3061</comments><doi>10.5121/csit.2012.2225</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  User mobility in wireless data networks is increasing because of
technological advances and the desire for voice and multimedia applications.
These applications, however, require fast handoffs between base stations to
maintain the quality of the connections. In this paper, the authors describe
the use of novel and efficient data structure which dynamically allocates guard
channel for handoffs and introduces the concept of channel borrowing strategy.
The proposed scheme allocates the guard channels for handoff requests
dynamically, based on the traffic load for certain time period. A new
originating call in the cell coverage area also uses these guard channels if
they are unused. Our basic idea is to allow Guard channels to be shared between
new calls and handoff calls. This approach maximizes the channel utilization.
The simulation results prove that the channel borrowing scheme improves the
overall throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3381</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3381</id><created>2012-06-15</created><authors><author><keyname>Gneiting</keyname><forenames>Tilmann</forenames></author></authors><title>On the Cover-Hart Inequality: What's a Sample of Size One Worth?</title><categories>math.ST cs.IT math.IT stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bob predicts a future observation based on a sample of size one. Alice can
draw a sample of any size before issuing her prediction. How much better can
she do than Bob? Perhaps surprisingly, under a large class of loss functions,
which we refer to as the Cover-Hart family, the best Alice can do is to halve
Bob's risk. In this sense, half the information in an infinite sample is
contained in a sample of size one. The Cover-Hart family is a convex cone that
includes metrics and negative definite functions, subject to slight regularity
conditions. These results may help explain the small relative differences in
empirical performance measures in applied classification and forecasting
problems, as well as the success of reasoning and learning by analogy in
general, and nearest neighbor techniques in particular.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3382</identifier>
 <datestamp>2012-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3382</id><created>2012-06-15</created><updated>2012-12-19</updated><authors><author><keyname>Feldman</keyname><forenames>Zohar</forenames></author><author><keyname>Domshlak</keyname><forenames>Carmel</forenames></author></authors><title>Simple Regret Optimization in Online Planning for Markov Decision
  Processes</title><categories>cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider online planning in Markov decision processes (MDPs). In online
planning, the agent focuses on its current state only, deliberates about the
set of possible policies from that state onwards and, when interrupted, uses
the outcome of that exploratory deliberation to choose what action to perform
next. The performance of algorithms for online planning is assessed in terms of
simple regret, which is the agent's expected performance loss when the chosen
action, rather than an optimal one, is followed.
  To date, state-of-the-art algorithms for online planning in general MDPs are
either best effort, or guarantee only polynomial-rate reduction of simple
regret over time. Here we introduce a new Monte-Carlo tree search algorithm,
BRUE, that guarantees exponential-rate reduction of simple regret and error
probability. This algorithm is based on a simple yet non-standard state-space
sampling scheme, MCTS2e, in which different parts of each sample are dedicated
to different exploratory objectives. Our empirical evaluation shows that BRUE
not only provides superior performance guarantees, but is also very effective
in practice and favorably compares to state-of-the-art. We then extend BRUE
with a variant of &quot;learning by forgetting.&quot; The resulting set of algorithms,
BRUE(alpha), generalizes BRUE, improves the exponential factor in the upper
bound on its reduction rate, and exhibits even more attractive empirical
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3392</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3392</id><created>2012-06-15</created><updated>2014-10-19</updated><authors><author><keyname>Vatedka</keyname><forenames>Shashank</forenames></author><author><keyname>Kashyap</keyname><forenames>Navin</forenames></author><author><keyname>Thangaraj</keyname><forenames>Andrew</forenames></author></authors><title>Secure Compute-and-Forward in a Bidirectional Relay</title><categories>cs.IT math.IT</categories><comments>v1 is a much expanded and updated version of arXiv:1204.6350; v2 is a
  minor revision to fix some notational issues; v3 is a much expanded and
  updated version of v2, and contains results on both perfect secrecy and
  strong secrecy; v3 is a revised manuscript submitted to the IEEE Transactions
  on Information Theory in April 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the basic bidirectional relaying problem, in which two users in a
wireless network wish to exchange messages through an intermediate relay node.
In the compute-and-forward strategy, the relay computes a function of the two
messages using the naturally-occurring sum of symbols simultaneously
transmitted by user nodes in a Gaussian multiple access (MAC) channel, and the
computed function value is forwarded to the user nodes in an ensuing broadcast
phase. In this paper, we study the problem under an additional security
constraint, which requires that each user's message be kept secure from the
relay. We consider two types of security constraints: perfect secrecy, in which
the MAC channel output seen by the relay is independent of each user's message;
and strong secrecy, which is a form of asymptotic independence. We propose a
coding scheme based on nested lattices, the main feature of which is that given
a pair of nested lattices that satisfy certain &quot;goodness&quot; properties, we can
explicitly specify probability distributions for randomization at the encoders
to achieve the desired security criteria. In particular, our coding scheme
guarantees perfect or strong secrecy even in the absence of channel noise. The
noise in the channel only affects reliability of computation at the relay, and
for Gaussian noise, we derive achievable rates for reliable and secure
computation. We also present an application of our methods to the multi-hop
line network in which a source needs to transmit messages to a destination
through a series of intermediate relays.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3408</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3408</id><created>2012-06-15</created><authors><author><keyname>Svensson</keyname><forenames>Ola</forenames></author></authors><title>Hardness of Vertex Deletion and Project Scheduling</title><categories>cs.CC</categories><comments>18 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Assuming the Unique Games Conjecture, we show strong inapproximability
results for two natural vertex deletion problems on directed graphs: for any
integer $k\geq 2$ and arbitrary small $\epsilon &gt; 0$, the Feedback Vertex Set
problem and the DAG Vertex Deletion problem are inapproximable within a factor
$k-\epsilon$ even on graphs where the vertices can be almost partitioned into
$k$ solutions. This gives a more structured and therefore stronger UGC-based
hardness result for the Feedback Vertex Set problem that is also simpler
(albeit using the &quot;It Ain't Over Till It's Over&quot; theorem) than the previous
hardness result.
  In comparison to the classical Feedback Vertex Set problem, the DAG Vertex
Deletion problem has received little attention and, although we think it is a
natural and interesting problem, the main motivation for our inapproximability
result stems from its relationship with the classical Discrete Time-Cost
Tradeoff Problem. More specifically, our results imply that the deadline
version is NP-hard to approximate within any constant assuming the Unique Games
Conjecture. This explains the difficulty in obtaining good approximation
algorithms for that problem and further motivates previous alternative
approaches such as bicriteria approximations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3417</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3417</id><created>2012-06-15</created><authors><author><keyname>Kanrar</keyname><forenames>Soumen</forenames></author></authors><title>Performance Improvement of VOD System by Policy Based Traffic Handle</title><categories>cs.NI</categories><comments>4 pages, 10 figures; ISSN: 2010-3697</comments><journal-ref>International Journal of Modeling and Optimization(IJMO), Vol. 2,
  No. 3, June 2012 pages 295-298</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The distributed system use to enhance the performance of all types of
multimedia service in the next generation network. The packet loss occurs in
the video on demand system due to delay and huge traffic load from the both
sides of client request and server response. It's bring a real challenge to the
researcher how to minimize the traffic load in the video on demand system to
provide better utilization of the available bandwidth in the very race
situation. The normal client server type architecture can't solve the huge
client requirement. In this work, I have used number of remote servers that
partly shears the total load in the video on demand system on behalf of the
total system load. This work presents the scenario of controlling the traffic
load in the localization wise sub domain that gives good impact to control the
traffic in the whole distributed system. The user increased rapidly in the
network it posed heavy load to the video servers. The requested clients,
servers are all distributed in nature and the data stream delivered according
to client request. In this work presents the performance of the video on demand
server by policy based traffic handle, at real time with respect to incoming
multi rate traffic pattern.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3431</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3431</id><created>2012-06-15</created><updated>2012-10-30</updated><authors><author><keyname>Avigad</keyname><forenames>Jeremy</forenames></author><author><keyname>Brattka</keyname><forenames>Vasco</forenames></author></authors><title>Computability and analysis: the legacy of Alan Turing</title><categories>math.LO cs.LO math.HO</categories><comments>49 pages</comments><journal-ref>in Rod Downey (ed.), Turing's Legacy, Developments from Turing's
  Ideas in Logic, Lecture Notes in Logic 42, Cambridge University Press 2014,
  p. 1-47</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss the legacy of Alan Turing and his impact on computability and
analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3437</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3437</id><created>2012-06-15</created><authors><author><keyname>Fages</keyname><forenames>Jean-Guillaume</forenames></author><author><keyname>Lorca</keyname><forenames>Xavier</forenames></author></authors><title>Improving the Asymmetric TSP by Considering Graph Structure</title><categories>cs.DM cs.AI cs.DS</categories><comments>Technical report</comments><report-no>12/4/INFO</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent works on cost based relaxations have improved Constraint Programming
(CP) models for the Traveling Salesman Problem (TSP). We provide a short survey
over solving asymmetric TSP with CP. Then, we suggest new implied propagators
based on general graph properties. We experimentally show that such implied
propagators bring robustness to pathological instances and highlight the fact
that graph structure can significantly improve search heuristics behavior.
Finally, we show that our approach outperforms current state of the art
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3460</identifier>
 <datestamp>2012-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3460</id><created>2012-06-15</created><updated>2012-09-28</updated><authors><author><keyname>Simonetto</keyname><forenames>Andrea</forenames></author><author><keyname>Keviczky</keyname><forenames>Tamas</forenames></author><author><keyname>Babuska</keyname><forenames>Robert</forenames></author></authors><title>Constrained Distributed Algebraic Connectivity Maximization in Robotic
  Networks</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of maximizing the algebraic connectivity of the
communication graph in a network of mobile robots by moving them into
appropriate positions. We define the Laplacian of the graph as dependent on the
pairwise distance between the robots and we approximate the problem as a
sequence of Semi-Definite Programs (SDP). We propose a distributed solution
consisting of local SDP's which use information only from nearby neighboring
robots. We show that the resulting distributed optimization framework leads to
feasible subproblems and through its repeated execution, the algebraic
connectivity increases monotonically. Moreover, we describe how to adjust the
communication load of the robots based on locally computable measures.
Numerical simulations show the performance of the algorithm with respect to the
centralized solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3463</identifier>
 <datestamp>2012-07-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3463</id><created>2012-06-15</created><updated>2012-07-25</updated><authors><author><keyname>Gerdt</keyname><forenames>Vladimir P.</forenames></author><author><keyname>Robertz</keyname><forenames>Daniel</forenames></author></authors><title>Computation of Difference Groebner Bases</title><categories>cs.SC math.RA</categories><comments>17 pages</comments><journal-ref>Computer Science Journal of Moldova, Vol. 20, No.2, 2012,
  pp.203-226</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To compute difference Groebner bases of ideals generated by linear
polynomials we adopt to difference polynomial rings the involutive algorithm
based on Janet-like division. The algorithm has been implemented in Maple in
the form of the package LDA (Linear Difference Algebra) and we describe the
main features of the package. Its applications are illustrated by generation of
finite difference approximations to linear partial differential equations and
by reduction of Feynman integrals. We also present the algorithm for an ideal
generated by a finite set of nonlinear difference polynomials. If the algorithm
terminates, then it constructs a Groebner basis of the ideal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3483</identifier>
 <datestamp>2012-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3483</id><created>2012-06-15</created><updated>2012-08-22</updated><authors><author><keyname>Koutis</keyname><forenames>Ioannis</forenames></author></authors><title>Constrained multilinear detection for faster functional motif discovery</title><categories>cs.DS</categories><doi>10.1016/j.ipl.2012.08.008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The GRAPH MOTIF problem asks whether a given multiset of colors appears on a
connected subgraph of a vertex-colored graph. The fastest known parameterized
algorithm for this problem is based on a reduction to the $k$-Multilinear
Detection (k-MlD) problem: the detection of multilinear terms of total degree k
in polynomials presented as circuits. We revisit k-MLD and define k-CMLD, a
constrained version of it which reflects GRAPH MOTIF more faithfully. We then
give a fast algorithm for k-CMLD. As a result we obtain faster parameterized
algorithms for GRAPH MOTIF and variants of it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3493</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3493</id><created>2012-06-13</created><updated>2014-11-02</updated><authors><author><keyname>Zhang</keyname><forenames>Zhilin</forenames></author><author><keyname>Jung</keyname><forenames>Tzyy-Ping</forenames></author><author><keyname>Makeig</keyname><forenames>Scott</forenames></author><author><keyname>Rao</keyname><forenames>Bhaskar D.</forenames></author></authors><title>Compressed Sensing of EEG for Wireless Telemonitoring with Low Energy
  Consumption and Inexpensive Hardware</title><categories>stat.AP cs.IT math.IT stat.ML</categories><comments>Matlab codes can be downloaded at:
  http://dsp.ucsd.edu/~zhilin/BSBL.html, or
  http://sites.google.com/site/researchbyzhang/bsbl</comments><doi>10.1109/TBME.2012.2217959</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Telemonitoring of electroencephalogram (EEG) through wireless body-area
networks is an evolving direction in personalized medicine. Among various
constraints in designing such a system, three important constraints are energy
consumption, data compression, and device cost. Conventional data compression
methodologies, although effective in data compression, consumes significant
energy and cannot reduce device cost. Compressed sensing (CS), as an emerging
data compression methodology, is promising in catering to these constraints.
However, EEG is non-sparse in the time domain and also non-sparse in
transformed domains (such as the wavelet domain). Therefore, it is extremely
difficult for current CS algorithms to recover EEG with the quality that
satisfies the requirements of clinical diagnosis and engineering applications.
Recently, Block Sparse Bayesian Learning (BSBL) was proposed as a new method to
the CS problem. This study introduces the technique to the telemonitoring of
EEG. Experimental results show that its recovery quality is better than
state-of-the-art CS algorithms, and sufficient for practical use. These results
suggest that BSBL is very promising for telemonitoring of EEG and other
non-sparse physiological signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3509</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3509</id><created>2012-06-15</created><authors><author><keyname>Sarkar</keyname><forenames>Saurabh</forenames></author><author><keyname>Malhotra</keyname><forenames>Prateek</forenames></author><author><keyname>Guman</keyname><forenames>Virender</forenames></author></authors><title>A Novel Approach for Protein Structure Prediction</title><categories>cs.LG q-bio.BM</categories><comments>Protein structure prediction is one of the most important goals
  pursued by bioinformatics. An undergraduate project report containing 107
  pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The idea of this project is to study the protein structure and sequence
relationship using the hidden markov model and artificial neural network. In
this context we have assumed two hidden markov models. In first model we have
taken protein secondary structures as hidden and protein sequences as observed.
In second model we have taken protein sequences as hidden and protein
structures as observed. The efficiencies for both the hidden markov models have
been calculated. The results show that the efficiencies of first model is
greater that the second one .These efficiencies are cross validated using
artificial neural network. This signifies the importance of protein secondary
structures as the main hidden controlling factors due to which we observe a
particular amino acid sequence. This also signifies that protein secondary
structure is more conserved in comparison to amino acid sequence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3511</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3511</id><created>2012-06-15</created><authors><author><keyname>Horsmalahti</keyname><forenames>Panu</forenames></author></authors><title>Comparison of Bucket Sort and RADIX Sort</title><categories>cs.DS</categories><comments>10 pages, 2 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Bucket sort and RADIX sort are two well-known integer sorting algorithms.
This paper measures empirically what is the time usage and memory consumption
for different kinds of input sequences. The algorithms are compared both from a
theoretical standpoint but also on how well they do in six different use cases
using randomized sequences of numbers. The measurements provide data on how
good they are in different real-life situations.
  It was found that bucket sort was faster than RADIX sort, but that bucket
sort uses more memory in most cases. The sorting algorithms performed faster
with smaller integers. The RADIX sort was not quicker with already sorted
inputs, but the bucket sort was.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3520</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3520</id><created>2012-06-15</created><authors><author><keyname>Roch</keyname><forenames>Sebastien</forenames></author><author><keyname>Snir</keyname><forenames>Sagi</forenames></author></authors><title>Recovering the tree-like trend of evolution despite extensive lateral
  genetic transfer: A probabilistic analysis</title><categories>math.PR cs.CE cs.DS math.ST q-bio.PE stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lateral gene transfer (LGT) is a common mechanism of non-vertical evolution
where genetic material is transferred between two more or less distantly
related organisms. It is particularly common in bacteria where it contributes
to adaptive evolution with important medical implications. In evolutionary
studies, LGT has been shown to create widespread discordance between gene trees
as genomes become mosaics of gene histories. In particular, the Tree of Life
has been questioned as an appropriate representation of bacterial evolutionary
history. Nevertheless a common hypothesis is that prokaryotic evolution is
primarily tree-like, but that the underlying trend is obscured by LGT.
Extensive empirical work has sought to extract a common tree-like signal from
conflicting gene trees. Here we give a probabilistic perspective on the problem
of recovering the tree-like trend despite LGT. Under a model of randomly
distributed LGT, we show that the species phylogeny can be reconstructed even
in the presence of surprisingly many (almost linear number of) LGT events per
gene tree. Our results, which are optimal up to logarithmic factors, are based
on the analysis of a robust, computationally efficient reconstruction method
and provides insight into the design of such methods. Finally we show that our
results have implications for the discovery of highways of gene sharing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3522</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3522</id><created>2012-06-15</created><authors><author><keyname>L&#xe4;ssig</keyname><forenames>J&#xf6;rg</forenames></author><author><keyname>Sudholt</keyname><forenames>Dirk</forenames></author></authors><title>General Upper Bounds on the Running Time of Parallel Evolutionary
  Algorithms</title><categories>cs.NE</categories><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new method for analyzing the running time of parallel
evolutionary algorithms with spatially structured populations. Based on the
fitness-level method, it yields upper bounds on the expected parallel running
time. This allows to rigorously estimate the speedup gained by parallelization.
Tailored results are given for common migration topologies: ring graphs, torus
graphs, hypercubes, and the complete graph. Example applications for
pseudo-Boolean optimization show that our method is easy to apply and that it
gives powerful results. In our examples the possible speedup increases with the
density of the topology. Surprisingly, even sparse topologies like ring graphs
lead to a significant speedup for many functions while not increasing the total
number of function evaluations by more than a constant factor. We also identify
which number of processors yield asymptotically optimal speedups, thus giving
hints on how to parametrize parallel evolutionary algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3523</identifier>
 <datestamp>2013-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3523</id><created>2012-06-15</created><updated>2012-12-19</updated><authors><author><keyname>Danner</keyname><forenames>N.</forenames></author><author><keyname>Paykin</keyname><forenames>J.</forenames></author><author><keyname>Royer</keyname><forenames>J. S.</forenames></author></authors><title>A static cost analysis for a higher-order language</title><categories>cs.PL</categories><comments>Final version</comments><acm-class>F.3.1; F.2.m; F.3.2</acm-class><journal-ref>M. Might and D. V. Horn (eds.), Proceedings of the 7th workshop on
  Programming languages meets program verification, pages 25-34. ACM Press,
  2013</journal-ref><doi>10.1145/2428116.2428123</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a static complexity analysis for a higher-order functional
language with structural list recursion. The complexity of an expression is a
pair consisting of a cost and a potential. The former is defined to be the size
of the expression's evaluation derivation in a standard big-step operational
semantics. The latter is a measure of the &quot;future&quot; cost of using the value of
that expression. A translation function tr maps target expressions to
complexities. Our main result is the following Soundness Theorem: If t is a
term in the target language, then the cost component of tr(t) is an upper bound
on the cost of evaluating t. The proof of the Soundness Theorem is formalized
in Coq, providing certified upper bounds on the cost of any expression in the
target language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3536</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3536</id><created>2012-06-15</created><updated>2013-04-15</updated><authors><author><keyname>Maier</keyname><forenames>Marc</forenames></author><author><keyname>Jensen</keyname><forenames>David</forenames></author></authors><title>Identifying Independence in Relational Models</title><categories>cs.AI</categories><comments>This paper has been revised and expanded. See &quot;Reasoning about
  Independence in Probabilistic Models of Relational Data&quot;
  http://arxiv.org/abs/1302.4381</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rules of d-separation provide a framework for deriving conditional
independence facts from model structure. However, this theory only applies to
simple directed graphical models. We introduce relational d-separation, a
theory for deriving conditional independence in relational models. We provide a
sound, complete, and computationally efficient method for relational
d-separation, and we present empirical results that demonstrate effectiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3538</identifier>
 <datestamp>2013-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3538</id><created>2012-06-15</created><updated>2013-11-07</updated><authors><author><keyname>Efthymiou</keyname><forenames>Charilaos</forenames></author></authors><title>Broadcasting colourings on trees. A combinatorial view</title><categories>cs.DM math.PR</categories><comments>Polishing and minor Corrections</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The broadcasting models on a d-ary tree T arise in many contexts such as
biology, information theory, statistical physics and computer science. We
consider the k-colouring model, i.e. the root of T is assigned an arbitrary
colour and, conditional on this assignment, we take a random colouring of T. A
basic question here is whether the information of the assignment at the root
affects the distribution of the colourings at the leaves. This is the so-called
reconstruction/non-reconstruction problem. It is well known that d/ln d is a
threshold function for this problem, i.e.
  * if k \geq (1+\eps)d/ln d, then the colouring of the root has a vanishing
effect on the distribution of the colourings at the leaves, as the height of
the tree grows
  * if $k\leq (1-\eps)d/ln d, then the colouring of the root biases the
distribution of the colouring of the leaves regardless of the height of the
tree.
  There is no apparent combinatorial reason why such a result should be true.
  When k\geq (1+\eps)d/ ln d, the threshold implies the following: We can
couple two broadcasting processes that assign the root different colours such
that the probability of having disagreement at the leaves reduces with their
distance from the root. It is natural to perceive such coupling as a mapping
from the colouring of the first broadcasting process to the colouring of the
second one. Here, we study how can we have such a mapping &quot;combinatorially&quot;.
Devising a mapping where the disagreements vanish as we move away from the root
turns out to be a non-trivial task to accomplish for any k \leq d.
  In this work we obtain a coupling which has the aforementioned property for
any k&gt;3d/ln d, i.e. much smaller than d. Interestingly enough, the decisions
that we make in the coupling are local. We relate our result to sampling
k-colourings of sparse random graphs, with expected degree d and k&lt;d.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3541</identifier>
 <datestamp>2014-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3541</id><created>2012-06-15</created><updated>2014-06-05</updated><authors><author><keyname>Alaei</keyname><forenames>Saeed</forenames></author><author><keyname>Fu</keyname><forenames>Hu</forenames></author><author><keyname>Haghpanah</keyname><forenames>Nima</forenames></author><author><keyname>Hartline</keyname><forenames>Jason</forenames></author></authors><title>The Simple Economics of Approximately Optimal Auctions</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The intuition that profit is optimized by maximizing marginal revenue is a
guiding principle in microeconomics. In the classical auction theory for agents
with linear utility and single-dimensional preferences, Bulow and Roberts
(1989) show that the optimal auction of Myerson (1981) is in fact optimizing
marginal revenue. In particular Myerson's virtual values are exactly the
derivative of an appropriate revenue curve.
  This paper considers mechanism design in environments where the agents have
multi-dimensional and non-linear preferences. Understanding good auctions for
these environments is considered to be the main challenge in Bayesian optimal
mechanism design. In these environments maximizing marginal revenue may not be
optimal and there is sometimes no direct way to implement the marginal revenue
maximization. Our contributions are three fold: we characterize the settings
for which marginal revenue maximization is optimal (by identifying an important
condition that we call revenue linearity), we give simple procedures for
implementing marginal revenue maximization in general, and we show that
marginal revenue maximization is approximately optimal. Our approximation
factor smoothly degrades in a term that quantifies how far the environment is
from ideal (where marginal revenue maximization is optimal). Because the
marginal revenue mechanism is optimal for single-dimensional agents, our
generalization immediately approximately extends many results for
single-dimensional agents.
  One of the biggest open questions in Bayesian algorithmic mechanism design is
developing methodologies that are not brute-force in the size of the agent type
space. Our methods identify a subproblem that, e.g., for unit-demand agents
with values drawn from product distributions, enables approximation mechanisms
that are polynomial in the dimension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3543</identifier>
 <datestamp>2013-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3543</id><created>2012-06-15</created><updated>2013-09-30</updated><authors><author><keyname>Vieland</keyname><forenames>V. J.</forenames></author><author><keyname>Das</keyname><forenames>J.</forenames></author><author><keyname>Hodge</keyname><forenames>S. E.</forenames></author><author><keyname>Seok</keyname><forenames>S. -C.</forenames></author></authors><title>Measurement of statistical evidence on an absolute scale following
  thermodynamic principles</title><categories>math.ST cs.IT math.IT physics.data-an q-bio.QM stat.TH</categories><comments>Final version of manuscript as published in Theory in Biosciences
  (2013)</comments><journal-ref>Theory Biosci 132:181-194 (2013)</journal-ref><doi>10.1007/s12064-013-0180-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistical analysis is used throughout biomedical research and elsewhere to
assess strength of evidence. We have previously argued that typical outcome
statistics (including p-values and maximum likelihood ratios) have poor
measure-theoretic properties: they can erroneously indicate decreasing evidence
as data supporting an hypothesis accumulate; and they are not amenable to
calibration, necessary for meaningful comparison of evidence across different
study designs, data types, and levels of analysis. We have also previously
proposed that thermodynamic theory, which allowed for the first time derivation
of an absolute measurement scale for temperature (T), could be used to derive
an absolute scale for evidence (E). Here we present a novel
thermodynamically-based framework in which measurement of E on an absolute
scale, for which &quot;one degree&quot; always means the same thing, becomes possible for
the first time. The new framework invites us to think about statistical
analyses in terms of the flow of (evidential) information, placing this work in
the context of a growing literature on connections among physics, information
theory, and statistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3551</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3551</id><created>2012-06-13</created><authors><author><keyname>Bhattacharjya</keyname><forenames>Debarun</forenames></author><author><keyname>Shachter</keyname><forenames>Ross D.</forenames></author></authors><title>Sensitivity analysis in decision circuits</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-34-42</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decision circuits have been developed to perform efficient evaluation of
influence diagrams [Bhattacharjya and Shachter, 2007], building on the advances
in arithmetic circuits for belief network inference [Darwiche,2003]. In the
process of model building and analysis, we perform sensitivity analysis to
understand how the optimal solution changes in response to changes in the
model. When sequential decision problems under uncertainty are represented as
decision circuits, we can exploit the efficient solution process embodied in
the decision circuit and the wealth of derivative information available to
compute the value of information for the uncertainties in the problem and the
effects of changes to model parameters on the value and the optimal strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3552</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3552</id><created>2012-06-15</created><authors><author><keyname>Coscia</keyname><forenames>Michele</forenames></author><author><keyname>Giannotti</keyname><forenames>Fosca</forenames></author><author><keyname>Pedreschi</keyname><forenames>Dino</forenames></author></authors><title>A Classification for Community Discovery Methods in Complex Networks</title><categories>cs.SI cs.DS physics.soc-ph</categories><comments>Published in the Statistical Analysis and Data Mining journal,
  Special Issue: Networks. Volume 4, Issue 5, pages 512-546, October 2011</comments><journal-ref>Statistical Analysis and Data Mining journal, Special Issue:
  Networks. Volume 4, Issue 5, pages 512-546, October 2011</journal-ref><doi>10.1002/sam.10133</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last few years many real-world networks have been found to show a
so-called community structure organization. Much effort has been devoted in the
literature to develop methods and algorithms that can efficiently highlight
this hidden structure of the network, traditionally by partitioning the graph.
Since network representation can be very complex and can contain different
variants in the traditional graph model, each algorithm in the literature
focuses on some of these properties and establishes, explicitly or implicitly,
its own definition of community. According to this definition it then extracts
the communities that are able to reflect only some of the features of real
communities. The aim of this survey is to provide a manual for the community
discovery problem. Given a meta definition of what a community in a social
network is, our aim is to organize the main categories of community discovery
based on their own definition of community. Given a desired definition of
community and the features of a problem (size of network, direction of edges,
multidimensionality, and so on) this review paper is designed to provide a set
of approaches that researchers could focus on.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3555</identifier>
 <datestamp>2012-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3555</id><created>2012-06-15</created><updated>2012-09-10</updated><authors><author><keyname>Stuhlm&#xfc;ller</keyname><forenames>Andreas</forenames></author><author><keyname>Goodman</keyname><forenames>Noah D.</forenames></author></authors><title>A Dynamic Programming Algorithm for Inference in Recursive Probabilistic
  Programs</title><categories>cs.AI cs.DS</categories><comments>Second Statistical Relational AI workshop at UAI 2012 (StaRAI-12)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a dynamic programming algorithm for computing the marginal
distribution of discrete probabilistic programs. This algorithm takes a
functional interpreter for an arbitrary probabilistic programming language and
turns it into an efficient marginalizer. Because direct caching of
sub-distributions is impossible in the presence of recursion, we build a graph
of dependencies between sub-distributions. This factored sum-product network
makes (potentially cyclic) dependencies between subproblems explicit, and
corresponds to a system of equations for the marginal distribution. We solve
these equations by fixed-point iteration in topological order. We illustrate
this algorithm on examples used in teaching probabilistic models, computational
cognitive science research, and game theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3559</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3559</id><created>2012-05-08</created><authors><author><keyname>Srivastava</keyname><forenames>Saumil</forenames></author></authors><title>Real time facial expression recognition using a novel method</title><categories>cs.CV</categories><comments>The International Journal of Multimedia &amp; Its Applications (IJMA)
  Vol.4, No.2, April 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses a novel method for Facial Expression Recognition System
which performs facial expression analysis in a near real time from a live web
cam feed. Primary objectives were to get results in a near real time with light
invariant, person independent and pose invariant way. The system is composed of
two different entities trainer and evaluator. Each frame of video feed is
passed through a series of steps including haar classifiers, skin detection,
feature extraction, feature points tracking, creating a learned Support Vector
Machine model to classify emotions to achieve a tradeoff between accuracy and
result rate. A processing time of 100-120 ms per 10 frames was achieved with
accuracy of around 60%. We measure our accuracy in terms of variety of
interaction and classification scenarios. We conclude by discussing relevance
of our work to human computer interaction and exploring further measures that
can be taken.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3562</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3562</id><created>2012-06-14</created><authors><author><keyname>Ding</keyname><forenames>Chunbao</forenames></author><author><keyname>Zhang</keyname><forenames>Wanrong</forenames></author><author><keyname>Jin</keyname><forenames>Dongyue</forenames></author><author><keyname>Xie</keyname><forenames>Hongyun</forenames></author><author><keyname>Shen</keyname><forenames>Pei</forenames></author><author><keyname>Chen</keyname><forenames>Liang</forenames></author></authors><title>A Novel Low Power UWB Cascode SiGe BiCMOS LNA with Current Reuse and
  Zero-Pole Cancellation</title><categories>cs.OH physics.ins-det</categories><comments>7 pages, 13 figures</comments><msc-class>94C30</msc-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  A low power cascode SiGe BiCMOS low noise amplifier (LNA) with current reuse
and zero-pole cancellation is presented for ultra-wideband (UWB) application.
The LNA is composed of cascode input stage and common emitter (CE) output stage
with dual loop feedbacks. The novel cascode-CE current reuse topology replaces
the traditional two stages topology so as to obtain low power consumption. The
emitter degenerative inductor in input stage is adopted to achieve good input
impedance matching and noise performance. The two poles are introduced by the
emitter inductor, which will degrade the gain performance, are cancelled by the
dual loop feedbacks of the resistance-inductor (RL) shunt-shunt feedback and
resistance-capacitor (RC) series-series feedback in the output stage.
Meanwhile, output impedance matching is also achieved. Based on TSMC 0.35{\mu}m
SiGe BiCMOS process, the topology and chip layout of the proposed LNA are
designed and post-simulated. The LNA achieves the noise figure of 2.3~4.1dB,
gain of 18.9~20.2dB, gain flatness of \pm0.65dB, input third order intercept
point (IIP3) of -7dBm at 6GHz, exhibits less than 16ps of group delay
variation, good input and output impedances matching, and unconditionally
stable over the whole band. The power consuming is only 18mW.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3564</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3564</id><created>2012-06-15</created><authors><author><keyname>Charon</keyname><forenames>Nicolas</forenames></author><author><keyname>Trouv&#xe9;</keyname><forenames>Alain</forenames></author></authors><title>Functional Currents : a new mathematical tool to model and analyse
  functional shapes</title><categories>cs.CG cs.CV math.DG</categories><comments>28 pages, 10 figures</comments><doi>10.1007/s10851-012-0413-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the concept of functional current as a mathematical
framework to represent and treat functional shapes, i.e. sub-manifold supported
signals. It is motivated by the growing occurrence, in medical imaging and
computational anatomy, of what can be described as geometrico-functional data,
that is a data structure that involves a deformable shape (roughly a finite
dimensional sub manifold) together with a function defined on this shape taking
value in another manifold.
  Indeed, if mathematical currents have already proved to be very efficient
theoretically and numerically to model and process shapes as curves or
surfaces, they are limited to the manipulation of purely geometrical objects.
We show that the introduction of the concept of functional currents offers a
genuine solution to the simultaneous processing of the geometric and signal
information of any functional shape. We explain how functional currents can be
equipped with a Hilbertian norm mixing geometrical and functional content of
functional shapes nicely behaving under geometrical and functional
perturbations and paving the way to various processing algorithms. We
illustrate this potential on two problems: the redundancy reduction of
functional shapes representations through matching pursuit schemes on
functional currents and the simultaneous geometric and functional registration
of functional shapes under diffeomorphic transport.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3582</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3582</id><created>2012-06-14</created><authors><author><keyname>Kalathil</keyname><forenames>Dileep</forenames></author><author><keyname>Nayyar</keyname><forenames>Naumaan</forenames></author><author><keyname>Jain</keyname><forenames>Rahul</forenames></author></authors><title>Decentralized Learning for Multi-player Multi-armed Bandits</title><categories>math.OC cs.LG cs.SY</categories><comments>33 pages, 3 figures. Submitted to IEEE Transactions on Information
  Theory</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  We consider the problem of distributed online learning with multiple players
in multi-armed bandits (MAB) models. Each player can pick among multiple arms.
When a player picks an arm, it gets a reward. We consider both i.i.d. reward
model and Markovian reward model. In the i.i.d. model each arm is modelled as
an i.i.d. process with an unknown distribution with an unknown mean. In the
Markovian model, each arm is modelled as a finite, irreducible, aperiodic and
reversible Markov chain with an unknown probability transition matrix and
stationary distribution. The arms give different rewards to different players.
If two players pick the same arm, there is a &quot;collision&quot;, and neither of them
get any reward. There is no dedicated control channel for coordination or
communication among the players. Any other communication between the users is
costly and will add to the regret. We propose an online index-based distributed
learning policy called ${\tt dUCB_4}$ algorithm that trades off
\textit{exploration v. exploitation} in the right way, and achieves expected
regret that grows at most as near-$O(\log^2 T)$. The motivation comes from
opportunistic spectrum access by multiple secondary users in cognitive radio
networks wherein they must pick among various wireless channels that look
different to different users. This is the first distributed learning algorithm
for multi-player MABs to the best of our knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3594</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3594</id><created>2012-06-15</created><authors><author><keyname>Bunyak</keyname><forenames>Yu. A.</forenames></author><author><keyname>Sofina</keyname><forenames>O. Yu.</forenames></author><author><keyname>Kvetnyy</keyname><forenames>R. N.</forenames></author></authors><title>Blind PSF estimation and methods of deconvolution optimization</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have shown that the left side null space of the autoregression (AR) matrix
operator is the lexicographical presentation of the point spread function (PSF)
on condition the AR parameters are common for original and blurred images. The
method of inverse PSF evaluation with regularization functional as the function
of surface area is offered. The inverse PSF was used for primary image
estimation. Two methods of original image estimate optimization were designed
basing on maximum entropy generalization of sought and blurred images
conditional probability density and regularization. The first method uses
balanced variations of convolution and deconvolution transforms to obtaining
iterative schema of image optimization. The variations balance was defined by
dynamic regularization basing on condition of iteration process convergence.
The regularization has dynamic character because depends on current and
previous image estimate variations. The second method implements the
regularization of deconvolution optimization in curved space with metric
defined on image estimate surface. It is basing on target functional invariance
to fluctuations of optimal argument value. The given iterative schemas have
faster convergence in comparison with known ones, so they can be used for
reconstruction of high resolution images series in real time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3599</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3599</id><created>2012-06-15</created><updated>2014-04-13</updated><authors><author><keyname>Banerjee</keyname><forenames>Siddhartha</forenames></author><author><keyname>Gopalan</keyname><forenames>Aditya</forenames></author><author><keyname>Das</keyname><forenames>Abhik Kumar</forenames></author><author><keyname>Shakkottai</keyname><forenames>Sanjay</forenames></author></authors><title>Epidemic Spreading with External Agents</title><categories>cs.SI cs.IT cs.NI math.IT physics.soc-ph</categories><doi>10.1109/TIT.2014.2316801</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study epidemic spreading processes in large networks, when the spread is
assisted by a small number of external agents: infection sources with bounded
spreading power, but whose movement is unrestricted vis-\`a-vis the underlying
network topology. For networks which are `spatially constrained', we show that
the spread of infection can be significantly speeded up even by a few such
external agents infecting randomly. Moreover, for general networks, we derive
upper-bounds on the order of the spreading time achieved by certain simple
(random/greedy) external-spreading policies. Conversely, for certain common
classes of networks such as line graphs, grids and random geometric graphs, we
also derive lower bounds on the order of the spreading time over all
(potentially network-state aware and adversarial) external-spreading policies;
these adversarial lower bounds match (up to logarithmic factors) the spreading
time achieved by an external agent with a random spreading policy. This
demonstrates that random, state-oblivious infection-spreading by an external
agent is in fact order-wise optimal for spreading in such spatially constrained
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3602</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3602</id><created>2012-06-15</created><authors><author><keyname>Park</keyname><forenames>Seok-Hwan</forenames><affiliation>Shitz</affiliation></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames><affiliation>Shitz</affiliation></author><author><keyname>Sahin</keyname><forenames>Onur</forenames><affiliation>Shitz</affiliation></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames><affiliation>Shitz</affiliation></author></authors><title>Robust and Efficient Distributed Compression for Cloud Radio Access
  Networks</title><categories>cs.IT math.IT</categories><comments>Technical report. 33 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work studies distributed compression for the uplink of a cloud radio
access network where multiple multi-antenna base stations (BSs) are connected
to a central unit, also referred to as cloud decoder, via capacity-constrained
backhaul links. Since the signals received at different BSs are correlated,
distributed source coding strategies are potentially beneficial, and can be
implemented via sequential source coding with side information. For the problem
of compression with side information, available compression strategies based on
the criteria of maximizing the achievable rate or minimizing the mean square
error are reviewed first. It is observed that, in either case, each BS requires
information about a specific covariance matrix in order to realize the
advantage of distributed source coding. Since this covariance matrix depends on
the channel realizations corresponding to other BSs, a robust compression
method is proposed for a practical scenario in which the information about the
covariance available at each BS is imperfect. The problem is formulated using a
deterministic worst-case approach, and an algorithm is proposed that achieves a
stationary point for the problem. Then, BS selection is addressed with the aim
of reducing the number of active BSs, thus enhancing the energy efficiency of
the network. An optimization problem is formulated in which compression and BS
selection are performed jointly by introducing a sparsity-inducing term into
the objective function. An iterative algorithm is proposed that is shown to
converge to a locally optimal point. From numerical results, it is observed
that the proposed robust compression scheme compensates for a large fraction of
the performance loss induced by the imperfect statistical information.
Moreover, the proposed BS selection algorithm is seen to perform close to the
more complex exhaustive search solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3603</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3603</id><created>2012-06-15</created><authors><author><keyname>Makarychev</keyname><forenames>Konstantin</forenames></author><author><keyname>Makarychev</keyname><forenames>Yury</forenames></author></authors><title>Approximation Algorithm for Non-Boolean MAX k-CSP</title><categories>cs.DS</categories><comments>The conference version of this paper will appear in the Proceedings
  of APPROX 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a randomized polynomial-time approximation
algorithm for k-CSPd. In k-CSPd, we are given a set of predicates of arity k
over an alphabet of size d. Our goal is to find an assignment that maximizes
the number of satisfied constraints.
  Our algorithm has approximation factor Omega(kd/d^k) (when k &gt; \Omega(log
d)). This bound is asymptotically optimal assuming the Unique Games Conjecture.
The best previously known algorithm has approximation factor Omega(k log
d/d^k).
  We also give an approximation algorithm for the boolean MAX k-CSP2 problem
with a slightly improved approximation guarantee.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3612</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3612</id><created>2012-06-15</created><authors><author><keyname>Huang</keyname><forenames>Shao-Lun</forenames></author><author><keyname>Zheng</keyname><forenames>Lizhong</forenames></author></authors><title>Linear Information Coupling Problems</title><categories>cs.IT math.IT</categories><comments>To appear, IEEE International Symposium on Information Theory, July,
  2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many network information theory problems face the similar difficulty of
single letterization. We argue that this is due to the lack of a geometric
structure on the space of probability distribution. In this paper, we develop
such a structure by assuming that the distributions of interest are close to
each other. Under this assumption, the K-L divergence is reduced to the squared
Euclidean metric in an Euclidean space. Moreover, we construct the notion of
coordinate and inner product, which will facilitate solving communication
problems. We will also present the application of this approach to the
point-to-point channel and the general broadcast channel, which demonstrates
how our technique simplifies information theory problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3614</identifier>
 <datestamp>2013-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3614</id><created>2012-06-15</created><updated>2013-08-06</updated><authors><author><keyname>Coffrin</keyname><forenames>Carleton</forenames></author><author><keyname>Van Hentenryck</keyname><forenames>Pascal</forenames></author></authors><title>A Linear-Programming Approximation of AC Power Flows</title><categories>cs.AI math.OC</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear active-power-only DC power flow approximations are pervasive in the
planning and control of power systems. However, these approximations fail to
capture reactive power and voltage magnitudes, both of which are necessary in
many applications to ensure voltage stability and AC power flow feasibility.
This paper proposes linear-programming models (the LPAC models) that
incorporate reactive power and voltage magnitudes in a linear power flow
approximation. The LPAC models are built on a convex approximation of the
cosine terms in the AC equations, as well as Taylor approximations of the
remaining nonlinear terms. Experimental comparisons with AC solutions on a
variety of standard IEEE and MatPower benchmarks show that the LPAC models
produce accurate values for active and reactive power, phase angles, and
voltage magnitudes. The potential benefits of the LPAC models are illustrated
on two &quot;proof-of-concept&quot; studies in power restoration and capacitor placement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3618</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3618</id><created>2012-06-15</created><authors><author><keyname>Veness</keyname><forenames>Joel</forenames></author><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author></authors><title>Sparse Sequential Dirichlet Coding</title><categories>cs.IT math.IT</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This short paper describes a simple coding technique, Sparse Sequential
Dirichlet Coding, for multi-alphabet memoryless sources. It is appropriate in
situations where only a small, unknown subset of the possible alphabet symbols
can be expected to occur in any particular data sequence. We provide a
competitive analysis which shows that the performance of Sparse Sequential
Dirichlet Coding will be close to that of a Sequential Dirichlet Coder that
knows in advance the exact subset of occurring alphabet symbols. Empirically we
show that our technique can perform similarly to the more computationally
demanding Sequential Sub-Alphabet Estimator, while using less computational
resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3628</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3628</id><created>2012-06-15</created><authors><author><keyname>Bauer</keyname><forenames>Matthew</forenames></author><author><keyname>Damian</keyname><forenames>Mirela</forenames></author></authors><title>An Infinite Class of Sparse-Yao Spanners</title><categories>cs.CG cs.DS</categories><comments>17 pages, 12 figures</comments><msc-class>05C85, 68R05</msc-class><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that, for any integer k &gt; 5, the Sparse-Yao graph YY_{6k} (also known
as Yao-Yao) is a spanner with stretch factor 11.67. The stretch factor drops
down to 4.75 for k &gt; 7.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3633</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3633</id><created>2012-06-16</created><authors><author><keyname>Mondal</keyname><forenames>Koushik</forenames></author><author><keyname>Dutta</keyname><forenames>Paramartha</forenames></author><author><keyname>Bhattacharyya</keyname><forenames>Siddhartha</forenames></author></authors><title>Feature Based Fuzzy Rule Base Design for Image Extraction</title><categories>cs.CV cs.AI</categories><comments>6 pages, 5 figures, Fuzzy Rule Base; Image Extraction; Fuzzy
  Inference System (FIS); Membership Functions; Region of Interests; Feature
  Selection</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the recent advancement of multimedia technologies, it becomes a major
concern of detecting visual attention regions in the field of image processing.
The popularity of the terminal devices in a heterogeneous environment of the
multimedia technology gives us enough scope for the betterment of image
visualization. Although there exist numerous methods, feature based image
extraction becomes a popular one in the field of image processing. The
objective of image segmentation is the domain-independent partition of the
image into a set of regions, which are visually distinct and uniform with
respect to some property, such as grey level, texture or colour. Segmentation
and subsequent extraction can be considered the first step and key issue in
object recognition, scene understanding and image analysis. Its application
area encompasses mobile devices, industrial quality control, medical
appliances, robot navigation, geophysical exploration, military applications,
etc. In all these areas, the quality of the final results depends largely on
the quality of the preprocessing work. Most of the times, acquiring
spurious-free preprocessing data requires a lot of application cum mathematical
intensive background works. We propose a feature based fuzzy rule guided novel
technique that is functionally devoid of any external intervention during
execution. Experimental results suggest that this approach is an efficient one
in comparison to different other techniques extensively addressed in
literature. In order to justify the supremacy of performance of our proposed
technique in respect of its competitors, we take recourse to effective metrics
like Mean Squared Error (MSE), Mean Absolute Error (MAE) and Peak Signal to
Noise Ratio (PSNR).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3634</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3634</id><created>2012-06-16</created><authors><author><keyname>Sahai</keyname><forenames>Ankur</forenames></author></authors><title>Balls into Bins: strict Capacities and Edge Weights</title><categories>cs.DS cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore a novel theoretical model for studying the performance of
distributed storage management systems where the data-centers have limited
capacities (as compared to storage space requested by the users). Prior schemes
such as Balls-into-bins (used for load balancing) neither consider bin
(consumer) capacities (multiple balls into a bin) nor the future performance of
the system after, balls (producer requests) are allocated to bins and restrict
number of balls as a function of the number of bins. Our problem consists of
finding an optimal assignment of the online producer requests to consumers (via
weighted edges) in a complete bipartite graph while ensuring that the total
size of request assigned on a consumer is limited by its capacity. The metric
used to measure the performance in this model is the (minimization of) weighted
sum of the requests assigned on the edges (loads) and their corresponding
weights. We first explore the optimal offline algorithms followed by
competitive analysis of different online techniques. Using oblivious adversary.
LP and Primal-Dual algorithms are used for calculating the optimal offline
solution in O(r*n) time (where r and n are the number of requests and consumers
respectively) while randomized algorithms are used for the online case.
  For the simplified model with equal consumer capacities an average-case
competitive ratio of AVG(d) / MIN(d) (where d is the edge weight / distance) is
achieved using an algorithm that has equal probability for selecting any of the
available edges with a running time of $O(r)$. In the extending the model to
arbitrary consumer capacities we show an average case competitive ratio of
AVG(d*c) / (AVG(c) *MIN(d)).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3658</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3658</id><created>2012-06-16</created><authors><author><keyname>Harnad</keyname><forenames>Stevan</forenames></author></authors><title>Alan Turing and the &quot;Hard&quot; and &quot;Easy&quot; Problem of Cognition: Doing and
  Feeling</title><categories>cs.AI cs.RO</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The &quot;easy&quot; problem of cognitive science is explaining how and why we can do
what we can do. The &quot;hard&quot; problem is explaining how and why we feel. Turing's
methodology for cognitive science (the Turing Test) is based on doing: Design a
model that can do anything a human can do, indistinguishably from a human, to a
human, and you have explained cognition. Searle has shown that the successful
model cannot be solely computational. Sensory-motor robotic capacities are
necessary to ground some, at least, of the model's words, in what the robot can
do with the things in the world that the words are about. But even grounding is
not enough to guarantee that -- nor to explain how and why -- the model feels
(if it does). That problem is much harder to solve (and perhaps insoluble).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3664</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3664</id><created>2012-06-16</created><authors><author><keyname>Gargouri</keyname><forenames>Yassine</forenames></author><author><keyname>Larivi&#xe8;re</keyname><forenames>Vincent</forenames></author><author><keyname>Gingras</keyname><forenames>Yves</forenames></author><author><keyname>Carr</keyname><forenames>Les</forenames></author><author><keyname>Harnad</keyname><forenames>Stevan</forenames></author></authors><title>Green and Gold Open Access Percentages and Growth, by Discipline</title><categories>cs.DL</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Most refereed journal articles today are published in subscription journals,
accessible only to subscribing institutions, hence losing considerable research
impact. Making articles freely accessible online (&quot;Open Access,&quot; OA) maximizes
their impact. Articles can be made OA in two ways: by self-archiving them on
the web (&quot;Green OA&quot;) or by publishing them in OA journals (&quot;Gold OA&quot;). We
compared the percent and growth rate of Green and Gold OA for 14 disciplines in
two random samples of 1300 articles per discipline out of the 12,500 journals
indexed by Thomson-Reuters-ISI using a robot that trawled the web for OA
full-texts. We sampled in 2009 and 2011 for publication year ranges 1998-2006
and 2005-2010, respectively. Green OA (21.4%) exceeds Gold OA (2.4%) in
proportion and growth rate in all but the biomedical disciplines, probably
because it can be provided for all journals articles and does not require
paying extra Gold OA publication fees. The spontaneous overall OA growth rate
is still very slow (about 1% per year). If institutions make Green OA
self-archiving mandatory, however, it triples percent Green OA as well as
accelerating its growth rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3666</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3666</id><created>2012-06-16</created><authors><author><keyname>G&#xfc;rel</keyname><forenames>Tayfun</forenames></author><author><keyname>Mehring</keyname><forenames>Carsten</forenames></author></authors><title>Unsupervised adaptation of brain machine interface decoders</title><categories>cs.LG q-bio.NC</categories><comments>28 pages, 13 figures, submitted to Frontiers in Neuroprosthetics</comments><acm-class>I.2.6; I.2.8; G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance of neural decoders can degrade over time due to
nonstationarities in the relationship between neuronal activity and behavior.
In this case, brain-machine interfaces (BMI) require adaptation of their
decoders to maintain high performance across time. One way to achieve this is
by use of periodical calibration phases, during which the BMI system (or an
external human demonstrator) instructs the user to perform certain movements or
behaviors. This approach has two disadvantages: (i) calibration phases
interrupt the autonomous operation of the BMI and (ii) between two calibration
phases the BMI performance might not be stable but continuously decrease. A
better alternative would be that the BMI decoder is able to continuously adapt
in an unsupervised manner during autonomous BMI operation, i.e. without knowing
the movement intentions of the user.
  In the present article, we present an efficient method for such unsupervised
training of BMI systems for continuous movement control. The proposed method
utilizes a cost function derived from neuronal recordings, which guides a
learning algorithm to evaluate the decoding parameters. We verify the
performance of our adaptive method by simulating a BMI user with an optimal
feedback control model and its interaction with our adaptive BMI decoder. The
simulation results show that the cost function and the algorithm yield fast and
precise trajectories towards targets at random orientations on a 2-dimensional
computer screen. For initially unknown and non-stationary tuning parameters,
our unsupervised method is still able to generate precise trajectories and to
keep its performance stable in the long term. The algorithm can optionally work
also with neuronal error signals instead or in conjunction with the proposed
unsupervised adaptation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3667</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3667</id><created>2012-06-16</created><authors><author><keyname>Ahuja</keyname><forenames>Sudhir</forenames></author><author><keyname>Goyal</keyname><forenames>Mr. Rinkaj</forenames></author></authors><title>Information Retrieval in Intelligent Systems: Current Scenario &amp; Issues</title><categories>cs.IR cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web space is the huge repository of data. Everyday lots of new information
get added to this web space. The more the information, more is demand for tools
to access that information. Answering users' queries about the online
information intelligently is one of the great challenges in information
retrieval in intelligent systems. In this paper, we will start with the brief
introduction on information retrieval and intelligent systems and explain how
swoogle, the semantic search engine, uses its algorithms and techniques to
search for the desired contents in the web. We then continue with the
clustering technique that is used to group the similar things together and
discuss the machine learning technique called Self-organizing maps [6] or SOM,
which is a data visualization technique that reduces the dimensions of data
through the use of self-organizing neural networks. We then discuss how SOM is
used to visualize the contents of the data, by following some lines of
algorithm, in the form of maps. So, we could say that websites or machines can
be used to retrieve the information that what exactly users want from them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3709</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3709</id><created>2012-06-16</created><authors><author><keyname>Bordalo</keyname><forenames>P.</forenames></author><author><keyname>Nunes</keyname><forenames>A. S.</forenames></author><author><keyname>Pires</keyname><forenames>C.</forenames></author><author><keyname>Quintans</keyname><forenames>C.</forenames></author><author><keyname>Ramos</keyname><forenames>S.</forenames></author></authors><title>Control Systems: an Application to a High Energy Physics Experiment
  (COMPASS)</title><categories>cs.SY hep-ex physics.ins-det</categories><comments>6 pages, 3 figures, to appear on the proceedings of the 2012 IEEE
  International Conference on Automation, Quality and Testing, Robotics (AQTR
  2012), Cluj-Napoca, May 24-27, 2012</comments><journal-ref>Proceedings of the 2012 IEEE International Conference on
  Automation, Quality and Testing, Robotics (AQTR 2012), p. 20-25</journal-ref><doi>10.1109/AQTR.2012.6237669</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Detector Control System (DCS) of the COMPASS experiment at CERN is
presented. The experiment has a high level of complexity and flexibility and a
long time of operation, that constitute a challenge for its full monitorisation
and control. A strategy to use a limited number of standardised,
cost-effective, industrial solutions of hardware and software was pursued. When
such solutions were not available or could not be used, customised solutions
were developed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3713</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3713</id><created>2012-06-16</created><updated>2015-05-03</updated><authors><author><keyname>Honorio</keyname><forenames>Jean</forenames></author><author><keyname>Ortiz</keyname><forenames>Luis</forenames></author></authors><title>Learning the Structure and Parameters of Large-Population Graphical
  Games from Behavioral Data</title><categories>cs.LG cs.GT stat.ML</categories><comments>Journal of Machine Learning Research. (accepted, pending
  publication.) Last conference version: submitted March 30, 2012 to UAI 2012.
  First conference version: entitled, Learning Influence Games, initially
  submitted on June 1, 2010 to NIPS 2010</comments><journal-ref>Journal of Machine Learning Research (JMLR), 16(Jul):1249--1274,
  2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider learning, from strictly behavioral data, the structure and
parameters of linear influence games (LIGs), a class of parametric graphical
games introduced by Irfan and Ortiz (2014). LIGs facilitate causal strategic
inference (CSI): Making inferences from causal interventions on stable behavior
in strategic settings. Applications include the identification of the most
influential individuals in large (social) networks. Such tasks can also support
policy-making analysis. Motivated by the computational work on LIGs, we cast
the learning problem as maximum-likelihood estimation (MLE) of a generative
model defined by pure-strategy Nash equilibria (PSNE). Our simple formulation
uncovers the fundamental interplay between goodness-of-fit and model
complexity: good models capture equilibrium behavior within the data while
controlling the true number of equilibria, including those unobserved. We
provide a generalization bound establishing the sample complexity for MLE in
our framework. We propose several algorithms including convex loss minimization
(CLM) and sigmoidal approximations. We prove that the number of exact PSNE in
LIGs is small, with high probability; thus, CLM is sound. We illustrate our
approach on synthetic data and real-world U.S. congressional voting records. We
briefly discuss our learning framework's generality and potential applicability
to general graphical games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3714</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3714</id><created>2012-06-16</created><authors><author><keyname>Divvala</keyname><forenames>Santosh K.</forenames></author><author><keyname>Efros</keyname><forenames>Alexei A.</forenames></author><author><keyname>Hebert</keyname><forenames>Martial</forenames></author></authors><title>How important are Deformable Parts in the Deformable Parts Model?</title><categories>cs.CV cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main stated contribution of the Deformable Parts Model (DPM) detector of
Felzenszwalb et al. (over the Histogram-of-Oriented-Gradients approach of Dalal
and Triggs) is the use of deformable parts. A secondary contribution is the
latent discriminative learning. Tertiary is the use of multiple components. A
common belief in the vision community (including ours, before this study) is
that their ordering of contributions reflects the performance of detector in
practice. However, what we have experimentally found is that the ordering of
importance might actually be the reverse. First, we show that by increasing the
number of components, and switching the initialization step from their
aspect-ratio, left-right flipping heuristics to appearance-based clustering,
considerable improvement in performance is obtained. But more intriguingly, we
show that with these new components, the part deformations can now be
completely switched off, yet obtaining results that are almost on par with the
original DPM detector. Finally, we also show initial results for using multiple
components on a different problem -- scene classification, suggesting that this
idea might have wider applications in addition to object detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3717</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3717</id><created>2012-06-16</created><authors><author><keyname>Zheng</keyname><forenames>Qingji</forenames></author><author><keyname>Zhang</keyname><forenames>Xinwen</forenames></author></authors><title>Multiparty Cloud Computation</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the increasing popularity of the cloud, clients oursource their data to
clouds in order to take advantage of unlimited virtualized storage space and
the low management cost. Such trend prompts the privately oursourcing
computation, called \emph{multiparty cloud computation} (\MCC): Given $k$
clients storing their data in the cloud, how can they perform the joint
functionality by contributing their private data as inputs, and making use of
cloud's powerful computation capability. Namely, the clients wish to oursource
computation to the cloud together with their private data stored in the cloud,
which naturally happens when the computation is involved with large datasets,
e.g., to analyze malicious URLs. We note that the \MCC\ problem is different
from widely considered concepts, e.g., secure multiparty computation and
multiparty computation with server aid.
  To address this problem, we introduce the notion of \emph{homomorphic
threshold proxy re-encryption} schemes, which are encryption schemes that enjoy
three promising properties: proxy re-encryption -- transforming encrypted data
of one user to encrypted data of target user, threshold decryption --
decrypting encrypted data by combining secret key shares obtained by a set of
users, and homomorphic computation -- evaluating functions on the encrypted
data. To demonstrate the feasibility of the proposed approach, we present an
encryption scheme which allows anyone to compute arbitrary many additions and
at most one multiplications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3718</identifier>
 <datestamp>2012-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3718</id><created>2012-06-16</created><updated>2012-07-02</updated><authors><author><keyname>Rothvoss</keyname><forenames>Thomas</forenames></author></authors><title>A simpler proof for O(congestion + dilation) packet routing</title><categories>cs.DS cs.DM math.CO</categories><acm-class>F.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the store-and-forward routing problem, packets have to be routed along
given paths such that the arrival time of the latest packet is minimized. A
groundbreaking result of Leighton, Maggs and Rao says that this can always be
done in time O(congestion + dilation), where the congestion is the maximum
number of paths using an edge and the dilation is the maximum length of a path.
However, the analysis is quite arcane and complicated and works by iteratively
improving an infeasible schedule. Here, we provide a more accessible analysis
which is based on conditional expectations. Like [LMR94], our easier analysis
also guarantees that constant size edge buffers suffice.
  Moreover, it was an open problem stated e.g. by Wiese, whether there is any
instance where all schedules need at least (1 + epsilon)*(congestion +
dilation) steps, for a constant epsilon &gt; 0. We answer this question
affirmatively by making use of a probabilistic construction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3719</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3719</id><created>2012-06-16</created><authors><author><keyname>Zamani</keyname><forenames>Mahdi</forenames></author><author><keyname>Khandani</keyname><forenames>Amir K.</forenames></author></authors><title>Broadcast Approaches to the Diamond Channel</title><categories>cs.IT math.IT</categories><comments>30 pages, 6 figures, Submitted to Trans. on Inform. Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of dual-hop transmission from a source to a destination via two
parallel full-duplex relays in block Rayleigh fading environment is
investigated. All nodes in the network are assumed to be oblivious to their
forward channel gains; however, they have perfect information about their
backward channel gains. The focus of this paper is on simple, efficient, and
practical relaying schemes to increase the expected-rate at the destination.
For this purpose, various combinations of relaying protocols and the broadcast
approach (multi-layer coding) are proposed. For the decode-forward (DF)
relaying, the maximum finite-layer expected-rate as well as two upper-bounds on
the continuous-layer expected-rate are obtained. The main feature of the
proposed DF scheme is that the layers being decoded at both relays are added
coherently at the destination although each relay has no information about the
number of layers being successfully decoded by the other relay. It is proved
that the optimal coding scheme is transmitting uncorrelated signals via the
relays. Next, the maximum expected-rate of ON/OFF based amplify-forward (AF)
relaying is analytically derived. For further performance improvement, a hybrid
decode-amplify-forward (DAF) relaying strategy, adopting the broadcast approach
at the source and relays, is proposed and its maximum throughput and maximum
finite-layer expected-rate are presented. Moreover, the maximum throughput and
maximum expected-rate in the compress-forward (CF) relaying adopting the
broadcast approach, using optimal quantizers and Wyner-Ziv compression at the
relays, are fully derived. All theoretical results are illustrated by numerical
simulations. As it turns out from the results, when the ratio of the relay
power to the source power is high, the CF relaying outperforms DAF (and hence
outperforms both DF and AF relaying); otherwise, DAF scheme is superior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3721</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3721</id><created>2012-06-17</created><authors><author><keyname>Takabatake</keyname><forenames>Kazuya</forenames></author><author><keyname>Akaho</keyname><forenames>Shotaro</forenames></author></authors><title>Constraint-free Graphical Model with Fast Learning Algorithm</title><categories>cs.LG stat.ML</categories><comments>9 pages, 11 figures, submitted to UAI2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a simple, versatile model for learning the
structure and parameters of multivariate distributions from a data set.
Learning a Markov network from a given data set is not a simple problem,
because Markov networks rigorously represent Markov properties, and this rigor
imposes complex constraints on the design of the networks. Our proposed model
removes these constraints, acquiring important aspects from the information
geometry. The proposed parameter- and structure-learning algorithms are simple
to execute as they are based solely on local computation at each node.
Experiments demonstrate that our algorithms work appropriately.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3722</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3722</id><created>2012-06-17</created><authors><author><keyname>Pandey</keyname><forenames>Umesh Kumar</forenames></author><author><keyname>Yadav</keyname><forenames>Surjeet Kumar</forenames></author><author><keyname>Pal</keyname><forenames>Saurabh</forenames></author></authors><title>Data Mining Application to Attract Students in HEI</title><categories>cs.CY</categories><comments>6 pages</comments><journal-ref>International Journal on Computer Science and Engineering
  (IJCSE)Vol. 4 No. 06 June 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last two decades, number of Higher Education Institutions (HEI) grows
in leaps and bounds. This causes a cut throat competition among these
institutions while attracting the student get admission in these institutions.
To make reach up to the students institution makes effort of advertisement.
Similarly developing and developed both type of institution launch several
services also to attract students. Most of the institutions are opened in self
finance mode. So all time they feel short hand in expenditure. Now a day a
number of advertisement methods are available. So it is difficult for an
institution to make advertisement through all modes and launch all services at
the same time due to different constraints. In this paper we use support and
confidence method to find out the best way of advertisement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3728</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3728</id><created>2012-06-17</created><authors><author><keyname>Zhao</keyname><forenames>Xiaochuan</forenames></author><author><keyname>Sayed</keyname><forenames>Ali H.</forenames></author></authors><title>Performance Limits for Distributed Estimation Over LMS Adaptive Networks</title><categories>cs.IT cs.DC cs.SY math.IT</categories><comments>39 pages, 7 figures, to appear in IEEE Transactions on Signal
  Processing, 2012</comments><doi>10.1109/TSP.2012.2204985</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we analyze the mean-square performance of different strategies
for distributed estimation over least-mean-squares (LMS) adaptive networks. The
results highlight some useful properties for distributed adaptation in
comparison to fusion-based centralized solutions. The analysis establishes
that, by optimizing over the combination weights, diffusion strategies can
deliver lower excess-mean-square-error than centralized solutions employing
traditional block or incremental LMS strategies. We first study in some detail
the situation involving combinations of two adaptive agents and then extend the
results to generic N-node ad-hoc networks. In the later case, we establish
that, for sufficiently small step-sizes, diffusion strategies can outperform
centralized block or incremental LMS strategies by optimizing over
left-stochastic combination weighting matrices. The results suggest more
efficient ways for organizing and processing data at fusion centers, and
present useful adaptive strategies that are able to enhance performance when
implemented in a distributed manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3738</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3738</id><created>2012-06-17</created><authors><author><keyname>Treibig</keyname><forenames>Jan</forenames></author><author><keyname>Hager</keyname><forenames>Georg</forenames></author><author><keyname>Wellein</keyname><forenames>Gerhard</forenames></author></authors><title>Best practices for HPM-assisted performance engineering on modern
  multicore processors</title><categories>cs.PF cs.DC</categories><comments>10 pages, 2 figures</comments><doi>10.1007/978-3-642-36949-0_50</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many tools and libraries employ hardware performance monitoring (HPM) on
modern processors, and using this data for performance assessment and as a
starting point for code optimizations is very popular. However, such data is
only useful if it is interpreted with care, and if the right metrics are chosen
for the right purpose. We demonstrate the sensible use of hardware performance
counters in the context of a structured performance engineering approach for
applications in computational science. Typical performance patterns and their
respective metric signatures are defined, and some of them are illustrated
using case studies. Although these generic concepts do not depend on specific
tools or environments, we restrict ourselves to modern x86-based multicore
processors and use the likwid-perfctr tool under the Linux OS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3746</identifier>
 <datestamp>2012-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3746</id><created>2012-06-17</created><updated>2012-09-04</updated><authors><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>Advances in Science Visualization: Social Networks, Semantic Maps, and
  Discursive Knowledge</title><categories>cs.CY cs.DL</categories><comments>forthcoming in: Blaise Cronin &amp; Cassidy Sugimoto (Eds.),
  Bibliometrics and Beyond: Metrics-Based Evaluation of Scholarly Research,
  Cambridge, MA: MIT Press</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Positional and relational perspectives on network data have led to two
different research traditions in textual analysis and social network analysis,
respectively. Latent Semantic Analysis (LSA) focuses on the latent dimensions
in textual data; social network analysis (SNA) on the observable networks. The
two coupled topographies of information-processing in the network space and
meaning-processing in the vector space operate with different (nonlinear)
dynamics. The historical dynamics of information processing in observable
networks organizes the system into instantiations; the systems dynamics,
however, can be considered as self-organizing in terms of fluxes of
communication along the various dimensions that operate with different codes.
The development over time adds evolutionary differentiation to the historical
integration; a richer structure can process more complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3747</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3747</id><created>2012-06-17</created><updated>2012-11-10</updated><authors><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>Statistics for the Dynamic Analysis of Scientometric Data: The evolution
  of the sciences in terms of trajectories and regimes</title><categories>cs.DL</categories><comments>Scientometrics, in press (Nov. 2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The gap in statistics between multi-variate and time-series analysis can be
bridged by using entropy statistics and recent developments in
multi-dimensional scaling. For explaining the evolution of the sciences as
non-linear dynamics, the configurations among variables can be important in
addition to the statistics of individual variables and trend lines. Animations
enable us to combine multiple perspectives (based on configurations of
variables) and to visualize path-dependencies in terms of trajectories and
regimes. Path-dependent transitions and systems formation can be tested using
entropy statistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3764</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3764</id><created>2012-06-17</created><authors><author><keyname>Das</keyname><forenames>Rajib</forenames></author><author><keyname>Purkayastha</keyname><forenames>Bipul Syam</forenames></author><author><keyname>Das</keyname><forenames>Prodipto</forenames></author></authors><title>Security Measures for Black Hole Attack in MANET: An Approach</title><categories>cs.NI</categories><comments>7 pages, 8 figures, International Journal of Engineering Science and
  Technology (IJEST)</comments><journal-ref>International Journal of Engineering Science and Technology
  (IJEST), Vol. 3 No. 4 Apr 2011, pp. 2832-2838, ISSN: 0975-5462</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Mobile Ad-Hoc Network is a collection of mobile nodes that are dynamically
and arbitrarily located in such a manner that the interconnections between
nodes are capable of changing on continual basis. Due to security
vulnerabilities of the routing protocols, wireless ad-hoc networks are
unprotected to attacks of the malicious nodes. One of these attacks is the
Black Hole Attack. In this paper, we give an algorithmic approach to focus on
analyzing and improving the security of AODV, which is one of the popular
routing protocols for MANET. Our aim is on ensuring the security against Black
hole attack. The proposed solution is capable of detecting &amp; removing Black
hole node(s) in the MANET at the beginning. Also the objective of this paper is
to provide a simulation study that illustrates the effects of Black hole attack
on network performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3768</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3768</id><created>2012-06-17</created><updated>2013-07-04</updated><authors><author><keyname>Di Napoli</keyname><forenames>Edoardo</forenames><affiliation>JSC, Forschungszentrum Juelich)</affiliation></author><author><keyname>Berljafa</keyname><forenames>Mario</forenames><affiliation>Dept. of Mathematics, Univ. of Zagreb</affiliation></author></authors><title>Block Iterative Eigensolvers for Sequences of Correlated Eigenvalue
  Problems</title><categories>cs.DS cs.PF physics.comp-ph</categories><comments>12 Pages, 5 figures. Accepted for publication on Computer Physics
  Communications</comments><report-no>AICES-2012/12-1</report-no><doi>10.1016/j.cpc.2013.06.017</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Density Functional Theory simulations based on the LAPW method, each
self-consistent field cycle comprises dozens of large dense generalized
eigenproblems. In contrast to real-space methods, eigenpairs solving for
problems at distinct cycles have either been believed to be independent or at
most very loosely connected. In a recent study [7], it was demonstrated that,
contrary to belief, successive eigenproblems in a sequence are strongly
correlated with one another. In particular, by monitoring the subspace angles
between eigenvectors of successive eigenproblems, it was shown that these
angles decrease noticeably after the first few iterations and become close to
collinear. This last result suggests that we can manipulate the eigenvectors,
solving for a specific eigenproblem in a sequence, as an approximate solution
for the following eigenproblem. In this work we present results that are in
line with this intuition. We provide numerical examples where opportunely
selected block iterative eigensolvers benefit from the reuse of eigenvectors by
achieving a substantial speed-up. The results presented will eventually open
the way to a widespread use of block iterative eigensolvers in ab initio
electronic structure codes based on the LAPW approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3777</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3777</id><created>2012-06-17</created><authors><author><keyname>Beg</keyname><forenames>Mahjabeen Mirza</forenames></author><author><keyname>Jain</keyname><forenames>Monika</forenames></author></authors><title>An Analysis of the Methods Employed for Breast Cancer Diagnosis</title><categories>cs.NE q-bio.TO</categories><comments>5 pages, 6 figures</comments><journal-ref>International Journal of Research in Computer Science 2 (2012)
  25-29</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Breast cancer research over the last decade has been tremendous. The ground
breaking innovations and novel methods help in the early detection, in setting
the stages of the therapy and in assessing the response of the patient to the
treatment. The prediction of the recurrent cancer is also crucial for the
survival of the patient. This paper studies various techniques used for the
diagnosis of breast cancer. Different methods are explored for their merits and
de-merits for the diagnosis of breast lesion. Some of the methods are yet
unproven but the studies look very encouraging. It was found that the recent
use of the combination of Artificial Neural Networks in most of the instances
gives accurate results for the diagnosis of breast cancer and their use can
also be extended to other diseases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3789</identifier>
 <datestamp>2012-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3789</id><created>2012-06-17</created><authors><author><keyname>Rinaudo</keyname><forenames>Philippe</forenames><affiliation>LRI, INRIA Saclay - Ile de France, PRISM</affiliation></author><author><keyname>Ponty</keyname><forenames>Yann</forenames><affiliation>INRIA Saclay - Ile de France, LIX</affiliation></author><author><keyname>Barth</keyname><forenames>Dominique</forenames><affiliation>PRISM</affiliation></author><author><keyname>Denise</keyname><forenames>Alain</forenames><affiliation>LRI, INRIA Saclay - Ile de France, IGM</affiliation></author></authors><title>Tree decomposition and parameterized algorithms for RNA
  structure-sequence alignment including tertiary interactions and pseudoknots</title><categories>q-bio.QM cs.DS</categories><comments>(2012)</comments><proxy>ccsd</proxy><journal-ref>WABI - 12th Workshop on Algorithms in Bioinformatics - 2012 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a general setting for structure-sequence comparison in a large
class of RNA structures that unifies and generalizes a number of recent works
on specific families on structures. Our approach is based on tree decomposition
of structures and gives rises to a general parameterized algorithm, where the
exponential part of the complexity depends on the family of structures. For
each of the previously studied families, our algorithm has the same complexity
as the specific algorithm that had been given before.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3793</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3793</id><created>2012-06-17</created><authors><author><keyname>Fagnani</keyname><forenames>Fabio</forenames></author><author><keyname>Fosson</keyname><forenames>Sophie M.</forenames></author><author><keyname>Ravazzi</keyname><forenames>Chiara</forenames></author></authors><title>A distributed classification/estimation algorithm for sensor networks</title><categories>cs.MA cs.SY math.OC</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, we address the problem of simultaneous classification and
estimation of hidden parameters in a sensor network with communications
constraints. In particular, we consider a network of noisy sensors which
measure a common scalar unknown parameter. We assume that a fraction of the
nodes represent faulty sensors, whose measurements are poorly reliable. The
goal for each node is to simultaneously identify its class (faulty or
non-faulty) and estimate the common parameter.
  We propose a novel cooperative iterative algorithm which copes with the
communication constraints imposed by the network and shows remarkable
performance. Our main result is a rigorous proof of the convergence of the
algorithm and a characterization of the limit behavior. We also show that, in
the limit when the number of sensors goes to infinity, the common unknown
parameter is estimated with arbitrary small error, while the classification
error converges to that of the optimal centralized maximum likelihood
estimator. We also show numerical results that validate the theoretical
analysis and support their possible generalization. We compare our strategy
with the Expectation-Maximization algorithm and we discuss trade-offs in terms
of robustness, speed of convergence and implementation simplicity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3804</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3804</id><created>2012-06-17</created><updated>2014-05-03</updated><authors><author><keyname>Papailiopoulos</keyname><forenames>Dimitris S.</forenames></author><author><keyname>Dimakis</keyname><forenames>Alexandros G.</forenames></author></authors><title>Locally Repairable Codes</title><categories>cs.IT cs.DC cs.NI math.IT</categories><comments>presented at ISIT 2012, accepted for publication in IEEE Trans. IT,
  2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed storage systems for large-scale applications typically use
replication for reliability. Recently, erasure codes were used to reduce the
large storage overhead, while increasing data reliability. A main limitation of
off-the-shelf erasure codes is their high-repair cost during single node
failure events. A major open problem in this area has been the design of codes
that {\it i)} are repair efficient and {\it ii)} achieve arbitrarily high data
rates.
  In this paper, we explore the repair metric of {\it locality}, which
corresponds to the number of disk accesses required during a
{\color{black}single} node repair. Under this metric we characterize an
information theoretic trade-off that binds together locality, code distance,
and the storage capacity of each node. We show the existence of optimal {\it
locally repairable codes} (LRCs) that achieve this trade-off. The achievability
proof uses a locality aware flow-graph gadget which leads to a randomized code
construction. Finally, we present an optimal and explicit LRC that achieves
arbitrarily high data-rates. Our locality optimal construction is based on
simple combinations of Reed-Solomon blocks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3806</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3806</id><created>2012-06-17</created><updated>2014-02-18</updated><authors><author><keyname>Kodejska</keyname><forenames>Milos</forenames></author><author><keyname>Mokry</keyname><forenames>Pavel</forenames></author><author><keyname>Linhart</keyname><forenames>Vaclav</forenames></author><author><keyname>Vaclavik</keyname><forenames>Jan</forenames></author><author><keyname>Sluka</keyname><forenames>Tomas</forenames></author></authors><title>Adaptive vibration suppression system: An iterative control law for a
  piezoelectric actuator shunted by a negative capacitor</title><categories>cs.CE cs.SY physics.class-ph</categories><comments>This is a revised version of the paper published in IEEE TUFFC in Dec
  2012, which includes the erratum submitted in Feb 2014</comments><journal-ref>IEEE TRANSACTIONS ON ULTRASONICS FERROELECTRICS AND FREQUENCY
  CONTROL, Volume: 59 Issue: 12 Pages: 2785-2796, 2012</journal-ref><doi>10.1109/TUFFC.2012.2520</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An adaptive system for the suppression of vibration transmission using a
single piezoelectric actuator shunted by a negative capacitance circuit is
presented. It is known that using negative capacitance shunt, the spring
constant of piezoelectric actuator can be controlled to extreme values of zero
or infinity. Since the value of spring constant controls a force transmitted
through an elastic element, it is possible to achieve a reduction of
transmissibility of vibrations through a piezoelectric actuator by reducing its
effective spring constant. The narrow frequency range and broad frequency range
vibration isolation systems are analyzed, modeled, and experimentally
investigated. The problem of high sensitivity of the vibration control system
to varying operational conditions is resolved by applying an adaptive control
to the circuit parameters of the negative capacitor. A control law that is
based on the estimation of the value of effective spring constant of shunted
piezoelectric actuator is presented. An adaptive system, which achieves a
self-adjustment of the negative capacitor parameters is presented. It is shown
that such an arrangement allows a design of a simple electronic system, which,
however, offers a great vibration isolation efficiency in variable vibration
conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3819</identifier>
 <datestamp>2013-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3819</id><created>2012-06-17</created><updated>2013-01-15</updated><authors><author><keyname>Yu</keyname><forenames>Zhenhua</forenames></author><author><keyname>Baxley</keyname><forenames>Robert J.</forenames></author><author><keyname>Zhou</keyname><forenames>G. Tong</forenames></author></authors><title>EVM and Achievable Data Rate Analysis of Clipped OFDM Signals in Visible
  Light Communication</title><categories>cs.IT math.IT</categories><journal-ref>EURASIP Journal on Wireless Communications and Networking 2012,
  2012:321</journal-ref><doi>10.1186/1687-1499-2012-321</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Orthogonal frequency division multiplexing (OFDM) has been considered for
visible light communication (VLC) thanks to its ability to boost data rates as
well as its robustness against frequency-selective fading channels. A major
disadvantage of OFDM is the large dynamic range of its time-domain waveforms,
making OFDM vulnerable to nonlinearity of light emitting diodes (LEDs). DC
biased optical OFDM (DCO-OFDM) and asymmetrically clipped optical OFDM
(ACO-OFDM) are two popular OFDM techniques developed for the VLC. In this
paper, we will analyze the performance of the DCO-OFDM and ACO-OFDM signals in
terms of error vector magnitude (EVM), signal-to-distortion ratio (SDR), and
achievable data rates under both average optical power and dynamic optical
power constraints. EVM is a commonly used metric to characterize distortions.
We will describe an approach to numerically calculate the EVM for DCO-OFDM and
ACO-OFDM. We will derive the optimum biasing ratio in the sense of minimizing
EVM for DCO-OFDM. Additionally, we will formulate the EVM minimization problem
as a convex linear optimization problem and obtain an EVM lower bound against
which to compare the DCO-OFDM and ACO-OFDM techniques. We will prove that the
ACO-OFDM can achieve the lower bound. Average optical power and dynamic optical
power are two main constraints in VLC. We will derive the achievable data rates
under these two constraints for both additive white Gaussian noise (AWGN)
channel and frequency-selective channel. We will compare the performance of
DCO-OFDM and ACO-OFDM under different power constraint scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3838</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3838</id><created>2012-06-18</created><authors><author><keyname>Doghri</keyname><forenames>In&#xe8;s</forenames><affiliation>INRIA Grenoble Rh&#xf4;ne-Alpes / LIP Laboratoire de l'Informatique du Parall&#xe9;lisme, LIP</affiliation></author><author><keyname>Reynaud</keyname><forenames>Laurent</forenames><affiliation>INRIA Grenoble Rh&#xf4;ne-Alpes / LIP Laboratoire de l'Informatique du Parall&#xe9;lisme, LIP</affiliation></author><author><keyname>Gu&#xe9;rin-Lassous</keyname><forenames>Isabelle</forenames><affiliation>INRIA Grenoble Rh&#xf4;ne-Alpes / LIP Laboratoire de l'Informatique du Parall&#xe9;lisme, LIP</affiliation></author></authors><title>On The Recovery Performance of Single- and Multipath OLSR in Wireless
  Multi-Hop Networks</title><categories>cs.NI</categories><comments>Third International ICST Conference on Ad Hoc Networks, ADHOCNETS
  2011, Paris : France (2011)</comments><proxy>ccsd</proxy><doi>10.1007/978-3-642-29096-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study and improve the recovery properties of single and
multipath routing strategies when facing network failure situations. In
particular, we focus our study on two MANET routing protocols: OLSR and its
multipath extension MP-OLSR. In various wireless multi-hop network
environments, especially in multiple chain topologies, we define and seek to
evaluate the latency introduced by these protocols to find a new path after a
link failure. Theoretical estimations and simulation results show that, under
dual chain-topologies, this latency can be too long and incompatible with the
needs of loss and delay constrained applications. As the source nodes cannot
detect link failures immediately because of the delay incurred by the
well-known nature of link state protocols in general, and of OLSR Topology
Control (TC) messages in particular, these nodes keep sending packets along
broken paths. We thus study the inconsistencies between the actual network
topology and the nodes' own representation. After analyzing the consequences of
this long latency, we seek to alleviate these problems with the introduction of
adapted mechanisms. We propose three new different schemes and accordingly
extend the original OLSR and MP-OLSR protocols in order to decrease the
expected latency and improve the protocol performance. Simulation results show
a steep decrease of the latency when using these new schemes in dual
chain-topologies. We also discuss these results in terms of packet loss,
end-to-end delay and overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3862</identifier>
 <datestamp>2013-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3862</id><created>2012-06-18</created><updated>2013-09-16</updated><authors><author><keyname>Wang</keyname><forenames>Tao</forenames></author></authors><title>Total coloring of 1-toroidal graphs of maximum degree at least 11 and no
  adjacent triangles</title><categories>math.CO cs.DM</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A {\em total coloring} of a graph $G$ is an assignment of colors to the
vertices and the edges of $G$ such that every pair of adjacent/incident
elements receive distinct colors. The {\em total chromatic number} of a graph
$G$, denoted by $\chiup''(G)$, is the minimum number of colors needed in a
total coloring of $G$. The most well-known Total Coloring Conjecture (TCC) says
that every graph with maximum degree $\Delta$ admits a total coloring with at
most $\Delta + 2$ colors. A graph is {\em 1-toroidal} if it can be drawn on
torus such that every edge crosses at most one other edge. In this paper, we
investigate the total coloring of 1-toroidal graphs, and prove that the TCC
holds for the 1-toroidal graphs with maximum degree at least 11 and some
restrictions on the triangles. Consequently, if $G$ is a 1-toroidal graph with
maximum degree $\Delta$ at least 11 and without adjacent triangles, then $G$
admits a total coloring with at most $\Delta + 2$ colors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3870</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3870</id><created>2012-06-18</created><updated>2013-06-07</updated><authors><author><keyname>Barri&#xe8;re</keyname><forenames>Lali</forenames></author><author><keyname>Huemer</keyname><forenames>Clemens</forenames></author><author><keyname>Mitsche</keyname><forenames>Dieter</forenames></author><author><keyname>Orden</keyname><forenames>David</forenames></author></authors><title>On the Fiedler value of large planar graphs</title><categories>math.CO cs.DM</categories><comments>21 pages, 4 figures, 1 table. Version accepted in Linear Algebra and
  Its Applications</comments><msc-class>05C50</msc-class><doi>10.1016/j.laa.2013.05.032</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Fiedler value $\lambda_2$, also known as algebraic connectivity, is the
second smallest Laplacian eigenvalue of a graph. We study the maximum Fiedler
value among all planar graphs $G$ with $n$ vertices, denoted by
$\lambda_{2\max}$, and we show the bounds $2+\Theta(\frac{1}{n^2}) \leq
\lambda_{2\max} \leq 2+O(\frac{1}{n})$. We also provide bounds on the maximum
Fiedler value for the following classes of planar graphs: Bipartite planar
graphs, bipartite planar graphs with minimum vertex degree~3, and outerplanar
graphs. Furthermore, we derive almost tight bounds on $\lambda_{2\max}$ for two
more classes of graphs, those of bounded genus and $K_h$-minor-free graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3877</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3877</id><created>2012-06-18</created><authors><author><keyname>Kucherov</keyname><forenames>Gregory</forenames></author><author><keyname>T&#xf3;thm&#xe9;r&#xe9;sz</keyname><forenames>Lilla</forenames></author><author><keyname>Vialette</keyname><forenames>St&#xe9;phane</forenames></author></authors><title>On the combinatorics of suffix arrays</title><categories>cs.DS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove several combinatorial properties of suffix arrays, including a
characterization of suffix arrays through a bijection with a certain
well-defined class of permutations. Our approach is based on the
characterization of Burrows-Wheeler arrays given in [1], that we apply by
reducing suffix sorting to cyclic shift sorting through the use of an
additional sentinel symbol. We show that the characterization of suffix arrays
for a special case of binary alphabet given in [2] easily follows from our
characterization. Based on our results, we also provide simple proofs for the
enumeration results for suffix arrays, obtained in [3]. Our approach to
characterizing suffix arrays is the first that exploits their relationship with
Burrows-Wheeler permutations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3880</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3880</id><created>2012-06-18</created><authors><author><keyname>Nabeel</keyname><forenames>M.</forenames></author><author><keyname>Zage</keyname><forenames>J.</forenames></author><author><keyname>Kerr</keyname><forenames>S.</forenames></author><author><keyname>Bertino</keyname><forenames>E.</forenames></author><author><keyname>Kulatunga</keyname><forenames>N. Athula.</forenames></author><author><keyname>Navaratne</keyname><forenames>U. Sudheera</forenames></author><author><keyname>Duren</keyname><forenames>M.</forenames></author></authors><title>Cryptographic Key Management for Smart Power Grids - Approaches and
  Issues</title><categories>cs.CR cs.SY</categories><report-no>CERIAS TR 2012-02 CERIAS TR 2012-02 CERIAS TR 2012-02</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The smart power grid promises to improve efficiency and reliability of power
delivery. This report introduces the logical components, associated
technologies, security protocols, and network designs of the system.
Undermining the potential benefits are security threats, and those threats
related to cyber security are described in this report. Concentrating on the
design of the smart meter and its communication links, this report describes
the ZigBee technology and implementation, and the communication between the
smart meter and the collector node, with emphasis on security attributes. It
was observed that many of the secure features are based on keys that must be
maintained; therefore, secure key management techniques become the basis to
securing the entire grid. The descriptions of current key management techniques
are delineated, highlighting their weaknesses. Finally some initial research
directions are outlined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3881</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3881</id><created>2012-06-18</created><authors><author><keyname>Ceruti</keyname><forenames>Claudio</forenames></author><author><keyname>Bassis</keyname><forenames>Simone</forenames></author><author><keyname>Rozza</keyname><forenames>Alessandro</forenames></author><author><keyname>Lombardi</keyname><forenames>Gabriele</forenames></author><author><keyname>Casiraghi</keyname><forenames>Elena</forenames></author><author><keyname>Campadelli</keyname><forenames>Paola</forenames></author></authors><title>DANCo: Dimensionality from Angle and Norm Concentration</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last decades the estimation of the intrinsic dimensionality of a
dataset has gained considerable importance. Despite the great deal of research
work devoted to this task, most of the proposed solutions prove to be
unreliable when the intrinsic dimensionality of the input dataset is high and
the manifold where the points lie is nonlinearly embedded in a higher
dimensional space. In this paper we propose a novel robust intrinsic
dimensionality estimator that exploits the twofold complementary information
conveyed both by the normalized nearest neighbor distances and by the angles
computed on couples of neighboring points, providing also closed-forms for the
Kullback-Leibler divergences of the respective distributions. Experiments
performed on both synthetic and real datasets highlight the robustness and the
effectiveness of the proposed algorithm when compared to state of the art
methodologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3883</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3883</id><created>2012-06-18</created><authors><author><keyname>Metodi</keyname><forenames>Amit</forenames></author><author><keyname>Codish</keyname><forenames>Michael</forenames></author></authors><title>Compiling Finite Domain Constraints to SAT with BEE</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present BEE, a compiler which enables to encode finite domain constraint
problems to CNF. Using BEE both eases the encoding process for the user and
also performs transformations to simplify constraints and optimize their
encoding to CNF. These optimizations are based primarily on equi-propagation
and on partial evaluation, and also on the idea that a given constraint may
have various possible CNF encodings. Often, the better encoding choice is made
after constraint simplification. BEE is written in Prolog and integrates
directly with a SAT solver through a suitable Prolog interface. We demonstrate
that constraint simplification is often highly beneficial when solving hard
finite domain constraint problems. A BEE implementation is available with this
paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3891</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3891</id><created>2012-06-18</created><authors><author><keyname>Jain</keyname><forenames>Deeksha</forenames></author><author><keyname>Krishna</keyname><forenames>P. Venkata</forenames></author><author><keyname>Saritha</keyname><forenames>V.</forenames></author></authors><title>A Study on Internet of Things based Applications</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper gives a detail analysis of various applications based on Internet
of Thing (IoT)s. This explains about how internet of things evolved from mobile
computing and ubiquitous computing. It emphasises the fact that objects are
connected over the internet rather than people. The properties of Internet of
Things (IOT) are product information, electronic tag, standard expressed and
uploading information. It utilises the Radio Frequency Identification (RFID)
technology and wireless sensor networks (WSN). IOT applications are used in
domains such as healthcare, supply chain management, defence and agriculture.
Lastly the paper focuses on issues involved in IOT. Though it is a boon, IOT
faces certain crucial issues like privacy and security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3897</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3897</id><created>2012-06-18</created><updated>2013-03-17</updated><authors><author><keyname>Dong</keyname><forenames>Daoyi</forenames></author><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author><author><keyname>Rabitz</keyname><forenames>Herschel</forenames></author></authors><title>Sampled-data design for robust control of a single qubit</title><categories>quant-ph cs.SY</categories><comments>33 pages, 5 figures, minor corrections</comments><journal-ref>IEEE Transactions on Automatic Control, vol. 58, pp.2054-2059,
  2013</journal-ref><doi>10.1109/TAC.2013.2256017</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a sampled-data approach for the robust control of a
single qubit (quantum bit). The required robustness is defined using a sliding
mode domain and the control law is designed offline and then utilized online
with a single qubit having bounded uncertainties. Two classes of uncertainties
are considered involving the system Hamiltonian and the coupling strength of
the system-environment interaction. Four cases are analyzed in detail including
without decoherence, with amplitude damping decoherence, phase damping
decoherence and depolarizing decoherence. Sampling periods are specifically
designed for these cases to guarantee the required robustness. Two sufficient
conditions are presented for guiding the design of unitary control for the
cases without decoherence and with amplitude damping decoherence. The proposed
approach has potential applications in quantum error-correction and in
constructing robust quantum gates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3902</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3902</id><created>2012-06-18</created><updated>2012-07-05</updated><authors><author><keyname>Chen</keyname><forenames>Hubie</forenames></author></authors><title>On the Complexity of Existential Positive Queries</title><categories>cs.LO cs.AI cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We systematically investigate the complexity of model checking the
existential positive fragment of first-order logic. In particular, for a set of
existential positive sentences, we consider model checking where the sentence
is restricted to fall into the set; a natural question is then to classify
which sentence sets are tractable and which are intractable. With respect to
fixed-parameter tractability, we give a general theorem that reduces this
classification question to the corresponding question for primitive positive
logic, for a variety of representations of structures. This general theorem
allows us to deduce that an existential positive sentence set having bounded
arity is fixed-parameter tractable if and only if each sentence is equivalent
to one in bounded-variable logic. We then use the lens of classical complexity
to study these fixed-parameter tractable sentence sets. We show that such a set
can be NP-complete, and consider the length needed by a translation from
sentences in such a set to bounded-variable logic; we prove superpolynomial
lower bounds on this length using the theory of compilability, obtaining an
interesting type of formula size lower bound. Overall, the tools, concepts, and
results of this article set the stage for the future consideration of the
complexity of model checking on more expressive logics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3924</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3924</id><created>2012-06-14</created><updated>2012-08-13</updated><authors><author><keyname>Blattner</keyname><forenames>Marcel</forenames></author><author><keyname>Medo</keyname><forenames>Matus</forenames></author></authors><title>Recommendation systems in the scope of opinion formation: a model</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>8 pages, 8 figures</comments><acm-class>H.1.m</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Aggregated data in real world recommender applications often feature
fat-tailed distributions of the number of times individual items have been
rated or favored. We propose a model to simulate such data. The model is mainly
based on social interactions and opinion formation taking place on a complex
network with a given topology. A threshold mechanism is used to govern the
decision making process that determines whether a user is or is not interested
in an item. We demonstrate the validity of the model by fitting attendance
distributions from different real data sets. The model is mathematically
analyzed by investigating its master equation. Our approach provides an attempt
to understand recommender system's data as a social process. The model can
serve as a starting point to generate artificial data sets useful for testing
and evaluating recommender systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3933</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3933</id><created>2012-06-18</created><updated>2013-04-04</updated><authors><author><keyname>&#xc9;rdi</keyname><forenames>P&#xe9;ter</forenames></author><author><keyname>Makovi</keyname><forenames>Kinga</forenames></author><author><keyname>Somogyv&#xe1;ri</keyname><forenames>Zolt&#xe1;n</forenames></author><author><keyname>Strandburg</keyname><forenames>Katherine</forenames></author><author><keyname>Tobochnik</keyname><forenames>Jan</forenames></author><author><keyname>Volf</keyname><forenames>P&#xe9;ter</forenames></author><author><keyname>Zal&#xe1;nyi</keyname><forenames>L&#xe1;szl&#xf3;</forenames></author></authors><title>Prediction of Emerging Technologies Based on Analysis of the U.S. Patent
  Citation Network</title><categories>cs.SI physics.soc-ph</categories><journal-ref>Scientometrics: Volume 95, Issue 1 (2013), Page 225-242</journal-ref><doi>10.1007/s11192-012-0796-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The network of patents connected by citations is an evolving graph, which
provides a representation of the innovation process. A patent citing another
implies that the cited patent reflects a piece of previously existing knowledge
that the citing patent builds upon. A methodology presented here (i) identifies
actual clusters of patents: i.e. technological branches, and (ii) gives
predictions about the temporal changes of the structure of the clusters. A
predictor, called the {citation vector}, is defined for characterizing
technological development to show how a patent cited by other patents belongs
to various industrial fields. The clustering technique adopted is able to
detect the new emerging recombinations, and predicts emerging new technology
clusters. The predictive ability of our new method is illustrated on the
example of USPTO subcategory 11, Agriculture, Food, Textiles. A cluster of
patents is determined based on citation data up to 1991, which shows
significant overlap of the class 442 formed at the beginning of 1997. These new
tools of predictive analytics could support policy decision making processes in
science and technology, and help formulate recommendations for action.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3953</identifier>
 <datestamp>2012-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3953</id><created>2012-06-18</created><authors><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author><author><keyname>M&#xe9;zard</keyname><forenames>Marc</forenames></author><author><keyname>Sausset</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Sun</keyname><forenames>Yifan</forenames></author><author><keyname>Zdeborov&#xe1;</keyname><forenames>Lenka</forenames></author></authors><title>Probabilistic Reconstruction in Compressed Sensing: Algorithms, Phase
  Diagrams, and Threshold Achieving Matrices</title><categories>cond-mat.stat-mech cs.IT math.IT</categories><comments>42 pages, 37 figures, 3 appendixes</comments><journal-ref>J. Stat. Mech. (2012) P08009</journal-ref><doi>10.1088/1742-5468/2012/08/P08009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed sensing is a signal processing method that acquires data directly
in a compressed form. This allows one to make less measurements than what was
considered necessary to record a signal, enabling faster or more precise
measurement protocols in a wide range of applications. Using an
interdisciplinary approach, we have recently proposed in [arXiv:1109.4424] a
strategy that allows compressed sensing to be performed at acquisition rates
approaching to the theoretical optimal limits. In this paper, we give a more
thorough presentation of our approach, and introduce many new results. We
present the probabilistic approach to reconstruction and discuss its optimality
and robustness. We detail the derivation of the message passing algorithm for
reconstruction and expectation max- imization learning of signal-model
parameters. We further develop the asymptotic analysis of the corresponding
phase diagrams with and without measurement noise, for different distribution
of signals, and discuss the best possible reconstruction performances
regardless of the algorithm. We also present new efficient seeding matrices,
test them on synthetic data and analyze their performance asymptotically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3959</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3959</id><created>2012-06-13</created><updated>2014-08-28</updated><authors><author><keyname>Bilmes</keyname><forenames>Jeff</forenames></author><author><keyname>Ng</keyname><forenames>Andrew</forenames></author></authors><title>Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial
  Intelligence (2009)</title><categories>cs.AI</categories><proxy>Martijn de Jongh</proxy><report-no>UAI2009</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the Proceedings of the Twenty-Fifth Conference on Uncertainty in
Artificial Intelligence, which was held in Montreal, QC, Canada, June 18 - 21
2009.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3963</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3963</id><created>2012-06-18</created><authors><author><keyname>Hlinka</keyname><forenames>Jaroslav</forenames></author><author><keyname>Hartman</keyname><forenames>David</forenames></author><author><keyname>Palu&#x161;</keyname><forenames>Milan</forenames></author></authors><title>Small-world topology of functional connectivity in randomly connected
  dynamical systems</title><categories>cs.SI physics.data-an physics.soc-ph q-bio.NC stat.AP</categories><comments>The following article has been submitted to Chaos: An
  interdisciplinary journal of nonlinear science. After it is published, it
  will be found at http://chaos.aip.org/</comments><doi>10.1063/1.4732541</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Characterization of real-world complex systems increasingly involves the
study of their topological structure using graph theory. Among global network
properties, small-world property, consisting in existence of relatively short
paths together with high clustering of the network, is one of the most
discussed and studied. When dealing with coupled dynamical systems, links among
units of the system are commonly quantified by a measure of pairwise
statistical dependence of observed time series (functional connectivity). We
argue that the functional connectivity approach leads to upwardly biased
estimates of small-world characteristics (with respect to commonly used random
graph models) due to partial transitivity of the accepted functional
connectivity measures such as the correlation coefficient. In particular, this
may lead to observation of small-world characteristics in connectivity graphs
estimated from generic randomly connected dynamical systems. The ubiquity and
robustness of the phenomenon is documented by an extensive parameter study of
its manifestation in a multivariate linear autoregressive process, with
discussion of the potential relevance for nonlinear processes and measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3965</identifier>
 <datestamp>2013-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3965</id><created>2012-06-18</created><authors><author><keyname>Lopez-Martinez</keyname><forenames>F. J.</forenames></author><author><keyname>Morales-Jimenez</keyname><forenames>D.</forenames></author><author><keyname>Martos-Naya</keyname><forenames>E.</forenames></author><author><keyname>Paris</keyname><forenames>J. F.</forenames></author></authors><title>On the Bivariate Nakagami-$m$ Cumulative Distribution Function:
  Closed-form Expression and Applications</title><categories>cs.IT math.IT</categories><comments>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</comments><journal-ref>IEEE Transactions on Communications, vol.61, no.4, pp.1404,1414,
  April 2013</journal-ref><doi>10.1109/TCOMM.2013.020412.120413</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we derive exact closed-form expressions for the bivariate
Nakagami-$m$ cumulative distribution function (CDF) with positive integer
fading severity index $m$ in terms of a class of hypergeometric functions.
Particularly, we show that the bivariate Nakagami-$m$ CDF can be expressed as a
finite sum of elementary functions and bivariate confluent hypergeometric
$\Phi_3$ functions. Direct applications which arise from the proposed
closed-form expression are the outage probability (OP) analysis of a
dual-branch selection combiner in correlated Nakagami-$m$ fading, or the
calculation of the level crossing rate (LCR) and average fade duration (AFD) of
a sampled Nakagami-$m$ fading envelope.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3975</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3975</id><created>2012-06-18</created><authors><author><keyname>Birkeland</keyname><forenames>&#xc5;smund</forenames></author><author><keyname>Solteszova</keyname><forenames>Veronika</forenames></author><author><keyname>H&#xf6;nigmann</keyname><forenames>Dieter</forenames></author><author><keyname>Gilja</keyname><forenames>Odd Helge</forenames></author><author><keyname>Brekke</keyname><forenames>Svein</forenames></author><author><keyname>Ropinski</keyname><forenames>Timo</forenames></author><author><keyname>Viola</keyname><forenames>Ivan</forenames></author></authors><title>The Ultrasound Visualization Pipeline - A Survey</title><categories>cs.GR cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ultrasound is one of the most frequently used imaging modality in medicine.
The high spatial resolution, its interactive nature and non-invasiveness makes
it the first choice in many examinations. Image interpretation is one of
ultrasound's main challenges. Much training is required to obtain a confident
skill level in ultrasound-based diagnostics. State-of-the-art graphics
techniques is needed to provide meaningful visualizations of ultrasound in
real-time. In this paper we present the process-pipeline for ultrasound
visualization, including an overview of the tasks performed in the specific
steps. To provide an insight into the trends of ultrasound visualization
research, we have selected a set of significant publications and divided them
into a technique-based taxonomy covering the topics pre-processing,
segmentation, registration, rendering and augmented reality. For the different
technique types we discuss the difference between ultrasound-based techniques
and techniques for other modalities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3980</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3980</id><created>2012-06-18</created><authors><author><keyname>Gansner</keyname><forenames>Emden</forenames></author><author><keyname>Hu</keyname><forenames>Yifan</forenames></author><author><keyname>North</keyname><forenames>Stephen</forenames></author></authors><title>Visualizing Streaming Text Data with Dynamic Maps</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The many endless rivers of text now available present a serious challenge in
the task of gleaning, analyzing and discovering useful information. In this
paper, we describe a methodology for visualizing text streams in real time. The
approach automatically groups similar messages into &quot;countries,&quot; with keyword
summaries, using semantic analysis, graph clustering and map generation
techniques. It handles the need for visual stability across time by dynamic
graph layout and Procrustes projection techniques, enhanced with a novel stable
component packing algorithm. The result provides a continuous, succinct view of
evolving topics of interest. It can be used in passive mode for overviews and
situational awareness, or as an interactive data exploration tool. To make
these ideas concrete, we describe their application to an online service called
TwitterScope.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3988</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3988</id><created>2012-06-18</created><authors><author><keyname>Ananthasubramaniam</keyname><forenames>Bharath</forenames></author><author><keyname>Madhow</keyname><forenames>Upamanyu</forenames></author></authors><title>Cooperative localization using angle of arrival measurements: sequential
  algorithms and non-line-of-sight suppression</title><categories>cs.NI cs.MA</categories><comments>31 pages, 11 figures, related to MELT'08 Workshop proceeding</comments><acm-class>C.3; C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate localization of a source based on angle of arrival (AoA)
measurements made at a geographically dispersed network of cooperating
receivers. The goal is to efficiently compute accurate estimates despite
outliers in the AoA measurements due to multipath reflections in
non-line-of-sight (NLOS) environments. Maximal likelihood (ML) location
estimation in such a setting requires exhaustive testing of estimates from all
possible subsets of &quot;good&quot; measurements, which has exponential complexity in
the number of measurements. We provide a randomized algorithm that approaches
ML performance with linear complexity in the number of measurements. The
building block for this algorithm is a low-complexity sequential algorithm for
updating the source location estimates under line-of-sight (LOS) environments.
Our Bayesian framework can exploit the ability to resolve multiple paths in
wideband systems to provide significant performance gains over narrowband
systems in NLOS environments, and easily extends to accommodate additional
information such as range measurements and prior information about location.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3992</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3992</id><created>2012-06-18</created><updated>2013-10-13</updated><authors><author><keyname>Havemann</keyname><forenames>Frank</forenames></author><author><keyname>Gl&#xe4;ser</keyname><forenames>Jochen</forenames></author><author><keyname>Heinz</keyname><forenames>Michael</forenames></author><author><keyname>Struck</keyname><forenames>Alexander</forenames></author></authors><title>Evaluating Overlapping Communities with the Conductance of their
  Boundary Nodes</title><categories>cs.SI physics.soc-ph</categories><comments>9 pages, 7 figures, corrected version, two sections and some
  sentences deleted, footnote 7 (p. 7) added</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Usually the boundary of a community in a network is drawn between nodes and
thus crosses its outgoing links. If we construct overlapping communities by
applying the link-clustering approach nodes and links interchange their roles.
Therefore, boundaries must drawn through the nodes shared by two or more
communities. For the purpose of community evaluation we define a conductance of
boundary nodes of overlapping communities analogously to the graph conductance
of boundary-crossing links used to partition a graph into disjoint communities.
We show that conductance of boundary nodes (or normalised node cut) can be
deduced from ordinary graph conductance of disjoint clusters in the network's
weighted line graph introduced by Evans and Lambiotte (2009) to get overlapping
communities of nodes in the original network. We test whether our definition
can be used to construct meaningful overlapping communities with a local greedy
algorithm of link clustering. In this note we present encouraging results we
obtained for Zachary's karate-club network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.3999</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.3999</id><created>2012-06-18</created><authors><author><keyname>Bentz</keyname><forenames>C&#xe9;dric</forenames></author></authors><title>A polynomial-time algorithm for planar multicuts with few source-sink
  pairs</title><categories>cs.DM cs.DS</categories><comments>11 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an edge-weighted undirected graph and a list of k source-sink pairs of
vertices, the well-known minimum multicut problem consists in selecting a
minimum-weight set of edges whose removal leaves no path between every source
and its corresponding sink. We give the first polynomial-time algorithm to
solve this problem in planar graphs, when k is fixed. Previously, this problem
was known to remain NP-hard in general graphs with fixed k, and in trees with
arbitrary k; the most noticeable tractable case known so far was in planar
graphs with fixed k and sources and sinks lying on the outer face.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4020</identifier>
 <datestamp>2013-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4020</id><created>2012-06-18</created><updated>2013-07-19</updated><authors><author><keyname>Heged&#xfc;s</keyname><forenames>G&#xe1;bor</forenames></author><author><keyname>Schicho</keyname><forenames>Josef</forenames></author><author><keyname>Schr&#xf6;cker</keyname><forenames>Hans-Peter</forenames></author></authors><title>The Theory of Bonds: A New Method for the Analysis of Linkages</title><categories>math.AG cs.RO math.MG math.RA</categories><comments>more detailed explanations and additional references</comments><msc-class>70B15, 51J15, 14H50</msc-class><journal-ref>Mech. Machine Theory, 70, 407-424 (2013)</journal-ref><doi>10.1016/j.mechmachtheory.2013.08.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce a new technique, based on dual quaternions, for
the analysis of closed linkages with revolute joints: the theory of bonds. The
bond structure comprises a lot of information on closed revolute chains with a
one-parametric mobility. We demonstrate the usefulness of bond theory by giving
a new and transparent proof for the well-known classification of
overconstrained 5R linkages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4042</identifier>
 <datestamp>2012-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4042</id><created>2012-06-17</created><authors><author><keyname>Wang</keyname><forenames>Junyan</forenames></author><author><keyname>Chan</keyname><forenames>Kap Luk</forenames></author></authors><title>The Stability of Convergence of Curve Evolutions in Vector Fields</title><categories>cs.CV math.AP</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Curve evolution is often used to solve computer vision problems. If the curve
evolution fails to converge, we would not be able to solve the targeted problem
in a lifetime. This paper studies the theoretical aspect of the convergence of
a type of general curve evolutions. We establish a theory for analyzing and
improving the stability of the convergence of the general curve evolutions.
Based on this theory, we ascertain that the convergence of a known curve
evolution is marginal stable. We propose a way of modifying the original curve
evolution equation to improve the stability of the convergence according to our
theory. Numerical experiments show that the modification improves the
convergence of the curve evolution, which validates our theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4074</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4074</id><created>2012-06-18</created><updated>2013-06-12</updated><authors><author><keyname>Li</keyname><forenames>Fuxin</forenames></author><author><keyname>Lebanon</keyname><forenames>Guy</forenames></author><author><keyname>Sminchisescu</keyname><forenames>Cristian</forenames></author></authors><title>A Linear Approximation to the chi^2 Kernel with Geometric Convergence</title><categories>cs.LG cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new analytical approximation to the $\chi^2$ kernel that
converges geometrically. The analytical approximation is derived with
elementary methods and adapts to the input distribution for optimal convergence
rate. Experiments show the new approximation leads to improved performance in
image classification and semantic segmentation tasks using a random Fourier
feature approximation of the $\exp-\chi^2$ kernel. Besides, out-of-core
principal component analysis (PCA) methods are introduced to reduce the
dimensionality of the approximation and achieve better performance at the
expense of only an additional constant factor to the time complexity. Moreover,
when PCA is performed jointly on the training and unlabeled testing data,
further performance improvements can be obtained. Experiments conducted on the
PASCAL VOC 2010 segmentation and the ImageNet ILSVRC 2010 datasets show
statistically significant improvements over alternative approximation methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4081</identifier>
 <datestamp>2015-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4081</id><created>2012-06-18</created><updated>2015-01-14</updated><authors><author><keyname>Cattan&#xe9;o</keyname><forenames>David</forenames></author><author><keyname>Perdrix</keyname><forenames>Simon</forenames></author></authors><title>Parametrized Complexity of Weak Odd Domination Problems</title><categories>cs.CC quant-ph</categories><comments>16 pages, 5 figures</comments><journal-ref>Fundamentals of Computation Theory (FCT'13), LNCS vol 8070 pp
  107-120, 2013</journal-ref><doi>10.1007/978-3-642-40164-0_13</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a graph $G=(V,E)$, a subset $B\subseteq V$ of vertices is a weak odd
dominated (WOD) set if there exists $D \subseteq V {\setminus} B$ such that
every vertex in $B$ has an odd number of neighbours in $D$. $\kappa(G)$ denotes
the size of the largest WOD set, and $\kappa'(G)$ the size of the smallest
non-WOD set. The maximum of $\kappa(G)$ and $|V|-\kappa'(G)$, denoted
$\kappa_Q(G)$, plays a crucial role in quantum cryptography. In particular
deciding, given a graph $G$ and $k&gt;0$, whether $\kappa_Q(G)\le k$ is of
practical interest in the design of graph-based quantum secret sharing schemes.
The decision problems associated with the quantities $\kappa$, $\kappa'$ and
$\kappa_Q$ are known to be NP-Complete. In this paper, we consider the
approximation of these quantities and the parameterized complexity of the
corresponding problems. We mainly prove the fixed-parameter intractability
(W$[1]$-hardness) of these problems. Regarding the approximation, we show that
$\kappa_Q$, $\kappa$ and $\kappa'$ admit a constant factor approximation
algorithm, and that $\kappa$ and $\kappa'$ have no polynomial approximation
scheme unless P=NP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4094</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4094</id><created>2012-06-18</created><updated>2012-08-30</updated><authors><author><keyname>Ochab</keyname><forenames>J. K.</forenames></author></authors><title>Maximal-entropy random walk unifies centrality measures</title><categories>physics.soc-ph cond-mat.dis-nn cond-mat.stat-mech cs.SI</categories><comments>7 pages, 2 figures</comments><journal-ref>Phys. Rev. E 86, 066109 (2012)</journal-ref><doi>10.1103/PhysRevE.86.066109</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper analogies between different (dis)similarity matrices are
derived. These matrices, which are connected to path enumeration and random
walks, are used in community detection methods or in computation of centrality
measures for complex networks. The focus is on a number of known centrality
measures, which inherit the connections established for similarity matrices.
These measures are based on the principal eigenvector of the adjacency matrix,
path enumeration, as well as on the stationary state, stochastic matrix or mean
first-passage times of a random walk. Particular attention is paid to the
maximal-entropy random walk, which serves as a very distinct alternative to the
ordinary random walk used in network analysis.
  The various importance measures, defined both with the use of ordinary random
walk and the maximal-entropy random walk, are compared numerically on a set of
benchmark graphs. It is shown that groups of centrality measures defined with
the two random walks cluster into two separate families. In particular, the
group of centralities for the maximal-entropy random walk, connected to the
eigenvector centrality and path enumeration, is strongly distinct from all the
other measures and produces largely equivalent results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4110</identifier>
 <datestamp>2012-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4110</id><created>2012-06-18</created><authors><author><keyname>Tran</keyname><forenames>Truyen T.</forenames></author><author><keyname>Pham</keyname><forenames>Duc Son</forenames></author></authors><title>ConeRANK: Ranking as Learning Generalized Inequalities</title><categories>cs.LG cs.IR</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We propose a new data mining approach in ranking documents based on the
concept of cone-based generalized inequalities between vectors. A partial
ordering between two vectors is made with respect to a proper cone and thus
learning the preferences is formulated as learning proper cones. A pairwise
learning-to-rank algorithm (ConeRank) is proposed to learn a non-negative
subspace, formulated as a polyhedral cone, over document-pair differences. The
algorithm is regularized by controlling the `volume' of the cone. The
experimental studies on the latest and largest ranking dataset LETOR 4.0 shows
that ConeRank is competitive against other recent ranking approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4116</identifier>
 <datestamp>2012-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4116</id><created>2012-06-18</created><authors><author><keyname>Yamada</keyname><forenames>Makoto</forenames></author><author><keyname>Sigal</keyname><forenames>Leonid</forenames></author><author><keyname>Raptis</keyname><forenames>Michalis</forenames></author><author><keyname>Sugiyama</keyname><forenames>Masashi</forenames></author></authors><title>Dependence Maximizing Temporal Alignment via Squared-Loss Mutual
  Information</title><categories>stat.ML cs.AI</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of temporal alignment is to establish time correspondence between
two sequences, which has many applications in a variety of areas such as speech
processing, bioinformatics, computer vision, and computer graphics. In this
paper, we propose a novel temporal alignment method called least-squares
dynamic time warping (LSDTW). LSDTW finds an alignment that maximizes
statistical dependency between sequences, measured by a squared-loss variant of
mutual information. The benefit of this novel information-theoretic formulation
is that LSDTW can align sequences with different lengths, different
dimensionality, high non-linearity, and non-Gaussianity in a computationally
efficient manner. In addition, model parameters such as an initial alignment
matrix can be systematically optimized by cross-validation. We demonstrate the
usefulness of LSDTW through experiments on synthetic and real-world Kinect
action recognition datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4120</identifier>
 <datestamp>2012-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4120</id><created>2012-06-19</created><authors><author><keyname>Jirapanthong</keyname><forenames>Waraporn</forenames></author></authors><title>Experience on Re-engineering Applying with Software Product Line</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present our experience based on a reengineering project.
The software project is to re-engineer the original system of a company to
answer the new requirements and changed business functions. Reengineering is a
process that involves not only the software system, but also underlying
business model. Particularly, the new business model is designed along with new
technologies to support the new system. This paper presents our experience that
applies with software product line approach to develop the new system
supporting original business functions and new ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4121</identifier>
 <datestamp>2012-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4121</id><created>2012-06-19</created><updated>2012-09-18</updated><authors><author><keyname>Wilde</keyname><forenames>Mark M.</forenames></author><author><keyname>Hayden</keyname><forenames>Patrick</forenames></author><author><keyname>Buscemi</keyname><forenames>Francesco</forenames></author><author><keyname>Hsieh</keyname><forenames>Min-Hsiu</forenames></author></authors><title>The information-theoretic costs of simulating quantum measurements</title><categories>quant-ph cs.IT math.IT</categories><comments>77 pages, 7 figures; v2, minor changes to clarify notation</comments><journal-ref>Journal of Physics A: Mathematical and Theoretical vol. 45, no.
  45, p. 453001 (2012)</journal-ref><doi>10.1088/1751-8113/45/45/453001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Winter's measurement compression theorem stands as one of the most
penetrating insights of quantum information theory (QIT). In addition to making
an original and profound statement about measurement in quantum theory, it also
underlies several other general protocols in QIT. In this paper, we provide a
full review of Winter's measurement compression theorem, detailing the
information processing task, giving examples for understanding it, reviewing
Winter's achievability proof, and detailing a new approach to its single-letter
converse theorem. We prove an extension of the theorem to the case in which the
sender is not required to receive the outcomes of the simulated measurement.
The total cost of common randomness and classical communication can be lower
for such a &quot;non-feedback&quot; simulation, and we prove a single-letter converse
theorem demonstrating optimality. We then review the Devetak-Winter theorem on
classical data compression with quantum side information, providing new proofs
of its achievability and converse parts. From there, we outline a new protocol
that we call &quot;measurement compression with quantum side information,&quot; announced
previously by two of us in our work on triple trade-offs in quantum Shannon
theory. This protocol has several applications, including its part in the
&quot;classically-assisted state redistribution&quot; protocol, which is the most general
protocol on the static side of the quantum information theory tree, and its
role in reducing the classical communication cost in a task known as local
purity distillation. We also outline a connection between measurement
compression with quantum side information and recent work on entropic
uncertainty relations in the presence of quantum memory. Finally, we prove a
single-letter theorem characterizing measurement compression with quantum side
information when the sender is not required to obtain the measurement outcome.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4123</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4123</id><created>2012-06-19</created><updated>2013-03-12</updated><authors><author><keyname>Li</keyname><forenames>Mingqiang</forenames></author></authors><title>On the Confidentiality of Information Dispersal Algorithms and Their
  Erasure Codes</title><categories>cs.IT cs.CR cs.DC math.IT</categories><comments>This version includes a correction on the example in Section IV</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  \emph{Information Dispersal Algorithms (IDAs)} have been widely applied to
reliable and secure storage and transmission of data files in distributed
systems. An IDA is a method that encodes a file $F$ of size $L=|F|$ into $n$
unrecognizable pieces $F_1$, $F_2$, ..., $F_n$, each of size $L/m$ ($m&lt;n$), so
that the original file $F$ can be reconstructed from any $m$ pieces. The core
of an IDA is the adopted non-systematic $m$-of-$n$ erasure code. This paper
makes a systematic study on the \emph{confidentiality} of an IDA and its
connection with the adopted erasure code. Two levels of confidentiality are
defined: \emph{weak confidentiality} (in the case where some parts of the
original file $F$ can be reconstructed explicitly from fewer than $m$ pieces)
and \emph{strong confidentiality} (in the case where nothing of the original
file $F$ can be reconstructed explicitly from fewer than $m$ pieces). For an
IDA that adopts an arbitrary non-systematic erasure code, its confidentiality
may fall into weak confidentiality. To achieve strong confidentiality, this
paper explores a sufficient and feasible condition on the adopted erasure code.
Then, this paper shows that Rabin's IDA has strong confidentiality. At the same
time, this paper presents an effective way to construct an IDA with strong
confidentiality from an arbitrary $m$-of-$(m+n)$ erasure code. Then, as an
example, this paper constructs an IDA with strong confidentiality from a
Reed-Solomon code, the computation complexity of which is comparable to or
sometimes even lower than that of Rabin's IDA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4124</identifier>
 <datestamp>2012-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4124</id><created>2012-06-19</created><authors><author><keyname>Kayarkar</keyname><forenames>Harshavardhan</forenames></author></authors><title>Classification of Various Security Techniques in Databases and their
  Comparative Analysis</title><categories>cs.CR</categories><comments>8 pages, 1 table</comments><journal-ref>ACTA Technica Corviniensis, Vol. 5, Issue 2, April-June 2012, pp.
  135-138</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data security is one of the most crucial and a major challenge in the digital
world. Security, privacy and integrity of data are demanded in every operation
performed on internet. Whenever security of data is discussed, it is mostly in
the context of secure transfer of data over the unreliable communication
networks. But the security of the data in databases is also as important. In
this paper we will be presenting some of the common security techniques for the
data that can be implemented in fortifying and strengthening the databases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4126</identifier>
 <datestamp>2012-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4126</id><created>2012-06-19</created><authors><author><keyname>Khan</keyname><forenames>Yousuf Ibrahim</forenames></author></authors><title>Image based Cryptography from a distance</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An information is a message which is received and understood. Information can
be sent one person to another over a long range but the process of sending
information must be done in a secure way especially in case of a private
message. Mathematicians and Engineers have historically relied on different
algorithmic techniques to secure messages and signals. Cryptography, to most
people, is concerned with keeping communications private. Indeed, the
protection of sensitive communications has been the emphasis of cryptography
throughout much of its history. Sometimes it is safer to send a message using
an image and thus cryptography can also be done using images during an
emergency. The need to extract information from images and interpret their
contents has been one of the driving factors in the development of image
processing and cryptography during the past decades. In this paper, a simple
cryptographic method was used to decode a message which was in an image and it
was done using a popular computational software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4155</identifier>
 <datestamp>2012-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4155</id><created>2012-06-19</created><authors><author><keyname>Borgohain</keyname><forenames>Rajdeep</forenames></author></authors><title>Data Hiding Techniques using number decompositions</title><categories>cs.CR</categories><comments>5 pages, 1 figure, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data hiding is the art of embedding data into digital media in a way such
that the existence of data remains concealed from everyone except the intended
recipient. In this paper, we discuss the various Least Significant Bit (LSB)
data hiding techniques. We first look at the classical LSB data hiding
technique and the method to embed secret data into cover media by bit
manipulation. We also take a look at the data hiding technique by bit plane
decomposition based on Fibonacci numbers. This method generates more bit planes
which allows users to embed more data into the cover image without causing
significant distortion. We also discuss the data hiding technique based on bit
plane decomposition by prime numbers and natural numbers. These methods are
based on mapping the sequence of image bit size to the decomposed bit number to
hide the intended information. Finally we present a comparative analysis of
these data hiding techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4164</identifier>
 <datestamp>2012-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4164</id><created>2012-06-19</created><authors><author><keyname>Razenshteyn</keyname><forenames>Ilya</forenames></author></authors><title>On Epsilon-Nets, Distance Oracles, and Metric Embeddings</title><categories>cs.DS math.CO math.MG</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give two new applications of an observation from \cite{ADFGW11}. The first
is an almost linear sized constant time data structure for reporting very large
distances in undirected graphs. The second is a generic transformation of
results about $\ell_1$-embeddability of metrics to a setting, where we are
interested in preservation of large distances only.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4166</identifier>
 <datestamp>2012-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4166</id><created>2012-06-19</created><authors><author><keyname>Latapy</keyname><forenames>Matthieu</forenames></author><author><keyname>Magnien</keyname><forenames>Cl&#xe9;mence</forenames></author><author><keyname>Fournier</keyname><forenames>Rapha&#xeb;l</forenames></author></authors><title>Quantifying Paedophile Activity in a Large P2P System</title><categories>cs.CY cs.NI</categories><comments>In press Information Processing and Management, 2012</comments><doi>10.1016/j.ipm.2012.02.008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Increasing knowledge of paedophile activity in P2P systems is a crucial
societal concern, with important consequences on child protection, policy
making, and internet regulation. Because of a lack of traces of P2P exchanges
and rigorous analysis methodology, however, current knowledge of this activity
remains very limited. We consider here a widely used P2P system, eDonkey, and
focus on two key statistics: the fraction of paedophile queries entered in the
system and the fraction of users who entered such queries. We collect hundreds
of millions of keyword-based queries; we design a paedophile query detection
tool for which we establish false positive and false negative rates using
assessment by experts; with this tool and these rates, we then estimate the
fraction of paedophile queries in our data; finally, we design and apply
methods for quantifying users who entered such queries. We conclude that
approximately 0.25% of queries are paedophile, and that more than 0.2% of users
enter such queries. These statistics are by far the most precise and reliable
ever obtained in this domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4167</identifier>
 <datestamp>2012-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4167</id><created>2012-06-19</created><authors><author><keyname>Fournier</keyname><forenames>Rapha&#xeb;l</forenames></author><author><keyname>Cholez</keyname><forenames>Thibault</forenames></author><author><keyname>Latapy</keyname><forenames>Matthieu</forenames></author><author><keyname>Magnien</keyname><forenames>Cl&#xe9;mence</forenames></author><author><keyname>Chrisment</keyname><forenames>Isabelle</forenames></author><author><keyname>Daniloff</keyname><forenames>Ivan</forenames></author><author><keyname>Festor</keyname><forenames>Olivier</forenames></author></authors><title>Comparing paedophile activity in different P2P systems</title><categories>cs.CY cs.NI</categories><comments>Submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Peer-to-peer (P2P) systems are widely used to exchange content over the
Internet. Knowledge on paedophile activity in such networks remains limited
while it has important social consequences. Moreover, though there are
different P2P systems in use, previous academic works on this topic focused on
one system at a time and their results are not directly comparable.
  We design a methodology for comparing \kad and \edonkey, two P2P systems
among the most prominent ones and with different anonymity levels. We monitor
two \edonkey servers and the \kad network during several days and record
hundreds of thousands of keyword-based queries. We detect paedophile-related
queries with a previously validated tool and we propose, for the first time, a
large-scale comparison of paedophile activity in two different P2P systems. We
conclude that there are significantly fewer paedophile queries in \kad than in
\edonkey (approximately 0.09% \vs 0.25%).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4169</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4169</id><created>2012-06-19</created><authors><author><keyname>Bui</keyname><forenames>Loc</forenames></author><author><keyname>Johari</keyname><forenames>Ramesh</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author></authors><title>Clustered Bandits</title><categories>cs.LG</categories><comments>19 pages, 2 figures, under review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a multi-armed bandit setting that is inspired by real-world
applications in e-commerce. In our setting, there are a few types of users,
each with a specific response to the different arms. When a user enters the
system, his type is unknown to the decision maker. The decision maker can
either treat each user separately ignoring the previously observed users, or
can attempt to take advantage of knowing that only few types exist and cluster
the users according to their response to the arms. We devise algorithms that
combine the usual exploration-exploitation tradeoff with clustering of users
and demonstrate the value of clustering. In the process of developing
algorithms for the clustered setting, we propose and analyze simple algorithms
for the setup where a decision maker knows that a user belongs to one of few
types, but does not know which one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4175</identifier>
 <datestamp>2012-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4175</id><created>2012-06-19</created><authors><author><keyname>Kermarrec</keyname><forenames>Anne-Marie</forenames></author><author><keyname>Merrer</keyname><forenames>Erwan Le</forenames></author><author><keyname>Straub</keyname><forenames>Gilles</forenames></author><author><keyname>van Kempen</keyname><forenames>Alexandre</forenames></author></authors><title>Clustered Network Coding for Maintenance in Practical Storage Systems</title><categories>cs.DC</categories><comments>14 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classical erasure codes, e.g. Reed-Solomon codes, have been acknowledged as
an efficient alternative to plain replication to reduce the storage overhead in
reliable distributed storage systems. Yet, such codes experience high overhead
during the maintenance process. In this paper we propose a novel erasure-coded
framework especially tailored for networked storage systems. Our approach
relies on the use of random codes coupled with a clustered placement strategy,
enabling the maintenance of a failed machine at the granularity of multiple
files. Our repair protocol leverages network coding techniques to reduce by
half the amount of data transferred during maintenance, as several files can be
repaired simultaneously. This approach, as formally proven and demonstrated by
our evaluation on a public experimental testbed, enables to dramatically
decrease the bandwidth overhead during the maintenance process, as well as the
time to repair a failure. In addition, the implementation is made as simple as
possible, aiming at a deployment into practical systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4176</identifier>
 <datestamp>2012-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4176</id><created>2012-06-19</created><authors><author><keyname>Souza</keyname><forenames>&#xc1;lvaro R. C.</forenames></author><author><keyname>Abr&#xe3;o</keyname><forenames>Taufik</forenames></author><author><keyname>Sampaio</keyname><forenames>Lucas H.</forenames></author><author><keyname>Jeszensky</keyname><forenames>Paul Jean E.</forenames></author></authors><title>Energy and Spectral Efficiencies Trade-off with Filter Optimization in
  Multiple Access Interference-Aware</title><categories>math.OC cs.IT math.IT</categories><comments>14 pages, 6 figures, 2 tables, conference paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work analyzes the optimized deployment of two resources scarcely
available in mobile multiple access systems, i.e., spectrum and energy, as well
as the impact of filter optimization in the system performance. Taking in
perspective the two conflicting metrics, throughput maximization and power
consumption minimization, the distributed energy efficiency (EE) cost function
is formulated. Furthermore, the best energy-spectral efficiencies (EE-SE)
trade-off is achieved when each node allocates exactly the power necessary to
attain the best SINR response, which guarantees the maximal EE. To demonstrate
the validity of our analysis, two low-complexity energy-spectral efficient
algorithms, based on distributed instantaneous SINR level are developed, and
the impact of single and multiuser detection filters on the EE-SE trade-off is
analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4181</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4181</id><created>2012-06-19</created><updated>2013-07-31</updated><authors><author><keyname>Laraki</keyname><forenames>Rida</forenames></author><author><keyname>Mertikopoulos</keyname><forenames>Panayotis</forenames></author></authors><title>Higher Order Game Dynamics</title><categories>math.OC cs.GT math.DS</categories><comments>32 pages, 6 figures; to appear in the Journal of Economic Theory.
  Updated material on the microfoundations of the dynamics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Continuous-time game dynamics are typically first order systems where payoffs
determine the growth rate of the players' strategy shares. In this paper, we
investigate what happens beyond first order by viewing payoffs as higher order
forces of change, specifying e.g. the acceleration of the players' evolution
instead of its velocity (a viewpoint which emerges naturally when it comes to
aggregating empirical data of past instances of play). To that end, we derive a
wide class of higher order game dynamics, generalizing first order imitative
dynamics, and, in particular, the replicator dynamics. We show that strictly
dominated strategies become extinct in n-th order payoff-monotonic dynamics n
orders as fast as in the corresponding first order dynamics; furthermore, in
stark contrast to first order, weakly dominated strategies also become extinct
for n&gt;1. All in all, higher order payoff-monotonic dynamics lead to the
elimination of weakly dominated strategies, followed by the iterated deletion
of strictly dominated strategies, thus providing a dynamic justification of the
well-known epistemic rationalizability process of Dekel and Fudenberg (1990).
Finally, we also establish a higher order analogue of the folk theorem of
evolutionary game theory, and we show that con- vergence to strict equilibria
in n-th order dynamics is n orders as fast as in first order.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4185</identifier>
 <datestamp>2012-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4185</id><created>2012-06-19</created><authors><author><keyname>Osherovich</keyname><forenames>Eliyahu</forenames></author></authors><title>Ant Robotics: Covering Continuous Domains by Multi-A(ge)nt Systems</title><categories>cs.RO cs.AI cs.MA</categories><report-no>MSC-2007-18</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we present an algorithm for covering continuous connected
domains by ant-like robots with very limited capabilities. The robots can mark
visited places with pheromone marks and sense the level of the pheromone in
their local neighborhood. In case of multiple robots these pheromone marks can
be sensed by all robots and provide the only way of (indirect) communication
between the robots. The robots are assumed to be memoryless, and to have no
global information such as the domain map, their own position (either absolute
or relative), total marked area percentage, maximal pheromone level, etc..
Despite the robots' simplicity, we show that they are able, by running a very
simple rule of behavior, to ensure efficient covering of arbitrary connected
domains, including non-planar and multidimensional ones. The novelty of our
algorithm lies in the fact that, unlike previously proposed methods, our
algorithm works on continuous domains without relying on some &quot;induced&quot;
underlying graph, that effectively reduces the problem to a discrete case of
graph covering. The algorithm guarantees complete coverage of any connected
domain. We also prove that the algorithm is noise immune, i.e., it is able to
cope with any initial pheromone profile (noise). In addition the algorithm
provides a bounded constant time between two successive visits of the robot,
and thus, is suitable for patrolling or surveillance applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4192</identifier>
 <datestamp>2012-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4192</id><created>2012-06-19</created><authors><author><keyname>Osherovich</keyname><forenames>Eliyahu</forenames></author></authors><title>Designing Incoherent Dictionaries for Compressed Sensing: Algorithm
  Comparison</title><categories>cs.IT cs.DS math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new method presented for design of incoherent dictionaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4206</identifier>
 <datestamp>2013-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4206</id><created>2012-06-19</created><authors><author><keyname>Kostakos</keyname><forenames>Vassilis</forenames></author><author><keyname>Hosio</keyname><forenames>Simo</forenames></author><author><keyname>Goncalves</keyname><forenames>Jorge</forenames></author></authors><title>Correlating Pedestrian Flows and Search Engine Queries</title><categories>cs.HC</categories><comments>4 pages, 1 figure, 1 table</comments><acm-class>H.5.m</acm-class><journal-ref>PLoS ONE 8(5): e63980, 2013</journal-ref><doi>10.1371/journal.pone.0063980</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important challenge for ubiquitous computing is the development of
techniques that can characterize a location vis-a-vis the richness and
diversity of urban settings. In this paper we report our work on correlating
urban pedestrian flows with Google search queries. Using longitudinal data we
show pedestrian flows at particular locations can be correlated with the
frequency of Google search terms that are semantically relevant to those
locations. Our approach can identify relevant content, media, and
advertisements for particular locations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4221</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4221</id><created>2012-06-19</created><authors><author><keyname>Kantas</keyname><forenames>Nikolas</forenames></author><author><keyname>Singh</keyname><forenames>Sumeetpal S.</forenames></author><author><keyname>Doucet</keyname><forenames>Arnaud</forenames></author></authors><title>Distributed Maximum Likelihood for Simultaneous Self-localization and
  Tracking in Sensor Networks</title><categories>math.OC cs.DC cs.SY stat.AP</categories><comments>shorter version is about to appear in IEEE Transactions of Signal
  Processing; 22 pages, 15 figures</comments><doi>10.1109/TSP.2012.2205923</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the sensor self-localization problem can be cast as a static
parameter estimation problem for Hidden Markov Models and we implement fully
decentralized versions of the Recursive Maximum Likelihood and on-line
Expectation-Maximization algorithms to localize the sensor network
simultaneously with target tracking. For linear Gaussian models, our algorithms
can be implemented exactly using a distributed version of the Kalman filter and
a novel message passing algorithm. The latter allows each node to compute the
local derivatives of the likelihood or the sufficient statistics needed for
Expectation-Maximization. In the non-linear case, a solution based on local
linearization in the spirit of the Extended Kalman Filter is proposed. In
numerical examples we demonstrate that the developed algorithms are able to
learn the localization parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4224</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4224</id><created>2012-06-19</created><updated>2013-05-14</updated><authors><author><keyname>Chattopadhyay</keyname><forenames>Arkadev</forenames></author><author><keyname>Grenet</keyname><forenames>Bruno</forenames></author><author><keyname>Koiran</keyname><forenames>Pascal</forenames></author><author><keyname>Portier</keyname><forenames>Natacha</forenames></author><author><keyname>Strozecki</keyname><forenames>Yann</forenames></author></authors><title>Factoring bivariate lacunary polynomials without heights</title><categories>cs.CC cs.SC</categories><comments>25 pages, 1 appendix</comments><acm-class>I.1.2; F.2.2</acm-class><journal-ref>Proceedings of the 38th International Symposium on Symbolic and
  Algebraic Computation (ISSAC'13), pp 141-148, ACM, 2013</journal-ref><doi>10.1145/2465506.2465932</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm which computes the multilinear factors of bivariate
lacunary polynomials. It is based on a new Gap Theorem which allows to test
whether a polynomial of the form P(X,X+1) is identically zero in time
polynomial in the number of terms of P(X,Y). The algorithm we obtain is more
elementary than the one by Kaltofen and Koiran (ISSAC'05) since it relies on
the valuation of polynomials of the previous form instead of the height of the
coefficients. As a result, it can be used to find some linear factors of
bivariate lacunary polynomials over a field of large finite characteristic in
probabilistic polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4226</identifier>
 <datestamp>2012-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4226</id><created>2012-06-19</created><authors><author><keyname>Mirmohseni</keyname><forenames>Mahtab</forenames></author><author><keyname>Akhbari</keyname><forenames>Bahareh</forenames></author><author><keyname>Aref</keyname><forenames>Mohammad Reza</forenames></author></authors><title>Three-User Cognitive Interference Channel: Capacity Region with Strong
  Interference</title><categories>cs.IT math.IT</categories><comments>To appear in IET Communications, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study investigates the capacity region of a three-user cognitive radio
network with two primary users and one cognitive user. A three-user Cognitive
Interference Channel (C-IFC) is proposed by considering a three-user
Interference Channel (IFC) where one of the transmitters has cognitive
capabilities and knows the messages of the other two transmitters in a
non-causal manner. First, two inner bounds on the capacity region of the
three-user C-IFC are obtained based on using the schemes which allow all
receivers to decode all messages with two different orders. Next, two sets of
conditions are derived, under which the capacity region of the proposed model
coincides with the capacity region of a three-user C-IFC in which all three
messages are required at all receivers. Under these conditions, referred to as
strong interference conditions, the capacity regions for the proposed
three-user C-IFC are characterized. Moreover, the Gaussian three-user C-IFC is
considered and the capacity results are derived for the Gaussian case. Some
numerical examples are also provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4229</identifier>
 <datestamp>2013-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4229</id><created>2012-06-19</created><updated>2012-12-28</updated><authors><author><keyname>En&#xdf;lin</keyname><forenames>Torsten A.</forenames></author></authors><title>Information field dynamics for simulation scheme construction</title><categories>physics.comp-ph astro-ph.IM cs.IT math.IT</categories><comments>19 pages, 3 color figures, accepted by Phys. Rev. E</comments><doi>10.1103/PhysRevE.87.013308</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information field dynamics (IFD) is introduced here as a framework to derive
numerical schemes for the simulation of physical and other fields without
assuming a particular sub-grid structure as many schemes do. IFD constructs an
ensemble of non-parametric sub-grid field configurations from the combination
of the data in computer memory, representing constraints on possible field
configurations, and prior assumptions on the sub-grid field statistics. Each of
these field configurations can formally be evolved to a later moment since any
differential operator of the dynamics can act on fields living in continuous
space. However, these virtually evolved fields need again a representation by
data in computer memory. The maximum entropy principle of information theory
guides the construction of updated datasets via entropic matching, optimally
representing these field configurations at the later time. The field dynamics
thereby become represented by a finite set of evolution equations for the data
that can be solved numerically. The sub-grid dynamics is treated within an
auxiliary analytic consideration and the resulting scheme acts solely on the
data space. It should provide a more accurate description of the physical field
dynamics than simulation schemes constructed ad-hoc, due to the more rigorous
accounting of sub-grid physics and the space discretization process.
Assimilation of measurement data into an IFD simulation is conceptually
straightforward since measurement and simulation data can just be merged. The
IFD approach is illustrated using the example of a coarsely discretized
representation of a thermally excited classical Klein-Gordon field. This should
pave the way towards the construction of schemes for more complex systems like
turbulent hydrodynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4232</identifier>
 <datestamp>2012-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4232</id><created>2012-06-19</created><authors><author><keyname>See</keyname><forenames>Phen Chiak</forenames></author><author><keyname>Tai</keyname><forenames>Vin Cent</forenames></author><author><keyname>Molinas</keyname><forenames>Marta</forenames></author><author><keyname>Uhlen</keyname><forenames>Kjetil</forenames></author><author><keyname>Fosso</keyname><forenames>Olav Bjarte</forenames></author></authors><title>Enhanced active power filter control for nonlinear non-stationary
  reactive power compensation</title><categories>math.OC cs.SY</categories><comments>Keywords: Reactive power compensation; p-q power theory; empirical
  mode decomposition</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper describes a method to implement Reactive Power Compensation (RPC)
in power systems that possess nonlinear non-stationary current disturbances.
The Empirical Mode Decomposition (EMD) introduced in the Hilbert-Huang
Transform (HHT) is used to separate the disturbances from the original current
waveform. These disturbances are subsequently removed. Following that, Power
Factor Correction (PFC) based on the well-known p-q power theory is conducted
to remove the reactive power. Both operations were implemented in a shunt
Active Power Filter (APF). The EMD significantly simplifies the singulation and
the removal of the current disturbances. This helps to effectively identify the
fundamental current waveform. Hence, it simplifies the implementation of RPC on
nonlinear non-stationary power systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4234</identifier>
 <datestamp>2012-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4234</id><created>2012-06-19</created><authors><author><keyname>Henry</keyname><forenames>Julien</forenames><affiliation>VERIMAG - IMAG</affiliation></author><author><keyname>Monniaux</keyname><forenames>David</forenames><affiliation>VERIMAG - IMAG</affiliation></author><author><keyname>Moy</keyname><forenames>Matthieu</forenames><affiliation>VERIMAG - IMAG</affiliation></author></authors><title>Succinct Representations for Abstract Interpretation</title><categories>cs.PL</categories><comments>Static analysis symposium (SAS), Deauville : France (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Abstract interpretation techniques can be made more precise by distinguishing
paths inside loops, at the expense of possibly exponential complexity.
SMT-solving techniques and sparse representations of paths and sets of paths
avoid this pitfall. We improve previously proposed techniques for guided static
analysis and the generation of disjunctive invariants by combining them with
techniques for succinct representations of paths and symbolic representations
for transitions based on static single assignment. Because of the
non-monotonicity of the results of abstract interpretation with widening
operators, it is difficult to conclude that some abstraction is more precise
than another based on theoretical local precision results. We thus conducted
extensive comparisons between our new techniques and previous ones, on a
variety of open-source packages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4236</identifier>
 <datestamp>2012-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4236</id><created>2012-06-19</created><authors><author><keyname>Chermakani</keyname><forenames>Deepak Ponvel</forenames></author></authors><title>Efficiently expressing feasibility problems in Linear Systems, as
  feasibility problems in Asymptotic-Linear-Programs</title><categories>cs.CC cs.DM math.AG</categories><comments>6 pages, 2 Theorems, 1 Figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a polynomial-time algorithm that obtains a set of Asymptotic
Linear Programs (ALPs) from a given linear system S, such that one of these
ALPs admits a feasible solution if and only if S admits a feasible solution. We
also show how to use the same algorithm to determine whether or not S admits a
non-trivial solution for any desired subset of its variables. S is allowed to
consist of linear constraints over real variables with integer coefficients,
where each constraint has either a lesser-than-or-equal-to, or a lesser-than,
or a not-equal-to relational operator. Each constraint of the obtained ALPs has
a lesser-than-or-equal-to relational operator, and the coefficients of its
variables vary linearly with respect to the time parameter that tends to
positive infinity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4241</identifier>
 <datestamp>2012-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4241</id><created>2012-06-19</created><updated>2012-06-20</updated><authors><author><keyname>Purkayastha</keyname><forenames>Bipul Syam</forenames></author><author><keyname>Das</keyname><forenames>Rajib</forenames></author></authors><title>Comparative Analysis of Routing Attacks in Ad Hoc Network</title><categories>cs.NI</categories><comments>6 pages, 6 figures, 1 table, International Journal of Advanced
  Networking and Applications. arXiv admin note: text overlap with
  arXiv:1202.4628 by other authors</comments><journal-ref>International Journal of Advanced Networking and Applications,
  Volume: 03 Issue: 05 Pages: 1352-1357 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the mobile ad hoc networks the major role is played by the routing
protocols in order to route the data from one mobile node to another mobile
node. But in such mobile networks, routing protocols are vulnerable to various
kinds of security attacks such as blackhole node attacks. The routing protocols
of MANET are unprotected and hence resulted into the network with the malicious
mobile nodes in the network. These malicious nodes in the network are basically
acts as attacks in the network. In this paper, we modify the existing DSR
protocol with the functionality of attacks detection without affecting overall
performance of the network. Also, we are considering the various attacks on
mobile ad hoc network called blackhole attack, flooding attack and show the
comparative analysis of these attacks using network simulator ns-2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4245</identifier>
 <datestamp>2012-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4245</id><created>2012-06-19</created><authors><author><keyname>Beirami</keyname><forenames>Ahmad</forenames></author><author><keyname>Fekri</keyname><forenames>Faramarz</forenames></author></authors><title>On Lossless Universal Compression of Distributed Identical Sources</title><categories>cs.IT math.IT</categories><comments>To appear in 2012 IEEE International Symposium on Information Theory
  (ISIT'2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Slepian-Wolf theorem is a well-known framework that targets almost lossless
compression of (two) data streams with symbol-by-symbol correlation between the
outputs of (two) distributed sources. However, this paper considers a different
scenario which does not fit in the Slepian-Wolf framework. We consider two
identical but spatially separated sources. We wish to study the universal
compression of a sequence of length $n$ from one of the sources provided that
the decoder has access to (i.e., memorized) a sequence of length $m$ from the
other source. Such a scenario occurs, for example, in the universal compression
of data from multiple mirrors of the same server. In this setup, the
correlation does not arise from symbol-by-symbol dependency of two outputs from
the two sources. Instead, the sequences are correlated through the information
that they contain about the unknown source parameter. We show that the
finite-length nature of the compression problem at hand requires considering a
notion of almost lossless source coding, where coding incurs an error
probability $p_e(n)$ that vanishes with sequence length $n$. We obtain a lower
bound on the average minimax redundancy of almost lossless codes as a function
of the sequence length $n$ and the permissible error probability $p_e$ when the
decoder has a memory of length $m$ and the encoders do not communicate. Our
results demonstrate that a strict performance loss is incurred when the two
encoders do not communicate even when the decoder knows the unknown parameter
vector (i.e., $m \to \infty$).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4256</identifier>
 <datestamp>2012-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4256</id><created>2012-06-19</created><authors><author><keyname>Khalili</keyname><forenames>Mehdi</forenames></author></authors><title>A Novel Effective, Secure and Robust CDMA Digital Image Watermarking in
  YUV Color Space Using DWT2</title><categories>cs.CR cs.MM</categories><comments>9 Pages, 16 Figures, International Journal of Computer Science
  Issues,May 2011</comments><msc-class>68U10, 68U20, 65C20, 94A08, 94A24, 94A60, 11T71, 14G50, 68P25, 81P94</msc-class><acm-class>D.4.6; K.6.5; K.4.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is allocated to CDMA digital images watermarking for ownership
verification and image authentication applications, which for more security,
watermark W is converted to a sequence and then a random binary sequence R of
size n is adopted to encrypt the watermark; where n is the size of the
watermark. This adopting process uses a pseudo-random number generator to
determine the pixel to be used on a given key. After converting the host image
to YUV color space and then wavelet decomposition of Y channel, this adopted
watermark is embedded into the selected subbands coefficients of Y channel
using the correlation properties of additive pseudo- random noise patterns. The
experimental results show that the proposed approach provides extra
imperceptibility, security and robustness against JPEG compression and
different noises attacks compared to the similar proposed methods. Moreover,
the proposed approach has no need of the original image to extract watermarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4275</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4275</id><created>2012-06-19</created><updated>2012-09-21</updated><authors><author><keyname>Truong</keyname><forenames>Kien T.</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>Joint Transmit Precoding for the Relay Interference Broadcast Channel</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Relays in cellular systems are interference limited. The highest end-to-end
sum rates are achieved when the relays are jointly optimized with the transmit
strategy. Unfortunately, interference couples the links together making joint
optimization challenging. Further, the end-to-end multi-hop performance is
sensitive to rate mismatch, when some links have a dominant first link while
others have a dominant second link. This paper proposes an algorithm for
designing the linear transmit precoders at the transmitters and relays of the
relay interference broadcast channel, a generic model for relay-based cellular
systems, to maximize the end-to-end sum-rates. First, the relays are designed
to maximize the second-hop sum-rates. Next, approximate end-to-end rates that
depend on the time-sharing fraction and the second-hop rates are used to
formulate a sum-utility maximization problem for designing the transmitters.
This problem is solved by iteratively minimizing the weighted sum of mean
square errors. Finally, the norms of the transmit precoders at the transmitters
are adjusted to eliminate rate mismatch. The proposed algorithm allows for
distributed implementation and has fast convergence. Numerical results show
that the proposed algorithm outperforms a reasonable application of single-hop
interference management strategies separately on two hops.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4280</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4280</id><created>2012-06-19</created><updated>2012-12-04</updated><authors><author><keyname>Biondo</keyname><forenames>A. E.</forenames></author><author><keyname>Pluchino</keyname><forenames>A.</forenames></author><author><keyname>Rapisarda</keyname><forenames>A.</forenames></author></authors><title>Return Migration After Brain Drain: A Simulation Approach</title><categories>physics.soc-ph cs.SI</categories><comments>21 pages, 10 figures, 1 table, accepted for publication in JASSS</comments><journal-ref>Journal of Artificial Societies and Social Simulation (JASSS) 16
  (2) 11, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Brain Drain phenomenon is particularly heterogeneous and is characterized
by peculiar specifications. It influences the economic fundamentals of both the
country of origin and the host one in terms of human capital accumulation.
Here, the brain drain is considered from a microeconomic perspective: more
precisely we focus on the individual rational decision to return, referring it
to the social capital owned by the worker. The presented model compares utility
levels to justify agent migration conduct and to simulate several scenarios
within a computational environment. In particular, we developed a simulation
framework based on two fundamental individual features, i.e. risk aversion and
initial expectation, which characterize the dynamics of different agents
according to the evolution of their social contacts. Our main result is that,
according to the value of risk aversion and initial expectation, the
probability of return migration depends on their ratio, with a certain degree
of approximation: when risk aversion is much bigger than the initial
expectation, the probability of returns is maximal, while, in the opposite
case, the probability for the agents to remain abroad is very high. In between,
when the two values are comparable, it does exist a broad intertwined region
where it is very difficult to draw any analytical forecast.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4283</identifier>
 <datestamp>2012-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4283</id><created>2012-06-15</created><authors><author><keyname>Mastroeni</keyname><forenames>Loretta</forenames></author><author><keyname>Naldi</keyname><forenames>Maurizio</forenames></author></authors><title>Pricing of insurance policies against cloud storage price rises</title><categories>cs.OH</categories><acm-class>H.3; K.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When a company migrates to cloud storage, the way back is neither easy nor
cheap. The company is then locked up in the storage contract and exposed to
upward market prices, which reduce the company's profit and may even bring it
below zero. We propose a protection means based on an insurance contract, by
which the cloud purchaser is indemnified when the current storage price exceeds
a pre-defined threshold. By applying the financial options theory, we provide a
formula for the insurance price (the premium). By using historical data on
market prices for disks, we apply the formula in realistic scenarios. We show
that the premium grows nearly quadratically with the length of the coverage
period as long as this is below one year, but grows more slowly, though faster
than linearly, over longer coverage periods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4300</identifier>
 <datestamp>2012-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4300</id><created>2012-06-19</created><authors><author><keyname>Vigna</keyname><forenames>Sebastiano</forenames></author></authors><title>Quasi-Succinct Indices</title><categories>cs.IR cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed inverted indices in use today are based on the idea of gap
compression: documents pointers are stored in increasing order, and the gaps
between successive document pointers are stored using suitable codes which
represent smaller gaps using less bits. Additional data such as counts and
positions is stored using similar techniques. A large body of research has been
built in the last 30 years around gap compression, including theoretical
modeling of the gap distribution, specialized instantaneous codes suitable for
gap encoding, and ad hoc document reorderings which increase the efficiency of
instantaneous codes. This paper proposes to represent an index using a
different architecture based on quasi-succinct representation of monotone
sequences. We show that, besides being theoretically elegant and simple, the
new index provides expected constant-time operations and, in practice,
significant performance improvements on conjunctive, phrasal and proximity
queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4326</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4326</id><created>2012-06-19</created><authors><author><keyname>Thirumalai</keyname><forenames>Vijayaraghavan</forenames></author><author><keyname>Frossard</keyname><forenames>Pascal</forenames></author></authors><title>Joint Reconstruction of Multi-view Compressed Images</title><categories>cs.MM cs.CV</categories><doi>10.1109/TIP.2013.2240006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The distributed representation of correlated multi-view images is an
important problem that arise in vision sensor networks. This paper concentrates
on the joint reconstruction problem where the distributively compressed
correlated images are jointly decoded in order to improve the reconstruction
quality of all the compressed images. We consider a scenario where the images
captured at different viewpoints are encoded independently using common coding
solutions (e.g., JPEG, H.264 intra) with a balanced rate distribution among
different cameras. A central decoder first estimates the underlying correlation
model from the independently compressed images which will be used for the joint
signal recovery. The joint reconstruction is then cast as a constrained convex
optimization problem that reconstructs total-variation (TV) smooth images that
comply with the estimated correlation model. At the same time, we add
constraints that force the reconstructed images to be consistent with their
compressed versions. We show by experiments that the proposed joint
reconstruction scheme outperforms independent reconstruction in terms of image
quality, for a given target bit rate. In addition, the decoding performance of
our proposed algorithm compares advantageously to state-of-the-art distributed
coding schemes based on disparity learning and on the DISCOVER.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4327</identifier>
 <datestamp>2012-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4327</id><created>2012-06-19</created><authors><author><keyname>Bakshy</keyname><forenames>Eytan</forenames></author><author><keyname>Eckles</keyname><forenames>Dean</forenames></author><author><keyname>Yan</keyname><forenames>Rong</forenames></author><author><keyname>Rosenn</keyname><forenames>Itamar</forenames></author></authors><title>Social Influence in Social Advertising: Evidence from Field Experiments</title><categories>cs.SI physics.soc-ph stat.AP</categories><comments>16 pages, 8 figures, ACM EC 2012</comments><acm-class>J.4; H.1.2</acm-class><journal-ref>E. Bakshy, D. Eckles, R. Yan, and I. Rosenn. 2012. Social
  influence in social advertising: evidence from field experiments. In
  Proceedings of the 13th ACM Conference on Electronic Commerce (EC '12). ACM,
  New York, NY, USA, 146-161</journal-ref><doi>10.1145/2229012.2229027</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social advertising uses information about consumers' peers, including peer
affiliations with a brand, product, organization, etc., to target ads and
contextualize their display. This approach can increase ad efficacy for two
main reasons: peers' affiliations reflect unobserved consumer characteristics,
which are correlated along the social network; and the inclusion of social cues
(i.e., peers' association with a brand) alongside ads affect responses via
social influence processes. For these reasons, responses may be increased when
multiple social signals are presented with ads, and when ads are affiliated
with peers who are strong, rather than weak, ties.
  We conduct two very large field experiments that identify the effect of
social cues on consumer responses to ads, measured in terms of ad clicks and
the formation of connections with the advertised entity. In the first
experiment, we randomize the number of social cues present in word-of-mouth
advertising, and measure how responses increase as a function of the number of
cues. The second experiment examines the effect of augmenting traditional ad
units with a minimal social cue (i.e., displaying a peer's affiliation below an
ad in light grey text). On average, this cue causes significant increases in ad
performance. Using a measurement of tie strength based on the total amount of
communication between subjects and their peers, we show that these influence
effects are greatest for strong ties. Our work has implications for ad
optimization, user interface design, and central questions in social science
research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4329</identifier>
 <datestamp>2012-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4329</id><created>2012-06-19</created><authors><author><keyname>Nandy</keyname><forenames>Sudarshan</forenames></author><author><keyname>Sarkar</keyname><forenames>Partha Pratim</forenames></author><author><keyname>Das</keyname><forenames>Achintya</forenames></author></authors><title>An Improved Gauss-Newtons Method based Back-propagation Algorithm for
  Fast Convergence</title><categories>cs.AI cs.NA</categories><comments>7 pages, 6 figures,2 tables, Published with International Journal of
  Computer Applications (IJCA)</comments><journal-ref>International Journal of Computer Applications 39(8):1-7, February
  2012. Published by Foundation of Computer Science, New York, USA</journal-ref><doi>10.5120/4837-7097</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present work deals with an improved back-propagation algorithm based on
Gauss-Newton numerical optimization method for fast convergence. The steepest
descent method is used for the back-propagation. The algorithm is tested using
various datasets and compared with the steepest descent back-propagation
algorithm. In the system, optimization is carried out using multilayer neural
network. The efficacy of the proposed method is observed during the training
period as it converges quickly for the dataset used in test. The requirement of
memory for computing the steps of algorithm is also analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4358</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4358</id><created>2012-06-19</created><updated>2013-04-12</updated><authors><author><keyname>Bassett</keyname><forenames>Danielle S.</forenames></author><author><keyname>Porter</keyname><forenames>Mason A.</forenames></author><author><keyname>Wymbs</keyname><forenames>Nicholas F.</forenames></author><author><keyname>Grafton</keyname><forenames>Scott T.</forenames></author><author><keyname>Carlson</keyname><forenames>Jean M.</forenames></author><author><keyname>Mucha</keyname><forenames>Peter J.</forenames></author></authors><title>Robust Detection of Dynamic Community Structure in Networks</title><categories>physics.data-an cond-mat.dis-nn cs.SI physics.bio-ph physics.soc-ph q-bio.NC</categories><comments>18 pages, 11 figures</comments><journal-ref>Chaos, 2013, 23, 1</journal-ref><doi>10.1063/1.4790830</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe techniques for the robust detection of community structure in
some classes of time-dependent networks. Specifically, we consider the use of
statistical null models for facilitating the principled identification of
structural modules in semi-decomposable systems. Null models play an important
role both in the optimization of quality functions such as modularity and in
the subsequent assessment of the statistical validity of identified community
structure. We examine the sensitivity of such methods to model parameters and
show how comparisons to null models can help identify system scales. By
considering a large number of optimizations, we quantify the variance of
network diagnostics over optimizations (`optimization variance') and over
randomizations of network structure (`randomization variance'). Because the
modularity quality function typically has a large number of nearly-degenerate
local optima for networks constructed using real data, we develop a method to
construct representative partitions that uses a null model to correct for
statistical noise in sets of partitions. To illustrate our results, we employ
ensembles of time-dependent networks extracted from both nonlinear oscillators
and empirical neuroscience data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4366</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4366</id><created>2012-06-19</created><updated>2015-10-11</updated><authors><author><keyname>Schulman</keyname><forenames>Leonard J.</forenames></author><author><keyname>Vazirani</keyname><forenames>Vijay V.</forenames></author></authors><title>Allocation of Divisible Goods under Lexicographic Preferences</title><categories>cs.GT cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a simple and natural non-pricing mechanism for allocating
divisible goods among strategic agents having lexicographic preferences. Our
mechanism has favorable properties of incentive compatibility
(strategy-proofness), Pareto efficiency, envy-freeness, and time efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4370</identifier>
 <datestamp>2012-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4370</id><created>2012-06-19</created><authors><author><keyname>Ding</keyname><forenames>Cunsheng</forenames></author></authors><title>Cyclic Codes from Dickson Polynomials</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to their efficient encoding and decoding algorithms cyclic codes, a
subclass of linear codes, have applications in consumer electronics, data
storage systems, and communication systems. In this paper, Dickson polynomials
of the first and second kind over finite fields are employed to construct a
number of classes of cyclic codes. Lower bounds on the minimum weight of some
classes of the cyclic codes are developed. The minimum weights of some other
classes of the codes constructed in this paper are determined. The dimensions
of the codes obtained in this paper are flexible. Most of the codes presented
in this paper are optimal or almost optimal in the sense that they meet some
bound on linear codes. Over ninety cyclic codes of this paper should be used to
update the current database of tables of best linear codes known. Among them
sixty are optimal in the sense that they meet some bound on linear codes and
the rest are cyclic codes having the same parameters as the best linear code in
the current database maintained at http://www.codetables.de/.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4377</identifier>
 <datestamp>2012-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4377</id><created>2012-06-19</created><authors><author><keyname>Afrati</keyname><forenames>Foto N.</forenames></author><author><keyname>Sarma</keyname><forenames>Anish Das</forenames></author><author><keyname>Salihoglu</keyname><forenames>Semih</forenames></author><author><keyname>Ullman</keyname><forenames>Jeffrey D.</forenames></author></authors><title>Upper and Lower Bounds on the Cost of a Map-Reduce Computation</title><categories>cs.DC cs.DS</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the tradeoff between parallelism and communication
cost in a map-reduce computation. For any problem that is not &quot;embarrassingly
parallel,&quot; the finer we partition the work of the reducers so that more
parallelism can be extracted, the greater will be the total communication
between mappers and reducers. We introduce a model of problems that can be
solved in a single round of map-reduce computation. This model enables a
generic recipe for discovering lower bounds on communication cost as a function
of the maximum number of inputs that can be assigned to one reducer. We use the
model to analyze the tradeoff for three problems: finding pairs of strings at
Hamming distance $d$, finding triangles and other patterns in a larger graph,
and matrix multiplication. For finding strings of Hamming distance 1, we have
upper and lower bounds that match exactly. For triangles and many other graphs,
we have upper and lower bounds that are the same to within a constant factor.
For the problem of matrix multiplication, we have matching upper and lower
bounds for one-round map-reduce algorithms. We are also able to explore
two-round map-reduce algorithms for matrix multiplication and show that these
never have more communication, for a given reducer size, than the best
one-round algorithm, and often have significantly less.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4389</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4389</id><created>2012-06-20</created><updated>2015-01-07</updated><authors><author><keyname>Zhou</keyname><forenames>Qing F.</forenames></author><author><keyname>Mow</keyname><forenames>Wai Ho</forenames></author><author><keyname>Zhang</keyname><forenames>Shengli</forenames></author><author><keyname>Toumpakaris</keyname><forenames>Dimitris</forenames></author></authors><title>Improving Two-Way Selective Decode-and-forward Wireless Relaying with
  Energy-Efficient One-bit Soft Forwarding</title><categories>cs.IT math.IT math.PR</categories><comments>32 pages, 9 figures, 1 table. submitted to IEEE Trans. Wireless
  Commun</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by applications such as battery-operated wireless sensor networks
(WSN), we propose an easy-to-implement energy-efficient two-way relaying
scheme. In particular, we address the challenge of improving the standard
two-way selective decode-and-forward protocol (TW-SDF) in terms of
block-error-rate (BLER) with minor additional complexity and energy
consumption. By following the principle of soft relaying, our solution is the
two-way one-bit soft forwarding (TW-1bSF) protocol in which the relay forwards
the one-bit quantization of a posterior information metric about the
transmitted bits, associated with an appropriately designed reliability
parameter.
  In WSN-related standards (such as IEEE802.15.6 and Bluetooth), block codes
are adopted instead of convolutional and other sophisticated codes, due to
their efficient decoder hardware implementation. As the second main
contribution, we derive tight upper bounds on the BLER performance for both
TW-SDF and TW-1bSF, when the two-way relaying network employs block codes and
hard decoding. The error probability analysis confirms the superiority of
TW-1bSF. Moreover, we derive the asymptotic performance gain of TW-1bSF over
TW-SDF, which further suggests that the proposed protocol is a good choice,
especially when long block codes are used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4391</identifier>
 <datestamp>2012-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4391</id><created>2012-06-20</created><authors><author><keyname>Mondal</keyname><forenames>Koushik</forenames></author><author><keyname>Dutta</keyname><forenames>Paramartha</forenames></author><author><keyname>Bhattacharyya</keyname><forenames>Siddhartha</forenames></author></authors><title>Gray Image extraction using Fuzzy Logic</title><categories>cs.CV cs.AI</categories><comments>8 pages, 5 figures, Fuzzy Rule Base, Image Extraction, Fuzzy
  Inference System (FIS), Membership Functions, Membership values,Image coding
  and Processing, Soft Computing, Computer Vision Accepted and published in
  IEEE. arXiv admin note: text overlap with arXiv:1206.3633</comments><doi>10.1109/ACCT.2012.60</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fuzzy systems concern fundamental methodology to represent and process
uncertainty and imprecision in the linguistic information. The fuzzy systems
that use fuzzy rules to represent the domain knowledge of the problem are known
as Fuzzy Rule Base Systems (FRBS). On the other hand image segmentation and
subsequent extraction from a noise-affected background, with the help of
various soft computing methods, are relatively new and quite popular due to
various reasons. These methods include various Artificial Neural Network (ANN)
models (primarily supervised in nature), Genetic Algorithm (GA) based
techniques, intensity histogram based methods etc. providing an extraction
solution working in unsupervised mode happens to be even more interesting
problem. Literature suggests that effort in this respect appears to be quite
rudimentary. In the present article, we propose a fuzzy rule guided novel
technique that is functional devoid of any external intervention during
execution. Experimental results suggest that this approach is an efficient one
in comparison to different other techniques extensively addressed in
literature. In order to justify the supremacy of performance of our proposed
technique in respect of its competitors, we take recourse to effective metrics
like Mean Squared Error (MSE), Mean Absolute Error (MAE), Peak Signal to Noise
Ratio (PSNR).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4436</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4436</id><created>2012-06-20</created><updated>2014-09-16</updated><authors><author><keyname>Horak</keyname><forenames>Peter</forenames></author><author><keyname>Hromada</keyname><forenames>Viliam</forenames></author></authors><title>Tiling $R^{5}$ by Crosses</title><categories>cs.IT math.CO math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An $n$-dimensional cross comprises $2n+1$ unit cubes: the center cube and
reflections in all its faces. It is well known that there is a tiling of
$R^{n}$ by crosses for all $n.$ AlBdaiwi and the first author proved that if
$2n+1$ is not a prime then there are $2^{\aleph_{0}}$ \ non-congruent regular
(= face-to-face) tilings of $R^{n}$ by crosses, while there is a unique tiling
of $R^{n}$ by crosses for $n=2,3$. They conjectured that this is always the
case if $2n+1$ is a prime. To support the conjecture we prove in this paper
that also for $R^{5}$ there is a unique regular, and no non-regular, tiling by
crosses. So there is a unique tiling of $R^{3}$ by crosses, there are
$2^{\aleph_{0}}$ tilings of $R^{4},$ but for $R^{5}$ there is again only one
tiling by crosses. We guess that this result goes against our intuition that
suggests &quot;the higher the dimension of the \ space, the more freedom we get&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4438</identifier>
 <datestamp>2012-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4438</id><created>2012-06-20</created><authors><author><keyname>Kramer</keyname><forenames>R. P.</forenames></author><author><keyname>van Schijndel</keyname><forenames>A. W. M.</forenames></author></authors><title>Inverse Modeling of Climate Responses of Monumental Buildings</title><categories>cs.CE</categories><comments>Preliminary conference paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The indoor climate conditions of monumental buildings are very important for
the conservation of these objects. Simplified models with physical meaning are
desired that are capable of simulating temperature and relative humidity. In
this paper we research state-space models as methodology for the inverse
modeling of climate responses of unheated monumental buildings. It is concluded
that this approach is very promising for obtaining physical models and
parameters of indoor climate responses. Furthermore state space models can be
simulated very efficiently: the simulation duration time of a 100 year hourly
based period take less than a second on an ordinary computer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4444</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4444</id><created>2012-06-20</created><updated>2012-06-25</updated><authors><author><keyname>Teige</keyname><forenames>Tino</forenames><affiliation>Carl von Ossietzky University of Oldenburg</affiliation></author><author><keyname>Fr&#xe4;nzle</keyname><forenames>Martin</forenames><affiliation>Carl von Ossietzky University of Oldenburg</affiliation></author></authors><title>Generalized Craig Interpolation for Stochastic Boolean Satisfiability
  Problems with Applications to Probabilistic State Reachability and Region
  Stability</title><categories>cs.LO</categories><proxy>LMCS</proxy><acm-class>D.2.4; F.3.1; F.4.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 2 (June 26,
  2012) lmcs:1031</journal-ref><doi>10.2168/LMCS-8(2:16)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The stochastic Boolean satisfiability (SSAT) problem has been introduced by
Papadimitriou in 1985 when adding a probabilistic model of uncertainty to
propositional satisfiability through randomized quantification. SSAT has many
applications, among them probabilistic bounded model checking (PBMC) of
symbolically represented Markov decision processes. This article identifies a
notion of Craig interpolant for the SSAT framework and develops an algorithm
for computing such interpolants based on a resolution calculus for SSAT. As a
potential application area of this novel concept of Craig interpolation, we
address the symbolic analysis of probabilistic systems. We first investigate
the use of interpolation in probabilistic state reachability analysis, turning
the falsification procedure employing PBMC into a verification technique for
probabilistic safety properties. We furthermore propose an interpolation-based
approach to probabilistic region stability, being able to verify that the
probability of stabilizing within some region is sufficiently large.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4458</identifier>
 <datestamp>2012-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4458</id><created>2012-06-20</created><authors><author><keyname>Fiorino</keyname><forenames>Guido</forenames></author></authors><title>Terminating Calculi for Propositional Dummett Logic with Subformula
  Property</title><categories>cs.LO</categories><comments>21 pages, rejected at CSL 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present two terminating tableau calculi for propositional
Dummett logic obeying the subformula property. The ideas of our calculi rely on
the linearly ordered Kripke semantics of Dummett logic. The first calculus
works on two semantical levels: the present and the next possible world. The
second calculus employs the usual object language of tableau systems and
exploits a property of the construction of the completeness theorem to
introduce a check which is an alternative to loop check mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4477</identifier>
 <datestamp>2012-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4477</id><created>2012-06-20</created><authors><author><keyname>Fonte</keyname><forenames>Daniela</forenames></author><author><keyname>Boas</keyname><forenames>Ismael Vilas</forenames></author><author><keyname>Azevedo</keyname><forenames>Jos&#xe9;</forenames></author><author><keyname>Peixoto</keyname><forenames>Jos&#xe9; Jo&#xe3;o</forenames></author><author><keyname>Faria</keyname><forenames>Pedro</forenames></author><author><keyname>Silva</keyname><forenames>Pedro</forenames></author><author><keyname>S&#xe1;</keyname><forenames>Tiago</forenames></author><author><keyname>Costa</keyname><forenames>Ulisses</forenames></author><author><keyname>da Cruz</keyname><forenames>Daniela</forenames></author><author><keyname>Henriques</keyname><forenames>Pedro Rangel</forenames></author></authors><title>Modeling Languages: metrics and assessing tools</title><categories>cs.SE</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Any traditional engineering field has metrics to rigorously assess the
quality of their products. Engineers know that the output must satisfy the
requirements, must comply with the production and market rules, and must be
competitive.
  Professionals in the new field of software engineering started a few years
ago to define metrics to appraise their product: individual programs and
software systems. This concern motivates the need to assess not only the
outcome but also the process and tools employed in its development. In this
context, assessing the quality of programming languages is a legitimate
objective; in a similar way, it makes sense to be concerned with models and
modeling approaches, as more and more people start the software development
process by a modeling phase.
  In this paper we introduce and motivate the assessment of models quality in
the Software Development cycle. After the general discussion of this topic, we
focus the attention on the most popular modeling language -- the UML --
presenting metrics. Through a Case-Study, we present and explore two tools. To
conclude we identify what is still lacking in the tools side.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4481</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4481</id><created>2012-06-20</created><updated>2012-09-10</updated><authors><author><keyname>Fauvel</keyname><forenames>M.</forenames></author><author><keyname>Villa</keyname><forenames>A.</forenames></author><author><keyname>Chanussot</keyname><forenames>J.</forenames></author><author><keyname>Benediktsson</keyname><forenames>J. A.</forenames></author></authors><title>Parsimonious Mahalanobis Kernel for the Classification of High
  Dimensional Data</title><categories>cs.NA cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classification of high dimensional data with kernel methods is considered
in this article. Exploit- ing the emptiness property of high dimensional
spaces, a kernel based on the Mahalanobis distance is proposed. The computation
of the Mahalanobis distance requires the inversion of a covariance matrix. In
high dimensional spaces, the estimated covariance matrix is ill-conditioned and
its inversion is unstable or impossible. Using a parsimonious statistical
model, namely the High Dimensional Discriminant Analysis model, the specific
signal and noise subspaces are estimated for each considered class making the
inverse of the class specific covariance matrix explicit and stable, leading to
the definition of a parsimonious Mahalanobis kernel. A SVM based framework is
used for selecting the hyperparameters of the parsimonious Mahalanobis kernel
by optimizing the so-called radius-margin bound. Experimental results on three
high dimensional data sets show that the proposed kernel is suitable for
classifying high dimensional data, providing better classification accuracies
than the conventional Gaussian kernel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4498</identifier>
 <datestamp>2012-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4498</id><created>2012-06-20</created><authors><author><keyname>Liu</keyname><forenames>Yuanpeng</forenames></author><author><keyname>Erkip</keyname><forenames>Elza</forenames></author></authors><title>On a Class of Discrete Memoryless Broadcast Interference Channels</title><categories>cs.IT math.IT</categories><comments>Accepted to ISIT2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a class of discrete memoryless broadcast interference channels
(DM-BICs), where one of the broadcast receivers is subject to the interference
from a point-to-point transmission. A general achievable rate region
$\mathcal{R}$ based on rate splitting, superposition coding and binning at the
broadcast transmitter and rate splitting at the interfering transmitter is
derived. Under two partial order broadcast conditions {\em
interference-oblivious less noisy} and {\em interference-cognizant less noisy},
a reduced form of $\mathcal{R}$ is shown to be equivalent to the region based
on a simpler scheme that uses only superposition coding at the broadcast
transmitter. Furthermore, the capacity regions of DM-BIC under the two partial
order broadcast conditions are characterized respectively for the strong and
very strong interference conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4504</identifier>
 <datestamp>2012-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4504</id><created>2012-06-19</created><authors><author><keyname>Chilton</keyname><forenames>Chris</forenames></author><author><keyname>Kwiatkowska</keyname><forenames>Marta</forenames></author><author><keyname>Wang</keyname><forenames>Xu</forenames></author></authors><title>Revisiting Timed Specification Theories: A Linear-Time Perspective</title><categories>cs.SE cs.LO cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the setting of component-based design for real-time systems with
critical timing constraints. Based on our earlier work, we propose a
compositional specification theory for timed automata with I/O distinction,
which supports substitutive refinement. Our theory provides the operations of
parallel composition for composing components at run-time, logical
conjunction/disjunction for independent development, and quotient for
incremental synthesis. The key novelty of our timed theory lies in a weakest
congruence preserving safety as well as bounded liveness properties. We show
that the congruence can be characterised by two linear-time semantics,
timed-traces and timed-strategies, the latter of which is derived from a
game-based interpretation of timed interaction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4509</identifier>
 <datestamp>2012-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4509</id><created>2012-06-20</created><authors><author><keyname>Franceschelli</keyname><forenames>Mauro</forenames></author><author><keyname>Gasparri</keyname><forenames>Andrea</forenames></author><author><keyname>Giua</keyname><forenames>Alessandro</forenames></author><author><keyname>Seatzu</keyname><forenames>Carla</forenames></author></authors><title>Decentralized Estimation of Laplacian Eigenvalues in Multi-Agent Systems</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a decentralized algorithm to estimate the
eigenvalues of the Laplacian matrix that encodes the network topology of a
multi-agent system. We consider network topologies modeled by undirected
graphs. The basic idea is to provide a local interaction rule among agents so
that their state trajectory is a linear combination of sinusoids oscillating
only at frequencies function of the eigenvalues of the Laplacian matrix. In
this way, the problem of decentralized estimation of the eigenvalues is mapped
into a standard signal processing problem in which the unknowns are the finite
number of frequencies at which the signal oscillates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4513</identifier>
 <datestamp>2012-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4513</id><created>2012-06-20</created><authors><author><keyname>Khalili</keyname><forenames>Mehdi</forenames></author><author><keyname>Asatryan</keyname><forenames>David</forenames></author></authors><title>Improved DWT Based Watermarking Using JPEG-YCbCr</title><categories>cs.CR cs.MM</categories><comments>4 Pages, 5 Figures, 3 Tables</comments><msc-class>68U10, 68U20, 65C20, 94A08, 94A24, 94A60, 11T71, 14G50, 68P25, 81P94</msc-class><acm-class>D.4.6; K.6.5; K.4.2</acm-class><journal-ref>CSIT Conference, Yerevan, Armenia, September 26-30, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a blind, Secure, imperceptible and robust watermarking
algorithm based on wavelet transform domain is proposed in which for more
security, the watermark W is converted to a sequence and then a random binary
sequence R of size n is adopted to encrypt the watermark, where n is the size
of the watermark image. Afterwards, the encrypted watermark sequence W1 is
generated by executing exclusive-OR operation on W and R. This generated
watermark embeds into low frequency selected coefficients of Y channel wavelet
decomposition of JPEG-YCbCr using LSB insertion technique. The experimental
results show that the proposed algorithm increases the security and
imperceptibility of watermark and has better robustness against wavelet
compression and cropping attacks compared to the earlier work in [1].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4520</identifier>
 <datestamp>2012-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4520</id><created>2012-06-20</created><authors><author><keyname>Khalili</keyname><forenames>Mehdi</forenames></author><author><keyname>Asatryan</keyname><forenames>David</forenames></author></authors><title>Effective Digital Image Watermarking in YCbCr Color Space Accompanied by
  Presenting a Novel Technique Using DWT</title><categories>cs.CR cs.MM</categories><comments>12 Pages, 4 Figures, 8 Tables</comments><msc-class>68U10, 68U20, 65C20, 94A08, 94A24, 94A60, 11T71, 14G50, 68P25, 81P94</msc-class><acm-class>D.4.6; K.6.5; K.4.2</acm-class><journal-ref>Mathematical Problems of Computer Science, 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a quantization based watermark casting and blind watermark
retrieval algorithm operating in YCbCr color space using discrete wavelet
transform (DWT), for ownership verification and image authentication
applications is implemented. This method uses implicit visual masking by
inserting watermark bits into only the wavelet coefficients of high magnitude,
in Y channel of YCbCr color space. A blind watermark retrieval technique that
can detect the embedded watermark without the help from the original
uncorrupted image is devised which is computationally efficient. The new
watermarking algorithm combines and adapts various aspects from existing
watermarking methods. Experimental results show that the proposed technique to
embed watermark provides extra imperceptibility and robustness against various
signal processing attacks in comparison with the same technique in RGB color
space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4522</identifier>
 <datestamp>2012-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4522</id><created>2012-06-20</created><authors><author><keyname>Gooch</keyname><forenames>Phil</forenames></author></authors><title>BADREX: In situ expansion and coreference of biomedical abbreviations
  using dynamic regular expressions</title><categories>cs.CL</categories><comments>6 pages, 2 figures</comments><acm-class>I.2.7; I.1.2; J.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  BADREX uses dynamically generated regular expressions to annotate term
definition-term abbreviation pairs, and corefers unpaired acronyms and
abbreviations back to their initial definition in the text. Against the
Medstract corpus BADREX achieves precision and recall of 98% and 97%, and
against a much larger corpus, 90% and 85%, respectively. BADREX yields improved
performance over previous approaches, requires no training data and allows
runtime customisation of its input parameters. BADREX is freely available from
https://github.com/philgooch/BADREX-Biomedical-Abbreviation-Expander as a
plugin for the General Architecture for Text Engineering (GATE) framework and
is licensed under the GPLv3.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4531</identifier>
 <datestamp>2012-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4531</id><created>2012-06-20</created><authors><author><keyname>Barcelo</keyname><forenames>Jaume</forenames></author><author><keyname>Bellalta</keyname><forenames>Boris</forenames></author><author><keyname>Cano</keyname><forenames>Cristina</forenames></author><author><keyname>Faridi</keyname><forenames>Azadeh</forenames></author><author><keyname>Oliver</keyname><forenames>Miquel</forenames></author></authors><title>On the Distributed Construction of a Collision-Free Schedule in
  Multi-Hop Packet Radio Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a protocol that distributively constructs a
collision-free schedule for multi-hop packet radio networks in the presence of
hidden terminals. As a preliminary step, each wireless station computes the
schedule length after gathering information about the number of flows in its
neighbourhood. Then, a combination of deterministic and random backoffs are
used to reach a collision-free schedule. A deterministic backoff is used after
successful transmissions and a random backoff is used otherwise. It is
explained that the short acknowledgement control packets can easily result in
channel time fragmentation and, to avoid this, the use of link layer delayed
acknowledgements is advocated and implemented. The performance results show
that a collision-free protocol easily outperforms a collision-prone protocol
such as Aloha. The time that is required for the network to converge to a
collision-free schedule is assessed by means of simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4547</identifier>
 <datestamp>2012-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4547</id><created>2012-06-20</created><authors><author><keyname>Ahrens</keyname><forenames>Benedikt</forenames></author></authors><title>Initiality for Typed Syntax and Semantics</title><categories>cs.LO</categories><comments>presented at WoLLIC 2012, 15 pages</comments><journal-ref>Logic, Language, Information and Computation, LNCS 7456 (2012),
  pp. 127-141, http://www.springerlink.com/content/n180nt1qxg76755g/</journal-ref><doi>10.1007/978-3-642-32621-9_10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an algebraic characterization of the syntax and semantics of a class
of simply-typed languages, such as the language PCF: we characterize
simply-typed binding syntax equipped with reduction rules via a universal
property, namely as the initial object of some category. For this purpose, we
employ techniques developed in two previous works: in [2], we model syntactic
translations between languages over different sets of types as initial
morphisms in a category of models. In [1], we characterize untyped syntax with
reduction rules as initial object in a category of models. In the present work,
we show that those techniques are modular enough to be combined: we thus
characterize simply-typed syntax with reduction rules as initial object in a
category. The universal property yields an operator which allows to specify
translations - that are semantically faithful by construction - between
languages over possibly different sets of types.
  We specify a language by a 2-signature, that is, a signature on two levels:
the syntactic level specifies the types and terms of the language, and
associates a type to each term. The semantic level specifies, through
inequations, reduction rules on the terms of the language. To any given
2-signature we associate a category of models. We prove that this category has
an initial object, which integrates the types and terms freely generated by the
2-signature, and the reduction relation on those terms generated by the given
inequations. We call this object the (programming) language generated by the
2-signature.
  [1] Ahrens, B.: Modules over relative monads for syntax and semantics (2011),
arXiv:1107.5252, to be published in Math. Struct. in Comp. Science
  [2] Ahrens, B.: Extended Initiality for Typed Abstract Syntax. Logical
Methods in Computer Science 8(2), 1-35 (2012)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4555</identifier>
 <datestamp>2012-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4555</id><created>2012-06-20</created><updated>2012-07-08</updated><authors><author><keyname>Duda</keyname><forenames>Jarek</forenames></author></authors><title>Optimal compression of hash-origin prefix trees</title><categories>cs.IT cs.DB cs.DS math.CO math.IT</categories><comments>13 pages, 3 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a common problem of operating on hash values of elements of some
database. In this paper there will be analyzed informational content of such
general task and how to practically approach such found lower boundaries.
Minimal prefix tree which distinguish elements turns out to require
asymptotically only about 2.77544 bits per element, while standard approaches
use a few times more. While being certain of working inside the database, the
cost of distinguishability can be reduced further to about 2.33275 bits per
elements. Increasing minimal depth of nodes to reduce probability of false
positives leads to simple relation with average depth of such random tree,
which is asymptotically larger by about 1.33275 bits than lg(n) of the perfect
binary tree. This asymptotic case can be also seen as a way to optimally encode
n large unordered numbers - saving lg(n!) bits of information about their
ordering, which can be the major part of contained information. This ability
itself allows to reduce memory requirements even to about 0.693 of required in
Bloom filter for the same false positive probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4556</identifier>
 <datestamp>2012-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4556</id><created>2012-06-20</created><authors><author><keyname>Ahrens</keyname><forenames>Benedikt</forenames></author></authors><title>Initiality for Typed Syntax and Semantics</title><categories>cs.LO</categories><comments>215 pages, 2012, PhD thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this thesis we give an algebraic characterization of the syntax and
semantics of simply-typed languages. More precisely, we characterize
simply-typed binding syntax equipped with reduction rules via a universal
property, namely as the initial object of some category. We specify a language
by a 2-signature ({\Sigma}, A), that is, a signature on two levels: the
syntactic level {\Sigma} specifies the sorts and terms of the language, and
associates a sort to each term. The semantic level A specifies, through
inequations, reduction rules on the terms of the language. To any given
2-signature ({\Sigma}, A) we associate a category of &quot;models&quot; of ({\Sigma}, A).
We prove that this category has an initial object, which integrates the terms
freely generated by {\Sigma} and the reduction relation - on those terms -
generated by A. We call this object the programming language generated by
({\Sigma}, A).
  Initiality provides an iteration principle which allows to specify
translations on the syntax, possibly to a language over different sorts.
Furthermore, translations specified via the iteration principle are by
construction type-safe and faithful with respect to reduction.
  To illustrate our results, we consider two examples extensively: firstly, we
specify a double negation translation from classical to intuitionistic
propositional logic via the category-theoretic iteration principle. Secondly,
we specify a translation from PCF to the untyped lambda calculus which is
faithful with respect to reduction in the source and target languages.
  In a second part, we formalize some of our initiality theorems in the proof
assistant Coq. The implementation yields a machinery which, when given a
2-signature, returns an implementation of its associated abstract syntax
together with certified substitution operation, iteration operator and a
reduction relation generated by the specified reduction rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4557</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4557</id><created>2012-06-20</created><updated>2013-04-02</updated><authors><author><keyname>Fujie</keyname><forenames>Ryo</forenames></author><author><keyname>Aihara</keyname><forenames>Kazuyuki</forenames></author><author><keyname>Masuda</keyname><forenames>Naoki</forenames></author></authors><title>A model of competition among more than two languages</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>28 pages, 7 figures</comments><journal-ref>Journal of Statistical Physics 151, 289-303 (2013)</journal-ref><doi>10.1007/s10955-012-0613-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend the Abrams-Strogatz model for competition between two languages
[Nature 424, 900 (2003)] to the case of n(&gt;=2) competing states (i.e.,
languages). Although the Abrams-Strogatz model for n=2 can be interpreted as
modeling either majority preference or minority aversion, the two mechanisms
are distinct when n&gt;=3. We find that the condition for the coexistence of
different states is independent of n under the pure majority preference,
whereas it depends on n under the pure minority aversion. We also show that the
stable coexistence equilibrium and stable monopoly equilibria can be
multistable under the minority aversion and not under the majority preference.
Furthermore, we obtain the phase diagram of the model when the effects of the
majority preference and minority aversion are mixed, under the condition that
different states have the same attractiveness. We show that the multistability
is a generic property of the model facilitated by large n.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4560</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4560</id><created>2012-06-18</created><authors><author><keyname>Kalaitzis</keyname><forenames>Alfredo</forenames><affiliation>University of Sheffield</affiliation></author><author><keyname>Lawrence</keyname><forenames>Neil</forenames><affiliation>University of Sheffield</affiliation></author></authors><title>Residual Component Analysis: Generalising PCA for more flexible
  inference in linear-Gaussian models</title><categories>cs.LG stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic principal component analysis (PPCA) seeks a low dimensional
representation of a data set in the presence of independent spherical Gaussian
noise. The maximum likelihood solution for the model is an eigenvalue problem
on the sample covariance matrix. In this paper we consider the situation where
the data variance is already partially explained by other actors, for example
sparse conditional dependencies between the covariates, or temporal
correlations leaving some residual variance. We decompose the residual variance
into its components through a generalised eigenvalue problem, which we call
residual component analysis (RCA). We explore a range of new algorithms that
arise from the framework, including one that factorises the covariance of a
Gaussian density into a low-rank and a sparse-inverse component. We illustrate
the ideas on the recovery of a protein-signaling network, a gene expression
time-series data set and the recovery of the human skeleton from motion capture
3-D cloud data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4564</identifier>
 <datestamp>2012-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4564</id><created>2012-06-20</created><authors><author><keyname>Lohmann</keyname><forenames>Peter</forenames></author></authors><title>Computational Aspects of Dependence Logic</title><categories>cs.LO cs.CC</categories><comments>PhD thesis; 138 pages (110 main matter)</comments><acm-class>F.2.2; F.4.1; F.1.3; D.2.4</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this thesis (modal) dependence logic is investigated. It was introduced in
2007 by Jouko V\&quot;a\&quot;aan\&quot;anen as an extension of first-order (resp. modal)
logic by the dependence operator =(). For first-order (resp. propositional)
variables x_1,...,x_n, =(x_1,...,x_n) intuitively states that the value of x_n
is determined by those of x_1,...,x_n-1.
  We consider fragments of modal dependence logic obtained by restricting the
set of allowed modal and propositional connectives. We classify these fragments
with respect to the complexity of their satisfiability and model-checking
problems. For satisfiability we obtain complexity degrees from P over NP,
Sigma_P^2 and PSPACE up to NEXP, while for model-checking we only classify the
fragments with respect to their tractability, i.e. we either show
NP-completeness or containment in P.
  We then study the extension of modal dependence logic by intuitionistic
implication. For this extension we again classify the complexity of the
model-checking problem for its fragments. Here we obtain complexity degrees
from P over NP and coNP up to PSPACE.
  Finally, we analyze first-order dependence logic, independence-friendly logic
and their two-variable fragments. We prove that satisfiability for two-variable
dependence logic is NEXP-complete, whereas for two-variable
independence-friendly logic it is undecidable; and use this to prove that the
latter is also more expressive than the former.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4572</identifier>
 <datestamp>2013-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4572</id><created>2012-06-20</created><updated>2013-04-14</updated><authors><author><keyname>Willms</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>Autocorrelations of Binary Sequences and Run Structure</title><categories>cs.IT math.CO math.IT</categories><comments>[v3]: minor revisions, accepted for publication in IEEE Trans. Inf.
  Theory, 17 pages</comments><msc-class>94A55, 68P30</msc-class><journal-ref>IEEE Transactions on Information Theory, vol.59, no.8,
  pp.4985-4993, Aug. 2013</journal-ref><doi>10.1109/TIT.2013.2259293</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the connection between the autocorrelation of a binary sequence
and its run structure given by the run length encoding. We show that both the
periodic and the aperiodic autocorrelation of a binary sequence can be
formulated in terms of the run structure. The run structure is given by the
consecutive runs of the sequence. Let C=(C(0), C(1),...,C(n)) denote the
autocorrelation vector of a binary sequence. We prove that the kth component of
the second order difference operator of C can be directly calculated by using
the consecutive runs of total length k. In particular this shows that the kth
autocorrelation is already determined by all consecutive runs of total length
L&lt;k. In the aperiodic case we show how the run vector R can be efficiently
calculated and give a characterization of skew-symmetric sequences in terms of
their run length encoding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4581</identifier>
 <datestamp>2014-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4581</id><created>2012-06-20</created><updated>2014-01-17</updated><authors><author><keyname>Blumberg</keyname><forenames>Andrew J.</forenames></author><author><keyname>Gal</keyname><forenames>Itamar</forenames></author><author><keyname>Mandell</keyname><forenames>Michael A.</forenames></author><author><keyname>Pancia</keyname><forenames>Matthew</forenames></author></authors><title>Robust statistics, hypothesis testing, and confidence intervals for
  persistent homology on metric measure spaces</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study distributions of persistent homology barcodes associated to taking
subsamples of a fixed size from metric measure spaces. We show that such
distributions provide robust invariants of metric measure spaces, and
illustrate their use in hypothesis testing and providing confidence intervals
for topological data analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4582</identifier>
 <datestamp>2012-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4582</id><created>2012-06-20</created><authors><author><keyname>Khalili</keyname><forenames>Mehdi</forenames></author></authors><title>A Comparison between Digital Image Watermarking in Tow Different Color
  Spaces Using DWT2</title><categories>cs.CR cs.MM</categories><comments>5 pages, 4 Figures, 6 Tables. arXiv admin note: text overlap with
  arXiv:1007.5136 by other authors</comments><msc-class>68U10, 68U20, 65C20, 94A08, 94A24, 94A60, 11T71, 14G50, 68P25, 81P94</msc-class><acm-class>D.4.6; K.6.5; K.4.2</acm-class><journal-ref>Computer Science and Information Technologies, 28 September - 2
  October, 2009, Yerevan, Armenia</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel digital watermarking for ownership verification and image
authentication applications using discrete wavelet transform (DWT) is proposed
in this paper. Most previous proposed watermarking algorithms embed sequences
of random numbers as watermarks. Here binary images are taken as watermark for
embedding. In the proposed approach, the host image is converted into the YCbCr
color space and then its Y channel decomposed into wavelet coefficients. The
selected approximation coefficients are quantized and then their four least
significant bits of the quantized coefficients are replaced by the watermark
using LSB insertion technique. At last, the watermarked image is synthesized
from the changed and unchanged DWT coefficients. The experiments show that the
proposed approach provides extra imperceptibility and robustness against
wavelet compression compared to the traditional embedding methods in RGB color
space. Moreover, the proposed approach has no need of the original image to
extract watermarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4588</identifier>
 <datestamp>2012-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4588</id><created>2012-05-03</created><authors><author><keyname>Ghosh</keyname><forenames>Avishek</forenames></author><author><keyname>Ghosh</keyname><forenames>Arnab</forenames></author><author><keyname>Chowdhury</keyname><forenames>Arkabandhu</forenames></author><author><keyname>Hazra</keyname><forenames>Jubin</forenames></author></authors><title>An Evolutionary Approach to Drug-Design Using Quantam Binary Particle
  Swarm Optimization Algorithm</title><categories>cs.NE cs.CE</categories><comments>4 pages, 6 figures (Published in IEEE SCEECS 2012). arXiv admin note:
  substantial text overlap with arXiv:1205.6412</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present work provides a new approach to evolve ligand structures which
represent possible drug to be docked to the active site of the target protein.
The structure is represented as a tree where each non-empty node represents a
functional group. It is assumed that the active site configuration of the
target protein is known with position of the essential residues. In this paper
the interaction energy of the ligands with the protein target is minimized.
Moreover, the size of the tree is difficult to obtain and it will be different
for different active sites. To overcome the difficulty, a variable tree size
configuration is used for designing ligands. The optimization is done using a
quantum discrete PSO. The result using fixed length and variable length
configuration are compared.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4598</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4598</id><created>2012-06-20</created><updated>2013-07-20</updated><authors><author><keyname>Vlad</keyname><forenames>Serban E.</forenames></author></authors><title>Defining the symmetry of the universal semi-regular autonomous
  asynchronous systems</title><categories>cs.OH</categories><comments>16 pages, 11 figures</comments><msc-class>94C10</msc-class><journal-ref>Symmetry 2012, 4, 116-128;</journal-ref><doi>10.3390/sym4010116</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The regular autonomous asynchronous systems are the non-deterministic Boolean
dynamical systems and universality means the greatest in the sense of the
inclusion. The paper gives four definitions of symmetry of these systems in a
slightly more general framework, called semi-regularity and also many examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4599</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4599</id><created>2012-06-18</created><authors><author><keyname>Takeda</keyname><forenames>Akiko</forenames><affiliation>Keio University</affiliation></author><author><keyname>Mitsugi</keyname><forenames>Hiroyuki</forenames><affiliation>Keio University</affiliation></author><author><keyname>Kanamori</keyname><forenames>Takafumi</forenames><affiliation>Nagoya University</affiliation></author></authors><title>A Unified Robust Classification Model</title><categories>cs.LG stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A wide variety of machine learning algorithms such as support vector machine
(SVM), minimax probability machine (MPM), and Fisher discriminant analysis
(FDA), exist for binary classification. The purpose of this paper is to provide
a unified classification model that includes the above models through a robust
optimization approach. This unified model has several benefits. One is that the
extensions and improvements intended for SVM become applicable to MPM and FDA,
and vice versa. Another benefit is to provide theoretical results to above
learning methods at once by dealing with the unified model. We give a
statistical interpretation of the unified classification model and propose a
non-convex optimization algorithm that can be applied to non-convex variants of
existing learning methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4600</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4600</id><created>2012-06-18</created><authors><author><keyname>Dundar</keyname><forenames>Murat</forenames><affiliation>IUPUI</affiliation></author><author><keyname>Akova</keyname><forenames>Ferit</forenames><affiliation>IUPUI</affiliation></author><author><keyname>Qi</keyname><forenames>Alan</forenames><affiliation>Purdue</affiliation></author><author><keyname>Rajwa</keyname><forenames>Bartek</forenames><affiliation>Purdue</affiliation></author></authors><title>Bayesian Nonexhaustive Learning for Online Discovery and Modeling of
  Emerging Classes</title><categories>cs.LG stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a framework for online inference in the presence of a
nonexhaustively defined set of classes that incorporates supervised
classification with class discovery and modeling. A Dirichlet process prior
(DPP) model defined over class distributions ensures that both known and
unknown class distributions originate according to a common base distribution.
In an attempt to automatically discover potentially interesting class
formations, the prior model is coupled with a suitably chosen data model, and
sequential Monte Carlo sampling is used to perform online inference. Our
research is driven by a biodetection application, where a new class of pathogen
may suddenly appear, and the rapid increase in the number of samples
originating from this class indicates the onset of an outbreak.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4601</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4601</id><created>2012-06-18</created><authors><author><keyname>Zhong</keyname><forenames>Wenliang</forenames><affiliation>HKUST</affiliation></author><author><keyname>Kwok</keyname><forenames>James</forenames><affiliation>HKUST</affiliation></author></authors><title>Convex Multitask Learning with Flexible Task Clusters</title><categories>cs.LG stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditionally, multitask learning (MTL) assumes that all the tasks are
related. This can lead to negative transfer when tasks are indeed incoherent.
Recently, a number of approaches have been proposed that alleviate this problem
by discovering the underlying task clusters or relationships. However, they are
limited to modeling these relationships at the task level, which may be
restrictive in some applications. In this paper, we propose a novel MTL
formulation that captures task relationships at the feature-level. Depending on
the interactions among tasks and features, the proposed method construct
different task clusters for different features, without even the need of
pre-specifying the number of clusters. Computationally, the proposed
formulation is strongly convex, and can be efficiently solved by accelerated
proximal methods. Experiments are performed on a number of synthetic and
real-world data sets. Under various degrees of task relationships, the accuracy
of the proposed method is consistently among the best. Moreover, the
feature-specific task clusters obtained agree with the known/plausible task
structures of the data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4602</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4602</id><created>2012-06-18</created><authors><author><keyname>Hennig</keyname><forenames>Philipp</forenames><affiliation>MPI Intelligent Systems</affiliation></author><author><keyname>Kiefel</keyname><forenames>Martin</forenames><affiliation>MPI for Intelligent Systems</affiliation></author></authors><title>Quasi-Newton Methods: A New Direction</title><categories>cs.NA cs.LG stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Four decades after their invention, quasi-Newton methods are still state of
the art in unconstrained numerical optimization. Although not usually
interpreted thus, these are learning algorithms that fit a local quadratic
approximation to the objective function. We show that many, including the most
popular, quasi-Newton methods can be interpreted as approximations of Bayesian
linear regression under varying prior assumptions. This new notion elucidates
some shortcomings of classical algorithms, and lights the way to a novel
nonparametric quasi-Newton method, which is able to make more efficient use of
available information at computational cost similar to its predecessors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4603</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4603</id><created>2012-06-18</created><authors><author><keyname>Weston</keyname><forenames>Jason</forenames><affiliation>Google</affiliation></author><author><keyname>Wang</keyname><forenames>Chong</forenames><affiliation>Princeton University</affiliation></author><author><keyname>Weiss</keyname><forenames>Ron</forenames><affiliation>Google</affiliation></author><author><keyname>Berenzweig</keyname><forenames>Adam</forenames><affiliation>Google</affiliation></author></authors><title>Latent Collaborative Retrieval</title><categories>cs.IR cs.AI</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Retrieval tasks typically require a ranking of items given a query.
Collaborative filtering tasks, on the other hand, learn to model user's
preferences over items. In this paper we study the joint problem of
recommending items to a user with respect to a given query, which is a
surprisingly common task. This setup differs from the standard collaborative
filtering one in that we are given a query x user x item tensor for training
instead of the more traditional user x item matrix. Compared to document
retrieval we do have a query, but we may or may not have content features (we
will consider both cases) and we can also take account of the user's profile.
We introduce a factorized model for this new task that optimizes the top-ranked
items returned for the given query and user. We report empirical results where
it outperforms several baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4604</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4604</id><created>2012-06-18</created><authors><author><keyname>Eban</keyname><forenames>Elad</forenames><affiliation>Hebrew University</affiliation></author><author><keyname>Birnbaum</keyname><forenames>Aharon</forenames><affiliation>Hebrew University</affiliation></author><author><keyname>Shalev-Shwartz</keyname><forenames>Shai</forenames><affiliation>Hebrew University</affiliation></author><author><keyname>Globerson</keyname><forenames>Amir</forenames><affiliation>Hebrew University</affiliation></author></authors><title>Learning the Experts for Online Sequence Prediction</title><categories>cs.LG cs.AI</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online sequence prediction is the problem of predicting the next element of a
sequence given previous elements. This problem has been extensively studied in
the context of individual sequence prediction, where no prior assumptions are
made on the origin of the sequence. Individual sequence prediction algorithms
work quite well for long sequences, where the algorithm has enough time to
learn the temporal structure of the sequence. However, they might give poor
predictions for short sequences. A possible remedy is to rely on the general
model of prediction with expert advice, where the learner has access to a set
of $r$ experts, each of which makes its own predictions on the sequence. It is
well known that it is possible to predict almost as well as the best expert if
the sequence length is order of $\log(r)$. But, without firm prior knowledge on
the problem, it is not clear how to choose a small set of {\em good} experts.
In this paper we describe and analyze a new algorithm that learns a good set of
experts using a training set of previously observed sequences. We demonstrate
the merits of our approach by applying it on the task of click prediction on
the web.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4605</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4605</id><created>2012-06-18</created><authors><author><keyname>Wang</keyname><forenames>Jiabing</forenames><affiliation>South China University of Tech</affiliation></author><author><keyname>Chen</keyname><forenames>Jiaye</forenames><affiliation>South China University of Technology</affiliation></author></authors><title>Clustering to Maximize the Ratio of Split to Diameter</title><categories>cs.DS cs.DM</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a weighted and complete graph G = (V, E), V denotes the set of n
objects to be clustered, and the weight d(u, v) associated with an edge (u, v)
belonging to E denotes the dissimilarity between objects u and v. The diameter
of a cluster is the maximum dissimilarity between pairs of objects in the
cluster, and the split of a cluster is the minimum dissimilarity between
objects within the cluster and objects outside the cluster. In this paper, we
propose a new criterion for measuring the goodness of clusters: the ratio of
the minimum split to the maximum diameter, and the objective is to maximize the
ratio. For k = 2, we present an exact algorithm. For k &gt;= 3, we prove that the
problem is NP-hard and present a factor of 2 approximation algorithm on the
precondition that the weights associated with E satisfy the triangle
inequality. The worst-case runtime of both algorithms is O(n^3). We compare the
proposed algorithms with the Normalized Cut by applying them to image
segmentation. The experimental results on both natural and synthetic images
demonstrate the effectiveness of the proposed algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4606</identifier>
 <datestamp>2012-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4606</id><created>2012-06-18</created><authors><author><keyname>Liu</keyname><forenames>Chao</forenames><affiliation>Tencent Inc.</affiliation></author><author><keyname>Wang</keyname><forenames>Yi-Min</forenames><affiliation>Microsoft Research</affiliation></author></authors><title>TrueLabel + Confusions: A Spectrum of Probabilistic Models in Analyzing
  Multiple Ratings</title><categories>cs.LG cs.AI stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper revisits the problem of analyzing multiple ratings given by
different judges. Different from previous work that focuses on distilling the
true labels from noisy crowdsourcing ratings, we emphasize gaining diagnostic
insights into our in-house well-trained judges. We generalize the well-known
DawidSkene model (Dawid &amp; Skene, 1979) to a spectrum of probabilistic models
under the same &quot;TrueLabel + Confusion&quot; paradigm, and show that our proposed
hierarchical Bayesian model, called HybridConfusion, consistently outperforms
DawidSkene on both synthetic and real-world data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4607</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4607</id><created>2012-06-18</created><authors><author><keyname>Zanzotto</keyname><forenames>Fabio Massimo</forenames><affiliation>University of Rome-Tor Vergata</affiliation></author><author><keyname>Dell'Arciprete</keyname><forenames>Lorenzo</forenames><affiliation>University of Rome-Tor Vergata</affiliation></author></authors><title>Distributed Tree Kernels</title><categories>cs.LG stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose the distributed tree kernels (DTK) as a novel
method to reduce time and space complexity of tree kernels. Using a linear
complexity algorithm to compute vectors for trees, we embed feature spaces of
tree fragments in low-dimensional spaces where the kernel computation is
directly done with dot product. We show that DTKs are faster, correlate with
tree kernels, and obtain a statistically similar performance in two natural
language processing tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4608</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4608</id><created>2012-06-18</created><authors><author><keyname>Laue</keyname><forenames>Soeren</forenames><affiliation>Friedrich-Schiller-University</affiliation></author></authors><title>A Hybrid Algorithm for Convex Semidefinite Optimization</title><categories>cs.LG cs.DS cs.NA stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a hybrid algorithm for optimizing a convex, smooth function over
the cone of positive semidefinite matrices. Our algorithm converges to the
global optimal solution and can be used to solve general large-scale
semidefinite programs and hence can be readily applied to a variety of machine
learning problems. We show experimental results on three machine learning
problems (matrix completion, metric learning, and sparse PCA) . Our approach
outperforms state-of-the-art algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4609</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4609</id><created>2012-06-18</created><authors><author><keyname>Memisevic</keyname><forenames>Roland</forenames><affiliation>University of Frankfurt</affiliation></author></authors><title>On multi-view feature learning</title><categories>cs.CV cs.LG stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse coding is a common approach to learning local features for object
recognition. Recently, there has been an increasing interest in learning
features from spatio-temporal, binocular, or other multi-observation data,
where the goal is to encode the relationship between images rather than the
content of a single image. We provide an analysis of multi-view feature
learning, which shows that hidden variables encode transformations by detecting
rotation angles in the eigenspaces shared among multiple image warps. Our
analysis helps explain recent experimental results showing that
transformation-specific features emerge when training complex cell models on
videos. Our analysis also shows that transformation-invariant features can
emerge as a by-product of learning representations of transformations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4610</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4610</id><created>2012-06-18</created><authors><author><keyname>Damianou</keyname><forenames>Andreas</forenames><affiliation>University of Sheffield</affiliation></author><author><keyname>Ek</keyname><forenames>Carl</forenames><affiliation>KTH</affiliation></author><author><keyname>Titsias</keyname><forenames>Michalis</forenames><affiliation>University of Oxford</affiliation></author><author><keyname>Lawrence</keyname><forenames>Neil</forenames><affiliation>University of Sheffield</affiliation></author></authors><title>Manifold Relevance Determination</title><categories>cs.LG cs.CV stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a fully Bayesian latent variable model which
exploits conditional nonlinear(in)-dependence structures to learn an efficient
latent representation. The latent space is factorized to represent shared and
private information from multiple views of the data. In contrast to previous
approaches, we introduce a relaxation to the discrete segmentation and allow
for a &quot;softly&quot; shared latent space. Further, Bayesian techniques allow us to
automatically estimate the dimensionality of the latent spaces. The model is
capable of capturing structure underlying extremely high dimensional spaces.
This is illustrated by modelling unprocessed images with tenths of thousands of
pixels. This also allows us to directly generate novel images from the trained
model by sampling from the discovered latent spaces. We also demonstrate the
model by prediction of human pose in an ambiguous setting. Our Bayesian
framework allows us to perform disambiguation in a principled manner by
including latent space priors which incorporate the dynamic nature of the data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4611</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4611</id><created>2012-06-18</created><authors><author><keyname>Jawanpuria</keyname><forenames>Pratik</forenames><affiliation>IIT Bombay</affiliation></author><author><keyname>Nath</keyname><forenames>J. Saketha</forenames><affiliation>IIT Bombay</affiliation></author></authors><title>A Convex Feature Learning Formulation for Latent Task Structure
  Discovery</title><categories>cs.LG stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the multi-task learning problem and in the setting where
some relevant features could be shared across few related tasks. Most of the
existing methods assume the extent to which the given tasks are related or
share a common feature space to be known apriori. In real-world applications
however, it is desirable to automatically discover the groups of related tasks
that share a feature space. In this paper we aim at searching the exponentially
large space of all possible groups of tasks that may share a feature space. The
main contribution is a convex formulation that employs a graph-based
regularizer and simultaneously discovers few groups of related tasks, having
close-by task parameters, as well as the feature space shared within each
group. The regularizer encodes an important structure among the groups of tasks
leading to an efficient algorithm for solving it: if there is no feature space
under which a group of tasks has close-by task parameters, then there does not
exist such a feature space for any of its supersets. An efficient active set
algorithm that exploits this simplification and performs a clever search in the
exponentially large space is presented. The algorithm is guaranteed to solve
the proposed formulation (within some precision) in a time polynomial in the
number of groups of related tasks discovered. Empirical results on benchmark
datasets show that the proposed formulation achieves good generalization and
outperforms state-of-the-art multi-task learning algorithms in some cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4612</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4612</id><created>2012-06-18</created><authors><author><keyname>Wang</keyname><forenames>Jialei</forenames><affiliation>NTU</affiliation></author><author><keyname>Zhao</keyname><forenames>Peilin</forenames><affiliation>NTU</affiliation></author><author><keyname>Hoi</keyname><forenames>Steven C. H.</forenames><affiliation>NTU</affiliation></author></authors><title>Exact Soft Confidence-Weighted Learning</title><categories>cs.LG</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new Soft Confidence-Weighted (SCW) online
learning scheme, which enables the conventional confidence-weighted learning
method to handle non-separable cases. Unlike the previous confidence-weighted
learning algorithms, the proposed soft confidence-weighted learning method
enjoys all the four salient properties: (i) large margin training, (ii)
confidence weighting, (iii) capability to handle non-separable data, and (iv)
adaptive margin. Our experimental results show that the proposed SCW algorithms
significantly outperform the original CW algorithm. When comparing with a
variety of state-of-the-art algorithms (including AROW, NAROW and NHERD), we
found that SCW generally achieves better or at least comparable predictive
accuracy, but enjoys significant advantage of computational efficiency (i.e.,
smaller number of updates and lower time cost).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4613</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4613</id><created>2012-06-18</created><authors><author><keyname>Araya</keyname><forenames>Mauricio</forenames><affiliation>LORIA/INRIA</affiliation></author><author><keyname>Buffet</keyname><forenames>Olivier</forenames><affiliation>LORIA/INRIA</affiliation></author><author><keyname>Thomas</keyname><forenames>Vincent</forenames><affiliation>LORIA/INRIA</affiliation></author></authors><title>Near-Optimal BRL using Optimistic Local Transitions</title><categories>cs.AI cs.LG stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model-based Bayesian Reinforcement Learning (BRL) allows a found
formalization of the problem of acting optimally while facing an unknown
environment, i.e., avoiding the exploration-exploitation dilemma. However,
algorithms explicitly addressing BRL suffer from such a combinatorial explosion
that a large body of work relies on heuristic algorithms. This paper introduces
BOLT, a simple and (almost) deterministic heuristic algorithm for BRL which is
optimistic about the transition function. We analyze BOLT's sample complexity,
and show that under certain parameters, the algorithm is near-optimal in the
Bayesian sense with high probability. Then, experimental results highlight the
key differences of this method compared to previous work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4614</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4614</id><created>2012-06-18</created><authors><author><keyname>Niu</keyname><forenames>Gang</forenames><affiliation>Tokyo Institute of Technology</affiliation></author><author><keyname>Dai</keyname><forenames>Bo</forenames><affiliation>Purdue University</affiliation></author><author><keyname>Yamada</keyname><forenames>Makoto</forenames><affiliation>Tokyo Institute of Technology</affiliation></author><author><keyname>Sugiyama</keyname><forenames>Masashi</forenames><affiliation>Tokyo Institute of Technology</affiliation></author></authors><title>Information-theoretic Semi-supervised Metric Learning via Entropy
  Regularization</title><categories>cs.LG stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a general information-theoretic approach called Seraph
(SEmi-supervised metRic leArning Paradigm with Hyper-sparsity) for metric
learning that does not rely upon the manifold assumption. Given the probability
parameterized by a Mahalanobis distance, we maximize the entropy of that
probability on labeled data and minimize it on unlabeled data following entropy
regularization, which allows the supervised and unsupervised parts to be
integrated in a natural and meaningful way. Furthermore, Seraph is regularized
by encouraging a low-rank projection induced from the metric. The optimization
of Seraph is solved efficiently and stably by an EM-like scheme with the
analytical E-Step and convex M-Step. Experiments demonstrate that Seraph
compares favorably with many well-known global and local metric learning
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4615</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4615</id><created>2012-06-18</created><authors><author><keyname>Wang</keyname><forenames>Yingjian</forenames><affiliation>Duke University</affiliation></author><author><keyname>Carin</keyname><forenames>Lawrence</forenames><affiliation>Duke University</affiliation></author></authors><title>Levy Measure Decompositions for the Beta and Gamma Processes</title><categories>stat.ME cs.LG math.ST stat.TH</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop new representations for the Levy measures of the beta and gamma
processes. These representations are manifested in terms of an infinite sum of
well-behaved (proper) beta and gamma distributions. Further, we demonstrate how
these infinite sums may be truncated in practice, and explicitly characterize
truncation errors. We also perform an analysis of the characteristics of
posterior distributions, based on the proposed decompositions. The
decompositions provide new insights into the beta and gamma processes (and
their generalizations), and we demonstrate how the proposed representation
unifies some properties of the two. This paper is meant to provide a rigorous
foundation for and new perspectives on Levy processes, as these are of
increasing importance in machine learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4616</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4616</id><created>2012-06-18</created><authors><author><keyname>Wulsin</keyname><forenames>Drausin</forenames><affiliation>University of Pennsylvania</affiliation></author><author><keyname>Jensen</keyname><forenames>Shane</forenames><affiliation>University of Pennsylvania</affiliation></author><author><keyname>Litt</keyname><forenames>Brian</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>A Hierarchical Dirichlet Process Model with Multiple Levels of
  Clustering for Human EEG Seizure Modeling</title><categories>stat.AP cs.LG stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Driven by the multi-level structure of human intracranial
electroencephalogram (iEEG) recordings of epileptic seizures, we introduce a
new variant of a hierarchical Dirichlet Process---the multi-level clustering
hierarchical Dirichlet Process (MLC-HDP)---that simultaneously clusters
datasets on multiple levels. Our seizure dataset contains brain activity
recorded in typically more than a hundred individual channels for each seizure
of each patient. The MLC-HDP model clusters over channels-types, seizure-types,
and patient-types simultaneously. We describe this model and its implementation
in detail. We also present the results of a simulation study comparing the
MLC-HDP to a similar model, the Nested Dirichlet Process and finally
demonstrate the MLC-HDP's use in modeling seizures across multiple patients. We
find the MLC-HDP's clustering to be comparable to independent human physician
clusterings. To our knowledge, the MLC-HDP model is the first in the epilepsy
literature capable of clustering seizures within and between patients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4617</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4617</id><created>2012-06-18</created><authors><author><keyname>Levine</keyname><forenames>Sergey</forenames><affiliation>Stanford University</affiliation></author><author><keyname>Koltun</keyname><forenames>Vladlen</forenames><affiliation>Stanford University</affiliation></author></authors><title>Continuous Inverse Optimal Control with Locally Optimal Examples</title><categories>cs.LG cs.AI stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inverse optimal control, also known as inverse reinforcement learning, is the
problem of recovering an unknown reward function in a Markov decision process
from expert demonstrations of the optimal policy. We introduce a probabilistic
inverse optimal control algorithm that scales gracefully with task
dimensionality, and is suitable for large, continuous domains where even
computing a full policy is impractical. By using a local approximation of the
reward function, our method can also drop the assumption that the
demonstrations are globally optimal, requiring only local optimality. This
allows it to learn from examples that are unsuitable for prior methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4618</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4618</id><created>2012-06-18</created><authors><author><keyname>Liu</keyname><forenames>Wei</forenames><affiliation>Columbia University</affiliation></author><author><keyname>Wang</keyname><forenames>Jun</forenames><affiliation>IBM T. J. Watson Research Center</affiliation></author><author><keyname>Mu</keyname><forenames>Yadong</forenames><affiliation>Columbia University</affiliation></author><author><keyname>Kumar</keyname><forenames>Sanjiv</forenames><affiliation>Google</affiliation></author><author><keyname>Chang</keyname><forenames>Shih-Fu</forenames><affiliation>Columbia University</affiliation></author></authors><title>Compact Hyperplane Hashing with Bilinear Functions</title><categories>cs.LG stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hyperplane hashing aims at rapidly searching nearest points to a hyperplane,
and has shown practical impact in scaling up active learning with SVMs.
Unfortunately, the existing randomized methods need long hash codes to achieve
reasonable search accuracy and thus suffer from reduced search speed and large
memory overhead. To this end, this paper proposes a novel hyperplane hashing
technique which yields compact hash codes. The key idea is the bilinear form of
the proposed hash functions, which leads to higher collision probability than
the existing hyperplane hash functions when using random projections. To
further increase the performance, we propose a learning based framework in
which the bilinear functions are directly learned from the data. This results
in short yet discriminative codes, and also boosts the search performance over
the random projection based solutions. Large-scale active learning experiments
carried out on two datasets with up to one million samples demonstrate the
overall superiority of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4619</identifier>
 <datestamp>2012-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4619</id><created>2012-06-18</created><authors><author><keyname>Zhang</keyname><forenames>Kai</forenames><affiliation>Siemens</affiliation></author><author><keyname>Lan</keyname><forenames>Liang</forenames><affiliation>temple university</affiliation></author><author><keyname>Liu</keyname><forenames>Jun</forenames><affiliation>Siemens</affiliation></author><author><keyname>Rauber</keyname><forenames>andreas</forenames><affiliation>TU Wien</affiliation></author><author><keyname>Moerchen</keyname><forenames>Fabian</forenames><affiliation>Siemens Corporate Research and Technology</affiliation></author></authors><title>Inductive Kernel Low-rank Decomposition with Priors: A Generalized
  Nystrom Method</title><categories>cs.LG</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low-rank matrix decomposition has gained great popularity recently in scaling
up kernel methods to large amounts of data. However, some limitations could
prevent them from working effectively in certain domains. For example, many
existing approaches are intrinsically unsupervised, which does not incorporate
side information (e.g., class labels) to produce task specific decompositions;
also, they typically work &quot;transductively&quot;, i.e., the factorization does not
generalize to new samples, so the complete factorization needs to be recomputed
when new samples become available. To solve these problems, in this paper we
propose an&quot;inductive&quot;-flavored method for low-rank kernel decomposition with
priors. We achieve this by generalizing the Nystr\&quot;om method in a novel way. On
the one hand, our approach employs a highly flexible, nonparametric structure
that allows us to generalize the low-rank factors to arbitrarily new samples;
on the other hand, it has linear time and space complexities, which can be
orders of magnitudes faster than existing approaches and renders great
efficiency in learning a low-rank kernel decomposition. Empirical results
demonstrate the efficacy and efficiency of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4620</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4620</id><created>2012-06-18</created><authors><author><keyname>Nowozin</keyname><forenames>Sebastian</forenames><affiliation>Microsoft Research Cambridge</affiliation></author></authors><title>Improved Information Gain Estimates for Decision Tree Induction</title><categories>cs.LG stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ensembles of classification and regression trees remain popular machine
learning methods because they define flexible non-parametric models that
predict well and are computationally efficient both during training and
testing. During induction of decision trees one aims to find predicates that
are maximally informative about the prediction target. To select good
predicates most approaches estimate an information-theoretic scoring function,
the information gain, both for classification and regression problems. We point
out that the common estimation procedures are biased and show that by replacing
them with improved estimators of the discrete and the differential entropy we
can obtain better decision trees. In effect our modifications yield improved
predictive performance and are simple to implement in any decision tree code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4621</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4621</id><created>2012-06-18</created><authors><author><keyname>Stulp</keyname><forenames>Freek</forenames><affiliation>Ecole Nationale Superieure de Techniques Avancees</affiliation></author><author><keyname>Sigaud</keyname><forenames>Olivier</forenames><affiliation>Universite Pierre et Marie Curie</affiliation></author></authors><title>Path Integral Policy Improvement with Covariance Matrix Adaptation</title><categories>cs.LG</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been a recent focus in reinforcement learning on addressing
continuous state and action problems by optimizing parameterized policies. PI2
is a recent example of this approach. It combines a derivation from first
principles of stochastic optimal control with tools from statistical estimation
theory. In this paper, we consider PI2 as a member of the wider family of
methods which share the concept of probability-weighted averaging to
iteratively update parameters to optimize a cost function. We compare PI2 to
other members of the same family - Cross-Entropy Methods and CMAES - at the
conceptual level and in terms of performance. The comparison suggests the
derivation of a novel algorithm which we call PI2-CMA for &quot;Path Integral Policy
Improvement with Covariance Matrix Adaptation&quot;. PI2-CMA's main advantage is
that it determines the magnitude of the exploration noise automatically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4622</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4622</id><created>2012-06-18</created><authors><author><keyname>Defazio</keyname><forenames>Aaron</forenames><affiliation>ANU</affiliation></author><author><keyname>Caetano</keyname><forenames>Tiberio</forenames><affiliation>NICTA and Australian National University</affiliation></author></authors><title>A Graphical Model Formulation of Collaborative Filtering Neighbourhood
  Methods with Fast Maximum Entropy Training</title><categories>cs.LG cs.IR stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Item neighbourhood methods for collaborative filtering learn a weighted graph
over the set of items, where each item is connected to those it is most similar
to. The prediction of a user's rating on an item is then given by that rating
of neighbouring items, weighted by their similarity. This paper presents a new
neighbourhood approach which we call item fields, whereby an undirected
graphical model is formed over the item graph. The resulting prediction rule is
a simple generalization of the classical approaches, which takes into account
non-local information in the graph, allowing its best results to be obtained
when using drastically fewer edges than other neighbourhood approaches. A fast
approximate maximum entropy training method based on the Bethe approximation is
presented, which uses a simple gradient ascent procedure. When using
precomputed sufficient statistics on the Movielens datasets, our method is
faster than maximum likelihood approaches by two orders of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4623</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4623</id><created>2012-06-18</created><authors><author><keyname>Sun</keyname><forenames>Yi</forenames><affiliation>IDSIA</affiliation></author><author><keyname>Gomez</keyname><forenames>Faustino</forenames><affiliation>IDSIA,</affiliation></author><author><keyname>Schmidhuber</keyname><forenames>Juergen</forenames><affiliation>IDSIA</affiliation></author></authors><title>On the Size of the Online Kernel Sparsification Dictionary</title><categories>cs.LG stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the size of the dictionary constructed from online kernel
sparsification, using a novel formula that expresses the expected determinant
of the kernel Gram matrix in terms of the eigenvalues of the covariance
operator. Using this formula, we are able to connect the cardinality of the
dictionary with the eigen-decay of the covariance operator. In particular, we
show that under certain technical conditions, the size of the dictionary will
always grow sub-linearly in the number of data points, and, as a consequence,
the kernel linear regressor constructed from the resulting dictionary is
consistent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4624</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4624</id><created>2012-06-18</created><authors><author><keyname>Gong</keyname><forenames>Dian</forenames><affiliation>Univ. of Southern California</affiliation></author><author><keyname>Zhao</keyname><forenames>Xuemei</forenames><affiliation>Univ of Southern California</affiliation></author><author><keyname>Medioni</keyname><forenames>Gerard</forenames><affiliation>University of Southern California</affiliation></author></authors><title>Robust Multiple Manifolds Structure Learning</title><categories>cs.LG stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a robust multiple manifolds structure learning (RMMSL) scheme to
robustly estimate data structures under the multiple low intrinsic dimensional
manifolds assumption. In the local learning stage, RMMSL efficiently estimates
local tangent space by weighted low-rank matrix factorization. In the global
learning stage, we propose a robust manifold clustering method based on local
structure learning results. The proposed clustering method is designed to get
the flattest manifolds clusters by introducing a novel curved-level similarity
function. Our approach is evaluated and compared to state-of-the-art methods on
synthetic data, handwritten digit images, human motion capture data and
motorbike videos. We demonstrate the effectiveness of the proposed approach,
which yields higher clustering accuracy, and produces promising results for
challenging tasks of human motion segmentation and motion flow learning from
videos.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4625</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4625</id><created>2012-06-18</created><authors><author><keyname>Nan</keyname><forenames>Ye</forenames><affiliation>NUS</affiliation></author><author><keyname>Chai</keyname><forenames>Kian Ming</forenames><affiliation>DSO National Laboratories</affiliation></author><author><keyname>Lee</keyname><forenames>Wee Sun</forenames><affiliation>NUS</affiliation></author><author><keyname>Chieu</keyname><forenames>Hai Leong</forenames><affiliation>DSO National Laboratories</affiliation></author></authors><title>Optimizing F-measure: A Tale of Two Approaches</title><categories>cs.LG</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  F-measures are popular performance metrics, particularly for tasks with
imbalanced data sets. Algorithms for learning to maximize F-measures follow two
approaches: the empirical utility maximization (EUM) approach learns a
classifier having optimal performance on training data, while the
decision-theoretic approach learns a probabilistic model and then predicts
labels with maximum expected F-measure. In this paper, we investigate the
theoretical justifications and connections for these two approaches, and we
study the conditions under which one approach is preferable to the other using
synthetic and real datasets. Given accurate models, our results suggest that
the two approaches are asymptotically equivalent given large training and test
sets. Nevertheless, empirically, the EUM approach appears to be more robust
against model misspecification, and given a good model, the decision-theoretic
approach appears to be better for handling rare classes and a common domain
adaptation scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4626</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4626</id><created>2012-06-18</created><authors><author><keyname>Li</keyname><forenames>Bin</forenames><affiliation>NTU</affiliation></author><author><keyname>Hoi</keyname><forenames>Steven C. H.</forenames><affiliation>NTU</affiliation></author></authors><title>On-Line Portfolio Selection with Moving Average Reversion</title><categories>cs.CE cs.LG q-fin.PM</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  On-line portfolio selection has attracted increasing interests in machine
learning and AI communities recently. Empirical evidences show that stock's
high and low prices are temporary and stock price relatives are likely to
follow the mean reversion phenomenon. While the existing mean reversion
strategies are shown to achieve good empirical performance on many real
datasets, they often make the single-period mean reversion assumption, which is
not always satisfied in some real datasets, leading to poor performance when
the assumption does not hold. To overcome the limitation, this article proposes
a multiple-period mean reversion, or so-called Moving Average Reversion (MAR),
and a new on-line portfolio selection strategy named &quot;On-Line Moving Average
Reversion&quot; (OLMAR), which exploits MAR by applying powerful online learning
techniques. From our empirical results, we found that OLMAR can overcome the
drawback of existing mean reversion algorithms and achieve significantly better
results, especially on the datasets where the existing mean reversion
algorithms failed. In addition to superior trading performance, OLMAR also runs
extremely fast, further supporting its practical applicability to a wide range
of applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4627</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4627</id><created>2012-06-18</created><authors><author><keyname>Honorio</keyname><forenames>Jean</forenames><affiliation>Stony Brook University</affiliation></author></authors><title>Convergence Rates of Biased Stochastic Optimization for Learning Sparse
  Ising Models</title><categories>cs.LG stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the convergence rate of stochastic optimization of exact (NP-hard)
objectives, for which only biased estimates of the gradient are available. We
motivate this problem in the context of learning the structure and parameters
of Ising models. We first provide a convergence-rate analysis of deterministic
errors for forward-backward splitting (FBS). We then extend our analysis to
biased stochastic errors, by first characterizing a family of samplers and
providing a high probability bound that allows understanding not only FBS, but
also proximal gradient (PG) methods. We derive some interesting conclusions:
FBS requires only a logarithmically increasing number of random samples in
order to converge (although at a very low rate); the required number of random
samples is the same for the deterministic and the biased stochastic setting for
FBS and basic PG; accelerated PG is not guaranteed to converge in the biased
stochastic setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4628</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4628</id><created>2012-06-18</created><authors><author><keyname>Feng</keyname><forenames>Jiashi</forenames><affiliation>NUS</affiliation></author><author><keyname>Xu</keyname><forenames>Huan</forenames><affiliation>NUS</affiliation></author><author><keyname>Yan</keyname><forenames>Shuicheng</forenames><affiliation>NUS</affiliation></author></authors><title>Robust PCA in High-dimension: A Deterministic Approach</title><categories>cs.LG stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider principal component analysis for contaminated data-set in the
high dimensional regime, where the dimensionality of each observation is
comparable or even more than the number of observations. We propose a
deterministic high-dimensional robust PCA algorithm which inherits all
theoretical properties of its randomized counterpart, i.e., it is tractable,
robust to contaminated points, easily kernelizable, asymptotic consistent and
achieves maximal robustness -- a breakdown point of 50%. More importantly, the
proposed method exhibits significantly better computational efficiency, which
makes it suitable for large-scale real applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4629</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4629</id><created>2012-06-18</created><authors><author><keyname>Yang</keyname><forenames>Tianbao</forenames><affiliation>Michigan State University</affiliation></author><author><keyname>Mahdavi</keyname><forenames>Mehrdad</forenames><affiliation>Michigan State University</affiliation></author><author><keyname>Jin</keyname><forenames>Rong</forenames><affiliation>Michigan State University</affiliation></author><author><keyname>Zhang</keyname><forenames>Lijun</forenames><affiliation>Michigan State University</affiliation></author><author><keyname>Zhou</keyname><forenames>Yang</forenames><affiliation>Yahoo! Labs</affiliation></author></authors><title>Multiple Kernel Learning from Noisy Labels by Stochastic Programming</title><categories>cs.LG</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of multiple kernel learning from noisy labels. This is
in contrast to most of the previous studies on multiple kernel learning that
mainly focus on developing efficient algorithms and assume perfectly labeled
training examples. Directly applying the existing multiple kernel learning
algorithms to noisily labeled examples often leads to suboptimal performance
due to the incorrect class assignments. We address this challenge by casting
multiple kernel learning from noisy labels into a stochastic programming
problem, and presenting a minimax formulation. We develop an efficient
algorithm for solving the related convex-concave optimization problem with a
fast convergence rate of $O(1/T)$ where $T$ is the number of iterations.
Empirical studies on UCI data sets verify both the effectiveness of the
proposed framework and the efficiency of the proposed optimization algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4630</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4630</id><created>2012-06-18</created><authors><author><keyname>Samdani</keyname><forenames>Rajhans</forenames><affiliation>University of Illinois, U-C</affiliation></author><author><keyname>Roth</keyname><forenames>Dan</forenames><affiliation>University of Illinois, U-C</affiliation></author></authors><title>Efficient Decomposed Learning for Structured Prediction</title><categories>cs.LG</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Structured prediction is the cornerstone of several machine learning
applications. Unfortunately, in structured prediction settings with expressive
inter-variable interactions, exact inference-based learning algorithms, e.g.
Structural SVM, are often intractable. We present a new way, Decomposed
Learning (DecL), which performs efficient learning by restricting the inference
step to a limited part of the structured spaces. We provide characterizations
based on the structure, target parameters, and gold labels, under which DecL is
equivalent to exact learning. We then show that in real world settings, where
our theoretical assumptions may not completely hold, DecL-based algorithms are
significantly more efficient and as accurate as exact learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4631</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4631</id><created>2012-06-18</created><updated>2014-07-27</updated><authors><author><keyname>Airoldi</keyname><forenames>Edoardo M</forenames></author><author><keyname>Bischof</keyname><forenames>Jonathan M</forenames></author></authors><title>A Poisson convolution model for characterizing topical content with word
  frequency and exclusivity</title><categories>cs.LG cs.CL cs.IR stat.ME stat.ML</categories><comments>Originally appeared in ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An ongoing challenge in the analysis of document collections is how to
summarize content in terms of a set of inferred themes that can be interpreted
substantively in terms of topics. The current practice of parametrizing the
themes in terms of most frequent words limits interpretability by ignoring the
differential use of words across topics. We argue that words that are both
common and exclusive to a theme are more effective at characterizing topical
content. We consider a setting where professional editors have annotated
documents to a collection of topic categories, organized into a tree, in which
leaf-nodes correspond to the most specific topics. Each document is annotated
to multiple categories, at different levels of the tree. We introduce a
hierarchical Poisson convolution model to analyze annotated documents in this
setting. The model leverages the structure among categories defined by
professional editors to infer a clear semantic description for each topic in
terms of words that are both frequent and exclusive. We carry out a large
randomized experiment on Amazon Turk to demonstrate that topic summaries based
on the FREX score are more interpretable than currently established frequency
based summaries, and that the proposed model produces more efficient estimates
of exclusivity than with currently models. We also develop a parallelized
Hamiltonian Monte Carlo sampler that allows the inference to scale to millions
of documents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4632</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4632</id><created>2012-06-18</created><authors><author><keyname>Vogt</keyname><forenames>Julia</forenames><affiliation>University of Basel</affiliation></author><author><keyname>Roth</keyname><forenames>Volker</forenames><affiliation>University of Basel</affiliation></author></authors><title>A Complete Analysis of the l_1,p Group-Lasso</title><categories>cs.LG math.OC stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Group-Lasso is a well-known tool for joint regularization in machine
learning methods. While the l_{1,2} and the l_{1,\infty} version have been
studied in detail and efficient algorithms exist, there are still open
questions regarding other l_{1,p} variants. We characterize conditions for
solutions of the l_{1,p} Group-Lasso for all p-norms with 1 &lt;= p &lt;= \infty, and
we present a unified active set algorithm. For all p-norms, a highly efficient
projected gradient algorithm is presented. This new algorithm enables us to
compare the prediction performance of many variants of the Group-Lasso in a
multi-task learning setting, where the aim is to solve many learning problems
in parallel which are coupled via the Group-Lasso constraint. We conduct
large-scale experiments on synthetic data and on two real-world data sets. In
accordance with theoretical characterizations of the different norms we observe
that the weak-coupling norms with p between 1.5 and 2 consistently outperform
the strong-coupling norms with p &gt;&gt; 2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4633</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4633</id><created>2012-06-18</created><authors><author><keyname>Zhao</keyname><forenames>Peilin</forenames><affiliation>NTU</affiliation></author><author><keyname>Wang</keyname><forenames>Jialei</forenames><affiliation>NTU</affiliation></author><author><keyname>Wu</keyname><forenames>Pengcheng</forenames><affiliation>NTU</affiliation></author><author><keyname>Jin</keyname><forenames>Rong</forenames><affiliation>MSU</affiliation></author><author><keyname>Hoi</keyname><forenames>Steven C. H.</forenames><affiliation>NTU</affiliation></author></authors><title>Fast Bounded Online Gradient Descent Algorithms for Scalable
  Kernel-Based Online Learning</title><categories>cs.LG stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kernel-based online learning has often shown state-of-the-art performance for
many online learning tasks. It, however, suffers from a major shortcoming, that
is, the unbounded number of support vectors, making it non-scalable and
unsuitable for applications with large-scale datasets. In this work, we study
the problem of bounded kernel-based online learning that aims to constrain the
number of support vectors by a predefined budget. Although several algorithms
have been proposed in literature, they are neither computationally efficient
due to their intensive budget maintenance strategy nor effective due to the use
of simple Perceptron algorithm. To overcome these limitations, we propose a
framework for bounded kernel-based online learning based on an online gradient
descent approach. We propose two efficient algorithms of bounded online
gradient descent (BOGD) for scalable kernel-based online learning: (i) BOGD by
maintaining support vectors using uniform sampling, and (ii) BOGD++ by
maintaining support vectors using non-uniform sampling. We present theoretical
analysis of regret bound for both algorithms, and found promising empirical
performance in terms of both efficacy and efficiency by comparing them to
several well-known algorithms for bounded kernel-based online learning on
large-scale datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4634</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4634</id><created>2012-06-18</created><authors><author><keyname>Xie</keyname><forenames>Ning</forenames><affiliation>Tokyo Institute of Technology</affiliation></author><author><keyname>Hachiya</keyname><forenames>Hirotaka</forenames><affiliation>Tokyo Institute of Technology</affiliation></author><author><keyname>Sugiyama</keyname><forenames>Masashi</forenames><affiliation>Tokyo Institute of Technology</affiliation></author></authors><title>Artist Agent: A Reinforcement Learning Approach to Automatic Stroke
  Generation in Oriental Ink Painting</title><categories>cs.LG cs.GR stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><doi>10.1587/transinf.E96.D.1134</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Oriental ink painting, called Sumi-e, is one of the most appealing painting
styles that has attracted artists around the world. Major challenges in
computer-based Sumi-e simulation are to abstract complex scene information and
draw smooth and natural brush strokes. To automatically find such strokes, we
propose to model the brush as a reinforcement learning agent, and learn desired
brush-trajectories by maximizing the sum of rewards in the policy search
framework. We also provide elaborate design of actions, states, and rewards
tailored for a Sumi-e agent. The effectiveness of our proposed approach is
demonstrated through simulated Sumi-e experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4635</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4635</id><created>2012-06-18</created><authors><author><keyname>Tang</keyname><forenames>Yichuan</forenames><affiliation>University of Toronto</affiliation></author><author><keyname>Salakhutdinov</keyname><forenames>Ruslan</forenames><affiliation>University of Toronto</affiliation></author><author><keyname>Hinton</keyname><forenames>Geoffrey</forenames><affiliation>University of Toronto</affiliation></author></authors><title>Deep Mixtures of Factor Analysers</title><categories>cs.LG stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An efficient way to learn deep density models that have many layers of latent
variables is to learn one layer at a time using a model that has only one layer
of latent variables. After learning each layer, samples from the posterior
distributions for that layer are used as training data for learning the next
layer. This approach is commonly used with Restricted Boltzmann Machines, which
are undirected graphical models with a single hidden layer, but it can also be
used with Mixtures of Factor Analysers (MFAs) which are directed graphical
models. In this paper, we present a greedy layer-wise learning algorithm for
Deep Mixtures of Factor Analysers (DMFAs). Even though a DMFA can be converted
to an equivalent shallow MFA by multiplying together the factor loading
matrices at different levels, learning and inference are much more efficient in
a DMFA and the sharing of each lower-level factor loading matrix by many
different higher level MFAs prevents overfitting. We demonstrate empirically
that DMFAs learn better density models than both MFAs and two types of
Restricted Boltzmann Machine on a wide variety of datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4636</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4636</id><created>2012-06-18</created><authors><author><keyname>Kumar</keyname><forenames>M. Pawan</forenames><affiliation>Ecole Centrale Paris</affiliation></author><author><keyname>Packer</keyname><forenames>Ben</forenames><affiliation>Stanford University</affiliation></author><author><keyname>Koller</keyname><forenames>Daphne</forenames><affiliation>Stanford University</affiliation></author></authors><title>Modeling Latent Variable Uncertainty for Loss-based Learning</title><categories>cs.LG cs.AI cs.CV</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of parameter estimation using weakly supervised
datasets, where a training sample consists of the input and a partially
specified annotation, which we refer to as the output. The missing information
in the annotation is modeled using latent variables. Previous methods
overburden a single distribution with two separate tasks: (i) modeling the
uncertainty in the latent variables during training; and (ii) making accurate
predictions for the output and the latent variables during testing. We propose
a novel framework that separates the demands of the two tasks using two
distributions: (i) a conditional distribution to model the uncertainty of the
latent variables for a given input-output pair; and (ii) a delta distribution
to predict the output and the latent variables for a given input. During
learning, we encourage agreement between the two distributions by minimizing a
loss-based dissimilarity coefficient. Our approach generalizes latent SVM in
two important ways: (i) it models the uncertainty over latent variables instead
of relying on a pointwise estimate; and (ii) it allows the use of loss
functions that depend on latent variables, which greatly increases its
applicability. We demonstrate the efficacy of our approach on two challenging
problems---object detection and action detection---using publicly available
datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4637</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4637</id><created>2012-06-18</created><authors><author><keyname>Prasse</keyname><forenames>Paul</forenames><affiliation>University of Potsdam</affiliation></author><author><keyname>Sawade</keyname><forenames>Christoph</forenames><affiliation>University of Potsdam</affiliation></author><author><keyname>Landwehr</keyname><forenames>Niels</forenames><affiliation>University of Potsdam</affiliation></author><author><keyname>Scheffer</keyname><forenames>Tobias</forenames><affiliation>University of Potsdam</affiliation></author></authors><title>Learning to Identify Regular Expressions that Describe Email Campaigns</title><categories>cs.LG cs.CL stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of inferring a regular expression from a
given set of strings that resembles, as closely as possible, the regular
expression that a human expert would have written to identify the language.
This is motivated by our goal of automating the task of postmasters of an email
service who use regular expressions to describe and blacklist email spam
campaigns. Training data contains batches of messages and corresponding regular
expressions that an expert postmaster feels confident to blacklist. We model
this task as a learning problem with structured output spaces and an
appropriate loss function, derive a decoder and the resulting optimization
problem, and a report on a case study conducted with an email service.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4638</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4638</id><created>2012-06-18</created><authors><author><keyname>Yu</keyname><forenames>Adams Wei</forenames><affiliation>The University of Hong Kong</affiliation></author><author><keyname>Su</keyname><forenames>Hao</forenames><affiliation>Stanford University</affiliation></author><author><keyname>Fei-Fei</keyname><forenames>Li</forenames><affiliation>Stanford University</affiliation></author></authors><title>Efficient Euclidean Projections onto the Intersection of Norm Balls</title><categories>cs.LG stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using sparse-inducing norms to learn robust models has received increasing
attention from many fields for its attractive properties. Projection-based
methods have been widely applied to learning tasks constrained by such norms.
As a key building block of these methods, an efficient operator for Euclidean
projection onto the intersection of $\ell_1$ and $\ell_{1,q}$ norm balls
$(q=2\text{or}\infty)$ is proposed in this paper. We prove that the projection
can be reduced to finding the root of an auxiliary function which is piecewise
smooth and monotonic. Hence, a bisection algorithm is sufficient to solve the
problem. We show that the time complexity of our solution is $O(n+g\log g)$ for
$q=2$ and $O(n\log n)$ for $q=\infty$, where $n$ is the dimensionality of the
vector to be projected and $g$ is the number of disjoint groups; we confirm
this complexity by experimentation. Empirical study reveals that our method
achieves significantly better performance than classical methods in terms of
running time and memory usage. We further show that embedded with our efficient
projection operator, projection-based algorithms can solve regression problems
with composite norm constraints more efficiently than other methods and give
superior accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4639</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4639</id><created>2012-06-18</created><authors><author><keyname>Crammer</keyname><forenames>Koby</forenames><affiliation>The Technion</affiliation></author><author><keyname>Chechik</keyname><forenames>Gal</forenames><affiliation>Bar Ilan University and Google research</affiliation></author></authors><title>Adaptive Regularization for Weight Matrices</title><categories>cs.LG cs.AI</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Algorithms for learning distributions over weight-vectors, such as AROW were
recently shown empirically to achieve state-of-the-art performance at various
problems, with strong theoretical guaranties. Extending these algorithms to
matrix models pose challenges since the number of free parameters in the
covariance of the distribution scales as $n^4$ with the dimension $n$ of the
matrix, and $n$ tends to be large in real applications. We describe, analyze
and experiment with two new algorithms for learning distribution of matrix
models. Our first algorithm maintains a diagonal covariance over the parameters
and can handle large covariance matrices. The second algorithm factors the
covariance to capture inter-features correlation while keeping the number of
parameters linear in the size of the original matrix. We analyze both
algorithms in the mistake bound model and show a superior precision performance
of our approach over other algorithms in two tasks: retrieving similar images,
and ranking similar documents. The factored algorithm is shown to attain faster
convergence rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4640</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4640</id><created>2012-06-18</created><authors><author><keyname>Wang</keyname><forenames>Yu-Xiang</forenames><affiliation>National University of Singapore</affiliation></author><author><keyname>Xu</keyname><forenames>Huan</forenames><affiliation>National University of Singapore</affiliation></author></authors><title>Stability of matrix factorization for collaborative filtering</title><categories>cs.NA cs.LG stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the stability vis a vis adversarial noise of matrix factorization
algorithm for matrix completion. In particular, our results include: (I) we
bound the gap between the solution matrix of the factorization method and the
ground truth in terms of root mean square error; (II) we treat the matrix
factorization as a subspace fitting problem and analyze the difference between
the solution subspace and the ground truth; (III) we analyze the prediction
error of individual users based on the subspace stability. We apply these
results to the problem of collaborative filtering under manipulator attack,
which leads to useful insights and guidelines for collaborative filtering
system design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4641</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4641</id><created>2012-06-18</created><authors><author><keyname>Lin</keyname><forenames>Tong</forenames><affiliation>Peking University</affiliation></author><author><keyname>Xue</keyname><forenames>Hanlin</forenames><affiliation>Peking University</affiliation></author><author><keyname>Wang</keyname><forenames>Ling</forenames><affiliation>LTCI, Telecom ParisTech, Paris</affiliation></author><author><keyname>Zha</keyname><forenames>Hongbin</forenames><affiliation>Peking University</affiliation></author></authors><title>Total Variation and Euler's Elastica for Supervised Learning</title><categories>cs.LG cs.CV stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, total variation (TV) and Euler's elastica (EE) have been
successfully applied to image processing tasks such as denoising and
inpainting. This paper investigates how to extend TV and EE to the supervised
learning settings on high dimensional data. The supervised learning problem can
be formulated as an energy functional minimization under Tikhonov
regularization scheme, where the energy is composed of a squared loss and a
total variation smoothing (or Euler's elastica smoothing). Its solution via
variational principles leads to an Euler-Lagrange PDE. However, the PDE is
always high-dimensional and cannot be directly solved by common methods.
Instead, radial basis functions are utilized to approximate the target
function, reducing the problem to finding the linear coefficients of basis
functions. We apply the proposed methods to supervised learning tasks
(including binary classification, multi-class classification, and regression)
on benchmark data sets. Extensive experiments have demonstrated promising
results of the proposed methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4642</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4642</id><created>2012-06-18</created><authors><author><keyname>Kimura</keyname><forenames>Daisuke</forenames><affiliation>The University of Tokyo</affiliation></author><author><keyname>Kashima</keyname><forenames>Hisashi</forenames><affiliation>The University of Tokyo</affiliation></author></authors><title>Fast Computation of Subpath Kernel for Trees</title><categories>cs.DS cs.LG stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The kernel method is a potential approach to analyzing structured data such
as sequences, trees, and graphs; however, unordered trees have not been
investigated extensively. Kimura et al. (2011) proposed a kernel function for
unordered trees on the basis of their subpaths, which are vertical
substructures of trees responsible for hierarchical information in them. Their
kernel exhibits practically good performance in terms of accuracy and speed;
however, linear-time computation is not guaranteed theoretically, unlike the
case of the other unordered tree kernel proposed by Vishwanathan and Smola
(2003). In this paper, we propose a theoretically guaranteed linear-time kernel
computation algorithm that is practically fast, and we present an efficient
prediction algorithm whose running time depends only on the size of the input
tree. Experimental results show that the proposed algorithms are quite
efficient in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4643</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4643</id><created>2012-06-18</created><authors><author><keyname>Mannor</keyname><forenames>Shie</forenames><affiliation>Technion</affiliation></author><author><keyname>Mebel</keyname><forenames>Ofir</forenames><affiliation>Technion</affiliation></author><author><keyname>Xu</keyname><forenames>Huan</forenames><affiliation>National University of Singapore</affiliation></author></authors><title>Lightning Does Not Strike Twice: Robust MDPs with Coupled Uncertainty</title><categories>cs.LG cs.GT cs.SY</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider Markov decision processes under parameter uncertainty. Previous
studies all restrict to the case that uncertainties among different states are
uncoupled, which leads to conservative solutions. In contrast, we introduce an
intuitive concept, termed &quot;Lightning Does not Strike Twice,&quot; to model coupled
uncertain parameters. Specifically, we require that the system can deviate from
its nominal parameters only a bounded number of times. We give probabilistic
guarantees indicating that this model represents real life situations and
devise tractable algorithms for computing optimal control policies using this
concept.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4644</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4644</id><created>2012-06-18</created><authors><author><keyname>Li</keyname><forenames>Ruijiang</forenames><affiliation>Fudan University</affiliation></author><author><keyname>Li</keyname><forenames>Bin</forenames><affiliation>University of Technology, Sydney</affiliation></author><author><keyname>Zhang</keyname><forenames>Ke</forenames><affiliation>Fudan Univ.</affiliation></author><author><keyname>Jin</keyname><forenames>Cheng</forenames><affiliation>Fudan University</affiliation></author><author><keyname>Xue</keyname><forenames>Xiangyang</forenames><affiliation>Fudan University</affiliation></author></authors><title>Groupwise Constrained Reconstruction for Subspace Clustering</title><categories>cs.LG stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reconstruction based subspace clustering methods compute a self
reconstruction matrix over the samples and use it for spectral clustering to
obtain the final clustering result. Their success largely relies on the
assumption that the underlying subspaces are independent, which, however, does
not always hold in the applications with increasing number of subspaces. In
this paper, we propose a novel reconstruction based subspace clustering model
without making the subspace independence assumption. In our model, certain
properties of the reconstruction matrix are explicitly characterized using the
latent cluster indicators, and the affinity matrix used for spectral clustering
can be directly built from the posterior of the latent cluster indicators
instead of the reconstruction matrix. Experimental results on both synthetic
and real-world datasets show that the proposed model can outperform the
state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4645</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4645</id><created>2012-06-18</created><authors><author><keyname>Hannah</keyname><forenames>Lauren</forenames><affiliation>Duke University</affiliation></author><author><keyname>Dunson</keyname><forenames>David</forenames><affiliation>Duke University</affiliation></author></authors><title>Ensemble Methods for Convex Regression with Applications to Geometric
  Programming Based Circuit Design</title><categories>cs.LG cs.NA stat.ME stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convex regression is a promising area for bridging statistical estimation and
deterministic convex optimization. New piecewise linear convex regression
methods are fast and scalable, but can have instability when used to
approximate constraints or objective functions for optimization. Ensemble
methods, like bagging, smearing and random partitioning, can alleviate this
problem and maintain the theoretical properties of the underlying estimator. We
empirically examine the performance of ensemble methods for prediction and
optimization, and then apply them to device modeling and constraint
approximation for geometric programming based circuit design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4646</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4646</id><created>2012-06-18</created><authors><author><keyname>Vladymyrov</keyname><forenames>Max</forenames><affiliation>UC Merced</affiliation></author><author><keyname>Carreira-Perpinan</keyname><forenames>Miguel</forenames><affiliation>UC Merced</affiliation></author></authors><title>Partial-Hessian Strategies for Fast Learning of Nonlinear Embeddings</title><categories>cs.LG stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic neighbor embedding (SNE) and related nonlinear manifold learning
algorithms achieve high-quality low-dimensional representations of similarity
data, but are notoriously slow to train. We propose a generic formulation of
embedding algorithms that includes SNE and other existing algorithms, and study
their relation with spectral methods and graph Laplacians. This allows us to
define several partial-Hessian optimization strategies, characterize their
global and local convergence, and evaluate them empirically. We achieve up to
two orders of magnitude speedup over existing training methods with a strategy
(which we call the spectral direction) that adds nearly no overhead to the
gradient and yet is simple, scalable and applicable to several existing and
future embedding algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4647</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4647</id><created>2012-06-18</created><authors><author><keyname>Charlin</keyname><forenames>Laurent</forenames><affiliation>University of Toronto</affiliation></author><author><keyname>Zemel</keyname><forenames>Rich</forenames><affiliation>University of Toronto</affiliation></author><author><keyname>Boutilier</keyname><forenames>Craig</forenames><affiliation>University of Toronto</affiliation></author></authors><title>Active Learning for Matching Problems</title><categories>cs.LG cs.AI cs.IR</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Effective learning of user preferences is critical to easing user burden in
various types of matching problems. Equally important is active query selection
to further reduce the amount of preference information users must provide. We
address the problem of active learning of user preferences for matching
problems, introducing a novel method for determining probabilistic matchings,
and developing several new active learning strategies that are sensitive to the
specific matching objective. Experiments with real-world data sets spanning
diverse domains demonstrate that matching-sensitive active learning
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4648</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4648</id><created>2012-06-18</created><authors><author><keyname>Boots</keyname><forenames>Byron</forenames><affiliation>Carnegie Mellon University</affiliation></author><author><keyname>Gordon</keyname><forenames>Geoff</forenames><affiliation>Carnegie Mellon University</affiliation></author></authors><title>Two-Manifold Problems with Applications to Nonlinear System
  Identification</title><categories>cs.LG</categories><comments>ICML2012. arXiv admin note: text overlap with arXiv:1112.6399</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, there has been much interest in spectral approaches to learning
manifolds---so-called kernel eigenmap methods. These methods have had some
successes, but their applicability is limited because they are not robust to
noise. To address this limitation, we look at two-manifold problems, in which
we simultaneously reconstruct two related manifolds, each representing a
different view of the same data. By solving these interconnected learning
problems together, two-manifold algorithms are able to succeed where a
non-integrated approach would fail: each view allows us to suppress noise in
the other, reducing bias. We propose a class of algorithms for two-manifold
problems, based on spectral decomposition of cross-covariance operators in
Hilbert space, and discuss when two-manifold problems are useful. Finally, we
demonstrate that solving a two-manifold problem can aid in learning a nonlinear
dynamical system from limited data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4649</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4649</id><created>2012-06-18</created><authors><author><keyname>Bronstein</keyname><forenames>Alex</forenames><affiliation>Tel Aviv University</affiliation></author><author><keyname>Sprechmann</keyname><forenames>Pablo</forenames><affiliation>University of Minnesota</affiliation></author><author><keyname>Sapiro</keyname><forenames>Guillermo</forenames><affiliation>University of Minnesota</affiliation></author></authors><title>Learning Efficient Structured Sparse Models</title><categories>cs.LG cs.CV stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a comprehensive framework for structured sparse coding and
modeling extending the recent ideas of using learnable fast regressors to
approximate exact sparse codes. For this purpose, we develop a novel
block-coordinate proximal splitting method for the iterative solution of
hierarchical sparse coding problems, and show an efficient feed forward
architecture derived from its iteration. This architecture faithfully
approximates the exact structured sparse codes with a fraction of the
complexity of the standard optimization methods. We also show that by using
different training objective functions, learnable sparse encoders are no longer
restricted to be mere approximants of the exact sparse code for a pre-given
dictionary, as in earlier formulations, but can be rather used as full-featured
sparse encoders or even modelers. A simple implementation shows several orders
of magnitude speedup compared to the state-of-the-art at minimal performance
degradation, making the proposed framework suitable for real time and
large-scale applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4650</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4650</id><created>2012-06-18</created><authors><author><keyname>Yu</keyname><forenames>Yaoliang</forenames><affiliation>University of Alberta</affiliation></author><author><keyname>Szepesvari</keyname><forenames>Csaba</forenames><affiliation>University of Alberta</affiliation></author></authors><title>Analysis of Kernel Mean Matching under Covariate Shift</title><categories>cs.LG stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In real supervised learning scenarios, it is not uncommon that the training
and test sample follow different probability distributions, thus rendering the
necessity to correct the sampling bias. Focusing on a particular covariate
shift problem, we derive high probability confidence bounds for the kernel mean
matching (KMM) estimator, whose convergence rate turns out to depend on some
regularity measure of the regression function and also on some capacity measure
of the kernel. By comparing KMM with the natural plug-in estimator, we
establish the superiority of the former hence provide concrete
evidence/understanding to the effectiveness of KMM under covariate shift.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4651</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4651</id><created>2012-06-18</created><authors><author><keyname>Shi</keyname><forenames>Qinfeng</forenames><affiliation>The University of Adelaide</affiliation></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames><affiliation>The University of Adelaide</affiliation></author><author><keyname>Hill</keyname><forenames>Rhys</forenames><affiliation>The University of Adelaide</affiliation></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames><affiliation>the University of Adelaide</affiliation></author></authors><title>Is margin preserved after random projection?</title><categories>cs.LG cs.CV stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random projections have been applied in many machine learning algorithms.
However, whether margin is preserved after random projection is non-trivial and
not well studied. In this paper we analyse margin distortion after random
projection, and give the conditions of margin preservation for binary
classification problems. We also extend our analysis to margin for multiclass
problems, and provide theoretical bounds on multiclass margin on the projected
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4652</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4652</id><created>2012-06-18</created><authors><author><keyname>Quadrianto</keyname><forenames>Novi</forenames><affiliation>University of Cambridge</affiliation></author><author><keyname>Chen</keyname><forenames>Chao</forenames><affiliation>IST Austria</affiliation></author><author><keyname>Lampert</keyname><forenames>Christoph</forenames><affiliation>IST Austria</affiliation></author></authors><title>The Most Persistent Soft-Clique in a Set of Sampled Graphs</title><categories>cs.LG cs.AI</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When searching for characteristic subpatterns in potentially noisy graph
data, it appears self-evident that having multiple observations would be better
than having just one. However, it turns out that the inconsistencies introduced
when different graph instances have different edge sets pose a serious
challenge. In this work we address this challenge for the problem of finding
maximum weighted cliques.
  We introduce the concept of most persistent soft-clique. This is subset of
vertices, that 1) is almost fully or at least densely connected, 2) occurs in
all or almost all graph instances, and 3) has the maximum weight. We present a
measure of clique-ness, that essentially counts the number of edge missing to
make a subset of vertices into a clique. With this measure, we show that the
problem of finding the most persistent soft-clique problem can be cast either
as: a) a max-min two person game optimization problem, or b) a min-min soft
margin optimization problem. Both formulations lead to the same solution when
using a partial Lagrangian method to solve the optimization problems. By
experiments on synthetic data and on real social network data, we show that the
proposed method is able to reliably find soft cliques in graph data, even if
that is distorted by random noise or unreliable observations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4653</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4653</id><created>2012-06-18</created><authors><author><keyname>Parrish</keyname><forenames>Nathan</forenames><affiliation>University of Washington</affiliation></author><author><keyname>Gupta</keyname><forenames>Maya</forenames><affiliation>University of Washington</affiliation></author></authors><title>Dimensionality Reduction by Local Discriminative Gaussians</title><categories>cs.LG cs.CV stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present local discriminative Gaussian (LDG) dimensionality reduction, a
supervised dimensionality reduction technique for classification. The LDG
objective function is an approximation to the leave-one-out training error of a
local quadratic discriminant analysis classifier, and thus acts locally to each
training point in order to find a mapping where similar data can be
discriminated from dissimilar data. While other state-of-the-art linear
dimensionality reduction methods require gradient descent or iterative solution
approaches, LDG is solved with a single eigen-decomposition. Thus, it scales
better for datasets with a large number of feature dimensions or training
examples. We also adapt LDG to the transfer learning setting, and show that it
achieves good performance when the test data distribution differs from that of
the training data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4654</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4654</id><created>2012-06-18</created><authors><author><keyname>Ravanbakhsh</keyname><forenames>Siamak</forenames><affiliation>University of Alberta</affiliation></author><author><keyname>Yu</keyname><forenames>Chun-Nam</forenames><affiliation>University of Alberta</affiliation></author><author><keyname>Greiner</keyname><forenames>Russell</forenames><affiliation>University of Alberta</affiliation></author></authors><title>A Generalized Loop Correction Method for Approximate Inference in
  Graphical Models</title><categories>cs.AI cs.LG stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Belief Propagation (BP) is one of the most popular methods for inference in
probabilistic graphical models. BP is guaranteed to return the correct answer
for tree structures, but can be incorrect or non-convergent for loopy graphical
models. Recently, several new approximate inference algorithms based on cavity
distribution have been proposed. These methods can account for the effect of
loops by incorporating the dependency between BP messages. Alternatively,
region-based approximations (that lead to methods such as Generalized Belief
Propagation) improve upon BP by considering interactions within small clusters
of variables, thus taking small loops within these clusters into account. This
paper introduces an approach, Generalized Loop Correction (GLC), that benefits
from both of these types of loop correction. We show how GLC relates to these
two families of inference methods, then provide empirical evidence that GLC
works effectively in general, and can be significantly more accurate than both
correction schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4655</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4655</id><created>2012-06-18</created><authors><author><keyname>Grunewalder</keyname><forenames>Steffen</forenames><affiliation>University College London</affiliation></author><author><keyname>Lever</keyname><forenames>Guy</forenames><affiliation>University College London</affiliation></author><author><keyname>Baldassarre</keyname><forenames>Luca</forenames><affiliation>University College London</affiliation></author><author><keyname>Pontil</keyname><forenames>Massi</forenames><affiliation>University College London</affiliation></author><author><keyname>Gretton</keyname><forenames>Arthur</forenames><affiliation>MPI for Intelligent Systems</affiliation></author></authors><title>Modelling transition dynamics in MDPs with RKHS embeddings</title><categories>cs.LG</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new, nonparametric approach to learning and representing
transition dynamics in Markov decision processes (MDPs), which can be combined
easily with dynamic programming methods for policy optimisation and value
estimation. This approach makes use of a recently developed representation of
conditional distributions as \emph{embeddings} in a reproducing kernel Hilbert
space (RKHS). Such representations bypass the need for estimating transition
probabilities or densities, and apply to any domain on which kernels can be
defined. This avoids the need to calculate intractable integrals, since
expectations are represented as RKHS inner products whose computation has
linear complexity in the number of points used to represent the embedding. We
provide guarantees for the proposed applications in MDPs: in the context of a
value iteration algorithm, we prove convergence to either the optimal policy,
or to the closest projection of the optimal policy in our model class (an
RKHS), under reasonable assumptions. In experiments, we investigate a learning
task in a typical classical control setting (the under-actuated pendulum), and
on a navigation problem where only images from a sensor are observed. For
policy optimisation we compare with least-squares policy iteration where a
Gaussian process is used for value function estimation. For value estimation we
also compare to the NPDP method. Our approach achieves better performance in
all experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4656</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4656</id><created>2012-06-18</created><authors><author><keyname>Wagstaff</keyname><forenames>Kiri</forenames><affiliation>Jet Propulsion Laboratory</affiliation></author></authors><title>Machine Learning that Matters</title><categories>cs.LG cs.AI stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Much of current machine learning (ML) research has lost its connection to
problems of import to the larger world of science and society. From this
perspective, there exist glaring limitations in the data sets we investigate,
the metrics we employ for evaluation, and the degree to which results are
communicated back to their originating domains. What changes are needed to how
we conduct research to increase the impact that ML has? We present six Impact
Challenges to explicitly focus the field?s energy and attention, and we discuss
existing obstacles that must be addressed. We aim to inspire ongoing discussion
and focus on ML that matters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4657</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4657</id><created>2012-06-18</created><authors><author><keyname>Hazan</keyname><forenames>Elad</forenames><affiliation>Technion</affiliation></author><author><keyname>Kale</keyname><forenames>Satyen</forenames><affiliation>IBM T.J. Watson Research Center</affiliation></author></authors><title>Projection-free Online Learning</title><categories>cs.LG cs.DS</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The computational bottleneck in applying online learning to massive data sets
is usually the projection step. We present efficient online learning algorithms
that eschew projections in favor of much more efficient linear optimization
steps using the Frank-Wolfe technique. We obtain a range of regret bounds for
online convex optimization, with better bounds for specific cases such as
stochastic online smooth convex optimization.
  Besides the computational advantage, other desirable features of our
algorithms are that they are parameter-free in the stochastic case and produce
sparse decisions. We apply our algorithms to computationally intensive
applications of collaborative filtering, and show the theoretical improvements
to be clearly visible on standard datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4658</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4658</id><created>2012-06-18</created><authors><author><keyname>Kim</keyname><forenames>Dongwoo</forenames><affiliation>KAIST</affiliation></author><author><keyname>Kim</keyname><forenames>Suin</forenames><affiliation>KAIST</affiliation></author><author><keyname>Oh</keyname><forenames>Alice</forenames><affiliation>KAIST</affiliation></author></authors><title>Dirichlet Process with Mixed Random Measures: A Nonparametric Topic
  Model for Labeled Data</title><categories>cs.LG stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a nonparametric topic model for labeled data. The model uses a
mixture of random measures (MRM) as a base distribution of the Dirichlet
process (DP) of the HDP framework, so we call it the DP-MRM. To model labeled
data, we define a DP distributed random measure for each label, and the
resulting model generates an unbounded number of topics for each label. We
apply DP-MRM on single-labeled and multi-labeled corpora of documents and
compare the performance on label prediction with MedLDA, LDA-SVM, and
Labeled-LDA. We further enhance the model by incorporating ddCRP and modeling
multi-labeled images for image segmentation and object labeling, comparing the
performance with nCuts and rddCRP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4659</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4659</id><created>2012-06-18</created><authors><author><keyname>Zhu</keyname><forenames>Jun</forenames><affiliation>Tsinghua University</affiliation></author></authors><title>Max-Margin Nonparametric Latent Feature Models for Link Prediction</title><categories>cs.LG stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a max-margin nonparametric latent feature model, which unites the
ideas of max-margin learning and Bayesian nonparametrics to discover
discriminative latent features for link prediction and automatically infer the
unknown latent social dimension. By minimizing a hinge-loss using the linear
expectation operator, we can perform posterior inference efficiently without
dealing with a highly nonlinear link likelihood function; by using a
fully-Bayesian formulation, we can avoid tuning regularization constants.
Experimental results on real datasets appear to demonstrate the benefits
inherited from max-margin learning and fully-Bayesian nonparametric inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4660</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4660</id><created>2012-06-18</created><authors><author><keyname>Duan</keyname><forenames>Lixin</forenames><affiliation>Nanyang Technological University</affiliation></author><author><keyname>Xu</keyname><forenames>Dong</forenames><affiliation>Nanyang Technological University</affiliation></author><author><keyname>Tsang</keyname><forenames>Ivor</forenames><affiliation>Nanyang Technological University</affiliation></author></authors><title>Learning with Augmented Features for Heterogeneous Domain Adaptation</title><categories>cs.LG</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new learning method for heterogeneous domain adaptation (HDA),
in which the data from the source domain and the target domain are represented
by heterogeneous features with different dimensions. Using two different
projection matrices, we first transform the data from two domains into a common
subspace in order to measure the similarity between the data from two domains.
We then propose two new feature mapping functions to augment the transformed
data with their original features and zeros. The existing learning methods
(e.g., SVM and SVR) can be readily incorporated with our newly proposed
augmented feature representations to effectively utilize the data from both
domains for HDA. Using the hinge loss function in SVM as an example, we
introduce the detailed objective function in our method called Heterogeneous
Feature Augmentation (HFA) for a linear case and also describe its
kernelization in order to efficiently cope with the data with very high
dimensions. Moreover, we also develop an alternating optimization algorithm to
effectively solve the nontrivial optimization problem in our HFA method.
Comprehensive experiments on two benchmark datasets clearly demonstrate that
HFA outperforms the existing HDA methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4661</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4661</id><created>2012-06-18</created><authors><author><keyname>Menon</keyname><forenames>Aditya</forenames><affiliation>UC San Diego</affiliation></author><author><keyname>Jiang</keyname><forenames>Xiaoqian</forenames><affiliation>UC San Diego</affiliation></author><author><keyname>Vembu</keyname><forenames>Shankar</forenames><affiliation>University of Toronto</affiliation></author><author><keyname>Elkan</keyname><forenames>Charles</forenames><affiliation>UC San Diego</affiliation></author><author><keyname>Ohno-Machado</keyname><forenames>Lucila</forenames><affiliation>UC San Diego</affiliation></author></authors><title>Predicting accurate probabilities with a ranking loss</title><categories>cs.LG stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many real-world applications of machine learning classifiers, it is
essential to predict the probability of an example belonging to a particular
class. This paper proposes a simple technique for predicting probabilities
based on optimizing a ranking loss, followed by isotonic regression. This
semi-parametric technique offers both good ranking and regression performance,
and models a richer set of probability distributions than statistical
workhorses such as logistic regression. We provide experimental results that
show the effectiveness of this technique on real-world applications of
probability prediction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4662</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4662</id><created>2012-06-18</created><authors><author><keyname>Shterev</keyname><forenames>Ivo</forenames><affiliation>Duke University</affiliation></author><author><keyname>Dunson</keyname><forenames>David</forenames><affiliation>Duke University</affiliation></author></authors><title>Bayesian Watermark Attacks</title><categories>cs.CR cs.LG cs.MM</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an application of statistical machine learning to the
field of watermarking. We propose a new attack model on additive
spread-spectrum watermarking systems. The proposed attack is based on Bayesian
statistics. We consider the scenario in which a watermark signal is repeatedly
embedded in specific, possibly chosen based on a secret message bitstream,
segments (signals) of the host data. The host signal can represent a patch of
pixels from an image or a video frame. We propose a probabilistic model that
infers the embedded message bitstream and watermark signal, directly from the
watermarked data, without access to the decoder. We develop an efficient Markov
chain Monte Carlo sampler for updating the model parameters from their
conjugate full conditional posteriors. We also provide a variational Bayesian
solution, which further increases the convergence speed of the algorithm.
Experiments with synthetic and real image signals demonstrate that the attack
model is able to correctly infer a large part of the message bitstream and
obtain a very accurate estimate of the watermark signal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4663</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4663</id><created>2012-06-18</created><authors><author><keyname>Reid</keyname><forenames>Mark</forenames><affiliation>The Australian National University and NICTA</affiliation></author><author><keyname>Williamson</keyname><forenames>Robert</forenames><affiliation>The Australian National University and NICTA</affiliation></author><author><keyname>Sun</keyname><forenames>Peng</forenames><affiliation>Tsinghua University</affiliation></author></authors><title>The Convexity and Design of Composite Multiclass Losses</title><categories>cs.LG stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider composite loss functions for multiclass prediction comprising a
proper (i.e., Fisher-consistent) loss over probability distributions and an
inverse link function. We establish conditions for their (strong) convexity and
explore the implications. We also show how the separation of concerns afforded
by using this composite representation allows for the design of families of
losses with the same Bayes risk.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4664</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4664</id><created>2012-06-18</created><authors><author><keyname>Ruderman</keyname><forenames>Avraham</forenames><affiliation>Australian National University and NICTA</affiliation></author><author><keyname>Reid</keyname><forenames>Mark</forenames><affiliation>Australian National University and NICTA</affiliation></author><author><keyname>Garcia-Garcia</keyname><forenames>Dario</forenames><affiliation>Australian National University and NICTA</affiliation></author><author><keyname>Petterson</keyname><forenames>James</forenames><affiliation>NICTA</affiliation></author></authors><title>Tighter Variational Representations of f-Divergences via Restriction to
  Probability Measures</title><categories>cs.LG stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the variational representations for f-divergences currently used
in the literature can be tightened. This has implications to a number of
methods recently proposed based on this representation. As an example
application we use our tighter representation to derive a general f-divergence
estimator based on two i.i.d. samples and derive the dual program for this
estimator that performs well empirically. We also point out a connection
between our estimator and MMD.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4665</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4665</id><created>2012-06-18</created><authors><author><keyname>Gershman</keyname><forenames>Samuel</forenames><affiliation>Princeton University</affiliation></author><author><keyname>Hoffman</keyname><forenames>Matt</forenames><affiliation>Princeton University</affiliation></author><author><keyname>Blei</keyname><forenames>David</forenames><affiliation>Princeton University</affiliation></author></authors><title>Nonparametric variational inference</title><categories>cs.LG stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Variational methods are widely used for approximate posterior inference.
However, their use is typically limited to families of distributions that enjoy
particular conjugacy properties. To circumvent this limitation, we propose a
family of variational approximations inspired by nonparametric kernel density
estimation. The locations of these kernels and their bandwidth are treated as
variational parameters and optimized to improve an approximate lower bound on
the marginal likelihood of the data. Using multiple kernels allows the
approximation to capture multiple modes of the posterior, unlike most other
variational approximations. We demonstrate the efficacy of the nonparametric
approximation with a hierarchical logistic regression model and a nonlinear
matrix factorization model. We obtain predictive performance as good as or
better than more specialized variational methods and sample-based
approximations. The method is easy to apply to more general graphical models
for which standard variational methods are difficult to derive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4666</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4666</id><created>2012-06-18</created><authors><author><keyname>Zhong</keyname><forenames>Mingjun</forenames><affiliation>Dalian University of Tech.</affiliation></author><author><keyname>Girolami</keyname><forenames>Mark</forenames><affiliation>University College London</affiliation></author></authors><title>A Bayesian Approach to Approximate Joint Diagonalization of Square
  Matrices</title><categories>stat.CO cs.LG stat.ME</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a Bayesian scheme for the approximate diagonalisation of several
square matrices which are not necessarily symmetric. A Gibbs sampler is derived
to simulate samples of the common eigenvectors and the eigenvalues for these
matrices. Several synthetic examples are used to illustrate the performance of
the proposed Gibbs sampler and we then provide comparisons to several other
joint diagonalization algorithms, which shows that the Gibbs sampler achieves
the state-of-the-art performance on the examples considered. As a byproduct,
the output of the Gibbs sampler could be used to estimate the log marginal
likelihood, however we employ the approximation based on the Bayesian
information criterion (BIC) which in the synthetic examples considered
correctly located the number of common eigenvectors. We then succesfully
applied the sampler to the source separation problem as well as the common
principal component analysis and the common spatial pattern analysis problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4667</identifier>
 <datestamp>2012-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4667</id><created>2012-06-18</created><updated>2012-07-18</updated><authors><author><keyname>Boyd</keyname><forenames>Kendrick</forenames><affiliation>University of Wisconsin Madison</affiliation></author><author><keyname>Costa</keyname><forenames>Vitor Santos</forenames><affiliation>University of Porto</affiliation></author><author><keyname>Davis</keyname><forenames>Jesse</forenames><affiliation>KU Leuven</affiliation></author><author><keyname>Page</keyname><forenames>David</forenames><affiliation>University of Wisconsin Madison</affiliation></author></authors><title>Unachievable Region in Precision-Recall Space and Its Effect on
  Empirical Evaluation</title><categories>cs.LG cs.AI cs.IR</categories><comments>ICML2012, fixed citations to use correct tech report number</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Precision-recall (PR) curves and the areas under them are widely used to
summarize machine learning results, especially for data sets exhibiting class
skew. They are often used analogously to ROC curves and the area under ROC
curves. It is known that PR curves vary as class skew changes. What was not
recognized before this paper is that there is a region of PR space that is
completely unachievable, and the size of this region depends only on the skew.
This paper precisely characterizes the size of that region and discusses its
implications for empirical evaluation methodology in machine learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4668</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4668</id><created>2012-06-18</created><authors><author><keyname>McCartin-Lim</keyname><forenames>Mark</forenames><affiliation>University of Massachusetts</affiliation></author><author><keyname>McGregor</keyname><forenames>Andrew</forenames><affiliation>University of Massachusetts</affiliation></author><author><keyname>Wang</keyname><forenames>Rui</forenames><affiliation>University of Massachusetts</affiliation></author></authors><title>Approximate Principal Direction Trees</title><categories>cs.LG cs.DS stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new spatial data structure for high dimensional data called
the \emph{approximate principal direction tree} (APD tree) that adapts to the
intrinsic dimension of the data. Our algorithm ensures vector-quantization
accuracy similar to that of computationally-expensive PCA trees with similar
time-complexity to that of lower-accuracy RP trees.
  APD trees use a small number of power-method iterations to find splitting
planes for recursively partitioning the data. As such they provide a natural
trade-off between the running-time and accuracy achieved by RP and PCA trees.
Our theoretical results establish a) strong performance guarantees regardless
of the convergence rate of the power-method and b) that $O(\log d)$ iterations
suffice to establish the guarantee of PCA trees when the intrinsic dimension is
$d$. We demonstrate this trade-off and the efficacy of our data structure on
both the CPU and GPU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4669</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4669</id><created>2012-06-18</created><authors><author><keyname>Balakrishnan</keyname><forenames>Sivaraman</forenames><affiliation>Carnegie Mellon University</affiliation></author><author><keyname>Puniyani</keyname><forenames>Kriti</forenames><affiliation>Carnegie Mellon University</affiliation></author><author><keyname>Lafferty</keyname><forenames>John</forenames><affiliation>Carnegie Mellon University</affiliation></author></authors><title>Sparse Additive Functional and Kernel CCA</title><categories>cs.LG stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Canonical Correlation Analysis (CCA) is a classical tool for finding
correlations among the components of two random vectors. In recent years, CCA
has been widely applied to the analysis of genomic data, where it is common for
researchers to perform multiple assays on a single set of patient samples.
Recent work has proposed sparse variants of CCA to address the high
dimensionality of such data. However, classical and sparse CCA are based on
linear models, and are thus limited in their ability to find general
correlations. In this paper, we present two approaches to high-dimensional
nonparametric CCA, building on recent developments in high-dimensional
nonparametric regression. We present estimation procedures for both approaches,
and analyze their theoretical properties in the high-dimensional setting. We
demonstrate the effectiveness of these procedures in discovering nonlinear
correlations via extensive simulations, as well as through experiments with
genomic data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4670</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4670</id><created>2012-06-18</created><authors><author><keyname>Hartikainen</keyname><forenames>Jouni</forenames><affiliation>Aalto University</affiliation></author><author><keyname>Seppanen</keyname><forenames>Mari</forenames><affiliation>Tampere University of Technology</affiliation></author><author><keyname>Sarkka</keyname><forenames>Simo</forenames><affiliation>Aalto University</affiliation></author></authors><title>State-Space Inference for Non-Linear Latent Force Models with
  Application to Satellite Orbit Prediction</title><categories>cs.IT astro-ph.EP cs.LG math.IT physics.data-an</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Latent force models (LFMs) are flexible models that combine mechanistic
modelling principles (i.e., physical models) with non-parametric data-driven
components. Several key applications of LFMs need non-linearities, which
results in analytically intractable inference. In this work we show how
non-linear LFMs can be represented as non-linear white noise driven state-space
models and present an efficient non-linear Kalman filtering and smoothing based
method for approximate state and parameter inference. We illustrate the
performance of the proposed methodology via two simulated examples, and apply
it to a real-world problem of long-term prediction of GPS satellite orbits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4671</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4671</id><created>2012-06-18</created><authors><author><keyname>Chen</keyname><forenames>Changyou</forenames><affiliation>ANU &amp; NICTA</affiliation></author><author><keyname>Ding</keyname><forenames>Nan</forenames><affiliation>Purdue University</affiliation></author><author><keyname>Buntine</keyname><forenames>Wray</forenames><affiliation>NICTA</affiliation></author></authors><title>Dependent Hierarchical Normalized Random Measures for Dynamic Topic
  Modeling</title><categories>cs.LG stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop dependent hierarchical normalized random measures and apply them
to dynamic topic modeling. The dependency arises via superposition, subsampling
and point transition on the underlying Poisson processes of these measures. The
measures used include normalised generalised Gamma processes that demonstrate
power law properties, unlike Dirichlet processes used previously in dynamic
topic modeling. Inference for the model includes adapting a recently developed
slice sampler to directly manipulate the underlying Poisson process.
Experiments performed on news, blogs, academic and Twitter collections
demonstrate the technique gives superior perplexity over a number of previous
models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4672</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4672</id><created>2012-06-18</created><authors><author><keyname>Krishnamurthy</keyname><forenames>Akshay</forenames><affiliation>Carnegie Mellon University</affiliation></author><author><keyname>Balakrishnan</keyname><forenames>Sivaraman</forenames><affiliation>Carnegie Mellon University</affiliation></author><author><keyname>Xu</keyname><forenames>Min</forenames><affiliation>Carnegie Mellon University</affiliation></author><author><keyname>Singh</keyname><forenames>Aarti</forenames><affiliation>Carnegie Mellon University</affiliation></author></authors><title>Efficient Active Algorithms for Hierarchical Clustering</title><categories>cs.LG stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advances in sensing technologies and the growth of the internet have resulted
in an explosion in the size of modern datasets, while storage and processing
power continue to lag behind. This motivates the need for algorithms that are
efficient, both in terms of the number of measurements needed and running time.
To combat the challenges associated with large datasets, we propose a general
framework for active hierarchical clustering that repeatedly runs an
off-the-shelf clustering algorithm on small subsets of the data and comes with
guarantees on performance, measurement complexity and runtime complexity. We
instantiate this framework with a simple spectral clustering algorithm and
provide concrete results on its performance, showing that, under some
assumptions, this algorithm recovers all clusters of size ?(log n) using O(n
log^2 n) similarities and runs in O(n log^3 n) time for a dataset of n objects.
Through extensive experimentation we also demonstrate that this framework is
practically alluring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4673</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4673</id><created>2012-06-18</created><authors><author><keyname>Yin</keyname><forenames>Junming</forenames><affiliation>Carnegie Mellon University</affiliation></author><author><keyname>Chen</keyname><forenames>Xi</forenames><affiliation>Carnegie Mellon University</affiliation></author><author><keyname>Xing</keyname><forenames>Eric</forenames><affiliation>Carnegie Mellon University</affiliation></author></authors><title>Group Sparse Additive Models</title><categories>cs.LG stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of sparse variable selection in nonparametric
additive models, with the prior knowledge of the structure among the covariates
to encourage those variables within a group to be selected jointly. Previous
works either study the group sparsity in the parametric setting (e.g., group
lasso), or address the problem in the non-parametric setting without exploiting
the structural information (e.g., sparse additive models). In this paper, we
present a new method, called group sparse additive models (GroupSpAM), which
can handle group sparsity in additive models. We generalize the l1/l2 norm to
Hilbert spaces as the sparsity-inducing penalty in GroupSpAM. Moreover, we
derive a novel thresholding condition for identifying the functional sparsity
at the group level, and propose an efficient block coordinate descent algorithm
for constructing the estimate. We demonstrate by simulation that GroupSpAM
substantially outperforms the competing methods in terms of support recovery
and prediction accuracy in additive models, and also conduct a comparative
experiment on a real breast cancer dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4674</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4674</id><created>2012-06-18</created><authors><author><keyname>Karbasi</keyname><forenames>Amin</forenames><affiliation>EPFL</affiliation></author><author><keyname>Ioannidis</keyname><forenames>Stratis</forenames><affiliation>Technicolor</affiliation></author><author><keyname>Massoulie</keyname><forenames>laurent</forenames><affiliation>Technicolor</affiliation></author></authors><title>Comparison-Based Learning with Rank Nets</title><categories>cs.LG cs.DS stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of search through comparisons, where a user is
presented with two candidate objects and reveals which is closer to her
intended target. We study adaptive strategies for finding the target, that
require knowledge of rank relationships but not actual distances between
objects. We propose a new strategy based on rank nets, and show that for target
distributions with a bounded doubling constant, it finds the target in a number
of comparisons close to the entropy of the target distribution and, hence, of
the optimum. We extend these results to the case of noisy oracles, and compare
this strategy to prior art over multiple datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4675</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4675</id><created>2012-06-18</created><authors><author><keyname>Haider</keyname><forenames>Peter</forenames><affiliation>University of Potsdam</affiliation></author><author><keyname>Scheffer</keyname><forenames>Tobias</forenames><affiliation>University of Potsdam</affiliation></author></authors><title>Finding Botnets Using Minimal Graph Clusterings</title><categories>cs.CR cs.DC cs.LG</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of identifying botnets and the IP addresses which they
comprise, based on the observation of a fraction of the global email spam
traffic. Observed mailing campaigns constitute evidence for joint botnet
membership, they are represented by cliques in the graph of all messages. No
evidence against an association of nodes is ever available. We reduce the
problem of identifying botnets to a problem of finding a minimal clustering of
the graph of messages. We directly model the distribution of clusterings given
the input graph; this avoids potential errors caused by distributional
assumptions of a generative model. We report on a case study in which we
evaluate the model by its ability to predict the spam campaign that a given IP
address is going to participate in.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4676</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4676</id><created>2012-06-18</created><authors><author><keyname>Yang</keyname><forenames>Zhirong</forenames><affiliation>Aalto University</affiliation></author><author><keyname>Oja</keyname><forenames>Erkki</forenames><affiliation>Aalto University</affiliation></author></authors><title>Clustering by Low-Rank Doubly Stochastic Matrix Decomposition</title><categories>cs.LG cs.CV cs.NA stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clustering analysis by nonnegative low-rank approximations has achieved
remarkable progress in the past decade. However, most approximation approaches
in this direction are still restricted to matrix factorization. We propose a
new low-rank learning method to improve the clustering performance, which is
beyond matrix factorization. The approximation is based on a two-step bipartite
random walk through virtual cluster nodes, where the approximation is formed by
only cluster assigning probabilities. Minimizing the approximation error
measured by Kullback-Leibler divergence is equivalent to maximizing the
likelihood of a discriminative model, which endows our method with a solid
probabilistic interpretation. The optimization is implemented by a relaxed
Majorization-Minimization algorithm that is advantageous in finding good local
minima. Furthermore, we point out that the regularized algorithm with Dirichlet
prior only serves as initialization. Experimental results show that the new
method has strong performance in clustering purity for various datasets,
especially for large-scale manifold data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4677</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4677</id><created>2012-06-18</created><authors><author><keyname>Plessis</keyname><forenames>Marthinus Du</forenames><affiliation>Tokyo Institute of Technology</affiliation></author><author><keyname>Sugiyama</keyname><forenames>Masashi</forenames><affiliation>Tokyo Institute of Technology</affiliation></author></authors><title>Semi-Supervised Learning of Class Balance under Class-Prior Change by
  Distribution Matching</title><categories>cs.LG stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In real-world classification problems, the class balance in the training
dataset does not necessarily reflect that of the test dataset, which can cause
significant estimation bias. If the class ratio of the test dataset is known,
instance re-weighting or resampling allows systematical bias correction.
However, learning the class ratio of the test dataset is challenging when no
labeled data is available from the test domain. In this paper, we propose to
estimate the class ratio in the test dataset by matching probability
distributions of training and test input data. We demonstrate the utility of
the proposed approach through experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4678</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4678</id><created>2012-06-18</created><authors><author><keyname>Hazan</keyname><forenames>Elad</forenames><affiliation>Technion</affiliation></author><author><keyname>Koren</keyname><forenames>Tomer</forenames><affiliation>Technion</affiliation></author></authors><title>Linear Regression with Limited Observation</title><categories>cs.LG stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the most common variants of linear regression, including Ridge,
Lasso and Support-vector regression, in a setting where the learner is allowed
to observe only a fixed number of attributes of each example at training time.
We present simple and efficient algorithms for these problems: for Lasso and
Ridge regression they need the same total number of attributes (up to
constants) as do full-information algorithms, for reaching a certain accuracy.
For Support-vector regression, we require exponentially less attributes
compared to the state of the art. By that, we resolve an open problem recently
posed by Cesa-Bianchi et al. (2010). Experiments show the theoretical bounds to
be justified by superior performance compared to the state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4679</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4679</id><created>2012-06-18</created><authors><author><keyname>Fujimaki</keyname><forenames>Ryohei</forenames><affiliation>NEC Laboratories America</affiliation></author><author><keyname>Hayashi</keyname><forenames>Kohei</forenames><affiliation>Nara Institute of Science and Technology</affiliation></author></authors><title>Factorized Asymptotic Bayesian Hidden Markov Models</title><categories>cs.LG stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the issue of model selection for hidden Markov models
(HMMs). We generalize factorized asymptotic Bayesian inference (FAB), which has
been recently developed for model selection on independent hidden variables
(i.e., mixture models), for time-dependent hidden variables. As with FAB in
mixture models, FAB for HMMs is derived as an iterative lower bound
maximization algorithm of a factorized information criterion (FIC). It
inherits, from FAB for mixture models, several desirable properties for
learning HMMs, such as asymptotic consistency of FIC with marginal
log-likelihood, a shrinkage effect for hidden state selection, monotonic
increase of the lower FIC bound through the iterative optimization. Further, it
does not have a tunable hyper-parameter, and thus its model selection process
can be fully automated. Experimental results shows that FAB outperforms
states-of-the-art variational Bayesian HMM and non-parametric Bayesian HMM in
terms of model selection accuracy and computational efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4680</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4680</id><created>2012-06-18</created><authors><author><keyname>Koepke</keyname><forenames>Hoyt</forenames><affiliation>University of Washington</affiliation></author><author><keyname>Bilenko</keyname><forenames>Mikhail</forenames><affiliation>Microsoft Research</affiliation></author></authors><title>Fast Prediction of New Feature Utility</title><categories>cs.LG math.ST stat.TH</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the new feature utility prediction problem: statistically testing
whether adding a new feature to the data representation can improve predictive
accuracy on a supervised learning task. In many applications, identifying new
informative features is the primary pathway for improving performance. However,
evaluating every potential feature by re-training the predictor with it can be
costly. The paper describes an efficient, learner-independent technique for
estimating new feature utility without re-training based on the current
predictor's outputs. The method is obtained by deriving a connection between
loss reduction potential and the new feature's correlation with the loss
gradient of the current predictor. This leads to a simple yet powerful
hypothesis testing procedure, for which we prove consistency. Our theoretical
analysis is accompanied by empirical evaluation on standard benchmarks and a
large-scale industrial dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4681</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4681</id><created>2012-06-18</created><authors><author><keyname>Pletscher</keyname><forenames>Patrick</forenames><affiliation>ETH Zurich</affiliation></author><author><keyname>Wulff</keyname><forenames>Sharon</forenames><affiliation>ETH Zurich</affiliation></author></authors><title>LPQP for MAP: Putting LP Solvers to Better Use</title><categories>cs.LG stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MAP inference for general energy functions remains a challenging problem.
While most efforts are channeled towards improving the linear programming (LP)
based relaxation, this work is motivated by the quadratic programming (QP)
relaxation. We propose a novel MAP relaxation that penalizes the
Kullback-Leibler divergence between the LP pairwise auxiliary variables, and QP
equivalent terms given by the product of the unaries. We develop two efficient
algorithms based on variants of this relaxation. The algorithms minimize the
non-convex objective using belief propagation and dual decomposition as
building blocks. Experiments on synthetic and real-world data show that the
solutions returned by our algorithms substantially improve over the LP
relaxation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4682</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4682</id><created>2012-06-18</created><authors><author><keyname>Poczos</keyname><forenames>Barnabas</forenames><affiliation>Carnegie Mellon University,</affiliation></author><author><keyname>Ghahramani</keyname><forenames>Zoubin</forenames><affiliation>University of Cambridge</affiliation></author><author><keyname>Schneider</keyname><forenames>Jeff</forenames><affiliation>Carnegie Mellon University,</affiliation></author></authors><title>Copula-based Kernel Dependency Measures</title><categories>cs.LG math.ST stat.ML stat.TH</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents a new copula based method for measuring dependence between
random variables. Our approach extends the Maximum Mean Discrepancy to the
copula of the joint distribution. We prove that this approach has several
advantageous properties. Similarly to Shannon mutual information, the proposed
dependence measure is invariant to any strictly increasing transformation of
the marginal variables. This is important in many applications, for example in
feature selection. The estimator is consistent, robust to outliers, and uses
rank statistics only. We derive upper bounds on the convergence rate and
propose independence tests too. We illustrate the theoretical contributions
through a series of experiments in feature selection and low-dimensional
embedding of distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4683</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4683</id><created>2012-06-18</created><authors><author><keyname>Chen</keyname><forenames>Minmin</forenames><affiliation>Washington University</affiliation></author><author><keyname>Xu</keyname><forenames>Zhixiang</forenames><affiliation>Washington University</affiliation></author><author><keyname>Weinberger</keyname><forenames>Kilian</forenames><affiliation>Washington University</affiliation></author><author><keyname>Sha</keyname><forenames>Fei</forenames><affiliation>University of Southern California</affiliation></author></authors><title>Marginalized Denoising Autoencoders for Domain Adaptation</title><categories>cs.LG</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stacked denoising autoencoders (SDAs) have been successfully used to learn
new representations for domain adaptation. Recently, they have attained record
accuracy on standard benchmark tasks of sentiment analysis across different
text domains. SDAs learn robust data representations by reconstruction,
recovering original features from data that are artificially corrupted with
noise. In this paper, we propose marginalized SDA (mSDA) that addresses two
crucial limitations of SDAs: high computational cost and lack of scalability to
high-dimensional features. In contrast to SDAs, our approach of mSDA
marginalizes noise and thus does not require stochastic gradient descent or
other optimization algorithms to learn parameters ? in fact, they are computed
in closed-form. Consequently, mSDA, which can be implemented in only 20 lines
of MATLAB^{TM}, significantly speeds up SDAs by two orders of magnitude.
Furthermore, the representations learnt by mSDA are as effective as the
traditional SDAs, attaining almost identical accuracies in benchmark tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4684</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4684</id><created>2012-06-18</created><authors><author><keyname>Purushotham</keyname><forenames>Sanjay</forenames><affiliation>Univ. of Southern California</affiliation></author><author><keyname>Liu</keyname><forenames>Yan</forenames><affiliation>Univ. of Southern California</affiliation></author><author><keyname>Kuo</keyname><forenames>C. -C. Jay</forenames><affiliation>Univ. of Southern California</affiliation></author></authors><title>Collaborative Topic Regression with Social Matrix Factorization for
  Recommendation Systems</title><categories>cs.IR cs.SI</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social network websites, such as Facebook, YouTube, Lastfm etc, have become a
popular platform for users to connect with each other and share content or
opinions. They provide rich information for us to study the influence of user's
social circle in their decision process. In this paper, we are interested in
examining the effectiveness of social network information to predict the user's
ratings of items. We propose a novel hierarchical Bayesian model which jointly
incorporates topic modeling and probabilistic matrix factorization of social
networks. A major advantage of our model is to automatically infer useful
latent topics and social information as well as their importance to
collaborative filtering from the training data. Empirical experiments on two
large-scale datasets show that our algorithm provides a more effective
recommendation system than the state-of-the art approaches. Our results reveal
interesting insight that the social circles have more influence on people's
decisions about the usefulness of information (e.g., bookmarking preference on
Delicious) than personal taste (e.g., music preference on Lastfm). We also
examine and discuss solutions on potential information leak in many
recommendation systems that utilize social information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4685</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4685</id><created>2012-06-18</created><authors><author><keyname>Liu</keyname><forenames>Yan</forenames><affiliation>USC</affiliation></author><author><keyname>Bahadori</keyname><forenames>Taha</forenames><affiliation>USC</affiliation></author><author><keyname>Li</keyname><forenames>Hongfei</forenames><affiliation>IBM T.J. Watson Research Center</affiliation></author></authors><title>Sparse-GEV: Sparse Latent Space Model for Multivariate Extreme Value
  Time Serie Modeling</title><categories>stat.ME cs.LG stat.AP</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many applications of time series models, such as climate analysis and
social media analysis, we are often interested in extreme events, such as
heatwave, wind gust, and burst of topics. These time series data usually
exhibit a heavy-tailed distribution rather than a Gaussian distribution. This
poses great challenges to existing approaches due to the significantly
different assumptions on the data distributions and the lack of sufficient past
data on extreme events. In this paper, we propose the Sparse-GEV model, a
latent state model based on the theory of extreme value modeling to
automatically learn sparse temporal dependence and make predictions. Our model
is theoretically significant because it is among the first models to learn
sparse temporal dependencies among multivariate extreme value time series. We
demonstrate the superior performance of our algorithm to the state-of-art
methods, including Granger causality, copula approach, and transfer entropy, on
one synthetic dataset, one climate dataset and two Twitter datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4686</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4686</id><created>2012-06-18</created><authors><author><keyname>Bonilla</keyname><forenames>Edwin</forenames><affiliation>NICTA</affiliation></author><author><keyname>Robles-Kelly</keyname><forenames>Antonio</forenames><affiliation>NICTA</affiliation></author></authors><title>Discriminative Probabilistic Prototype Learning</title><categories>cs.LG stat.ML</categories><comments>ICML2012</comments><proxy>icml2012</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a simple yet powerful method for learning
representations in supervised learning scenarios where each original input
datapoint is described by a set of vectors and their associated outputs may be
given by soft labels indicating, for example, class probabilities. We represent
an input datapoint as a mixture of probabilities over the corresponding set of
feature vectors where each probability indicates how likely each vector is to
belong to an unknown prototype pattern. We propose a probabilistic model that
parameterizes these prototype patterns in terms of hidden variables and
therefore it can be trained with conventional approaches based on likelihood
maximization. More importantly, both the model parameters and the prototype
patterns can be learned from data in a discriminative way. We show that our
model can be seen as a probabilistic generalization of learning vector
quantization (LVQ). We apply our method to the problems of shape
classification, hyperspectral imaging classification and people's work class
categorization, showing the superior performance of our method compared to the
standard prototype-based classification approach and other competitive
benchmark methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4687</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4687</id><created>2012-06-19</created><authors><author><keyname>Ding</keyname><forenames>Cunsheng</forenames></author></authors><title>Cyclic Codes from APN and Planar Functions</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1206.4370</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cyclic codes are a subclass of linear codes and have applications in consumer
electronics, data storage systems, and communication systems as they have
efficient encoding and decoding algorithms. In this paper, almost perfect
nonlinear functions and planar functions over finite fields are employed to
construct a number of classes of cyclic codes. Lower bounds on the minimum
weight of some classes of the cyclic codes are developed. The minimum weights
of some other classes of the codes constructed in this paper are determined.
The dimensions of the codes are flexible. Many of the codes presented in this
paper are optimal or almost optimal in the sense that they meet some bound on
linear codes. Ten open problems regarding cyclic codes from highly nonlinear
functions are also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4691</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4691</id><created>2012-06-20</created><authors><author><keyname>Kauki&#x10d;</keyname><forenames>Michal</forenames></author></authors><title>Equal Aperture Angles Curve for some Convex Sets in the Plane</title><categories>math.MG cs.CG math.NA</categories><comments>6 pages, 8 figures, Conference OSSConf 2012, \v{Z}ilina, Slovakia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For given convex set $K$ in the plane (not necessarily bounded), we can
construct the curve $C$ for which the visibility (aperture) angle of this set
has the same, prescribed value. We give the implicit formula for $C$, discuss
some issues concerning practical computations of $C$ and bring several simple
examples, when the equal visibility angle curves can be effectively computed
explicitly. We conclude with some remarks about possible directions for further
research in this area. Extensive use of Open Source software (Sage, Pylab,
IPython,...) is a key feature of this article.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4708</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4708</id><created>2012-06-20</created><authors><author><keyname>Vlad</keyname><forenames>Serban E.</forenames></author></authors><title>On the serial connection of the regular asynchronous systems</title><categories>cs.GL</categories><comments>9 pages; ROMAI Journal, Vol. 7, Nr. 2, 2011</comments><msc-class>94C99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The asynchronous systems f are multi-valued functions, representing the
non-deterministic models of the asynchronous circuits from the digital
electrical engineering. In real time, they map an 'admissible input' function
u:R\rightarrow{0,1}^{m} to a set f(u) of 'possible states' x\inf(u), where
x:R\rightarrow{0,1}^{m}. When f is defined by making use of a 'generator
function' {\Phi}:{0,1}^{n}\times{0,1}^{m}\rightarrow{0,1}^{n}, the system is
called regular. The usual definition of the serial connection of systems as
composition of multi-valued functions does not bring the regular systems into
regular systems, thus the first issue in this study is to modify in an
acceptable manner the definition of the serial connection in a way that matches
regularity. This intention was expressed for the first time, without proving
the regularity of the serial connection of systems, in a previous work. Our
present purpose is to restate with certain corrections and prove that result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4710</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4710</id><created>2012-06-20</created><updated>2013-07-20</updated><authors><author><keyname>Vlad</keyname><forenames>Serban E.</forenames></author></authors><title>On the basins of attraction of the regular autonomous asynchronous
  systems</title><categories>cs.OH</categories><comments>21 pages, Acta universitatis apulensis, Mathematics-informatics,
  special issue, 2011. arXiv admin note: substantial text overlap with
  arXiv:1012.5838</comments><msc-class>94C10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Boolean autonomous dynamical systems, also called regular autonomous
asynchronous systems are systems whose 'vector field' is a function
{\Phi}:{0,1}^{n}{\to}{0,1}^{n} and time is discrete or continuous. While the
synchronous systems have their coordinate functions {\Phi}_{1},...,{\Phi}_{n}
computed at the same time:
{\Phi},{\Phi}{\circ}{\Phi},{\Phi}{\circ}{\Phi}{\circ}{\Phi},... the
asynchronous systems have {\Phi}_{1},...,{\Phi}_{n} computed independently on
each other. The purpose of the paper is that of studying the basins of
attraction of the fixed points, of the orbits and of the {\omega}-limit sets of
the regular autonomous asynchronous systems. The bibliography consists in
analogies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4713</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4713</id><created>2012-06-20</created><updated>2013-07-20</updated><authors><author><keyname>Vlad</keyname><forenames>Serban E.</forenames></author></authors><title>Universal Regular Autonomous Asynchronous Systems: Fixed Points,
  Equivalencies and Dynamic Bifurcations</title><categories>cs.OH</categories><comments>21 pages, 11 figures, ROMAI Journal, Vol. 5, Nr. 1, 2009</comments><msc-class>94C99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The asynchronous systems are the non-deterministic models of the asynchronous
circuits from the digital electrical engineering. In the autonomous version,
such a system is a set of functions x:R{\to}{0,1}^{n} called states (R is the
time set). If an asynchronous system is defined by making use of a so called
generator function {\Phi}:{0,1}^{n}{\to}{0,1}^{n}, then it is called regular.
The property of universality means the greatest in the sense of the inclusion.
The purpose of the paper is that of defining and of characterizing the fixed
points, the equivalencies and the dynamical bifurcations of the universal
regular autonomous asynchronous systems. We use analogies with the dynamical
systems theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4717</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4717</id><created>2012-06-20</created><authors><author><keyname>Vlad</keyname><forenames>Serban E.</forenames></author></authors><title>The decomposition of the regular asynchronous systems as parallel
  connection of regular asynchronous systems</title><categories>cs.OH</categories><comments>13 pages, the Proceedings of ICTAMI, Alba Iulia, September 3-6, 2009</comments><msc-class>94C99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The asynchronous systems are the non-deterministic models of the asynchronous
circuits from the digital electrical engineering, where non-determinism is a
consequence of the fact that modelling is made in the presence of unknown and
variable parameters. Such a system is a multi-valued function f that assigns to
an (admissible) input u:R{\to}{0,1}^{m} a set f(u) of (possible) states
x:R{\to}{0,1}^{n}. When this assignment is defined by making use of a so-called
generator function {\Phi}:{0,1}^{n}{\times}{0,1}^{m}{\to}{0,1}^{n}, then the
asynchronous system f is called regular. The generator function {\Phi} acts in
this asynchronous framework similarly with the next state function from a
synchronous framework. The parallel connection of the asynchronous systems f'
and f&quot; is the asynchronous system (f'||f&quot;)(u)=f'(u){\times}f&quot;(u). The purpose
of the paper is to give the circumstances under which a regular asynchronous
system f may be written as a parallel connection of regular asynchronous
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4723</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4723</id><created>2012-06-20</created><authors><author><keyname>Madhusudhanan</keyname><forenames>Prasanna</forenames><affiliation>Eugene</affiliation></author><author><keyname>Restrepo</keyname><forenames>Juan G.</forenames><affiliation>Eugene</affiliation></author><author><keyname>Youjian</keyname><affiliation>Eugene</affiliation></author><author><keyname>Liu</keyname></author><author><keyname>Brown</keyname><forenames>Timothy X.</forenames></author></authors><title>Downlink Coverage Analysis in a Heterogeneous Cellular Network</title><categories>cs.IT math.IT</categories><comments>6 pages, 5 figures, submitted to IEEE Globecom 2012 - Wireless
  Communications Symposium on Apr 2, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the downlink signal-to-interference-plus-noise
ratio (SINR) analysis in a heterogeneous cellular network with K tiers. Each
tier is characterized by a base-station (BS) arrangement according to a
homogeneous Poisson point process with certain BS density, transmission power,
random shadow fading factors with arbitrary distribution, arbitrary path-loss
exponent and a certain bias towards admitting the mobile-station (MS). The MS
associates with the BS that has the maximum SINR under the open access cell
association scheme. For such a general setting, we provide an analytical
characterization of the coverage probability at the MS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4728</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4728</id><created>2012-06-20</created><updated>2012-09-10</updated><authors><author><keyname>Couvreur</keyname><forenames>Alain</forenames></author></authors><title>Codes and the Cartier Operator</title><categories>math.NT cs.IT math.AG math.IT</categories><msc-class>11G20, 14G50, 94B27</msc-class><journal-ref>Proc. Amer. Math. Soc. 142(6), 1983-1996, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we present a new construction of codes from algebraic
curves. Given a curve over a non-prime finite field, the obtained codes are
defined over a subfield. We call them Cartier Codes since their construction
involves the Cartier operator. This new class of codes can be regarded as a
natural geometric generalisation of classical Goppa codes. In particular, we
prove that a well-known property satisfied by classical Goppa codes extends
naturally to Cartier codes. We prove general lower bounds for the dimension and
the minimum distance of these codes and compare our construction with a
classical one: the subfield subcodes of Algebraic Geometry codes. We prove that
every Cartier code is contained in a subfield subcode of an Algebraic Geometry
code and that the two constructions have similar asymptotic performances.
  We also show that some known results on subfield subcodes of Algebraic
Geometry codes can be proved nicely by using properties of the Cartier operator
and that some known bounds on the dimension of subfield subcodes of Algebraic
Geometry codes can be improved thanks to Cartier codes and the Cartier
operator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4753</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4753</id><created>2012-06-20</created><updated>2012-10-07</updated><authors><author><keyname>Liu</keyname><forenames>Daofu</forenames></author><author><keyname>Chen</keyname><forenames>Yunji</forenames></author><author><keyname>Guo</keyname><forenames>Qi</forenames></author><author><keyname>Chen</keyname><forenames>Tianshi</forenames></author><author><keyname>Li</keyname><forenames>Ling</forenames></author><author><keyname>Dong</keyname><forenames>Qunfeng</forenames></author><author><keyname>Hu</keyname><forenames>Weiwu</forenames></author></authors><title>DLS: Directoryless Shared Last-level Cache</title><categories>cs.AR</categories><comments>This paper has been withdrawn by the authors due to a major revision</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Directory-based protocols have been the de facto solution for maintaining
cache coherence in shared-memory parallel systems comprising multi/many cores,
where each store instruction is eagerly made globally visible by invalidating
the private cache (PC) backups of other cores. Consequently, the directory not
only consumes large chip area, but also incurs considerable energy consumption
and performance degradation, due to the large number of Invalidation/Ack
messages transferred in the interconnection network and resulting network
congestion. In this paper, we reveal the interesting fact that the directory is
actually an unnecessary luxury for practical parallel systems. Because of
widely deployed software/hardware techniques involving instruction reordering,
most (if not all) parallel systems work under the weak consistency model, where
a remote store instruction is allowed to be invisible to a core before the next
synchronization of the core, instead of being made visible eagerly by
invalidating PC backups of other cores. Based on this key observation, we
propose a lightweight novel scheme called {\em DLS (DirectoryLess Shared
last-level cache)}, which completely removes the directory and Invalidation/Ack
messages, and efficiently maintains cache coherence using a novel {\em
self-suspicion + speculative execution} mechanism. Experimental results over
SPLASH-2 benchmarks show that on a 16-core processor, DLS not only completely
removes the chip area cost of the directory, but also improves processor
performance by 11.08%, reduces overall network traffic by 28.83%, and reduces
energy consumption of the network by 15.65% on average (compared with
traditional MESI protocol with full directory). Moreover, DLS does not involve
any modification to programming languages and compilers, and hence is
seamlessly compatible with legacy codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4755</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4755</id><created>2012-06-20</created><authors><author><keyname>Ayach</keyname><forenames>Omar El</forenames></author><author><keyname>Peters</keyname><forenames>Steven W.</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>The Practical Challenges of Interference Alignment</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Wireless Communications Magazine</comments><journal-ref>IEEE Wireless Communications Magazine, vol. 20, no. 1, pp. 35-42,
  February 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interference alignment (IA) is a revolutionary wireless transmission strategy
that reduces the impact of interference. The idea of interference alignment is
to coordinate multiple transmitters so that their mutual interference aligns at
the receivers, facilitating simple interference cancellation techniques. Since
IA's inception, researchers have investigated its performance and proposed
improvements, verifying IA's ability to achieve the maximum degrees of freedom
(an approximation of sum capacity) in a variety of settings, developing
algorithms for determining alignment solutions, and generalizing transmission
strategies that relax the need for perfect alignment but yield better
performance. This article provides an overview of the concept of interference
alignment as well as an assessment of practical issues including performance in
realistic propagation environments, the role of channel state information at
the transmitter, and the practicality of interference alignment in large
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4767</identifier>
 <datestamp>2012-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4767</id><created>2012-06-20</created><updated>2012-06-24</updated><authors><author><keyname>Lin</keyname><forenames>Pin-Hsun</forenames></author><author><keyname>Su</keyname><forenames>Chien-Li</forenames></author><author><keyname>Su</keyname><forenames>Hsuan-Jung</forenames></author></authors><title>On the Secrecy Rate Region of a Fading Multiple-Antenna Gaussian
  Broadcast Channel with Confidential Messages and Partial CSIT</title><categories>cs.IT math.IT</categories><comments>6 pages, conference, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the secure transmission over the fast fading
multiple antenna Gaussian broadcast channels with confidential messages
(FMGBC-CM), where a multiple-antenna transmitter sends independent confidential
messages to two users with information theoretic secrecy and only the
statistics of the receivers' channel state information are known at the
transmitter. We first use the same marginal property of the FMGBC-CM to
classify the non-trivial cases, i.e., those not degraded to the common wiretap
channels. We then derive the achievable rate region for the FMGBC-CM by solving
the channel input covariance matrices and the inflation factor. Due to the
complicated rate region formulae, we resort to low SNR analysis to investigate
the characteristics of the channel. Finally, the numerical examples show that
under the information-theoretic secrecy requirement both users can achieve
positive rates simultaneously.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4771</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4771</id><created>2012-06-20</created><authors><author><keyname>Syrgkanis</keyname><forenames>Vasilis</forenames></author><author><keyname>Tardos</keyname><forenames>Eva</forenames></author></authors><title>Bayesian Sequential Auctions</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many natural settings agents participate in multiple different auctions
that are not simultaneous. In such auctions, future opportunities affect
strategic considerations of the players. The goal of this paper is to develop a
quantitative understanding of outcomes of such sequential auctions. In earlier
work (Paes Leme et al. 2012) we initiated the study of the price of anarchy in
sequential auctions. We considered sequential first price auctions in the full
information model, where players are aware of all future opportunities, as well
as the valuation of all players. In this paper, we study efficiency in
sequential auctions in the Bayesian environment, relaxing the informational
assumption on the players. We focus on two environments, both studied in the
full information model in Paes Leme et al. 2012, matching markets and matroid
auctions. In the full information environment, a sequential first price cut
auction for matroid settings is efficient. In Bayesian environments this is no
longer the case, as we show using a simple example with three players. Our main
result is a bound of $1+\frac{e}{e-1}\approx 2.58$ on the price of anarchy in
both matroid auctions and single-value matching markets (even with correlated
types) and a bound of $2\frac{e}{e-1}\approx 3.16$ for general matching markets
with independent types. To bound the price of anarchy we need to consider
possible deviations at an equilibrium. In a sequential Bayesian environment the
effect of deviations is more complex than in one-shot games; early bids allow
others to infer information about the player's value. We create effective
deviations despite the presence of this difficulty by introducing a bluffing
technique of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4781</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4781</id><created>2012-06-21</created><authors><author><keyname>Zhang</keyname><forenames>Yan</forenames></author><author><keyname>Wang</keyname><forenames>Lin</forenames></author><author><keyname>Zhang</keyname><forenames>Yi-Qing</forenames></author><author><keyname>Li</keyname><forenames>Xiang</forenames></author></authors><title>Towards a temporal network analysis of interactive WiFi users</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>6 pages, 6 figures</comments><journal-ref>EPL 98(2012) 68002</journal-ref><doi>10.1209/0295-5075/98/68002</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Complex networks are used to depict topological features of complex systems.
The structure of a network characterizes the interactions among elements of the
system, and facilitates the study of many dynamical processes taking place on
it. In previous investigations, the topological infrastructure underlying
dynamical systems is simplified as a static and invariable skeleton. However,
this assumption cannot cover the temporal features of many time-evolution
networks, whose components are evolving and mutating. In this letter, utilizing
the log data of WiFi users in a Chinese university campus, we infuse the
temporal dimension into the construction of dynamical human contact network. By
quantitative comparison with the traditional aggregation approach, we find that
the temporal contact network differs in many features, e.g., the reachability,
the path length distribution. We conclude that the correlation between temporal
path length and duration is not only determined by their definitions, but also
influenced by the microdynamical features of human activities under certain
social circumstance as well. The time order of individuals' interaction events
plays a critical role in understanding many dynamical processes via human close
proximity interactions studied in this letter. Besides, our study also provides
a promising measure to identify the potential superspreaders by distinguishing
the nodes functioning as the relay hub.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4802</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4802</id><created>2012-06-21</created><authors><author><keyname>Schaer</keyname><forenames>Philipp</forenames></author></authors><title>Better Than Their Reputation? On the Reliability of Relevance
  Assessments with Students</title><categories>cs.IR</categories><comments>12 pages, to be published in Proceedings of Conference and Labs of
  the Evaluation Forum 2012 (CLEF 2012)</comments><acm-class>H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During the last three years we conducted several information retrieval
evaluation series with more than 180 LIS students who made relevance
assessments on the outcomes of three specific retrieval services. In this study
we do not focus on the retrieval performance of our system but on the relevance
assessments and the inter-assessor reliability. To quantify the agreement we
apply Fleiss' Kappa and Krippendorff's Alpha. When we compare these two
statistical measures on average Kappa values were 0.37 and Alpha values 0.15.
We use the two agreement measures to drop too unreliable assessments from our
data set. When computing the differences between the unfiltered and the
filtered data set we see a root mean square error between 0.02 and 0.12. We see
this as a clear indicator that disagreement affects the reliability of
retrieval evaluations. We suggest not to work with unfiltered results or to
clearly document the disagreement rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4809</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4809</id><created>2012-06-21</created><authors><author><keyname>Brattka</keyname><forenames>Vasco</forenames></author><author><keyname>Roux</keyname><forenames>St&#xe9;phane Le</forenames></author><author><keyname>Pauly</keyname><forenames>Arno</forenames></author></authors><title>Connected Choice and the Brouwer Fixed Point Theorem</title><categories>math.LO cs.LO</categories><comments>26 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the computational content of the Brouwer Fixed Point Theorem in the
Weihrauch lattice. One of our main results is that for any fixed dimension the
Brouwer Fixed Point Theorem of that dimension is computably equivalent to
connected choice of the Euclidean unit cube of the same dimension. Connected
choice is the operation that finds a point in a non-empty connected closed set
given by negative information. Another main result is that connected choice is
complete for dimension greater or equal to three in the sense that it is
computably equivalent to Weak K\H{o}nig's Lemma. In contrast to this, the
connected choice operations in dimensions zero, one and two form a strictly
increasing sequence of Weihrauch degrees, where connected choice of dimension
one is known to be equivalent to the Intermediate Value Theorem. Whether
connected choice of dimension two is strictly below connected choice of
dimension three or equivalent to it is unknown, but we conjecture that the
reduction is strict. As a side result we also prove that finding a
connectedness component in a closed subset of the Euclidean unit cube of any
dimension greater or equal to one is equivalent to Weak K\H{o}nig's Lemma. In
order to describe all these results we introduce a representation of closed
subsets of the unit cube by trees of rational complexes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4812</identifier>
 <datestamp>2013-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4812</id><created>2012-06-21</created><updated>2013-06-11</updated><authors><author><keyname>Galtier</keyname><forenames>Mathieu</forenames></author><author><keyname>Wainrib</keyname><forenames>Gilles</forenames></author></authors><title>A biological gradient descent for prediction through a combination of
  STDP and homeostatic plasticity</title><categories>q-bio.NC cs.NE math.DS</categories><comments>36 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identifying, formalizing and combining biological mechanisms which implement
known brain functions, such as prediction, is a main aspect of current research
in theoretical neuroscience. In this letter, the mechanisms of Spike Timing
Dependent Plasticity (STDP) and homeostatic plasticity, combined in an original
mathematical formalism, are shown to shape recurrent neural networks into
predictors. Following a rigorous mathematical treatment, we prove that they
implement the online gradient descent of a distance between the network
activity and its stimuli. The convergence to an equilibrium, where the network
can spontaneously reproduce or predict its stimuli, does not suffer from
bifurcation issues usually encountered in learning in recurrent neural
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4822</identifier>
 <datestamp>2012-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4822</id><created>2012-06-21</created><updated>2012-12-05</updated><authors><author><keyname>Saidi</keyname><forenames>Rabie</forenames></author><author><keyname>Aridhi</keyname><forenames>Sabeur</forenames></author><author><keyname>Maddouri</keyname><forenames>Mondher</forenames></author><author><keyname>Nguifo</keyname><forenames>Engelbert Mephu</forenames></author></authors><title>Feature extraction in protein sequences classification : a new stability
  measure</title><categories>cs.LG cs.CE q-bio.QM</categories><comments>The paper has been accepted by the ACM Conference on Bioinformatics,
  Computational Biology and Biomedicine (ACM BCB) 2012. We want to cancel the
  submission because of the double entries of the paper in DBLP. Thank you for
  your understanding</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature extraction is an unavoidable task, especially in the critical step of
preprocessing biological sequences. This step consists for example in
transforming the biological sequences into vectors of motifs where each motif
is a subsequence that can be seen as a property (or attribute) characterizing
the sequence. Hence, we obtain an object-property table where objects are
sequences and properties are motifs extracted from sequences. This output can
be used to apply standard machine learning tools to perform data mining tasks
such as classification. Several previous works have described feature
extraction methods for bio-sequence classification, but none of them discussed
the robustness of these methods when perturbing the input data. In this work,
we introduce the notion of stability of the generated motifs in order to study
the robustness of motif extraction methods. We express this robustness in terms
of the ability of the method to reveal any change occurring in the input data
and also its ability to target the interesting motifs. We use these criteria to
evaluate and experimentally compare four existing extraction methods for
biological sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4832</identifier>
 <datestamp>2014-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4832</id><created>2012-06-21</created><updated>2014-07-03</updated><authors><author><keyname>Ghoshdastidar</keyname><forenames>Debarghya</forenames></author><author><keyname>Dukkipati</keyname><forenames>Ambedkar</forenames></author><author><keyname>Bhatnagar</keyname><forenames>Shalabh</forenames></author></authors><title>Smoothed Functional Algorithms for Stochastic Optimization using
  q-Gaussian Distributions</title><categories>cs.IT cs.LG math.IT stat.ME</categories><acm-class>G.1.6; I.6.8</acm-class><doi>10.1145/2628434</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Smoothed functional (SF) schemes for gradient estimation are known to be
efficient in stochastic optimization algorithms, specially when the objective
is to improve the performance of a stochastic system. However, the performance
of these methods depends on several parameters, such as the choice of a
suitable smoothing kernel. Different kernels have been studied in literature,
which include Gaussian, Cauchy and uniform distributions among others. This
paper studies a new class of kernels based on the q-Gaussian distribution, that
has gained popularity in statistical physics over the last decade. Though the
importance of this family of distributions is attributed to its ability to
generalize the Gaussian distribution, we observe that this class encompasses
almost all existing smoothing kernels. This motivates us to study SF schemes
for gradient estimation using the q-Gaussian distribution. Using the derived
gradient estimates, we propose two-timescale algorithms for optimization of a
stochastic objective function in a constrained setting with projected gradient
search approach. We prove the convergence of our algorithms to the set of
stationary points of an associated ODE. We also demonstrate their performance
numerically through simulations on a queuing model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4833</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4833</id><created>2012-06-21</created><authors><author><keyname>Brunel</keyname><forenames>Alo&#xef;s</forenames></author><author><keyname>Madet</keyname><forenames>Antoine</forenames></author></authors><title>Indexed realizability for bounded-time programming with references and
  type fixpoints</title><categories>cs.LO cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The field of implicit complexity has recently produced several
bounded-complexity programming languages. This kind of language allows to
implement exactly the functions belonging to a certain complexity class. We
here present a realizability semantics for a higher-order functional language
based on a fragment of linear logic called LAL which characterizes the
complexity class PTIME. This language features recursive types and higher-order
store. Our realizability is based on biorthogonality, step-indexing and is
moreover quantitative. This last feature enables us not only to derive a
semantical proof of termination, but also to give bounds on the number of
computational steps needed by typed programs to terminate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4835</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4835</id><created>2012-06-21</created><updated>2013-05-25</updated><authors><author><keyname>Mukherjee</keyname><forenames>Satyam</forenames></author></authors><title>Complex Network Analysis in Cricket : Community structure, player's role
  and performance index</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>31 Pages, 4 Figures and 7 Tables (Accepted for publication in
  Advanced in Complex Systems 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the applications of network methods for understanding
interaction within members of sport teams.We analyze the interaction of batsmen
in International Cricket matches. We generate batting partnership network (BPN)
for different teams and determine the exact values of clustering coefficient,
average degree, average shortest path length of the networks and compare them
with the Erd\text{\&quot;{o}}s-R\text{\'{e}}nyi model. We observe that the networks
display small-world behavior and are disassortative in nature. We find that
most connected batsman is not necessarily the most central and most central
players are not necessarily the one with high batting averages. We study the
community structure of the BPNs and identify each player's role based on
inter-community and intra-community links. We observe that {\it Sir DG
Bradman}, regarded as the best batsman in Cricket history does not occupy the
central position in the network $-$ the so-called connector hub. We extend our
analysis to quantify the performance, relative importance and effect of
removing a player from the team, based on different centrality scores.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4854</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4854</id><created>2012-06-21</created><updated>2014-01-18</updated><authors><author><keyname>Bulatov</keyname><forenames>Andrei A.</forenames></author><author><keyname>Marx</keyname><forenames>D&#xe1;niel</forenames></author></authors><title>Constraint satisfaction parameterized by solution size</title><categories>cs.CC cs.DS</categories><comments>To appear in SICOMP. Conference version in ICALP 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the constraint satisfaction problem (CSP) corresponding to a constraint
language (i.e., a set of relations) $\Gamma$, the goal is to find an assignment
of values to variables so that a given set of constraints specified by
relations from $\Gamma$ is satisfied. The complexity of this problem has
received substantial amount of attention in the past decade. In this paper we
study the fixed-parameter tractability of constraint satisfaction problems
parameterized by the size of the solution in the following sense: one of the
possible values, say 0, is &quot;free,&quot; and the number of variables allowed to take
other, &quot;expensive,&quot; values is restricted. A size constraint requires that
exactly $k$ variables take nonzero values. We also study a more refined version
of this restriction: a global cardinality constraint prescribes how many
variables have to be assigned each particular value. We study the parameterized
complexity of these types of CSPs where the parameter is the required number
$k$ of nonzero variables. As special cases, we can obtain natural and
well-studied parameterized problems such as Independent Set, Vertex Cover,
d-Hitting Set, Biclique, etc.
  In the case of constraint languages closed under substitution of constants,
we give a complete characterization of the fixed-parameter tractable cases of
CSPs with size constraints, and we show that all the remaining problems are
W[1]-hard. For CSPs with cardinality constraints, we obtain a similar
classification, but for some of the problems we are only able to show that they
are Biclique-hard. The exact parameterized complexity of the Biclique problem
is a notorious open problem, although it is believed to be W[1]-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4855</identifier>
 <datestamp>2012-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4855</id><created>2012-06-21</created><updated>2012-07-12</updated><authors><author><keyname>Garcia</keyname><forenames>Esther</forenames></author><author><keyname>Pedroche</keyname><forenames>Francisco</forenames></author><author><keyname>Romance</keyname><forenames>Miguel</forenames></author></authors><title>On the Localization of the Personalized PageRank of Complex Networks</title><categories>cs.DM cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper new results on personalized PageRank are shown. We consider
directed graphs that may contain dangling nodes. The main result presented
gives an analytical characterization of all the possible values of the
personalized PageRank for any node.We use this result to give a theoretical
justification of a recent model that uses the personalized PageRank to classify
users of Social Networks Sites. We introduce new concepts concerning
competitivity and leadership in complex networks. We also present some
theoretical techniques to locate leaders and competitors which are valid for
any personalization vector and by using only information related to the
adjacency matrix of the graph and the distribution of its dangling nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4860</identifier>
 <datestamp>2014-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4860</id><created>2012-06-21</created><updated>2014-02-13</updated><authors><author><keyname>Furia</keyname><forenames>Carlo A.</forenames></author></authors><title>Asynchronous Multi-Tape Automata Intersection: Undecidability and
  Approximation</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When their reading heads are allowed to move completely asynchronously,
finite-state automata with multiple tapes achieve a significant expressive
power, but also lose useful closure properties---closure under intersection, in
particular. This paper investigates to what extent it is still feasible to use
multi-tape automata as recognizer of polyadic predicates on words. On the
negative side, determining whether the intersection of asynchronous multi-tape
automata is expressible is not even semidecidable. On the positive side, we
present an algorithm that computes under-approximations of the intersection;
and discuss simple conditions under which it can construct complete
intersections. A prototype implementation and a few non-trivial examples
demonstrate the algorithm in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1206.4863</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1206.4863</id><created>2012-06-21</created><authors><author><keyname>Waltman</keyname><forenames>Ludo</forenames></author></authors><title>An empirical analysis of the use of alphabetical authorship in
  scientific publishing</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are different ways in which the authors of a scientific publication can
determine the order in which their names are listed. Sometimes author names are
simply listed alphabetically. In other cases, authorship order is determined
based on the contribution authors have made to a publication.
Contribution-based authorship can facilitate proper credit assignment, for
instance by giving most credits to the first author. In the case of
alphabetical authorship, nothing can be inferred about the relative
contribution made by the different authors of a publication. In this paper, we
present an empirical analysis of the use of alphabetical authorship in
scientific publishing. Our analysis covers all fields of science. We find that
the use of alphabetical authorship is declining over time. In 2011, the authors
of less than 4% of all publications intentionally chose to list their names
alphabetically. The use of alphabetical authorship is most common in
mathematics, economics (including finance), and high energy physics. Also, the
use of alphabetical authorship is relatively more common in the case of
publications with either a small or a large number of authors.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="32000" completeListSize="102538">1122234|33001</resumptionToken>
</ListRecords>
</OAI-PMH>
