<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T02:02:04Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|51001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1310.0807</identifier>
 <datestamp>2015-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.0807</id><created>2013-10-02</created><updated>2015-03-19</updated><authors><author><keyname>Chen</keyname><forenames>Yuxin</forenames></author><author><keyname>Chi</keyname><forenames>Yuejie</forenames></author><author><keyname>Goldsmith</keyname><forenames>Andrea</forenames></author></authors><title>Exact and Stable Covariance Estimation from Quadratic Sampling via
  Convex Programming</title><categories>cs.IT cs.LG math.IT math.NA math.ST stat.ML stat.TH</categories><comments>accepted to IEEE Transactions on Information Theory, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistical inference and information processing of high-dimensional data
often require efficient and accurate estimation of their second-order
statistics. With rapidly changing data, limited processing power and storage at
the acquisition devices, it is desirable to extract the covariance structure
from a single pass over the data and a small number of stored measurements. In
this paper, we explore a quadratic (or rank-one) measurement model which
imposes minimal memory requirements and low computational complexity during the
sampling process, and is shown to be optimal in preserving various
low-dimensional covariance structures. Specifically, four popular structural
assumptions of covariance matrices, namely low rank, Toeplitz low rank,
sparsity, jointly rank-one and sparse structure, are investigated, while
recovery is achieved via convex relaxation paradigms for the respective
structure.
  The proposed quadratic sampling framework has a variety of potential
applications including streaming data processing, high-frequency wireless
communication, phase space tomography and phase retrieval in optics, and
non-coherent subspace detection. Our method admits universally accurate
covariance estimation in the absence of noise, as soon as the number of
measurements exceeds the information theoretic limits. We also demonstrate the
robustness of this approach against noise and imperfect structural assumptions.
Our analysis is established upon a novel notion called the mixed-norm
restricted isometry property (RIP-$\ell_{2}/\ell_{1}$), as well as the
conventional RIP-$\ell_{2}/\ell_{2}$ for near-isotropic and bounded
measurements. In addition, our results improve upon the best-known phase
retrieval (including both dense and sparse signals) guarantees using PhaseLift
with a significantly simpler approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.0810</identifier>
 <datestamp>2013-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.0810</id><created>2013-10-01</created><authors><author><keyname>Vinay</keyname><forenames>Siri</forenames></author><author><keyname>Vaseekharan</keyname><forenames>Manoj</forenames></author><author><keyname>Mohamedally</keyname><forenames>Dean</forenames></author></authors><title>RoboRun: A gamification approach to control flow learning for young
  students with TouchDevelop</title><categories>cs.CY cs.HC</categories><comments>Promoto 2013 Workshop demo paper at SPLASH 2013</comments><report-no>PrMoTo/2013/08</report-no><msc-class>97Q60</msc-class><acm-class>K.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This demo paper introduces young students to writing code in a touch enabled
interactive maze game. Problem-based learning is given a gamified approach to
learning, while simultaneously introducing the TouchDevelop platform to build
basic first control flow algorithms and to learn about ordering and loops in
conditional statements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.0833</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.0833</id><created>2013-10-02</created><updated>2014-10-31</updated><authors><author><keyname>Aichholzer</keyname><forenames>Oswin</forenames></author><author><keyname>Hackl</keyname><forenames>Thomas</forenames></author><author><keyname>Orden</keyname><forenames>David</forenames></author><author><keyname>Pilz</keyname><forenames>Alexander</forenames></author><author><keyname>Saumell</keyname><forenames>Maria</forenames></author><author><keyname>Vogtenhuber</keyname><forenames>Birgit</forenames></author></authors><title>Flips in combinatorial pointed pseudo-triangulations with face degree at
  most four</title><categories>math.CO cs.CG cs.DM</categories><comments>21 pages, 24 figures. Accepted for publication in the special volume
  of International Journal of Computational Geometry &amp; Applications devoted to
  the XV Spanish Meeting on Computational Geometry</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the flip operation for combinatorial pointed
pseudo-triangulations where faces have size 3 or 4, so-called combinatorial
4-PPTs. We show that every combinatorial 4-PPT is stretchable to a geometric
pseudo-triangulation, which in general is not the case if faces may have size
larger than 4. Moreover, we prove that the flip graph of combinatorial 4-PPTs
is connected and has diameter $O(n^2)$, even in the case of labeled vertices
with fixed outer face. For this case we provide an $\Omega(n\log n)$ lower
bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.0864</identifier>
 <datestamp>2013-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.0864</id><created>2013-10-02</created><authors><author><keyname>Kester</keyname><forenames>Quist-Aphetsi</forenames></author></authors><title>Criminal Geographical Profiling: Using FCA for Visualization and
  Analysis of Crime Data</title><categories>cs.CY</categories><comments>Submitted to International Journal of Research in Engineering &amp;
  Advanced Technology (IJREAT).pp: 1-5.1.5.(2013). arXiv admin note:
  substantial text overlap with arXiv:1307.8112, arXiv:1307.7788</comments><journal-ref>International Journal of Research in Engineering &amp; Advanced
  Technology (IJREAT).pp: 1-5.1.5.(2013)</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Fighting criminal activities in our modern societies required the engagement
of intelligent information systems that can analyze crime data geographically
and enable new concepts to be deduced from it. These information systems should
be able to create visualization of such data as well as have the capability of
giving new incite of information, if data is updated whilst maintaining the
previously predicted patterns. This paper proposed the use of Formal Concept
Analysis, or Galois Lattices, a data analysis technique grounded on Lattice
Theory and Propositional Calculus, for the visualization and analysis of crime
data. This method considered the set of common and distinct attributes of
crimes in such a way that categorization are done based on related crime types,
geographical locations and the persons involved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.0865</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.0865</id><created>2013-10-02</created><updated>2014-03-05</updated><authors><author><keyname>Kekatos</keyname><forenames>Vassilis</forenames></author><author><keyname>Zhang</keyname><forenames>Yu</forenames></author><author><keyname>Giannakis</keyname><forenames>Georgios B.</forenames></author></authors><title>Electricity Market Forecasting via Low-Rank Multi-Kernel Learning</title><categories>stat.ML cs.LG cs.SY</categories><comments>10 pages</comments><doi>10.1109/JSTSP.2014.2336611</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The smart grid vision entails advanced information technology and data
analytics to enhance the efficiency, sustainability, and economics of the power
grid infrastructure. Aligned to this end, modern statistical learning tools are
leveraged here for electricity market inference. Day-ahead price forecasting is
cast as a low-rank kernel learning problem. Uniquely exploiting the market
clearing process, congestion patterns are modeled as rank-one components in the
matrix of spatio-temporally varying prices. Through a novel nuclear norm-based
regularization, kernels across pricing nodes and hours can be systematically
selected. Even though market-wide forecasting is beneficial from a learning
perspective, it involves processing high-dimensional market data. The latter
becomes possible after devising a block-coordinate descent algorithm for
solving the non-convex optimization problem involved. The algorithm utilizes
results from block-sparse vector recovery and is guaranteed to converge to a
stationary point. Numerical tests on real data from the Midwest ISO (MISO)
market corroborate the prediction accuracy, computational efficiency, and the
interpretative merits of the developed approach over existing alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.0867</identifier>
 <datestamp>2013-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.0867</id><created>2013-10-02</created><authors><author><keyname>Dong</keyname><forenames>Zheng</forenames></author><author><keyname>Samuel</keyname><forenames>Arjmand</forenames></author></authors><title>Touch-enabled Programming for the Lab of Things</title><categories>cs.PL cs.HC</categories><comments>3 pages, 3 figures, PROMOTO 2013, 1309.5500</comments><report-no>PrMoTo/2013/06</report-no><acm-class>D.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lab of Things (LoT, lab-of-things.com) is a research platform for
interconnection, programming, and large scale deployment of devices and
sensors. These devices and sensors can then be used for deployment of field
studies in a variety of research areas including elderly care, energy
management, and the like. LoT is built on top of HomeOS, a middle-ware
component, making interconnection of a wide range of devices possible. LoT also
provides cloud storage and remote monitoring capabilities. Traditionally
programming on the LoT platform has been done using C# in Microsoft Visual
Studio. While LoT programs developed on the .NET framework offer a rich set of
functionality, writing programs on LoT can be challenging for developers who
are not experienced with the technology involved. In this demonstration, we
introduce an innovative programming approach on the LoT platform by building a
Generic Application and creating corresponding libraries on the user-friendly
TouchDevelop (touchdevelop.com) programming environment. As an example, we
implemented the same functionality of the Lab of Things Alerts application
using the new Generic App. In addition to a touch-enabled programming
environment, the new approach also significantly saves time and effort
developers have to devote when creating a customized Lab of Things application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.0872</identifier>
 <datestamp>2013-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.0872</id><created>2013-10-02</created><authors><author><keyname>Lee</keyname><forenames>Heunchul</forenames></author><author><keyname>Kim</keyname><forenames>Taeyoon</forenames></author><author><keyname>Park</keyname><forenames>Wonwoo</forenames></author><author><keyname>Lim</keyname><forenames>Jonghan</forenames></author></authors><title>Link Performance Abstraction for Interference-Aware Communications (IAC)</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advanced co-channel interference aware signal detection has drawn research
attention during the recent development of Long Term Evolution-Advanced (LTE-A)
systems and the interference-aware communications (IAC) is currently being
studied by 3GPP. This paper investigates link performance abstraction for the
IAC systems employing maximum-likelihood detector (MLD). The link performance
of MLD can be estimated by combining two performance bounds, namely, linear
receiver and genie-aided maximum-likelihood (ML) receiver. It is shown that the
conventional static approach based on static parameterization, while working
well under moderate and weak interference conditions, fails to generate a
well-behaved solution in the strong interference case. Inspired by this
observation, we propose a new adaptive approach where the combining parameter
is adaptively adjusted according to instantaneous interference-to-signal ratio
(ISR). The basic idea is to exploit the probabilistic behavior of the optimal
combining ratio over the ISR. The link-level simulation results are provided to
verify the prediction accuracy of the proposed link abstraction method.
Moreover, we use the proposed link abstraction model as a link-to-system
interface mapping in system-level simulations to demonstrate the performance of
the IAC receiver in interference-limited LTE systems
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.0873</identifier>
 <datestamp>2013-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.0873</id><created>2013-10-02</created><updated>2013-10-04</updated><authors><author><keyname>Wang</keyname><forenames>Yang</forenames></author><author><keyname>Xu</keyname><forenames>Zhiqiang</forenames></author></authors><title>Phase Retrieval for Sparse Signals</title><categories>cs.IT math.IT math.NA</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to build up the theoretical framework for the
recovery of sparse signals from the magnitude of the measurement. We first
investigate the minimal number of measurements for the success of the recovery
of sparse signals without the phase information. We completely settle the
minimality question for the real case and give a lower bound for the complex
case. We then study the recovery performance of the $\ell_1$ minimization. In
particular, we present the null space property which, to our knowledge, is the
first sufficient and necessary condition for the success of $\ell_1$
minimization for $k$-sparse phase retrievable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.0883</identifier>
 <datestamp>2013-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.0883</id><created>2013-10-02</created><authors><author><keyname>Sunarso</keyname><forenames>Freddie</forenames></author><author><keyname>Venugopal</keyname><forenames>Srikumar</forenames></author><author><keyname>Lauro</keyname><forenames>Federico</forenames></author></authors><title>Scalable Protein Sequence Similarity Search using Locality-Sensitive
  Hashing and MapReduce</title><categories>cs.DC cs.CE</categories><report-no>UNSW CSE TR 201325</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Metagenomics is the study of environments through genetic sampling of their
microbiota. Metagenomic studies produce large datasets that are estimated to
grow at a faster rate than the available computational capacity. A key step in
the study of metagenome data is sequence similarity searching which is
computationally intensive over large datasets. Tools such as BLAST require
large dedicated computing infrastructure to perform such analysis and may not
be available to every researcher.
  In this paper, we propose a novel approach called ScalLoPS that performs
searching on protein sequence datasets using LSH (Locality-Sensitive Hashing)
that is implemented using the MapReduce distributed framework. ScalLoPS is
designed to scale across computing resources sourced from cloud computing
providers. We present the design and implementation of ScalLoPS followed by
evaluation with datasets derived from both traditional as well as metagenomic
studies. Our experiments show that with this method approximates the quality of
BLAST results while improving the scalability of protein sequence search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.0890</identifier>
 <datestamp>2013-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.0890</id><created>2013-10-02</created><authors><author><keyname>Liu</keyname><forenames>Fayao</forenames></author><author><keyname>Zhou</keyname><forenames>Luping</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Yin</keyname><forenames>Jianping</forenames></author></authors><title>Multiple Kernel Learning in the Primal for Multi-modal Alzheimer's
  Disease Classification</title><categories>cs.LG cs.CE</categories><comments>7 pages. Appearing in IEEE Journal of Biomedical and Health
  Informatics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To achieve effective and efficient detection of Alzheimer's disease (AD),
many machine learning methods have been introduced into this realm. However,
the general case of limited training samples, as well as different feature
representations typically makes this problem challenging. In this work, we
propose a novel multiple kernel learning framework to combine multi-modal
features for AD classification, which is scalable and easy to implement.
Contrary to the usual way of solving the problem in the dual space, we look at
the optimization from a new perspective. By conducting Fourier transform on the
Gaussian kernel, we explicitly compute the mapping function, which leads to a
more straightforward solution of the problem in the primal space. Furthermore,
we impose the mixed $L_{21}$ norm constraint on the kernel weights, known as
the group lasso regularization, to enforce group sparsity among different
feature modalities. This actually acts as a role of feature modality selection,
while at the same time exploiting complementary information among different
kernels. Therefore it is able to extract the most discriminative features for
classification. Experiments on the ADNI data set demonstrate the effectiveness
of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.0894</identifier>
 <datestamp>2013-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.0894</id><created>2013-10-03</created><authors><author><keyname>Chow</keyname><forenames>Richard</forenames></author><author><keyname>Jin</keyname><forenames>Hongxia</forenames></author><author><keyname>Knijnenburg</keyname><forenames>Bart</forenames></author><author><keyname>Saldamli</keyname><forenames>Gokay</forenames></author></authors><title>Differential Data Analysis for Recommender Systems</title><categories>cs.IR</categories><comments>Extended version of RecSys 2013 paper</comments><acm-class>H.3.3; K.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present techniques to characterize which data is important to a
recommender system and which is not. Important data is data that contributes
most to the accuracy of the recommendation algorithm, while less important data
contributes less to the accuracy or even decreases it. Characterizing the
importance of data has two potential direct benefits: (1) increased privacy and
(2) reduced data management costs, including storage. For privacy, we enable
increased recommendation accuracy for comparable privacy levels using existing
data obfuscation techniques. For storage, our results indicate that we can
achieve large reductions in recommendation data and yet maintain recommendation
accuracy.
  Our main technique is called differential data analysis. The name is inspired
by other sorts of differential analysis, such as differential power analysis
and differential cryptanalysis, where insight comes through analysis of
slightly differing inputs. In differential data analysis we chunk the data and
compare results in the presence or absence of each chunk. We present results
applying differential data analysis to two datasets and three different kinds
of attributes. The first attribute is called user hardship. This is a novel
attribute, particularly relevant to location datasets, that indicates how
burdensome a data point was to achieve. The second and third attributes are
more standard: timestamp and user rating. For user rating, we confirm previous
work concerning the increased importance to the recommender of data
corresponding to high and low user ratings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.0900</identifier>
 <datestamp>2013-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.0900</id><created>2013-10-03</created><authors><author><keyname>Paisitkriangkrai</keyname><forenames>Sakrapee</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author></authors><title>Efficient pedestrian detection by directly optimize the partial area
  under the ROC curve</title><categories>cs.CV cs.LG</categories><comments>10 pages. Appearing in Int. Conf. Computer Vision (ICCV) 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many typical applications of object detection operate within a prescribed
false-positive range. In this situation the performance of a detector should be
assessed on the basis of the area under the ROC curve over that range, rather
than over the full curve, as the performance outside the range is irrelevant.
  This measure is labelled as the partial area under the ROC curve (pAUC).
Effective cascade-based classification, for example, depends on training node
classifiers that achieve the maximal detection rate at a moderate false
positive rate, e.g., around 40% to 50%. We propose a novel ensemble learning
method which achieves a maximal detection rate at a user-defined range of false
positive rates by directly optimizing the partial AUC using structured
learning. By optimizing for different ranges of false positive rates, the
proposed method can be used to train either a single strong classifier or a
node classifier forming part of a cascade classifier. Experimental results on
both synthetic and real-world data sets demonstrate the effectiveness of our
approach, and we show that it is possible to train state-of-the-art pedestrian
detectors using the proposed structured ensemble learning method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.0901</identifier>
 <datestamp>2013-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.0901</id><created>2013-10-03</created><authors><author><keyname>Baumann</keyname><forenames>Thomas M.</forenames></author><author><keyname>Gracia</keyname><forenames>Jose</forenames></author></authors><title>Cudagrind: A Valgrind Extension for CUDA</title><categories>cs.SE cs.OS cs.PL</categories><comments>10 pages, 2 figures, accepted for publication in ParCo 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Valgrind, and specifically the included tool Memcheck, offers an easy and
reliable way for checking the correctness of memory operations in programs.
This works in an unintrusive way where Valgrind translates the program into
intermediate code and executes it on an emulated CPU. The heavy weight tool
Memcheck uses this to keep a full shadow copy of the memory used by a program
and tracking accesses to it. This allows the detection of memory leaks and
checking the validity of accesses.
  Though suited for a wide variety of programs, this approach still fails when
accelerator based programming models are involved. The code running on these
devices is separate from the code running on the host. Access to memory on the
device and starting of kernels is being handled by an API provided by the
driver being used. Hence Valgrind is unable to understand and instrument
operations being run on the device.
  To circumvent this limitation a new set of wrapper functions have been
introduced. These wrap a subset of the CUDA Driver API function that is
responsible for (de-)allocation memory regions on the device and the respective
memory copy operations. This allows to check whether memory is fully allocated
during a transfer and, through the functionality provided by Valgrind, whether
the memory transfered to the device from the host is defined and addressable.
Through this technique it is possible to detect a number of common programming
mistakes, which are very difficult to debug by other means. The combination of
these wrappers together with the Valgrind tool Memcheck is being called
Cudagrind.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.0919</identifier>
 <datestamp>2013-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.0919</id><created>2013-10-03</created><authors><author><keyname>Akutsu</keyname><forenames>Tatsuya</forenames></author><author><keyname>Tamura</keyname><forenames>Takeyuki</forenames></author><author><keyname>Takasu</keyname><forenames>Atsuhiro</forenames></author></authors><title>On the Parameterized Complexity of Associative and Commutative
  Unification</title><categories>cs.SC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the unification problem with associative, commutative, and
associative-commutative functions mainly from a viewpoint of the parameterized
complexity on the number of variables. It is shown that both associative and
associative-commutative unification problems are $W[1]$-hard. A fixed-parameter
algorithm and a polynomial-time algorithm are presented for special cases of
commutative unification in which one input term is variable-free and the number
of variables is bounded by a constant, respectively. Related results including
those on the string and tree edit distance problems with variables are shown
too.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.0927</identifier>
 <datestamp>2013-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.0927</id><created>2013-10-03</created><authors><author><keyname>Corander</keyname><forenames>Jukka</forenames></author><author><keyname>Janhunen</keyname><forenames>Tomi</forenames></author><author><keyname>Rintanen</keyname><forenames>Jussi</forenames></author><author><keyname>Nyman</keyname><forenames>Henrik</forenames></author><author><keyname>Pensar</keyname><forenames>Johan</forenames></author></authors><title>Learning Chordal Markov Networks by Constraint Satisfaction</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of learning the structure of a Markov network from
data. It is shown that the structure of such networks can be described in terms
of constraints which enables the use of existing solver technology with
optimization capabilities to compute optimal networks starting from initial
scores computed from the data. To achieve efficient encodings, we develop a
novel characterization of Markov network structure using a balancing condition
on the separators between cliques forming the network. The resulting
translations into propositional satisfiability and its extensions such as
maximum satisfiability, satisfiability modulo theories, and answer set
programming, enable us to prove optimal certain network structures which have
been previously found by stochastic search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.0932</identifier>
 <datestamp>2013-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.0932</id><created>2013-10-03</created><authors><author><keyname>Forni</keyname><forenames>Fulvio</forenames></author><author><keyname>Galeani</keyname><forenames>Sergio</forenames></author><author><keyname>Nesic</keyname><forenames>Dragan</forenames></author><author><keyname>Zaccarian</keyname><forenames>Luca</forenames></author></authors><title>Event-triggered transmission for linear control over communication
  channels</title><categories>cs.SY</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider an exponentially stable closed loop interconnection of a
continuous linear plant and a continuous linear controller, and we study the
problem of interconnecting the plant output to the controller input through a
digital channel. We propose a family of &quot;transmission-lazy&quot; sensors whose goal
is to transmit the measured plant output information as little as possible
while preserving closed-loop stability. In particular, we propose two
transmission policies, providing conditions on the transmission parameters.
These guarantee global asymptotic stability when the plant state is available
or when an estimate of the state is available (provided by a classical
continuous linear observer). Moreover, under a specific condition, they
guarantee global exponential stability
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.0933</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.0933</id><created>2013-10-03</created><updated>2013-10-27</updated><authors><author><keyname>Elder</keyname><forenames>Murray</forenames></author><author><keyname>Kalka</keyname><forenames>Arkadius</forenames></author></authors><title>Logspace computations for Garside groups of spindle type</title><categories>math.GR cs.CC cs.SC</categories><comments>22 pages; short version as v1. Terminolgy and title changed. In
  particular, in previous versions we called Garside groups of spindle type
  &quot;rigid Garside groups&quot;</comments><msc-class>20F65, 68Q15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  M. Picantin introduced the notion of Garside groups of spindle type,
generalizing the 3-strand braid group. We show that, for linear Garside groups
of spindle type, a normal form and a solution to the conjugacy problem are
logspace computable. For linear Garside groups of spindle type with homogenous
presentation we compute a geodesic normal form in logspace.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.0955</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.0955</id><created>2013-10-03</created><updated>2014-02-16</updated><authors><author><keyname>Lipman</keyname><forenames>Yaron</forenames></author></authors><title>Bijective Mappings Of Meshes With Boundary And The Degree In Mesh
  Processing</title><categories>cs.CG math.MG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces three sets of sufficient conditions, for generating
bijective simplicial mappings of manifold meshes. A necessary condition for a
simplicial mapping of a mesh to be injective is that it either maintains the
orientation of all elements or flips all the elements. However, these
conditions are known to be insufficient for injectivity of a simplicial map. In
this paper we provide additional simple conditions that, together with the
above mentioned necessary conditions guarantee injectivity of the simplicial
map.
  The first set of conditions generalizes classical global inversion theorems
to the mesh (piecewise-linear) case. That is, proves that in case the boundary
simplicial map is bijective and the necessary condition holds then the map is
injective and onto the target domain. The second set of conditions is concerned
with mapping of a mesh to a polytope and replaces the (often hard) requirement
of a bijective boundary map with a collection of linear constraints and
guarantees that the resulting map is injective over the interior of the mesh
and onto. These linear conditions provide a practical tool for optimizing a map
of the mesh onto a given polytope while allowing the boundary map to adjust
freely and keeping the injectivity property in the interior of the mesh. The
third set of conditions adds to the second set the requirement that the
boundary maps are orientation preserving as-well (with a proper definition of
boundary map orientation). This set of conditions guarantees that the map is
injective on the boundary of the mesh as-well as its interior. Several
experiments using the sufficient conditions are shown for mapping triangular
meshes.
  A secondary goal of this paper is to advocate and develop the tool of degree
in the context of mesh processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.0967</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.0967</id><created>2013-10-03</created><updated>2014-03-07</updated><authors><author><keyname>Bardoscia</keyname><forenames>Marco</forenames></author><author><keyname>Nagaj</keyname><forenames>Daniel</forenames></author><author><keyname>Scardicchio</keyname><forenames>Antonello</forenames></author></authors><title>The SAT-UNSAT transition in the adversarial SAT problem</title><categories>cs.CC cond-mat.dis-nn cond-mat.stat-mech cs.AI cs.LO</categories><comments>13 pages, 8 figures</comments><journal-ref>Phys. Rev. E 89, 032128 (2014)</journal-ref><doi>10.1103/PhysRevE.89.032128</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adversarial SAT (AdSAT) is a generalization of the satisfiability (SAT)
problem in which two players try to make a boolean formula true (resp. false)
by controlling their respective sets of variables. AdSAT belongs to a higher
complexity class in the polynomial hierarchy than SAT and therefore the nature
of the critical region and the transition are not easily paralleled to those of
SAT and worth of independent study. AdSAT also provides an upper bound for the
transition threshold of the quantum satisfiability problem (QSAT). We present a
complete algorithm for AdSAT, show that 2-AdSAT is in $\mathbf{P}$, and then
study two stochastic algorithms (simulated annealing and its improved variant)
and compare their performances in detail for 3-AdSAT. Varying the density of
clauses $\alpha$ we find a sharp SAT-UNSAT transition at a critical value whose
upper bound is $\alpha_c \lesssim 1.5$, thus providing a much stricter upper
bound for the QSAT transition than those previously found.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1012</identifier>
 <datestamp>2013-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1012</id><created>2013-10-03</created><authors><author><keyname>Cohen-Addad</keyname><forenames>Vincent</forenames></author><author><keyname>Habib</keyname><forenames>Michel</forenames></author><author><keyname>de Montgolfier</keyname><forenames>Fabien</forenames></author></authors><title>Algorithmic Aspects of Switch Cographs</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the notion of involution module, the first
generalization of the modular decomposition of 2-structure which has a unique
linear-sized decomposition tree. We derive an O(n^2) decomposition algorithm
and we take advantage of the involution modular decomposition tree to state
several algorithmic results. Cographs are the graphs that are totally
decomposable w.r.t modular decomposition. In a similar way, we introduce the
class of switch cographs, the class of graphs that are totally decomposable
w.r.t involution modular decomposition. This class generalizes the class of
cographs and is exactly the class of (Bull, Gem, Co-Gem, C_5)-free graphs. We
use our new decomposition tool to design three practical algorithms for the
maximum cut, vertex cover and vertex separator problems. The complexity of
these problems was still unknown for this class of graphs. This paper also
improves the complexity of the maximum clique, the maximum independant set, the
chromatic number and the maximum clique cover problems by giving efficient
algorithms, thanks to the decomposition tree. Eventually, we show that this
class of graphs has Clique-Width at most 4 and that a Clique-Width expression
can be computed in linear time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1015</identifier>
 <datestamp>2013-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1015</id><created>2013-10-03</created><authors><author><keyname>Partov</keyname><forenames>Bahar</forenames></author><author><keyname>Leith</keyname><forenames>Douglas J.</forenames></author><author><keyname>Razavi</keyname><forenames>Rouzbeh</forenames></author></authors><title>Utility Fair Optimisation of Antenna Tilt Angles in LTE Networks</title><categories>cs.NI</categories><comments>11 pages, submitted to IEEE/ACM Transactions on Networking</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We formulate adaptation of antenna tilt angle as a utility fair optimisation
task. This optimisation problem is non-convex, but in this paper we show that
under reasonable conditions it can be reformulated as a convex optimisation.
Using this insight, we develop a lightweight method for finding the optimal
antenna tilt angles, making use of measurements which are already available at
base stations, and suited to distributed implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1016</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1016</id><created>2013-10-03</created><updated>2015-09-10</updated><authors><author><keyname>Martin</keyname><forenames>Barnaby D.</forenames><affiliation>Middlesex University, UK.</affiliation></author><author><keyname>Chen</keyname><forenames>Hubie</forenames><affiliation>Universidad del Pais Vasco and IKERBASQUE, Basque Foundation of Science</affiliation></author><author><keyname>Madelaine</keyname><forenames>Florent R.</forenames><affiliation>Universit&#xe9; d'Auvergne</affiliation></author></authors><title>Quantified Constraints and Containment Problems</title><categories>cs.LO</categories><comments>This paper is a considerably expanded journal version of a LICS 2008
  paper of the same title together with the most significant parts of a CP 2012
  paper from the latter two authors</comments><proxy>LMCS</proxy><journal-ref>LMCS 11 (3:9) 2015</journal-ref><doi>10.2168/LMCS-11(3:9)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The quantified constraint satisfaction problem $\mathrm{QCSP}(\mathcal{A})$
is the problem to decide whether a positive Horn sentence, involving nothing
more than the two quantifiers and conjunction, is true on some fixed structure
$\mathcal{A}$. We study two containment problems related to the QCSP. Firstly,
we give a combinatorial condition on finite structures $\mathcal{A}$ and
$\mathcal{B}$ that is necessary and sufficient to render
$\mathrm{QCSP}(\mathcal{A}) \subseteq \mathrm{QCSP}(\mathcal{B})$. We prove
that $\mathrm{QCSP}(\mathcal{A}) \subseteq \mathrm{QCSP}(\mathcal{B})$, that is
all sentences of positive Horn logic true on $\mathcal{A}$ are true on
$\mathcal{B}$, iff there is a surjective homomorphism from
$\mathcal{A}^{|A|^{|B|}}$ to $\mathcal{B}$. This can be seen as improving an
old result of Keisler that shows the former equivalent to there being a
surjective homomorphism from $\mathcal{A}^\omega$ to $\mathcal{B}$. We note
that this condition is already necessary to guarantee containment of the
$\Pi_2$ restriction of the QCSP, that is $\Pi_2$-$\mathrm{CSP}(\mathcal{A})
\subseteq \Pi_2$-$\mathrm{CSP}(\mathcal{B})$. The exponent's bound of
${|A|^{|B|}}$ places the decision procedure for the model containment problem
in non-deterministic double-exponential time complexity. We further show the
exponent's bound $|A|^{|B|}$ to be close to tight by giving a sequence of
structures $\mathcal{A}$ together with a fixed $\mathcal{B}$, $|B|=2$, such
that there is a surjective homomorphism from $\mathcal{A}^r$ to $\mathcal{B}$
only when $r \geq |A|$. Secondly, we prove that the entailment problem for
positive Horn fragment of first-order logic is decidable. That is, given two
sentences $\varphi$ and $\psi$ of positive Horn, we give an algorithm that
determines whether $\varphi \rightarrow \psi$ is true in all structures
(models). Our result is in some sense tight, since we show that the entailment
problem for positive first-order logic (i.e. positive Horn plus disjunction) is
undecidable. In the final part of the paper we ponder a notion of Q-core that
is some canonical representative among the class of templates that engender the
same QCSP. Although the Q-core is not as well-behaved as its better known
cousin the core, we demonstrate that it is still a useful notion in the realm
of QCSP complexity classifications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1025</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1025</id><created>2013-10-03</created><updated>2013-12-29</updated><authors><author><keyname>Madjidian</keyname><forenames>Daria</forenames></author><author><keyname>Mirkin</keyname><forenames>Leonid</forenames></author></authors><title>Distributed Control with Low-Rank Coordination</title><categories>cs.SY math.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A common approach to distributed control design is to impose sparsity
constraints on the controller structure. Such constraints, however, may greatly
complicate the control design procedure. This paper puts forward an alternative
structure, which is not sparse yet might nevertheless be well suited for
distributed control purposes. The structure appears as the optimal solution to
a class of coordination problems arising in multi-agent applications. The
controller comprises a diagonal (decentralized) part, complemented by a
rank-one coordination term. Although this term relies on information about all
subsystems, its implementation only requires a simple averaging operation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1032</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1032</id><created>2013-10-03</created><authors><author><keyname>Janus Collaboration</keyname></author><author><keyname>Baity-Jesi</keyname><forenames>M.</forenames></author><author><keyname>Ba&#xf1;os</keyname><forenames>R. A.</forenames></author><author><keyname>Cruz</keyname><forenames>A.</forenames></author><author><keyname>Fernandez</keyname><forenames>L. A.</forenames></author><author><keyname>Gil-Narvion</keyname><forenames>J. M.</forenames></author><author><keyname>Gordillo-Guerrero</keyname><forenames>A.</forenames></author><author><keyname>I&#xf1;iguez</keyname><forenames>D.</forenames></author><author><keyname>Maiorano</keyname><forenames>A.</forenames></author><author><keyname>Mantovani</keyname><forenames>F.</forenames></author><author><keyname>Marinari</keyname><forenames>E.</forenames></author><author><keyname>Martin-Mayor</keyname><forenames>V.</forenames></author><author><keyname>Monforte-Garcia</keyname><forenames>J.</forenames></author><author><keyname>Sudupe</keyname><forenames>A. Mu&#xf1;oz</forenames></author><author><keyname>Navarro</keyname><forenames>D.</forenames></author><author><keyname>Parisi</keyname><forenames>G.</forenames></author><author><keyname>Perez-Gaviro</keyname><forenames>S.</forenames></author><author><keyname>Pivanti</keyname><forenames>M.</forenames></author><author><keyname>Ricci-Tersenghi</keyname><forenames>F.</forenames></author><author><keyname>Ruiz-Lorenzo</keyname><forenames>J. J.</forenames></author><author><keyname>Schifano</keyname><forenames>S. F.</forenames></author><author><keyname>Seoane</keyname><forenames>B.</forenames></author><author><keyname>Tarancon</keyname><forenames>A.</forenames></author><author><keyname>Tripiccione</keyname><forenames>R.</forenames></author><author><keyname>Yllanes</keyname><forenames>D.</forenames></author></authors><title>Janus II: a new generation application-driven computer for spin-system
  simulations</title><categories>cs.AR cond-mat.dis-nn cond-mat.stat-mech physics.comp-ph</categories><comments>28 pages, 6 figures</comments><journal-ref>Computer Physics Communications 185 (2014) 550-559</journal-ref><doi>10.1016/j.cpc.2013.10.019</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the architecture, the development and the implementation
of Janus II, a new generation application-driven number cruncher optimized for
Monte Carlo simulations of spin systems (mainly spin glasses). This domain of
computational physics is a recognized grand challenge of high-performance
computing: the resources necessary to study in detail theoretical models that
can make contact with experimental data are by far beyond those available using
commodity computer systems. On the other hand, several specific features of the
associated algorithms suggest that unconventional computer architectures, which
can be implemented with available electronics technologies, may lead to order
of magnitude increases in performance, reducing to acceptable values on human
scales the time needed to carry out simulation campaigns that would take
centuries on commercially available machines. Janus II is one such machine,
recently developed and commissioned, that builds upon and improves on the
successful JANUS machine, which has been used for physics since 2008 and is
still in operation today. This paper describes in detail the motivations behind
the project, the computational requirements, the architecture and the
implementation of this new machine and compares its expected performances with
those of currently available commercial systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1042</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1042</id><created>2013-10-03</created><updated>2013-11-09</updated><authors><author><keyname>M&#xe1;&#x10d;ajov&#xe1;</keyname><forenames>Edita</forenames></author><author><keyname>Maz&#xe1;k</keyname><forenames>J&#xe1;n</forenames></author></authors><title>Cubic graphs with large circumference deficit</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The circumference $c(G)$ of a graph $G$ is the length of a longest cycle. By
exploiting our recent results on resistance of snarks, we construct infinite
classes of cyclically $4$-, $5$- and $6$-edge-connected cubic graphs with
circumference ratio $c(G)/|V(G)|$ bounded from above by $0.876$, $0.960$ and
$0.990$, respectively. In contrast, the dominating cycle conjecture implies
that the circumference ratio of a cyclically $4$-edge-connected cubic graph is
at least $0.75$.
  In addition, we construct snarks with large girth and large circumference
deficit, solving Problem 1 proposed in [J. H\&quot;agglund and K. Markstr\&quot;om, On
stable cycles and cycle double covers of graphs with large circumference, Disc.
Math. 312 (2012), 2540--2544].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1048</identifier>
 <datestamp>2013-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1048</id><created>2013-10-02</created><authors><author><keyname>Bose</keyname><forenames>Prosenjit</forenames></author><author><keyname>De Carufel</keyname><forenames>Jean-Lou</forenames></author><author><keyname>Durocher</keyname><forenames>Stephane</forenames></author></authors><title>Revisiting the Problem of Searching on a Line</title><categories>cs.DS cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the problem of searching for a target at an unknown location on a
line when given upper and lower bounds on the distance D that separates the
initial position of the searcher from the target. Prior to this work, only
asymptotic bounds were known for the optimal competitive ratio achievable by
any search strategy in the worst case. We present the first tight bounds on the
exact optimal competitive ratio achievable, parameterized in terms of the given
bounds on D, along with an optimal search strategy that achieves this
competitive ratio. We prove that this optimal strategy is unique. We
characterize the conditions under which an optimal strategy can be computed
exactly and, when it cannot, we explain how numerical methods can be used
efficiently. In addition, we answer several related open questions, including
the maximal reach problem, and we discuss how to generalize these results to m
rays, for any m &gt;= 2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1050</identifier>
 <datestamp>2013-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1050</id><created>2013-09-27</created><authors><author><keyname>Kasthurirathna</keyname><forenames>Dharshana</forenames></author><author><keyname>Dong</keyname><forenames>Andy</forenames></author><author><keyname>Piraveenan</keyname><forenames>Mahendrarajah</forenames></author><author><keyname>Tumer</keyname><forenames>Irem Y.</forenames></author></authors><title>The failure tolerance of mechatronic software systems to random and
  targeted attacks</title><categories>cs.DC cs.SE cs.SY</categories><comments>Proceedings of the 2013 ASME International Design Engineering
  Technical Conferences &amp; Computers and Information in Engineering Conference
  IDETC/CIE 2013 August 4-7, 2013, Portland, Oregon, USA (In Print)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a complex networks approach to study the failure
tolerance of mechatronic software systems under various types of hardware
and/or software failures. We produce synthetic system architectures based on
evidence of modular and hierarchical modular product architectures and known
motifs for the interconnection of physical components to software. The system
architectures are then subject to various forms of attack. The attacks simulate
failure of critical hardware or software. Four types of attack are
investigated: degree centrality, betweenness centrality, closeness centrality
and random attack. Failure tolerance of the system is measured by a 'robustness
coefficient', a topological 'size' metric of the connectedness of the attacked
network. We find that the betweenness centrality attack results in the most
significant reduction in the robustness coefficient, confirming betweenness
centrality, rather than the number of connections (i.e. degree), as the most
conservative metric of component importance. A counter-intuitive finding is
that &quot;designed&quot; system architectures, including a bus, ring, and star
architecture, are not significantly more failure-tolerant than interconnections
with no prescribed architecture, that is, a random architecture. Our research
provides a data-driven approach to engineer the architecture of mechatronic
software systems for failure tolerance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1076</identifier>
 <datestamp>2013-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1076</id><created>2013-10-03</created><authors><author><keyname>Li</keyname><forenames>Ping</forenames></author><author><keyname>Zhang</keyname><forenames>Cun-Hui</forenames></author><author><keyname>Zhang</keyname><forenames>Tong</forenames></author></authors><title>Compressed Counting Meets Compressed Sensing</title><categories>stat.ME cs.DS cs.IT cs.LG math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed sensing (sparse signal recovery) has been a popular and important
research topic in recent years. By observing that natural signals are often
nonnegative, we propose a new framework for nonnegative signal recovery using
Compressed Counting (CC). CC is a technique built on maximally-skewed p-stable
random projections originally developed for data stream computations. Our
recovery procedure is computationally very efficient in that it requires only
one linear scan of the coordinates. Our analysis demonstrates that, when
0&lt;p&lt;=0.5, it suffices to use M= O(C/eps^p log N) measurements so that all
coordinates will be recovered within eps additive precision, in one scan of the
coordinates. The constant C=1 when p-&gt;0 and C=pi/2 when p=0.5. In particular,
when p-&gt;0 the required number of measurements is essentially M=K\log N, where K
is the number of nonzero coordinates of the signal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1105</identifier>
 <datestamp>2013-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1105</id><created>2013-10-03</created><authors><author><keyname>Zeng</keyname><forenames>Ruochen</forenames></author><author><keyname>Tepedelenlioglu</keyname><forenames>Cihan</forenames></author></authors><title>Cognitive Radio with Random Number of Secondary Number of Users</title><categories>cs.IT math.IT</categories><comments>28 pages, 3 figures, submitted to IEEE Transactions on Wireless
  Communication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A single primary user cognitive radio system with multi-user diversity at the
secondary users is considered where there is an interference constraint between
secondary and primary users. The secondary user with the highest instantaneous
SNR is selected for communication from a set of active users which also
satisfies the interference constraint. The active number of secondary users is
shown to be binomial, negative binomial, or Poisson-binomial distributed
depending on various modes of operation. Outage probability in the slow fading
scenario is also studied. This is then followed by a derivation of the scaling
law of the ergodic capacity and BER averaged across the fading, and user
distribution for a large mean number of users. The ergodic capacity and average
BER under the binomial user distribution is shown to outperform the negative
binomial case with the same mean number of users. Moreover, the Poisson
distribution is used to approximate the user distribution under the non-i.i.d
interference scenario, and compared with binomial and negative binomial
distributions in a stochastic ordering sense. Monte-Carlo simulations are used
to supplement our analytical results and compare the performances under
different user distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1118</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1118</id><created>2013-10-03</created><updated>2013-10-08</updated><authors><author><keyname>Krishnamoorthy</keyname><forenames>Mukkai</forenames></author><author><keyname>Miller</keyname><forenames>Wesley</forenames></author><author><keyname>Krishnamoorthy</keyname><forenames>Raju</forenames></author></authors><title>Evolution of choices over time: The U.S. Presidential election 2012 and
  the NY City Mayoral Election, 2013</title><categories>cs.OH</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We conducted surveys before and after the 2012 U.S. Presidential election and
prior to the NY City Mayoral election in 2013. The surveys were done using
Amazon Turk. This poster describes the results of our analysis of the surveys
and predicts the winner of the NY City Mayoral Election.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1129</identifier>
 <datestamp>2013-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1129</id><created>2013-10-03</created><authors><author><keyname>Razaque</keyname><forenames>Abdul</forenames></author><author><keyname>Elleithy</keyname><forenames>Khaled. M.</forenames></author></authors><title>Efficient Search (RES) for One-Hop Destination over Wireless Sensor
  Network</title><categories>cs.NI</categories><journal-ref>CATA-2013, March 4-6, 2013, Honolulu, Hawaii, USA</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The revolution of wireless sensors networks (WSNs) has highly augmented the
expectations of people to get the work done efficiently, but there is little
bit impediment to deal with deployed nodes in WSNs. The nature of used routing
and medium access control (MAC) protocols in WSNs is completely different from
wireless adhoc network protocols. Sensor nodes do not have enough capability to
synchronize with robust way, in resulting causes of longer delay and waste of
energy. In this paper, we deploy efficientenergy consuming sensors and to find
one hop robust and efficient destination search in WSNs. We firstly deploy BT
(Bluetooth enabled) sensors, which offer passive and active sensing capability
to save energy. This work is a continuation of previous published work in [2].
The BT node is supported with efficient searchmethodss. The main objective of
this contribution is to control different types of objects from remote places
using cellular phone. To validate our proposed methodology,simulation is done
with network simulator (ns2) to examine the behavior of WSNs. Based on
simulation results, we claim that our approach saves 62% energy spent for
finding best one- hop destination as compared with existing techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1137</identifier>
 <datestamp>2013-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1137</id><created>2013-10-03</created><authors><author><keyname>Blocki</keyname><forenames>Jeremiah</forenames></author><author><keyname>Blum</keyname><forenames>Manuel</forenames></author><author><keyname>Datta</keyname><forenames>Anupam</forenames></author></authors><title>GOTCHA Password Hackers!</title><categories>cs.CR cs.AI</categories><comments>2013 ACM Workshop on Artificial Intelligence and Security (AISec)</comments><doi>10.1145/2517312.2517319</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce GOTCHAs (Generating panOptic Turing Tests to Tell Computers and
Humans Apart) as a way of preventing automated offline dictionary attacks
against user selected passwords. A GOTCHA is a randomized puzzle generation
protocol, which involves interaction between a computer and a human.
Informally, a GOTCHA should satisfy two key properties: (1) The puzzles are
easy for the human to solve. (2) The puzzles are hard for a computer to solve
even if it has the random bits used by the computer to generate the final
puzzle --- unlike a CAPTCHA. Our main theorem demonstrates that GOTCHAs can be
used to mitigate the threat of offline dictionary attacks against passwords by
ensuring that a password cracker must receive constant feedback from a human
being while mounting an attack. Finally, we provide a candidate construction of
GOTCHAs based on Inkblot images. Our construction relies on the usability
assumption that users can recognize the phrases that they originally used to
describe each Inkblot image --- a much weaker usability assumption than
previous password systems based on Inkblots which required users to recall
their phrase exactly. We conduct a user study to evaluate the usability of our
GOTCHA construction. We also generate a GOTCHA challenge where we encourage
artificial intelligence and security researchers to try to crack several
passwords protected with our scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1141</identifier>
 <datestamp>2013-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1141</id><created>2013-10-03</created><authors><author><keyname>Adcock</keyname><forenames>Ben</forenames></author><author><keyname>Hansen</keyname><forenames>Anders</forenames></author><author><keyname>Roman</keyname><forenames>Bogdan</forenames></author><author><keyname>Teschke</keyname><forenames>Gerd</forenames></author></authors><title>Generalized sampling: stable reconstructions, inverse problems and
  compressed sensing over the continuum</title><categories>math.NA cs.IT math.IT</categories><comments>59 pages, 25 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this paper is to report on recent approaches to reconstruction
problems based on analog, or in other words, infinite-dimensional, image and
signal models. We describe three main contributions to this problem. First,
linear reconstructions from sampled measurements via so-called generalized
sampling (GS). Second, the extension of generalized sampling to inverse and
ill-posed problems. And third, the combination of generalized sampling with
sparse recovery techniques. This final contribution leads to a theory and set
of methods for infinite-dimensional compressed sensing, or as we shall also
refer to it, compressed sensing over the continuum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1153</identifier>
 <datestamp>2013-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1153</id><created>2013-10-03</created><authors><author><keyname>V</keyname><forenames>Prathyusha</forenames></author><author><keyname>Bhashyam</keyname><forenames>Srikrishna</forenames></author><author><keyname>Thangaraj</keyname><forenames>Andrew</forenames></author></authors><title>The Gaussian Two-way Diamond Channel</title><categories>cs.IT math.IT</categories><comments>8 pages, 7 figures Proceedings of 51st Annual Allerton Conference on
  Communication, Control, and Computing, Monticello, IL, USA, Oct 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider two-way relaying in a Gaussian diamond channel, where two
terminal nodes wish to exchange information using two relays. A simple baseline
protocol is obtained by time-sharing between two one-way protocols. To improve
upon the baseline performance, we propose two compute-and-forward (CF)
protocols: Compute-and-forward Compound multiple access channel (CF-CMAC) and
Compute-and-forward-Broadcast (CF-BC). These protocols mix the two flows
through the two relays and achieve rates better than the simple time-sharing
protocol. We derive an outer bound to the capacity region that is satisfied by
any relaying protocol, and observe that the proposed protocols provide rates
close to the outer bound in certain channel conditions. Both the CF-CMAC and
CF-BC protocols use nested lattice codes in the compute phases. In the CF-CMAC
protocol, both relays simultaneously forward to the destinations over a
Compound Multiple Access Channel (CMAC). In the simpler CF-BC protocol's
forward phase, one relay is selected at a time for Broadcast Channel (BC)
transmission depending on the rate-pair to be achieved. We also consider the
diamond channel with direct source-destination link and the diamond channel
with interfering relays. Outer bounds and achievable rate regions are compared
for these two channels as well. Mixing of flows using the CF-CMAC protocol is
shown to be good for symmetric two-way rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1161</identifier>
 <datestamp>2013-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1161</id><created>2013-10-03</created><authors><author><keyname>Lahiri</keyname><forenames>Bibudh</forenames></author><author><keyname>Mukherjee</keyname><forenames>Arko Provo</forenames></author><author><keyname>Tirthapura</keyname><forenames>Srikanta</forenames></author></authors><title>Identifying Correlated Heavy-Hitters in a Two-Dimensional Data Stream</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider online mining of correlated heavy-hitters from a data stream.
Given a stream of two-dimensional data, a correlated aggregate query first
extracts a substream by applying a predicate along a primary dimension, and
then computes an aggregate along a secondary dimension. Prior work on
identifying heavy-hitters in streams has almost exclusively focused on
identifying heavy-hitters on a single dimensional stream, and these yield
little insight into the properties of heavy-hitters along other dimensions. In
typical applications however, an analyst is interested not only in identifying
heavy-hitters, but also in understanding further properties such as: what other
items appear frequently along with a heavy-hitter, or what is the frequency
distribution of items that appear along with the heavy-hitters. We consider
queries of the following form: In a stream S of (x, y) tuples, on the substream
H of all x values that are heavy-hitters, maintain those y values that occur
frequently with the x values in H. We call this problem as Correlated
Heavy-Hitters (CHH). We formulate an approximate formulation of CHH
identification, and present an algorithm for tracking CHHs on a data stream.
The algorithm is easy to implement and uses workspace which is orders of
magnitude smaller than the stream itself. We present provable guarantees on the
maximum error, as well as detailed experimental results that demonstrate the
space-accuracy trade-off.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1162</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1162</id><created>2013-10-04</created><updated>2014-01-22</updated><authors><author><keyname>Roy</keyname><forenames>Dhrubojyoti</forenames></author><author><keyname>Sridharan</keyname><forenames>Mukundan</forenames></author><author><keyname>Deshpande</keyname><forenames>Satyajeet</forenames></author><author><keyname>Arora</keyname><forenames>Anish</forenames></author></authors><title>A Little Prediction Goes a Long Way: Routing in Semi-Deterministic Delay
  Tolerant Networks</title><categories>cs.NI</categories><comments>This paper has been withdrawn by the authors. Withdrawn since
  document intended to be anonymous</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Realizing delay-capacity in intermittently connected mobile networks remains
a largely open question, with state-of-the-art routing schemes typically
focusing either on delay or on capacity. We show the feasibility of routing
with both high goodput and desired delay constraints, with REAPER (for
Reliable, Efficient, and Predictive Routing), a fully distributed convergecast
routing framework that jointly optimizes both path length and path delay. A key
idea for efficient instantiation of REAPER is to exploit predictability of
mobility patterns, in terms of a semi-deterministic model which appropriately
captures several vehicular and human inter-contact patterns. Packets are thus
routed using paths that are jointly optimal at their time of arrival, in
contrast to extant DTN protocols which use time-average metrics for routing.
REAPER is also self-stabilizing to changes in the mobility pattern. A
simulation-based evaluation confirms that, across the spectrum of ultra-light
to heavy traffics, REAPER achieves up to 135% and 200% higher throughput and up
to 250% and 1666% higher energy efficiency than state-of-the-art single-copy
protocols MEED-DVR and PROPHET, which optimize a single metric only,
specifically, expected delay and path probability respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1166</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1166</id><created>2013-10-04</created><updated>2016-03-04</updated><authors><author><keyname>Bose</keyname><forenames>Prosenjit</forenames></author><author><keyname>Lubiw</keyname><forenames>Anna</forenames></author><author><keyname>Pathak</keyname><forenames>Vinayak</forenames></author><author><keyname>Verdonschot</keyname><forenames>Sander</forenames></author></authors><title>Flipping Edge-Labelled Triangulations</title><categories>cs.CG</categories><comments>Significantly expanded journal version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Flips in triangulations have received a lot of attention over the past
decades. However, the problem of tracking where particular edges go during the
flipping process has not been addressed. We examine this question by attaching
unique labels to the triangulation edges. We introduce the concept of the orbit
of an edge $e$, which is the set of all edges reachable from $e$ via flips.
  We establish the first upper and lower bounds on the diameter of the flip
graph in this setting. Specifically, we prove tight $\Theta(n \log n)$ bounds
for edge-labelled triangulations of $n$-vertex convex polygons and
combinatorial triangulations, contrasting with the $\Theta(n)$ bounds in their
respective unlabelled settings. The $\Omega(n \log n)$ lower bound for the
convex polygon setting might be of independent interest, as it generalizes
lower bounds on certain sorting models. When simultaneous flips are allowed,
the upper bound for convex polygons decreases to $O(\log^2 n)$, although we no
longer have a matching lower bound.
  Moving beyond convex polygons, we show that edge-labelled triangulated
polygons with a single reflex vertex can have a disconnected flip graph. This
is in sharp contrast with the unlabelled case, where the flip graph is
connected for any triangulated polygon. For spiral polygons, we provide a
complete characterization of the orbits. This allows us to decide connectivity
of the flip graph of a spiral polygon in linear time. We also prove an upper
bound of $O(n^2)$ on the diameter of each connected component, which is optimal
in the worst case. We conclude with an example of a non-spiral polygon whose
flip graph has diameter $\Omega(n^3)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1167</identifier>
 <datestamp>2013-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1167</id><created>2013-10-04</created><authors><author><keyname>Bhawkar</keyname><forenames>Aniket</forenames></author><author><keyname>Belsare</keyname><forenames>Rohit</forenames></author><author><keyname>Gandhi</keyname><forenames>Fenil</forenames></author><author><keyname>Somani</keyname><forenames>Pratiksha</forenames></author></authors><title>Analysis of Errors: A Support System for Teachers to Analyse the Error
  Occurring to a Novice Programmer</title><categories>cs.CY cs.SE</categories><comments>4 pages, 1 figure</comments><journal-ref>Analysis of Errors - A Support System for Teachers to Analyze
  IJCSN-2013-2-5-44</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a novice programmer, coding is equivalent to a nightmare. A novice
programmer tries to replicate steps provided by the faculty and on compilation
gets a number of errors which the novice programmer is not able to resolve.
This system provides support to the faculty about the coding ability of the
students and their ability to solve those errors. Also, the faculty can provide
a solution to the errors which are occurring to the students and the solution
is displayed accordingly. The emphasis of this paper is on developing this
system within JAVA and making use of Online Compilers. Moreover, we focus on a
new system which is able to provide online code management and these codes get
compiled using an online compiler and these programs can be viewed by the
respective faculty for cross verification. This paper takes into account the
syntactic errors, runtime and semantic errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1174</identifier>
 <datestamp>2013-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1174</id><created>2013-10-04</created><authors><author><keyname>Romanov</keyname><forenames>Alexander M.</forenames></author></authors><title>Full-Rank Perfect Codes over Finite Fields</title><categories>cs.IT math.IT</categories><comments>8 pages; submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a construction of full-rank q-ary 1-perfect codes
over finite fields. This construction is a generalization of the Etzion and
Vardy construction of full-rank binary 1-perfect codes (1994). Properties of
i-components of q-ary Hamming codes are investigated and the construction of
full-rank q-ary 1-perfect codes is based on these properties. The switching
construction of 1-perfect codes are generalized for the q-ary case. We give a
generalization of the concept of i-component of 1-perfect codes and introduce
the concept of (i,{\sigma})-components of q-ary 1-perfect codes. We also
present a generalization of the Lindstr\&quot;om and Sch\&quot;onheim construction of
q-ary 1-perfect codes and provide a lower bound on the number of pairwise
distinct q-ary 1-perfect codes of length n.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1177</identifier>
 <datestamp>2013-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1177</id><created>2013-10-04</created><authors><author><keyname>Shao</keyname><forenames>Weixiang</forenames><affiliation>University of Illinois at Chicago</affiliation></author><author><keyname>Shi</keyname><forenames>Xiaoxiao</forenames><affiliation>University of Illinois at Chicago</affiliation></author><author><keyname>Yu</keyname><forenames>Philip S.</forenames><affiliation>University of Illinois at Chicago</affiliation></author></authors><title>Clustering on Multiple Incomplete Datasets via Collective Kernel
  Learning</title><categories>cs.LG</categories><acm-class>H.2.8; I.5.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple datasets containing different types of features may be available for
a given task. For instance, users' profiles can be used to group users for
recommendation systems. In addition, a model can also use users' historical
behaviors and credit history to group users. Each dataset contains different
information and suffices for learning. A number of clustering algorithms on
multiple datasets were proposed during the past few years. These algorithms
assume that at least one dataset is complete. So far as we know, all the
previous methods will not be applicable if there is no complete dataset
available. However, in reality, there are many situations where no dataset is
complete. As in building a recommendation system, some new users may not have a
profile or historical behaviors, while some may not have a credit history.
Hence, no available dataset is complete. In order to solve this problem, we
propose an approach called Collective Kernel Learning to infer hidden sample
similarity from multiple incomplete datasets. The idea is to collectively
completes the kernel matrices of incomplete datasets by optimizing the
alignment of the shared instances of the datasets. Furthermore, a clustering
algorithm is proposed based on the kernel matrix. The experiments on both
synthetic and real datasets demonstrate the effectiveness of the proposed
approach. The proposed clustering algorithm outperforms the comparison
algorithms by as much as two times in normalized mutual information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1184</identifier>
 <datestamp>2013-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1184</id><created>2013-10-04</created><updated>2013-11-11</updated><authors><author><keyname>Saini</keyname><forenames>Gurpreet Singh</forenames></author><author><keyname>Kumar</keyname><forenames>Manoj</forenames></author></authors><title>Multi Constraint Satisfying AODV routing using Fuzzy Logic</title><categories>cs.NI</categories><comments>The paper needs to be withdrawn on basis of no publication from the
  journal it is linked with till now, except for title indexing</comments><journal-ref>European Journal Of Scientific Research volume 104 Issue 2 June
  2013 pp 190-198</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Mobile Networks are in need of what we call as an e-age. There have been
numerous advancements in this field and soft computing has gained roots in this
Mobile Ad-hoc Network domain. This documents studies the use of fuzzy logic
over Ad Hoc On-Demand Distance vector Routing. The evaluation of the network
using fuzzy logic over AODV has been conducted using selective criterion. Both
the terms come from two different computing environments, but when combined
together can achieve better results; when it comes to QoS based upon selective
parameters. The crisp the rules formed in fuzzy logic, better the performance
or quality will be achieved in AODV routing mechanism of MANET.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1187</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1187</id><created>2013-10-04</created><authors><author><keyname>Pensar</keyname><forenames>Johan</forenames></author><author><keyname>Nyman</keyname><forenames>Henrik</forenames></author><author><keyname>Koski</keyname><forenames>Timo</forenames></author><author><keyname>Corander</keyname><forenames>Jukka</forenames></author></authors><title>Labeled Directed Acyclic Graphs: a generalization of context-specific
  independence in directed graphical models</title><categories>stat.ML cs.AI cs.LG</categories><comments>26 pages, 17 figures</comments><doi>10.1007/s10618-014-0355-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a novel class of labeled directed acyclic graph (LDAG) models
for finite sets of discrete variables. LDAGs generalize earlier proposals for
allowing local structures in the conditional probability distribution of a
node, such that unrestricted label sets determine which edges can be deleted
from the underlying directed acyclic graph (DAG) for a given context. Several
properties of these models are derived, including a generalization of the
concept of Markov equivalence classes. Efficient Bayesian learning of LDAGs is
enabled by introducing an LDAG-based factorization of the Dirichlet prior for
the model parameters, such that the marginal likelihood can be calculated
analytically. In addition, we develop a novel prior distribution for the model
structures that can appropriately penalize a model for its labeling complexity.
A non-reversible Markov chain Monte Carlo algorithm combined with a greedy hill
climbing approach is used for illustrating the useful properties of LDAG models
for both real and synthetic data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1190</identifier>
 <datestamp>2013-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1190</id><created>2013-10-04</created><authors><author><keyname>Dash</keyname><forenames>Priyanka</forenames></author><author><keyname>Rout</keyname><forenames>Ranjita</forenames></author><author><keyname>Pratihari</keyname><forenames>Satya Bhusan</forenames></author><author><keyname>Padhi</keyname><forenames>Sanjay Kumar</forenames></author></authors><title>Review on Fragment Allocation by using Clustering Technique in
  Distributed Database System</title><categories>cs.DB</categories><comments>9 pages,3 figures</comments><journal-ref>IJCSN,October,2013,Volume-2 Issue-5</journal-ref><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Considerable Progress has been made in the last few years in improving the
performance of the distributed database systems. The development of Fragment
allocation models in Distributed database is becoming difficult due to the
complexity of huge number of sites and their communication considerations.
Under such conditions, simulation of clustering and data allocation is adequate
tools for understanding and evaluating the performance of data allocation in
Distributed databases. Clustering sites and fragment allocation are key
challenges in Distributed database performance, and are considered to be
efficient methods that have a major role in reducing transferred and accessed
data during the execution of applications. In this paper a review on Fragment
allocation by using Clustering technique is given in Distributed Database
System.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1191</identifier>
 <datestamp>2014-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1191</id><created>2013-10-04</created><authors><author><keyname>Bana&#x15b;</keyname><forenames>Krzysztof</forenames></author><author><keyname>P&#x142;aszewski</keyname><forenames>Przemys&#x142;aw</forenames></author><author><keyname>Macio&#x142;</keyname><forenames>Pawe&#x142;</forenames></author></authors><title>Numerical integration on GPUs for higher order finite elements</title><categories>cs.MS</categories><journal-ref>Computers and Mathematics with Applications, Volume 67, Issue 6,
  April 2014, Pages 1319-1344</journal-ref><doi>10.1016/j.camwa.2014.01.021</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper considers the problem of implementation on graphics processors of
numerical integration routines for higher order finite element approximations.
The design of suitable GPU kernels is investigated in the context of general
purpose integration procedures, as well as particular example applications. The
most important characteristic of the problem investigated is the large
variation of required processor and memory resources associated with different
degrees of approximating polynomials. The questions that we try to answer are
whether it is possible to design a single integration kernel for different GPUs
and different orders of approximation and what performance can be expected in
such a case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1194</identifier>
 <datestamp>2014-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1194</id><created>2013-10-04</created><authors><author><keyname>Kru&#x17c;el</keyname><forenames>Filip</forenames></author><author><keyname>Bana&#x15b;</keyname><forenames>Krzysztof</forenames></author></authors><title>Vectorized OpenCL implementation of numerical integration for higher
  order finite elements</title><categories>cs.MS</categories><comments>published online in Computers and Mathematics with Applications:
  http://www.sciencedirect.com/science/article/pii/S089812211300521X</comments><journal-ref>Computers &amp; Mathematics with Applications, Volume 66, Issue 10,
  December 2013, Pages 2030-2044</journal-ref><doi>10.1016/j.camwa.2013.08.026</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In our work we analyze computational aspects of the problem of numerical
integration in finite element calculations and consider an OpenCL
implementation of related algorithms for processors with wide vector registers.
  As a platform for testing the implementation we choose the PowerXCell
processor, being an example of the Cell Broadband Engine (CellBE) architecture.
Although the processor is considered old for today's standards (its design
dates back to year 2001), we investigate its performance due to two features
that it shares with recent Xeon Phi family of coprocessors: wide vector units
and relatively slow connection of computing cores with main global memory. The
performed analysis of parallelization options can also be used for designing
numerical integration algorithms for other processors with vector registers,
such as contemporary x86 microprocessors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1197</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1197</id><created>2013-10-04</created><updated>2015-10-06</updated><authors><author><keyname>Scarlett</keyname><forenames>Jonathan</forenames></author><author><keyname>Tan</keyname><forenames>Vincent Y. F.</forenames></author></authors><title>Second-Order Asymptotics for the Gaussian MAC with Degraded Message Sets</title><categories>cs.IT math.IT</categories><comments>27 pages, 5 figures, IEEE Transactions on Information Theory, 2015</comments><doi>10.1109/TIT.2015.2487340</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the second-order asymptotics of the Gaussian
multiple-access channel with degraded message sets. For a fixed average error
probability $\varepsilon \in (0,1)$ and an arbitrary point on the boundary of
the capacity region, we characterize the speed of convergence of rate pairs
that converge to that boundary point for codes that have asymptotic error
probability no larger than $\varepsilon$. As a stepping stone to this local
notion of second-order asymptotics, we study a global notion, and establish
relationships between the two. We provide a numerical example to illustrate how
the angle of approach to a boundary point affects the second-order coding rate.
This is the first conclusive characterization of the second-order asymptotics
of a network information theory problem in which the capacity region is not a
polygon.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1217</identifier>
 <datestamp>2014-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1217</id><created>2013-10-04</created><authors><author><keyname>Valsesia</keyname><forenames>Diego</forenames></author><author><keyname>Coluccia</keyname><forenames>Giulio</forenames></author><author><keyname>Magli</keyname><forenames>Enrico</forenames></author></authors><title>Graded Quantization: Democracy for Multiple Descriptions in Compressed
  Sensing</title><categories>cs.IT math.IT</categories><journal-ref>Proceedings of the 38th International Conference on Acoustics,
  Speech, and Signal Processing (ICASSP), Vancouver, Canada, May 26 - 31, 2013,
  pp. 5825-5829</journal-ref><doi>10.1109/ICASSP.2013.6638781</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The compressed sensing paradigm allows to efficiently represent sparse
signals by means of their linear measurements. However, the problem of
transmitting these measurements to a receiver over a channel potentially prone
to packet losses has received little attention so far. In this paper, we
propose novel methods to generate multiple descriptions from compressed sensing
measurements to increase the robustness over unreliable channels. In
particular, we exploit the democracy property of compressive measurements to
generate descriptions in a simple manner by partitioning the measurement vector
and properly allocating bit-rate, outperforming classical methods like the
multiple description scalar quantizer. In addition, we propose a modified
version of the Basis Pursuit Denoising recovery procedure that is specifically
tailored to the proposed methods. Experimental results show significant
performance gains with respect to existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1221</identifier>
 <datestamp>2013-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1221</id><created>2013-10-04</created><authors><author><keyname>Valsesia</keyname><forenames>Diego</forenames></author><author><keyname>Magli</keyname><forenames>Enrico</forenames></author></authors><title>Spatially Scalable Compressed Image Sensing with Hybrid Transform and
  Inter-layer Prediction Model</title><categories>cs.IT cs.CV cs.MM math.IT</categories><comments>Proceedings of the 15th International Workshop on Multimedia Signal
  Processing, September 30-October 3, 2013, Pula, Italy</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressive imaging is an emerging application of compressed sensing, devoted
to acquisition, encoding and reconstruction of images using random projections
as measurements. In this paper we propose a novel method to provide a scalable
encoding of an image acquired by means of compressed sensing techniques. Two
bit-streams are generated to provide two distinct quality levels: a
low-resolution base layer and full-resolution enhancement layer. In the
proposed method we exploit a fast preview of the image at the encoder in order
to perform inter-layer prediction and encode the prediction residuals only. The
proposed method successfully provides resolution and quality scalability with
modest complexity and it provides gains in the quality of the reconstructed
images with respect to separate encoding of the quality layers. Remarkably, we
also show that the scheme can also provide significant gains with respect to a
direct, non-scalable system, thus accomplishing two features at once:
scalability and improved reconstruction performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1227</identifier>
 <datestamp>2013-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1227</id><created>2013-10-04</created><authors><author><keyname>Khedkar</keyname><forenames>Anagha P.</forenames></author><author><keyname>Subbaraman</keyname><forenames>Shaila</forenames></author></authors><title>The Novel Approach of Adaptive Twin Probability for Genetic Algorithm</title><categories>cs.NE</categories><comments>7 pages, International Journal of Advanced Studies in Computer
  Science and Engineering (IJASCSE), Volume 2, Special Issue 2, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance of GA is measured and analyzed in terms of its performance
parameters against variations in its genetic operators and associated
parameters. Since last four decades huge numbers of researchers have been
working on the performance of GA and its enhancement. This earlier research
work on analyzing the performance of GA enforces the need to further
investigate the exploration and exploitation characteristics and observe its
impact on the behavior and overall performance of GA. This paper introduces the
novel approach of adaptive twin probability associated with the advanced twin
operator that enhances the performance of GA. The design of the advanced twin
operator is extrapolated from the twin offspring birth due to single ovulation
in natural genetic systems as mentioned in the earlier works. The twin
probability of this operator is adaptively varied based on the fitness of best
individual thereby relieving the GA user from statically defining its value.
This novel approach of adaptive twin probability is experimented and tested on
the standard benchmark optimization test functions. The experimental results
show the increased accuracy in terms of the best individual and reduced
convergence time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1240</identifier>
 <datestamp>2013-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1240</id><created>2013-10-04</created><authors><author><keyname>Romaszewski</keyname><forenames>Micha&#x142;</forenames></author><author><keyname>Gawron</keyname><forenames>Piotr</forenames></author><author><keyname>Opozda</keyname><forenames>Sebastian</forenames></author></authors><title>Compression of animated 3D models using HO-SVD</title><categories>cs.GR</categories><comments>15 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents an analysis of Higher Order Singular Value Decomposition
(HO-SVD) applied to lossy compression of 3D mesh animations. We describe
strategies for choosing a number of preserved spatial and temporal components
after tensor decomposition. Compression error is measured using three metrics
(MSE, Hausdorff, MSDM). Results are compared with a method based on Principal
Component Analysis (PCA) and presented on a set of animations with typical mesh
deformations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1249</identifier>
 <datestamp>2013-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1249</id><created>2013-10-04</created><authors><author><keyname>Jarynowski</keyname><forenames>Andrzej</forenames></author><author><keyname>Rostami</keyname><forenames>Amir</forenames></author></authors><title>Reading Stockholm Riots 2013 in social media by text-mining</title><categories>cs.SI cs.CL physics.soc-ph stat.AP</categories><comments>5p</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The riots in Stockholm in May 2013 were an event that reverberated in the
world media for its dimension of violence that had spread through the Swedish
capital. In this study we have investigated the role of social media in
creating media phenomena via text mining and natural language processing. We
have focused on two channels of communication for our analysis: Twitter and
Poloniainfo.se (Forum of Polish community in Sweden). Our preliminary results
show some hot topics driving discussion related mostly to Swedish Police and
Swedish Politics by counting word usage. Typical features for media
intervention are presented. We have built networks of most popular phrases,
clustered by categories (geography, media institution, etc.). Sentiment
analysis shows negative connotation with Police. The aim of this preliminary
exploratory quantitative study was to generate questions and hypotheses, which
we could carefully follow by deeper more qualitative methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1250</identifier>
 <datestamp>2013-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1250</id><created>2013-08-15</created><authors><author><keyname>Ligeiro</keyname><forenames>Rui</forenames></author><author><keyname>Mendes</keyname><forenames>R. Vilela</forenames></author></authors><title>Learning ambiguous functions by neural networks</title><categories>cs.NE cs.LG physics.data-an</categories><comments>13 pages, 9 figures</comments><msc-class>68T37, 82C32</msc-class><acm-class>I.2.6; I.5.1; I.5.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is not, in general, possible to have access to all variables that
determine the behavior of a system. Having identified a number of variables
whose values can be accessed, there may still be hidden variables which
influence the dynamics of the system. The result is model ambiguity in the
sense that, for the same (or very similar) input values, different objective
outputs should have been obtained. In addition, the degree of ambiguity may
vary widely across the whole range of input values. Thus, to evaluate the
accuracy of a model it is of utmost importance to create a method to obtain the
degree of reliability of each output result. In this paper we present such a
scheme composed of two coupled artificial neural networks: the first one being
responsible for outputting the predicted value, whereas the other evaluates the
reliability of the output, which is learned from the error values of the first
one. As an illustration, the scheme is applied to a model for tracking slopes
in a straw chamber and to a credit scoring model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1257</identifier>
 <datestamp>2013-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1257</id><created>2013-08-10</created><authors><author><keyname>Eickenberg</keyname><forenames>Michael</forenames><affiliation>INRIA Saclay - Ile de France, LNAO</affiliation></author><author><keyname>Pedregosa</keyname><forenames>Fabian</forenames><affiliation>INRIA Saclay - Ile de France, INRIA Paris - Rocquencourt</affiliation></author><author><keyname>Mehdi</keyname><forenames>Senoussi</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author><author><keyname>Gramfort</keyname><forenames>Alexandre</forenames><affiliation>LTCI</affiliation></author><author><keyname>Thirion</keyname><forenames>Bertrand</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author></authors><title>Second order scattering descriptors predict fMRI activity due to visual
  textures</title><categories>cs.CV</categories><comments>3nd International Workshop on Pattern Recognition in NeuroImaging
  (2013)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Second layer scattering descriptors are known to provide good classification
performance on natural quasi-stationary processes such as visual textures due
to their sensitivity to higher order moments and continuity with respect to
small deformations. In a functional Magnetic Resonance Imaging (fMRI)
experiment we present visual textures to subjects and evaluate the predictive
power of these descriptors with respect to the predictive power of simple
contour energy - the first scattering layer. We are able to conclude not only
that invariant second layer scattering coefficients better encode voxel
activity, but also that well predicted voxels need not necessarily lie in known
retinotopic regions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1259</identifier>
 <datestamp>2014-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1259</id><created>2013-10-04</created><authors><author><keyname>Coluccia</keyname><forenames>Giulio</forenames></author><author><keyname>Magli</keyname><forenames>Enrico</forenames></author></authors><title>A Novel Progressive Image Scanning and Reconstruction Scheme based on
  Compressed Sensing and Linear Prediction</title><categories>cs.IT cs.CV math.IT</categories><comments>2012 IEEE International Conference on Multimedia and Expo (ICME),
  Melbourne, Australia, 9-13 July 2012, pp.866-871</comments><doi>10.1109/ICME.2012.71</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed sensing (CS) is an innovative technique allowing to represent
signals through a small number of their linear projections. In this paper we
address the application of CS to the scenario of progressive acquisition of 2D
visual signals in a line-by-line fashion. This is an important setting which
encompasses diverse systems such as flatbed scanners and remote sensing
imagers. The use of CS in such setting raises the problem of reconstructing a
very high number of samples, as are contained in an image, from their linear
projections. Conventional reconstruction algorithms, whose complexity is cubic
in the number of samples, are computationally intractable. In this paper we
develop an iterative reconstruction algorithm that reconstructs an image by
iteratively estimating a row, and correlating adjacent rows by means of linear
prediction. We develop suitable predictors and test the proposed algorithm in
the context of flatbed scanners and remote sensing imaging systems. We show
that this approach can significantly improve the results of separate
reconstruction of each row, providing very good reconstruction quality with
reasonable complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1266</identifier>
 <datestamp>2014-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1266</id><created>2013-10-04</created><authors><author><keyname>Coluccia</keyname><forenames>Giulio</forenames></author><author><keyname>Kamden-Kuiteng</keyname><forenames>Simeon</forenames></author><author><keyname>Abrardo</keyname><forenames>Andrea</forenames></author><author><keyname>Barni</keyname><forenames>Mauro</forenames></author><author><keyname>Magli</keyname><forenames>Enrico</forenames></author></authors><title>Progressive Compressed Sensing and Reconstruction of Multidimensional
  Signals Using Hybrid Transform/Prediction Sparsity Model</title><categories>cs.IT math.IT</categories><journal-ref>IEEE Journal on Emerging and Selected Topics in Circuits and
  Systems, vol.2, no.3, pp.340,352, Sept. 2012</journal-ref><doi>10.1109/JETCAS.2012.2214891</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed sensing (CS) is an innovative technique allowing to represent
signals through a small number of their linear projections. Hence, CS can be
thought of as a natural candidate for acquisition of multidimensional signals,
as the amount of data acquired and processed by conventional sensors could
create problems in terms of computational complexity. In this paper, we propose
a framework for the acquisition and reconstruction of multidimensional
correlated signals. The approach is general and can be applied to D dimensional
signals, even if the algorithms we propose to practically implement such
architectures apply to 2-D and 3-D signals. The proposed architectures employ
iterative local signal reconstruction based on a hybrid transform/prediction
correlation model, coupled with a proper initialization strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1278</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1278</id><created>2013-10-03</created><updated>2014-09-22</updated><authors><author><keyname>Karandikar</keyname><forenames>Prateek</forenames></author><author><keyname>Kufleitner</keyname><forenames>Manfred</forenames></author><author><keyname>Schnoebelen</keyname><forenames>Philippe</forenames></author></authors><title>On the index of Simon's congruence for piecewise testability</title><categories>cs.FL cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simon's congruence, denoted \sim_n, relates words having the same subwords of
length up to n. We show that, over a k-letter alphabet, the number of words
modulo \sim_n is in 2^{\Theta(n^{k-1} log n)}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1285</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1285</id><created>2013-10-04</created><updated>2013-12-06</updated><authors><author><keyname>Harispe</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Ranwez</keyname><forenames>Sylvie</forenames></author><author><keyname>Janaqi</keyname><forenames>Stefan</forenames></author><author><keyname>Montmain</keyname><forenames>Jacky</forenames></author></authors><title>Semantic Measures for the Comparison of Units of Language, Concepts or
  Instances from Text and Knowledge Base Analysis</title><categories>cs.CL</categories><comments>survey</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Semantic measures are widely used today to estimate the strength of the
semantic relationship between elements of various types: units of language
(e.g., words, sentences, documents), concepts or even instances semantically
characterized (e.g., diseases, genes, geographical locations). Semantic
measures play an important role to compare such elements according to semantic
proxies: texts and knowledge representations, which support their meaning or
describe their nature. Semantic measures are therefore essential for designing
intelligent agents which will for example take advantage of semantic analysis
to mimic human ability to compare abstract or concrete objects. This paper
proposes a comprehensive survey of the broad notion of semantic measure for the
comparison of units of language, concepts or instances based on semantic proxy
analyses. Semantic measures generalize the well-known notions of semantic
similarity, semantic relatedness and semantic distance, which have been
extensively studied by various communities over the last decades (e.g.,
Cognitive Sciences, Linguistics, and Artificial Intelligence to mention a few).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1294</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1294</id><created>2013-10-04</created><updated>2015-06-11</updated><authors><author><keyname>Macris</keyname><forenames>Nicolas</forenames></author><author><keyname>Vuffray</keyname><forenames>Marc</forenames></author></authors><title>The Bethe Free Energy Allows to Compute the Conditional Entropy of
  Graphical Code Instances. A Proof from the Polymer Expansion</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main objective of this paper is to explore the precise relationship
between the Bethe free energy (or entropy) and the Shannon conditional entropy
of graphical error correcting codes. The main result shows that the Bethe free
energy associated with a low-density parity-check code used over a binary
symmetric channel in a large noise regime is, with high probability,
asymptotically exact as the block length grows. To arrive at this result we
develop new techniques for rather general graphical models based on the loop
sum as a starting point and the polymer expansion from statistical mechanics.
The true free energy is computed as a series expansion containing the Bethe
free energy as its zero-th order term plus a series of corrections. It is
easily seen that convergence criteria for such expansions are satisfied for
general high-temperature models. We apply these general results to ensembles of
low-density generator-matrix and parity-check codes. While the application to
generator-matrix codes follows standard &quot;high temperature&quot; methods, the case of
parity-check codes requires non-trivial new ideas because the hard constraints
correspond to a zero-temperature regime. Nevertheless one can combine the
polymer expansion with expander and counting arguments to show that the
difference between the true and Bethe free energies vanishes with high
probability in the large block
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1308</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1308</id><created>2013-10-04</created><authors><author><keyname>Bodlak</keyname><forenames>M.</forenames></author><author><keyname>Frolov</keyname><forenames>V.</forenames></author><author><keyname>Jary</keyname><forenames>V.</forenames></author><author><keyname>Huber</keyname><forenames>S.</forenames></author><author><keyname>Konorov</keyname><forenames>I.</forenames></author><author><keyname>Levit</keyname><forenames>D.</forenames></author><author><keyname>Novy</keyname><forenames>J.</forenames></author><author><keyname>Paul</keyname><forenames>S.</forenames></author><author><keyname>Salac</keyname><forenames>R.</forenames></author><author><keyname>Virius</keyname><forenames>M.</forenames></author></authors><title>FPGA based data acquisition system for COMPASS experiment</title><categories>physics.ins-det cs.SY</categories><comments>8 pages, CHEP 2013</comments><doi>10.1088/1742-6596/513/1/012029</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses the present data acquisition system (DAQ) of the COMPASS
experiment at CERN and presents development of a new DAQ. The new DAQ must
preserve present data format and be able to communicate with FPGA cards. Parts
of the new DAQ are based on state machines and they are implemented in C++ with
usage of the QT framework, the DIM library, and the IPBus technology. Prototype
of the system is prepared and communication through DIM between parts was
tested. An implementation of the IPBus technology was prepared and tested. The
new DAQ proved to be able to fulfill requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1314</identifier>
 <datestamp>2013-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1314</id><created>2013-10-04</created><authors><author><keyname>Gherekhloo</keyname><forenames>Soheyl</forenames></author><author><keyname>Chaaban</keyname><forenames>Anas</forenames></author><author><keyname>Sezgin</keyname><forenames>Aydin</forenames></author></authors><title>The Generalized Degrees of Freedom of the Interference Relay Channel
  with Strong Interference</title><categories>cs.IT math.IT</categories><comments>8 pages, 4 figures, Allerton 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The interference relay channel (IRC) under strong interference is considered.
A high-signal-to-noise ratio (SNR) generalized degrees of freedom (GDoF)
characterization of the capacity is obtained. To this end, a new GDoF upper
bound is derived based on a genie-aided approach. The achievability of the GDoF
is based on cooperative interference neutralization. It turns out that the
relay increases the GDoF even if the relay-destination link is weak. Moreover,
in contrast to the standard interference channel, the GDoF is not a
monotonically increasing function of the interference strength in the strong
interference regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1316</identifier>
 <datestamp>2013-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1316</id><created>2013-10-04</created><authors><author><keyname>Frochaux</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Schweikardt</keyname><forenames>Nicole</forenames></author></authors><title>A note on monadic datalog on unranked trees</title><categories>cs.LO cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the article 'Recursive queries on trees and data trees' (ICDT'13),
Abiteboul et al., asked whether the containment problem for monadic datalog
over unordered unranked labeled trees using the child relation and the
descendant relation is decidable. This note gives a positive answer to this
question, as well as an overview of the relative expressive power of monadic
datalog on various representations of unranked trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1328</identifier>
 <datestamp>2013-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1328</id><created>2013-10-04</created><authors><author><keyname>Davis</keyname><forenames>Ernest</forenames></author></authors><title>The Relevance of Proofs of the Rationality of Probability Theory to
  Automated Reasoning and Cognitive Models</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A number of well-known theorems, such as Cox's theorem and de Finetti's
theorem. prove that any model of reasoning with uncertain information that
satisfies specified conditions of &quot;rationality&quot; must satisfy the axioms of
probability theory. I argue here that these theorems do not in themselves
demonstrate that probabilistic models are in fact suitable for any specific
task in automated reasoning or plausible for cognitive models. First, the
theorems only establish that there exists some probabilistic model; they do not
establish that there exists a useful probabilistic model, i.e. one with a
tractably small number of numerical parameters and a large number of
independence assumptions. Second, there are in general many different
probabilistic models for a given situation, many of which may be far more
irrational, in the usual sense of the term, than a model that violates the
axioms of probability theory. I illustrate this second point with an extended
examples of two tasks of induction, of a similar structure, where the
reasonable probabilistic models are very different.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1341</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1341</id><created>2013-10-04</created><updated>2014-10-18</updated><authors><author><keyname>Singh</keyname><forenames>Vijay</forenames></author><author><keyname>Tchernookov</keyname><forenames>Martin</forenames></author><author><keyname>Butterfield</keyname><forenames>Rebecca</forenames></author><author><keyname>Nemenman</keyname><forenames>Ilya</forenames></author></authors><title>Director Field Model of the Primary Visual Cortex for Contour Detection</title><categories>q-bio.NC cs.CV</categories><comments>9 pages, 7 figures</comments><journal-ref>PLoS ONE 9(10): e108991 (2014)</journal-ref><doi>10.1371/journal.pone.0108991</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We aim to build the simplest possible model capable of detecting long, noisy
contours in a cluttered visual scene. For this, we model the neural dynamics in
the primate primary visual cortex in terms of a continuous director field that
describes the average rate and the average orientational preference of active
neurons at a particular point in the cortex. We then use a linear-nonlinear
dynamical model with long range connectivity patterns to enforce long-range
statistical context present in the analyzed images. The resulting model has
substantially fewer degrees of freedom than traditional models, and yet it can
distinguish large contiguous objects from the background clutter by suppressing
the clutter and by filling-in occluded elements of object contours. This
results in high-precision, high-recall detection of large objects in cluttered
scenes. Parenthetically, our model has a direct correspondence with the Landau
- de Gennes theory of nematic liquid crystal in two dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1351</identifier>
 <datestamp>2014-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1351</id><created>2013-10-04</created><updated>2014-08-15</updated><authors><author><keyname>Ak&#xe7;akaya</keyname><forenames>Mehmet</forenames></author><author><keyname>Tarokh</keyname><forenames>Vahid</forenames></author></authors><title>New Conditions for Sparse Phase Retrieval</title><categories>cs.IT math.IT math.NA math.OC</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of sparse phase retrieval, where a $k$-sparse signal
${\bf x} \in {\mathbb R}^n \textrm{ (or } {\mathbb C}^n\textrm{)}$ is measured
as ${\bf y} = |{\bf Ax}|,$ where ${\bf A} \in {\mathbb R}^{m \times n} \textrm{
(or } {\mathbb C}^{m \times n}\textrm{ respectively)}$ is a measurement matrix
and $|\cdot|$ is the element-wise absolute value. For a real signal and a real
measurement matrix ${\bf A}$, we show that $m = 2k$ measurements are necessary
and sufficient to recover ${\bf x}$ uniquely. For complex signal ${\bf x} \in
{\mathbb C}^n$ and ${\bf A} \in {\mathbb C}^{m \times n}$, we show that $m =
4k-2$ phaseless measurements are sufficient to recover ${\bf x}$. It is known
that the multiplying constant $4$ in $m = 4k-2$ cannot be improved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1362</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1362</id><created>2013-10-04</created><updated>2015-03-10</updated><authors><author><keyname>Gesmundo</keyname><forenames>Fulvio</forenames></author><author><keyname>Hauenstein</keyname><forenames>Jonathan</forenames></author><author><keyname>Ikenmeyer</keyname><forenames>Christian</forenames></author><author><keyname>Landsberg</keyname><forenames>JM</forenames></author></authors><title>Complexity of linear circuits and geometry</title><categories>cs.CC math.AG</categories><comments>29 pages, final version to appear in FOCM</comments><msc-class>68Q17, 15B05, 65T</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use algebraic geometry to study matrix rigidity, and more generally, the
complexity of computing a matrix-vector product, continuing a study initiated
by Kumar, et. al. We (i) exhibit many non-obvious equations testing for
(border) rigidity, (ii) compute degrees of varieties associated to rigidity,
(iii) describe algebraic varieties associated to families of matrices that are
expected to have super-linear rigidity, and (iv) prove results about the ideals
and degrees of cones that are of interest in their own right.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1363</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1363</id><created>2013-10-04</created><updated>2015-09-15</updated><authors><author><keyname>Wager</keyname><forenames>Stefan</forenames></author><author><keyname>Blocker</keyname><forenames>Alexander</forenames></author><author><keyname>Cardin</keyname><forenames>Niall</forenames></author></authors><title>Weakly supervised clustering: Learning fine-grained signals from coarse
  labels</title><categories>stat.ML cs.LG</categories><comments>Published at http://dx.doi.org/10.1214/15-AOAS812 in the Annals of
  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOAS-AOAS812</report-no><journal-ref>Annals of Applied Statistics 2015, Vol. 9, No. 2, 801-820</journal-ref><doi>10.1214/15-AOAS812</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a classification problem where we do not have access to labels for
individual training examples, but only have average labels over subpopulations.
We give practical examples of this setup and show how such a classification
task can usefully be analyzed as a weakly supervised clustering problem. We
propose three approaches to solving the weakly supervised clustering problem,
including a latent variables model that performs well in our experiments. We
illustrate our methods on an analysis of aggregated elections data and an
industry data set that was the original motivation for this research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1366</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1366</id><created>2013-10-01</created><authors><author><keyname>Ara&#xfa;jo</keyname><forenames>E. B.</forenames></author><author><keyname>Moreira</keyname><forenames>A. A.</forenames></author><author><keyname>Furtado</keyname><forenames>V.</forenames></author><author><keyname>Pequeno</keyname><forenames>T. H. C.</forenames></author><author><keyname>Andrade</keyname><forenames>J. S.</forenames><suffix>Jr</suffix></author></authors><title>Collaboration networks from a large CV database: dynamics, topology and
  bonus impact</title><categories>physics.soc-ph cs.DL cs.SI</categories><comments>8 pages, 8 figures</comments><doi>10.1371/journal.pone.0090537</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the dynamics of research production and collaboration may
reveal better strategies for scientific careers, academic institutions and
funding agencies. Here we propose the use of a large and multidisciplinar
database of scientific curricula in Brazil, namely, the Lattes Platform, to
study patterns of scientific production and collaboration. In this database,
detailed information about publications and researchers are made available by
themselves so that coauthorship is unambiguous and individuals can be evaluated
by scientific productivity, geographical location and field of expertise. Our
results show that the collaboration network is growing exponentially for the
last three decades, with a distribution of number of collaborators per
researcher that approaches a power-law as the network gets older. Moreover,
both the distributions of number of collaborators and production per researcher
obey power-law behaviors, regardless of the geographical location or field,
suggesting that the same universal mechanism might be responsible for network
growth and productivity.We also show that the collaboration network under
investigation displays a typical assortative mixing behavior, where teeming
researchers (i.e., with high degree) tend to collaborate with others alike.
Finally, our analysis reveals that the distinctive collaboration profile of
researchers awarded with governmental scholarships suggests a strong bonus
impact on their productivity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1368</identifier>
 <datestamp>2013-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1368</id><created>2013-10-04</created><authors><author><keyname>Cherkashin</keyname><forenames>Danila D.</forenames></author><author><keyname>Kozik</keyname><forenames>Jakub</forenames></author></authors><title>A note on random greedy coloring of uniform hypergraphs</title><categories>math.CO cs.DM</categories><msc-class>05C15, 05C65, 05D40</msc-class><acm-class>G.2.1; G.2.2; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The smallest number of edges forming an n-uniform hypergraph which is not
r-colorable is denoted by m(n,r). Erd\H{o}s and Lov\'{a}sz conjectured that
m(n,2)=\theta(n 2^n)$. The best known lower bound m(n,2)=\Omega(sqrt(n/log(n))
2^n) was obtained by Radhakrishnan and Srinivasan in 2000. We present a simple
proof of their result. The proof is based on analysis of random greedy coloring
algorithm investigated by Pluh\'ar in 2009. The proof method extends to the
case of r-coloring, and we show that for any fixed r we have
m(n,r)=\Omega((n/log(n))^(1-1/r) r^n) improving the bound of Kostochka from
2004. We also derive analogous bounds on minimum edge degree of an n-uniform
hypergraph that is not r-colorable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1371</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1371</id><created>2013-10-02</created><updated>2015-09-06</updated><authors><author><keyname>Afik</keyname><forenames>Eldad</forenames></author></authors><title>Robust and highly performant ring detection algorithm for 3d particle
  tracking using 2d microscope imaging</title><categories>cs.CV cond-mat.soft physics.flu-dyn</categories><comments>Software source-code is available at
  https://github.com/eldad-a/ridge-directed-ring-detector, as well as the
  linking (tracking) procedure and the natural cubic smoothing splines under
  particle-tracking &amp; natural-cubic-smoothing-splines, correspondingly. 20
  manuscript pages, including 7 SI &amp; 3 references pages, 2 manuscript figures &amp;
  3 supporting figures</comments><journal-ref>Sci. Rep. 5, 13584, (2015)</journal-ref><doi>10.1038/srep13584</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Three-dimensional particle tracking is an essential tool in studying dynamics
under the microscope, namely, fluid dynamics in microfluidic devices, bacteria
taxis, cellular trafficking. The 3d position can be determined using 2d imaging
alone by measuring the diffraction rings generated by an out-of-focus
fluorescent particle, imaged on a single camera. Here I present a ring
detection algorithm exhibiting a high detection rate, which is robust to the
challenges arising from ring occlusion, inclusions and overlaps, and allows
resolving particles even when near to each other. It is capable of real time
analysis thanks to its high performance and low memory footprint. The proposed
algorithm, an offspring of the circle Hough transform, addresses the need to
efficiently trace the trajectories of many particles concurrently, when their
number in not necessarily fixed, by solving a classification problem, and
overcomes the challenges of finding local maxima in the complex parameter space
which results from ring clusters and noise. Several algorithmic concepts
introduced here can be advantageous in other cases, particularly when dealing
with noisy and sparse data. The implementation is based on open-source and
cross-platform software packages only, making it easy to distribute and modify.
It is implemented in a microfluidic experiment allowing real-time
multi-particle tracking at 70 Hz, achieving a detection rate which exceeds 94%
and only 1% false-detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1378</identifier>
 <datestamp>2013-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1378</id><created>2013-10-04</created><authors><author><keyname>C&#xe9;gielski</keyname><forenames>Patrick</forenames></author><author><keyname>Grigorieff</keyname><forenames>Serge</forenames></author><author><keyname>Guessarian</keyname><forenames>Ir&#xe8;ne</forenames></author></authors><title>On Lattices of Regular Sets of Natural Integers Closed under
  Decrementation</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider lattices of regular sets of non negative integers, i.e. of sets
definable in Presbuger arithmetic. We prove that if such a lattice is closed
under decrement then it is also closed under many other functions: quotients by
an integer, roots, etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1384</identifier>
 <datestamp>2013-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1384</id><created>2013-10-04</created><authors><author><keyname>Kamalapurkar</keyname><forenames>Rushikesh</forenames></author><author><keyname>Klotz</keyname><forenames>Justin</forenames></author><author><keyname>Dixon</keyname><forenames>Warren E.</forenames></author></authors><title>Concurrent learning-based online approximate feedback-Nash equilibrium
  solution of N-player nonzero-sum differential games</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a concurrent learning-based actor-critic-identifier
architecture to obtain an approximate feedback-Nash equilibrium solution to an
infinite horizon N-player nonzero-sum differential game online, without
requiring persistence of excitation (PE), for a nonlinear control-affine
system. Under a condition milder than PE, uniformly ultimately bounded
convergence of the developed control policies to the feedback-Nash equilibrium
policies is established.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1390</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1390</id><created>2013-10-06</created><authors><author><keyname>Harzl</keyname><forenames>Annemarie</forenames></author><author><keyname>Neidhoefer</keyname><forenames>Philipp</forenames></author><author><keyname>Rock</keyname><forenames>Valentin</forenames></author><author><keyname>Schafzahl</keyname><forenames>Maximilian</forenames></author><author><keyname>Slany</keyname><forenames>Wolfgang</forenames></author></authors><title>A Scratch-like visual programming system for Microsoft Windows Phone 8</title><categories>cs.CY cs.HC cs.SE</categories><comments>2 pages, 5 figures, Published in PRoMoTo'13 [arXiv:1309.5500]</comments><report-no>PrMoTo/2013/07</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pocket Code is a free and open source mobile visual programming system for
the Catrobat language. It allows users, starting from the age of eight, to
develop games and animations with their smartphones. Children can create
programs with their Android phone, iPhone, Windows Phone, or other smartphone
with an HTML5 browser. No notebook or desktop computer is needed. Pocket Code
is inspired by, but distinct from, the Scratch programming system developed by
the Lifelong Kindergarten Group at the MIT Media Lab. This tool demo describes
an in-practice experience with Pocket Code, the Windows Phone IDE for the
Catrobat language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1404</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1404</id><created>2013-10-04</created><authors><author><keyname>Cherkassky</keyname><forenames>Michael</forenames></author><author><keyname>Bornn</keyname><forenames>Luke</forenames></author></authors><title>Sequential Monte Carlo Bandits</title><categories>stat.ML cs.LG stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a flexible and efficient framework for handling
multi-armed bandits, combining sequential Monte Carlo algorithms with
hierarchical Bayesian modeling techniques. The framework naturally encompasses
restless bandits, contextual bandits, and other bandit variants under a single
inferential model. Despite the model's generality, we propose efficient Monte
Carlo algorithms to make inference scalable, based on recent developments in
sequential Monte Carlo methods. Through two simulation studies, the framework
is shown to outperform other empirical methods, while also naturally scaling to
more complex problems for which existing approaches can not cope. Additionally,
we successfully apply our framework to online video-based advertising
recommendation, and show its increased efficacy as compared to current state of
the art bandit algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1415</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1415</id><created>2013-10-04</created><authors><author><keyname>Denil</keyname><forenames>Misha</forenames></author><author><keyname>Matheson</keyname><forenames>David</forenames></author><author><keyname>de Freitas</keyname><forenames>Nando</forenames></author></authors><title>Narrowing the Gap: Random Forests In Theory and In Practice</title><categories>stat.ML cs.LG</categories><comments>Under review by the International Conference on Machine Learning
  (ICML) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite widespread interest and practical use, the theoretical properties of
random forests are still not well understood. In this paper we contribute to
this understanding in two ways. We present a new theoretically tractable
variant of random regression forests and prove that our algorithm is
consistent. We also provide an empirical evaluation, comparing our algorithm
and other theoretically tractable random forest models to the random forest
algorithm used in practice. Our experiments provide insight into the relative
importance of different simplifications that theoreticians have made to obtain
tractable models for analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1419</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1419</id><created>2013-10-04</created><authors><author><keyname>Singh</keyname><forenames>Sarabjot</forenames></author><author><keyname>Baccelli</keyname><forenames>Francois</forenames></author><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author></authors><title>On Association Cells in Random Heterogeneous Networks</title><categories>cs.IT cs.NI math.IT math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Characterizing user to access point (AP) association strategies in
heterogeneous cellular networks (HetNets) is critical for their performance
analysis, as it directly influences the load across the network. In this
letter, we introduce and analyze a class of association strategies, which we
term stationary association, and the resulting association cells. For random
HetNets, where APs are distributed according to a stationary point process, the
area of the resulting association cells are shown to be the marks of the
corresponding point process. Addressing the need of quantifying the load
experienced by a typical user, a &quot;Feller-paradox&quot; like relationship is
established between the area of the association cell containing origin and that
of a typical association cell. For the specific case of Poisson point process
and max power/SINR association, the mean association area of each tier is
derived and shown to increase with channel gain variance and decrease in the
path loss exponents of the corresponding tier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1425</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1425</id><created>2013-10-04</created><authors><author><keyname>Nasiruddin</keyname><forenames>Mohammad</forenames></author></authors><title>A State of the Art of Word Sense Induction: A Way Towards Word Sense
  Disambiguation for Under-Resourced Languages</title><categories>cs.CL</categories><comments>14 pages TALN/RECITAL 2013</comments><msc-class>68T50</msc-class><acm-class>I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Word Sense Disambiguation (WSD), the process of automatically identifying the
meaning of a polysemous word in a sentence, is a fundamental task in Natural
Language Processing (NLP). Progress in this approach to WSD opens up many
promising developments in the field of NLP and its applications. Indeed,
improvement over current performance levels could allow us to take a first step
towards natural language understanding. Due to the lack of lexical resources it
is sometimes difficult to perform WSD for under-resourced languages. This paper
is an investigation on how to initiate research in WSD for under-resourced
languages by applying Word Sense Induction (WSI) and suggests some interesting
topics to focus on.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1426</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1426</id><created>2013-10-04</created><authors><author><keyname>Hassan</keyname><forenames>Foyzul</forenames></author><author><keyname>Kotwal</keyname><forenames>Mohammed Rokibul Alam</forenames></author><author><keyname>Rahman</keyname><forenames>Md. Mostafizur</forenames></author><author><keyname>Nasiruddin</keyname><forenames>Mohammad</forenames></author><author><keyname>Latif</keyname><forenames>Md. Abdul</forenames></author><author><keyname>Huda</keyname><forenames>Mohammad Nurul</forenames></author></authors><title>Local Feature or Mel Frequency Cepstral Coefficients - Which One is
  Better for MLN-Based Bangla Speech Recognition?</title><categories>cs.CL</categories><comments>9 pages Advances in Computing and Communications (ACC) 2011</comments><msc-class>68T50</msc-class><acm-class>I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses the dominancy of local features (LFs), as input to the
multilayer neural network (MLN), extracted from a Bangla input speech over mel
frequency cepstral coefficients (MFCCs). Here, LF-based method comprises three
stages: (i) LF extraction from input speech, (ii) phoneme probabilities
extraction using MLN from LF and (iii) the hidden Markov model (HMM) based
classifier to obtain more accurate phoneme strings. In the experiments on
Bangla speech corpus prepared by us, it is observed that the LFbased automatic
speech recognition (ASR) system provides higher phoneme correct rate than the
MFCC-based system. Moreover, the proposed system requires fewer mixture
components in the HMMs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1428</identifier>
 <datestamp>2014-08-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1428</id><created>2013-10-04</created><updated>2014-08-21</updated><authors><author><keyname>Whitfield</keyname><forenames>J. D.</forenames></author><author><keyname>Yung</keyname><forenames>M. -H.</forenames></author><author><keyname>Tempel</keyname><forenames>D. G.</forenames></author><author><keyname>Boixo</keyname><forenames>S.</forenames></author><author><keyname>Aspuru-Guzik</keyname><forenames>A.</forenames></author></authors><title>Computational complexity of time-dependent density functional theory</title><categories>quant-ph cs.CC physics.chem-ph</categories><journal-ref>New J. Phys. 16 (2014) 083035</journal-ref><doi>10.1088/1367-2630/16/8/083035</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time-dependent density functional theory (TDDFT) is rapidly emerging as a
premier method for solving dynamical many-body problems in physics and
chemistry. The mathematical foundations of TDDFT are established through the
formal existence of a fictitious non-interacting system (known as the Kohn-Sham
system), which can reproduce the one-electron reduced probability density of
the actual system. We build upon these works and show that on the interior of
the domain of existence, the Kohn-Sham system can be efficiently obtained given
the time-dependent density. Since a quantum computer can efficiently produce
such time-dependent densities, we present a polynomial time quantum algorithm
to generate the time-dependent Kohn-Sham potential with controllable error
bounds. As a consequence, in contrast to the known intractability result for
ground state density functional theory (DFT), the computation of the necessary
time-dependent potentials given the initial state is in the complexity class
described by bounded error quantum computation in polynomial time (BQP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1440</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1440</id><created>2013-10-05</created><updated>2015-09-06</updated><authors><author><keyname>Kucherov</keyname><forenames>Gregory</forenames></author><author><keyname>Salikhov</keyname><forenames>Kamil</forenames></author><author><keyname>Tsur</keyname><forenames>Dekel</forenames></author></authors><title>Approximate String Matching using a Bidirectional Index</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study strategies of approximate pattern matching that exploit
bidirectional text indexes, extending and generalizing ideas of Lam et al. We
introduce a formalism, called search schemes, to specify search strategies of
this type, then develop a probabilistic measure for the efficiency of a search
scheme, prove several combinatorial results on efficient search schemes, and
finally, provide experimental computations supporting the superiority of our
strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1442</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1442</id><created>2013-10-05</created><authors><author><keyname>Ding</keyname><forenames>Cunsheng</forenames></author><author><keyname>Zhou</keyname><forenames>Zhengchun</forenames></author></authors><title>Binary Cyclic Codes from Explicit Polynomials over $\gf(2^m)$</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1206.4687,
  arXiv:1206.4370</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cyclic codes are a subclass of linear codes and have applications in consumer
electronics, data storage systems, and communication systems as they have
efficient encoding and decoding algorithms. In this paper, monomials and
trinomials over finite fields with even characteristic are employed to
construct a number of families of binary cyclic codes. Lower bounds on the
minimum weight of some families of the cyclic codes are developed. The minimum
weights of other families of the codes constructed in this paper are
determined. The dimensions of the codes are flexible. Some of the codes
presented in this paper are optimal or almost optimal in the sense that they
meet some bounds on linear codes. Open problems regarding binary cyclic codes
from monomials and trinomials are also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1448</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1448</id><created>2013-10-05</created><authors><author><keyname>Goto</keyname><forenames>Keisuke</forenames></author><author><keyname>Bannai</keyname><forenames>Hideo</forenames></author></authors><title>Space Efficient Linear Time Lempel-Ziv Factorization on
  Constant~Size~Alphabets</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new algorithm for computing the Lempel-Ziv Factorization (LZ77)
of a given string of length $N$ in linear time, that utilizes only $N\log N +
O(1)$ bits of working space, i.e., a single integer array, for constant size
integer alphabets. This greatly improves the previous best space requirement
for linear time LZ77 factorization (K\&quot;arkk\&quot;ainen et al. CPM 2013), which
requires two integer arrays of length $N$. Computational experiments show that
despite the added complexity of the algorithm, the speed of the algorithm is
only around twice as slow as previous fastest linear time algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1485</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1485</id><created>2013-10-05</created><authors><author><keyname>Dannert</keyname><forenames>Tilman</forenames></author><author><keyname>Marek</keyname><forenames>Andreas</forenames></author><author><keyname>Rampp</keyname><forenames>Markus</forenames></author></authors><title>Porting Large HPC Applications to GPU Clusters: The Codes GENE and
  VERTEX</title><categories>physics.comp-ph astro-ph.SR cs.DC physics.plasm-ph</categories><comments>10 pages, accepted for publication in ParCo 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have developed GPU versions for two major high-performance-computing (HPC)
applications originating from two different scientific domains. GENE is a
plasma microturbulence code which is employed for simulations of nuclear fusion
plasmas. VERTEX is a neutrino-radiation hydrodynamics code for &quot;first
principles&quot;-simulations of core-collapse supernova explosions. The codes are
considered state of the art in their respective scientific domains, both
concerning their scientific scope and functionality as well as the achievable
compute performance, in particular parallel scalability on all relevant HPC
platforms. GENE and VERTEX were ported by us to HPC cluster architectures with
two NVidia Kepler GPUs mounted in each node in addition to two Intel Xeon CPUs
of the Sandy Bridge family. On such platforms we achieve up to twofold gains in
the overall application performance in the sense of a reduction of the time to
solution for a given setup with respect to a pure CPU cluster. The paper
describes our basic porting strategies and benchmarking methodology, and
details the main algorithmic and technical challenges we faced on the new,
heterogeneous architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1493</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1493</id><created>2013-10-05</created><updated>2014-07-02</updated><authors><author><keyname>Raghavendra</keyname><forenames>Prasad</forenames></author><author><keyname>Schramm</keyname><forenames>Tselil</forenames></author></authors><title>Gap Amplification for Small-Set Expansion via Random Walks</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we achieve gap amplification for the Small-Set Expansion
problem. Specifically, we show that an instance of the Small-Set Expansion
Problem with completeness $\epsilon$ and soundness $\frac{1}{2}$ is at least as
difficult as Small-Set Expansion with completeness $\epsilon$ and soundness
$f(\epsilon)$, for any function $f(\epsilon)$ which grows faster than
$\sqrt{\epsilon}$. We achieve this amplification via random walks -- our gadget
is the graph with adjacency matrix corresponding to a random walk on the
original graph. An interesting feature of our reduction is that unlike gap
amplification via parallel repetition, the size of the instances (number of
vertices) produced by the reduction remains the same.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1498</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1498</id><created>2013-10-05</created><authors><author><keyname>Landia</keyname><forenames>Nikolas</forenames></author><author><keyname>Doerfel</keyname><forenames>Stephan</forenames></author><author><keyname>J&#xe4;schke</keyname><forenames>Robert</forenames></author><author><keyname>Anand</keyname><forenames>Sarabjot Singh</forenames></author><author><keyname>Hotho</keyname><forenames>Andreas</forenames></author><author><keyname>Griffiths</keyname><forenames>Nathan</forenames></author></authors><title>Deeper Into the Folksonomy Graph: FolkRank Adaptations and Extensions
  for Improved Tag Recommendations</title><categories>cs.IR</categories><acm-class>H.3.3; H.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The information contained in social tagging systems is often modelled as a
graph of connections between users, items and tags. Recommendation algorithms
such as FolkRank, have the potential to leverage complex relationships in the
data, corresponding to multiple hops in the graph. We present an in-depth
analysis and evaluation of graph models for social tagging data and propose
novel adaptations and extensions of FolkRank to improve tag recommendations. We
highlight implicit assumptions made by the widely used folksonomy model, and
propose an alternative and more accurate graph-representation of the data. Our
extensions of FolkRank address the new item problem by incorporating content
data into the algorithm, and significantly improve prediction results on
unpruned datasets. Our adaptations address issues in the iterative weight
spreading calculation that potentially hinder FolkRank's ability to leverage
the deep graph as an information source. Moreover, we evaluate the benefit of
considering each deeper level of the graph, and present important insights
regarding the characteristics of social tagging data in general. Our results
suggest that the base assumption made by conventional weight propagation
methods, that closeness in the graph always implies a positive relationship,
does not hold for the social tagging domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1500</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1500</id><created>2013-10-05</created><authors><author><keyname>Aggarwal</keyname><forenames>Abhinav</forenames></author><author><keyname>Kumar</keyname><forenames>Padam</forenames></author></authors><title>A theory of function-induced-orders to study recursion termination</title><categories>cs.LO</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Termination property of functions is an important issue in computability
theory. In this paper, we show that repeated iterations of a function can
induce an order amongst the elements of its domain set. Hasse diagram of the
poset, thus obtained, is shown to look like a forest of trees, with a possible
base set and a generator set (defined in the paper). Isomorphic forests may
arise for different functions and equivalences classes are, thus, formed. Based
on this analysis, a study of the class of deterministically terminating
functions is presented, in which the existence of a Self-Ranking Program, which
can prove its own termination, and a Universal Terminating Function, from which
every other terminating function can be derived, is conjectured.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1502</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1502</id><created>2013-10-05</created><updated>2014-05-15</updated><authors><author><keyname>Holodnak</keyname><forenames>John T.</forenames></author><author><keyname>Ipsen</keyname><forenames>Ilse C. F.</forenames></author></authors><title>Randomized Approximation of the Gram Matrix: Exact Computation and
  Probabilistic Bounds</title><categories>math.NA cs.LG stat.ML</categories><comments>Update to title in third version. Major revisions in second version
  including new bounds and a more detailed experimental section. Submitted to
  SIMAX</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a real matrix A with n columns, the problem is to approximate the Gram
product AA^T by c &lt;&lt; n weighted outer products of columns of A. Necessary and
sufficient conditions for the exact computation of AA^T (in exact arithmetic)
from c &gt;= rank(A) columns depend on the right singular vector matrix of A. For
a Monte-Carlo matrix multiplication algorithm by Drineas et al. that samples
outer products, we present probabilistic bounds for the 2-norm relative error
due to randomization. The bounds depend on the stable rank or the rank of A,
but not on the matrix dimensions. Numerical experiments illustrate that the
bounds are informative, even for stringent success probabilities and matrices
of small dimension. We also derive bounds for the smallest singular value and
the condition number of matrices obtained by sampling rows from orthonormal
matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1506</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1506</id><created>2013-10-05</created><authors><author><keyname>Abadi</keyname><forenames>Aharon</forenames></author><author><keyname>Dubinsky</keyname><forenames>Yael</forenames></author><author><keyname>Kirshin</keyname><forenames>Andrei</forenames></author><author><keyname>Mesika</keyname><forenames>Yossi</forenames></author><author><keyname>Ben-Harrush</keyname><forenames>Idan</forenames></author></authors><title>Codeless Screen-Oriented Programming for Enterprise Mobile Applications</title><categories>cs.SE</categories><comments>6 pages, 16 figures, PROMOTO Workshop in SPLASH'13</comments><report-no>PrMoTo/2013/02</report-no><acm-class>D.2.2; D.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing the user interface (UI) of mobile applications in the enterprise is
performed in many cases by the application users who represent the business
needs. This is based on existing developed enterprise services that can be
accessed by these applications. Design the UI of the mobile applications
require programming environments that are as much as possible codeless and easy
to use. In this paper, we present NitroGen which is a cloud-based
platform-independent tool that provides with no installation, a consumable
integrated set of capabilities to construct mobile solutions aiming at reducing
development and maintenance costs. We further illustrate how to use NitroGen
showing its visual touch-based mostly codeless way of programming. NitroGen can
easily connect to back-end services thus enable fast and facile development in
enterprises.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1507</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1507</id><created>2013-10-05</created><authors><author><keyname>Cegielski</keyname><forenames>Patrick</forenames></author><author><keyname>Grigorieff</keyname><forenames>Serge</forenames></author><author><keyname>Guessarian</keyname><forenames>Irene</forenames></author></authors><title>Newton representation of functions over natural integers having integral
  difference ratios</title><categories>cs.DM math.NT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Different questions lead to the same class of functions from natural integers
to integers: those which have integral difference ratios, i.e. verifying
$f(a)-f(b)\equiv0 \pmod {(a-b)}$ for all $a&gt;b$.
  We characterize this class of functions via their representations as Newton
series. This class, which obviously contains all polynomials with integral
coefficients, also contains unexpected functions, for instance all functions
$x\mapsto\lfloor e^{1/a}\;a^x\;x!\rfloor$, with $a\in\Z\setminus\{0,1\}$, and a
function equal to $\lfloor e\;x!\rfloor$ except on 0. Finally, to study the
complement class, we look at functions $\N\to\RR$ which are not uniformly close
to any function having integral difference ratios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1510</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1510</id><created>2013-10-05</created><authors><author><keyname>Ngo</keyname><forenames>Hien Quoc</forenames></author><author><keyname>Larsson</keyname><forenames>Erik G.</forenames></author><author><keyname>Marzetta</keyname><forenames>Thomas L.</forenames></author></authors><title>Massive MU-MIMO Downlink TDD Systems with Linear Precoding and Downlink
  Pilots</title><categories>cs.IT math.IT</categories><comments>Allerton Conference on Communication, Control, and Computing,
  Urbana-Champaign, Illinois, Oct. 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a massive MU-MIMO downlink time-division duplex system where a
base station (BS) equipped with many antennas serves several single-antenna
users in the same time-frequency resource. We assume that the BS uses linear
precoding for the transmission. To reliably decode the signals transmitted from
the BS, each user should have an estimate of its channel. In this work, we
consider an efficient channel estimation scheme to acquire CSI at each user,
called beamforming training scheme. With the beamforming training scheme, the
BS precodes the pilot sequences and forwards to all users. Then, based on the
received pilots, each user uses minimum mean-square error channel estimation to
estimate the effective channel gains. The channel estimation overhead of this
scheme does not depend on the number of BS antennas, and is only proportional
to the number of users. We then derive a lower bound on the capacity for
maximum-ratio transmission and zero-forcing precoding techniques which enables
us to evaluate the spectral efficiency taking into account the spectral
efficiency loss associated with the transmission of the downlink pilots.
Comparing with previous work where each user uses only the statistical channel
properties to decode the transmitted signals, we see that the proposed
beamforming training scheme is preferable for moderate and low-mobility
environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1512</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1512</id><created>2013-10-05</created><authors><author><keyname>Calmon</keyname><forenames>Flavio du Pin</forenames></author><author><keyname>Varia</keyname><forenames>Mayank</forenames></author><author><keyname>M&#xe9;dard</keyname><forenames>Muriel</forenames></author><author><keyname>Christiansen</keyname><forenames>Mark M.</forenames></author><author><keyname>Duffy</keyname><forenames>Ken R.</forenames></author><author><keyname>Tessaro</keyname><forenames>Stefano</forenames></author></authors><title>Bounds on inference</title><categories>cs.IT math.IT</categories><comments>Allerton 2013 with extended proof, 10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lower bounds for the average probability of error of estimating a hidden
variable X given an observation of a correlated random variable Y, and Fano's
inequality in particular, play a central role in information theory. In this
paper, we present a lower bound for the average estimation error based on the
marginal distribution of X and the principal inertias of the joint distribution
matrix of X and Y. Furthermore, we discuss an information measure based on the
sum of the largest principal inertias, called k-correlation, which generalizes
maximal correlation. We show that k-correlation satisfies the Data Processing
Inequality and is convex in the conditional distribution of Y given X. Finally,
we investigate how to answer a fundamental question in inference and privacy:
given an observation Y, can we estimate a function f(X) of the hidden random
variable X with an average error below a certain threshold? We provide a
general method for answering this question using an approach based on
rate-distortion theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1518</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1518</id><created>2013-10-05</created><authors><author><keyname>Mitra</keyname><forenames>Rangeet</forenames></author><author><keyname>Mishra</keyname><forenames>Amit Kumar</forenames></author></authors><title>Contraction Principle based Robust Iterative Algorithms for Machine
  Learning</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Iterative algorithms are ubiquitous in the field of data mining. Widely known
examples of such algorithms are the least mean square algorithm,
backpropagation algorithm of neural networks. Our contribution in this paper is
an improvement upon this iterative algorithms in terms of their respective
performance metrics and robustness. This improvement is achieved by a new
scaling factor which is multiplied to the error term. Our analysis shows that
in essence, we are minimizing the corresponding LASSO cost function, which is
the reason of its increased robustness. We also give closed form expressions
for the number of iterations for convergence and the MSE floor of the original
cost function for a minimum targeted value of the L1 norm. As a concluding
theme based on the stochastic subgradient algorithm, we give a comparison
between the well known Dantzig selector and our algorithm based on contraction
principle. By these simulations we attempt to show the optimality of our
approach for any widely used parent iterative optimization problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1525</identifier>
 <datestamp>2014-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1525</id><created>2013-10-05</created><updated>2014-09-18</updated><authors><author><keyname>Yang</keyname><forenames>Yang</forenames></author><author><keyname>Dong</keyname><forenames>Yuxiao</forenames></author><author><keyname>Chawla</keyname><forenames>Nitesh V.</forenames></author></authors><title>Microscopic Evolution of Social Networks by Triad Position Profile</title><categories>cs.SI physics.soc-ph</categories><comments>12 pages, 13 figures</comments><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Disentangling the mechanisms underlying the social network evolution is one
of social science's unsolved puzzles. Preferential attachment is a powerful
mechanism explaining social network dynamics, yet not able to explain all
scaling-laws in social networks. Recent advances in understanding social
network dynamics demonstrate that several scaling-laws in social networks
follow as natural consequences of triadic closure. Macroscopic comparisons
between them are discussed empirically in many works. However the network
evolution drives not only the emergence of macroscopic scaling but also the
microscopic behaviors. Here we exploit two fundamental aspects of the network
microscopic evolution: the individual influence evolution and the process of
link formation. First we develop a novel framework for the microscopic
evolution, where the mechanisms of preferential attachment and triadic closure
are well balanced. Then on four real-world datasets we apply our approach for
two microscopic problems: node's prominence prediction and link prediction,
where our method yields significant predictive improvement over baseline
solutions. Finally to be rigorous and comprehensive, we further observe that
our framework has a stronger generalization capacity across different kinds of
social networks for two microscopic prediction problems. We unveil the
significant factors with a greater degree of precision than has heretofore been
possible, and shed new light on networks evolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1530</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1530</id><created>2013-10-05</created><updated>2014-02-14</updated><authors><author><keyname>Dai</keyname><forenames>Hong-Ning</forenames></author><author><keyname>Wong</keyname><forenames>Raymond Chi-Wing</forenames></author><author><keyname>Zhao</keyname><forenames>Qinglin</forenames></author></authors><title>Multi-channel Wireless Networks with Infrastructure Support: Capacity
  and Delay</title><categories>cs.NI</categories><comments>12 pages, 6 figures, 3 tables</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we propose a novel multi-channel network with infrastructure
support, called an \textit{MC-IS} network, which has not been studied in the
literature. To the best of our knowledge, we are the first to study such an
\textit{MC-IS} network. Our \textit{MC-IS} network is equipped with a number of
infrastructure nodes which can communicate with common nodes using a number of
channels where a communication between a common node and an infrastructure node
is called an infrastructure communication and a communication between two
common nodes is called an ad-hoc communication. Our proposed \textit{MC-IS}
network has a number of advantages over three existing conventional networks,
namely a single-channel wireless ad hoc network (called an \textit{SC-AH}
network), a multi-channel wireless ad hoc network (called an \textit{MC-AH}
network) and a single-channel network with infrastructure support (called an
\textit{SC-IS} network). In particular, the \textit{network capacity} of our
proposed \textit{MC-IS} network is $\sqrt{n \log n}$ times higher than that of
an \textit{SC-AH} network and an \textit{MC-AH} network and the same as that of
an \textit{SC-IS} network, where $n$ is the number of nodes in the network. The
\textit{average delay} of our \textit{MC-IS} network is $\sqrt{\log n/n}$ times
lower than that of an \textit{SC-AH} network and an \textit{MC-AH} network, and
$\min(C_I,m)$ times lower than the average delay of an \textit{SC-IS} network,
where $C_I$ and $m$ denote the number of channels dedicated for infrastructure
communications and the number of interfaces mounted at each infrastructure
node, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1531</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1531</id><created>2013-10-05</created><authors><author><keyname>Donahue</keyname><forenames>Jeff</forenames></author><author><keyname>Jia</keyname><forenames>Yangqing</forenames></author><author><keyname>Vinyals</keyname><forenames>Oriol</forenames></author><author><keyname>Hoffman</keyname><forenames>Judy</forenames></author><author><keyname>Zhang</keyname><forenames>Ning</forenames></author><author><keyname>Tzeng</keyname><forenames>Eric</forenames></author><author><keyname>Darrell</keyname><forenames>Trevor</forenames></author></authors><title>DeCAF: A Deep Convolutional Activation Feature for Generic Visual
  Recognition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We evaluate whether features extracted from the activation of a deep
convolutional network trained in a fully supervised fashion on a large, fixed
set of object recognition tasks can be re-purposed to novel generic tasks. Our
generic tasks may differ significantly from the originally trained tasks and
there may be insufficient labeled or unlabeled data to conventionally train or
adapt a deep architecture to the new tasks. We investigate and visualize the
semantic clustering of deep convolutional features with respect to a variety of
such tasks, including scene recognition, domain adaptation, and fine-grained
recognition challenges. We compare the efficacy of relying on various network
levels to define a fixed feature, and report novel results that significantly
outperform the state-of-the-art on several important vision challenges. We are
releasing DeCAF, an open-source implementation of these deep convolutional
activation features, along with all associated network parameters to enable
vision researchers to be able to conduct experimentation with deep
representations across a range of visual concept learning paradigms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1533</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1533</id><created>2013-10-05</created><updated>2014-12-01</updated><authors><author><keyname>B&#xfc;hlmann</keyname><forenames>Peter</forenames></author><author><keyname>Peters</keyname><forenames>Jonas</forenames></author><author><keyname>Ernest</keyname><forenames>Jan</forenames></author></authors><title>CAM: Causal additive models, high-dimensional order search and penalized
  regression</title><categories>stat.ME cs.LG stat.ML</categories><comments>Published in at http://dx.doi.org/10.1214/14-AOS1260 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS1260</report-no><journal-ref>Annals of Statistics 2014, Vol. 42, No. 6, 2526-2556</journal-ref><doi>10.1214/14-AOS1260</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop estimation for potentially high-dimensional additive structural
equation models. A key component of our approach is to decouple order search
among the variables from feature or edge selection in a directed acyclic graph
encoding the causal structure. We show that the former can be done with
nonregularized (restricted) maximum likelihood estimation while the latter can
be efficiently addressed using sparse regression techniques. Thus, we
substantially simplify the problem of structure search and estimation for an
important class of causal models. We establish consistency of the (restricted)
maximum likelihood estimator for low- and high-dimensional scenarios, and we
also allow for misspecification of the error distribution. Furthermore, we
develop an efficient computational algorithm which can deal with many
variables, and the new method's accuracy and performance is illustrated on
simulated and real data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1536</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1536</id><created>2013-10-05</created><authors><author><keyname>Ma</keyname><forenames>Xiao</forenames></author><author><keyname>Lin</keyname><forenames>Lei</forenames></author><author><keyname>Liang</keyname><forenames>Chulong</forenames></author><author><keyname>Huang</keyname><forenames>Xiujie</forenames></author><author><keyname>Bai</keyname><forenames>Baoming</forenames></author></authors><title>An information spectrum approach to the capacity region of GIFC</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, we present a general formula for the capacity region of a
general interference channel with two pairs of users. The formula shows that
the capacity region is the union of a family of rectangles, where each
rectangle is determined by a pair of spectral inf-mutual information rates.
Although the presented formula is usually difficult to compute, it provides us
useful insights into the interference channels. In particular, when the inputs
are discrete ergodic Markov processes and the channel is stationary memoryless,
the formula can be evaluated by BCJR algorithm. Also the formula suggests us
that the simplest inner bounds (obtained by treating the interference as noise)
could be improved by taking into account the structure of the interference
processes. This is verified numerically by computing the mutual information
rates for Gaussian interference channels with embedded convolutional codes.
Moreover, we present a coding scheme to approach the theoretical achievable
rate pairs. Numerical results show that decoding gain can be achieved by
considering the structure of the interference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1537</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1537</id><created>2013-10-06</created><updated>2014-11-19</updated><authors><author><keyname>Mahani</keyname><forenames>Alireza S.</forenames></author><author><keyname>Sharabiani</keyname><forenames>Mansour T. A.</forenames></author></authors><title>SIMD Parallel MCMC Sampling with Applications for Big-Data Bayesian
  Analytics</title><categories>stat.CO cs.AI cs.DC</categories><doi>10.1016/j.csda.2015.02.010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computational intensity and sequential nature of estimation techniques for
Bayesian methods in statistics and machine learning, combined with their
increasing applications for big data analytics, necessitate both the
identification of potential opportunities to parallelize techniques such as
MCMC sampling, and the development of general strategies for mapping such
parallel algorithms to modern CPUs in order to elicit the performance up the
compute-based and/or memory-based hardware limits. Two opportunities for
Single-Instruction Multiple-Data (SIMD) parallelization of MCMC sampling for
probabilistic graphical models are presented. In exchangeable models with many
observations such as Bayesian Generalized Linear Models, child-node
contributions to the conditional posterior of each node can be calculated
concurrently. In undirected graphs with discrete nodes, concurrent sampling of
conditionally-independent nodes can be transformed into a SIMD form.
High-performance libraries with multi-threading and vectorization capabilities
can be readily applied to such SIMD opportunities to gain decent speedup, while
a series of high-level source-code and runtime modifications provide further
performance boost by reducing parallelization overhead and increasing data
locality for NUMA architectures. For big-data Bayesian GLM graphs, the
end-result is a routine for evaluating the conditional posterior and its
gradient vector that is 5 times faster than a naive implementation using
(built-in) multi-threaded Intel MKL BLAS, and reaches within the striking
distance of the memory-bandwidth-induced hardware limit. The proposed
optimization strategies improve the scaling of performance with number of cores
and width of vector units (applicable to many-core SIMD processors such as
Intel Xeon Phi and GPUs), resulting in cost-effectiveness, energy efficiency,
and higher speed on multi-core x86 processors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1538</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1538</id><created>2013-10-06</created><updated>2015-06-10</updated><authors><author><keyname>Griffith</keyname><forenames>Virgil</forenames></author><author><keyname>Chong</keyname><forenames>Edwin K. P.</forenames></author><author><keyname>James</keyname><forenames>Ryan G.</forenames></author><author><keyname>Ellison</keyname><forenames>Christopher J.</forenames></author><author><keyname>Crutchfield</keyname><forenames>James P.</forenames></author></authors><title>Intersection Information based on Common Randomness</title><categories>cs.IT math.IT</categories><comments>19 pages, 5 figures</comments><journal-ref>Entropy (2014) Volume 16, Issue 4</journal-ref><doi>10.3390/e16041985</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The introduction of the partial information decomposition generated a flurry
of proposals for defining an intersection information that quantifies how much
of &quot;the same information&quot; two or more random variables specify about a target
random variable. As of yet, none is wholly satisfactory. A palatable measure of
intersection information would provide a principled way to quantify slippery
concepts, such as synergy. Here, we introduce an intersection information
measure based on the G\'acs-K\&quot;orner common random variable that is the first
to satisfy the coveted target monotonicity property. Our measure is imperfect,
too, and we suggest directions for improvement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1540</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1540</id><created>2013-10-06</created><authors><author><keyname>Mohamed</keyname><forenames>Manar</forenames></author><author><keyname>Sachdeva</keyname><forenames>Niharika</forenames></author><author><keyname>Georgescu</keyname><forenames>Michael</forenames></author><author><keyname>Gao</keyname><forenames>Song</forenames></author><author><keyname>Saxena</keyname><forenames>Nitesh</forenames></author><author><keyname>Zhang</keyname><forenames>Chengcui</forenames></author><author><keyname>Kumaraguru</keyname><forenames>Ponnurangam</forenames></author><author><keyname>van Oorschot</keyname><forenames>Paul C.</forenames></author><author><keyname>Chen</keyname><forenames>Wei-Bang</forenames></author></authors><title>Three-Way Dissection of a Game-CAPTCHA: Automated Attacks, Relay
  Attacks, and Usability</title><categories>cs.CR cs.HC</categories><comments>16 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing captcha solutions on the Internet are a major source of user
frustration. Game captchas are an interesting and, to date, little-studied
approach claiming to make captcha solving a fun activity for the users. One
broad form of such captchas -- called Dynamic Cognitive Game (DCG) captchas --
challenge the user to perform a game-like cognitive task interacting with a
series of dynamic images. We pursue a comprehensive analysis of a
representative category of DCG captchas. We formalize, design and implement
such captchas, and dissect them across: (1) fully automated attacks, (2)
human-solver relay attacks, and (3) usability. Our results suggest that the
studied DCG captchas exhibit high usability and, unlike other known captchas,
offer some resistance to relay attacks, but they are also vulnerable to our
novel dictionary-based automated attack.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1545</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1545</id><created>2013-10-06</created><authors><author><keyname>Fan</keyname><forenames>Xuhui</forenames></author><author><keyname>Da Xu</keyname><forenames>Richard Yi</forenames></author><author><keyname>Cao</keyname><forenames>Longbing</forenames></author><author><keyname>Song</keyname><forenames>Yin</forenames></author></authors><title>Learning Hidden Structures with Relational Models by Adequately
  Involving Rich Information in A Network</title><categories>cs.LG cs.SI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Effectively modelling hidden structures in a network is very practical but
theoretically challenging. Existing relational models only involve very limited
information, namely the binary directional link data, embedded in a network to
learn hidden networking structures. There is other rich and meaningful
information (e.g., various attributes of entities and more granular information
than binary elements such as &quot;like&quot; or &quot;dislike&quot;) missed, which play a critical
role in forming and understanding relations in a network. In this work, we
propose an informative relational model (InfRM) framework to adequately involve
rich information and its granularity in a network, including metadata
information about each entity and various forms of link data. Firstly, an
effective metadata information incorporation method is employed on the prior
information from relational models MMSB and LFRM. This is to encourage the
entities with similar metadata information to have similar hidden structures.
Secondly, we propose various solutions to cater for alternative forms of link
data. Substantial efforts have been made towards modelling appropriateness and
efficiency, for example, using conjugate priors. We evaluate our framework and
its inference algorithms in different datasets, which shows the generality and
effectiveness of our models in capturing implicit structures in networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1551</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1551</id><created>2013-10-06</created><authors><author><keyname>Kumar</keyname><forenames>T. Ravi</forenames></author><author><keyname>Krishnaiah</keyname><forenames>R. V.</forenames></author></authors><title>Optical Disk with Blu-Ray Technology</title><categories>cs.OH</categories><comments>10 pages International Journal of Computer Engineering and
  Applications; 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Blu-ray is the name of a next-generation optical disc format jointly
developed by the Blu-ray Disc Association a group of the world's leading
consumer electronics, personal computer and media manufacturers. The format was
developed to enable recording, rewriting and playback of high-definition video,
as well as storing large amounts of data. This extra capacity combined with the
use of advanced video and audio codec will offer consumers an unprecedented HD
experience. While current optical disc technologies such as DVD and DVDRAM rely
on a red laser to read and write data, the new format uses a blue-violet laser
instead, hence the name Blu-ray. Blu ray also promises some added security,
making ways for copyright protections. Blu-ray discs can have a unique ID
written on them to have copyright protection inside the recorded streams. Blu
.ray disc takes the DVD technology one step further, just by using a laser with
a nice color.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1552</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1552</id><created>2013-10-06</created><authors><author><keyname>Salem</keyname><forenames>Amer O. Abu</forenames></author><author><keyname>Alhmiedat</keyname><forenames>Tareq</forenames></author><author><keyname>Samara</keyname><forenames>Ghassan</forenames></author></authors><title>Cache Discovery Policies of MANET</title><categories>cs.NI</categories><comments>9 pages</comments><journal-ref>World of Computer Science and Information TechnologyJournal
  (WCSIT), ISSN: 2221-0741, Vol. 3, No. 8, 135-143, 2013,</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In situations where establishing a network infrastructure is impossible,
Ad-hoc networks are considered particularly important. Most of the previous
research in Ad-hoc networks concentrated on the development and enhancement of
dynamic routing protocols, which could efficiently discover routes between two
communicating nodes. Although routing strategies is an important topic in
MANETs, other topics such as data access are also crucial since the final goal
of using Ad-hoc networks is to provide data access to mobile nodes. One of the
most attractive techniques used to improve the data access performance in MANET
environment is cooperative caching; which means multiple caching nodes share
and cooperatively manage the cached contents. It is lead the research to
important questions, what data should be cached, where, when, and how? A
cooperative caching addressed into two basic issues: cache discovery and cache
management, in other words, how to find requested data efficiently and how to
manage an individual cache to improve the overall capacity of a cooperated
cache. In this paper we have made a review of the existing cache discovery
algorithms to address four stages after application request and before server
response, using an historical file to record the previous data requests, and
proposed cluster architecture with data cluster head election to store
efficient information for future use and reducing the cost of flooding. In
addition, this paper suggests some alternative techniques for cache discovery.
Finally, the paper concludes with a discussion on future research directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1553</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1553</id><created>2013-10-06</created><authors><author><keyname>Gritsenko</keyname><forenames>Andrey</forenames></author></authors><title>A Workflow-Forecast Approach To The Task Scheduling Problem In
  Distributed Computing Systems</title><categories>cs.DC</categories><comments>7 pages, 5 tables, 7 figures</comments><journal-ref>International Journal of Advanced Studies in Computer Science and
  Engineering, Volume 2, Special Issue 2, pp. 1-7. September 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to provide a description of deep-learning-based
scheduling approach for academic-purpose high-performance computing systems.
The share of academic-purpose distributed computing systems (DCS) reaches 17.4
percents amongst TOP500 supercomputer sites (15.6 percents in performance
scale) that makes them a valuable object of research. The core of this approach
is to predict the future workflow of the system depending on the previously
submitted tasks using deep learning algorithm. Information on predicted tasks
is used by the resource management system (RMS) to perform efficient schedule.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1571</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1571</id><created>2013-10-06</created><authors><author><keyname>Shah</keyname><forenames>Tapan</forenames></author><author><keyname>Dabeer</keyname><forenames>Onkar</forenames></author></authors><title>Transmit Beamforming for MIMO Communication Systems with Low Precision
  ADC at the Receiver</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple antenna systems have been extensively used by standards designing
multi-gigabit communication systems operating in bandwidth of several GHz. In
this paper, we study the use of transmitter (Tx) beamforming techniques to
improve the performance of a MIMO system with a low precision ADC. We motivate
an approach to use eigenmode transmit beamforming (which imposes a diagonal
structure in the complete MIMO system) and use an eigenmode power allocation
which minimizes the uncoded BER of the finite precision system. Although we
cannot guarantee optimality of this approach, we observe that even low with
precision ADC, it performs comparably to full precision system with no
eigenmode power allocation. For example, in a high throughput MIMO system with
a finite precision ADC at the receiver, simulation results show that for a 3/4
LDPC coded 2x2 MIMO OFDM 16-QAM system with 3-bit precision ADC at the
receiver, a BER of 0.0001 is achieved at an SNR of 26 dB. This is 1 dB better
than that required for the same system with full precision but equal eigenmode
power allocation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1582</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1582</id><created>2013-10-06</created><authors><author><keyname>Nagy</keyname><forenames>Marcin</forenames></author><author><keyname>Singh</keyname><forenames>Varun</forenames></author><author><keyname>Ott</keyname><forenames>Joerg</forenames></author><author><keyname>Eggert</keyname><forenames>Lars</forenames></author></authors><title>Congestion Control using FEC for Conversational Multimedia Communication</title><categories>cs.NI cs.MM</categories><acm-class>C.2.2</acm-class><doi>10.1145/2557642.2557649</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new rate control algorithm for conversational
multimedia flows. In our approach, along with Real-time Transport Protocol
(RTP) media packets, we propose sending redundant packets to probe for
available bandwidth. These redundant packets are Forward Error Correction (FEC)
encoded RTP packets. A straightforward interpretation is that if no losses
occur, the sender can increase the sending rate to include the FEC bit rate,
and in the case of losses due to congestion the redundant packets help in
recovering the lost packets. We also show that by varying the FEC bit rate, the
sender is able to conservatively or aggressively probe for available bandwidth.
We evaluate our FEC-based Rate Adaptation (FBRA) algorithm in a network
simulator and in the real-world and compare it to other congestion control
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1588</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1588</id><created>2013-10-06</created><authors><author><keyname>Paporovic</keyname><forenames>Sasa</forenames></author></authors><title>Impacting the bioscience progress by backporting software for Bio-Linux</title><categories>cs.OS cs.SE</categories><comments>10 pages,2 Figures, 1 Table and 1 notice</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In year 2006 Bio-Linux with the work of Tim Booth and team gives its rising
and provide an operating system that was and still specialized in providing a
bioinformatic specific software environment for the working needs in this
corner of bioscience. It is shown that Bio-Linux is affected by a 2 year
release cycle and with this the final releases of Bio-Linux will not have the
latest bioinformatic software on board. The paper shows how to get around this
huge time gap and bring new software for Bio-Linux on board through a process
that is called backporting. A summary of within the work to this paper just
backported bioinformatic tools is given. A describtion of a workflow for
continuously integration of the newest bioinformatic tools gives an outlook to
further concrete planned developments and the influence of speeding up
scientific progress.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1590</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1590</id><created>2013-10-06</created><authors><author><keyname>Bhattacharya</keyname><forenames>Paheli</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Arnab</forenames></author></authors><title>Evolution of the Modern Phase of Written Bangla: A Statistical Study</title><categories>cs.CL</categories><comments>LCC 2013</comments><acm-class>I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Active languages such as Bangla (or Bengali) evolve over time due to a
variety of social, cultural, economic, and political issues. In this paper, we
analyze the change in the written form of the modern phase of Bangla
quantitatively in terms of character-level, syllable-level, morpheme-level and
word-level features. We collect three different types of corpora---classical,
newspapers and blogs---and test whether the differences in their features are
statistically significant. Results suggest that there are significant changes
in the length of a word when measured in terms of characters, but there is not
much difference in usage of different characters, syllables and morphemes in a
word or of different words in a sentence. To the best of our knowledge, this is
the first work on Bangla of this kind.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1593</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1593</id><created>2013-10-06</created><updated>2013-10-17</updated><authors><author><keyname>Kono</keyname><forenames>Shota</forenames></author><author><keyname>Aminaka</keyname><forenames>Daiki</forenames></author><author><keyname>Makino</keyname><forenames>Shoji</forenames></author><author><keyname>Rutkowski</keyname><forenames>Tomasz M.</forenames></author></authors><title>EEG Signal Processing and Classification for the Novel Tactile-Force
  Brain-Computer Interface Paradigm</title><categories>q-bio.NC cs.HC</categories><comments>6 pages (in conference proceedings original version); 6 figures,
  submitted to The 9th International Conference on Signal Image Technology &amp;
  Internet Based Systems, December 2-5, 2013, Kyoto, Japan; to be available at
  IEEE Xplore; IEEE Copyright 2013</comments><msc-class>92C55, 92C50, 92C30</msc-class><acm-class>D.2.2; H.5.2</acm-class><journal-ref>Proceedings of the 9th International Conference on Signal Image
  Technology and Internet Based Systems, (Kyoto, Japan), pp. 812-817, IEEE
  Computer Society, December 3-5, 2013</journal-ref><doi>10.1109/SITIS.2013.132</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The presented study explores the extent to which tactile-force stimulus
delivered to a hand holding a joystick can serve as a platform for a brain
computer interface (BCI). The four pressure directions are used to evoke
tactile brain potential responses, thus defining a tactile-force brain computer
interface (tfBCI). We present brain signal processing and classification
procedures leading to successful interfacing results. Experimental results with
seven subjects performing online BCI experiments provide a validation of the
hand location tfBCI paradigm, while the feasibility of the concept is
illuminated through remarkable information-transfer rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1597</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1597</id><created>2013-10-06</created><authors><author><keyname>Wang</keyname><forenames>Mengqiu</forenames></author><author><keyname>Manning</keyname><forenames>Christopher D.</forenames></author></authors><title>Cross-lingual Pseudo-Projected Expectation Regularization for Weakly
  Supervised Learning</title><categories>cs.CL cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a multilingual weakly supervised learning scenario where
knowledge from annotated corpora in a resource-rich language is transferred via
bitext to guide the learning in other languages. Past approaches project labels
across bitext and use them as features or gold labels for training. We propose
a new method that projects model expectations rather than labels, which
facilities transfer of model uncertainty across language boundaries. We encode
expectations as constraints and train a discriminative CRF model using
Generalized Expectation Criteria (Mann and McCallum, 2010). Evaluated on
standard Chinese-English and German-English NER datasets, our method
demonstrates F1 scores of 64% and 60% when no labeled data is used. Attaining
the same accuracy with supervised CRFs requires 12k and 1.5k labeled sentences.
Furthermore, when combined with labeled examples, our method yields significant
improvements over state-of-the-art supervised methods, achieving best reported
numbers to date on Chinese OntoNotes and German CoNLL-03 datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1608</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1608</id><created>2013-10-06</created><updated>2013-12-11</updated><authors><author><keyname>Gyongyosi</keyname><forenames>Laszlo</forenames></author></authors><title>Adaptive Multicarrier Quadrature Division Modulation for
  Continuous-Variable Quantum Key Distribution</title><categories>quant-ph cs.IT math.IT</categories><comments>v2: 37 pages, 8 figures, 1 table, added security proof</comments><doi>10.1117/12.2050095</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a continuous-variable quantum key distribution (CVQKD) system, the
information is conveyed by coherent state carriers. The quantum continuous
variables are sent through a quantum channel, where the presence of the
eavesdropper adds a white Gaussian noise to the transmission. The amount of
tolerable noise and loss is a crucial point in CVQKD, since it determines the
overall performance of the protocol, including the secure key rates and
transmission distances. In this work, we propose the adaptive multicarrier
quadrature division (AMQD) modulation technique for CVQKD. The method
granulates the Gaussian random input into Gaussian subcarrier continuous
variables in the encoding phase, which are then decoded by a continuous unitary
transformation. The subcarrier coherent variables formulate Gaussian
sub-channels from the physical link with strongly diverse transmission
capabilities, which leads to significantly improved transmission efficiency,
higher tolerable loss, and excess noise. We also investigate a
modulation-variance adaption technique within the AMQD scheme, which provides
optimal capacity-achieving communication over the sub-channels in the presence
of a Gaussian noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1622</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1622</id><created>2013-10-06</created><updated>2013-10-09</updated><authors><author><keyname>Bernadet</keyname><forenames>Alexis</forenames><affiliation>&#xc9;cole Polytechnique, France</affiliation></author><author><keyname>Lengrand</keyname><forenames>St&#xe9;phane Jean</forenames><affiliation>CNRS and &#xc9;cole Polytechnique, France</affiliation></author></authors><title>Non-idempotent intersection types and strong normalisation</title><categories>cs.LO</categories><comments>46 pages</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 4 (October
  10, 2013) lmcs:939</journal-ref><doi>10.2168/LMCS-9(4:3)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a typing system with non-idempotent intersection types, typing a
term syntax covering three different calculi: the pure {\lambda}-calculus, the
calculus with explicit substitutions {\lambda}S, and the calculus with explicit
substitutions, contractions and weakenings {\lambda}lxr. In each of the three
calculi, a term is typable if and only if it is strongly normalising, as it is
the case in (many) systems with idempotent intersections. Non-idempotency
brings extra information into typing trees, such as simple bounds on the
longest reduction sequence reducing a term to its normal form. Strong
normalisation follows, without requiring reducibility techniques. Using this,
we revisit models of the {\lambda}-calculus based on filters of intersection
types, and extend them to {\lambda}S and {\lambda}lxr. Non-idempotency
simplifies a methodology, based on such filter models, that produces modular
proofs of strong normalisation for well-known typing systems (e.g. System F).
We also present a filter model by means of orthogonality techniques, i.e. as an
instance of an abstract notion of orthogonality model formalised in this paper
and inspired by classical realisability. Compared to other instances based on
terms (one of which rephrases a now standard proof of strong normalisation for
the {\lambda}-calculus), the instance based on filters is shown to be better at
proving strong normalisation results for {\lambda}S and {\lambda}lxr. Finally,
the bounds on the longest reduction sequence, read off our typing trees, are
refined into an exact measure, read off a specific typing tree (called
principal); in each of the three calculi, a specific reduction sequence of such
length is identified. In the case of the {\lambda}-calculus, this complexity
result is, for longest reduction sequences, the counterpart of de Carvalho's
result for linear head-reduction sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1635</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1635</id><created>2013-10-06</created><authors><author><keyname>Krishnan</keyname><forenames>Rajet</forenames></author><author><keyname>Amat</keyname><forenames>Alexandre Graell i</forenames></author><author><keyname>Eriksson</keyname><forenames>Thomas</forenames></author><author><keyname>Colavolpe</keyname><forenames>Giulio</forenames></author></authors><title>Constellation Optimization in the Presence of Strong Phase Noise</title><categories>cs.IT math.IT</categories><comments>10 page, 10 figures, Accepted to IEEE Trans. Commun</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the problem of optimizing signal constellations for
strong phase noise. The problem is investigated by considering three
optimization formulations, which provide an analytical framework for
constellation design. In the first formulation, we seek to design
constellations that minimize the symbol error probability (SEP) for an
approximate ML detector in the presence of phase noise. In the second
formulation, we optimize constellations in terms of mutual information (MI) for
the effective discrete channel consisting of phase noise, additive white
Gaussian noise, and the approximate ML detector. To this end, we derive the MI
of this discrete channel. Finally, we optimize constellations in terms of the
MI for the phase noise channel. We give two analytical characterizations of the
MI of this channel, which are shown to be accurate for a wide range of
signal-to-noise ratios and phase noise variances. For each formulation, we
present a detailed analysis of the optimal constellations and their performance
in the presence of strong phase noise. We show that the optimal constellations
significantly outperform conventional constellations and those proposed in the
literature in terms of SEP, error floors, and MI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1638</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1638</id><created>2013-10-06</created><authors><author><keyname>Krishnan</keyname><forenames>Rajet</forenames></author><author><keyname>Khanzadi</keyname><forenames>M. Reza</forenames></author><author><keyname>Eriksson</keyname><forenames>Thomas</forenames></author><author><keyname>Svensson</keyname><forenames>Tommy</forenames></author></authors><title>Soft metrics and their Performance Analysis for Optimal Data Detection
  in the Presence of Strong Oscillator Phase Noise</title><categories>cs.IT math.IT</categories><journal-ref>IEEE Trans. Commun. p. 1-11, Aug. 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the classical problem of maximum-likelihood (ML)
detection of data in the presence of random phase noise. We consider a system,
where the random phase noise affecting the received signal is first compensated
by a tracker/estimator. Then the phase error and its statistics are used for
deriving the ML detector. Specifically, we derive an ML detector based on a
Gaussian assumption for the phase error probability density function (PDF).
Further without making any assumptions on the phase error PDF, we show that the
actual ML detector can be reformulated as a weighted sum of central moments of
the phase error PDF. We present a simple approximation of this new ML rule
assuming that the phase error distribution is unknown. The ML detectors derived
are also the aposteriori probabilities of the transmitted symbols, and are
referred to as soft metrics. Then, using the detector developed based on
Gaussian phase error assumption, we derive the symbol error probability (SEP)
performance and error floor analytically for arbitrary constellations. Finally
we compare SEP performance of the various detectors/metrics in this work and
those from literature for different signal constellations, phase noise
scenarios and SNR values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1649</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1649</id><created>2013-10-06</created><authors><author><keyname>Haws</keyname><forenames>David</forenames></author></authors><title>QuickLexSort: An efficient algorithm for lexicographically sorting
  nested restrictions of a database</title><categories>cs.DS</categories><comments>17, 1 figure</comments><msc-class>68Q25, 68P10, 62H17</msc-class><acm-class>F.2.2; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lexicographical sorting is a fundamental problem with applications to
contingency tables, databases, Bayesian networks, and more. A standard method
to lexicographically sort general data is to iteratively use a stable sort -- a
sort which preserves existing orders. Here we present a new method of
lexicographical sorting called QuickLexSort. Whereas a stable sort based
lexicographical sorting algorithm operates from the least important to most
important features, in contrast, QuickLexSort sorts from the most important to
least important features, refining the sort as it goes. QuickLexSort first
requires a one-time modest pre-processing step where each feature of the data
set is sorted independently. When lexicographically sorting a database,
QuickLexSort (including pre-processing) has comparable running time to using a
stable sort based approach. For a data base with $m$ rows and $n$ columns, and
a sorting algorithm running in time $O(mlog(m))$, a stable sort based
lexicographical sort and QuickLexSort will both take time $O(nmlog(m))$.
However in many applications one has the need to lexicographically sort nested
data, e.g.\ all possible sub-matrices up to a certain cardinality of columns.
In such cases we show QuickLexSort gives a performance improvement of a log
factor of the database length (rows in matrix) over using a standard stable
sort based approach. E.g.\ to sort all sub-matrices up to cardinality $k$,
QuickLexSort has running time $O(mn^k)$ whereas a stable sort based
lexicographical sort will take time $O(mlog(m)n^k)$. After the pre-processing
step that is run only once for the entire matrix, QuickLexSort has a running
time linear in the number of nested sub-matrices to sort. We conclude with an
application to Bayesian network scoring to detect epistasis using SNP marker
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1651</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1651</id><created>2013-10-06</created><authors><author><keyname>Elovici</keyname><forenames>Yuval</forenames></author><author><keyname>Fire</keyname><forenames>Michael</forenames></author><author><keyname>Herzberg</keyname><forenames>Amir</forenames></author><author><keyname>Shulman</keyname><forenames>Haya</forenames></author></authors><title>Ethical Considerations when Employing Fake Identities in OSN for
  Research</title><categories>cs.CY cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online Social Networks (OSNs) have rapidly become a prominent and widely used
service, offering a wealth of personal and sensitive information with
significant security and privacy implications. Hence, OSNs are also an
important - and popular - subject for research. To perform research based on
real-life evidence, however, researchers may need to access OSN data, such as
texts and files uploaded by users and connections among users. This raises
significant ethical problems. Currently, there are no clear ethical guidelines,
and researchers may end up (unintentionally) performing ethically questionable
research, sometimes even when more ethical research alternatives exist. For
example, several studies have employed `fake identities` to collect data from
OSNs, but fake identities may be used for attacks and are considered a security
issue. Is it legitimate to use fake identities for studying OSNs or for
collecting OSN data for research? We present a taxonomy of the ethical
challenges facing researchers of OSNs and compare different approaches. We
demonstrate how ethical considerations have been taken into account in previous
studies that used fake identities. In addition, several possible approaches are
offered to reduce or avoid ethical misconducts. We hope this work will
stimulate the development and use of ethical practices and methods in the
research of online social networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1659</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1659</id><created>2013-10-06</created><authors><author><keyname>He</keyname><forenames>Dan</forenames></author><author><keyname>Rish</keyname><forenames>Irina</forenames></author><author><keyname>Haws</keyname><forenames>David</forenames></author><author><keyname>Teyssedre</keyname><forenames>Simon</forenames></author><author><keyname>Karaman</keyname><forenames>Zivan</forenames></author><author><keyname>Parida</keyname><forenames>Laxmi</forenames></author></authors><title>MINT: Mutual Information based Transductive Feature Selection for
  Genetic Trait Prediction</title><categories>cs.LG cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Whole genome prediction of complex phenotypic traits using high-density
genotyping arrays has attracted a great deal of attention, as it is relevant to
the fields of plant and animal breeding and genetic epidemiology. As the number
of genotypes is generally much bigger than the number of samples, predictive
models suffer from the curse-of-dimensionality. The curse-of-dimensionality
problem not only affects the computational efficiency of a particular genomic
selection method, but can also lead to poor performance, mainly due to
correlation among markers. In this work we proposed the first transductive
feature selection method based on the MRMR (Max-Relevance and Min-Redundancy)
criterion which we call MINT. We applied MINT on genetic trait prediction
problems and showed that in general MINT is a better feature selection method
than the state-of-the-art inductive method mRMR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1672</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1672</id><created>2013-10-07</created><authors><author><keyname>Vakharia</keyname><forenames>Donna</forenames></author><author><keyname>Lease</keyname><forenames>Matthew</forenames></author></authors><title>Beyond AMT: An Analysis of Crowd Work Platforms</title><categories>cs.CY cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While Amazon's Mechanical Turk (AMT) helped launch the paid crowd work
industry eight years ago, many new vendors now offer a range of alternative
models. Despite this, little crowd work research has explored other platforms.
Such near-exclusive focus risks letting AMT's particular vagaries and
limitations overly shape our understanding of crowd work and the research
questions and directions being pursued. To address this, we present a
cross-platform content analysis of seven crowd work platforms. We begin by
reviewing how AMT assumptions and limitations have influenced prior research.
Next, we formulate key criteria for characterizing and differentiating crowd
work platforms. Our analysis of platforms contrasts them with AMT, informing
both methodology of use and directions for future research. Our cross-platform
analysis represents the only such study by researchers for researchers,
intended to further enrich the diversity of research on crowd work and
accelerate progress.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1679</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1679</id><created>2013-10-07</created><authors><author><keyname>Cui</keyname><forenames>Qingpei</forenames></author><author><keyname>Ye</keyname><forenames>Tong</forenames></author><author><keyname>Lee</keyname><forenames>Tony T.</forenames></author><author><keyname>Guo</keyname><forenames>Wei</forenames></author><author><keyname>Hu</keyname><forenames>Weisheng</forenames></author></authors><title>Stability and Delay Analysis of EPON Registration Protocol</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Ethernet passive optical network (EPON) has recently emerged as the
mainstream of broadband access networks. The registration process of EPON,
defined by the IEEE 802.3av standard, is a multi-point control protocol (MPCP)
within the media access control (MAC) layer. As with other contention-based
channel access methods, such as ALOHA and CSMA, stability and delay are
critical issues concerning the performances of implementing the protocol on
systems with finite channel capacity. In this paper, the registration process
of an EPON subscriber, called optical network units (ONUs), is modeled as a
discrete-time Markov chain, from which we derive the fundamental throughput
equation of EPON that characterizes the registration processes. The solutions
of this characteristic equation depend on the maximum waiting time. The aim of
our stability analysis is to pinpoint the region of the maximum waiting time
that can guarantee a stable registration throughput and a bounded registration
delay. For a maximum waiting time selected from the stable region, we obtain
the expression of registration delay experienced by an ONU attempting to
register. All analytic results presented in this paper were verified by
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1690</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1690</id><created>2013-10-07</created><authors><author><keyname>Liu</keyname><forenames>Fayao</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Reid</keyname><forenames>Ian</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author></authors><title>Online Unsupervised Feature Learning for Visual Tracking</title><categories>cs.CV</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature encoding with respect to an over-complete dictionary learned by
unsupervised methods, followed by spatial pyramid pooling, and linear
classification, has exhibited powerful strength in various vision applications.
Here we propose to use the feature learning pipeline for visual tracking.
Tracking is implemented using tracking-by-detection and the resulted framework
is very simple yet effective. First, online dictionary learning is used to
build a dictionary, which captures the appearance changes of the tracking
target as well as the background changes. Given a test image window, we extract
local image patches from it and each local patch is encoded with respect to the
dictionary. The encoded features are then pooled over a spatial pyramid to form
an aggregated feature vector. Finally, a simple linear classifier is trained on
these features.
  Our experiments show that the proposed powerful---albeit simple---tracker,
outperforms all the state-of-the-art tracking methods that we have tested.
Moreover, we evaluate the performance of different dictionary learning and
feature encoding methods in the proposed tracking framework, and analyse the
impact of each component in the tracking scenario. We also demonstrate the
flexibility of feature learning by plugging it into Hare et al.'s tracking
method. The outcome is, to our knowledge, the best tracker ever reported, which
facilitates the advantages of both feature learning and structured output
prediction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1693</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1693</id><created>2013-10-07</created><updated>2014-04-07</updated><authors><author><keyname>Sanandaji</keyname><forenames>Borhan M.</forenames></author><author><keyname>Hao</keyname><forenames>He</forenames></author><author><keyname>Poolla</keyname><forenames>Kameshwar</forenames></author><author><keyname>Vincent</keyname><forenames>Tyrone L.</forenames></author></authors><title>Improved Battery Models of an Aggregation of Thermostatically Controlled
  Loads for Frequency Regulation</title><categories>cs.SY</categories><comments>to appear in the 2014 American Control Conference - ACC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently it has been shown that an aggregation of Thermostatically Controlled
Loads (TCLs) can be utilized to provide fast regulating reserve service for
power grids and the behavior of the aggregation can be captured by a stochastic
battery with dissipation. In this paper, we address two practical issues
associated with the proposed battery model. First, we address clustering of a
heterogeneous collection and show that by finding the optimal dissipation
parameter for a given collection, one can divide these units into few clusters
and improve the overall battery model. Second, we analytically characterize the
impact of imposing a no-short-cycling requirement on TCLs as constraints on the
ramping rate of the regulation signal. We support our theorems by providing
simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1709</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1709</id><created>2013-10-07</created><authors><author><keyname>Olivier</keyname><forenames>Mullier</forenames></author><author><keyname>Goubault</keyname><forenames>&#xc9;ric</forenames></author><author><keyname>Kieffer</keyname><forenames>Michel</forenames></author><author><keyname>Putot</keyname><forenames>Sylvie</forenames></author></authors><title>General inner approximation of vector-valued functions</title><categories>cs.NA math.NA</categories><comments>26 pages, 17 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of evaluating a subset of the range of a
vector-valued function. It is based on a work by Gold- sztejn and Jaulin which
provides methods based on interval analysis to address this problem when the
dimension of the domain and co-domain of the function are equal. This paper
extends this result to vector-valued functions with domain and co-domain of
different dimensions. This ex- tension requires the knowledge of the rank of
the Jacobian function on the whole domain. This leads to the sub-problem of
extracting an in- terval sub-matrix of maximum rank from a given interval
matrix. Three different techniques leading to approximate solutions of this
extraction are proposed and compared.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1710</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1710</id><created>2013-10-07</created><authors><author><keyname>Chun</keyname><forenames>Lam Ka</forenames></author><author><keyname>Lui</keyname><forenames>Lok Ming</forenames></author></authors><title>Landmark and Intensity Based Registration with Large Deformations via
  Quasi-conformal Maps</title><categories>math.DG cs.CG cs.GR</categories><comments>27 pages, 21 figures. arXiv admin note: text overlap with
  arXiv:1307.2679 by other authors</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Registration, which aims to find an optimal one-to-one correspondence between
different data, is an important problem in various fields. This problem is
especially challenging when large deformations occur. In this paper, we present
a novel algorithm to obtain diffeomorphic image or surface registrations with
large deformations via quasi-conformal maps. The basic idea is to minimize an
energy functional involving a Beltrami coefficient term, which measures the
distortion of the quasi-conformal map. The Beltrami coefficient effectively
controls the bijectivity and smoothness of the registration, even with very
large deformations. Using the proposed algorithm, landmark-based registration
between images or surfaces can be effectively computed. The obtained
registration is guaranteed to be diffeomorphic (1-1 and onto), even with a
large deformation or large number of landmark constraints. The proposed
algorithm can also be combined with matching intensity (such as image intensity
or surface curvature) to improve the accuracy of the registration. Experiments
have been carried out on both synthetic and real data. Results demonstrate the
efficacy of the proposed algorithm to obtain diffeomorphic registration between
images or surfaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1712</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1712</id><created>2013-10-07</created><updated>2015-01-09</updated><authors><author><keyname>Berhault</keyname><forenames>Guillaume</forenames></author><author><keyname>Leroux</keyname><forenames>Camille</forenames></author><author><keyname>Jego</keyname><forenames>Christophe</forenames></author><author><keyname>Dallet</keyname><forenames>Dominique</forenames></author></authors><title>Partial Sums Computation In Polar Codes Decoding</title><categories>cs.AR cs.IT math.IT</categories><comments>Accepted to ISCAS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polar codes are the first error-correcting codes to provably achieve the
channel capacity but with infinite codelengths. For finite codelengths the
existing decoder architectures are limited in working frequency by the partial
sums computation unit. We explain in this paper how the partial sums
computation can be seen as a matrix multiplication. Then, an efficient hardware
implementation of this product is investigated. It has reduced logic resources
and interconnections. Formalized architectures, to compute partial sums and to
generate the bits of the generator matrix k^n, are presented. The proposed
architecture allows removing the multiplexing resources used to assigned to
each processing elements the required partial sums.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1726</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1726</id><created>2013-10-07</created><authors><author><keyname>Diwan</keyname><forenames>Ajit Arvind</forenames></author><author><keyname>Ghosh</keyname><forenames>Subir Kumar</forenames></author><author><keyname>Roy</keyname><forenames>Bodhayan</forenames></author></authors><title>Four-connected triangulations of planar point sets</title><categories>cs.CG math.CO math.MG</categories><comments>35 pages, 35 figures, 5 references</comments><msc-class>68R10</msc-class><acm-class>G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of determining in polynomial time
whether a given planar point set $P$ of $n$ points admits 4-connected
triangulation. We propose a necessary and sufficient condition for recognizing
$P$, and present an $O(n^3)$ algorithm of constructing a 4-connected
triangulation of $P$. Thus, our algorithm solves a longstanding open problem in
computational geometry and geometric graph theory. We also provide a simple
method for constructing a noncomplex triangulation of $P$ which requires
$O(n^2)$ steps. This method provides a new insight to the structure of
4-connected triangulation of point sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1732</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1732</id><created>2013-10-07</created><authors><author><keyname>Chaaban</keyname><forenames>Anas</forenames></author><author><keyname>Sezgin</keyname><forenames>Aydin</forenames></author></authors><title>The Approximate Capacity Region of the Gaussian Y-Channel</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A full-duplex wireless network with three users that want to establish full
message-exchange via a relay is considered. Thus, the network known as the
Y-channel has a total of 6 messages, 2 outgoing and 2 incoming at each user.
The users are not physically connected, and thus the relay is essential for
their communication. The linear-shift deterministic Y-channel is considered
first, its capacity region is characterized and shown not to be given by the
cut-set bounds. The capacity achieving scheme has three different components
(strategies): a bi-directional, a cyclic, and a uni-directional strategy.
Network coding is used to realize the bi-directional and the cyclic strategies,
and thus to prove the achievability of the capacity region. The result is then
extended to the Gaussian Y-channel where the capacity region is characterized
within a constant gap independent of the channel parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1746</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1746</id><created>2013-10-07</created><authors><author><keyname>Subramanian</keyname><forenames>Ashwin</forenames></author><author><keyname>Kanth</keyname><forenames>G Sai</forenames></author><author><keyname>Vaze</keyname><forenames>Rahul</forenames></author></authors><title>Offline and Online Incentive Mechanism Design for Smart-phone
  Crowd-sourcing</title><categories>cs.CY cs.GT cs.NI</categories><comments>12 Pages, 7 Figures and submitted to IEEE/ACM Transactions on
  Networking</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of incentive mechanism design for
smart-phone crowd-sourcing. Each user participating in crowd-sourcing submits a
set of tasks it can accomplish and its corresponding bid. The platform then
selects the users and their payments to maximize its utility while ensuring
truthfulness, individual rationality, profitability, and polynomial algorithm
complexity. Both the offline and the online scenarios are considered, where in
the offline case, all users submit their profiles simultaneously, while in the
online case they do it sequentially, and the decision whether to accept or
reject each user is done instantaneously with no revocation. The proposed
algorithms for both the offline and the online case are shown to satisfy all
the four desired properties of an efficient auction. Through extensive
simulation, the performance of the offline and the online algorithm is also
compared.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1757</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1757</id><created>2013-10-07</created><updated>2014-01-11</updated><authors><author><keyname>Uria</keyname><forenames>Benigno</forenames></author><author><keyname>Murray</keyname><forenames>Iain</forenames></author><author><keyname>Larochelle</keyname><forenames>Hugo</forenames></author></authors><title>A Deep and Tractable Density Estimator</title><categories>stat.ML cs.LG</categories><comments>9 pages, 4 tables, 1 algorithm, 5 figures. To appear ICML 2014, JMLR
  W&amp;CP volume 32</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Neural Autoregressive Distribution Estimator (NADE) and its real-valued
version RNADE are competitive density models of multidimensional data across a
variety of domains. These models use a fixed, arbitrary ordering of the data
dimensions. One can easily condition on variables at the beginning of the
ordering, and marginalize out variables at the end of the ordering, however
other inference tasks require approximate inference. In this work we introduce
an efficient procedure to simultaneously train a NADE model for each possible
ordering of the variables, by sharing parameters across all these models. We
can thus use the most convenient model for each inference task at hand, and
ensembles of such models with different orderings are immediately available.
Moreover, unlike the original NADE, our training procedure scales to deep
models. Empirically, ensembles of Deep NADE models obtain state of the art
density estimation performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1758</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1758</id><created>2013-10-07</created><authors><author><keyname>Sottet</keyname><forenames>Jean-Sebastien</forenames></author><author><keyname>Vagner</keyname><forenames>Alain</forenames></author></authors><title>GENIUS: Generating Usable User Interfaces</title><categories>cs.HC cs.SE</categories><comments>Public Research Center Henri Tudor- Technical Report of the FNR's
  project GENIUS</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this report we describe the implementation and approach developed during
the GENIUS Project. The GENIUS project is about the generation of usable user
interfaces. It tries to cope with issues related to automatic generation where,
usually end-user complain about the poor quality (in term of usability) of
generated UI. To solve this issue GENIUS relies on Model-Driven Engineering
principles and several MDE tools. Notably, it consists in a set of metamodels
specific to the interaction, a set of model transformation embedding usability
criteria and an environment for model execution/ interpretation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1761</identifier>
 <datestamp>2014-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1761</id><created>2013-10-07</created><updated>2014-09-18</updated><authors><author><keyname>Gafni</keyname><forenames>Eli</forenames></author><author><keyname>Kuznetsov</keyname><forenames>Petr</forenames></author></authors><title>Simple CHT: A New Derivation of the Weakest Failure Detector for
  Consensus</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper proposes an alternative proof that Omega, an oracle that outputs a
process identifier and guarantees that eventually the same correct process
identifier is output at all correct processes, provides minimal information
about failures for solving consensus in read-write shared-memory systems: every
oracle that gives enough failure information to solve consensus can be used to
implement Omega.
  Unlike the original proof by Chandra, Hadzilacos and Toueg (CHT), the proof
presented in this paper builds upon the very fact that 2-process wait-free
consensus is impossible. Also, since the oracle that is used to implement can
solve consensus, the implementation is allowed to directly access consensus
objects. As a result, the proposed proof is shorter and conceptually simpler
than the original one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1763</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1763</id><created>2013-10-07</created><authors><author><keyname>Lago</keyname><forenames>Ugo Dal</forenames></author><author><keyname>Pellitta</keyname><forenames>Giulio</forenames></author></authors><title>Complexity Analysis in Presence of Control Operators and Higher-Order
  Functions (Long Version)</title><categories>cs.LO cs.PL</categories><acm-class>F.3.1; F.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A polarized version of Girard, Scedrov and Scott's Bounded Linear Logic is
introduced and its normalization properties studied. Following Laurent, the
logic naturally gives rise to a type system for the lambda-mu-calculus, whose
derivations reveal bounds on the time complexity of the underlying term. This
is the first example of a type system for the lambda-mu-calculus guaranteeing
time complexity bounds for typable programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1766</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1766</id><created>2013-10-07</created><authors><author><keyname>Foukalas</keyname><forenames>F.</forenames></author><author><keyname>Khattab</keyname><forenames>T.</forenames></author><author><keyname>Poor</keyname><forenames>H. V.</forenames></author></authors><title>Adaptive Modulation in Multi-user Cognitive Radio Networks over Fading
  Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the performance of adaptive modulation in multi-user cognitive
radio networks over fading channels is analyzed. Multi-user diversity is
considered for opportunistic user selection among multiple secondary users. The
analysis is obtained for Nakagami-$m$ fading channels. Both adaptive continuous
rate and adaptive discrete rate schemes are analysed in opportunistic spectrum
access and spectrum sharing. Numerical results are obtained and depicted to
quantify the effects of multi-user fading environments on adaptive modulation
operating in cognitive radio networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1767</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1767</id><created>2013-10-07</created><authors><author><keyname>Lazic</keyname><forenames>Ranko</forenames></author></authors><title>The reachability problem for vector addition systems with a stack is not
  elementary</title><categories>cs.FL cs.LO</categories><comments>Informal presentation, 6th International Workshop on Reachability
  Problems (RP), Bordeaux, September 2012</comments><acm-class>F.3.1; F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By adapting the iterative yardstick construction of Stockmeyer, we show that
the reachability problem for vector addition systems with a stack does not have
elementary complexity. As a corollary, the same lower bound holds for the
satisfiability problem for a two-variable first-order logic on trees in which
unbounded data may label only leaf nodes. Whether the two problems are
decidable remains an open question.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1771</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1771</id><created>2013-10-07</created><authors><author><keyname>Gridchyn</keyname><forenames>Igor</forenames></author><author><keyname>Kolmogorov</keyname><forenames>Vladimir</forenames></author></authors><title>Potts model, parametric maxflow and k-submodular functions</title><categories>cs.CV</categories><comments>Accepted to ICCV 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of minimizing the Potts energy function frequently occurs in
computer vision applications. One way to tackle this NP-hard problem was
proposed by Kovtun [19,20]. It identifies a part of an optimal solution by
running $k$ maxflow computations, where $k$ is the number of labels. The number
of &quot;labeled&quot; pixels can be significant in some applications, e.g. 50-93% in our
tests for stereo. We show how to reduce the runtime to $O(\log k)$ maxflow
computations (or one {\em parametric maxflow} computation). Furthermore, the
output of our algorithm allows to speed-up the subsequent alpha expansion for
the unlabeled part, or can be used as it is for time-critical applications.
  To derive our technique, we generalize the algorithm of Felzenszwalb et al.
[7] for {\em Tree Metrics}. We also show a connection to {\em $k$-submodular
functions} from combinatorial optimization, and discuss {\em $k$-submodular
relaxations} for general energy functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1777</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1777</id><created>2013-10-07</created><authors><author><keyname>Janson</keyname><forenames>Svante</forenames></author><author><keyname>Sorkin</keyname><forenames>Gregory B.</forenames></author></authors><title>VCG Auction Mechanism Cost Expectations and Variances</title><categories>cs.GT</categories><msc-class>60C05, 91B26, 52B40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider Vickrey-Clarke-Groves (VCG) auctions for a very general
combinatorial structure, in an average-case setting where item costs are
independent, identically distributed uniform random variables. We prove that
the expected VCG cost is at least double the expected nominal cost, and exactly
double when the desired structure is a basis of a bridgeless matroid. In the
matroid case we further show that, conditioned upon the VCG cost, the
expectation of the nominal cost is exactly half the VCG cost, and we show
several results on variances and covariances among the nominal cost, the VCG
cost, and related quantities. As an application, we find the asymptotic
variance of the VCG cost of the minimum spanning tree in a complete graph with
random edge costs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1799</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1799</id><created>2013-10-07</created><updated>2014-07-24</updated><authors><author><keyname>Kammoun</keyname><forenames>Abla</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Axel</forenames></author><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Debbah</keyname><forenames>M&#xe9;rouane</forenames></author></authors><title>Linear Precoding Based on Polynomial Expansion: Large-Scale Multi-Cell
  MIMO Systems</title><categories>cs.IT math.IT</categories><journal-ref>IEEE Journal of Selected Topics in Signal Processing, vol. 8, no.
  5, pp. 861-875, October 2014</journal-ref><doi>10.1109/JSTSP.2014.2322582</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large-scale MIMO systems can yield a substantial improvement in spectral
efficiency for future communication systems. Due to the finer spatial
resolution achieved by a huge number of antennas at the base stations, these
systems have shown to be robust to inter-user interference and the use of
linear precoding is asymptotically optimal. However, most precoding schemes
exhibit high computational complexity as the system dimensions increase. For
example, the near-optimal RZF requires the inversion of a large matrix. This
motivated our companion paper, where we proposed to solve the issue in
single-cell multi-user systems by approximating the matrix inverse by a
truncated polynomial expansion (TPE), where the polynomial coefficients are
optimized to maximize the system performance. We have shown that the proposed
TPE precoding with a small number of coefficients reaches almost the
performance of RZF but never exceeds it. In a realistic multi-cell scenario
involving large-scale multi-user MIMO systems, the optimization of RZF
precoding has thus far not been feasible. This is mainly attributed to the high
complexity of the scenario and the non-linear impact of the necessary
regularizing parameters. On the other hand, the scalar weights in TPE precoding
give hope for possible throughput optimization. Following the same methodology
as in the companion paper, we exploit random matrix theory to derive a
deterministic expression for the asymptotic SINR for each user. We also provide
an optimization algorithm to approximate the weights that maximize the
network-wide weighted max-min fairness. The optimization weights can be used to
mimic the user throughput distribution of RZF precoding. Using simulations, we
compare the network throughput of the TPE precoding with that of the suboptimal
RZF scheme and show that our scheme can achieve higher throughput using a TPE
order of only 3.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1803</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1803</id><created>2013-10-07</created><updated>2013-12-29</updated><authors><author><keyname>Scheibler</keyname><forenames>Robin</forenames></author><author><keyname>Haghighatshoar</keyname><forenames>Saeid</forenames></author><author><keyname>Vetterli</keyname><forenames>Martin</forenames></author></authors><title>A Fast Hadamard Transform for Signals with Sub-linear Sparsity in the
  Transform Domain</title><categories>cs.IT math.IT stat.ML</categories><comments>17 pages. 11 figures. A shorter version was submitted to the 51st
  Allerton Conference on Communication, Control and Computing (2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new iterative low complexity algorithm has been presented for computing the
Walsh-Hadamard transform (WHT) of an $N$ dimensional signal with a $K$-sparse
WHT, where $N$ is a power of two and $K = O(N^\alpha)$, scales sub-linearly in
$N$ for some $0 &lt; \alpha &lt; 1$. Assuming a random support model for the non-zero
transform domain components, the algorithm reconstructs the WHT of the signal
with a sample complexity $O(K \log_2(\frac{N}{K}))$, a computational complexity
$O(K\log_2(K)\log_2(\frac{N}{K}))$ and with a very high probability
asymptotically tending to 1.
  The approach is based on the subsampling (aliasing) property of the WHT,
where by a carefully designed subsampling of the time domain signal, one can
induce a suitable aliasing pattern in the transform domain. By treating the
aliasing patterns as parity-check constraints and borrowing ideas from erasure
correcting sparse-graph codes, the recovery of the non-zero spectral values has
been formulated as a belief propagation (BP) algorithm (peeling decoding) over
a sparse-graph code for the binary erasure channel (BEC). Tools from coding
theory are used to analyze the asymptotic performance of the algorithm in the
very sparse ($\alpha\in(0,\frac{1}{3}]$) and the less sparse
($\alpha\in(\frac{1}{3},1)$) regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1805</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1805</id><created>2013-10-07</created><authors><author><keyname>Pons</keyname><forenames>Viviane</forenames></author></authors><title>Combinatoire alg\'ebrique li\'ee aux ordres sur les permutations</title><categories>math.CO cs.DM</categories><comments>PhD Thesis, 214 pages, 8 chapters, 59 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This thesis comes within the scope of algebraic combinatorics and studies
problems related to three orders on permutations: the two said weak orders
(right and left) and the strong order or Bruhat order. The first part deals
with bases of multivariate polynomials. Most specifically, we study a product
of Grothendieck polynomials and prove that it can interpreted as a sum over the
Bruhat order. We also present our implementation of Grothendieck polynomials
and other bases in Sage. In a second part, we study the Tamari order binary
trees. We obtain a new enumeration formula on the Tamari lattice and a new
combinatorial prove of Chapoton's functional equation of the generating
functions of Tamari intervals. We extend our results to the m-Tamari case and
thus retrieve a formula given by Bousquet-M\'elou, Pr\'eville-Ratelle and Fusy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1806</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1806</id><created>2013-10-07</created><updated>2014-07-24</updated><authors><author><keyname>M&#xfc;ller</keyname><forenames>Axel</forenames></author><author><keyname>Kammoun</keyname><forenames>Abla</forenames></author><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Debbah</keyname><forenames>M&#xe9;rouane</forenames></author></authors><title>Linear Precoding Based on Polynomial Expansion: Reducing Complexity in
  Massive MIMO</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large-scale multi-user multiple-input multiple-output (MIMO) techniques have
the potential to bring tremendous improvements for future communication
systems. Counter-intuitively, the practical issues of having uncertain channel
knowledge, high propagation losses, and implementing optimal non-linear
precoding are solved more-or-less automatically by enlarging system dimensions.
However, the computational precoding complexity grows with the system
dimensions. For example, the close-to-optimal regularized zero-forcing (RZF)
precoding is very complicated to implement in practice, since it requires fast
inversions of large matrices in every coherence period. Motivated by the high
performance of RZF, we propose to replace the matrix inversion by a truncated
polynomial expansion (TPE), thereby obtaining the new TPE precoding scheme
which is more suitable for real-time hardware implementation. The degree of the
matrix polynomial can be adapted to the available hardware resources and
enables smooth transition between simple maximum ratio transmission (MRT) and
more advanced RZF.
  By deriving new random matrix results, we obtain a deterministic expression
for the asymptotic signal-to-interference-and-noise ratio (SINR) achieved by
TPE precoding in large-scale MIMO systems. Furthermore, we provide a
closed-form expression for the polynomial coefficients that maximizes this
SINR. To maintain a fixed per-user rate loss as compared to RZF, the polynomial
degree does not need to scale with the system, but it should be increased with
the quality of the channel knowledge and the signal-to-noise ratio (SNR).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1811</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1811</id><created>2013-10-07</created><authors><author><keyname>Alsharif</keyname><forenames>Ouais</forenames></author><author><keyname>Pineau</keyname><forenames>Joelle</forenames></author></authors><title>End-to-End Text Recognition with Hybrid HMM Maxout Models</title><categories>cs.CV</categories><comments>9 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of detecting and recognizing text in natural scenes has proved to
be more challenging than its counterpart in documents, with most of the
previous work focusing on a single part of the problem. In this work, we
propose new solutions to the character and word recognition problems and then
show how to combine these solutions in an end-to-end text-recognition system.
We do so by leveraging the recently introduced Maxout networks along with
hybrid HMM models that have proven useful for voice recognition. Using these
elements, we build a tunable and highly accurate recognition system that beats
state-of-the-art results on all the sub-problems for both the ICDAR 2003 and
SVT benchmark datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1814</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1814</id><created>2013-10-07</created><authors><author><keyname>Wang</keyname><forenames>Yunpeng</forenames></author><author><keyname>Saad</keyname><forenames>Walid</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author><author><keyname>Ba&#x15f;ar</keyname><forenames>Tamer</forenames></author></authors><title>A Game-Theoretic Approach to Energy Trading in the Smart Grid</title><categories>cs.GT</categories><comments>11 pages, 11 figures, journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Electric storage units constitute a key element in the emerging smart grid
system. In this paper, the interactions and energy trading decisions of a
number of geographically distributed storage units are studied using a novel
framework based on game theory. In particular, a noncooperative game is
formulated between storage units, such as PHEVs, or an array of batteries that
are trading their stored energy. Here, each storage unit's owner can decide on
the maximum amount of energy to sell in a local market so as to maximize a
utility that reflects the tradeoff between the revenues from energy trading and
the accompanying costs. Then in this energy exchange market between the storage
units and the smart grid elements, the price at which energy is traded is
determined via an auction mechanism. The game is shown to admit at least one
Nash equilibrium and a novel proposed algorithm that is guaranteed to reach
such an equilibrium point is proposed. Simulation results show that the
proposed approach yields significant performance improvements, in terms of the
average utility per storage unit, reaching up to 130.2% compared to a
conventional greedy approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1822</identifier>
 <datestamp>2013-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1822</id><created>2013-10-07</created><updated>2013-12-03</updated><authors><author><keyname>Ozcan</keyname><forenames>Gozde</forenames></author><author><keyname>Gursoy</keyname><forenames>M. Cenk</forenames></author><author><keyname>Gezici</keyname><forenames>Sinan</forenames></author></authors><title>Error Rate Analysis of Cognitive Radio Transmissions with Imperfect
  Channel Sensing</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the symbol error rate performance of cognitive radio
transmissions in the presence of imperfect sensing decisions. Two different
transmission schemes, namely sensing-based spectrum sharing (SSS) and
opportunistic spectrum access (OSA), are considered. In both schemes, secondary
users first perform channel sensing, albeit with possible errors. In SSS,
depending on the sensing decisions, they adapt the transmission power level and
coexist with primary users in the channel. On the other hand, in OSA, secondary
users are allowed to transmit only when the primary user activity is not
detected. Initially, for both transmission schemes, general formulations for
the optimal decision rule and error probabilities are provided for arbitrary
modulation schemes under the assumptions that the receiver is equipped with the
sensing decision and perfect knowledge of the channel fading, and the primary
user's received faded signals at the secondary receiver has a Gaussian mixture
distribution. Subsequently, the general approach is specialized to rectangular
quadrature amplitude modulation (QAM). More specifically, optimal decision rule
is characterized for rectangular QAM, and closed-form expressions for the
average symbol error probability attained with the optimal detector are derived
under both transmit power and interference constraints. The effects of
imperfect channel sensing decisions, interference from the primary user and its
Gaussian mixture model, and the transmit power and interference constraints on
the error rate performance of cognitive transmissions are analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1829</identifier>
 <datestamp>2013-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1829</id><created>2013-10-07</created><updated>2013-12-20</updated><authors><author><keyname>Sobolevsky</keyname><forenames>Stanislav</forenames></author><author><keyname>Szell</keyname><forenames>Michael</forenames></author><author><keyname>Campari</keyname><forenames>Riccardo</forenames></author><author><keyname>Couronn&#xe9;</keyname><forenames>Thomas</forenames></author><author><keyname>Smoreda</keyname><forenames>Zbigniew</forenames></author><author><keyname>Ratti</keyname><forenames>Carlo</forenames></author></authors><title>Delineating geographical regions with networks of human interactions in
  an extensive set of countries</title><categories>cs.SI physics.soc-ph</categories><comments>10 pages, 4 figures</comments><journal-ref>PLOS ONE 8(12): e81707 (2013)</journal-ref><doi>10.1371/journal.pone.0081707</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Large-scale networks of human interaction, in particular country-wide
telephone call networks, can be used to redraw geographical maps by applying
algorithms of topological community detection. The geographic projections of
the emerging areas in a few recent studies on single regions have been
suggested to share two distinct properties: first, they are cohesive, and
second, they tend to closely follow socio-economic boundaries and are similar
to existing political regions in size and number. Here we use an extended set
of countries and clustering indices to quantify overlaps, providing ample
additional evidence for these observations using phone data from countries of
various scales across Europe, Asia, and Africa: France, the UK, Italy, Belgium,
Portugal, Saudi Arabia, and Ivory Coast. In our analysis we use the known
approach of partitioning country-wide networks, and an additional iterative
partitioning of each of the first level communities into sub-communities,
revealing that cohesiveness and matching of official regions can also be
observed on a second level if spatial resolution of the data is high enough.
The method has possible policy implications on the definition of the
borderlines and sizes of administrative regions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1833</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1833</id><created>2013-10-07</created><authors><author><keyname>Havary-Nassab</keyname><forenames>Veria</forenames></author><author><keyname>Shahbazpanahi</keyname><forenames>Shahram</forenames></author><author><keyname>Valaee</keyname><forenames>Shahrokh</forenames></author></authors><title>Mobility Diversity in Mobile Wireless Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the novel concept of mobility diversity for mobile sensor or
communication networks as the diversity introduced by transmitting data over
different topologies of the network. We show how node mobility can provide
diversity by changing the topology of the network. More specifically, we
consider a mobile network of a sensor node and a number of sink nodes which are
all moving randomly according to different Wiener process mobility models.
Assuming that the network topology evolves with time and assuming that the
connectivity of the sensor node to at least one sink node is needed for
successful communication, we calculate three performance measures for this
network, i) the expected number of time instants, where the sensor node is
connected to at least one sink node, ii) the probability of outage, being the
probability that no sink node is in the vicinity of the sensor node during the
observation interval, and finally, iii) the maximum number of consequent
failures in the communication. Our theoretical and numerical analysis show that
increasing the mobility parameter of the sensor node increases the average
number of successful transmissions, decreases the probability of outage, and
reduces the maximum delay in the senor-sink communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1840</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1840</id><created>2013-10-07</created><authors><author><keyname>Fercoq</keyname><forenames>Olivier</forenames></author></authors><title>Parallel coordinate descent for the Adaboost problem</title><categories>cs.LG math.OC stat.ML</categories><comments>7 pages, 3 figures, extended version of the paper presented to
  ICMLA'13</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We design a randomised parallel version of Adaboost based on previous studies
on parallel coordinate descent. The algorithm uses the fact that the logarithm
of the exponential loss is a function with coordinate-wise Lipschitz continuous
gradient, in order to define the step lengths. We provide the proof of
convergence for this randomised Adaboost algorithm and a theoretical
parallelisation speedup factor. We finally provide numerical examples on
learning problems of various sizes that show that the algorithm is competitive
with concurrent approaches, especially for large scale problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1841</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1841</id><created>2013-10-07</created><authors><author><keyname>Bell</keyname><forenames>Jason</forenames></author><author><keyname>Brzozowski</keyname><forenames>Janusz</forenames></author><author><keyname>Moreira</keyname><forenames>Nelma</forenames></author><author><keyname>Reis</keyname><forenames>Rog&#xe9;rio</forenames></author></authors><title>Symmetric Groups and Quotient Complexity of Boolean Operations</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The quotient complexity of a regular language L is the number of left
quotients of L, which is the same as the state complexity of L. Suppose that L
and L' are binary regular languages with quotient complexities m and n, and
that the transition semigroups of the minimal deterministic automata accepting
L and L' are the symmetric groups S_m and S_n of degrees m and n, respectively.
Denote by o any binary boolean operation that is not a constant and not a
function of one argument only. For m,n &gt;= 2 with (m,n) not in
{(2,2),(3,4),(4,3),(4,4)} we prove that the quotient complexity of LoL' is mn
if and only either (a) m is not equal to n or (b) m=n and the bases (ordered
pairs of generators) of S_m and S_n are not conjugate. For (m,n)\in
{(2,2),(3,4),(4,3),(4,4)} we give examples to show that this need not hold. In
proving these results we generalize the notion of uniform minimality to direct
products of automata. We also establish a non-trivial connection between
complexity of boolean operations and group theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1845</identifier>
 <datestamp>2013-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1845</id><created>2013-10-07</created><updated>2013-10-24</updated><authors><author><keyname>Biedl</keyname><forenames>Therese</forenames></author></authors><title>On triangulating k-outerplanar graphs</title><categories>cs.DM cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A k-outerplanar graph is a graph that can be drawn in the plane without
crossing such that after k-fold removal of the vertices on the outer-face there
are no vertices left. In this paper, we study how to triangulate a
k-outerplanar graph while keeping its outerplanarity small. Specifically, we
show that not all k-outerplanar graphs can be triangulated so that the result
is k-outerplanar, but they can be triangulated so that the result is
(k+1)-outerplanar.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1847</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1847</id><created>2013-10-07</created><authors><author><keyname>Nguyen-Xuan</keyname><forenames>H.</forenames></author><author><keyname>Tran</keyname><forenames>Loc V.</forenames></author><author><keyname>Thai</keyname><forenames>Chien H.</forenames></author><author><keyname>Kulasegaram</keyname><forenames>S.</forenames></author><author><keyname>Bordas</keyname><forenames>S. P. A.</forenames></author></authors><title>Isogeometric finite element analysis of functionally graded plates using
  a refined plate theory</title><categories>cs.NA math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose in this paper a novel inverse tangent transverse shear deformation
formulation for functionally graded material (FGM) plates. The isogeometric
finite element analysis (IGA) of static, free vibration and buckling problems
of FGM plates is then addressed using a refined plate theory (RPT). The RPT
enables us to describe the non-linear distribution of shear stresses through
the plate thickness without any requirement of shear correction factors (SCF).
IGA utilizes basis functions, namely B-splines or non-uniform rational
B-splines (NURBS), which achieve easily the smoothness of any arbitrary order.
It hence satisfies the C1 requirement of the RPT model. The present method
approximates the displacement field of four degrees of freedom per each control
point and retains the computational efficiency while ensuring the high accuracy
in solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1855</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1855</id><created>2013-10-07</created><authors><author><keyname>Chen</keyname><forenames>Junzhou</forenames></author><author><keyname>You</keyname><forenames>Yong</forenames></author></authors><title>Early Fire Detection Using HEP and Space-time Analysis</title><categories>cs.CV cs.MM</categories><comments>9 pages, 13 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this article, a video base early fire alarm system is developed by
monitoring the smoke in the scene. There are two major contributions in this
work. First, to find the best texture feature for smoke detection, a general
framework, named Histograms of Equivalent Patterns (HEP), is adopted to achieve
an extensive evaluation of various kinds of texture features. Second, the
\emph{Block based Inter-Frame Difference} (BIFD) and a improved version of
LBP-TOP are proposed and ensembled to describe the space-time characteristics
of the smoke. In order to reduce the false alarms, the Smoke History Image
(SHI) is utilized to register the recent classification results of candidate
smoke blocks. Experimental results using SVM show that the proposed method can
achieve better accuracy and less false alarm compared with the state-of-the-art
technologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1857</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1857</id><created>2013-10-07</created><authors><author><keyname>Karafyllis</keyname><forenames>Iasson</forenames></author><author><keyname>Malisoff</keyname><forenames>Michael</forenames></author><author><keyname>de Queiroz</keyname><forenames>Marcio</forenames></author><author><keyname>Krstic</keyname><forenames>Miroslav</forenames></author></authors><title>Predictor-Based Tracking For Neuromuscular Electrical Stimulation</title><categories>math.OC cs.SY</categories><comments>33 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new hybrid tracking controller for neuromuscular electrical stimulation is
proposed. The control scheme uses sampled measurements and is designed by
utilizing a numerical prediction of the state variables. The tracking error of
the closed-loop system converges exponentially to zero and robustness to
perturbations of the sampling schedule is exhibited. One of the novelties of
our approach is the ability to satisfy a state constraint imposed by the
physical system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1861</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1861</id><created>2013-10-07</created><updated>2015-09-28</updated><authors><author><keyname>Dean</keyname><forenames>Thomas</forenames></author><author><keyname>Goldsmith</keyname><forenames>Andrea</forenames></author></authors><title>Physical-Layer Cryptography Through Massive MIMO</title><categories>cs.IT cs.CR math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose the new technique of physical-layer cryptography based on using a
massive MIMO channel as a key between the sender and desired receiver, which
need not be secret. The goal is for low-complexity encoding and decoding by the
desired transmitter-receiver pair, whereas decoding by an eavesdropper is hard
in terms of prohibitive complexity. The decoding complexity is analyzed by
mapping the massive MIMO system to a lattice. We show that the eavesdropper's
decoder for the MIMO system with M-PAM modulation is equivalent to solving
standard lattice problems that are conjectured to be of exponential complexity
for both classical and quantum computers. Hence, under the widely-held
conjecture that standard lattice problems are hard to solve in the worst-case,
the proposed encryption scheme has a more robust notion of security than that
of the most common encryption methods used today such as RSA and
Diffie-Hellman. Additionally, we show that this scheme could be used to
securely communicate without a pre-shared secret and little computational
overhead. Thus, by exploiting the physical layer properties of the radio
channel, the massive MIMO system provides for low-complexity encryption
commensurate with the most sophisticated forms of application-layer encryption
that are currently known.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1863</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1863</id><created>2013-10-07</created><updated>2013-10-08</updated><authors><author><keyname>Salge</keyname><forenames>Christoph</forenames></author><author><keyname>Glackin</keyname><forenames>Cornelius</forenames></author><author><keyname>Polani</keyname><forenames>Daniel</forenames></author></authors><title>Empowerment -- an Introduction</title><categories>cs.AI cs.IT math.IT nlin.AO</categories><comments>46 pages, 8 figures, to be published in Prokopenko, M., editor,
  Guided Self-Organization: Inception. Springer. In Press</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This book chapter is an introduction to and an overview of the
information-theoretic, task independent utility function &quot;Empowerment&quot;, which
is defined as the channel capacity between an agent's actions and an agent's
sensors. It quantifies how much influence and control an agent has over the
world it can perceive. This book chapter discusses the general idea behind
empowerment as an intrinsic motivation and showcases several previous
applications of empowerment to demonstrate how empowerment can be applied to
different sensor-motor configuration, and how the same formalism can lead to
different observed behaviors. Furthermore, we also present a fast approximation
for empowerment in the continuous domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1869</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1869</id><created>2013-10-07</created><authors><author><keyname>Kolev</keyname><forenames>Vasil</forenames></author><author><keyname>Tsvetkova</keyname><forenames>Katya</forenames></author><author><keyname>Tsvetkov</keyname><forenames>Milcho</forenames></author></authors><title>Singular Value Decomposition of Images from Scanned Photographic Plates</title><categories>cs.CV astro-ph.IM cs.CE</categories><comments>pages 15, Proceedings of the VII Bulgarian-Serbian Astronomical
  Conference,Bulgaria,2010</comments><journal-ref>Proceedings of the VII Bulgarian-Serbian Astronomical Conference
  (VII BSAC) Chepelare, Bulgaria, June 1-4,pp.187-200, 2010,</journal-ref><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  We want to approximate the mxn image A from scanned astronomical photographic
plates (from the Sofia Sky Archive Data Center) by using far fewer entries than
in the original matrix. By using rank of a matrix, k we remove the redundant
information or noise and use as Wiener filter, when rank k&lt;m or k&lt;n. With this
approximation more than 98% compression ration of image of astronomical plate
without that image details, is obtained. The SVD of images from scanned
photographic plates (SPP) is considered and its possible image compression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1891</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1891</id><created>2013-10-07</created><authors><author><keyname>Rudra</keyname><forenames>Atri</forenames></author><author><keyname>Wootters</keyname><forenames>Mary</forenames></author></authors><title>Every list-decodable code for high noise has abundant near-optimal rate
  puncturings</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that any q-ary code with sufficiently good distance can be randomly
punctured to obtain, with high probability, a code that is list decodable up to
radius $1 - 1/q - \epsilon$ with near-optimal rate and list sizes. Our results
imply that &quot;most&quot; Reed-Solomon codes are list decodable beyond the Johnson
bound, settling the long-standing open question of whether any Reed Solomon
codes meet this criterion.
  More precisely, we show that a Reed-Solomon code with random evaluation
points is, with high probability, list decodable up to radius $1 - \epsilon$
with list sizes $O(1/\epsilon)$ and rate $\Omega(\epsilon)$. As a second
corollary of our argument, we obtain improved bounds on the list decodability
of random linear codes over large fields.
  Our approach exploits techniques from high dimensional probability. Previous
work used similar tools to obtain bounds on the list decodability of random
linear codes, but the bounds did not scale with the size of the alphabet. In
this paper, we use a chaining argument to deal with large alphabet sizes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1896</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1896</id><created>2013-10-07</created><authors><author><keyname>Correa</keyname><forenames>Jos&#xe9; R.</forenames></author><author><keyname>Larr&#xe9;</keyname><forenames>Omar</forenames></author><author><keyname>Soto</keyname><forenames>Jos&#xe9; A.</forenames></author></authors><title>TSP Tours in Cubic Graphs: Beyond 4/3</title><categories>cs.DS cs.DM math.CO</categories><comments>23 pages. A preliminary version appeared in ESA 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  After a sequence of improvements Boyd, Sitters, van der Ster, and Stougie
proved that any 2-connected graph whose n vertices have degree 3, i.e., a cubic
2-connected graph, has a Hamiltonian tour of length at most (4/3)n,
establishing in particular that the integrality gap of the subtour LP is at
most 4/3 for cubic 2-connected graphs and matching the conjectured value of the
famous 4/3 conjecture. In this paper we improve upon this result by designing
an algorithm that finds a tour of length (4/3 - 1/61236)n, implying that cubic
2-connected graphs are among the few interesting classes of graphs for which
the integrality gap of the subtour LP is strictly less than 4/3. With the
previous result, and by considering an even smaller epsilon, we show that the
integrality gap of the TSP relaxation is at most 4/3 - epsilon, even if the
graph is not 2-connected (i.e. for cubic connected graphs), implying that the
approximability threshold of the TSP in cubic graphs is strictly below 4/3.
Finally, using similar techniques we show, as an additional result, that every
Barnette graph admits a tour of length at most (4/3 - 1/18)n.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1906</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1906</id><created>2013-10-07</created><updated>2014-01-01</updated><authors><author><keyname>Parand</keyname><forenames>K.</forenames></author><author><keyname>Hossayni</keyname><forenames>Sayyed A.</forenames></author><author><keyname>Rad</keyname><forenames>J. A.</forenames></author></authors><title>The application of the exact operational matrices for solving the
  Emden-Fowler equations, arising in astrophysics</title><categories>cs.NA math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The objective of this paper is to apply the well-known exact operational
matrices (EOMs) idea for solving the Emden-Fowler equations, illustrating the
superiority of EOMs versus ordinary operational matrices (OOMs). Up to now, a
few studies have been conducted on EOMs and the differential equations solved
by them do not have high-degree nonlinearity and the reported results are not
regarded as appropriate criteria for the excellence of the new method. So, we
chose Emden-Fowler type differential equations and solved them by this method.
To confirm the accuracy of the new method and to show the preeminence of EOMs
versus OOMs, the norm1 of the residual and error function of both methods are
evaluated for multiple $m$ values, where $m$ is the degree of the Bernstein
polynomials. We reported the results in form of plots to illustrate the error
convergence of both methods to zero and also to show the primacy of the new
method versus OOMs. The obtained results have demonstrated the increased
accuracy of the new method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1930</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1930</id><created>2013-10-07</created><updated>2014-02-11</updated><authors><author><keyname>Vlassis</keyname><forenames>Nikos</forenames></author><author><keyname>Jungers</keyname><forenames>Rapha&#xeb;l</forenames></author></authors><title>Polytopic uncertainty for linear systems: New and old complexity results</title><categories>cs.SY cs.CC math.DS</categories><comments>Fixed some typos and added some references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We survey the problem of deciding the stability or stabilizability of
uncertain linear systems whose region of uncertainty is a polytope. This
natural setting has applications in many fields of applied science, from
Control Theory to Systems Engineering to Biology. We focus on the algorithmic
decidability of this property when one is given a particular polytope. This
setting gives rise to several different algorithmic questions, depending on the
nature of time (discrete/continuous), the property asked
(stability/stabilizability), or the type of uncertainty (fixed/switching).
Several of these questions have been answered in the literature in the last
thirty years. We point out the ones that have remained open, and we answer all
of them, except one which we raise as an open question. In all the cases, the
results are negative in the sense that the questions are NP-hard. As a
byproduct, we obtain complexity results for several other matrix problems in
Systems and Control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1934</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1934</id><created>2013-10-07</created><authors><author><keyname>Karampatziakis</keyname><forenames>Nikos</forenames></author><author><keyname>Mineiro</keyname><forenames>Paul</forenames></author></authors><title>Discriminative Features via Generalized Eigenvectors</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Representing examples in a way that is compatible with the underlying
classifier can greatly enhance the performance of a learning system. In this
paper we investigate scalable techniques for inducing discriminative features
by taking advantage of simple second order structure in the data. We focus on
multiclass classification and show that features extracted from the generalized
eigenvectors of the class conditional second moments lead to classifiers with
excellent empirical performance. Moreover, these features have attractive
theoretical properties, such as inducing representations that are invariant to
linear transformations of the input. We evaluate classifiers built from these
features on three different tasks, obtaining state of the art results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1942</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1942</id><created>2013-10-07</created><authors><author><keyname>Bradonji&#x107;</keyname><forenames>Milan</forenames></author><author><keyname>Molloy</keyname><forenames>Michael</forenames></author><author><keyname>Yan</keyname><forenames>Guanhua</forenames></author></authors><title>Containing Viral Spread on Sparse Random Graphs: Bounds, Algorithms, and
  Experiments</title><categories>math.PR cs.DM cs.SI math.CO</categories><comments>28 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Viral spread on large graphs has many real-life applications such as malware
propagation in computer networks and rumor (or misinformation) spread in
Twitter-like online social networks. Although viral spread on large graphs has
been intensively analyzed on classical models such as
Susceptible-Infectious-Recovered, there still exits a deficit of effective
methods in practice to contain epidemic spread once it passes a critical
threshold. Against this backdrop, we explore methods of containing viral spread
in large networks with the focus on sparse random networks. The viral
containment strategy is to partition a large network into small components and
then to ensure the sanity of all messages delivered across different
components. With such a defense mechanism in place, an epidemic spread starting
from any node is limited to only those nodes belonging to the same component as
the initial infection node. We establish both lower and upper bounds on the
costs of inspecting inter-component messages. We further propose
heuristic-based approaches to partition large input graphs into small
components. Finally, we study the performance of our proposed algorithms under
different network topologies and different edge weight models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1947</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1947</id><created>2013-10-07</created><authors><author><keyname>Hutter</keyname><forenames>Frank</forenames></author><author><keyname>Hoos</keyname><forenames>Holger</forenames></author><author><keyname>Leyton-Brown</keyname><forenames>Kevin</forenames></author></authors><title>Bayesian Optimization With Censored Response Data</title><categories>cs.AI cs.LG stat.ML</categories><comments>Extended version of NIPS 2011 workshop paper</comments><acm-class>G.3; G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian optimization (BO) aims to minimize a given blackbox function using a
model that is updated whenever new evidence about the function becomes
available. Here, we address the problem of BO under partially right-censored
response data, where in some evaluations we only obtain a lower bound on the
function value. The ability to handle such response data allows us to
adaptively censor costly function evaluations in minimization problems where
the cost of a function evaluation corresponds to the function value. One
important application giving rise to such censored data is the
runtime-minimizing variant of the algorithm configuration problem: finding
settings of a given parametric algorithm that minimize the runtime required for
solving problem instances from a given distribution. We demonstrate that
terminating slow algorithm runs prematurely and handling the resulting
right-censored observations can substantially improve the state of the art in
model-based algorithm configuration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1949</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1949</id><created>2013-10-07</created><updated>2013-10-21</updated><authors><author><keyname>Agarwal</keyname><forenames>Alekh</forenames></author><author><keyname>Kakade</keyname><forenames>Sham M.</forenames></author><author><keyname>Karampatziakis</keyname><forenames>Nikos</forenames></author><author><keyname>Song</keyname><forenames>Le</forenames></author><author><keyname>Valiant</keyname><forenames>Gregory</forenames></author></authors><title>Least Squares Revisited: Scalable Approaches for Multi-class Prediction</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work provides simple algorithms for multi-class (and multi-label)
prediction in settings where both the number of examples n and the data
dimension d are relatively large. These robust and parameter free algorithms
are essentially iterative least-squares updates and very versatile both in
theory and in practice. On the theoretical front, we present several variants
with convergence guarantees. Owing to their effective use of second-order
structure, these algorithms are substantially better than first-order methods
in many practical scenarios. On the empirical side, we present a scalable
stagewise variant of our approach, which achieves dramatic computational
speedups over popular optimization packages such as Liblinear and Vowpal Wabbit
on standard datasets (MNIST and CIFAR-10), while attaining state-of-the-art
accuracies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1953</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1953</id><created>2013-10-07</created><authors><author><keyname>Tria</keyname><forenames>F.</forenames></author><author><keyname>Loreto</keyname><forenames>V.</forenames></author><author><keyname>Servedio</keyname><forenames>V. D. P.</forenames></author><author><keyname>Strogatz</keyname><forenames>S. H.</forenames></author></authors><title>The dynamics of correlated novelties</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One new thing often leads to another. Such correlated novelties are a
familiar part of daily life. They are also thought to be fundamental to the
evolution of biological systems, human society, and technology. By opening new
possibilities, one novelty can pave the way for others in a process that
Kauffman has called &quot;expanding the adjacent possible&quot;. The dynamics of
correlated novelties, however, have yet to be quantified empirically or modeled
mathematically. Here we propose a simple mathematical model that mimics the
process of exploring a physical, biological or conceptual space that enlarges
whenever a novelty occurs. The model, a generalization of Polya's urn, predicts
statistical laws for the rate at which novelties happen (analogous to Heaps'
law) and for the probability distribution on the space explored (analogous to
Zipf's law), as well as signatures of the hypothesized process by which one
novelty sets the stage for another. We test these predictions on four data sets
of human activity: the edit events of Wikipedia pages, the emergence of tags in
annotation systems, the sequence of words in texts, and listening to new songs
in online music catalogues. By quantifying the dynamics of correlated
novelties, our results provide a starting point for a deeper understanding of
the ever-expanding adjacent possible and its role in biological, linguistic,
cultural, and technological evolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1964</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1964</id><created>2013-10-07</created><authors><author><keyname>Cecchini</keyname><forenames>Flavio Massimiliano</forenames><affiliation>Universit&#xe0; degli Studi di Milano</affiliation></author><author><keyname>Fersini</keyname><forenames>Elisabetta</forenames><affiliation>Universiy of Milano-Bicocca</affiliation></author></authors><title>Named entity recognition using conditional random fields with non-local
  relational constraints</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We begin by introducing the Computer Science branch of Natural Language
Processing, then narrowing the attention on its subbranch of Information
Extraction and particularly on Named Entity Recognition, discussing briefly its
main methodological approaches. It follows an introduction to state-of-the-art
Conditional Random Fields under the form of linear chains. Subsequently, the
idea of constrained inference as a way to model long-distance relationships in
a text is presented, based on an Integer Linear Programming representation of
the problem. Adding such relationships to the problem as automatically inferred
logical formulas, translatable into linear conditions, we propose to solve the
resulting more complex problem with the aid of Lagrangian relaxation, of which
some technical details are explained. Lastly, we give some experimental
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1968</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1968</id><created>2013-10-07</created><authors><author><keyname>Cormode</keyname><forenames>Graham</forenames></author><author><keyname>Muthukrishnan</keyname><forenames>S.</forenames></author><author><keyname>Yan</keyname><forenames>Jinyun</forenames></author></authors><title>First Author Advantage: Citation Labeling in Research</title><categories>cs.DL</categories><comments>Computational Scientometrics: Theory and Applications at The 22nd
  CIKM 2013</comments><doi>10.1145/2508497.2508500</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Citations among research papers, and the networks they form, are the primary
object of study in scientometrics. The act of making a citation reflects the
citer's knowledge of the related literature, and of the work being cited. We
aim to gain insight into this process by studying citation keys: user-chosen
labels to identify a cited work. Our main observation is that the first listed
author is disproportionately represented in such labels, implying a strong
mental bias towards the first author.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1970</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1970</id><created>2013-10-07</created><authors><author><keyname>Boche</keyname><forenames>Holger</forenames></author><author><keyname>Noetzel</keyname><forenames>Janis</forenames></author></authors><title>The Classical-Quantum Multiple Access Channel with Conferencing Encoders
  and with Common Messages</title><categories>quant-ph cs.IT math-ph math.IT math.MP</categories><comments>17 pages, no figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove coding theorems for two scenarios of cooperating encoders for the
multiple access channel with two classical inputs and one quantum output. In
the first scenario (ccq-MAC with common messages), the two senders each have
their private messages, but would also like to transmit common messages. In the
second scenario (ccq-MAC with conferencing encoders), each sender has its own
set of messages, but they are allowed to use a limited amount of noiseless
classical communication amongst each other prior to encoding their messages.
This conferencing protocol may depend on each individual message they intend to
send. The two scenarios are related to each other not only in spirit - the
existence of near-optimal codes for the ccq-MAC with common messages is used
for proving the existence of near-optimal codes for the ccq-MAC with
conferencing encoders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1971</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1971</id><created>2013-10-07</created><updated>2014-02-04</updated><authors><author><keyname>Gillet</keyname><forenames>Frederic</forenames></author></authors><title>Solving 3-SAT and 3-dimensional matching in polynomial time</title><categories>cs.DS cs.CC</categories><comments>The proposed method does not work. Updated the article with an
  analysis of why the general method suggested cannot work</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how the implementation of conservative logic gates on flow networks
suggests a way to solve 3SAT and 3-dimensional matching problems in polynomial
time by using standard minimum-cost flow methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1975</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1975</id><created>2013-10-07</created><authors><author><keyname>O'Connor</keyname><forenames>Brendan</forenames></author><author><keyname>Heilman</keyname><forenames>Michael</forenames></author></authors><title>ARKref: a rule-based coreference resolution system</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  ARKref is a tool for noun phrase coreference. It is a deterministic,
rule-based system that uses syntactic information from a constituent parser,
and semantic information from an entity recognition component. Its architecture
is based on the work of Haghighi and Klein (2009). ARKref was originally
written in 2009. At the time of writing, the last released version was in March
2011. This document describes that version, which is open-source and publicly
available at: http://www.ark.cs.cmu.edu/ARKref
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.1976</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.1976</id><created>2013-10-07</created><authors><author><keyname>Donalek</keyname><forenames>Ciro</forenames></author><author><keyname>A.</keyname><forenames>Arun Kumar</forenames></author><author><keyname>Djorgovski</keyname><forenames>S. G.</forenames></author><author><keyname>Mahabal</keyname><forenames>Ashish A.</forenames></author><author><keyname>Graham</keyname><forenames>Matthew J.</forenames></author><author><keyname>Fuchs</keyname><forenames>Thomas J.</forenames></author><author><keyname>Turmon</keyname><forenames>Michael J.</forenames></author><author><keyname>Philip</keyname><forenames>N. Sajeeth</forenames></author><author><keyname>Yang</keyname><forenames>Michael Ting-Chang</forenames></author><author><keyname>Longo</keyname><forenames>Giuseppe</forenames></author></authors><title>Feature Selection Strategies for Classifying High Dimensional
  Astronomical Data Sets</title><categories>astro-ph.IM cs.CV</categories><comments>7 pages, to appear in refereed proceedings of Scalable Machine
  Learning: Theory and Applications, IEEE BigData 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The amount of collected data in many scientific fields is increasing, all of
them requiring a common task: extract knowledge from massive, multi parametric
data sets, as rapidly and efficiently possible. This is especially true in
astronomy where synoptic sky surveys are enabling new research frontiers in the
time domain astronomy and posing several new object classification challenges
in multi dimensional spaces; given the high number of parameters available for
each object, feature selection is quickly becoming a crucial task in analyzing
astronomical data sets. Using data sets extracted from the ongoing Catalina
Real-Time Transient Surveys (CRTS) and the Kepler Mission we illustrate a
variety of feature selection strategies used to identify the subsets that give
the most information and the results achieved applying these techniques to
three major astronomical problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2001</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2001</id><created>2013-10-08</created><authors><author><keyname>Nomura</keyname><forenames>Ryo</forenames></author></authors><title>Overflow Probability of Variable-length Codes with Codeword Cost</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lossless variable-length source coding with codeword cost is considered for
general sources. The problem setting, where we impose on unequal costs on code
symbols, is called the variable-length coding with codeword cost. In this
problem, the infimum of average codeword cost have been determined for general
sources. On the other hand, overflow probability, which is defined as the
probability of codeword cost being above a threshold, have not been considered
yet. In this paper, we determine the infimum of achievable threshold in the
first-order sense and the second-order sense for general sources and compute it
for some special sources such as i.i.d. sources and mixed sources. A
relationship between the overflow probability of variable-length coding and the
error probability of fixed-length coding is also revealed. Our analysis is
based on the information-spectrum methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2017</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2017</id><created>2013-10-08</created><authors><author><keyname>Benjamini</keyname><forenames>Itai</forenames></author><author><keyname>Cohen</keyname><forenames>Gil</forenames></author><author><keyname>Shinkar</keyname><forenames>Igor</forenames></author></authors><title>Bi-Lipschitz Bijection between the Boolean Cube and the Hamming Ball</title><categories>math.CO cs.CC math.MG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct a bi-Lipschitz bijection from the Boolean cube to the Hamming
ball of equal volume. More precisely, we show that for all even n there exists
an explicit bijection f from the n-dimensional Boolean cube to the Hamming ball
of equal volume embedded in (n+1)-dimensional Boolean cube, such that for all x
and y it holds that distance(x,y) / 5 &lt;= distance(f(x),f(y)) &lt;= 4 distance(x,y)
where distance(,) denotes the Hamming distance. In particular, this implies
that the Hamming ball is bi-Lipschitz transitive.
  This result gives a strong negative answer to an open problem of Lovett and
Viola [CC 2012], who raised the question in the context of sampling
distributions in low-level complexity classes. The conceptual implication is
that the problem of proving lower bounds in the context of sampling
distributions will require some new ideas beyond the sensitivity-based
structural results of Boppana [IPL 97].
  We study the mapping f further and show that it (and its inverse) are
computable in DLOGTIME-uniform TC0, but not in AC0. Moreover, we prove that f
is &quot;approximately local&quot; in the sense that all but the last output bit of f are
essentially determined by a single input bit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2026</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2026</id><created>2013-10-08</created><updated>2015-09-12</updated><authors><author><keyname>Venkataramanan</keyname><forenames>Ramji</forenames></author><author><keyname>Swamy</keyname><forenames>Vasuki Narasimha</forenames></author><author><keyname>Ramchandran</keyname><forenames>Kannan</forenames></author></authors><title>Low-Complexity Interactive Algorithms for Synchronization from
  Deletions, Insertions, and Substitutions</title><categories>cs.IT cs.DS math.IT</categories><journal-ref>IEEE Transactions on Information Theory, vol. 61, no. 10, pp.
  5670-5689, October 2015</journal-ref><doi>10.1109/TIT.2015.2466635</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider two remote nodes having binary sequences $X$ and $Y$, respectively.
$Y$ is an edited version of ${X}$, where the editing involves random deletions,
insertions, and substitutions, possibly in bursts. The goal is for the node
with $Y$ to reconstruct $X$ with minimal exchange of information over a
noiseless link. The communication is measured in terms of both the total number
of bits exchanged and the number of interactive rounds of communication.
  This paper focuses on the setting where the number of edits is
$o(\tfrac{n}{\log n})$, where $n$ is the length of $X$. We first consider the
case where the edits are a mixture of insertions and deletions (indels), and
propose an interactive synchronization algorithm with near-optimal
communication rate and average computational complexity of $O(n)$ arithmetic
operations. The algorithm uses interaction to efficiently split the source
sequence into substrings containing exactly one deletion or insertion. Each of
these substrings is then synchronized using an optimal one-way synchronization
code based on the single-deletion correcting channel codes of Varshamov and
Tenengolts (VT codes).
  We then build on this synchronization algorithm in three different ways.
First, it is modified to work with a single round of interaction. The reduction
in the number of rounds comes at the expense of higher communication, which is
quantified. Next, we present an extension to the practically important case
where the insertions and deletions may occur in (potentially large) bursts.
Finally, we show how to synchronize the sources to within a target Hamming
distance. This feature can be used to differentiate between substitution and
indel edits. In addition to theoretical performance bounds, we provide several
validating simulation results for the proposed algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2028</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2028</id><created>2013-10-08</created><authors><author><keyname>Yang</keyname><forenames>Hyun Jong</forenames></author><author><keyname>Jung</keyname><forenames>Bang Chul</forenames></author><author><keyname>Shin</keyname><forenames>Won-Yong</forenames></author><author><keyname>Paulraj</keyname><forenames>Arogyaswami</forenames></author></authors><title>Codebook-Based Opportunistic Interference Alignment</title><categories>cs.IT math.IT</categories><comments>23 pages, 6 figures, Submitted to IEEE Transactions on Signal
  Processing (The material in this paper was presented in part at the IEEE
  International Symposium on Information Theory, Cambridge, MA, July 2012.)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Opportunistic interference alignment (OIA) asymptotically achieves the
optimal degrees-of-freedom (DoF) in interfering multiple-access channels
(IMACs) in a distributed fashion, as a certain user scaling condition is
satisfied. For the multiple-input multiple-output IMAC, it was shown that the
singular value decomposition (SVD)-based beamforming at the users fundamentally
reduces the user scaling condition required to achieve any target DoF compared
to that for the single-inputmultiple-output IMAC. In this paper, we tackle two
practical challenges of the existing SVD-based OIA: 1) the need of full
feedforward of the selected users' beamforming weight vectors and 2) a low rate
achieved based on the exiting zero-forcing (ZF) receiver. We first propose a
codebook-based OIA, in which the weight vectors are chosen from a pre-defined
codebook with a finite size so that information of the weight vectors can be
sent to the belonging BS with limited feedforward. We derive the codebook size
required to achieve the same user scaling condition as the SVD-based OIA case
for both Grassmannian and random codebooks. Surprisingly, it is shown that the
derived codebook size is the same for the two considered codebook approaches.
Second, we take into account an enhanced receiver at the base stations (BSs) in
pursuit of improving the achievable rate based on the ZF receiver. Assuming no
collaboration between the BSs, the interfering links between a BS and the
selected users in neighboring cells are difficult to be acquired at the
belonging BS. We propose the use of a simple minimum Euclidean distance
receiver operating with no information of the interfering links. With the help
of the OIA, we show that this new receiver asymptotically achieves the channel
capacity as the number of users increases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2031</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2031</id><created>2013-10-08</created><updated>2013-10-10</updated><authors><author><keyname>Birkedal</keyname><forenames>Lars</forenames><affiliation>IT University of Copenhagen</affiliation></author><author><keyname>Bizjak</keyname><forenames>Ale&#x161;</forenames><affiliation>Aarhus University, Denmark</affiliation></author><author><keyname>Schwinghammer</keyname><forenames>Jan</forenames><affiliation>Saarland University</affiliation></author></authors><title>Step-Indexed Relational Reasoning for Countable Nondeterminism</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 4 (October
  15, 2013) lmcs:940</journal-ref><doi>10.2168/LMCS-9(4:4)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Programming languages with countable nondeterministic choice are
computationally interesting since countable nondeterminism arises when modeling
fairness for concurrent systems. Because countable choice introduces
non-continuous behaviour, it is well-known that developing semantic models for
programming languages with countable nondeterminism is challenging. We present
a step-indexed logical relations model of a higher-order functional programming
language with countable nondeterminism and demonstrate how it can be used to
reason about contextually defined may- and must-equivalence. In earlier
step-indexed models, the indices have been drawn from {\omega}. Here the
step-indexed relations for must-equivalence are indexed over an ordinal greater
than {\omega}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2037</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2037</id><created>2013-10-08</created><authors><author><keyname>Shiwen</keyname><forenames>He</forenames></author><author><keyname>Yongming</keyname><forenames>Huang</forenames></author><author><keyname>Shi</keyname><forenames>Jin</forenames></author><author><keyname>Luxi</keyname><forenames>Yang</forenames></author></authors><title>Coordinated Beamforming for Energy Efficient Transmission in Multicell
  Multiuser Systems</title><categories>cs.IT math.IT</categories><comments>25 pages, 9 figures, submitted to IEEE Transaction on Communications,
  May, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study energy efficient joint power allocation and
beamforming for coordinated multicell multiuser downlink systems. The
considered optimization problem is in a non-convex fractional form and hard to
tackle. We propose to first transform the original problem into an equivalent
optimization problem in a parametric subtractive form, by which we reach its
solution through a two-layer optimization scheme. The outer layer only involves
one-dimension search for the energy efficiency parameter which can be addressed
using the bi-section search, the key issue lies in the inner layer where a
non-fractional sub-problem needs to tackle. By exploiting the relationship
between the user rate and the mean square error, we then develop an iterative
algorithm to solve it. The convergence of this algorithm is proved and the
solution is further derived in closed-form. Our analysis also shows that the
proposed algorithm can be implemented in parallel with reasonable complexity.
Numerical results illustrate that our algorithm has a fast convergence and
achieves near-optimal energy efficiency. It is also observed that at the low
transmit power region, our solution almost achieves the optimal sum rate and
the optimal energy efficiency simultaneously; while at the middle-high transmit
power region, a certain sum rate loss is suffered in order to guarantee the
energy efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2040</identifier>
 <datestamp>2014-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2040</id><created>2013-10-08</created><authors><author><keyname>Van Meter</keyname><forenames>Rodney</forenames></author></authors><title>Quantum Computing's Classical Problem, Classical Computing's Quantum
  Problem</title><categories>quant-ph cs.ET</categories><comments>Post-proceedings of Quantum Horizons, Taipei, Oct. 2012. 10 pages, no
  figures</comments><journal-ref>Foundations of Physics 44(8), 819-828, Aug. 2014</journal-ref><doi>10.1007/s10701-014-9807-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tasked with the challenge to build better and better computers, quantum
computing and classical computing face the same conundrum: the success of
classical computing systems. Small quantum computing systems have been
demonstrated, and intermediate-scale systems are on the horizon, capable of
calculating numeric results or simulating physical systems far beyond what
humans can do by hand. However, to be commercially viable, they must surpass
what our wildly successful, highly advanced classical computers can already do.
At the same time, those classical computers continue to advance, but those
advances are now constrained by thermodynamics, and will soon be limited by the
discrete nature of atomic matter and ultimately quantum effects. Technological
advances benefit both quantum and classical machinery, altering the competitive
landscape. Can we build quantum computing systems that out-compute classical
systems capable of some $10^{30}$ logic gates per month? This article will
discuss the interplay in these competing and cooperating technological trends.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2045</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2045</id><created>2013-10-08</created><authors><author><keyname>Johnson</keyname><forenames>Oliver</forenames></author></authors><title>A de Bruijn identity for symmetric stable laws</title><categories>cs.IT math.IT math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how some attractive information--theoretic properties of Gaussians
pass over to more general families of stable densities. We define a new score
function for symmetric stable laws, and use it to give a stable version of the
heat equation. Using this, we derive a version of the de Bruijn identity,
allowing us to write the derivative of relative entropy as an inner product of
score functions. We discuss maximum entropy properties of symmetric stable
densities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2049</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2049</id><created>2013-10-08</created><authors><author><keyname>Huang</keyname><forenames>Sheng-Jun</forenames></author><author><keyname>Zhou</keyname><forenames>Zhi-Hua</forenames></author></authors><title>Fast Multi-Instance Multi-Label Learning</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many real-world tasks, particularly those involving data objects with
complicated semantics such as images and texts, one object can be represented
by multiple instances and simultaneously be associated with multiple labels.
Such tasks can be formulated as multi-instance multi-label learning (MIML)
problems, and have been extensively studied during the past few years. Existing
MIML approaches have been found useful in many applications; however, most of
them can only handle moderate-sized data. To efficiently handle large data
sets, in this paper we propose the MIMLfast approach, which first constructs a
low-dimensional subspace shared by all labels, and then trains label specific
linear models to optimize approximated ranking loss via stochastic gradient
descent. Although the MIML problem is complicated, MIMLfast is able to achieve
excellent performance by exploiting label relations with shared space and
discovering sub-concepts for complicated labels. Experiments show that the
performance of MIMLfast is highly competitive to state-of-the-art techniques,
whereas its time cost is much less; particularly, on a data set with 20K bags
and 180K instances, MIMLfast is more than 100 times faster than existing MIML
approaches. On a larger data set where none of existing approaches can return
results in 24 hours, MIMLfast takes only 12 minutes. Moreover, our approach is
able to identify the most representative instance for each label, and thus
providing a chance to understand the relation between input patterns and output
label semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2050</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2050</id><created>2013-10-08</created><authors><author><keyname>Berger</keyname><forenames>Kai</forenames></author></authors><title>A State Of the Art Report on Research in Multiple RGB-D sensor Setups</title><categories>cs.CV</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  That the Microsoft Kinect, an RGB-D sensor, transformed the gaming and end
consumer sector has been anticipated by the developers. That it also impacted
in rigorous computer vision research has probably been a surprise to the whole
community. Shortly before the commercial deployment of its successor, Kinect
One, the research literature fills with resumees and state-of-the art papers to
summarize the development over the past 3 years. This particular report
describes significant research projects which have built on sensoring setups
that include two or more RGB-D sensors in one scene.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2051</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2051</id><created>2013-10-08</created><authors><author><keyname>Liu</keyname><forenames>Yi</forenames></author><author><keyname>Xia</keyname><forenames>Xiang-Gen</forenames></author><author><keyname>Zhang</keyname><forenames>Hailin</forenames></author></authors><title>Distributed Space-Time Coding for Full-Duplex Asynchronous Cooperative
  Communications</title><categories>cs.IT math.IT</categories><comments>9 pages, 7 figures</comments><journal-ref>IEEE transactions on wireless communications, Vol. 11, No. 7, July
  2012</journal-ref><doi>10.1109/TWC.2012.060212.112214</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose two distributed linear convolutional space-time
coding (DLC-STC) schemes for full-duplex (FD) asynchronous cooperative
communications. The DLC-STC Scheme 1 is for the case of the complete loop
channel cancellation, which achieves the full asynchronous cooperative
diversity. The DLC-STC Scheme 2 is for the case of the partial loop channel
cancellation and amplifying, where some loop signals are used as the
self-coding instead of treated as interference to be directly cancelled. We
show this scheme can achieve full asynchronous cooperative diversity. We then
evaluate the performance of the two schemes when loop channel information is
not accurate and present an amplifying factor control method for the DLC-STC
Scheme 2 to improve its performance with inaccurate loop channel information.
Simulation results show that the DLC-STC Scheme 1 outperforms the DLC-STC
Scheme 2 and the delay diversity scheme if perfect or high quality loop channel
information is available at the relay, while the DLC-STC Scheme 2 achieves
better performance if the loop channel information is imperfect.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2053</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2053</id><created>2013-10-08</created><authors><author><keyname>Berger</keyname><forenames>Kai</forenames></author></authors><title>The role of RGB-D benchmark datasets: an overview</title><categories>cs.CV</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The advent of the Microsoft Kinect three years ago stimulated not only the
computer vision community for new algorithms and setups to tackle well-known
problems in the community but also sparked the launch of several new benchmark
datasets to which future algorithms can be compared 019 to. This review of the
literature and industry developments concludes that the current RGB-D benchmark
datasets can be useful to determine the accuracy of a variety of applications
of a single or multiple RGB-D sensors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2055</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2055</id><created>2013-10-08</created><authors><author><keyname>Liu</keyname><forenames>Yi</forenames></author><author><keyname>Xia</keyname><forenames>Xiang-Gen</forenames></author><author><keyname>Zhang</keyname><forenames>Hailin</forenames></author></authors><title>Distributed Linear Convolutional Space-Time Coding for Two-Relay
  Full-Duplex Asynchronous Cooperative Networks</title><categories>cs.IT math.IT</categories><comments>11 pages, 7 figures, accepted by IEEE transactions on wireless
  communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a two-relay full-duplex asynchronous cooperative network with
the amplify-and-forward (AF) protocol is considered. We propose two distributed
space-time coding schemes for the cases with and without cross-talks,
respectively. In the first case, each relay can receive the signal sent by the
other through the cross-talk link. We first study the feasibility of cross-talk
cancellation in this network and show that the cross-talk interference cannot
be removed well. For this reason, we design space-time codes by utilizing the
cross-talk signals instead of removing them. In the other case, the self-coding
is realized individually through the loop channel at each relay node and the
signals from the two relay nodes form a space-time code. The achievable
cooperative diversity of both cases is investigated and the conditions to
achieve full cooperative diversity are presented. Simulation results verify the
theoretical analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2059</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2059</id><created>2013-10-08</created><authors><author><keyname>Richt&#xe1;rik</keyname><forenames>Peter</forenames></author><author><keyname>Tak&#xe1;&#x10d;</keyname><forenames>Martin</forenames></author></authors><title>Distributed Coordinate Descent Method for Learning with Big Data</title><categories>stat.ML cs.DC cs.LG math.OC</categories><comments>11 two-column pages, 1 algorithm, 4 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we develop and analyze Hydra: HYbriD cooRdinAte descent method
for solving loss minimization problems with big data. We initially partition
the coordinates (features) and assign each partition to a different node of a
cluster. At every iteration, each node picks a random subset of the coordinates
from those it owns, independently from the other computers, and in parallel
computes and applies updates to the selected coordinates based on a simple
closed-form formula. We give bounds on the number of iterations sufficient to
approximately solve the problem with high probability, and show how it depends
on the data and on the partitioning. We perform numerical experiments with a
LASSO instance described by a 3TB matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2063</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2063</id><created>2013-10-08</created><updated>2014-05-25</updated><authors><author><keyname>van Hateren</keyname><forenames>J. H.</forenames></author></authors><title>Active causation and the origin of meaning</title><categories>q-bio.PE cs.NE nlin.AO q-bio.NC</categories><comments>revised and extended</comments><journal-ref>Biological Cybernetics 109, 33-46 (2015)</journal-ref><doi>10.1007/s00422-014-0622-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Purpose and meaning are necessary concepts for understanding mind and
culture, but appear to be absent from the physical world and are not part of
the explanatory framework of the natural sciences. Understanding how meaning
(in the broad sense of the term) could arise from a physical world has proven
to be a tough problem. The basic scheme of Darwinian evolution produces
adaptations that only represent apparent (&quot;as if&quot;) goals and meaning. Here I
use evolutionary models to show that a slight, evolvable extension of the basic
scheme is sufficient to produce genuine goals. The extension, targeted
modulation of mutation rate, is known to be generally present in biological
cells, and gives rise to two phenomena that are absent from the non-living
world: intrinsic meaning and the ability to initiate goal-directed chains of
causation (active causation). The extended scheme accomplishes this by
utilizing randomness modulated by a feedback loop that is itself regulated by
evolutionary pressure. The mechanism can be extended to behavioural variability
as well, and thus shows how freedom of behaviour is possible. A further
extension to communication suggests that the active exchange of intrinsic
meaning between organisms may be the origin of consciousness, which in
combination with active causation can provide a physical basis for the
phenomenon of free will.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2066</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2066</id><created>2013-10-08</created><authors><author><keyname>Kumar</keyname><forenames>Vinay</forenames></author><author><keyname>Thareja</keyname><forenames>Reema</forenames></author></authors><title>A Simplified Approach for Quality Management in Data Warehouse</title><categories>cs.DB cs.CY</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data warehousing is continuously gaining importance as organizations are
realizing the benefits of decision oriented data bases. However, the stumbling
block to this rapid development is data quality issues at various stages of
data warehousing. Quality can be defined as a measure of excellence or a state
free from defects. Users appreciate quality products and available literature
suggests that many organization`s have significant data quality problems that
have substantial social and economic impacts. A metadata based quality system
is introduced to manage quality of data in data warehouse. The approach is used
to analyze the quality of data warehouse system by checking the expected value
of quality parameters with that of actual values. The proposed approach is
supported with a metadata framework that can store additional information to
analyze the quality parameters, whenever required.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2071</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2071</id><created>2013-10-08</created><authors><author><keyname>Adhatrao</keyname><forenames>Kalpesh</forenames></author><author><keyname>Gaykar</keyname><forenames>Aditya</forenames></author><author><keyname>Dhawan</keyname><forenames>Amiraj</forenames></author><author><keyname>Jha</keyname><forenames>Rohit</forenames></author><author><keyname>Honrao</keyname><forenames>Vipul</forenames></author></authors><title>Predicting Students' Performance Using ID3 And C4.5 Classification
  Algorithms</title><categories>cs.CY cs.LG</categories><doi>10.5121/ijdkp.2013.3504</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An educational institution needs to have an approximate prior knowledge of
enrolled students to predict their performance in future academics. This helps
them to identify promising students and also provides them an opportunity to
pay attention to and improve those who would probably get lower grades. As a
solution, we have developed a system which can predict the performance of
students from their previous performances using concepts of data mining
techniques under Classification. We have analyzed the data set containing
information about students, such as gender, marks scored in the board
examinations of classes X and XII, marks and rank in entrance examinations and
results in first year of the previous batch of students. By applying the ID3
(Iterative Dichotomiser 3) and C4.5 classification algorithms on this data, we
have predicted the general and individual performance of freshly admitted
students in future examinations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2079</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2079</id><created>2013-10-08</created><authors><author><keyname>Dabbes</keyname><forenames>Ajayeb Abu</forenames></author><author><keyname>Kharbat</keyname><forenames>Faten</forenames></author></authors><title>Mining The Relationship Between Demographic Variables And Brand
  Associations</title><categories>cs.CY cs.DB</categories><comments>10 pages, 2 tables, 1 figure, Journal paper</comments><journal-ref>Abu Daabes, A. and Kharbat F., (2013), Mining the Relationship
  Between Demographic Variables and Brand Associations, The International
  Journal of Managing Value and Supply Chains, vol 4, no 3</journal-ref><doi>10.5121/ijmvsc.2013.4301</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This research aims to mine the relationship between demographic variables and
brand associations, and study the relative importance of these variables. The
study is conducted on fast-food restaurant brands chains in Jordan. The result
ranks and evaluates the demographic variables in relation with the brand
associations for the selected sample. Discovering brand associations according
to demographic variables reveals many facts and linkages in the context of
Jordanian culture. Suggestions are given accordingly for marketers to benefits
from to build their strategies and direct their decisions. Also, data mining
technique used in this study reflects a new trend for studying and analyzing
marketing samples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2081</identifier>
 <datestamp>2015-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2081</id><created>2013-10-08</created><updated>2014-09-06</updated><authors><author><keyname>Rueda</keyname><forenames>Sonia L.</forenames></author></authors><title>Differential elimination by differential specialization of Sylvester
  style matrices</title><categories>math.AP cs.SC</categories><msc-class>34G10, 34L99</msc-class><journal-ref>Rueda, S.L. Differential elimination by differential
  specialization of Sylvester style matrices. Advances in Applied Mathematics
  72 (2016), 4-37</journal-ref><doi>10.1016/j.aam.2015.07.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Differential resultant formulas are defined, for a system $\mathcal{P}$ of
$n$ ordinary Laurent differential polynomials in $n-1$ differential variables.
These are determinants of coefficient matrices of an extended system of
polynomials obtained from $\mathcal{P}$ through derivations and multiplications
by Laurent monomials. To start, through derivations, a system $ps(\mathcal{P})$
of $L$ polynomials in $L-1$ algebraic variables is obtained, which is non
sparse in the order of derivation. This enables the use of existing formulas
for the computation of algebraic resultants, of the multivariate sparse
algebraic polynomials in $ps(\mathcal{P})$, to obtain polynomials in the
differential elimination ideal generated by $\mathcal{P}$. The formulas
obtained are multiples of the sparse differential resultant defined by Li, Yuan
and Gao, and provide order and degree bounds in terms of mixed volumes in the
generic case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2085</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2085</id><created>2013-10-08</created><authors><author><keyname>Welk</keyname><forenames>Martin</forenames></author></authors><title>A Robust Variational Model for Positive Image Deconvolution</title><categories>cs.CV</categories><acm-class>I.4.3; I.4.4; G.1.9</acm-class><doi>10.1007/s11760-015-0750-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, an iterative method for robust deconvolution with positivity
constraints is discussed. It is based on the known variational interpretation
of the Richardson-Lucy iterative deconvolution as fixed-point iteration for the
minimisation of an information divergence functional under a multiplicative
perturbation model. The asymmetric penaliser function involved in this
functional is then modified into a robust penaliser, and complemented with a
regulariser. The resulting functional gives rise to a fixed point iteration
that we call robust and regularised Richardson-Lucy deconvolution. It achieves
an image restoration quality comparable to state-of-the-art robust variational
deconvolution with a computational efficiency similar to that of the original
Richardson-Lucy method. Experiments on synthetic and real-world image data
demonstrate the performance of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2086</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2086</id><created>2013-10-08</created><authors><author><keyname>Ma</keyname><forenames>Yuanyuan</forenames></author><author><keyname>Fretheim</keyname><forenames>Harald</forenames></author><author><keyname>Persson</keyname><forenames>Erik</forenames></author><author><keyname>Haugen</keyname><forenames>Trond</forenames></author></authors><title>An Iterative Method Applied to Correct the Actual Compressor Performance
  to the Equivalent Performance under the Specified Reference Conditions</title><categories>cs.SY</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  This paper proposes a correction method, which corrects the actual compressor
performance in real operating conditions to the equivalent performance under
specified reference condition. The purpose is to make fair comparisons between
actual performance against design performance or reference maps under the same
operating conditions. Then the abnormal operating conditions or early failure
indications can be identified through condition monitoring, which helps to
avoid mandatory shutdown and reduces maintenance costs. The corrections are
based on an iterative scheme, which simultaneously correct the main performance
parameters known as the polytropic head, the gas power, and the polytropic
efficiency. The excellent performance of the method is demonstrated by
performing the corrections over real industrial measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2089</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2089</id><created>2013-10-08</created><authors><author><keyname>Emdadi</keyname><forenames>Habib</forenames></author><author><keyname>Yazdanian</keyname><forenames>Mahsa</forenames></author><author><keyname>Ettefagh</keyname><forenames>Mir Mohammad</forenames></author><author><keyname>Feizi-Derakhshi</keyname><forenames>Mohammad-Reza</forenames></author></authors><title>Double four-bar crank-slider mechanism dynamic balancing by
  meta-heuristic algorithms</title><categories>cs.AI</categories><comments>18 pages-19 figures</comments><msc-class>68-02</msc-class><journal-ref>International Journal of Artificial Intelligence &amp; Applications
  (IJAIA), Vol. 4, No. 5, September 2013</journal-ref><doi>10.5121/ijaia.2013.4501</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new method for dynamic balancing of double four-bar crank
slider mechanism by meta- heuristic-based optimization algorithms is proposed.
For this purpose, a proper objective function which is necessary for balancing
of this mechanism and corresponding constraints has been obtained by dynamic
modeling of the mechanism. Then PSO, ABC, BGA and HGAPSO algorithms have been
applied for minimizing the defined cost function in optimization step. The
optimization results have been studied completely by extracting the cost
function, fitness, convergence speed and runtime values of applied algorithms.
It has been shown that PSO and ABC are more efficient than BGA and HGAPSO in
terms of convergence speed and result quality. Also, a laboratory scale
experimental doublefour-bar crank-slider mechanism was provided for validating
the proposed balancing method practically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2095</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2095</id><created>2013-10-08</created><authors><author><keyname>Piyare</keyname><forenames>Rajeev</forenames></author><author><keyname>Lee</keyname><forenames>Seong Ro</forenames></author></authors><title>Towards Internet of Things (IOTS):Integration of Wireless Sensor Network
  to Cloud Services for Data Collection and Sharing</title><categories>cs.NI</categories><comments>14 pages, 11 Figures, Journal Article</comments><journal-ref>&quot;International Journal of Computer Networks &amp;
  Communications,vol.5,pp.59-72, September 2013</journal-ref><doi>10.5121/ijcnc.2013.5505</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Cloud computing provides great benefits for applications hosted on the Web
that also have special computational and storage requirements. This paper
proposes an extensible and flexible architecture for integrating Wireless
Sensor Networks with the Cloud. We have used REST based Web services as an
interoperable application layer that can be directly integrated into other
application domains for remote monitoring such as e-health care services, smart
homes, or even vehicular area networks (VAN). For proof of concept, we have
implemented a REST based Web services on an IP based low power WSN test bed,
which enables data access from anywhere. The alert feature has also been
implemented to notify users via email or tweets for monitoring data when they
exceed values and events of interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2098</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2098</id><created>2013-10-08</created><authors><author><keyname>Deng</keyname><forenames>Xinyang</forenames></author><author><keyname>Deng</keyname><forenames>Yong</forenames></author></authors><title>A short note on the axiomatic requirements of uncertainty measure</title><categories>cs.IT cs.AI math.IT</categories><comments>4 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, we argue that the axiomatic requirement of range to the measure
of aggregated total uncertainty (ATU) in Dempster-Shafer theory is not
reasonable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2102</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2102</id><created>2013-10-08</created><authors><author><keyname>Torres</keyname><forenames>Vasco Pereira</forenames></author></authors><title>Development of Biofeedback Mechanisms in a Procedural Environment Using
  Biometric Sensors</title><categories>cs.HC</categories><comments>MSc thesis, 96 pages, University of Porto</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Before the computer age, games were played in the physical world where
players would have to interact with real objects and each other, triggering a
series of emotions. Nowadays, the computer games have become one of the most
popular forms of entertainment due to their high-level of attraction and
accessibility. However, the game industry is always trying to find new ways of
making games more interactive and exciting in order to attract new players, and
one of the recent trends on the area of human-computer interaction is
Biofeedback.
  The goal of this dissertation is to study different approaches on the use of
indirect biofeedback within videogames, with the purpose of creating a better
human-computer interaction, and provide a more appealing and immersive user
experience. For this, we focused on the development of a framework capable of
testing different indirect biofeedback models within a specified game, in order
to assess the effect of each of these variations on the user experience. This
framework is game independent, with the intention of being used on further
studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2118</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2118</id><created>2013-10-08</created><authors><author><keyname>Fraczak</keyname><forenames>Wojciech</forenames></author><author><keyname>Georgiadis</keyname><forenames>Loukas</forenames></author><author><keyname>Miller</keyname><forenames>Andrew</forenames></author><author><keyname>Tarjan</keyname><forenames>Robert E.</forenames></author></authors><title>Finding Dominators via Disjoint Set Union</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of finding dominators in a directed graph has many important
applications, notably in global optimization of computer code. Although linear
and near-linear-time algorithms exist, they use sophisticated data structures.
We develop an algorithm for finding dominators that uses only a &quot;static tree&quot;
disjoint set data structure in addition to simple lists and maps. The algorithm
runs in near-linear or linear time, depending on the implementation of the
disjoint set data structure. We give several versions of the algorithm,
including one that computes loop nesting information (needed in many kinds of
global code optimization) and that can be made self-certifying, so that the
correctness of the computed dominators is very easy to verify.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2121</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2121</id><created>2013-10-08</created><authors><author><keyname>Caltagirone</keyname><forenames>Francesco</forenames></author><author><keyname>Franz</keyname><forenames>Silvio</forenames></author><author><keyname>Morris</keyname><forenames>Richard</forenames></author><author><keyname>Zdeborov&#xe1;</keyname><forenames>Lenka</forenames></author></authors><title>Dynamics and termination cost of spatially coupled mean-field models</title><categories>cond-mat.stat-mech cs.IT math.IT</categories><comments>12 pages, 11 figures</comments><journal-ref>Phys. Rev. E 89, 012102 (2014)</journal-ref><doi>10.1103/PhysRevE.89.012102</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work is motivated by recent progress in information theory and signal
processing where the so-called `spatially coupled' design of systems leads to
considerably better performance. We address relevant open questions about
spatially coupled systems through the study of a simple Ising model. In
particular, we consider a chain of Curie-Weiss models that are coupled by
interactions up to a certain range. Indeed, it is well known that the pure
(uncoupled) Curie-Weiss model undergoes a first order phase transition driven
by the magnetic field, and furthermore, in the spinodal region such systems are
unable to reach equilibrium in sub-exponential time if initialized in the
metastable state. By contrast, the spatially coupled system is, instead, able
to reach the equilibrium even when initialized to the metastable state. The
equilibrium phase propagates along the chain in the form of a travelling wave.
Here we study the speed of the wave-front and the so-called `termination
cost'--- \textit{i.e.}, the conditions necessary for the propagation to occur.
We reach several interesting conclusions about optimization of the speed and
the cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2125</identifier>
 <datestamp>2014-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2125</id><created>2013-10-08</created><updated>2014-03-06</updated><authors><author><keyname>Dutta</keyname><forenames>Ritabrata</forenames></author><author><keyname>Seth</keyname><forenames>Sohan</forenames></author><author><keyname>Kaski</keyname><forenames>Samuel</forenames></author></authors><title>Retrieval of Experiments with Sequential Dirichlet Process Mixtures in
  Model Space</title><categories>stat.ML cs.IR stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of retrieving relevant experiments given a query
experiment, motivated by the public databases of datasets in molecular biology
and other experimental sciences, and the need of scientists to relate to
earlier work on the level of actual measurement data. Since experiments are
inherently noisy and databases ever accumulating, we argue that a retrieval
engine should possess two particular characteristics. First, it should compare
models learnt from the experiments rather than the raw measurements themselves:
this allows incorporating experiment-specific prior knowledge to suppress noise
effects and focus on what is important. Second, it should be updated
sequentially from newly published experiments, without explicitly storing
either the measurements or the models, which is critical for saving storage
space and protecting data privacy: this promotes life long learning. We
formulate the retrieval as a ``supermodelling'' problem, of sequentially
learning a model of the set of posterior distributions, represented as sets of
MCMC samples, and suggest the use of Particle-Learning-based sequential
Dirichlet process mixture (DPM) for this purpose. The relevance measure for
retrieval is derived from the supermodel through the mixture representation. We
demonstrate the performance of the proposed retrieval method on simulated data
and molecular biological experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2127</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2127</id><created>2013-10-08</created><authors><author><keyname>Shanmugapriyaa</keyname><forenames>S.</forenames></author><author><keyname>Kuppusamy</keyname><forenames>K. S.</forenames></author><author><keyname>Aghila</keyname><forenames>G.</forenames></author></authors><title>BloSEn: Blog Search Engine Based On Post Concept Clustering</title><categories>cs.IR</categories><comments>12 pages</comments><acm-class>H.3.3</acm-class><journal-ref>International Journal of Ambient Systems and Applications (IJASA)
  Vol.1, No.3, September 2013</journal-ref><doi>10.5121/ijasa.2013.1302</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on building a blog search engine which doesn't focus only
on keyword search but includes extended search capabilities. It also
incorporates the blog-post concept clustering which is based on the category
extracted from the blog post semantic content analysis. The proposed approach
is titled as &quot;BloSen (Blog Search Engine)&quot;. It involves in extracting the posts
from blogs and parsing them to extract the blog elements and store them as
fields in a document format. Inverted index is being built on the fields of the
documents. Search is induced on the index and requested query is processed
based on the documents so far made from blog posts. It currently focuses on
Blogger and Wordpress hosted blogs since both these hosting services are the
most popular ones in the blogosphere. The proposed BloSen model is experimented
with a prototype implementation and the results of the experiments with the
user's relevance cumulative metric value of 95.44% confirms the efficiency of
the proposed model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2143</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2143</id><created>2013-10-08</created><authors><author><keyname>Esparza</keyname><forenames>Javier</forenames></author><author><keyname>Jezequel</keyname><forenames>Lo&#xef;g</forenames></author><author><keyname>Schwoon</keyname><forenames>Stefan</forenames></author></authors><title>Computation of Summaries Using Net Unfoldings</title><categories>cs.LO cs.FL</categories><comments>Extended version of our FSTTCS 2013 paper</comments><acm-class>D.2.2; F.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the following summarization problem: given a parallel composition
A=A1||...||An of labelled transition systems communicating with the environment
through a distinguished component Ai, efficiently compute a summary Si such
that E||A and E||Si are trace-equivalent for every environment E. While Si can
be computed using elementary automata theory, the resulting algorithm suffers
from the state-explosion problem. We present a new, simple but subtle algorithm
based on net unfoldings, a partial-order semantics, give some experimental
results using an implementation on top of MOLE, and show that our algorithm can
handle divergences and compute weighted summaries with minor modifications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2148</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2148</id><created>2013-10-03</created><authors><author><keyname>McGilvary</keyname><forenames>Gary A.</forenames></author><author><keyname>Rius</keyname><forenames>Josep</forenames></author><author><keyname>Goiri</keyname><forenames>&#xcd;&#xf1;igo</forenames></author><author><keyname>Solsona</keyname><forenames>Francesc</forenames></author><author><keyname>Barker</keyname><forenames>Adam</forenames></author><author><keyname>Atkinson</keyname><forenames>Malcolm</forenames></author></authors><title>C2MS: Dynamic Monitoring and Management of Cloud Infrastructures</title><categories>cs.DC cs.OS</categories><comments>Proceedings of the The 5th IEEE International Conference on Cloud
  Computing Technology and Science (CloudCom 2013), 8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Server clustering is a common design principle employed by many organisations
who require high availability, scalability and easier management of their
infrastructure. Servers are typically clustered according to the service they
provide whether it be the application(s) installed, the role of the server or
server accessibility for example. In order to optimize performance, manage load
and maintain availability, servers may migrate from one cluster group to
another making it difficult for server monitoring tools to continuously monitor
these dynamically changing groups. Server monitoring tools are usually
statically configured and with any change of group membership requires manual
reconfiguration; an unreasonable task to undertake on large-scale cloud
infrastructures.
  In this paper we present the Cloudlet Control and Management System (C2MS); a
system for monitoring and controlling dynamic groups of physical or virtual
servers within cloud infrastructures. The C2MS extends Ganglia - an open source
scalable system performance monitoring tool - by allowing system administrators
to define, monitor and modify server groups without the need for server
reconfiguration. In turn administrators can easily monitor group and individual
server metrics on large-scale dynamic cloud infrastructures where roles of
servers may change frequently. Furthermore, we complement group monitoring with
a control element allowing administrator-specified actions to be performed over
servers within service groups as well as introduce further customized
monitoring metrics. This paper outlines the design, implementation and
evaluation of the C2MS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2155</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2155</id><created>2013-10-08</created><updated>2014-11-19</updated><authors><author><keyname>Walter</keyname><forenames>Michael</forenames></author><author><keyname>Renes</keyname><forenames>Joseph M.</forenames></author></authors><title>Lower Bounds for Quantum Parameter Estimation</title><categories>quant-ph cs.IT math-ph math.IT math.MP</categories><comments>17 pages, 3 figures</comments><journal-ref>IEEE Transactions on Information Theory vol. 60, no. 12, pages
  8007-8023 (December 2014)</journal-ref><doi>10.1109/TIT.2014.2365174</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The laws of quantum mechanics place fundamental limits on the accuracy of
measurements and therefore on the estimation of unknown parameters of a quantum
system. In this work, we prove lower bounds on the size of confidence regions
reported by any region estimator for a given ensemble of probe states and
probability of success. Our bounds are derived from a previously unnoticed
connection between the size of confidence regions and the error probabilities
of a corresponding binary hypothesis test. In group-covariant scenarios, we
find that there is an ultimate bound for any estimation scheme which depends
only on the representation-theoretic data of the probe system, and we evaluate
its asymptotics in the limit of many systems, establishing a general
&quot;Heisenberg limit&quot; for region estimation. We apply our results to several
examples, in particular to phase estimation, where our bounds allow us to
recover the well-known Heisenberg and shot-noise scaling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2166</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2166</id><created>2013-10-08</created><authors><author><keyname>Rocha</keyname><forenames>Marcus V. M.</forenames></author><author><keyname>Rodrigues</keyname><forenames>Carlo Kleber da S.</forenames></author></authors><title>On client interactive behaviour to design peer selection policies for
  BitTorrent-like protocols</title><categories>cs.NI</categories><doi>10.5121/ijcnc.2013.5511</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Peer-to-peer swarming protocols have been proven to be very efficient for
content replication over Internet. This fact has certainly motivated proposals
to adapt these protocols to meet the requirements of on-demand streaming
system. The vast majority of these proposals focus on modifying the piece and
peer selection policies, respectively, of the original protocols. Nonetheless,
it is true that more attention has often been given to the piece selection
policy rather than to the peer selection policy. Within this context, this
article proposes a simple algorithm to be used as basis for peer selection
policies of BitTorrent-like protocols, considering interactive scenarios. To
this end, we analyze the client interactive behaviour when accessing real
multimedia systems. This analysis consists of looking into workloads of real
content providers and assessing three important metrics, namely temporal
dispersion, spatial dispersion and object position popularity. These metrics
are then used as the main guidelines for writing the algorithm. To the best of
our knowledge, this is the first time that the client interactive behaviour is
specially considered to derive an algorithm for peer selection policies.
Finally, the conclusion of this article is drawn with key challenges and
possible future work in this research field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2169</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2169</id><created>2013-10-08</created><authors><author><keyname>Bu</keyname><forenames>Yilei</forenames></author><author><keyname>Gregory</keyname><forenames>Steve</forenames></author><author><keyname>Mills</keyname><forenames>Harriet L.</forenames></author></authors><title>Efficient local behavioral change strategies to reduce the spread of
  epidemics in networks</title><categories>physics.soc-ph cs.SI</categories><journal-ref>Phys. Rev. E 88, 042801 (2013)</journal-ref><doi>10.1103/PhysRevE.88.042801</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has recently become established that the spread of infectious diseases
between humans is affected not only by the pathogen itself but also by changes
in behavior as the population becomes aware of the epidemic; for example,
social distancing. It is also well known that community structure (the
existence of relatively densely connected groups of vertices) in contact
networks influences the spread of disease. We propose a set of local strategies
for social distancing, based on community structure, that can be employed in
the event of an epidemic to reduce the epidemic size. Unlike most social
distancing methods, ours do not require individuals to know the disease state
(infected or susceptible, etc.) of others, and we do not make the unrealistic
assumption that the structure of the entire contact network is known. Instead,
the recommended behavior change is based only on an individual's local view of
the network. Each individual avoids contact with a fraction of his/her
contacts, using knowledge of his/her local network to decide which contacts
should be avoided. If the behavior change occurs only when an individual
becomes ill or aware of the disease, these strategies can substantially reduce
epidemic size with a relatively small cost, measured by the number of contacts
avoided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2182</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2182</id><created>2013-10-08</created><authors><author><keyname>Ismaeel</keyname><forenames>Ayad Ghany</forenames></author></authors><title>New Approach for Prediction Pre-cancer via Detecting Mutated in Tumor
  Protein P53</title><categories>cs.CE q-bio.OT</categories><comments>6 pages, 9 figures and 1 table,
  http://www.ijser.org/researchpaper/New-Approach-for-Prediction-Pre-cancer-via-Detecting-Mutated-in-Tumor-Protein-P53.pdf</comments><journal-ref>International Journal of Scientific &amp; Engineering Research, Volume
  4,Issue 10, October 2013 ISSN 2229-5518</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tumor protein P53 is believed to be involved in over half of human cancers
cases, the prediction of malignancies plays essential roles not only in advance
detection for cancer, but also in discovering effective prevention and
treatment of cancer, till now there isn't approach be able in prediction the
mutated in tumor protein P53 which is caused high ratio of human cancers like
breast, Blood, skin, liver, lung, bladder etc. This research proposed a new
approach for prediction pre-cancer via detection malignant mutations in tumor
protein P53 using bioinformatics tools like FASTA, BLAST, CLUSTALW and TP53
databases worldwide. Implement and apply this new approach of prediction
pre-cancer through mutations at tumor protein P53 shows an effective result
when used more specific parameters/features to extract the prediction result
that means when the user increase the number of filters of the results which
obtained from the database gives more specific diagnosis and classify, addition
that the detecting pre-cancer via prediction mutated tumor protein P53 will
reduces a person's cancers in the future by avoiding exposure to toxins,
radiation or monitoring themselves at older ages by change their food,
environment, even the pace of living. Also that new approach of prediction
pre-cancer will help if there is any treatment can give for that person to
therapy the mutated tumor protein P53. Index Terms (Normal Homology TP53 gene,
Tumor Protein P53, Oncogene Labs, GC and AT content, FASTA, BLAST, ClustalW)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2188</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2188</id><created>2013-10-08</created><authors><author><keyname>Wang</keyname><forenames>Wei</forenames></author><author><keyname>Yan</keyname><forenames>Zhidan</forenames></author><author><keyname>Zhang</keyname><forenames>Xin</forenames></author></authors><title>On r-equitable chromatic threshold of Kronecker products of complete
  graphs</title><categories>math.CO cs.DM</categories><msc-class>05C15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph $G$ is $r$-equitably $k$-colorable if its vertex set can be
partitioned into $k$ independent sets, any two of which differ in size by at
most $r$. The $r$-equitable chromatic threshold of a graph $G$, denoted by
$\chi_{r=}^*(G)$, is the minimum $k$ such that $G$ is $r$-equitably
$k'$-colorable for all $k'\ge k$. Let $G\times H$ denote the Kronecker product
of graphs $G$ and $H$. In this paper, we completely determine the exact value
of $\chi_{r=}^*(K_m\times K_n)$ for general $m,n$ and $r$. As a consequence, we
show that for $r\ge 2$, if $n\ge \frac{1}{r-1}(m+r)(m+2r-1)$ then $K_m\times
K_n$ and its spanning supergraph $K_{m(n)}$ have the same $r$-equitable
colorability, and in particular $\chi_{r=}^*(K_m\times
K_n)=\chi_{r=}^*(K_{m(n)})$, where $K_{m(n)}$ is the complete $m$-partite graph
with $n$ vertices in each part.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2206</identifier>
 <datestamp>2013-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2206</id><created>2013-10-08</created><authors><author><keyname>Brislawn</keyname><forenames>Christopher M.</forenames></author></authors><title>Group lifting structures for multirate filter banks I: Uniqueness of
  lifting factorizations</title><categories>cs.IT math.IT</categories><comments>19 pages, 3 figures</comments><report-no>LA-UR-09-8101</report-no><msc-class>42C40 (Primary), 94A29 (Secondary)</msc-class><acm-class>E.4; G.1.2</acm-class><journal-ref>IEEE Transactions on Signal Processing, vol. 58, no. 4, pp.
  2068-2077, April 2010</journal-ref><doi>10.1109/TSP.2009.2039816</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Group lifting structures are introduced to provide an algebraic framework for
studying lifting factorizations of two-channel perfect reconstruction
finite-impulse-response (FIR) filter banks. The lifting factorizations
generated by a group lifting structure are characterized by Abelian groups of
lower and upper triangular lifting matrices, an Abelian group of unimodular
gain scaling matrices, and a set of base filter banks. Examples of group
lifting structures are given for linear phase lifting factorizations of the two
nontrivial classes of two-channel linear phase FIR filter banks, the whole- and
half-sample symmetric classes, including both the reversible and irreversible
cases. This covers the lifting specifications for whole-sample symmetric filter
banks in Parts 1 and 2 of the ISO/IEC JPEG 2000 still image coding standard.
The theory is used to address the uniqueness of lifting factorizations. With no
constraints on the lifting process, it is shown that lifting factorizations are
highly nonunique. When certain hypotheses developed in the paper are satisfied,
however, lifting factorizations generated by a group lifting structure are
shown to be unique. A companion paper applies the uniqueness results proven in
this paper to the linear phase group lifting structures for whole- and
half-sample symmetric filter banks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2208</identifier>
 <datestamp>2013-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2208</id><created>2013-10-08</created><authors><author><keyname>Brislawn</keyname><forenames>Christopher M.</forenames></author></authors><title>Group lifting structures for multirate filter banks II: Linear phase
  filter banks</title><categories>cs.IT math.IT</categories><comments>22 pages</comments><report-no>LA-UR-09-8102</report-no><msc-class>42C40 (Primary), 94A29 (Secondary)</msc-class><acm-class>E.4; G.1.2</acm-class><journal-ref>IEEE Transactions on Signal Processing, vol. 58, no. 4, pp.
  2078-2087, April 2010</journal-ref><doi>10.1109/TSP.2009.2039818</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The theory of group lifting structures is applied to linear phase lifting
factorizations for the two nontrivial classes of two-channel linear phase
perfect reconstruction filter banks, the whole- and half-sample symmetric
classes. Group lifting structures defined for the reversible and irreversible
classes of whole- and half-sample symmetric filter banks are shown to satisfy
the hypotheses of the uniqueness theorem for group lifting structures. It
follows that linear phase group lifting factorizations of whole- and
half-sample symmetric filter banks are therefore independent of the
factorization methods used to construct them. These results cover the
specification of whole-sample symmetric filter banks in the ISO/IEC JPEG 2000
image coding standard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2217</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2217</id><created>2013-10-08</created><authors><author><keyname>Kosowski</keyname><forenames>Adrian</forenames><affiliation>INRIA Bordeaux - Sud-Ouest</affiliation></author><author><keyname>Markiewicz</keyname><forenames>Marcin</forenames><affiliation>IFTIA</affiliation></author></authors><title>Lower Bounds on the Communication Complexity of Binary Local Quantum
  Measurement Simulation</title><categories>quant-ph cs.IT math.IT</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of the classical simulation of quantum measurements
in the scenario of communication complexity. Regev and Toner (2007) have
presented a 2-bit protocol which simulates one particular correlation function
arising from binary projective quantum measurements on arbitrary state, and in
particular does not preserve local averages. The question of simulating other
correlation functions using a protocol with bounded communication, or
preserving local averages, has been posed as an open one. Within this paper we
resolve it in the negative: we show that any such protocol must have unbounded
communication for some subset of executions. In particular, we show that for
any protocol, there exist inputs for which the random variable describing the
number of communicated bits has arbitrarily large variance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2232</identifier>
 <datestamp>2013-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2232</id><created>2013-10-08</created><updated>2013-10-18</updated><authors><author><keyname>Wang</keyname><forenames>Jiasong</forenames></author><author><keyname>Dang</keyname><forenames>Chuangyin</forenames></author><author><keyname>Yin</keyname><forenames>Changchuan</forenames></author></authors><title>A Comparative Study of the Signal-to-Noise Ratios of Different
  Representations for Symbolic Sequences</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on the numerical representations by T basic vectors of a symbolic
sequence consisting of T symbols, first, we prove mathematical that the total
Fourier spectrum of the sequence is the square of the length of the sequence.
In the meantime, we define the indicator sequences vector. Using the orthogonal
or row orthogonal transformations of the indicator sequences vector, we
construct some special numerical representations of the symbolic sequence and
characterize the signal-to-noise ratios of the power spectrum of the numerical
representations. After calculating the discrete Fourier transform of those
special numerical representations, the signal-to-noise ratios of them can be
figured out. Mathematical theorems prove that the signal-to-noise ratio of the
Fourier spectrum of those special representations of the symbolic sequence is
T/(T-1) times the signal-to-noise ratio of the representation by T base
vectors. The results are applied in analyzing the properties of the DNA
sequences or protein sequences in the frequency domain, if one uses the
signal-to-noise ratios of special representations as the distinguishing
criterion, the distinguishing results only depend upon the distribution of the
symbols in the symbolic sequence and their mathematical constructions of
representations, but do not relate to the chemical or biological meanings of
the representations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2267</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2267</id><created>2013-10-08</created><updated>2014-11-19</updated><authors><author><keyname>Gross</keyname><forenames>D.</forenames></author><author><keyname>Krahmer</keyname><forenames>F.</forenames></author><author><keyname>Kueng</keyname><forenames>R.</forenames></author></authors><title>A Partial Derandomization of PhaseLift using Spherical Designs</title><categories>cs.IT math.IT quant-ph</categories><comments>32 pages, 1 figure. V2: added numerics, improved presentation. To
  appear in Journal of Fourier Analysis and Applications</comments><journal-ref>Journal of Fourier Analysis and Applications 21, 229 - 266 (2015)</journal-ref><doi>10.1007/s00041-014-9361-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of retrieving phase information from amplitude measurements alone
has appeared in many scientific disciplines over the last century. PhaseLift is
a recently introduced algorithm for phase recovery that is computationally
efficient, numerically stable, and comes with rigorous performance guarantees.
PhaseLift is optimal in the sense that the number of amplitude measurements
required for phase reconstruction scales linearly with the dimension of the
signal. However, it specifically demands Gaussian random measurement vectors -
a limitation that restricts practical utility and obscures the specific
properties of measurement ensembles that enable phase retrieval. Here we
present a partial derandomization of PhaseLift that only requires sampling from
certain polynomial size vector configurations, called t-designs. Such
configurations have been studied in algebraic combinatorics, coding theory, and
quantum information. We prove reconstruction guarantees for a number of
measurements that depends on the degree t of the design. If the degree is
allowed to to grow logarithmically with the dimension, the bounds become tight
up to polylog-factors. Beyond the specific case of PhaseLift, this work
highlights the utility of spherical designs for the derandomization of data
recovery schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2273</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2273</id><created>2013-10-08</created><updated>2014-09-16</updated><authors><author><keyname>Gillis</keyname><forenames>Nicolas</forenames></author><author><keyname>Vavasis</keyname><forenames>Stephen A.</forenames></author></authors><title>Semidefinite Programming Based Preconditioning for More Robust
  Near-Separable Nonnegative Matrix Factorization</title><categories>stat.ML cs.LG math.OC</categories><comments>25 pages, 6 figures, 4 tables. New numerical experiments, additional
  remarks and comments</comments><journal-ref>SIAM Journal on Optimization 25 (1), pp. 677-698, 2015</journal-ref><doi>10.1137/130940670</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nonnegative matrix factorization (NMF) under the separability assumption can
provably be solved efficiently, even in the presence of noise, and has been
shown to be a powerful technique in document classification and hyperspectral
unmixing. This problem is referred to as near-separable NMF and requires that
there exists a cone spanned by a small subset of the columns of the input
nonnegative matrix approximately containing all columns. In this paper, we
propose a preconditioning based on semidefinite programming making the input
matrix well-conditioned. This in turn can improve significantly the performance
of near-separable NMF algorithms which is illustrated on the popular successive
projection algorithm (SPA). The new preconditioned SPA is provably more robust
to noise, and outperforms SPA on several synthetic data sets. We also show how
an active-set method allow us to apply the preconditioning on large-scale
real-world hyperspectral images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2274</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2274</id><created>2013-10-08</created><authors><author><keyname>Varghese</keyname><forenames>Blesson</forenames></author><author><keyname>Rau-Chaplin</keyname><forenames>Andrew</forenames></author></authors><title>Accounting for Secondary Uncertainty: Efficient Computation of Portfolio
  Risk Measures on Multi and Many Core Architectures</title><categories>cs.DC cs.CE</categories><comments>10 pages, Workshop on High Performance Computational Finance at SC
  2013, Denver, Colorado, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Aggregate Risk Analysis is a computationally intensive and a data intensive
problem, thereby making the application of high-performance computing
techniques interesting. In this paper, the design and implementation of a
parallel Aggregate Risk Analysis algorithm on multi-core CPU and many-core GPU
platforms are explored. The efficient computation of key risk measures,
including Probable Maximum Loss (PML) and the Tail Value-at-Risk (TVaR) in the
presence of both primary and secondary uncertainty for a portfolio of property
catastrophe insurance treaties is considered. Primary Uncertainty is the the
uncertainty associated with whether a catastrophe event occurs or not in a
simulated year, while Secondary Uncertainty is the uncertainty in the amount of
loss when the event occurs.
  A number of statistical algorithms are investigated for computing secondary
uncertainty. Numerous challenges such as loading large data onto hardware with
limited memory and organising it are addressed. The results obtained from
experimental studies are encouraging. Consider for example, an aggregate risk
analysis involving 800,000 trials, with 1,000 catastrophic events per trial, a
million locations, and a complex contract structure taking into account
secondary uncertainty. The analysis can be performed in just 41 seconds on a
GPU, that is 24x faster than the sequential counterpart on a fast multi-core
CPU. The results indicate that GPUs can be used to efficiently accelerate
aggregate risk analysis even in the presence of secondary uncertainty.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2279</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2279</id><created>2013-10-08</created><authors><author><keyname>Varghese</keyname><forenames>Blesson</forenames></author><author><keyname>McKee</keyname><forenames>Gerard</forenames></author></authors><title>A Mathematical Model, Implementation and Study of a Swarm System</title><categories>cs.RO</categories><journal-ref>Robotics and Autonomous Systems, Vol. 58, Issue 3, 2010, pp.
  287-294</journal-ref><doi>10.1016/j.robot.2009.08.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The work reported in this paper is motivated towards the development of a
mathematical model for swarm systems based on macroscopic primitives. A pattern
formation and transformation model is proposed. The pattern transformation
model comprises two general methods for pattern transformation, namely a
macroscopic transformation and mathematical transformation method. The problem
of transformation is formally expressed and four special cases of
transformation are considered. Simulations to confirm the feasibility of the
proposed models and transformation methods are presented. Comparison between
the two transformation methods is also reported.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2289</identifier>
 <datestamp>2014-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2289</id><created>2013-10-08</created><updated>2014-10-23</updated><authors><author><keyname>Brislawn</keyname><forenames>Christopher M.</forenames></author><author><keyname>Woodring</keyname><forenames>Jonathan L.</forenames></author><author><keyname>Mniszewski</keyname><forenames>Susan M.</forenames></author><author><keyname>DeMarle</keyname><forenames>David E.</forenames></author><author><keyname>Ahrens</keyname><forenames>James P.</forenames></author></authors><title>Subband coding for large-scale scientific simulation data using JPEG
  2000</title><categories>cs.IT cs.MM math.IT</categories><comments>4 pages, 5 figures. Version 2: added BibTeX citation
  (BibTeX_citation.txt) as an ancillary file</comments><report-no>LA-UR-12-1352</report-no><msc-class>94A29</msc-class><acm-class>E.4</acm-class><journal-ref>Proceedings IEEE Southwest Symposium on Image Analysis and
  Interpretation, Santa Fe, NM: IEEE Computer Society, April 2012, pp. 201-204</journal-ref><doi>10.1109/SSIAI.2012.6202488</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ISO/IEC JPEG 2000 image coding standard is a family of source coding
algorithms targeting high-resolution image communications. JPEG 2000 features
highly scalable embedded coding features that allow one to interactively zoom
out to reduced resolution thumbnails of enormous data sets or to zoom in on
highly localized regions of interest with very economical communications and
rendering requirements. While intended for fixed-precision input data, the
implementation of the irreversible version of the standard is often done
internally in floating point arithmetic. Moreover, the standard is designed to
support high-bit-depth data. Part 2 of the standard also provides support for
three-dimensional data sets such as multicomponent or volumetric imagery. These
features make JPEG 2000 an appealing candidate for highly scalable
communications coding and visualization of two- and three-dimensional data
produced by scientific simulation software. We present results of initial
experiments applying JPEG 2000 to scientific simulation data produced by the
Parallel Ocean Program (POP) global ocean circulation model, highlighting both
the promise and the many challenges this approach holds for scientific
visualization applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2290</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2290</id><created>2013-10-08</created><authors><author><keyname>Edmonds</keyname><forenames>Bruce</forenames></author><author><keyname>Gershenson</keyname><forenames>Carlos</forenames></author></authors><title>Modelling Complexity for Policy: Opportunities and Challenges</title><categories>cs.MA cs.CY nlin.AO physics.soc-ph</categories><comments>Draft for the Handbook on Complexity and Public Policy, edited by
  Robert Geyer and Paul Cairney</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This chapter reviews the purpose and use of models from the field of complex
systems and, in particular, the implications of trying to use models to
understand or make decisions within complex situations, such as policy makers
usually face. A discussion of the different dimensions one can formalise
situations, the different purposes for models and the different kinds of
relationship they can have with the policy making process, is followed by an
examination of the compromises forced by the complexity of the target issues.
Several modelling approaches from complexity science are briefly described,
with notes as to their abilities and limitations. These approaches include
system dynamics, network theory, information theory, cellular automata, and
agent-based modelling. Some examples of policy models are presented and
discussed in the context of the previous analysis. Finally we conclude by
outlining some of the major pitfalls facing those wishing to use such models
for policy evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2291</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2291</id><created>2013-10-08</created><authors><author><keyname>Rezagah</keyname><forenames>Farideh Ebrahim</forenames></author><author><keyname>Erkip</keyname><forenames>Elza</forenames></author></authors><title>Interactive Function Computation with Reconstruction Constraints</title><categories>cs.IT math.IT</categories><comments>Accepted to 51st Annual Allerton Conference on Communication,
  Control, and Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates two-terminal interactive function computation with
reconstruction constraints. Each terminal wants to compute a (possibly
different) function of two correlated sources, but can only access one of the
sources directly. In addition to distortion constraints at the terminals, each
terminal is required to estimate the computed function value at the other
terminal in a lossy fashion, leading to the constrained reconstruction
constraint. A special case of constrained reconstruction is the common
reconstruction constraint, in which both terminals agree on the functions
computed with probability one. The terminals exchange information in multiple
rate constrained communication rounds. A characterization of the multi-round
rate-distortion region for the above problem with constrained reconstruction
constraints is provided. To gain more insights and to highlight the value of
interaction and order of communication, the rate-distortion region for
computing various functions of jointly Gaussian sources according to common
reconstruction constraints is studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2296</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2296</id><created>2013-10-08</created><authors><author><keyname>Rezagah</keyname><forenames>Farideh Ebrahim</forenames></author><author><keyname>Erkip</keyname><forenames>Elza</forenames></author></authors><title>Interactive Relay Assisted Source Coding</title><categories>cs.IT math.IT</categories><comments>Invited Paper submitted to GlobalSIP: IEEE Global Conference on
  Signal and Information Processing 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates a source coding problem in which two terminals
communicating through a relay wish to estimate one another's source within some
distortion constraint. The relay has access to side information that is
correlated with the sources. Two different schemes based on the order of
communication, \emph{distributed source coding/delivery} and \emph{two cascaded
rounds}, are proposed and inner and outer bounds for the resulting
rate-distortion regions are provided. Examples are provided to show that
neither rate-distortion region includes the other one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2298</identifier>
 <datestamp>2013-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2298</id><created>2013-10-08</created><updated>2013-10-16</updated><authors><author><keyname>Belov</keyname><forenames>Anton</forenames></author><author><keyname>Morgado</keyname><forenames>Antonio</forenames></author><author><keyname>Marques-Silva</keyname><forenames>Joao</forenames></author></authors><title>SAT-based Preprocessing for MaxSAT (extended version)</title><categories>cs.AI</categories><comments>Extended version of LPAR'19 paper with the same title</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  State-of-the-art algorithms for industrial instances of MaxSAT problem rely
on iterative calls to a SAT solver. Preprocessing is crucial for the
acceleration of SAT solving, and the key preprocessing techniques rely on the
application of resolution and subsumption elimination. Additionally,
satisfiability-preserving clause elimination procedures are often used. Since
MaxSAT computation typically involves a large number of SAT calls, we are
interested in whether an input instance to a MaxSAT problem can be preprocessed
up-front, i.e. prior to running the MaxSAT solver, rather than (or, in addition
to) during each iterative SAT solver call. The key requirement in this setting
is that the preprocessing has to be sound, i.e. so that the solution can be
reconstructed correctly and efficiently after the execution of a MaxSAT
algorithm on the preprocessed instance. While, as we demonstrate in this paper,
certain clause elimination procedures are sound for MaxSAT, it is well-known
that this is not the case for resolution and subsumption elimination. In this
paper we show how to adapt these preprocessing techniques to MaxSAT. To achieve
this we recast the MaxSAT problem in a recently introduced labelled-CNF
framework, and show that within the framework the preprocessing techniques can
be applied soundly. Furthermore, we show that MaxSAT algorithms restated in the
framework have a natural implementation on top of an incremental SAT solver. We
evaluate the prototype implementation of a MaxSAT algorithm WMSU1 in this
setting, demonstrate the effectiveness of preprocessing, and show overall
improvement with respect to non-incremental versions of the algorithm on some
classes of problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2300</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2300</id><created>2013-10-08</created><authors><author><keyname>Brunthaler</keyname><forenames>Stefan</forenames></author></authors><title>Speculative Staging for Interpreter Optimization</title><categories>cs.PL</categories><comments>16 pages, 4 figures, 3 tables. Uses CPython 3.2.3 and PyPy 1.9</comments><acm-class>D.3.4; F.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interpreters have a bad reputation for having lower performance than
just-in-time compilers. We present a new way of building high performance
interpreters that is particularly effective for executing dynamically typed
programming languages. The key idea is to combine speculative staging of
optimized interpreter instructions with a novel technique of incrementally and
iteratively concerting them at run-time.
  This paper introduces the concepts behind deriving optimized instructions
from existing interpreter instructions---incrementally peeling off layers of
complexity. When compiling the interpreter, these optimized derivatives will be
compiled along with the original interpreter instructions. Therefore, our
technique is portable by construction since it leverages the existing
compiler's backend. At run-time we use instruction substitution from the
interpreter's original and expensive instructions to optimized instruction
derivatives to speed up execution.
  Our technique unites high performance with the simplicity and portability of
interpreters---we report that our optimization makes the CPython interpreter up
to more than four times faster, where our interpreter closes the gap between
and sometimes even outperforms PyPy's just-in-time compiler.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2305</identifier>
 <datestamp>2013-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2305</id><created>2013-10-08</created><authors><author><keyname>Brislawn</keyname><forenames>Christopher M.</forenames></author></authors><title>Gain scaling for multirate filter banks</title><categories>cs.IT math.IT</categories><comments>5 pages, 4 figures</comments><report-no>LA-UR-09-7904</report-no><msc-class>94A29</msc-class><acm-class>E.4</acm-class><journal-ref>In: Proceedings IEEE Asilomar Conference on Signals, Systems and
  Computers, Asilomar, CA, Nov. 2009, pp. 437-441</journal-ref><doi>10.1109/ACSSC.2009.5469853</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Eliminating two trivial degrees of freedom corresponding to the lowpass DC
response and the highpass Nyquist response in a two-channel multirate filter
bank seems simple enough. Nonetheless, the ISO/IEC JPEG 2000 image coding
standard manages to make this mundane task look totally mysterious. We reveal
the true meaning behind JPEG 2000's arcane specifications for filter bank
normalization and point out how the seemingly trivial matter of gain scaling
leads to highly nontrivial issues concerning uniqueness of lifting
factorizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2306</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2306</id><created>2013-10-08</created><authors><author><keyname>Sparavalo</keyname><forenames>Myroslav</forenames></author></authors><title>Robust Adaptive Control for Circadian Dynamics: Poincare Approach to
  Backstepping Method</title><categories>cs.SY</categories><comments>2 double-column pages, 3 figures, 3 references</comments><msc-class>93C10, 93C15, 93C40, 93B52, 93B51, 93D09, 34D45, 14H30</msc-class><acm-class>I.2.8; I.6</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  A mathematical model of the circadian dynamics in the form of Van der Pol
equation with an external force as a control is investigated. The combination
of backstepping method and differential-topological techniques based on the
Poincare's ideas is used. The robust model identification adaptive control for
a specific adaptation law is designed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2320</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2320</id><created>2013-10-08</created><authors><author><keyname>McIver</keyname><forenames>Annabelle</forenames></author><author><keyname>Rabehaja</keyname><forenames>Tahiry</forenames></author><author><keyname>Struth</keyname><forenames>Georg</forenames></author></authors><title>An Event Structure Model for Probabilistic Concurrent Kleene Algebra</title><categories>cs.LO</categories><comments>Submitted and accepted for LPAR19 (2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a new true-concurrent model for probabilistic concurrent Kleene
algebra. The model is based on probabilistic event structures, which combines
ideas from Katoen's work on probabilistic concurrency and Varacca's
probabilistic prime event structures. The event structures are compared with a
true-concurrent version of Segala's probabilistic simulation. Finally, the
algebraic properties of the model are summarised to the extent that they can be
used to derive techniques such as probabilistic rely/guarantee inference rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2322</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2322</id><created>2013-10-08</created><updated>2014-04-28</updated><authors><author><keyname>Chleb&#xed;kov&#xe1;</keyname><forenames>Janka</forenames></author><author><keyname>Chopin</keyname><forenames>Morgan</forenames></author></authors><title>The Firefighter Problem: A Structural Analysis</title><categories>cs.DM cs.DS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the complexity of the firefighter problem where b&gt;=1 firefighters
are available at each time step. This problem is proved NP-complete even on
trees of degree at most three and budget one (Finbow et al.,2007) and on trees
of bounded degree b+3 for any fixed budget b&gt;=2 (Bazgan et al.,2012). In this
paper, we provide further insight into the complexity landscape of the problem
by showing that the pathwidth and the maximum degree of the input graph govern
its complexity. More precisely, we first prove that the problem is NP-complete
even on trees of pathwidth at most three for any fixed budget b&gt;=1. We then
show that the problem turns out to be fixed parameter-tractable with respect to
the combined parameter &quot;pathwidth&quot; and &quot;maximum degree&quot; of the input graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2323</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2323</id><created>2013-10-08</created><authors><author><keyname>Xiao</keyname><forenames>Yuanzhang</forenames></author><author><keyname>van der Schaar</keyname><forenames>Mihaela</forenames></author></authors><title>Socially-Optimal Design of Service Exchange Platforms with Imperfect
  Monitoring</title><categories>cs.GT</categories><comments>53 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In service exchange platforms, anonymous users exchange services with each
other: clients request services and are matched to servers who provide
services. Because providing good-quality services requires effort, in any
single interaction a server will have no incentive to exert effort and will
shirk. We show that if current servers will later become clients and want
good-quality services, shirking can be eliminated by rating protocols, which
maintain ratings for each user, prescribe behavior in each client-server
interaction, and update ratings based on whether observed/reported behavior
conforms with prescribed behavior. The rating protocols proposed are the first
to achieve social optimum even when observation/reporting is imperfect (quality
is incorrectly assessed/reported or reports are lost). The proposed protocols
are remarkably simple, requiring only binary ratings and three possible
prescribed behaviors. Key to the efficacy of the proposed protocols is that
they are nonstationary, and tailor prescriptions to both current and past
rating distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2332</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2332</id><created>2013-10-08</created><authors><author><keyname>Huang</keyname><forenames>Heliang</forenames></author><author><keyname>Bao</keyname><forenames>Wansu</forenames></author></authors><title>Middle-Solving F4 to Compute Grobner bases for Cryptanalysis over GF(2)</title><categories>cs.SC cs.CR cs.NI math.AC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Algebraic cryptanalysis usually requires to recover the secret key by solving
polynomial equations. Faugere's F4 is a well-known Grobner bases algorithm to
solve this problem. However, a serious drawback exists in the Grobner bases
based algebraic attacks, namely, any information won't be got if we couldn't
work out the Grobner bases of the polynomial equations system. In this paper,
we in-depth research the F4 algorithm over GF(2). By using S-polynomials to
replace critical pairs and computing the normal form of the productions with
respect to the field equations in certain steps, many &quot;redundant&quot; reductors are
avoided during the computation process of the F4 algorithm. By slightly
modifying the logic of F4 algorithm, we solve the univariate polynomials
appeared in the algorithm and then back-substitute the values of the solved
variables at each iteration of the algorithm. We call our improvements
Middle-Solving F4. The heuristic strategy of Middle-Solving overcomes the
drawback of algebraic attacks and well suits algebraic attacks. It has never
been applied to the Grobner bases algorithm before. Experiments to some Hidden
Field Equation instances and some classical benchmarks (Cyclic 6, Gonnet83)
show that Middle-Solving F4 is faster and uses less memory than Faugere's F4.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2338</identifier>
 <datestamp>2014-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2338</id><created>2013-10-09</created><updated>2014-03-13</updated><authors><author><keyname>Dumas</keyname><forenames>Jean-Guillaume</forenames><affiliation>LJK</affiliation></author><author><keyname>Duval</keyname><forenames>Dominique</forenames><affiliation>LJK</affiliation></author><author><keyname>Ekici</keyname><forenames>Burak</forenames><affiliation>LJK</affiliation></author><author><keyname>Reynaud</keyname><forenames>Jean-Claude</forenames><affiliation>RC</affiliation></author></authors><title>Certified proofs in programs involving exceptions</title><categories>cs.LO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exception handling is provided by most modern programming languages. It
allows to deal with anomalous or exceptional events which require special
processing. In computer algebra, exception handling is an efficient way to
implement the dynamic evaluation paradigm: for instance, in linear algebra,
dynamic evaluation can be used for applying programs which have been written
for matrices with coefficients in a field to matrices with coefficients in a
ring. Thus, a proof system for computer algebra should include a treatement of
exceptions, which must rely on a careful description of a semantics of
exceptions. The categorical notion of monad can be used for formalizing the
raising of exceptions: this has been proposed by Moggi and implemented in
Haskell. In this paper, we provide a proof system for exceptions which involves
both raising and handling, by extending Moggi's approach. Moreover, the core
part of this proof system is dual to a proof system for side effects in
imperative languages, which relies on the categorical notion of comonad. Both
proof systems are implemented in the Coq proof assistant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2342</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2342</id><created>2013-10-09</created><authors><author><keyname>Aly</keyname><forenames>Heba</forenames></author><author><keyname>Youssef</keyname><forenames>Moustafa</forenames></author></authors><title>Dejavu: An Accurate Energy-Efficient Outdoor Localization System</title><categories>cs.CY</categories><journal-ref>21st ACM SIGSPATIAL International Conference on Advances in
  Geographic Information Systems (ACM SIGSPATIAL GIS 2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present Dejavu, a system that uses standard cell-phone sensors to provide
accurate and energy-efficient outdoor localization suitable for car navigation.
Our analysis shows that different road landmarks have a unique signature on
cell-phone sensors; For example, going inside tunnels, moving over bumps, going
up a bridge, and even potholes all affect the inertial sensors on the phone in
a unique pattern. Dejavu employs a dead-reckoning localization approach and
leverages these road landmarks, among other automatically discovered abundant
virtual landmarks, to reset the accumulated error and achieve accurate
localization. To maintain a low energy profile, Dejavu uses only
energy-efficient sensors or sensors that are already running for other
purposes. We present the design of Dejavu and how it leverages crowd-sourcing
to automatically learn virtual landmarks and their locations. Our evaluation
results from implementation on different android devices in both city and
highway driving show that Dejavu can localize cell phones to within 8.4m median
error in city roads and 16.6m on highways. Moreover, compared to GPS and other
state-of-the-art systems, Dejavu can extend the battery lifetime by 347%,
achieving even better localization results than GPS in the more challenging
in-city driving conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2346</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2346</id><created>2013-10-09</created><authors><author><keyname>Aguzzoli</keyname><forenames>Stefano</forenames></author><author><keyname>Marra</keyname><forenames>Vincenzo</forenames></author></authors><title>Two principles in many-valued logic</title><categories>math.LO cs.LO</categories><comments>11 pages. Manuscript submitted to H\'ajek's Festschrift</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classically, two propositions are logically equivalent precisely when they
are true under the same logical valuations. Also, two logical valuations are
distinct if, and only if, there is a formula that is true according to one
valuation, and false according to the other. By a real-valued logic we mean a
many-valued logic in the sense of Petr H\'ajek that is complete with respect to
a subalgebra of truth values of a BL-algebra given by a continuous triangular
norm on [0, 1]. Abstracting the two foregoing properties from classical logic
leads us to two principles that a real-valued logic may or may not satisfy. We
prove that the two principles are sufficient to characterise {\L}ukasiewicz and
G\&quot;odel logic, to within extensions. We also prove that, under the additional
assumption that the set of truth values be closed in the Euclidean topology of
[0,1], the two principles also afford a characterisation of Product logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2350</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2350</id><created>2013-10-09</created><authors><author><keyname>Pintea</keyname><forenames>Camelia-M.</forenames></author><author><keyname>Pop</keyname><forenames>Petrica C.</forenames></author><author><keyname>Chira</keyname><forenames>Camelia</forenames></author></authors><title>The Generalized Traveling Salesman Problem solved with Ant Algorithms</title><categories>cs.AI cs.NE</categories><comments>indexed in Scopus, ORCID</comments><journal-ref>J.UCS 13(7)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A well known N P-hard problem called the Generalized Traveling Salesman
Problem (GTSP) is considered. In GTSP the nodes of a complete undirected graph
are partitioned into clusters. The objective is to find a minimum cost tour
passing through exactly one node from each cluster. An exact exponential time
algorithm and an effective meta-heuristic algorithm for the problem are
presented. The meta-heuristic proposed is a modified Ant Colony System (ACS)
algorithm called Reinforcing Ant Colony System (RACS) which introduces new
correction rules in the ACS algorithm. Computational results are reported for
many standard test problems. The proposed algorithm is competitive with the
other already proposed heuristics for the GTSP in both solution quality and
computational time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2351</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2351</id><created>2013-10-09</created><authors><author><keyname>Aggarwal</keyname><forenames>Abhinav</forenames></author></authors><title>Algebraic Message Authentication Codes</title><categories>cs.CR</categories><comments>8 pages, 2 Figures, Double Column</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper suggests a message authentication scheme, which can be efficiently
used for secure digital signature creation. The algorithm used here is an
adjusted union of the concepts which underlie projective geometry and group
structure on circles. The authentication is done through a key, which iterates
over the complete message string to produce the signature. The iteration is not
only based on the frequency distribution of the message string alphabet, but
also on the probability of occurrence of another given reference string in the
message. The complete process can be easily computed in a small time, producing
signatures which are highly dependent on the message string. Consequently, the
odds in favor of existence of a forgery are highly reduced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2354</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2354</id><created>2013-10-09</created><authors><author><keyname>Southwell</keyname><forenames>Richard</forenames></author><author><keyname>Chen</keyname><forenames>Xu</forenames></author><author><keyname>Huang</keyname><forenames>Jianwei</forenames></author></authors><title>Quality of Service Games for Spectrum Sharing</title><categories>cs.NI cs.GT</categories><comments>The paper has been accepted by IEEE Journal of Selected Areas in
  Communications (JSAC)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today's wireless networks are increasingly crowded with an explosion of
wireless users, who have greater and more diverse quality of service (QoS)
demands than ever before. However, the amount of spectrum that can be used to
satisfy these demands remains finite. This leads to a great challenge for
wireless users to effectively share the spectrum to achieve their QoS
requirements. This paper presents a game theoretic model for spectrum sharing,
where users seek to satisfy their QoS demands in a distributed fashion. Our
spectrum sharing model is quite general, because we allow different wireless
channels to provide different QoS, depending upon their channel conditions and
how many users are trying to access them. Also, users can be highly
heterogeneous, with different QoS demands, depending upon their activities,
hardware capabilities, and technology choices. Under such a general setting, we
show that it is NP hard to find a spectrum allocation which satisfies the
maximum number of users' QoS requirements in a centralized fashion. We also
show that allowing users to self-organize through distributed channel
selections is a viable alternative to the centralized optimization, because
better response updating is guaranteed to reach a pure Nash equilibria in
polynomial time. By bounding the price of anarchy, we demonstrate that the
worst case pure Nash equilibrium can be close to optimal, when users and
channels are not very heterogenous. We also extend our model by considering the
frequency spatial reuse, and consider the user interactions as a game upon a
graph where players only contend with their neighbors. We prove that better
response updating is still guaranteed to reach a pure Nash equilibrium in this
more general spatial QoS satisfaction game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2357</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2357</id><created>2013-10-09</created><authors><author><keyname>Aldecoa</keyname><forenames>Rodrigo</forenames></author><author><keyname>Mar&#xed;n</keyname><forenames>Ignacio</forenames></author></authors><title>SurpriseMe: an integrated tool for network community structure
  characterization using Surprise maximization</title><categories>q-bio.MN cs.SI physics.soc-ph</categories><comments>6 pages</comments><journal-ref>Bioinformatics 30, 1041 (2014)</journal-ref><doi>10.1093/bioinformatics/btt741</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detecting communities, densely connected groups may contribute to unravel the
underlying relationships among the units present in diverse biological networks
(e.g., interactome, coexpression networks, ecological networks, etc.). We
recently showed that communities can be very precisely characterized by
maximizing Surprise, a global network parameter. Here we present SurpriseMe, a
tool that integrates the outputs of seven of the best algorithms available to
estimate the maximum Surprise value. SurpriseMe also generates distance
matrices that allow to visualize the relationships among the solutions
generated by the algorithms. We show that the communities present in small and
medium-sized networks, with up to 10.000 nodes, can be easily characterized: on
standard PC computers, these analyses take less than an hour. Also, four of the
algorithms may quite rapidly analyze networks with up to 100.000 nodes, given
enough memory resources. Because of its performance and simplicity, SurpriseMe
is a reference tool for community structure characterization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2361</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2361</id><created>2013-10-09</created><authors><author><keyname>Panse</keyname><forenames>Chanda</forenames></author><author><keyname>Kshirsagar</keyname><forenames>Dr. Manali</forenames></author></authors><title>Survey on Modelling Methods Applicable to Gene Regulatory Network</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gene Regulatory Network (GRN) plays an important role in knowing insight of
cellular life cycle. It gives information about at which different
environmental conditions genes of particular interest get over expressed or
under expressed. Modelling of GRN is nothing but finding interactive
relationships between genes. Interaction can be positive or negative. For
inference of GRN, time series data provided by Microarray technology is used.
Key factors to be considered while constructing GRN are scalability,
robustness, reliability and maximum detection of true positive interactions
between genes. This paper gives detailed technical review of existing methods
applied for building of GRN along with scope for future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2365</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2365</id><created>2013-10-09</created><authors><author><keyname>Jagli</keyname><forenames>Mrs. Dhanamma</forenames></author><author><keyname>Temkar</keyname><forenames>Mrs. Rohini</forenames></author></authors><title>The Unified Approach For Organizational Network Vulnerability Assessment</title><categories>cs.SE</categories><comments>12 pages,5Figures</comments><journal-ref>International Journal of Software Engineering &amp; Applications
  (IJSEA),Vol.4, No.5, September 2013</journal-ref><doi>10.5121/ijsea.2013.4503</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The present business network infrastructure is quickly varying with latest
servers, services, connections, and ports added often, at times day by day, and
with a uncontrollably inflow of laptops, storage media and wireless networks.
With the increasing amount of vulnerabilities and exploits coupled with the
recurrent evolution of IT infrastructure, organizations at present require more
numerous vulnerability assessments. In this paper new approach the Unified
process for Network vulnerability Assessments hereafter called as a unified NVA
is proposed for network vulnerability assessment derived from Unified Software
Development Process or Unified Process, it is a popular iterative and
incremental software development process framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2367</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2367</id><created>2013-10-09</created><authors><author><keyname>Jagli</keyname><forenames>Mrs. Dhanamma</forenames></author><author><keyname>Gaikwad</keyname><forenames>Ms. Priyanka</forenames></author><author><keyname>Gunjal</keyname><forenames>Ms. Shubhangi</forenames></author><author><keyname>Bilaware</keyname><forenames>Mr. Chaitanya</forenames></author></authors><title>Handy Annotations within Oracle 10g</title><categories>cs.DB</categories><comments>5Pages,2Figures</comments><journal-ref>International Journal of Scientific &amp; Engineering Research Volume
  4, Issue 1, January-2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes practical observations during the Database system Lab.
Oracle 10g DBMS is used in the data base system lab and performed SQL queries
based many concepts like Data Definition Language Commands (DDL), Data
Modification Language Commands ((DML), Views, Integrity Constraints, Aggregate
functions, Joins and Abstract type . While performing practical during the lab
session, many problems occurred, in order to solve them many text books and
websites referred but could not obtain expected help from them. Even though by
spending much time in the database labs with Oracle 10g, tried in numerous
ways, as a final point expected output is achieved. This paper describes
annotations which were experimentally proved in the Database lab.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2368</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2368</id><created>2013-10-09</created><authors><author><keyname>Richet</keyname><forenames>Jean-Loup</forenames></author></authors><title>Laundering Money Online: a review of cybercriminals methods</title><categories>cs.CY</categories><comments>Industry report, White paper. Tools and Resources for Anti-Corruption
  Knowledge, June, 01, 2013. United Nations Office on Drugs and Crime (UNODC)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Money laundering is a critical step in the cyber crime process which is
experiencing some changes as hackers and their criminal colleagues continually
alter and optimize payment mechanisms. Conducting quantitative research on
underground laundering activity poses an inherent challenge: Bad guys and their
banks do not share information on criminal pursuits. However, by analyzing
forums, we have identified two growth areas in money laundering: online gaming
and micro laundering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2369</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2369</id><created>2013-10-09</created><authors><author><keyname>Jagli</keyname><forenames>Dhanamma</forenames></author><author><keyname>Solanki</keyname><forenames>Ramesh</forenames></author><author><keyname>Temkar</keyname><forenames>Rohini</forenames></author><author><keyname>Veshapogu</keyname><forenames>Laxmi</forenames></author></authors><title>Semi Symmetric Method Of SAN Storage Virtualization</title><categories>cs.SE cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Virtualization is one of the biggest buzzwords of the technology industry
right at this moment. The fast growth in storage capacity and processing power
in enterprise installations coupled with the need for high availability,
requires Storage Area Network (SAN) architecture to provide seamless addition
of storage and performance elements without downtime. The usual goal of
virtualization is to centralize administrative tasks while improving
scalability and work loads. This paper, describing about new proposed method
for virtualization, which would be overcome limitations of existed methods for
storage virtualization
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2375</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2375</id><created>2013-10-09</created><authors><author><keyname>Jagli</keyname><forenames>Dhanamma</forenames></author><author><keyname>Oswal</keyname><forenames>Sangeeta</forenames></author></authors><title>Web Usage Mining: Pattern Discovery and Forecasting</title><categories>cs.DB cs.IR</categories><journal-ref>IFRSA International Journal of Data Warehousing &amp; Mining |Vol
  2|issue4|November 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web usage mining: automatic discovery of patterns in clickstreams and
associated data collected or generated as a result of user interactions with
one or more Web sites. This paper describes web usage mining for our college
log files to analyze the behavioral patterns and profiles of users interacting
with a Web site. The discovered patterns are represented as clusters that are
frequently accessed by groups of visitors with common interests. In this paper,
the visitors and hits were forecasted to predict the further access statistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2378</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2378</id><created>2013-10-09</created><updated>2015-12-17</updated><authors><author><keyname>Adler</keyname><forenames>Isolde</forenames></author><author><keyname>Kolliopoulos</keyname><forenames>Stavros G.</forenames></author><author><keyname>Krause</keyname><forenames>Philipp Klaus</forenames></author><author><keyname>Lokshtanov</keyname><forenames>Daniel</forenames></author><author><keyname>Saurabhh</keyname><forenames>Saket</forenames></author><author><keyname>Thilikos</keyname><forenames>Dimitrios M.</forenames></author></authors><title>Irrelevant Vertices for the Planar Disjoint Paths Problem</title><categories>math.CO cs.DS</categories><msc-class>05C10, 05C85, 68R10</msc-class><acm-class>G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The {\sc Disjoint Paths Problem} asks, given a graph $G$ and a set of pairs
of terminals $(s_{1},t_{1}),\ldots,(s_{k},t_{k})$, whether there is a
collection of $k$ pairwise vertex-disjoint paths linking $s_{i}$ and $t_{i}$,
for $i=1,\ldots,k.$ In their $f(k)\cdot n^{3}$ algorithm for this problem,
Robertson and Seymour introduced the {\sl irrelevant vertex technique}
according to which in every instance of treewidth greater than $g(k)$ there is
an &quot;irrelevant&quot; vertex whose removal creates an equivalent instance of the
problem. This fact is based on the celebrated {\sl Unique Linkage Theorem},
whose - very technical - proof gives a function $g(k)$ that is responsible for
an immense parameter dependence in the running time of the algorithm. In this
paper we give a new and self-contained proof of this result that strongly
exploits the combinatorial properties of planar graphs and achieves
$g(k)=O(k^{3/2}\cdot 2^{k}).$ Our bound is radically better than the bounds
known for general graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2381</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2381</id><created>2013-10-09</created><updated>2014-05-19</updated><authors><author><keyname>Wang</keyname><forenames>Yan</forenames></author><author><keyname>Yin</keyname><forenames>Xunrui</forenames></author><author><keyname>Wang</keyname><forenames>Xin</forenames></author></authors><title>MDR Codes: A New Class of RAID-6 Codes with Optimal Rebuilding and
  Encoding</title><categories>cs.IT math.IT</categories><comments>Accepted version. Please refer to
  http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&amp;arnumber=6804945 for
  the published version. 0733-8716/14/$31.00 \c{opyright} 2014 IEEE</comments><journal-ref>IEEE Journal on Selected Areas in Communications, Vol 32, Issue 5,
  May 2014, page 1008-1018</journal-ref><doi>10.1109/JSAC.2014.140520</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As storage systems grow in size, device failures happen more frequently than
ever before. Given the commodity nature of hard drives employed, a storage
system needs to tolerate a certain number of disk failures while maintaining
data integrity, and to recover lost data with minimal interference to normal
disk I/O operations. RAID-6, which can tolerate up to two disk failures with
the minimum redundancy, is becoming widespread. However, traditional RAID-6
codes suffer from high disk I/O overhead during recovery. In this paper, we
propose a new family of RAID-6 codes, the Minimum Disk I/O Repairable (MDR)
codes, which achieve the optimal disk I/O overhead for single failure
recoveries. Moreover, we show that MDR codes can be encoded with the minimum
number of bit-wise XOR operations. Simulation results show that MDR codes help
to save about half of disk read operations than traditional RAID-6 codes, and
thus can reduce the recovery time by up to 40%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2385</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2385</id><created>2013-10-09</created><authors><author><keyname>Gherekhloo</keyname><forenames>Soheyl</forenames></author><author><keyname>Chaaban</keyname><forenames>Anas</forenames></author><author><keyname>Sezgin</keyname><forenames>Aydin</forenames></author></authors><title>Topological Interference Management with Alternating Connectivity: The
  Wyner-Type Three User Interference Channel</title><categories>cs.IT math.IT</categories><comments>4 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interference management in a three-user interference channel with alternating
connectivity with only topological knowledge at the transmitters is considered.
The network has a Wyner-type channel flavor, i.e., for each connectivity state
the receivers observe at most one interference signal in addition to their
desired signal. Degrees of freedom (DoF) upper bounds and lower bounds are
derived. The lower bounds are obtained from a scheme based on joint encoding
across the alternating states. Given a uniform distribution among the
connectivity states, it is shown that the channel has 2+ 1/9 DoF. This provides
an increase in the DoF as compared to encoding over each state separately,
which achieves 2 DoF only.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2386</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2386</id><created>2013-10-09</created><authors><author><keyname>Byrka</keyname><forenames>Jaroslaw</forenames></author><author><keyname>Li</keyname><forenames>Shanfei</forenames></author><author><keyname>Rybicki</keyname><forenames>Bartosz</forenames></author></authors><title>Improved approximation algorithm for k-level UFL with penalties, a
  simplistic view on randomizing the scaling parameter</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The state of the art in approximation algorithms for facility location
problems are complicated combinations of various techniques. In particular, the
currently best 1.488-approximation algorithm for the uncapacitated facility
location (UFL) problem by Shi Li is presented as a result of a non-trivial
randomization of a certain scaling parameter in the LP-rounding algorithm by
Chudak and Shmoys combined with a primal-dual algorithm of Jain et al. In this
paper we ?rst give a simple interpretation of this randomization process in
terms of solving an aux- iliary (factor revealing) LP. Then, armed with this
simple view point, Abstract. we exercise the randomization on a more
complicated algorithm for the k-level version of the problem with penalties in
which the planner has the option to pay a penalty instead of connecting chosen
clients, which results in an improved approximation algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2387</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2387</id><created>2013-10-09</created><authors><author><keyname>Cornelissen</keyname><forenames>Kamiel</forenames></author><author><keyname>Hoeksma</keyname><forenames>Ruben</forenames></author><author><keyname>Manthey</keyname><forenames>Bodo</forenames></author><author><keyname>Narayanaswamy</keyname><forenames>N. S.</forenames></author><author><keyname>Rahul</keyname><forenames>C. S.</forenames></author></authors><title>Approximability of Connected Factors</title><categories>cs.DS</categories><comments>To appear in the proceedings of WAOA 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding a d-regular spanning subgraph (or d-factor) of a graph is easy by
Tutte's reduction to the matching problem. By the same reduction, it is easy to
find a minimal or maximal d-factor of a graph. However, if we require that the
d-factor is connected, these problems become NP-hard - finding a minimal
connected 2-factor is just the traveling salesman problem (TSP).
  Given a complete graph with edge weights that satisfy the triangle
inequality, we consider the problem of finding a minimal connected $d$-factor.
We give a 3-approximation for all $d$ and improve this to an
(r+1)-approximation for even d, where r is the approximation ratio of the TSP.
This yields a 2.5-approximation for even d. The same algorithm yields an
(r+1)-approximation for the directed version of the problem, where r is the
approximation ratio of the asymmetric TSP. We also show that none of these
minimization problems can be approximated better than the corresponding TSP.
  Finally, for the decision problem of deciding whether a given graph contains
a connected d-factor, we extend known hardness results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2396</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2396</id><created>2013-10-09</created><authors><author><keyname>Yao</keyname><forenames>Hua</forenames></author><author><keyname>Zhu</keyname><forenames>William</forenames></author></authors><title>A necessary and sufficient condition for two relations to induce the
  same definable set family</title><categories>cs.AI</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Pawlak rough sets, the structure of the definable set families is simple
and clear, but in generalizing rough sets, the structure of the definable set
families is a bit more complex. There has been much research work focusing on
this topic. However, as a fundamental issue in relation based rough sets, under
what condition two relations induce the same definable set family has not been
discussed. In this paper, based on the concept of the closure of relations, we
present a necessary and sufficient condition for two relations to induce the
same definable set family.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2408</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2408</id><created>2013-10-09</created><authors><author><keyname>Zhu</keyname><forenames>Jun</forenames></author><author><keyname>Zheng</keyname><forenames>Xun</forenames></author><author><keyname>Zhang</keyname><forenames>Bo</forenames></author></authors><title>Improved Bayesian Logistic Supervised Topic Models with Data
  Augmentation</title><categories>cs.LG cs.CL stat.AP stat.ML</categories><comments>9 pages, ACL 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Supervised topic models with a logistic likelihood have two issues that
potentially limit their practical use: 1) response variables are usually
over-weighted by document word counts; and 2) existing variational inference
methods make strict mean-field assumptions. We address these issues by: 1)
introducing a regularization constant to better balance the two parts based on
an optimization formulation of Bayesian inference; and 2) developing a simple
Gibbs sampling algorithm by introducing auxiliary Polya-Gamma variables and
collapsing out Dirichlet variables. Our augment-and-collapse sampling algorithm
has analytical forms of each conditional distribution without making any
restricting assumptions and can be easily parallelized. Empirical results
demonstrate significant improvements on prediction performance and time
efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2409</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2409</id><created>2013-10-09</created><authors><author><keyname>Chen</keyname><forenames>Ning</forenames></author><author><keyname>Zhu</keyname><forenames>Jun</forenames></author><author><keyname>Xia</keyname><forenames>Fei</forenames></author><author><keyname>Zhang</keyname><forenames>Bo</forenames></author></authors><title>Discriminative Relational Topic Models</title><categories>cs.LG cs.IR stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many scientific and engineering fields involve analyzing network data. For
document networks, relational topic models (RTMs) provide a probabilistic
generative process to describe both the link structure and document contents,
and they have shown promise on predicting network structures and discovering
latent topic representations. However, existing RTMs have limitations in both
the restricted model expressiveness and incapability of dealing with imbalanced
network data. To expand the scope and improve the inference accuracy of RTMs,
this paper presents three extensions: 1) unlike the common link likelihood with
a diagonal weight matrix that allows the-same-topic interactions only, we
generalize it to use a full weight matrix that captures all pairwise topic
interactions and is applicable to asymmetric networks; 2) instead of doing
standard Bayesian inference, we perform regularized Bayesian inference
(RegBayes) with a regularization parameter to deal with the imbalanced link
structure issue in common real networks and improve the discriminative ability
of learned latent representations; and 3) instead of doing variational
approximation with strict mean-field assumptions, we present collapsed Gibbs
sampling algorithms for the generalized relational topic models by exploring
data augmentation without making restricting assumptions. Under the generic
RegBayes framework, we carefully investigate two popular discriminative loss
functions, namely, the logistic log-loss and the max-margin hinge loss.
Experimental results on several real network datasets demonstrate the
significance of these extensions on improving the prediction performance, and
the time efficiency can be dramatically improved with a simple fast
approximation method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2410</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2410</id><created>2013-10-09</created><updated>2014-04-12</updated><authors><author><keyname>Song</keyname><forenames>Chao-Bing</forenames></author><author><keyname>Xia</keyname><forenames>Shu-Tao</forenames></author></authors><title>Sparse signal recovery by $\ell_q$ minimization under restricted
  isometry property</title><categories>cs.IT math.IT</categories><doi>10.1109/LSP.2014.2323238</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of compressed sensing, the nonconvex $\ell_q$ minimization
with $0&lt;q&lt;1$ has been studied in recent years. In this paper, by generalizing
the sharp bound for $\ell_1$ minimization of Cai and Zhang, we show that the
condition $\delta_{(s^q+1)k}&lt;\dfrac{1}{\sqrt{s^{q-2}+1}}$ in terms of
\emph{restricted isometry constant (RIC)} can guarantee the exact recovery of
$k$-sparse signals in noiseless case and the stable recovery of approximately
$k$-sparse signals in noisy case by $\ell_q$ minimization. This result is more
general than the sharp bound for $\ell_1$ minimization when the order of RIC is
greater than $2k$ and illustrates the fact that a better approximation to
$\ell_0$ minimization is provided by $\ell_q$ minimization than that provided
by $\ell_1$ minimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2418</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2418</id><created>2013-10-09</created><updated>2014-06-02</updated><authors><author><keyname>Leborgne</keyname><forenames>Aur&#xe9;lie</forenames></author><author><keyname>Mille</keyname><forenames>Julien</forenames></author><author><keyname>Tougne</keyname><forenames>Laure</forenames></author></authors><title>Linear Algorithm for Digital Euclidean Connected Skeleton</title><categories>cs.CV</categories><comments>This paper has been withdrawn by the author because it was not
  correct and not accepted by CVIU</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The skeleton is an essential shape characteristic providing a compact
representation of the studied shape. Its computation on the image grid raises
many issues. Due to the effects of discretization, the required properties of
the skeleton - thinness, homotopy to the shape, reversibility, connectivity -
may become incompatible. However, as regards practical use, the choice of a
specific skeletonization algorithm depends on the application. This allows to
classify the desired properties by order of importance, and tend towards the
most critical ones. Our goal is to make a skeleton dedicated to shape matching
for recognition. So, the discrete skeleton has to be thin - so that it can be
represented by a graph -, robust to noise, reversible - so that the initial
shape can be fully reconstructed - and homotopic to the shape. We propose a
linear-time skeletonization algorithm based on the squared Euclidean distance
map from which we extract the maximal balls and ridges. After a thinning and
pruning process, we obtain the skeleton. The proposed method is finally
compared to fairly recent methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2431</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2431</id><created>2013-10-09</created><authors><author><keyname>Dennis</keyname><forenames>Louise A.</forenames></author><author><keyname>Fisher</keyname><forenames>Michael</forenames></author><author><keyname>Lincoln</keyname><forenames>Nicholas K.</forenames></author><author><keyname>Lisitsa</keyname><forenames>Alexei</forenames></author><author><keyname>Veres</keyname><forenames>Sandor M.</forenames></author></authors><title>Practical Verification of Decision-Making in Agent-Based Autonomous
  Systems</title><categories>cs.LO cs.MA</categories><comments>Submitted to Journal of Automated Software Engineering</comments><msc-class>03B70</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a verification methodology for analysing the decision-making
component in agent-based hybrid systems. Traditionally hybrid automata have
been used to both implement and verify such systems, but hybrid automata based
modelling, programming and verification techniques scale poorly as the
complexity of discrete decision-making increases making them unattractive in
situations where complex logical reasoning is required. In the programming of
complex systems it has, therefore, become common to separate out logical
decision-making into a separate, discrete, component. However, verification
techniques have failed to keep pace with this development. We are exploring
agent-based logical components and have developed a model checking technique
for such components which can then be composed with a separate analysis of the
continuous part of the hybrid system. Among other things this allows program
model checkers to be used to verify the actual implementation of the
decision-making in hybrid autonomous systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2435</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2435</id><created>2013-10-09</created><authors><author><keyname>Guillaud</keyname><forenames>Maxime</forenames></author><author><keyname>Rezaee</keyname><forenames>Mohsen</forenames></author><author><keyname>Matz</keyname><forenames>Gerald</forenames></author></authors><title>Interference Alignment via Message-Passing</title><categories>cs.IT math.IT</categories><comments>Submitted to the IEEE International Conference on Communications
  (ICC) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce an iterative solution to the problem of interference alignment
(IA) over MIMO channels based on a message-passing formulation. We propose a
parameterization of the messages that enables the computation of IA precoders
by a min-sum algorithm over continuous variable spaces -- under this
parameterization, suitable approximations of the messages can be computed in
closed-form. We show that the iterative leakage minimization algorithm of
Cadambe et al. is a special case of our message-passing algorithm, obtained for
a particular schedule. Finally, we show that the proposed algorithm compares
favorably to iterative leakage minimization in terms of convergence speed, and
discuss a distributed implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2441</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2441</id><created>2013-10-09</created><authors><author><keyname>Gaurav</keyname><forenames>Kumar</forenames><affiliation>INRIA Paris-Rocquencourt, UPMC</affiliation></author><author><keyname>Blaszczyszyn</keyname><forenames>Bartlomiej</forenames><affiliation>INRIA Paris-Rocquencourt</affiliation></author><author><keyname>Keeler</keyname><forenames>Holger Paul</forenames><affiliation>INRIA Paris-Rocquencourt</affiliation></author></authors><title>Pioneers of Influence Propagation in Social Networks</title><categories>cs.SI cs.DM physics.soc-ph</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the growing importance of corporate viral marketing campaigns on online
social networks, the interest in studies of influence propagation through
networks is higher than ever. In a viral marketing campaign, a firm initially
targets a small set of pioneers and hopes that they would influence a sizeable
fraction of the population by diffusion of influence through the network. In
general, any marketing campaign might fail to go viral in the first try. As
such, it would be useful to have some guide to evaluate the effectiveness of
the campaign and judge whether it is worthy of further resources, and in case
the campaign has potential, how to hit upon a good pioneer who can make the
campaign go viral. In this paper, we present a diffusion model developed by
enriching the generalized random graph (a.k.a. configuration model) to provide
insight into these questions. We offer the intuition behind the results on this
model, rigorously proved in Blaszczyszyn &amp; Gaurav(2013), and illustrate them
here by taking examples of random networks having prototypical degree
distributions - Poisson degree distribution, which is commonly used as a kind
of benchmark, and Power Law degree distribution, which is normally used to
approximate the real-world networks. On these networks, the members are assumed
to have varying attitudes towards propagating the information. We analyze three
cases, in particular - (1) Bernoulli transmissions, when a member influences
each of its friend with probability p; (2) Node percolation, when a member
influences all its friends with probability p and none with probability 1-p;
(3) Coupon-collector transmissions, when a member randomly selects one of his
friends K times with replacement. We assume that the configuration model is the
closest approximation of a large online social network, when the information
available about the network is very limited. The key insight offered by this
study from a firm's perspective is regarding how to evaluate the effectiveness
of a marketing campaign and do cost-benefit analysis by collecting relevant
statistical data from the pioneers it selects. The campaign evaluation
criterion is informed by the observation that if the parameters of the
underlying network and the campaign effectiveness are such that the campaign
can indeed reach a significant fraction of the population, then the set of good
pioneers also forms a significant fraction of the population. Therefore, in
such a case, the firms can even adopt the naive strategy of repeatedly picking
and targeting some number of pioneers at random from the population. With this
strategy, the probability of them picking a good pioneer will increase
geometrically fast with the number of tries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2447</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2447</id><created>2013-10-09</created><updated>2014-07-23</updated><authors><author><keyname>Koiran</keyname><forenames>Pascal</forenames><affiliation>LIP</affiliation></author><author><keyname>Portier</keyname><forenames>Natacha</forenames><affiliation>LIP</affiliation></author><author><keyname>Tavenas</keyname><forenames>S&#xe9;bastien</forenames><affiliation>LIP</affiliation></author></authors><title>On the intersection of a sparse curve and a low-degree curve: A
  polynomial version of the lost theorem</title><categories>cs.CC math.AG</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a system of two polynomial equations in two variables:
$$F(X,Y)=G(X,Y)=0$$ where $F \in \rr[X,Y]$ has degree $d \geq 1$ and $G \in
\rr[X,Y]$ has $t$ monomials. We show that the system has only $O(d^3t+d^2t^3)$
real solutions when it has a finite number of real solutions. This is the first
polynomial bound for this problem. In particular, the bounds coming from the
theory of fewnomials are exponential in $t$, and count only nondegenerate
solutions. More generally, we show that if the set of solutions is infinite, it
still has at most $O(d^3t+d^2t^3)$ connected components. By contrast, the
following question seems to be open: if $F$ and $G$ have at most $t$ monomials,
is the number of (nondegenerate) solutions polynomial in $t$? The authors'
interest for these problems was sparked by connections between lower bounds in
algebraic complexity theory and upper bounds on the number of real roots of
&quot;sparse like&quot; polynomials.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2449</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2449</id><created>2013-10-09</created><updated>2013-12-25</updated><authors><author><keyname>De Castro</keyname><forenames>Rodrigo</forenames></author><author><keyname>Ram&#xed;rez</keyname><forenames>Andr&#xe9;s L.</forenames></author><author><keyname>Ram&#xed;rez</keyname><forenames>Jos&#xe9; L.</forenames></author></authors><title>Applications in Enumerative Combinatorics of Infinite Weighted Automata
  and Graphs</title><categories>cs.DM cs.FL math.CO</categories><msc-class>05A19, 05A15, 30B70, 68Q45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we studied infinite weighted automata and a general methodology
to solve a wide variety of classical lattice path counting problems in an
uniform way. This counting problems are related to Dyck paths, Motzkin paths
and some generalizations. These methodology uses weighted automata, equations
of ordinary generating functions and continued fractions. It is a variation of
the one proposed by J. Rutten.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2451</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2451</id><created>2013-10-09</created><authors><author><keyname>Audiffren</keyname><forenames>Julien</forenames><affiliation>LIF</affiliation></author><author><keyname>Kadri</keyname><forenames>Hachem</forenames><affiliation>LIF</affiliation></author></authors><title>M-Power Regularized Least Squares Regression</title><categories>stat.ML cs.LG</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regularization is used to find a solution that both fits the data and is
sufficiently smooth, and thereby is very effective for designing and refining
learning algorithms. But the influence of its exponent remains poorly
understood. In particular, it is unclear how the exponent of the reproducing
kernel Hilbert space (RKHS) regularization term affects the accuracy and the
efficiency of kernel-based learning algorithms. Here we consider regularized
least squares regression (RLSR) with an RKHS regularization raised to the power
of m, where m is a variable real exponent. We design an efficient algorithm for
solving the associated minimization problem, we provide a theoretical analysis
of its stability, and we {compare it %/ demonstrate its advantage with respect
to computational complexity, speed of convergence and prediction accuracy to
%/over} the classical kernel ridge regression algorithm where the
regularization exponent m is fixed at 2. Our results show that the m-power RLSR
problem can be solved efficiently, and support the suggestion that one can use
a regularization term that grows significantly slower than the standard
quadratic growth in the RKHS norm.}
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2452</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2452</id><created>2013-10-09</created><authors><author><keyname>Sharma</keyname><forenames>Iti</forenames></author></authors><title>Fully Homomorphic Encryption Scheme with Symmetric Keys</title><categories>cs.CR</categories><comments>64 pages, dissertation report</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Homomorphic encryption has largely been studied in context of public key
cryptosystems. But there are applications which inherently would require
symmetric keys. We propose a symmetric key encryption scheme with fully
homomorphic evaluation capabilities. The operations are matrix based, that is
the scheme consists of mapping the operations on integers to operations on
matrix. We also include a protocol which uses the proposed scheme for private
data processing in clouds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2456</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2456</id><created>2013-10-09</created><authors><author><keyname>Sparrer</keyname><forenames>Susanne</forenames></author><author><keyname>Fischer</keyname><forenames>Robert F. H.</forenames></author></authors><title>Discrete Sparse Signals: Compressed Sensing by Combining OMP and the
  Sphere Decoder</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the reconstruction of discrete-valued sparse signals from
underdetermined systems of linear equations. On the one hand, classical
compressed sensing (CS) is designed to deal with real-valued sparse signals. On
the other hand, algorithms known from MIMO communications, especially the
sphere decoder (SD), are capable to reconstruct discrete-valued non-sparse
signals from well- or overdefined system of linear equations. Hence, a
combination of both approaches is required. We discuss strategies to include
the knowledge of the discrete nature of the signal in the reconstruction
process. For brevity, the exposition is done for combining the orthogonal
matching pursuit (OMP) with the SD; design guidelines are derived. It is shown
that by suitably combining OMP and SD an efficient low-complexity scheme for
the detection of discrete sparse signals is obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2464</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2464</id><created>2013-10-09</created><authors><author><keyname>Riccobene</keyname><forenames>Elvinia</forenames></author></authors><title>Eclipse-IT 2013: Proceedings of VIII Workshop of the Italian Eclipse
  Community</title><categories>cs.SE</categories><comments>conference eclipse it 2013 - Crema 19-20 September 2013 - ISBN:
  978-88-904388-4-4</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the extended abstracts of the contributions presented at
EclipseIT 2013, the 8th workshop of the Italian Eclipse Community, hosted by
the Computer Science Department of the University of Milan (Crema Campus) on
September 19-20, 2013. Although Eclipse was initially designed as an integrated
development environment (IDE) for object-oriented application development,
today it represents an open development platform comprised of extensible
frameworks, tools and runtimes for building, deploying and managing software.
Around Eclipse, an international live community continuously works on improving
the framework and on promoting the use of Eclipse. That happens also in Italy.
This workshop is, indeed, the eighth yearly meeting of the Italian Eclipse
Community which includes universities, public institutions and industries,
researchers and practitioners, students and professionals, all joined by the
interest in experimenting, extending, and supporting the Eclipse platform. The
special topic of this edition is the Software cooperative development for
mobile applications. Two tutorials are offered on this theme: (1) Sviluppo di
applicazioni enterprise per il mobile con IBM Jazz, Eclipse e Worklight by
Ferdinando Gorga from IBM, and (2) Uso di Eclipse per lo sviluppo cooperativo
del software, by Paolo Maresca of the University of Naples, Federico II.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2468</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2468</id><created>2013-10-09</created><authors><author><keyname>Golovinski</keyname><forenames>P. A.</forenames></author></authors><title>Scenarios of Destruction for Large Network and Increasing Reliability</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Damage scenarios for large networks are considered. The cascade scenario is
described by means of powers of adjacency matrix. More difficult probabilistic
variants of the large network damage are modeling by Markov chains. For
reliability augmentation of networks we add a set of random intermediate agents
with big dimensionality. It provides high reliability of all system even with
low reliability of single components. Probabilistic estimation of reliability
for reinforced network is made.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2473</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2473</id><created>2013-10-08</created><authors><author><keyname>Giacomelli</keyname><forenames>Irene</forenames></author></authors><title>Improved Decoding Algorithms for Reed-Solomon Codes</title><categories>cs.IT math.IT</categories><comments>Master thesis</comments><msc-class>94B35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In coding theory, Reed-Solomon codes are one of the most well-known and
widely used classes of error-correcting codes. In this thesis we study and
compare two major strategies known for their decoding procedure, the
Peterson-Gorenstein-Zierler (PGZ) and the Berlekamp-Massey (BM) decoder, in
order to improve existing decoding algorithms and propose faster new ones. In
particular we study a modified version of the PGZ decoder, which we will call
the fast Peterson-Gorenstein-Zierler (fPGZ) decoding algorithm. This
improvement was presented in 1997 by exploiting the Hankel structure of the
syndrome matrix. In this thesis we show that the fPGZ decoding algorithm can be
seen as a particular case of the BM one. Indeed we prove that the intermediate
outcomes obtained in the implementation of fPGZ are a subset of those of the BM
decoding algorithm. In this way, we also uncover the existing relationship
between the leading principal minors of syndrome matrix and the discrepancies
computed by the BM algorithm. Finally, thanks to the study done on the
structure of the syndrome matrix and its leading principal minors, we improve
the error value computation in both the decoding strategies studied
(specifically we prove new error value formulas for the fPGZ and the BM
decoding algorithm) and moreover we state a new iterative formulation of the
PGZ decoder well suited to a parallel implementation on integrated microchips.
Thus using techniques of linear algebra we obtain a parallel decoding algorithm
for Reed-Solomon codes with an O(e) computational time complexity, where e is
the number of errors which occurred, although a fairly large number of
elementary circuit elements is needed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2474</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2474</id><created>2013-10-09</created><updated>2014-01-23</updated><authors><author><keyname>Devroey</keyname><forenames>Xavier</forenames></author><author><keyname>Cordy</keyname><forenames>Maxime</forenames></author><author><keyname>Perrouin</keyname><forenames>Gilles</forenames></author><author><keyname>Schobbens</keyname><forenames>Pierre-Yves</forenames></author><author><keyname>Legay</keyname><forenames>Axel</forenames></author><author><keyname>Heymans</keyname><forenames>Patrick</forenames></author></authors><title>Towards Statistical Prioritization for Software Product Lines Testing</title><categories>cs.SE</categories><comments>Extended version published at VaMoS '14
  (http://dx.doi.org/10.1145/2556624.2556635)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software Product Lines (SPL) are inherently difficult to test due to the
combinatorial explosion of the number of products to consider. To reduce the
number of products to test, sampling techniques such as combinatorial
interaction testing have been proposed. They usually start from a feature model
and apply a coverage criterion (e.g. pairwise feature interaction or
dissimilarity) to generate tractable, fault-finding, lists of configurations to
be tested. Prioritization can also be used to sort/generate such lists,
optimizing coverage criteria or weights assigned to features. However, current
sampling/prioritization techniques barely take product behavior into account.
We explore how ideas of statistical testing, based on a usage model (a Markov
chain), can be used to extract configurations of interest according to the
likelihood of their executions. These executions are gathered in featured
transition systems, compact representation of SPL behavior. We discuss possible
scenarios and give a prioritization procedure illustrated on an example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2477</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2477</id><created>2013-10-09</created><authors><author><keyname>Michel</keyname><forenames>Lo&#xef;c</forenames></author><author><keyname>Michiels</keyname><forenames>Wim</forenames></author><author><keyname>Boucher</keyname><forenames>Xavier</forenames></author></authors><title>Model-free control of nonlinear power converters</title><categories>cs.SY math.OC</categories><comments>4 pages, 5 figures - Accepted to IEEE CCECE 2013. arXiv admin note:
  substantial text overlap with arXiv:1104.0215</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  A new &quot;model-free&quot; control methodology is applied to a boost power converter.
The properties of the boost converter allow to evaluate the performances of the
model-free strategy in the case of switching nonlinear transfer functions,
regarding load variations. Our approach, which utilizes &quot;intelligent&quot; PI
controllers, does not require any converter model identification while ensuring
the stability and the robustness of the controlled system. Simulation results
show that, with a simple control structure, the proposed control method is
almost insensitive to fluctuations and large load variations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2479</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2479</id><created>2013-10-09</created><updated>2013-11-02</updated><authors><author><keyname>Alis</keyname><forenames>Christian M.</forenames></author><author><keyname>Lim</keyname><forenames>May T.</forenames></author></authors><title>Spatio-temporal variation of conversational utterances on Twitter</title><categories>physics.soc-ph cs.CL cs.SI</categories><comments>13 pages, 7 figures, published in PLoS One</comments><journal-ref>PLoS ONE 8: e77793</journal-ref><doi>10.1371/journal.pone.0077793</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Conversations reflect the existing norms of a language. Previously, we found
that utterance lengths in English fictional conversations in books and movies
have shortened over a period of 200 years. In this work, we show that this
shortening occurs even for a brief period of 3 years (September 2009-December
2012) using 229 million utterances from Twitter. Furthermore, the subset of
geographically-tagged tweets from the United States show an inverse proportion
between utterance lengths and the state-level percentage of the Black
population. We argue that shortening of utterances can be explained by the
increasing usage of jargon including coined words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2488</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2488</id><created>2013-10-09</created><authors><author><keyname>Fischione</keyname><forenames>Carlo</forenames></author></authors><title>Teaching Wireless Sensor Networks: An Holistic Approach Bridging Theory
  and Practice at the Master Level</title><categories>cs.CY cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Sensor Networks (WSNs) are a new technology that has received a
substantial attention from several academic research fields in the last years.
There are many applications of WSNs, including environmental monitoring,
industrial automation, intelligent transportation systems, healthcare and
wellbeing, smart energy, to mention a few. Courses have been introduced both at
the PhD and at the Master levels. However, these existing courses focus on
particular aspects of WSNs (Networking, or Signal Processing, or Embedded
Software), whereas WSNs encompass disciplines traditionally separated in
Electrical Engineering and Computer Sciences. This paper gives two original
contributions: the essential knowledge that should be brought in a WSNs course
is characterized, and a course structure with an harmonious holistic approach
is proposed. A method based on both theory and experiments is illustrated for
the design of this course, whereby the students have hands-on to implement,
understand, and develop in practice the implications of theoretical concepts.
Theory and applications are thus considered all together. Ultimately, the
objective of this paper is to design a new course, to use innovative hands-on
experiments to illustrate the theoretical concepts in the course, to show that
theoretical aspects are essential for the solution of real-life engineering
WSNs problems, and finally to create a fun and interesting teaching and
learning environments for WSNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2490</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2490</id><created>2013-10-09</created><updated>2014-11-05</updated><authors><author><keyname>Koliander</keyname><forenames>G&#xfc;nther</forenames></author><author><keyname>Riegler</keyname><forenames>Erwin</forenames></author><author><keyname>Durisi</keyname><forenames>Giuseppe</forenames></author><author><keyname>Hlawatsch</keyname><forenames>Franz</forenames></author></authors><title>Degrees of Freedom of Generic Block-Fading MIMO Channels without A
  Priori Channel State Information</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Trans. Inf. Theory</comments><msc-class>94A15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We studynthe high-SNR capacity of generic MIMO Rayleigh block-fading channels
in the noncoherent setting where neither transmitter nor receiver has a priori
channel state information but both are aware of the channel statistics. In
contrast to the well-established constant block-fading model, we allow the
fading to vary within each block with a temporal correlation that is &quot;generic&quot;
(in the sense used in the interference-alignment literature). We show that the
number of degrees of freedom of a generic MIMO Rayleigh block-fading channel
with $T$ transmit antennas and block length $N$ is given by $T(1-1/N)$ provided
that $T&lt;N$ and the number of receive antennas is at least $T(N-1)/(N-T)$. A
comparison with the constant block-fading channel (where the fading is constant
within each block) shows that, for large block lengths, generic correlation
increases the number of degrees of freedom by a factor of up to four.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2491</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2491</id><created>2013-10-09</created><authors><author><keyname>Janota</keyname><forenames>Mikolas</forenames></author><author><keyname>Grigore</keyname><forenames>Radu</forenames></author><author><keyname>Marques-Silva</keyname><forenames>Joao</forenames></author></authors><title>On QBF Proofs and Preprocessing</title><categories>cs.LO</categories><comments>LPAR 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  QBFs (quantified boolean formulas), which are a superset of propositional
formulas, provide a canonical representation for PSPACE problems. To overcome
the inherent complexity of QBF, significant effort has been invested in
developing QBF solvers as well as the underlying proof systems. At the same
time, formula preprocessing is crucial for the application of QBF solvers. This
paper focuses on a missing link in currently-available technology: How to
obtain a certificate (e.g. proof) for a formula that had been preprocessed
before it was given to a solver? The paper targets a suite of commonly-used
preprocessing techniques and shows how to reconstruct certificates for them. On
the negative side, the paper discusses certain limitations of the
currently-used proof systems in the light of preprocessing. The presented
techniques were implemented and evaluated in the state-of-the-art QBF
preprocessor bloqqer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2493</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2493</id><created>2013-10-09</created><authors><author><keyname>Vouros</keyname><forenames>George A.</forenames></author><author><keyname>Santipantakis</keyname><forenames>Georgios</forenames></author></authors><title>Combining Ontologies with Correspondences and Link Relations: The E-SHIQ
  Representation Framework</title><categories>cs.AI</categories><acm-class>I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Combining knowledge and beliefs of autonomous peers in distributed settings,
is a ma- jor challenge. In this paper we consider peers that combine ontologies
and reason jointly with their coupled knowledge. Ontologies are within the SHIQ
fragment of Description Logics. Although there are several representation
frameworks for modular Description Log- ics, each one makes crucial assumptions
concerning the subjectivity of peers' knowledge, the relation between the
domains over which ontologies are interpreted, the expressivity of the
constructors used for combining knowledge, and the way peers share their
knowledge. However in settings where autonomous peers can evolve and extend
their knowledge and beliefs independently from others, these assumptions may
not hold. In this article, we moti- vate the need for a representation
framework that allows peers to combine their knowledge in various ways,
maintaining the subjectivity of their own knowledge and beliefs, and that
reason collaboratively, constructing a tableau that is distributed among them,
jointly. The paper presents the proposed E-SHIQ representation framework, the
implementation of the E-SHIQ distributed tableau reasoner, and discusses the
efficiency of this reasoner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2494</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2494</id><created>2013-10-09</created><authors><author><keyname>Blin</keyname><forenames>L&#xe9;lia</forenames></author></authors><title>Algorithmes auto-stabilisants pour la construction d'arbres couvrants et
  la gestion d'entit\'es autonomes</title><categories>cs.DC</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In the context of large-scale networks, the consideration of faults is an
evident necessity. This document is focussing on the self-stabilizing approach
which aims at conceiving algorithms &quot;repairing themselves&quot; in case of transient
faults, that is of faults implying an arbitrary modification of the states of
the processes. The document focuses on two different contexts, covering the
major part of my research work these last years. The first part of the document
is dedicated to the design and analysis of self-stabilizing algorithms for
networks of processes. The second part of the document is dedicated to the
design and analysis of self-stabilizing algorithms for autonomous entities
(i.e., software agents, robots, etc.) moving in a network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2514</identifier>
 <datestamp>2014-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2514</id><created>2013-10-09</created><updated>2014-01-17</updated><authors><author><keyname>Fu</keyname><forenames>Hongfei</forenames></author></authors><title>Maximal Cost-Bounded Reachability Probability on Continuous-Time Markov
  Decision Processes</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider multi-dimensional maximal cost-bounded
reachability probability over continuous-time Markov decision processes
(CTMDPs). Our major contributions are as follows. Firstly, we derive an
integral characterization which states that the maximal cost-bounded
reachability probability function is the least fixed point of a system of
integral equations. Secondly, we prove that the maximal cost-bounded
reachability probability can be attained by a measurable deterministic
cost-positional scheduler. Thirdly, we provide a numerical approximation
algorithm for maximal cost-bounded reachability probability. We present these
results under the setting of both early and late schedulers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2518</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2518</id><created>2013-10-09</created><updated>2014-11-14</updated><authors><author><keyname>Bisht</keyname><forenames>Mayank</forenames><affiliation>University College of Engineering, Rajasthan Technical University, INDIA</affiliation></author><author><keyname>Gupta</keyname><forenames>C. P.</forenames><affiliation>University College of Engineering, Rajasthan Technical University, INDIA</affiliation></author></authors><title>Distance energy based routing algorithm with priority handling for UWSN</title><categories>cs.NI</categories><comments>This paper has been withdrawn due illegal uses by users on other
  websites</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Under Water Sensor Networks (UWSN) have different characteristics than
terrestrial sensor networks, so routing protocols developed for terrestrial
networks are not suitable for UWSN. Underwater routing is comprehended by the
underlying network architecture and design of sensor nodes. Therefore,
efficient routing protocols need to be developed for UWSNs. We present a
Routing scheme based on routing factor (Rf) which is a function of distance and
residual energy. We also define Energy Scale Value to rationalize the available
energy to take heterogeneity in energies into account. The algorithm routes a
packet from one node to another by computing Rf for each neighboring node and
selecting the node with minimum Rf as forwarding node, thereby providing an
energy balancing mechanism among a set of candidate forwarding nodes. We have
also included priority packets in our algorithm which are forwarded in a way so
as to reduce the end to end delays between the packets and at respective nodes.
Simulations of our proposed routing algorithm shows improvement in terms of
network lifetime and number of packets delivered. Our simulations show
improvement in lifetime by a factor of 2 when analyzed with Sector Based
Routing(SBR-DLP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2527</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2527</id><created>2013-10-08</created><authors><author><keyname>Amblard</keyname><forenames>Maxime</forenames><affiliation>INRIA Nancy - Grand Est / LORIA, MSH Lorraine</affiliation></author></authors><title>Treating clitics with minimalist grammars</title><categories>cs.CL cs.LO</categories><comments>The 11th conference on Formal Grammar, Malaga : Spain (2006)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an extension of Stabler's version of clitics treatment for a wider
coverage of the French language. For this, we present the lexical entries
needed in the lexicon. Then, we show the recognition of complex syntactic
phenomena as (left and right) dislo- cation, clitic climbing over modal and
extraction from determiner phrase. The aim of this presentation is the
syntax-semantic interface for clitics analyses in which we will stress on
clitic climbing over verb and raising verb.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2539</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2539</id><created>2013-10-09</created><updated>2014-09-12</updated><authors><author><keyname>Barrau</keyname><forenames>Axel</forenames></author><author><keyname>Bonnabel</keyname><forenames>Silvere</forenames></author></authors><title>Intrinsic filtering on Lie groups with applications to attitude
  estimation</title><categories>cs.SY cs.RO math.OC</categories><comments>Submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a probabilistic approach to the problem of intrinsic
filtering of a system on a matrix Lie group with invariance properties. The
problem of an invariant continuous-time model with discrete-time measurements
is cast into a rigorous stochastic and geometric framework. Building upon the
theory of continuous-time invariant observers, we show that, as in the linear
case, the error equation is a Markov chain that does not depend on the state
estimate. Thus, when the filter's gains are held fixed, and the filter admits
almost-global convergence properties with noise turned off, the noisy error's
distribution is proved to converge to a stationary distribution, providing
insight into the mathematical theory of filtering on Lie groups. For
engineering purposes we also introduce the discrete-time Invariant Extended
Kalman Filter, for which the trusted covariance matrix is shown to
asymptotically converge, and some numerically more involved sample-based
methods as well to compute the Kalman gains. The methods are applied to
attitude estimation, allowing to derive novel theoretical results in this
field, and illustrated through simulations on synthetic data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2545</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2545</id><created>2013-10-09</created><updated>2014-11-25</updated><authors><author><keyname>Kayes</keyname><forenames>Imrul</forenames></author><author><keyname>Sarker</keyname><forenames>Mithun</forenames></author><author><keyname>Chakareski</keyname><forenames>Jacob</forenames></author></authors><title>Product Backlog Rating: A Case Study On Measuring Test Quality In Scrum</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Agile software development methodologies focus on software projects which are
behind schedule or highly likely to have a problematic development phase. In
the last decade, Agile methods have transformed from cult techniques to
mainstream methodologies. Scrum, an Agile software development method, has been
widely adopted due to its adaptive nature.
  This paper presents a metric that measures the quality of the testing process
in a Scrum process. As product quality and process quality correlate, improved
test quality can ensure high quality products. Also, gaining experience from
eight years of successful Scrum implementation at SoftwarePeople, we describe
the Scrum process emphasizing the testing process. We propose a metric Product
Backlog Rating (PBR) to assess the testing process in Scrum. PBR considers the
complexity of the features to be developed in an iteration of Scrum, assesses
test ratings and offers a numerical score of the testing process. This metric
is able to provide a comprehensive overview of the testing process over the
development cycle of a product.
  We present a case study which shows how the metric is used at SoftwarePeople.
The case study explains some features that have been developed in a Sprint in
terms of feature complexity and potential test assessment difficulties and
shows how PBR is calculated during the Sprint. We propose a test process
assessment metric that provides insights into the Scrum testing process.
However, the metric needs further evaluation considering associated resources
(e.g., quality assurance engineers, the length of the Scrum cycle).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2547</identifier>
 <datestamp>2013-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2547</id><created>2013-10-09</created><updated>2013-10-09</updated><authors><author><keyname>Li</keyname><forenames>Muyuan</forenames></author><author><keyname>Zhu</keyname><forenames>Haojin</forenames></author><author><keyname>Gao</keyname><forenames>Zhaoyu</forenames></author><author><keyname>Chen</keyname><forenames>Si</forenames></author><author><keyname>Ren</keyname><forenames>Kui</forenames></author><author><keyname>Yu</keyname><forenames>Le</forenames></author><author><keyname>Hu</keyname><forenames>Shangqian</forenames></author></authors><title>All Your Location are Belong to Us: Breaking Mobile Social Networks for
  Automated User Location Tracking</title><categories>cs.SI cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many popular location-based social networks (LBSNs) support built-in
location-based social discovery with hundreds of millions of users around the
world. While user (near) realtime geographical information is essential to
enable location-based social discovery in LBSNs, the importance of user
location privacy has also been recognized by leading real-world LBSNs. To
protect user's exact geographical location from being exposed, a number of
location protection approaches have been adopted by the industry so that only
relative location information are publicly disclosed. These techniques are
assumed to be secure and are exercised on the daily base. In this paper, we
question the safety of these location-obfuscation techniques used by existing
LBSNs. We show, for the first time, through real world attacks that they can
all be easily destroyed by an attacker with the capability of no more than a
regular LBSN user. In particular, by manipulating location information fed to
LBSN client app, an ill-intended regular user can easily deduce the exact
location information by running LBSN apps as location oracle and performing a
series of attacking strategies. We develop an automated user location tracking
system and test it on the most popular LBSNs including Wechat, Skout and Momo.
We demonstrate its effectiveness and efficiency via a 3 week real-world
experiment with 30 volunteers. Our evaluation results show that we could
geo-locate a target with high accuracy and can readily recover users' Top 5
locations. We also propose to use grid reference system and location
classification to mitigate the attacks. Our work shows that the current
industrial best practices on user location privacy protection are completely
broken, and it is critical to address this immediate threat.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2561</identifier>
 <datestamp>2014-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2561</id><created>2013-10-09</created><updated>2014-04-17</updated><authors><author><keyname>Martin</keyname><forenames>Travis</forenames></author><author><keyname>Schoenebeck</keyname><forenames>Grant</forenames></author><author><keyname>Wellman</keyname><forenames>Michael P.</forenames></author></authors><title>Characterizing Strategic Cascades on Networks</title><categories>cs.SI cs.GT physics.soc-ph</categories><comments>To appear in EC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transmission of disease, spread of information and rumors, adoption of new
products, and many other network phenomena can be fruitfully modeled as
cascading processes, where actions chosen by nodes influence the subsequent
behavior of neighbors in the network graph. Current literature on cascades
tends to assume nodes choose myopically based on the state of choices already
taken by other nodes. We examine the possibility of strategic choice, where
agents representing nodes anticipate the choices of others who have not yet
decided, and take into account their own influence on such choices. Our study
employs the framework of Chierichetti et al. [2012], who (under assumption of
myopic node behavior) investigate the scheduling of node decisions to promote
cascades of product adoptions preferred by the scheduler. We show that when
nodes behave strategically, outcomes can be extremely different. We exhibit
cases where in the strategic setting 100% of agents adopt, but in the myopic
setting only an arbitrarily small epsilon % do. Conversely, we present cases
where in the strategic setting 0% of agents adopt, but in the myopic setting
(100-epsilon)% do, for any constant epsilon &gt; 0. Additionally, we prove some
properties of cascade processes with strategic agents, both in general and for
particular classes of graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2578</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2578</id><created>2013-10-09</created><updated>2014-01-09</updated><authors><author><keyname>Sanfelice</keyname><forenames>Ricardo G.</forenames></author><author><keyname>Yong</keyname><forenames>Sze Zheng</forenames></author><author><keyname>Frazzoli</keyname><forenames>Emilio</forenames></author></authors><title>On Minimum-time Paths of Bounded Curvature with Position-dependent
  Constraints</title><categories>math.OC cs.SY math.DS</categories><comments>Expanded version of paper in Automatica</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of a particle traveling from an initial configuration
to a final configuration (given by a point in the plane along with a prescribed
velocity vector) in minimum time with non-homogeneous velocity and with
constraints on the minimum turning radius of the particle over multiple regions
of the state space. Necessary conditions for optimality of these paths are
derived to characterize the nature of optimal paths, both when the particle is
inside a region and when it crosses boundaries between neighboring regions.
These conditions are used to characterize families of optimal and nonoptimal
paths. Among the optimality conditions, we derive a &quot;refraction&quot; law at the
boundary of the regions that generalizes the so-called Snell's law of
refraction in optics to the case of paths with bounded curvature. Tools
employed to deduce our results include recent principles of optimality for
hybrid systems. The results are validated numerically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2586</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2586</id><created>2013-10-09</created><authors><author><keyname>Espinas</keyname><forenames>J&#xe9;r&#xe9;my</forenames><affiliation>LIRIS</affiliation></author><author><keyname>Chaine</keyname><forenames>Rapha&#xeb;lle</forenames><affiliation>LIRIS</affiliation></author><author><keyname>Gandoin</keyname><forenames>Pierre-Marie</forenames><affiliation>LIRIS</affiliation></author></authors><title>Practical Reduction of Edge Flip Sequences in Two-Dimensional
  Triangulations</title><categories>cs.CG</categories><comments>15 pages, submitted in ESA 2013</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The development of laser scanning techniques has popularized the
representation of 3D shapes by triangular meshes with a large number of
vertices. Compression techniques dedicated to such meshes have emerged, which
exploit the idea that the connectivity of a dense mesh does not deviate much
from the connectivity that can be constructed automatically from the vertex
positions (while possibly being guided by additional codes). The edge flip is
one of the tools that can encode the differences between two meshes, and it is
important to control the length of a sequence of flips that transform one
triangulation into another. This paper provides a practical solution to this
problem. Indeed, the problem of determining a minimal sequence of edge flips
between two triangulations is NP-complete for some types of triangulations
including manifold triangulations of surfaces, so that it is necessary to
develop heuristics. Moreover, it is sometimes difficult to establish a first
sequence of flips between two meshes, and we advocate a solution based on the
reduction of an existing sequence. The new approach we propose is founded on
the assignment of labels to identify the edges, with a property of label
transfer during a flip. This gives a meaning to the tracking of an edge in a
sequence of flips and offers the exploitation of very simple combinatorial
properties. All the operations are performed directly on the sequence of labels
denoting the edges to be flipped, almost regardless of the underlying surface,
since only local temporary connectivity is involved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2592</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2592</id><created>2013-10-09</created><authors><author><keyname>Patterson</keyname><forenames>Stacy</forenames></author><author><keyname>Bamieh</keyname><forenames>Bassam</forenames></author></authors><title>Consensus and Coherence in Fractal Networks</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider first and second order consensus algorithms in networks with
stochastic disturbances. We quantify the deviation from consensus using the
notion of network coherence, which can be expressed as an $H_2$ norm of the
stochastic system. We use the setting of fractal networks to investigate the
question of whether a purely topological measure, such as the fractal
dimension, can capture the asymptotics of coherence in the large system size
limit. Our analysis for first-order systems is facilitated by connections
between first-order stochastic consensus and the global mean first passage time
of random walks. We then show how to apply similar techniques to analyze
second-order stochastic consensus systems. Our analysis reveals that two
networks with the same fractal dimension can exhibit different asymptotic
scalings for network coherence. Thus, this topological characterization of the
network does not uniquely determine coherence behavior. The question of whether
the performance of stochastic consensus algorithms in large networks can be
captured by purely topological measures, such as the spatial dimension, remains
open.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2619</identifier>
 <datestamp>2014-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2619</id><created>2013-10-09</created><updated>2014-03-19</updated><authors><author><keyname>Ghosh</keyname><forenames>Rumi</forenames></author><author><keyname>Huberman</keyname><forenames>Bernardo A.</forenames></author></authors><title>Information Relaxation is Ultradiffusive</title><categories>cs.SI cs.CY physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate how the overall response to a piece of information (a story or
an article) evolves and relaxes as a function of time in social networks like
Reddit, Digg and Youtube. This response or popularity is measured in terms of
the number of votes/comments that the story (or article) accrued over time. We
find that the temporal evolution of popularity can be described by a universal
function whose parameters depend upon the system under consideration. Unlike
most previous studies, which empirically investigated the dynamics of voting
behavior, we also give a theoretical interpretation of the observed behavior
using ultradiffusion.
  Whether it is the inter-arrival time between two consecutive votes on a story
on Reddit or the comments on a video shared on Youtube, there is always a
hierarchy of time scales in information propagation. One vote/comment might
occur almost simultaneously with the previous, whereas another vote/comment
might occur hours after the preceding one. This hierarchy of time scales leads
us to believe that the dynamical response of users to information is
ultradiffusive in nature. We show that a ultradiffusion based stochastic
process can be used to rationalize the observed temporal evolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2627</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2627</id><created>2013-10-09</created><updated>2015-11-07</updated><authors><author><keyname>Yogatama</keyname><forenames>Dani</forenames></author><author><keyname>Routledge</keyname><forenames>Bryan R.</forenames></author><author><keyname>Smith</keyname><forenames>Noah A.</forenames></author></authors><title>A Sparse and Adaptive Prior for Time-Dependent Model Parameters</title><categories>stat.ML cs.AI cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We consider the scenario where the parameters of a probabilistic model are
expected to vary over time. We construct a novel prior distribution that
promotes sparsity and adapts the strength of correlation between parameters at
successive timesteps, based on the data. We derive approximate variational
inference procedures for learning and prediction with this prior. We test the
approach on two tasks: forecasting financial quantities from relevant text, and
modeling language contingent on time-varying financial measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2631</identifier>
 <datestamp>2013-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2631</id><created>2013-10-09</created><authors><author><keyname>Hague</keyname><forenames>Matthew</forenames></author></authors><title>Saturation of Concurrent Collapsible Pushdown Systems</title><categories>cs.FL</categories><comments>Long version of paper appearing in FSTTCS 2013</comments><acm-class>F.1.1</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Multi-stack pushdown systems are a well-studied model of concurrent
computation using threads with first-order procedure calls. While, in general,
reachability is undecidable, there are numerous restrictions on stack behaviour
that lead to decidability. To model higher-order procedures calls, a
generalisation of pushdown stacks called collapsible pushdown stacks are
required. Reachability problems for multi-stack collapsible pushdown systems
have been little studied. Here, we study ordered, phase-bounded and
scope-bounded multi-stack collapsible pushdown systems using saturation
techniques, showing decidability of control state reachability and giving a
regular representation of all configurations that can reach a given control
state.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2632</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2632</id><created>2013-10-09</created><updated>2014-06-05</updated><authors><author><keyname>Parker</keyname><forenames>Jason T.</forenames></author><author><keyname>Schniter</keyname><forenames>Philip</forenames></author><author><keyname>Cevher</keyname><forenames>Volkan</forenames></author></authors><title>Bilinear Generalized Approximate Message Passing</title><categories>cs.IT math.IT</categories><doi>10.1109/TSP.2014.2357776</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend the generalized approximate message passing (G-AMP) approach,
originally proposed for high-dimensional generalized-linear regression in the
context of compressive sensing, to the generalized-bilinear case, which enables
its application to matrix completion, robust PCA, dictionary learning, and
related matrix-factorization problems. In the first part of the paper, we
derive our Bilinear G-AMP (BiG-AMP) algorithm as an approximation of the
sum-product belief propagation algorithm in the high-dimensional limit, where
central-limit theorem arguments and Taylor-series approximations apply, and
under the assumption of statistically independent matrix entries with known
priors. In addition, we propose an adaptive damping mechanism that aids
convergence under finite problem sizes, an expectation-maximization (EM)-based
method to automatically tune the parameters of the assumed priors, and two
rank-selection strategies. In the second part of the paper, we discuss the
specializations of EM-BiG-AMP to the problems of matrix completion, robust PCA,
and dictionary learning, and present the results of an extensive empirical
study comparing EM-BiG-AMP to state-of-the-art algorithms on each problem. Our
numerical results, using both synthetic and real-world datasets, demonstrate
that EM-BiG-AMP yields excellent reconstruction accuracy (often best in class)
while maintaining competitive runtimes and avoiding the need to tune
algorithmic parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2636</identifier>
 <datestamp>2013-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2636</id><created>2013-10-09</created><authors><author><keyname>Marvel</keyname><forenames>Seth A.</forenames></author><author><keyname>Martin</keyname><forenames>Travis</forenames></author><author><keyname>Doering</keyname><forenames>Charles R.</forenames></author><author><keyname>Lusseau</keyname><forenames>David</forenames></author><author><keyname>Newman</keyname><forenames>M. E. J.</forenames></author></authors><title>The small-world effect is a modern phenomenon</title><categories>physics.soc-ph cs.SI</categories><comments>8 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The &quot;small-world effect&quot; is the observation that one can find a short chain
of acquaintances, often of no more than a handful of individuals, connecting
almost any two people on the planet. It is often expressed in the language of
networks, where it is equivalent to the statement that most pairs of
individuals are connected by a short path through the acquaintance network.
Although the small-world effect is well-established empirically for
contemporary social networks, we argue here that it is a relatively recent
phenomenon, arising only in the last few hundred years: for most of mankind's
tenure on Earth the social world was large, with most pairs of individuals
connected by relatively long chains of acquaintances, if at all. Our
conclusions are based on observations about the spread of diseases, which
travel over contact networks between individuals and whose dynamics can give us
clues to the structure of those networks even when direct network measurements
are not available. As an example we consider the spread of the Black Death in
14th-century Europe, which is known to have traveled across the continent in
well-defined waves of infection over the course of several years. Using
established epidemiological models, we show that such wave-like behavior can
occur only if contacts between individuals living far apart are exponentially
rare. We further show that if long-distance contacts are exponentially rare,
then the shortest chain of contacts between distant individuals is on average a
long one. The observation of the wave-like spread of a disease like the Black
Death thus implies a network without the small-world effect.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2646</identifier>
 <datestamp>2013-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2646</id><created>2013-10-09</created><authors><author><keyname>Narang</keyname><forenames>Sunil K.</forenames></author><author><keyname>Gadde</keyname><forenames>Akshay</forenames></author><author><keyname>Sanou</keyname><forenames>Eduard</forenames></author><author><keyname>Ortega</keyname><forenames>Antonio</forenames></author></authors><title>Localized Iterative Methods for Interpolation in Graph Structured Data</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present two localized graph filtering based methods for
interpolating graph signals defined on the vertices of arbitrary graphs from
only a partial set of samples. The first method is an extension of previous
work on reconstructing bandlimited graph signals from partially observed
samples. The iterative graph filtering approach very closely approximates the
solution proposed in the that work, while being computationally more efficient.
As an alternative, we propose a regularization based framework in which we
define the cost of reconstruction to be a combination of smoothness of the
graph signal and the reconstruction error with respect to the known samples,
and find solutions that minimize this cost. We provide both a closed form
solution and a computationally efficient iterative solution of the optimization
problem. The experimental results on the recommendation system datasets
demonstrate effectiveness of the proposed methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2648</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2648</id><created>2013-10-09</created><updated>2014-02-02</updated><authors><author><keyname>Neely</keyname><forenames>Michael J.</forenames></author></authors><title>A Lyapunov Optimization Approach to Repeated Stochastic Games</title><categories>cs.GT</categories><comments>13 pages, this version fixes an incorrect statement of the previous
  arxiv version (see footnote 1, page 5 in current version for the correction)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a time-varying game with $N$ players. Every time slot,
players observe their own random events and then take a control action. The
events and control actions affect the individual utilities earned by each
player. The goal is to maximize a concave function of time average utilities
subject to equilibrium constraints. Specifically, participating players are
provided access to a common source of randomness from which they can optimally
correlate their decisions. The equilibrium constraints incentivize
participation by ensuring that players cannot earn more utility if they choose
not to participate. This form of equilibrium is similar to the notions of Nash
equilibrium and correlated equilibrium, but is simpler to attain. A Lyapunov
method is developed that solves the problem in an online \emph{max-weight}
fashion by selecting actions based on a set of time-varying weights. The
algorithm does not require knowledge of the event probabilities and has
polynomial convergence time. A similar method can be used to compute a standard
correlated equilibrium, albeit with increased complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2665</identifier>
 <datestamp>2013-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2665</id><created>2013-10-09</created><authors><author><keyname>Ferrara</keyname><forenames>Emilio</forenames></author><author><keyname>JafariAsbagh</keyname><forenames>Mohsen</forenames></author><author><keyname>Varol</keyname><forenames>Onur</forenames></author><author><keyname>Qazvinian</keyname><forenames>Vahed</forenames></author><author><keyname>Menczer</keyname><forenames>Filippo</forenames></author><author><keyname>Flammini</keyname><forenames>Alessandro</forenames></author></authors><title>Clustering Memes in Social Media</title><categories>cs.SI cs.CY physics.data-an physics.soc-ph</categories><comments>Proceedings of the 2013 IEEE/ACM International Conference on Advances
  in Social Networks Analysis and Mining (ASONAM'13), 2013</comments><doi>10.1145/2492517.2492530</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing pervasiveness of social media creates new opportunities to
study human social behavior, while challenging our capability to analyze their
massive data streams. One of the emerging tasks is to distinguish between
different kinds of activities, for example engineered misinformation campaigns
versus spontaneous communication. Such detection problems require a formal
definition of meme, or unit of information that can spread from person to
person through the social network. Once a meme is identified, supervised
learning methods can be applied to classify different types of communication.
The appropriate granularity of a meme, however, is hardly captured from
existing entities such as tags and keywords. Here we present a framework for
the novel task of detecting memes by clustering messages from large streams of
social data. We evaluate various similarity measures that leverage content,
metadata, network features, and their combinations. We also explore the idea of
pre-clustering on the basis of existing entities. A systematic evaluation is
carried out using a manually curated dataset as ground truth. Our analysis
shows that pre-clustering and a combination of heterogeneous features yield the
best trade-off between number of clusters and their quality, demonstrating that
a simple combination based on pairwise maximization of similarity is as
effective as a non-trivial optimization of parameters. Our approach is fully
automatic, unsupervised, and scalable for real-time detection of memes in
streaming data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2671</identifier>
 <datestamp>2013-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2671</id><created>2013-10-09</created><authors><author><keyname>Ferrara</keyname><forenames>Emilio</forenames></author><author><keyname>Varol</keyname><forenames>Onur</forenames></author><author><keyname>Menczer</keyname><forenames>Filippo</forenames></author><author><keyname>Flammini</keyname><forenames>Alessandro</forenames></author></authors><title>Traveling Trends: Social Butterflies or Frequent Fliers?</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>Proceedings of the first ACM conference on Online social networks,
  pp. 213-222, 2013</comments><doi>10.1145/2512938.2512956</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Trending topics are the online conversations that grab collective attention
on social media. They are continually changing and often reflect exogenous
events that happen in the real world. Trends are localized in space and time as
they are driven by activity in specific geographic areas that act as sources of
traffic and information flow. Taken independently, trends and geography have
been discussed in recent literature on online social media; although, so far,
little has been done to characterize the relation between trends and geography.
Here we investigate more than eleven thousand topics that trended on Twitter in
63 main US locations during a period of 50 days in 2013. This data allows us to
study the origins and pathways of trends, how they compete for popularity at
the local level to emerge as winners at the country level, and what dynamics
underlie their production and consumption in different geographic areas. We
identify two main classes of trending topics: those that surface locally,
coinciding with three different geographic clusters (East coast, Midwest and
Southwest); and those that emerge globally from several metropolitan areas,
coinciding with the major air traffic hubs of the country. These hubs act as
trendsetters, generating topics that eventually trend at the country level, and
driving the conversation across the country. This poses an intriguing
conjecture, drawing a parallel between the spread of information and diseases:
Do trends travel faster by airplane than over the Internet?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2686</identifier>
 <datestamp>2013-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2686</id><created>2013-10-09</created><authors><author><keyname>Lee</keyname><forenames>Wijik</forenames></author><author><keyname>Kim</keyname><forenames>Ji-Youp</forenames></author><author><keyname>No</keyname><forenames>Jong-Seon</forenames></author></authors><title>New Families of $p$-ary Sequences of Period $\frac{p^n-1}{2}$ With Low
  Maximum Correlation Magnitude</title><categories>cs.IT math.IT</categories><comments>9 page, no figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $p$ be an odd prime such that $p \equiv 3\;{\rm mod}\;4$ and $n$ be an
odd integer. In this paper, two new families of $p$-ary sequences of period $N
= \frac{p^n-1}{2}$ are constructed by two decimated $p$-ary m-sequences $m(2t)$
and $m(dt)$, where $d = 4$ and $d = (p^n + 1)/2=N+1$. The upper bound on the
magnitude of correlation values of two sequences in the family is derived using
Weil bound. Their upper bound is derived as $\frac{3}{\sqrt{2}}
\sqrt{N+\frac{1}{2}}+\frac{1}{2}$ and the family size is 4N, which is four
times the period of the sequence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2700</identifier>
 <datestamp>2013-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2700</id><created>2013-10-10</created><updated>2013-10-17</updated><authors><author><keyname>Weinstein</keyname><forenames>M.</forenames></author><author><keyname>Meirer</keyname><forenames>F.</forenames></author><author><keyname>Hume</keyname><forenames>A.</forenames></author><author><keyname>Sciau</keyname><forenames>Ph.</forenames></author><author><keyname>Shaked</keyname><forenames>G.</forenames></author><author><keyname>Hofstetter</keyname><forenames>R.</forenames></author><author><keyname>Persi</keyname><forenames>E.</forenames></author><author><keyname>Mehta</keyname><forenames>A.</forenames></author><author><keyname>Horn</keyname><forenames>D.</forenames></author></authors><title>Analyzing Big Data with Dynamic Quantum Clustering</title><categories>physics.data-an cs.LG physics.comp-ph</categories><comments>37 pages, 22 figures, 1 Table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How does one search for a needle in a multi-dimensional haystack without
knowing what a needle is and without knowing if there is one in the haystack?
This kind of problem requires a paradigm shift - away from hypothesis driven
searches of the data - towards a methodology that lets the data speak for
itself. Dynamic Quantum Clustering (DQC) is such a methodology. DQC is a
powerful visual method that works with big, high-dimensional data. It exploits
variations of the density of the data (in feature space) and unearths subsets
of the data that exhibit correlations among all the measured variables. The
outcome of a DQC analysis is a movie that shows how and why sets of data-points
are eventually classified as members of simple clusters or as members of - what
we call - extended structures. This allows DQC to be successfully used in a
non-conventional exploratory mode where one searches data for unexpected
information without the need to model the data. We show how this works for big,
complex, real-world datasets that come from five distinct fields: i.e., x-ray
nano-chemistry, condensed matter, biology, seismology and finance. These
studies show how DQC excels at uncovering unexpected, small - but meaningful -
subsets of the data that contain important information. We also establish an
important new result: namely, that big, complex datasets often contain
interesting structures that will be missed by many conventional clustering
techniques. Experience shows that these structures appear frequently enough
that it is crucial to know they can exist, and that when they do, they encode
important hidden information. In short, we not only demonstrate that DQC can be
flexibly applied to datasets that present significantly different challenges,
we also show how a simple analysis can be used to look for the needle in the
haystack, determine what it is, and find what this means.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2703</identifier>
 <datestamp>2013-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2703</id><created>2013-10-10</created><authors><author><keyname>He</keyname><forenames>Shiwen</forenames></author><author><keyname>Huang</keyname><forenames>Yongming</forenames></author><author><keyname>Jin</keyname><forenames>Shi</forenames></author><author><keyname>Yu</keyname><forenames>Fei</forenames></author><author><keyname>Yang</keyname><forenames>Luxi</forenames></author></authors><title>Max-Min Energy Efficient Beamforming for Multicell Multiuser Joint
  Transmission Systems</title><categories>cs.IT math.IT</categories><comments>12 pages, 2 figures, IEEE Communications Letters, accepted for
  publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy efficient communication technology has attracted much attention due to
the explosive growth of energy consumption in current wireless communication
systems. In this letter we focus on fairness-based energy efficiency and aim to
maximize the minimum user energy efficiency in the multicell multiuser joint
beamforming system, taking both dynamic and static power consumptions into
account. This optimization problem is a non-convex fractional programming
problem and hard to tackle. In order to find its solution, the original problem
is transformed into a parameterized polynomial subtractive form by exploiting
the relationship between the user rate and the minimum mean square error, and
using the fractional programming theorem. Furthermore, an iterative algorithm
with proved convergence is developed to achieve a near-optimal performance.
Numerical results validate the effectiveness of the proposed solution and show
that our algorithm significantly outperforms the max-min rate optimization
algorithm in terms of maximizing the minimum energy efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2711</identifier>
 <datestamp>2013-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2711</id><created>2013-10-10</created><updated>2013-12-04</updated><authors><author><keyname>Hajiaghayi</keyname><forenames>Mohammad T.</forenames></author><author><keyname>Khandekar</keyname><forenames>Rohit</forenames></author><author><keyname>Kortsarz</keyname><forenames>Guy</forenames></author></authors><title>Fixed Parameter Inapproximability for Clique and SetCover in Time
  Super-exponential in OPT</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider proving inapproximability in terms of OPT and thus
we base the foundations of fixed parameter inapproximability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2717</identifier>
 <datestamp>2014-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2717</id><created>2013-10-10</created><authors><author><keyname>Nagy</keyname><forenames>Tamas</forenames></author><author><keyname>Gingl</keyname><forenames>Zoltan</forenames></author></authors><title>Low-cost photoplethysmograph solutions using the Raspberry Pi</title><categories>cs.SY</categories><comments>14th IEEE International Symposium on Computational Intelligence and
  Informatics (CINTI 2013), November 19-21, 2013, Budapest, Hungary</comments><journal-ref>CINTI 2013 - 14th IEEE International Symposium on Computational
  Intelligence and Informatics, Proceedings 2013, Article number 6705184, Pages
  163-167</journal-ref><doi>10.1109/CINTI.2013.6705184</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Photoplethysmography is a prevalent, non-invasive heart monitoring method. In
this paper an implementation of photoplethysmography on the Raspberry Pi is
presented. Two modulation techniques are discussed, which make possible to
measure these signals by the Raspberry Pi, using an external sound card as A/D
converter. Furthermore, it is shown, how can digital signal processing improve
signal quality. The presented methods can be used in low-cost cardiac function
monitoring, in telemedicine applications and in education as well, since cheap
and current hardware are used. Full documentation and open-source software for
the measurement available:
http://www.noise.inf.u-szeged.hu/Instruments/raspberryplet/
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2728</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2728</id><created>2013-10-10</created><updated>2014-10-31</updated><authors><author><keyname>Coja-Oghlan</keyname><forenames>Amin</forenames></author><author><keyname>Panagiotou</keyname><forenames>Konstantinos</forenames></author></authors><title>The asymptotic $k$-SAT threshold</title><categories>math.CO cs.DM math.PR</categories><msc-class>05C80</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since the early 2000s physicists have developed an ingenious but non-rigorous
formalism called the cavity method to put forward precise conjectures on phase
transitions in random problems [Mezard, Parisi, Zecchina: Science 2002]. The
cavity method predicts that the satisfiability threshold in the random $k$-SAT
problem is $2^k\ln2-\frac12(1+\ln 2)+\epsilon_k$, with
$\lim_{k\rightarrow\infty}\epsilon_k=0$ [Mertens, Mezard, Zecchina: Random
Structures and Algorithms 2006]. This paper contains a proof of that
conjecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2741</identifier>
 <datestamp>2013-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2741</id><created>2013-10-10</created><authors><author><keyname>Chari</keyname><forenames>Guido</forenames><affiliation>LAFHIS</affiliation></author><author><keyname>Garbervetsky</keyname><forenames>Diego</forenames><affiliation>LAFHIS</affiliation></author><author><keyname>Bruni</keyname><forenames>Camillo</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Denker</keyname><forenames>Marcus</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Ducasse</keyname><forenames>St&#xe9;phane</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author></authors><title>Waterfall: Primitives Generation on the Fly</title><categories>cs.PL</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern languages are typically supported by managed runtimes (Virtual
Machines). Since VMs have to deal with many concepts such as memory management,
abstract execution model and scheduling, they tend to be very complex.
Additionally, VMs have to meet strong performance requirements. This demand of
performance is one of the main reasons why many VMs are built statically. Thus,
design decisions are frozen at compile time preventing changes at runtime. One
clear example is the impossibility to dynamically adapt or change primitives of
the VM once it has been compiled. In this work we present a toolchain that
allows for altering and configuring components such as primitives and plug-ins
at runtime. The main contribution is Waterfall, a dynamic and reflective
translator from Slang, a restricted subset of Smalltalk, to native code.
Waterfall generates primitives on demand and executes them on the fly. We
validate our approach by implementing dynamic primitive modification and
runtime customization of VM plug-ins.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2743</identifier>
 <datestamp>2013-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2743</id><created>2013-10-10</created><authors><author><keyname>Dufour-Lussier</keyname><forenames>Valmi</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author><author><keyname>Ber</keyname><forenames>Florence Le</forenames><affiliation>LHyGeS</affiliation></author><author><keyname>Lieber</keyname><forenames>Jean</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author><author><keyname>Martin</keyname><forenames>Laura</forenames><affiliation>ASTER Mirecourt</affiliation></author></authors><title>Case Adaptation with Qualitative Algebras</title><categories>cs.AI</categories><proxy>ccsd</proxy><journal-ref>International Joint Conferences on Artificial Intelligence (2013)
  3002-3006</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes an approach for the adaptation of spatial or temporal
cases in a case-based reasoning system. Qualitative algebras are used as
spatial and temporal knowledge representation languages. The intuition behind
this adaptation approach is to apply a substitution and then repair potential
inconsistencies, thanks to belief revision on qualitative algebras. A temporal
example from the cooking domain is given. (The paper on which this extended
abstract is based was the recipient of the best paper award of the 2012
International Conference on Case-Based Reasoning.)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2745</identifier>
 <datestamp>2013-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2745</id><created>2013-10-10</created><authors><author><keyname>de Verdi&#xe8;re</keyname><forenames>&#xc9;ric Colin</forenames></author><author><keyname>de Mesmay</keyname><forenames>Arnaud</forenames></author></authors><title>Testing Graph Isotopy on Surfaces</title><categories>cs.CG cs.DS math.GT</categories><comments>31 pages, to appear in Discrete and Computational Geometry</comments><msc-class>05C10, 57M15, 57N05, 68Q25, 68R10, 68W05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the following problem: Given two embeddings G_1 and G_2 of the
same abstract graph G on an orientable surface S, decide whether G_1 and G_2
are isotopic; in other words, whether there exists a continuous family of
embeddings between G_1 and G_2. We provide efficient algorithms to solve this
problem in two models. In the first model, the input consists of the
arrangement of G_1 (resp., G_2) with a fixed graph cellularly embedded on S;
our algorithm is linear in the input complexity, and thus, optimal. In the
second model, G_1 and G_2 are piecewise-linear embeddings in the plane minus a
finite set of points; our algorithm runs in O(n^{3/2}\log n) time, where n is
the complexity of the input. The graph isotopy problem is a natural variation
of the homotopy problem for closed curves on surfaces and on the punctured
plane, for which algorithms have been given by various authors; we use some of
these algorithms as a subroutine. As a by-product, we reprove the following
mathematical characterization, first observed by Ladegaillerie (1984): Two
graph embeddings are isotopic if and only if they are homotopic and congruent
by an oriented homeomorphism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2748</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2748</id><created>2013-10-10</created><updated>2013-12-10</updated><authors><author><keyname>Kim</keyname><forenames>Juhoon</forenames></author><author><keyname>Khalili</keyname><forenames>Ramin</forenames></author><author><keyname>Feldmann</keyname><forenames>Anja</forenames></author><author><keyname>Chen</keyname><forenames>Yung-Chih</forenames></author><author><keyname>Towsley</keyname><forenames>Don</forenames></author></authors><title>Multi-Source Multi-Path HTTP (mHTTP): A Proposal</title><categories>cs.NI</categories><comments>12 pages</comments><acm-class>C.2.0; C.4.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today, most devices have multiple network interfaces. Coupled with
wide-spread replication of popular content at multiple locations, this provides
substantial path diversity in the Internet. We propose Multi-source Multipath
HTTP, mHTTP, which takes advantage of all existing types of path diversity in
the Internet. mHTTP needs only client-side but not server-side or network
modifications as it is a receiver-oriented mechanism. Moreover, the
modifications are restricted to the socket interface. Thus, no changes are
needed to the applications or to the kernel.
  As mHTTP relies on HTTP range requests, it is specific to HTTP which accounts
for more than 60% of the Internet traffic. We implement mHTTP and study its
performance by conducting measurements over a testbed and in the wild. Our
results show that mHTTP indeed takes advantage of all types of path diversity
in the Internet, and that it is a viable alternative to Multipath TCP for HTTP
traffic. mHTTP decreases download times for large objects up to 50%, whereas it
does no harm to small object downloads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2773</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2773</id><created>2013-10-10</created><updated>2015-02-26</updated><authors><author><keyname>Pappas</keyname><forenames>Nikolaos</forenames></author><author><keyname>Kountouris</keyname><forenames>Marios</forenames></author><author><keyname>Ephremides</keyname><forenames>Anthony</forenames></author><author><keyname>Traganitis</keyname><forenames>Apostolos</forenames></author></authors><title>Relay-assisted Multiple Access with Full-duplex Multi-Packet Reception</title><categories>cs.IT cs.NI math.IT</categories><comments>Accepted for publication in the IEEE Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The effect of full-duplex cooperative relaying in a random access multiuser
network is investigated here. First, we model the self-interference incurred
due to full-duplex operation, assuming multi-packet reception capabilities for
both the relay and the destination node. Traffic at the source nodes is
considered saturated and the cooperative relay, which does not have packets of
its own, stores a source packet that it receives successfully in its queue when
the transmission to the destination has failed. We obtain analytical
expressions for key performance metrics at the relay, such as arrival and
service rates, stability conditions, and average queue length, as functions of
the transmission probabilities, the self interference coefficient, and the
links' outage probabilities. Furthermore, we study the impact of the relay node
and the self-interference coefficient on the per-user and aggregate throughput,
and the average delay per packet. We show that perfect self-interference
cancelation plays a crucial role when the SINR threshold is small, since it may
result to worse performance in throughput and delay comparing with the
half-duplex case. This is because perfect self-interference cancelation can
cause an unstable queue at the relay under some conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2775</identifier>
 <datestamp>2013-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2775</id><created>2013-10-10</created><authors><author><keyname>Romain</keyname><forenames>Absil</forenames></author><author><keyname>M&#xe9;lot</keyname><forenames>Hadrien</forenames></author></authors><title>On price of symmetrisation</title><categories>cs.DM math.CO</categories><comments>26 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the price of symmetrisation, a concept that aims to compare
fundamental differences (gap and quotient) between values of a given graph
invariant for digraphs and the values of the same invariant of the symmetric
versions of these digraphs. Basically, given some invariant our goal is to
characterise digraphs that maximise price of symmetrisation. In particular, we
show that for some invariants, as diameter or domination number, the problem is
easy.
  The main contribution of this paper is about (partial) results on the price
of symmetrisation of the average distance. It appears to be much more intricate
than the simple cases mentioned above. First, we state a conjecture about
digraphs that maximise this price of symmetrisation. Then, we prove that this
conjecture is true for some particular class of digraphs (called bags) but it
remains open for general digraphs. Moreover, we study several graph
transformations in order to remove some configurations that do not appear in
the conjectured extremal digraphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2778</identifier>
 <datestamp>2013-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2778</id><created>2013-10-10</created><authors><author><keyname>Bostan</keyname><forenames>Alin</forenames><affiliation>INRIA Saclay - Ile de France, MSR - INRIA</affiliation></author><author><keyname>Ch&#xe8;ze</keyname><forenames>Guillaume</forenames><affiliation>IMT</affiliation></author><author><keyname>Cluzeau</keyname><forenames>Thomas</forenames><affiliation>XLIM</affiliation></author><author><keyname>Weil</keyname><forenames>Jacques-Arthur</forenames><affiliation>XLIM</affiliation></author></authors><title>Efficient Algorithms for Computing Rational First Integrals and Darboux
  Polynomials of Planar Polynomial Vector Fields</title><categories>cs.SC cs.DS math.CA nlin.SI</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present fast algorithms for computing rational first integrals with
bounded degree of a planar polynomial vector field. Our approach is inspired by
an idea of Ferragut and Giacomini. We improve upon their work by proving that
rational first integrals can be computed via systems of linear equations
instead of systems of quadratic equations. This leads to a probabilistic
algorithm with arithmetic complexity $\bigOsoft(N^{2 \omega})$ and to a
deterministic algorithm solving the problem in $\bigOsoft(d^2N^{2 \omega+1})$
arithmetic operations, where $N$ denotes the given bound for the degree of the
rational first integral, and where $d \leq N$ is the degree of the vector
field, and $\omega$ the exponent of linear algebra. We also provide a fast
heuristic variant which computes a rational first integral, or fails, in
$\bigOsoft(N^{\omega+2})$ arithmetic operations. By comparison, the best
previous algorithm uses at least $d^{\omega+1}\, N^{4\omega +4}$ arithmetic
operations. We then show how to apply a similar method to the computation of
Darboux polynomials. The algorithms are implemented in a Maple package which is
available to interested readers with examples showing its efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2797</identifier>
 <datestamp>2013-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2797</id><created>2013-10-10</created><authors><author><keyname>Kaliszyk</keyname><forenames>Cezary</forenames></author><author><keyname>Urban</keyname><forenames>Josef</forenames></author></authors><title>Lemma Mining over HOL Light</title><categories>cs.AI cs.DL cs.LG cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large formal mathematical libraries consist of millions of atomic inference
steps that give rise to a corresponding number of proved statements (lemmas).
Analogously to the informal mathematical practice, only a tiny fraction of such
statements is named and re-used in later proofs by formal mathematicians. In
this work, we suggest and implement criteria defining the estimated usefulness
of the HOL Light lemmas for proving further theorems. We use these criteria to
mine the large inference graph of all lemmas in the core HOL Light library,
adding thousands of the best lemmas to the pool of named statements that can be
re-used in later proofs. The usefulness of the new lemmas is then evaluated by
comparing the performance of automated proving of the core HOL Light theorems
with and without such added lemmas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2805</identifier>
 <datestamp>2013-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2805</id><created>2013-10-10</created><authors><author><keyname>Kaliszyk</keyname><forenames>Cezary</forenames></author><author><keyname>Urban</keyname><forenames>Josef</forenames></author></authors><title>MizAR 40 for Mizar 40</title><categories>cs.AI cs.DL cs.LG cs.LO cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As a present to Mizar on its 40th anniversary, we develop an AI/ATP system
that in 30 seconds of real time on a 14-CPU machine automatically proves 40% of
the theorems in the latest official version of the Mizar Mathematical Library
(MML). This is a considerable improvement over previous performance of large-
theory AI/ATP methods measured on the whole MML. To achieve that, a large suite
of AI/ATP methods is employed and further developed. We implement the most
useful methods efficiently, to scale them to the 150000 formulas in MML. This
reduces the training times over the corpus to 1-3 seconds, allowing a simple
practical deployment of the methods in the online automated reasoning service
for the Mizar users (MizAR).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2806</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2806</id><created>2013-10-10</created><updated>2014-03-28</updated><authors><author><keyname>Vila</keyname><forenames>Jeremy</forenames></author><author><keyname>Schniter</keyname><forenames>Philip</forenames></author></authors><title>An Empirical-Bayes Approach to Recovering Linearly Constrained
  Non-Negative Sparse Signals</title><categories>cs.IT math.IT</categories><doi>10.1109/TSP.2014.2337841</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose two novel approaches to the recovery of an (approximately) sparse
signal from noisy linear measurements in the case that the signal is a priori
known to be non-negative and obey given linear equality constraints, such as
simplex signals. This problem arises in, e.g., hyperspectral imaging, portfolio
optimization, density estimation, and certain cases of compressive imaging. Our
first approach solves a linearly constrained non-negative version of LASSO
using the max-sum version of the generalized approximate message passing (GAMP)
algorithm, where we consider both quadratic and absolute loss, and where we
propose a novel approach to tuning the LASSO regularization parameter via the
expectation maximization (EM) algorithm. Our second approach is based on the
sum-product version of the GAMP algorithm, where we propose the use of a
Bernoulli non-negative Gaussian-mixture signal prior and a Laplacian
likelihood, and propose an EM-based approach to learning the underlying
statistical parameters. In both approaches, the linear equality constraints are
enforced by augmenting GAMP's generalized-linear observation model with
noiseless pseudo-measurements. Extensive numerical experiments demonstrate the
state-of-the-art performance of our proposed approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2809</identifier>
 <datestamp>2013-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2809</id><created>2013-10-10</created><authors><author><keyname>Bavirisetti</keyname><forenames>Teja Damodaram</forenames></author><author><keyname>Ganesan</keyname><forenames>Abhinav</forenames></author><author><keyname>Prasad</keyname><forenames>K.</forenames></author><author><keyname>Rajan</keyname><forenames>B. Sundar</forenames></author></authors><title>Precoding Based Network Alignment using Transform Approach for Acyclic
  Networks with Delay</title><categories>cs.IT math.IT</categories><comments>Single column, 62 pages, and 7 figures. Contains the results of
  arXiv:1103.3882v4 [cs.IT] and arXiv:1203.5915 [cs.IT], plus a few more
  results</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The algebraic formulation for linear network coding in acyclic networks with
the links having integer delay is well known. Based on this formulation, for a
given set of connections over an arbitrary acyclic network with integer delay
assumed for the links, the output symbols at the sink nodes, at any given time
instant, is a $\mathbb{F}_{p^m}$-linear combination of the input symbols across
different generations where, $\mathbb{F}_{p^m}$ denotes the field over which
the network operates ($p$ is prime and $m$ is a positive integer). We use
finite-field discrete fourier transform (DFT) to convert the output symbols at
the sink nodes, at any given time instant, into a $\mathbb{F}_{p^m}$-linear
combination of the input symbols generated during the same generation without
making use of memory at the intermediate nodes. We call this as transforming
the acyclic network with delay into {\em $n$-instantaneous networks} ($n$ is
sufficiently large). We show that under certain conditions, there exists a
network code satisfying sink demands in the usual (non-transform) approach if
and only if there exists a network code satisfying sink demands in the
transform approach. When the zero-interference conditions are not satisfied, we
propose three Precoding Based Network Alignment (PBNA) schemes for three-source
three-destination multiple unicast network with delays (3-S 3-D MUN-D) termed
as PBNA using transform approach and time-invariant local encoding coefficients
(LECs), PBNA using time-varying LECs, and PBNA using transform approach and
block time-varying LECs. Their feasibility conditions are then analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2814</identifier>
 <datestamp>2013-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2814</id><created>2013-10-10</created><authors><author><keyname>Gupta</keyname><forenames>Suyash</forenames></author><author><keyname>Nandivada</keyname><forenames>V. Krishna</forenames></author></authors><title>IMSuite: A Benchmark Suite for Simulating Distributed Algorithms</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Considering the diverse nature of real-world distributed applications that
makes it hard to identify a representative subset of distributed benchmarks, we
focus on their underlying distributed algorithms. We present and characterize a
new kernel benchmark suite (named IMSuite) that simulates some of the classical
distributed algorithms in task parallel languages. We present multiple
variations of our kernels, broadly categorized under two heads: (a) varying
synchronization primitives (with and without fine grain synchronization
primitives); and (b) varying forms of parallelization (data parallel and
recursive task parallel). Our characterization covers interesting aspects of
distributed applications such as distribution of remote communication requests,
number of synchronization, task creation, task termination and atomic
operations. We study the behavior (execution time) of our kernels by varying
the problem size, the number of compute threads, and the input configurations.
We also present an involved set of input generators and output validators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2816</identifier>
 <datestamp>2013-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2816</id><created>2013-10-10</created><authors><author><keyname>Zhu</keyname><forenames>Jun</forenames></author><author><keyname>Chen</keyname><forenames>Ning</forenames></author><author><keyname>Perkins</keyname><forenames>Hugh</forenames></author><author><keyname>Zhang</keyname><forenames>Bo</forenames></author></authors><title>Gibbs Max-margin Topic Models with Data Augmentation</title><categories>stat.ML cs.LG stat.CO stat.ME</categories><comments>35 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Max-margin learning is a powerful approach to building classifiers and
structured output predictors. Recent work on max-margin supervised topic models
has successfully integrated it with Bayesian topic models to discover
discriminative latent semantic structures and make accurate predictions for
unseen testing data. However, the resulting learning problems are usually hard
to solve because of the non-smoothness of the margin loss. Existing approaches
to building max-margin supervised topic models rely on an iterative procedure
to solve multiple latent SVM subproblems with additional mean-field assumptions
on the desired posterior distributions. This paper presents an alternative
approach by defining a new max-margin loss. Namely, we present Gibbs max-margin
supervised topic models, a latent variable Gibbs classifier to discover hidden
topic representations for various tasks, including classification, regression
and multi-task learning. Gibbs max-margin supervised topic models minimize an
expected margin loss, which is an upper bound of the existing margin loss
derived from an expected prediction rule. By introducing augmented variables
and integrating out the Dirichlet variables analytically by conjugacy, we
develop simple Gibbs sampling algorithms with no restricting assumptions and no
need to solve SVM subproblems. Furthermore, each step of the
&quot;augment-and-collapse&quot; Gibbs sampling algorithms has an analytical conditional
distribution, from which samples can be easily drawn. Experimental results
demonstrate significant improvements on time efficiency. The classification
performance is also significantly improved over competitors on binary,
multi-class and multi-label classification tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2823</identifier>
 <datestamp>2013-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2823</id><created>2013-10-09</created><authors><author><keyname>Aggarwal</keyname><forenames>Abhinav</forenames></author></authors><title>Introduction to Functional Grammars</title><categories>cs.FL</categories><comments>6 pages, Double column</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Formal grammars are extensively used in Computer Science and related fields
to study the rules which govern production of a language. The use of these
grammars can be extended beyond mere language production. One possibility is to
view these grammars as logical machines, similar to automata, which can be
modified to compute or help in computation, while also performing the basic
task of language production. The difference between such a modified grammar and
an automaton will then lie in the semantics of computation performed. It is
even possible for such a grammar to appear non-functional (when no language is
produced as a result of its productions), but in reality, it might be carrying
out important tasks. Such grammars have been named Functional Grammars
(including a special sub-category, called Virtual Grammars), and their
properties are studied in the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2830</identifier>
 <datestamp>2013-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2830</id><created>2013-10-07</created><updated>2013-10-24</updated><authors><author><keyname>Pokress</keyname><forenames>Shaileen Crawford</forenames></author><author><keyname>Veiga</keyname><forenames>Jos&#xe9; Juan Dominguez</forenames></author></authors><title>MIT App Inventor: Enabling Personal Mobile Computing</title><categories>cs.CY cs.HC</categories><comments>3 pages, 2 figures, PRoMoTo 2013 proceedings (arXiv:1309.5509)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MIT App Inventor is a drag-and-drop visual programming tool for designing and
building fully functional mobile apps for Android. App Inventor promotes a new
era of personal mobile computing in which people are empowered to design,
create, and use personally meaningful mobile technology solutions for their
daily lives, in endlessly unique situations. App Inventor's intuitive
programming metaphor and incremental development capabilities allow the
developer to focus on the logic for programming an app rather than the syntax
of the coding language, fostering digital literacy for all. Since it was moved
from Google to MIT, a number of improvements have been added, and research
projects are underway.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2832</identifier>
 <datestamp>2013-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2832</id><created>2013-10-08</created><authors><author><keyname>Paulus</keyname><forenames>Sachar</forenames></author><author><keyname>Riemann</keyname><forenames>Ute</forenames></author></authors><title>An approach for a business-driven cloudcompliance analysis covering
  public sector process improvement requirements</title><categories>cs.CY</categories><comments>Cloud Services, Business Processes, Value Chain, Compliance, Public
  Sector, International Journal of Managing Public Sector Information and
  Communication Technologies (IJMPICT) Vol. 4, No. 3, September 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The need for process improvement is an important target that does affect as
well the government processes. Specifically in the public sector there are
specific challenges to face.New technology approaches within government
processes such as cloud services are necessary to address these challenges.
Following the current discussion of cloudification of business processes all
processes are considered similar in regards to their usability within the
cloud. The truth is, that neither all processes have the same usability for
cloud services not do they have the same importance for a specific company.The
most comprehensive process within a company is the corporate value chain. In
this article one key proposition is to use the corporate value chain as the
fundamental structuring backbone for all business process analysis and
improvement activities. It is a pre-requisite to identify the core elements of
the value chain that are essential for the individual companys business and the
root cause for any company success. In this paper we propose to use the
company-specific value-creation for the cloud-affinity and the cloud-usability
of a business process in public sector considering the specific challenges of
addressing processes in cloud services. Therefor it is necessary to formalize
the way the processes with its interdependencies are documented in context of
their company-specific value chain (as part of the various deployment- and
governance alternatives. Moreover, it is essential in the public sector to
describe in detail the environmental and external restrictions of processes.
With the use of this proposed methodology it becomes relatively easy to
identify cloud-suitable processes within the public sector and thus optimize
the public companies value generation tightly focused with the use of this new
technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2841</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2841</id><created>2013-10-10</created><updated>2014-06-11</updated><authors><author><keyname>Iwata</keyname><forenames>Yoichi</forenames></author><author><keyname>Wahlstr&#xf6;m</keyname><forenames>Magnus</forenames></author><author><keyname>Yoshida</keyname><forenames>Yuichi</forenames></author></authors><title>Half-integrality, LP-branching and FPT Algorithms</title><categories>cs.DS</categories><comments>Added results on linear-time FPT algorithms (not present in SODA
  paper)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A recent trend in parameterized algorithms is the application of polytope
tools (specifically, LP-branching) to FPT algorithms (e.g., Cygan et al., 2011;
Narayanaswamy et al., 2012). However, although interesting results have been
achieved, the methods require the underlying polytope to have very restrictive
properties (half-integrality and persistence), which are known only for few
problems (essentially Vertex Cover (Nemhauser and Trotter, 1975) and Node
Multiway Cut (Garg et al., 1994)). Taking a slightly different approach, we
view half-integrality as a \emph{discrete} relaxation of a problem, e.g., a
relaxation of the search space from $\{0,1\}^V$ to $\{0,1/2,1\}^V$ such that
the new problem admits a polynomial-time exact solution. Using tools from CSP
(in particular Thapper and \v{Z}ivn\'y, 2012) to study the existence of such
relaxations, we provide a much broader class of half-integral polytopes with
the required properties, unifying and extending previously known cases.
  In addition to the insight into problems with half-integral relaxations, our
results yield a range of new and improved FPT algorithms, including an
$O^*(|\Sigma|^{2k})$-time algorithm for node-deletion Unique Label Cover with
label set $\Sigma$ and an $O^*(4^k)$-time algorithm for Group Feedback Vertex
Set, including the setting where the group is only given by oracle access. All
these significantly improve on previous results. The latter result also implies
the first single-exponential time FPT algorithm for Subset Feedback Vertex Set,
answering an open question of Cygan et al. (2012).
  Additionally, we propose a network flow-based approach to solve some cases of
the relaxation problem. This gives the first linear-time FPT algorithm to
edge-deletion Unique Label Cover.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2842</identifier>
 <datestamp>2013-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2842</id><created>2013-10-10</created><authors><author><keyname>Ammari</keyname><forenames>Habib</forenames></author><author><keyname>Mallat</keyname><forenames>St&#xe9;phane</forenames></author><author><keyname>Waldspurger</keyname><forenames>Ir&#xe8;ne</forenames></author><author><keyname>Wang</keyname><forenames>Han</forenames></author></authors><title>Wavelet methods for shape perception in electro-sensing</title><categories>math.NA cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims at presenting a new approach to the electro-sensing problem
using wavelets. It provides an efficient algorithm for recognizing the shape of
a target from micro-electrical impedance measurements. Stability and resolution
capabilities of the proposed algorithm are quantified in numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2858</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2858</id><created>2013-10-10</created><updated>2015-07-27</updated><authors><author><keyname>Becchetti</keyname><forenames>Luca</forenames></author><author><keyname>Clementi</keyname><forenames>Andrea</forenames></author><author><keyname>Natale</keyname><forenames>Emanuele</forenames></author><author><keyname>Pasquale</keyname><forenames>Francesco</forenames></author><author><keyname>Silvestri</keyname><forenames>Riccardo</forenames></author><author><keyname>Trevisan</keyname><forenames>Luca</forenames></author></authors><title>Simple Dynamics for Plurality Consensus</title><categories>cs.DM cs.DC</categories><comments>Preprint of journal version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a \emph{Plurality-Consensus} process in which each of $n$ anonymous
agents of a communication network initially supports an opinion (a color chosen
from a finite set $[k]$). Then, in every (synchronous) round, each agent can
revise his color according to the opinions currently held by a random sample of
his neighbors. It is assumed that the initial color configuration exhibits a
sufficiently large \emph{bias} $s$ towards a fixed plurality color, that is,
the number of nodes supporting the plurality color exceeds the number of nodes
supporting any other color by $s$ additional nodes. The goal is having the
process to converge to the \emph{stable} configuration in which all nodes
support the initial plurality. We consider a basic model in which the network
is a clique and the update rule (called here the \emph{3-majority dynamics}) of
the process is the following: each agent looks at the colors of three random
neighbors and then applies the majority rule (breaking ties uniformly).
  We prove that the process converges in time $\mathcal{O}( \min\{ k, (n/\log
n)^{1/3} \} \, \log n )$ with high probability, provided that $s \geqslant c
\sqrt{ \min\{ 2k, (n/\log n)^{1/3} \}\, n \log n}$.
  We then prove that our upper bound above is tight as long as $k \leqslant
(n/\log n)^{1/4}$. This fact implies an exponential time-gap between the
plurality-consensus process and the \emph{median} process studied by Doerr et
al. in [ACM SPAA'11].
  A natural question is whether looking at more (than three) random neighbors
can significantly speed up the process. We provide a negative answer to this
question: In particular, we show that samples of polylogarithmic size can speed
up the process by a polylogarithmic factor only.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2860</identifier>
 <datestamp>2013-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2860</id><created>2013-10-10</created><authors><author><keyname>Wang</keyname><forenames>Chien-Yi</forenames></author><author><keyname>Jeon</keyname><forenames>Sang-Woon</forenames></author><author><keyname>Gastpar</keyname><forenames>Michael</forenames></author></authors><title>Interactive Computation of Type-Threshold Functions in Collocated
  Broadcast-Superposition Networks</title><categories>cs.IT math.IT</categories><comments>21 pages, 4 figures, submitted to IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In wireless sensor networks, various applications involve learning one or
multiple functions of the measurements observed by sensors, rather than the
measurements themselves. This paper focuses on type-threshold functions, e.g.,
the maximum and indicator functions. Previous work studied this problem under
the collocated collision network model and showed that under many probabilistic
models for the measurements, the achievable computation rates converge to zero
as the number of sensors increases. This paper considers two network models
reflecting both the broadcast and superposition properties of wireless
channels: the collocated linear finite field network and the collocated
Gaussian network. A general multi-round coding scheme exploiting not only the
broadcast property but particularly also the superposition property of the
networks is developed. Through careful scheduling of concurrent transmissions
to reduce redundancy, it is shown that given any independent measurement
distribution, all type-threshold functions can be computed reliably with a
non-vanishing rate in the collocated Gaussian network, even if the number of
sensors tends to infinity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2861</identifier>
 <datestamp>2013-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2861</id><created>2013-10-10</created><authors><author><keyname>Karoui</keyname><forenames>Kamel</forenames></author><author><keyname>Ftima</keyname><forenames>Fakher Ben</forenames></author><author><keyname>Ghezala</keyname><forenames>Henda Ben</forenames></author></authors><title>Distributed firewalls and IDS interoperability checking based on a
  formal approach</title><categories>cs.NI cs.CR</categories><comments>Security component, relevancy, misconfigurations detection,
  interoperability cheking, formal correction,formal verification, projection,
  IDS, Firewall</comments><acm-class>C.2; C.2.0; C.2.1; C.2.3; D.2.12</acm-class><journal-ref>International Journal of Computer Networks &amp; Communications,
  September 2013, Volume 5. Number 5, pp 95-115</journal-ref><doi>10.5121/ijcnc.2013.5508</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To supervise and guarantee a network security, the administrator uses
different security components, such as firewalls, IDS and IPS. For a perfect
interoperability between these components, they must be configured properly to
avoid misconfiguration between them. Nevertheless, the existence of a set of
anomalies between filtering rules and alerting rules, particularly in
distributed multi-component architectures is very likely to degrade the network
security. The main objective of this paper is to check if a set of security
components are interoperable. A case study using a firewall and an IDS as
examples will illustrate the usefulness of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2864</identifier>
 <datestamp>2013-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2864</id><created>2013-10-10</created><authors><author><keyname>von der Weth</keyname><forenames>Christian</forenames></author><author><keyname>Hegde</keyname><forenames>Vinod</forenames></author><author><keyname>Hauswirth</keyname><forenames>Manfred</forenames></author></authors><title>Virtual Location-Based Services: Merging the Physical and Virtual World</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Location-based services gained much popularity through providing users with
helpful information with respect to their current location. The search and
recommendation of nearby locations or places, and the navigation to a specific
location are some of the most prominent location-based services. As a recent
trend, virtual location-based services consider webpages or sites associated
with a location as 'virtual locations' that online users can visit in spite of
not being physically present at the location. The presence of links between
virtual locations and the corresponding physical locations (e.g., geo-location
information of a restaurant linked to its website), allows for novel types of
services and applications which constitute virtual location-based services
(VLBS). The quality and potential benefits of such services largely depends on
the existence of websites referring to physical locations. In this paper, we
investigate the usefulness of linking virtual and physical locations. For this,
we analyze the presence and distribution of virtual locations, i.e., websites
referring to places, for two Irish cities. Using simulated tracks based on a
user movement model, we investigate how mobile users move through the Web as
virtual space. Our results show that virtual locations are omnipresent in urban
areas, and that the situation that a user is close to even several such
locations at any time is rather the normal case instead of the exception.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2880</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2880</id><created>2013-10-10</created><updated>2016-02-23</updated><authors><author><keyname>Barbu</keyname><forenames>Adrian</forenames></author><author><keyname>She</keyname><forenames>Yiyuan</forenames></author><author><keyname>Ding</keyname><forenames>Liangjing</forenames></author><author><keyname>Gramajo</keyname><forenames>Gary</forenames></author></authors><title>Feature Selection with Annealing for Computer Vision and Big Data
  Learning</title><categories>stat.ML cs.CV cs.LG math.ST stat.TH</categories><comments>18 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many computer vision and medical imaging problems are faced with learning
from large-scale datasets, with millions of observations and features. In this
paper we propose a novel efficient learning scheme that tightens a sparsity
constraint by gradually removing variables based on a criterion and a schedule.
The attractive fact that the problem size keeps dropping throughout the
iterations makes it particularly suitable for big data learning. Our approach
applies generically to the optimization of any differentiable loss function,
and finds applications in regression, classification and ranking. The resultant
algorithms build variable screening into estimation and are extremely simple to
implement. We provide theoretical guarantees of convergence and selection
consistency. In addition, one dimensional piecewise linear response functions
are used to account for nonlinearity and a second order prior is imposed on
these functions to avoid overfitting. Experiments on real and synthetic data
show that the proposed method compares very well with other state of the art
methods in regression, classification and ranking while being computationally
very efficient and scalable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2882</identifier>
 <datestamp>2013-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2882</id><created>2013-10-10</created><authors><author><keyname>B&#xf6;cherer</keyname><forenames>Georg</forenames></author><author><keyname>Amjad</keyname><forenames>Rana Ali</forenames></author></authors><title>Informational Divergence and Entropy Rate on Rooted Trees with
  Probabilities</title><categories>cs.IT math.IT</categories><comments>5 pages. With proofs and illustrating example</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rooted trees with probabilities are used to analyze properties of a variable
length code. A bound is derived on the difference between the entropy rates of
the code and a memoryless source. The bound is in terms of normalized
informational divergence. The bound is used to derive converses for exact
random number generation, resolution coding, and distribution matching.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2885</identifier>
 <datestamp>2013-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2885</id><created>2013-10-10</created><updated>2013-12-19</updated><authors><author><keyname>Yuen</keyname><forenames>Henry</forenames></author></authors><title>A quantum lower bound for distinguishing random functions from random
  permutations</title><categories>cs.CC quant-ph</categories><comments>9 pages. Comments welcome. Fixed minor errors and typos in v2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of distinguishing between a random function and a random
permutation on a domain of size $N$ is important in theoretical cryptography,
where the security of many primitives depend on the problem's hardness. We
study the quantum query complexity of this problem, and show that any quantum
algorithm that solves this problem with bounded error must make
$\Omega(N^{1/5}/\log N)$ queries to the input function. Our lower bound proof
uses a combination of the Collision Problem lower bound and Ambainis's
adversary theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2886</identifier>
 <datestamp>2013-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2886</id><created>2013-10-10</created><authors><author><keyname>Bi</keyname><forenames>Huibo</forenames></author></authors><title>Emergency and Normal Navigation in Confined Spaces</title><categories>cs.OH</categories><comments>This is a internal research report with 35 pages and 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Emergency navigation algorithms direct evacuees to exits when disastrous
events such as fire take place. Due to the spread of hazards, latency in
information updating and unstable flows of civilians, emergency evacuation is
absolutely a complex transshipment problem involving numerous sources and
multiple destinations. Previous algorithms which commonly need either a full
graph search or a convergence process suffer from high computational and
communication overheads. This research report surveys the current emergency
navigation algorithms and adapts the concept of Cognitive Packet Network (CPN)
to the context of emergency evacuation. By using random neural networks, the
CPN based algorithm can explore optimal routes rapidly and adaptively in a
highly dynamic emergency environment with low expense. Simultaneously, in
emergency situations there are typically different categories of evacuees such
as people of different age groups. However, current algorithms only consider
&quot;normal&quot; evacuees and do not meet the specific requirements of diverse
evacuees. Our algorithms make use of the flexibility of CPN which can operate
with different user-defined goals to customize appropriate paths for each
category. The CPN algorithm is simulated in a graph based discrete-event
simulator and Dijkstra's shortest path algorithm is taken as reference. The
results show that the CPN algorithm reaches the performance of ideal
path-finding algorithm and quality of service is improved by using specific
goal functions for diverse categories of evacuees. Finally, we present a future
plan for further research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2916</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2916</id><created>2013-10-10</created><updated>2014-04-07</updated><authors><author><keyname>Xiong</keyname><forenames>Ying</forenames></author><author><keyname>Chakrabarti</keyname><forenames>Ayan</forenames></author><author><keyname>Basri</keyname><forenames>Ronen</forenames></author><author><keyname>Gortler</keyname><forenames>Steven J.</forenames></author><author><keyname>Jacobs</keyname><forenames>David W.</forenames></author><author><keyname>Zickler</keyname><forenames>Todd</forenames></author></authors><title>From Shading to Local Shape</title><categories>cs.CV</categories><journal-ref>IEEE Trans. PAMI 37 (2015) 67-79</journal-ref><doi>10.1109/TPAMI.2014.2343211</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a framework for extracting a concise representation of the shape
information available from diffuse shading in a small image patch. This
produces a mid-level scene descriptor, comprised of local shape distributions
that are inferred separately at every image patch across multiple scales. The
framework is based on a quadratic representation of local shape that, in the
absence of noise, has guarantees on recovering accurate local shape and
lighting. And when noise is present, the inferred local shape distributions
provide useful shape information without over-committing to any particular
image explanation. These local shape distributions naturally encode the fact
that some smooth diffuse regions are more informative than others, and they
enable efficient and robust reconstruction of object-scale shape. Experimental
results show that this approach to surface reconstruction compares well against
the state-of-art on both synthetic images and captured photographs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2923</identifier>
 <datestamp>2013-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2923</id><created>2013-10-10</created><authors><author><keyname>Cai</keyname><forenames>Haipeng</forenames></author><author><keyname>Chen</keyname><forenames>Jian</forenames></author><author><keyname>Auchus</keyname><forenames>Alexander P.</forenames></author><author><keyname>Laidlaw</keyname><forenames>David H.</forenames></author></authors><title>Composing DTI Visualizations with End-user Programming</title><categories>cs.GR cs.HC cs.PL</categories><comments>11 pages, 9 figures, 2 tables</comments><acm-class>H.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the design and prototype implementation of a scientific
visualization language called Zifazah for composing 3D visualizations of
diffusion tensor magnetic resonance imaging (DT-MRI or DTI) data. Unlike
existing tools allowing flexible customization of data visualizations that are
programmer-oriented, we focus on domain scientists as end users in order to
enable them to freely compose visualizations of their scientific data set. We
analyzed end-user descriptions extracted from interviews with neurologists and
physicians conducting clinical practices using DTI about how they would build
and use DTI visualizations to collect syntax and semantics for the language
design, and have discovered the elements and structure of the proposed
language. Zifazah makes use of the initial set of lexical terms and semantics
to provide a declarative language in the spirit of intuitive syntax and usage.
This work contributes three, among others, main design principles for
scientific visualization language design as well as a practice of such language
for DTI visualization with Zifazah. First, Zifazah incorporated visual symbolic
mapping based on color, size and shape, which is a sub-set of Bertin's taxonomy
migrated to scientific visualizations. Second, Zifazah is defined as a spatial
language whereby lexical representation of spatial relationship for 3D object
visualization and manipulations, which is characteristic of scientific data,
can be programmed. Third, built on top of Bertin's semiology, flexible data
encoding specifically for scientific visualizations is integrated in our
language in order to allow end users to achieve optimal visual composition at
their best. Along with sample scripts representative of our language design
features, some new DTI visualizations as the running results created by end
users using the novel visualization language have also been presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2928</identifier>
 <datestamp>2013-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2928</id><created>2013-10-10</created><authors><author><keyname>Crowston</keyname><forenames>Robert</forenames></author><author><keyname>Jones</keyname><forenames>Mark</forenames></author><author><keyname>Muciaccia</keyname><forenames>Gabriele</forenames></author><author><keyname>Philip</keyname><forenames>Geevarghese</forenames></author><author><keyname>Rai</keyname><forenames>Ashutosh</forenames></author><author><keyname>Saurabh</keyname><forenames>Saket</forenames></author></authors><title>Polynomial Kernels for {\lambda}-extendible Properties Parameterized
  Above the Poljak-Turz\'ik Bound</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Poljak and Turzik (Discrete Mathematics 1986) introduced the notion of
{\lambda}-extendible properties of graphs as a generalization of the property
of being bipartite. They showed that for any 0 &lt; {\lambda} &lt; 1 and
{\lambda}-extendible property {\Pi}, any connected graph G on n vertices and m
edges contains a spanning subgraph H in {\Pi} with at least {\lambda}m +
(1-{\lambda})(n-1)/2 edges. The property of being bipartite is
{\lambda}-extendible for {\lambda} = 1/2, and so the Poljak-Turzik bound
generalizes the well-known Edwards-Erdos bound for Max-Cut. Other examples of
{\lambda}-extendible properties include: being an acyclic oriented graph, a
balanced signed graph, or a q-colorable graph for some integer q.
  Mnich et. al. (FSTTCS 2012) defined the closely related notion of strong
{\lambda}-extendibility. They showed that the problem of finding a subgraph
satisfying a given strongly {\lambda}-extendible property {\Pi} is
fixed-parameter tractable (FPT) when parameterized above the Poljak-Turzik
bound - does there exist a spanning subgraph H of a connected graph G such that
H in {\Pi} and H has at least {\lambda}m + (1-{\lambda})(n-1)/2 + k edges? -
subject to the condition that the problem is FPT on a certain simple class of
graphs called almost-forests of cliques.
  In this paper we settle the kernelization complexity of nearly all problems
parameterized above Poljak-Turzik bounds, in the affirmative. We show that
these problems admit quadratic kernels (cubic when {\lambda} = 1/2), without
using the assumption that the problem is FPT on almost-forests of cliques. Thus
our results not only remove the technical condition of being FPT on
almost-forests of cliques from previous results, but also unify and extend
previously known kernelization results in this direction. Our results add to
the select list of generic kernelization results known in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2931</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2931</id><created>2013-10-10</created><updated>2014-10-31</updated><authors><author><keyname>Wager</keyname><forenames>Stefan</forenames></author><author><keyname>Chamandy</keyname><forenames>Nick</forenames></author><author><keyname>Muralidharan</keyname><forenames>Omkar</forenames></author><author><keyname>Najmi</keyname><forenames>Amir</forenames></author></authors><title>Feedback Detection for Live Predictors</title><categories>stat.ME cs.LG stat.ML</categories><comments>Advances in Neural Information Processing Systems (NIPS), 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A predictor that is deployed in a live production system may perturb the
features it uses to make predictions. Such a feedback loop can occur, for
example, when a model that predicts a certain type of behavior ends up causing
the behavior it predicts, thus creating a self-fulfilling prophecy. In this
paper we analyze predictor feedback detection as a causal inference problem,
and introduce a local randomization scheme that can be used to detect
non-linear feedback in real-world problems. We conduct a pilot study for our
proposed methodology using a predictive system currently deployed as a part of
a search engine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2935</identifier>
 <datestamp>2013-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2935</id><created>2013-10-10</created><updated>2013-10-31</updated><authors><author><keyname>Doyen</keyname><forenames>Laurent</forenames></author><author><keyname>Massart</keyname><forenames>Thierry</forenames></author><author><keyname>Shirmohammadi</keyname><forenames>Mahsa</forenames></author></authors><title>Limit Synchronization in Markov Decision Processes</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Markov decision processes (MDP) are finite-state systems with both strategic
and probabilistic choices. After fixing a strategy, an MDP produces a sequence
of probability distributions over states. The sequence is eventually
synchronizing if the probability mass accumulates in a single state, possibly
in the limit. Precisely, for 0 &lt;= p &lt;= 1 the sequence is p-synchronizing if a
probability distribution in the sequence assigns probability at least p to some
state, and we distinguish three synchronization modes: (i) sure winning if
there exists a strategy that produces a 1-synchronizing sequence; (ii)
almost-sure winning if there exists a strategy that produces a sequence that
is, for all epsilon &gt; 0, a (1-epsilon)-synchronizing sequence; (iii) limit-sure
winning if for all epsilon &gt; 0, there exists a strategy that produces a
(1-epsilon)-synchronizing sequence.
  We consider the problem of deciding whether an MDP is sure, almost-sure,
limit-sure winning, and we establish the decidability and optimal complexity
for all modes, as well as the memory requirements for winning strategies. Our
main contributions are as follows: (a) for each winning modes we present
characterizations that give a PSPACE complexity for the decision problems, and
we establish matching PSPACE lower bounds; (b) we show that for sure winning
strategies, exponential memory is sufficient and may be necessary, and that in
general infinite memory is necessary for almost-sure winning, and unbounded
memory is necessary for limit-sure winning; (c) along with our results, we
establish new complexity results for alternating finite automata over a
one-letter alphabet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2954</identifier>
 <datestamp>2013-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2954</id><created>2013-10-09</created><authors><author><keyname>Abdel-Hamid</keyname><forenames>Ayman T.</forenames></author><author><keyname>Zahran</keyname><forenames>Ahmed H.</forenames></author><author><keyname>ElBatt</keyname><forenames>Tamer</forenames></author></authors><title>Improved Spectrum Mobility using Virtual Reservation in Collaborative
  Cognitive Radio Networks</title><categories>cs.NI cs.IT cs.PF math.IT</categories><comments>7 pages, 10 figures, IEEE ISCC 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cognitive radio technology would enable a set of secondary users (SU) to
opportunistically use the spectrum licensed to a primary user (PU). On the
appearance of this PU on a specific frequency band, any SU occupying this band
should free it for PUs. Typically, SUs may collaborate to reduce the impact of
cognitive users on the primary network and to improve the performance of the
SUs. In this paper, we propose and analyze the performance of virtual
reservation in collaborative cognitive networks. Virtual reservation is a novel
link maintenance strategy that aims to maximize the throughput of the cognitive
network through full spectrum utilization. Our performance evaluation shows
significant improvements not only in the SUs blocking and forced termination
probabilities but also in the throughput of cognitive users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2955</identifier>
 <datestamp>2013-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2955</id><created>2013-10-10</created><authors><author><keyname>Pickett</keyname><forenames>Marc</forenames></author><author><keyname>Aha</keyname><forenames>David W.</forenames></author></authors><title>Spontaneous Analogy by Piggybacking on a Perceptual System</title><categories>cs.AI cs.LG</categories><comments>Proceedings of the 35th Meeting of the Cognitive Science Society,
  2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most computational models of analogy assume they are given a delineated
source domain and often a specified target domain. These systems do not address
how analogs can be isolated from large domains and spontaneously retrieved from
long-term memory, a process we call spontaneous analogy. We present a system
that represents relational structures as feature bags. Using this
representation, our system leverages perceptual algorithms to automatically
create an ontology of relational structures and to efficiently retrieve analogs
for new relational structures from long-term memory. We provide a demonstration
of our approach that takes a set of unsegmented stories, constructs an ontology
of analogical schemas (corresponding to plot devices), and uses this ontology
to efficiently find analogs within new stories, yielding significant
time-savings over linear analog retrieval at a small accuracy cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2959</identifier>
 <datestamp>2014-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2959</id><created>2013-10-10</created><updated>2014-02-27</updated><authors><author><keyname>Talukdar</keyname><forenames>Partha Pratim</forenames></author><author><keyname>Cohen</keyname><forenames>William</forenames></author></authors><title>Scaling Graph-based Semi Supervised Learning to Large Number of Labels
  Using Count-Min Sketch</title><categories>cs.LG</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph-based Semi-supervised learning (SSL) algorithms have been successfully
used in a large number of applications. These methods classify initially
unlabeled nodes by propagating label information over the structure of graph
starting from seed nodes. Graph-based SSL algorithms usually scale linearly
with the number of distinct labels (m), and require O(m) space on each node.
Unfortunately, there exist many applications of practical significance with
very large m over large graphs, demanding better space and time complexity. In
this paper, we propose MAD-SKETCH, a novel graph-based SSL algorithm which
compactly stores label distribution on each node using Count-min Sketch, a
randomized data structure. We present theoretical analysis showing that under
mild conditions, MAD-SKETCH can reduce space complexity at each node from O(m)
to O(log m), and achieve similar savings in time complexity as well. We support
our analysis through experiments on multiple real world datasets. We observe
that MAD-SKETCH achieves similar performance as existing state-of-the-art
graph- based SSL algorithms, while requiring smaller memory footprint and at
the same time achieving up to 10x speedup. We find that MAD-SKETCH is able to
scale to datasets with one million labels, which is beyond the scope of
existing graph- based SSL algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2960</identifier>
 <datestamp>2013-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2960</id><created>2013-10-10</created><authors><author><keyname>Zhang</keyname><forenames>Wei</forenames></author><author><keyname>Liu</keyname><forenames>Wei</forenames></author><author><keyname>Wu</keyname><forenames>Siliang</forenames></author><author><keyname>Wang</keyname><forenames>Ju</forenames></author></authors><title>Joint DOA and Array Manifold Estimation for a MIMO Array Using Two
  Calibrated Antennas</title><categories>cs.IT math.IT math.NA</categories><comments>5 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A simple scheme for joint direction of arrival (DOA) and array manifold
estimation for a MIMO array system is proposed, where only two transmit
antennas are calibrated initially. It first obtains a set of initial DOA
results by employing a rotational invariance property between two sets of
received data, and then more accurate DOA and array manifold estimation is
obtained through a local searching algorithm with several iterations. No strict
half wavelength spacing is required for the uncalibrated antennas to avoid the
spatial aliasing problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2961</identifier>
 <datestamp>2013-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2961</id><created>2013-10-09</created><authors><author><keyname>de Vries</keyname><forenames>Jeroen</forenames></author><author><keyname>Schellenberg</keyname><forenames>Dimitri</forenames></author><author><keyname>Abelmann</keyname><forenames>Leon</forenames></author><author><keyname>Manz</keyname><forenames>Andreas</forenames></author><author><keyname>Elwenspoek</keyname><forenames>Miko</forenames></author></authors><title>Towards Gigayear Storage Using a Silicon-Nitride/Tungsten Based Medium</title><categories>cs.ET physics.pop-ph physics.soc-ph</categories><comments>19 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current digital data storage systems are able to store huge amounts of data.
Even though the data density of digital information storage has increased
tremendously over the last few decades, the data longevity is limited to only a
few decades. If we want to preserve anything about the human race which can
outlast the human race itself, we require a data storage medium designed to
last for 1 million to 1 billion years. In this paper a medium is investigated
consisting of tungsten encapsulated by siliconnitride which, according to
elevated temperature tests, will last for well over the suggested time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2963</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2963</id><created>2013-10-10</created><updated>2014-09-16</updated><authors><author><keyname>Santi</keyname><forenames>Paolo</forenames></author><author><keyname>Resta</keyname><forenames>Giovanni</forenames></author><author><keyname>Szell</keyname><forenames>Michael</forenames></author><author><keyname>Sobolevsky</keyname><forenames>Stanislav</forenames></author><author><keyname>Strogatz</keyname><forenames>Steven</forenames></author><author><keyname>Ratti</keyname><forenames>Carlo</forenames></author></authors><title>Quantifying the benefits of vehicle pooling with shareability networks</title><categories>physics.soc-ph cs.CY cs.SI</categories><comments>Main text: 6 pages, 3 figures, SI: 24 pages</comments><journal-ref>PNAS 111(37), 13290-13294 (2014)</journal-ref><doi>10.1073/pnas.1403657111</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Taxi services are a vital part of urban transportation, and a considerable
contributor to traffic congestion and air pollution causing substantial adverse
effects on human health. Sharing taxi trips is a possible way of reducing the
negative impact of taxi services on cities, but this comes at the expense of
passenger discomfort quantifiable in terms of a longer travel time. Due to
computational challenges, taxi sharing has traditionally been approached on
small scales, such as within airport perimeters, or with dynamical ad-hoc
heuristics. However, a mathematical framework for the systematic understanding
of the tradeoff between collective benefits of sharing and individual passenger
discomfort is lacking. Here we introduce the notion of shareability network
which allows us to model the collective benefits of sharing as a function of
passenger inconvenience, and to efficiently compute optimal sharing strategies
on massive datasets. We apply this framework to a dataset of millions of taxi
trips taken in New York City, showing that with increasing but still relatively
low passenger discomfort, cumulative trip length can be cut by 40% or more.
This benefit comes with reductions in service cost, emissions, and with split
fares, hinting towards a wide passenger acceptance of such a shared service.
Simulation of a realistic online system demonstrates the feasibility of a
shareable taxi service in New York City. Shareability as a function of trip
density saturates fast, suggesting effectiveness of the taxi sharing system
also in cities with much sparser taxi fleets or when willingness to share is
low.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2994</identifier>
 <datestamp>2013-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2994</id><created>2013-10-10</created><updated>2013-10-14</updated><authors><author><keyname>Cai</keyname><forenames>Haipeng</forenames></author><author><keyname>Chen</keyname><forenames>Jian</forenames></author><author><keyname>Auchus</keyname><forenames>Alexander P.</forenames></author></authors><title>Depth-dependent Parallel Visualization with 3D Stylized Dense Tubes</title><categories>cs.DC cs.GR</categories><comments>10 pages, 9 figures, 1 table</comments><acm-class>I.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a parallel visualization algorithm for the illustrative rendering
of depth-dependent stylized dense tube data at interactive frame rates. While
this computation could be efficiently performed on a GPU device, we target a
parallel framework to enable it to be efficiently running on an ordinary
multi-core CPU platform which is much more available than GPUs for common
users. Our approach is to map the depth information in each tube onto each of
the visual dimensions of shape, color, texture, value, and size on the basis of
Bertin's semiology theory. The purpose is to enable more legible displays in
the dense tube environments. A major contribution of our work is an efficient
and effective parallel depthordering algorithm that makes use of the message
passing interface (MPI) with VTK. We evaluated our framework with
visualizations of depth-stylized tubes derived from 3D diffusion tensor MRI
data by comparing its efficiency with several other alternative parallelization
platforms running the same computations. As our results show, the
parallelization framework we proposed can efficiently render highly dense 3D
data sets like the tube data and thus is useful as a complement to parallel
visualization environments that rely on GPUs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.2997</identifier>
 <datestamp>2013-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.2997</id><created>2013-10-10</created><updated>2013-11-19</updated><authors><author><keyname>Dekel</keyname><forenames>Ofer</forenames></author><author><keyname>Ding</keyname><forenames>Jian</forenames></author><author><keyname>Koren</keyname><forenames>Tomer</forenames></author><author><keyname>Peres</keyname><forenames>Yuval</forenames></author></authors><title>Bandits with Switching Costs: T^{2/3} Regret</title><categories>cs.LG math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the adversarial multi-armed bandit problem in a setting where the
player incurs a unit cost each time he switches actions. We prove that the
player's $T$-round minimax regret in this setting is
$\widetilde{\Theta}(T^{2/3})$, thereby closing a fundamental gap in our
understanding of learning with bandit feedback. In the corresponding
full-information version of the problem, the minimax regret is known to grow at
a much slower rate of $\Theta(\sqrt{T})$. The difference between these two
rates provides the \emph{first} indication that learning with bandit feedback
can be significantly harder than learning with full-information feedback
(previous results only showed a different dependence on the number of actions,
but not on $T$.)
  In addition to characterizing the inherent difficulty of the multi-armed
bandit problem with switching costs, our results also resolve several other
open problems in online learning. One direct implication is that learning with
bandit feedback against bounded-memory adaptive adversaries has a minimax
regret of $\widetilde{\Theta}(T^{2/3})$. Another implication is that the
minimax regret of online learning in adversarial Markov decision processes
(MDPs) is $\widetilde{\Theta}(T^{2/3})$. The key to all of our results is a new
randomized construction of a multi-scale random walk, which is of independent
interest and likely to prove useful in additional settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3015</identifier>
 <datestamp>2013-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3015</id><created>2013-10-10</created><authors><author><keyname>Kim</keyname><forenames>Donggun</forenames></author><author><keyname>Sung</keyname><forenames>Youngchul</forenames></author><author><keyname>Chung</keyname><forenames>Jihoon</forenames></author></authors><title>Filter-And-Forward Relay Design for MIMO-OFDM Systems</title><categories>cs.IT math.IT</categories><comments>29 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the filter-and-forward (FF) relay design for multiple-input
multiple-output (MIMO) orthogonal frequency-division multiplexing (OFDM)
systems is considered. Due to the considered MIMO structure, the problem of
joint design of the linear MIMO transceiver at the source and the destination
and the FF relay at the relay is considered. As the design criterion, the
minimization of weighted sum mean-square-error (MSE) is considered first, and
the joint design in this case is approached based on alternating optimization
that iterates between optimal design of the FF relay for a given set of MIMO
precoder and decoder and optimal design of the MIMO precoder and decoder for a
given FF relay filter. Next, the joint design problem for rate maximization is
considered based on the obtained result regarding weighted sum MSE and the
existing result regarding the relationship between weighted MSE minimization
and rate maximization. Numerical results show the effectiveness of the proposed
FF relay design and significant performance improvement by FF relays over
widely-considered simple AF relays for MIMO-ODFM systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3017</identifier>
 <datestamp>2013-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3017</id><created>2013-10-11</created><authors><author><keyname>Livinsky</keyname><forenames>Ivan</forenames></author><author><keyname>Lange</keyname><forenames>Alexander</forenames></author><author><keyname>Radziszowski</keyname><forenames>Stanis&#x142;aw</forenames></author></authors><title>Computation of the Ramsey Numbers $R(C_4,K_9)$ and $R(C_4,K_{10})$</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Ramsey number $R(C_4,K_m)$ is the smallest $n$ such that any graph on $n$
vertices contains a cycle of length four or an independent set of order $m$.
With the help of computer algorithms we obtain the exact values of the Ramsey
numbers $R(C_4,K_9)=30$ and $R(C_4,K_{10})=36$. New bounds for the next two
open cases are also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3031</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3031</id><created>2013-10-11</created><updated>2014-07-22</updated><authors><author><keyname>Fasino</keyname><forenames>Dario</forenames></author><author><keyname>Tudisco</keyname><forenames>Francesco</forenames></author></authors><title>An algebraic analysis of the graph modularity</title><categories>math.NA cs.SI math.SP</categories><msc-class>05C50, 05C70, 15A18, 15A48</msc-class><journal-ref>SIAM. J. Matrix Anal. Appl., 35(3), 997-1018, 2014</journal-ref><doi>10.1137/130943455</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most relevant tasks in network analysis is the detection of
community structures, or clustering. Most popular techniques for community
detection are based on the maximization of a quality function called
modularity, which in turn is based upon particular quadratic forms associated
to a real symmetric modularity matrix $M$, defined in terms of the adjacency
matrix and a rank one null model matrix. That matrix could be posed inside the
set of relevant matrices involved in graph theory, alongside adjacency,
incidence and Laplacian matrices. This is the reason we propose a graph
analysis based on the algebraic and spectral properties of such matrix. In
particular, we propose a nodal domain theorem for the eigenvectors of $M$; we
point out several relations occurring between graph's communities and
nonnegative eigenvalues of $M$; and we derive a Cheeger-type inequality for the
graph optimal modularity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3032</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3032</id><created>2013-10-11</created><updated>2016-01-14</updated><authors><author><keyname>Kuusisto</keyname><forenames>Antti</forenames></author></authors><title>A Double Team Semantics for Generalized Quantifiers</title><categories>math.LO cs.LO</categories><msc-class>03C80</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a semantics for first-order logic with generalized quantifiers
based on double teams. We also define and investigate a notion of a generalized
atom. Such atoms can be used in order to define extensions of first-order logic
with a team-based semantics. We also define a game semantics and compare it
with the double team semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3040</identifier>
 <datestamp>2014-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3040</id><created>2013-10-11</created><updated>2014-01-31</updated><authors><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author><author><keyname>Perevodchikov</keyname><forenames>Evgeniy</forenames></author><author><keyname>Uvarov</keyname><forenames>Alexander</forenames></author></authors><title>Measuring Triple-Helix Synergy in the Russian Innovation Systems at
  Regional, Provincial, and National Levels</title><categories>cs.CY</categories><comments>accepted for publication in the Journal of the Association for
  Information Science and Technology JASIST</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We measure synergy for the Russian national, provincial, and regional
innovation systems as reduction of uncertainty using mutual information among
the three distributions of firm sizes, technological knowledge-bases of firms,
and geographical locations. Half a million data at firm level in 2011 were
obtained from the Orbis database of Bureau Van Dijk. The firm level data were
aggregated at the levels of eight Federal Districts, the regional level of 83
Federal Subjects, and the single level of the Russian Federation. Not
surprisingly, the knowledge base of the economy is concentrated in the Moscow
region (22.8%); St. Petersburg follows with 4.0%. Only 0.4% of the firms are
classified as high-tech, and 2.7% as medium-tech manufacturing (NACE, Rev. 2).
Except in Moscow itself, high-tech manufacturing does not add synergy to any
other unit at any of the various levels of geographical granularity; instead it
disturbs regional coordination even in the region surrounding Moscow (&quot;Moscow
Region&quot;). In the case of medium-tech manufacturing, there is also synergy in
St. Petersburg. Knowledge-intensive services (KIS; including laboratories)
contribute 12.8% to the economy in terms of establishments and contribute to
the synergy in all Federal Districts (except the North-Caucasian Federal
District), but only in 30 of the 83 Federal Subjects. The synergy in KIS is
concentrated in centers of administration. Unlike Western European countries,
the knowledge-intensive services (which are often state-affiliated) thus
provide backbone to an emerging knowledge-based economy at the level of Federal
Districts, but the economy is otherwise not knowledge-based (except for the
Moscow region).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3049</identifier>
 <datestamp>2013-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3049</id><created>2013-10-11</created><updated>2013-10-14</updated><authors><author><keyname>Shalini</keyname></author></authors><title>Hybrid Mutual Authentication Protocol for 802.16j against Rogue Station
  and DoS Attack</title><categories>cs.CR</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in equation 1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  WiMAX is a promising technology that provides high data throughput with low
delays for various user types and modes of operation. These advantages make
WiMAX applicable both for infrastructure purposes and end-client usage. Since
WiMAX is presented as a network framework and a last-mile technology, it is
believed to be capable of handling a wide range of usage scenarios. For
example, while the end users have an opportunity to use WiMAX as the primary
connection medium for acquiring services such as on-demand video streaming,
VoIP connections and mobile bank transactions, the service providers may use it
for data relaying purposes among access points. To meet the technical
requirements of these various scenarios, majority of the WiMAX research has
been conducted on physical and MAC layers; however little has been invested in
a comprehensive and efficient security solution, which has resulted in a wide
range of security weaknesses and reactive solutions. Many security problems
remain to be addressed in different modes and for different user types even in
the final security standard of WiMAX, PKMv2. Mobile multi-hop relay (MMR) WiMAX
networks have attracted lots of interest in wireless communication industry
because of its scalable coverage, improved data rate and relatively low cost.
Even with the additional security functionalities security of MMR WiMAX network
is the main challenge because messages have to be transmitted through one or
more relay stations, which makes it more difficult to ensure the authenticity
of messages and entities involved in the transmission. In this thesis, we
present a hybrid security solution to overcome the security problem of rogue
station (BS/RS) attack and denial of service attack (DoS) in MMR WiMAX.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3062</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3062</id><created>2013-10-11</created><updated>2014-01-10</updated><authors><author><keyname>Narasimhan</keyname><forenames>T. Lakshmi</forenames></author><author><keyname>Chockalingam</keyname><forenames>A.</forenames></author></authors><title>Channel Hardening-Exploiting Message Passing (CHEMP) Receiver in
  Large-Scale MIMO Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a MIMO receiver algorithm that exploits {\em
channel hardening} that occurs in large MIMO channels. Channel hardening refers
to the phenomenon where the off-diagonal terms of the ${\bf H}^H{\bf H}$ matrix
become increasingly weaker compared to the diagonal terms as the size of the
channel gain matrix ${\bf H}$ increases. Specifically, we propose a message
passing detection (MPD) algorithm which works with the real-valued matched
filtered received vector (whose signal term becomes ${\bf H}^T{\bf H}{\bf x}$,
where ${\bf x}$ is the transmitted vector), and uses a Gaussian approximation
on the off-diagonal terms of the ${\bf H}^T{\bf H}$ matrix. We also propose a
simple estimation scheme which directly obtains an estimate of ${\bf H}^T{\bf
H}$ (instead of an estimate of ${\bf H}$), which is used as an effective
channel estimate in the MPD algorithm. We refer to this receiver as the {\em
channel hardening-exploiting message passing (CHEMP)} receiver. The proposed
CHEMP receiver achieves very good performance in large-scale MIMO systems
(e.g., in systems with 16 to 128 uplink users and 128 base station antennas).
For the considered large MIMO settings, the complexity of the proposed MPD
algorithm is almost the same as or less than that of the minimum mean square
error (MMSE) detection. This is because the MPD algorithm does not need a
matrix inversion. It also achieves a significantly better performance compared
to MMSE and other message passing detection algorithms using MMSE estimate of
${\bf H}$. We also present a convergence analysis of the proposed MPD
algorithm. Further, we design optimized irregular low density parity check
(LDPC) codes specific to the considered large MIMO channel and the CHEMP
receiver through EXIT chart matching. The LDPC codes thus obtained achieve
improved coded bit error rate performance compared to off-the-shelf irregular
LDPC codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3070</identifier>
 <datestamp>2013-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3070</id><created>2013-10-11</created><authors><author><keyname>Mishra</keyname><forenames>Umakant</forenames></author></authors><title>Inventions on reducing keyboard size: A TRIZ based analysis</title><categories>cs.HC</categories><comments>Published in TRIZsite Journal, March 2005, also available in
  http://papers.ssrn.com/abstract=931683</comments><doi>10.2139/ssrn.931683</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A conventional computer keyboard consists of as many as 101 keys. The
keyboard has several sections, such as text entry section, navigation section,
and numeric keypad etc. and each having several keys on the keyboard.
  The size of the keyboard is a major inconvenience for portable computers, as
they cannot be carried easily. Thus there are certain circumstances which
compels to reduce the size of a keyboard.
  Reducing the size of a keyboard leads to several problems. A reduced size
keyboard may not contain all the keys available on a full size keyboard; a
reduced size keyboard may not be convenient to operate, a reduced size keyboard
may have a different key layout which is difficult to learn etc.
  This article illustrates 10 inventions on reducing the size of the keyboard.
Various inventions have attempted to solve the contradictions so that the user
achieves the benefits of both &quot;reduced size&quot; and &quot;typing comfort&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3085</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3085</id><created>2013-10-11</created><updated>2014-03-25</updated><authors><author><keyname>Kourtellaris</keyname><forenames>Christos</forenames></author><author><keyname>Charalambous</keyname><forenames>Charalambos D.</forenames></author><author><keyname>Stavrou</keyname><forenames>Photios A.</forenames></author></authors><title>Source-Channel Matching for Sources with Memory</title><categories>cs.IT math.IT</categories><comments>6 pages. arXiv admin note: substantial text overlap with
  arXiv:1304.6528</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we analyze the probabilistic matching of sources with memory to
channels with memory so that symbol-by-symbol code with memory without
anticipation are optimal, with respect to an average distortion and excess
distortion probability. We show achievability of such a symbolby- symbol code
with memory without anticipation, and we show matching for the Binary Symmetric
Markov source (BSMS(p)) over a first-order symmetric channel with a cost
constraint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3099</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3099</id><created>2013-10-11</created><updated>2014-09-22</updated><authors><author><keyname>Maas</keyname><forenames>Roland</forenames></author><author><keyname>Huemmer</keyname><forenames>Christian</forenames></author><author><keyname>Sehr</keyname><forenames>Armin</forenames></author><author><keyname>Kellermann</keyname><forenames>Walter</forenames></author></authors><title>A Bayesian Network View on Acoustic Model-Based Techniques for Robust
  Speech Recognition</title><categories>cs.LG cs.CL stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article provides a unifying Bayesian network view on various approaches
for acoustic model adaptation, missing feature, and uncertainty decoding that
are well-known in the literature of robust automatic speech recognition. The
representatives of these classes can often be deduced from a Bayesian network
that extends the conventional hidden Markov models used in speech recognition.
These extensions, in turn, can in many cases be motivated from an underlying
observation model that relates clean and distorted feature vectors. By
converting the observation models into a Bayesian network representation, we
formulate the corresponding compensation rules leading to a unified view on
known derivations as well as to new formulations for certain approaches. The
generic Bayesian perspective provided in this contribution thus highlights
structural differences and similarities between the analyzed approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3101</identifier>
 <datestamp>2013-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3101</id><created>2013-10-11</created><authors><author><keyname>Strobl</keyname><forenames>Eric</forenames></author><author><keyname>Visweswaran</keyname><forenames>Shyam</forenames></author></authors><title>Deep Multiple Kernel Learning</title><categories>stat.ML cs.LG</categories><comments>4 pages, 1 figure, 1 table, conference paper</comments><journal-ref>IEEE 12th International Conference on Machine Learning and
  Applications (ICMLA 2013)</journal-ref><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Deep learning methods have predominantly been applied to large artificial
neural networks. Despite their state-of-the-art performance, these large
networks typically do not generalize well to datasets with limited sample
sizes. In this paper, we take a different approach by learning multiple layers
of kernels. We combine kernels at each layer and then optimize over an estimate
of the support vector machine leave-one-out error rather than the dual
objective function. Our experiments on a variety of datasets show that each
layer successively increases performance with only a few base kernels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3107</identifier>
 <datestamp>2013-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3107</id><created>2013-10-11</created><authors><author><keyname>Zawirski</keyname><forenames>Marek</forenames><affiliation>INRIA Rocquencourt, LIP6</affiliation></author><author><keyname>Bieniusa</keyname><forenames>Annette</forenames><affiliation>CITI</affiliation></author><author><keyname>Balegas</keyname><forenames>Valter</forenames><affiliation>CITI</affiliation></author><author><keyname>Duarte</keyname><forenames>S&#xe9;rgio</forenames><affiliation>CITI</affiliation></author><author><keyname>Baquero</keyname><forenames>Carlos</forenames><affiliation>Universidade do Minho Departamento de Inform&#xe1;tica</affiliation></author><author><keyname>Shapiro</keyname><forenames>Marc</forenames><affiliation>INRIA Rocquencourt, LIP6</affiliation></author><author><keyname>Pregui&#xe7;a</keyname><forenames>Nuno</forenames><affiliation>CITI</affiliation></author></authors><title>SwiftCloud: Fault-Tolerant Geo-Replication Integrated all the Way to the
  Client Machine</title><categories>cs.DC cs.DB</categories><proxy>ccsd</proxy><report-no>RR-8347</report-no><journal-ref>N&amp;deg; RR-8347 (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Client-side logic and storage are increasingly used in web and mobile
applications to improve response time and availability. Current approaches tend
to be ad-hoc and poorly integrated with the server-side logic. We present a
principled approach to integrate client- and server-side storage. We support
mergeable and strongly consistent transactions that target either client or
server replicas and provide access to causally-consistent snapshots
efficiently. In the presence of infrastructure faults, a client-assisted
failover solution allows client execution to resume immediately and seamlessly
access consistent snapshots without waiting. We implement this approach in
SwiftCloud, the first transactional system to bring geo-replication all the way
to the client machine. Example applications show that our programming model is
useful across a range of application areas. Our experimental evaluation shows
that SwiftCloud provides better fault tolerance and at the same time can
improve both latency and throughput by up to an order of magnitude, compared to
classical geo-replication techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3111</identifier>
 <datestamp>2013-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3111</id><created>2013-10-11</created><authors><author><keyname>Mishra</keyname><forenames>Umakant</forenames></author></authors><title>Keyboard for inputting Chinese language</title><categories>cs.HC</categories><comments>Published in TRIZsite Journal, Apr 2005, also available in
  http://papers.ssrn.com/abstract=932271</comments><doi>10.2139/ssrn.932271</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the structure of Chinese characters are very different, it is very
difficult to input Chinese characters into computer quickly and conveniently.
The conventional keyboard does not support the pictorial characters in Chinese
language. There are 3000 to 6000 commonly used pictorial Chinese characters
(Hanzi).
  There are a few existing systems which include &quot;PinYin&quot; (phonetic) system, a
combination of the PinYin system and character form techniques, whole character
encoding, stroke input encoding, and stoke form encoding. Each of the methods
have their own advantages and disadvantages. This article describes two
inventions on inputting Chinese language through a standard keyboard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3115</identifier>
 <datestamp>2013-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3115</id><created>2013-10-11</created><authors><author><keyname>Mishra</keyname><forenames>Umakant</forenames></author></authors><title>Keyboards for inputting Japanese language -A study based on US patents</title><categories>cs.HC</categories><comments>Published in TRIZsite Journal, April 2005, also available at
  http://papers.ssrn.com/abstract=932272</comments><doi>10.2139/ssrn.932272</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The most commonly used Japanese alphabets are Kanji, Hiragana and Katakana.
The Kanji alphabet includes pictographs or ideographic characters that were
adopted from the Chinese alphabet. Hiragana is used to spell words of Japanese
origin, while Katakana is used to spell words of western or other foreign
origin.
  Two methods are commonly used to input Japanese to the computer. One, the
'kana input method' that uses a keyboard having 46 Japanese iroha (or kana)
letter keys. The other method is 'Roma-ji input method', where the Japanese
letters are composed of English input from a standard QWERTY keyboard. Both the
methods have their advantages and disadvantages.
  This article analyses two inventions on inputting Japanese language through a
computer keyboard. One invention uses a standard English keyboard to input
Japanese characters, the other invention uses a standard mobile phone key board
to input the Japanese characters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3119</identifier>
 <datestamp>2013-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3119</id><created>2013-10-11</created><authors><author><keyname>Br&#xe1;zdil</keyname><forenames>Tom&#xe1;&#x161;</forenames></author><author><keyname>Chen</keyname><forenames>Taolue</forenames></author><author><keyname>Forejt</keyname><forenames>Vojt&#x11b;ch</forenames></author><author><keyname>Novotn&#xfd;</keyname><forenames>Petr</forenames></author><author><keyname>Simaitis</keyname><forenames>Aistis</forenames></author></authors><title>Solvency Markov Decision Processes with Interest</title><categories>cs.CE cs.GT</categories><comments>25 pages. This is a full version of a paper accepted at FST&amp;TCS 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Solvency games, introduced by Berger et al., provide an abstract framework
for modelling decisions of a risk-averse investor, whose goal is to avoid ever
going broke. We study a new variant of this model, where, in addition to
stochastic environment and fixed increments and decrements to the investor's
wealth, we introduce interest, which is earned or paid on the current level of
savings or debt, respectively.
  We study problems related to the minimum initial wealth sufficient to avoid
bankruptcy (i.e. steady decrease of the wealth) with probability at least p. We
present an exponential time algorithm which approximates this minimum initial
wealth, and show that a polynomial time approximation is not possible unless P
= NP. For the qualitative case, i.e. p=1, we show that the problem whether a
given number is larger than or equal to the minimum initial wealth belongs to
both NP and coNP, and show that a polynomial time algorithm would yield a
polynomial time algorithm for mean-payoff games, existence of which is a
longstanding open problem. We also identify some classes of solvency MDPs for
which this problem is in P. In all above cases the algorithms also give
corresponding bankruptcy avoiding strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3128</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3128</id><created>2013-10-11</created><authors><author><keyname>Del Genio</keyname><forenames>Charo I.</forenames></author><author><keyname>House</keyname><forenames>Thomas</forenames></author></authors><title>Endemic infections are always possible on regular networks</title><categories>physics.soc-ph cs.SI q-bio.PE</categories><comments>5 pages, 4 figures</comments><doi>10.1103/PhysRevE.88.040801</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the dependence of the largest component in regular networks on the
clustering coefficient, showing that its size changes smoothly without
undergoing a phase transition. We explain this behaviour via an analytical
approach based on the network structure, and provide an exact equation
describing the numerical results. Our work indicates that intrinsic structural
properties always allow the spread of epidemics on regular networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3138</identifier>
 <datestamp>2013-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3138</id><created>2013-10-11</created><authors><author><keyname>Stattner</keyname><forenames>Erick</forenames></author><author><keyname>Collard</keyname><forenames>Martine</forenames></author></authors><title>Dynamiques globales et locales dans un r\'eseau de
  t\'el\'ecommunications</title><categories>cs.SI</categories><comments>10 pages, 6 figures, Conf\'erence MARAMI 2013</comments><journal-ref>4e Conf\'erence sur les mod\`eles et l'analyse des r\'eseaux :
  Approches math\'ematiques et informatiques, MARAMI 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional network generation models attempt to replicate global structural
properties (degree distribution, average distance, clustering coefficient,
communities, etc.) through synthetic link formation mechanisms such as triadic
closure or preferential attachment. In this work, we study the evolution of a
very big communication network coming from mobile telephony and we analyse the
link formation process. A first study conducted on the standard mechanisms
allows observing that several mechanisms are responsible for the properties
observed in this network. In a second study, we characterize more precisely the
link formation process by searching for correlations between the probability of
creating a new link and some individual properties such as the degree, the
clustering coefficient and the age of the nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3140</identifier>
 <datestamp>2013-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3140</id><created>2013-10-11</created><authors><author><keyname>Mishra</keyname><forenames>Umakant</forenames></author></authors><title>10 Inventions on Keyboard Key Switch Mechanism</title><categories>cs.HC</categories><comments>Published in TRIZsite Journal, May 2005, also available in
  http://papers.ssrn.com/abstract=932273</comments><doi>10.2139/ssrn.932273</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The key switches of the keyboard of a computer system are generally comprised
of a key cap having a plunger, conductive rubber disposed above a membrane
circuit and compressed by the plunger to trigger the membrane circuit causing
it to produce an electric signal to the computer.
  Some key switches use springs. Some other keyboards use rubber domes or a
dome sheet, which do the function of springs. When the user depresses the key
button the spring or domes collapse. The key switch depresses the key stem,
which actuates the button on the membrane circuit. When the user releases the
button the springs or rubber domes push the button up to the rest position.
  This article illustrates 10 inventions on different key switch mechanism. The
inventions are selected from US patent database. The inventions try to simplify
the mechanism, reduce manufacturing cost, increase accuracy and reliability,
and increase strength and robustness of the keys.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3145</identifier>
 <datestamp>2013-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3145</id><created>2013-10-11</created><authors><author><keyname>Mishra</keyname><forenames>Umakant</forenames></author></authors><title>10 Inventions on special type of keyboards -A study based on US patents</title><categories>cs.HC</categories><comments>Published in TRIZsite Journal, May 2005, also available in
  http://papers.ssrn.com/abstract=932275</comments><doi>10.2139/ssrn.932275</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A keyboard is the most important input device for a computer. It is used with
various types and sizes of computer. But the same standard keyboard will not
work efficiently with different types of computers at different environments.
There is a need to develop special keyboards to meet special requirements.
  This article illustrates 10 inventions on special types of keyboards. The
special keyboard are used in special computers or computers used for special
purposes. A special keyboard is to be understood as a keyboard having features
beyond a normal keyboard.
  The example of special keyboards are, a children's keyboard which may be
colorful and attractive, a keyboard in public place need to be more robust, the
keyboard in a palmtop may have less number of keys, the keyboard for a game
station may have special attachments, a multimedia keyboard may have CD ROM and
speakers, a wireless keyboard may have remote control features, a touch
sensitive keyboard may have sensors on the keys and so on.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3153</identifier>
 <datestamp>2013-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3153</id><created>2013-10-11</created><authors><author><keyname>Duetting</keyname><forenames>Paul</forenames></author><author><keyname>Henzinger</keyname><forenames>Monika</forenames></author><author><keyname>Starnberger</keyname><forenames>Martin</forenames></author></authors><title>Valuation Compressions in VCG-Based Combinatorial Auctions</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The focus of classic mechanism design has been on truthful direct-revelation
mechanisms. In the context of combinatorial auctions the truthful
direct-revelation mechanism that maximizes social welfare is the VCG mechanism.
For many valuation spaces computing the allocation and payments of the VCG
mechanism, however, is a computationally hard problem. We thus study the
performance of the VCG mechanism when bidders are forced to choose bids from a
subspace of the valuation space for which the VCG outcome can be computed
efficiently. We prove improved upper bounds on the welfare loss for
restrictions to additive bids and upper and lower bounds for restrictions to
non-additive bids. These bounds show that the welfare loss increases in
expressiveness. All our bounds apply to equilibrium concepts that can be
computed in polynomial time as well as to learning outcomes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3165</identifier>
 <datestamp>2013-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3165</id><created>2013-10-11</created><authors><author><keyname>Mishra</keyname><forenames>Umakant</forenames></author></authors><title>10 Inventions on Key Guides and Keyboard Templates</title><categories>cs.HC</categories><comments>Published in TRIZsite Journal, June 2005, also available at
  http://papers.ssrn.com/abstract=932276</comments><doi>10.2139/ssrn.932276</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A keyboard has many function keys and each function key can have multiple
functions when used with control, shift and alt keys, it is difficult for a
user to remember the functionality of the function keys. We need a mechanism to
indicate the operations assigned to each function key for different software
programs. A keyboard guide or template is used for this purpose.
  This article illustrates 10 inventions on keyboard key guide and function key
templates selected from US patent database. Various mechanisms of keyboard
templates have been proposed, including static, dynamic, manual, mechanical,
onscreen display and others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3174</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3174</id><created>2013-10-11</created><updated>2015-06-19</updated><authors><author><keyname>Clement</keyname><forenames>Benjamin</forenames></author><author><keyname>Roy</keyname><forenames>Didier</forenames></author><author><keyname>Oudeyer</keyname><forenames>Pierre-Yves</forenames></author><author><keyname>Lopes</keyname><forenames>Manuel</forenames></author></authors><title>Multi-Armed Bandits for Intelligent Tutoring Systems</title><categories>cs.AI</categories><journal-ref>Journal of Educational Data Mining (JEDM), Vol 7, No 2, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an approach to Intelligent Tutoring Systems which adaptively
personalizes sequences of learning activities to maximize skills acquired by
students, taking into account the limited time and motivational resources. At a
given point in time, the system proposes to the students the activity which
makes them progress faster. We introduce two algorithms that rely on the
empirical estimation of the learning progress, RiARiT that uses information
about the difficulty of each exercise and ZPDES that uses much less knowledge
about the problem.
  The system is based on the combination of three approaches. First, it
leverages recent models of intrinsically motivated learning by transposing them
to active teaching, relying on empirical estimation of learning progress
provided by specific activities to particular students. Second, it uses
state-of-the-art Multi-Arm Bandit (MAB) techniques to efficiently manage the
exploration/exploitation challenge of this optimization process. Third, it
leverages expert knowledge to constrain and bootstrap initial exploration of
the MAB, while requiring only coarse guidance information of the expert and
allowing the system to deal with didactic gaps in its knowledge. The system is
evaluated in a scenario where 7-8 year old schoolchildren learn how to
decompose numbers while manipulating money. Systematic experiments are
presented with simulated students, followed by results of a user study across a
population of 400 school children.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3195</identifier>
 <datestamp>2013-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3195</id><created>2013-10-11</created><authors><author><keyname>Huschenbett</keyname><forenames>Martin</forenames></author><author><keyname>Kufleitner</keyname><forenames>Manfred</forenames></author></authors><title>Ehrenfeucht-Fraisse Games on Omega-Terms</title><categories>cs.FL cs.LO math.GR</categories><acm-class>F.4.1; F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fragments of first-order logic over words can often be characterized in terms
of finite monoids or finite semigroups. Usually these algebraic descriptions
yield decidability of the question whether a given regular language is
definable in a particular fragment. An effective algebraic characterization can
be obtained from identities of so-called omega-terms. In order to show that a
given fragment satisfies some identity of omega-terms, one can use
Ehrenfeucht-Fraisse games on word instances of the omega-terms. The resulting
proofs often require a significant amount of book-keeping with respect to the
constants involved. In this paper we introduce Ehrenfeucht-Fraisse games on
omega-terms. To this end we assign a labeled linear order to every omega-term.
Our main theorem shows that a given fragment satisfies some identity of
omega-terms if and only if Duplicator has a winning strategy for the game on
the resulting linear orders. This allows to avoid the book-keeping. As an
application of our main result, we show that one can decide in exponential time
whether all aperiodic monoids satisfy some given identity of omega-terms,
thereby improving a result of McCammond (Int. J. Algebra Comput., 2001).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3197</identifier>
 <datestamp>2013-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3197</id><created>2013-10-11</created><authors><author><keyname>Erlich</keyname><forenames>Yaniv</forenames></author><author><keyname>Narayanan</keyname><forenames>Arvind</forenames></author></authors><title>Routes for breaching and protecting genetic privacy</title><categories>q-bio.GN cs.CR stat.AP</categories><comments>Draft for comments</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are entering the era of ubiquitous genetic information for research,
clinical care, and personal curiosity. Sharing these datasets is vital for
rapid progress in understanding the genetic basis of human diseases. However,
one growing concern is the ability to protect the genetic privacy of the data
originators. Here, we technically map threats to genetic privacy and discuss
potential mitigation strategies for privacy-preserving dissemination of genetic
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3202</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3202</id><created>2013-10-11</created><updated>2013-11-07</updated><authors><author><keyname>Couvreur</keyname><forenames>Alain</forenames></author><author><keyname>Otmani</keyname><forenames>Ayoub</forenames></author><author><keyname>Tillich</keyname><forenames>Jean-Pierre</forenames></author></authors><title>New Identities Relating Wild Goppa Codes</title><categories>cs.IT math.IT math.NT</categories><comments>14 pages</comments><journal-ref>Finite Fields Appl, 29, 178-197, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a given support $L \in \mathbb{F}_{q^m}^n$ and a polynomial $g\in
\mathbb{F}_{q^m}[x]$ with no roots in $\mathbb{F}_{q^m}$, we prove equality
between the $q$-ary Goppa codes $\Gamma_q(L,N(g)) = \Gamma_q(L,N(g)/g)$ where
$N(g)$ denotes the norm of $g$, that is $g^{q^{m-1}+\cdots +q+1}.$ In
particular, for $m=2$, that is, for a quadratic extension, we get
$\Gamma_q(L,g^q) = \Gamma_q(L,g^{q+1})$. If $g$ has roots in
$\mathbb{F}_{q^m}$, then we do not necessarily have equality and we prove that
the difference of the dimensions of the two codes is bounded above by the
number of distinct roots of $g$ in $\mathbb{F}_{q^m}$. These identities provide
numerous code equivalences and improved designed parameters for some families
of classical Goppa codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3203</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3203</id><created>2013-10-09</created><authors><author><keyname>Saha</keyname><forenames>Dipankar</forenames></author><author><keyname>Basak</keyname><forenames>Subhramita</forenames></author><author><keyname>Mukherjee</keyname><forenames>Sagar</forenames></author><author><keyname>Chatterjee</keyname><forenames>Sayan</forenames></author><author><keyname>Sarkar</keyname><forenames>C. K.</forenames></author></authors><title>Implementation of the Cluster Based Tunable Sleep Transistor Cell Power
  Gating Technique for a 4x4 Multiplier Circuit</title><categories>cs.OH</categories><comments>another version of this work can be downloaded from &quot;International
  Journal of Computer Applications&quot;</comments><journal-ref>International Journal of Computer Applications, Volume 66, Number
  23, pages 35-40, Year of Publication 2013</journal-ref><doi>10.5120/11259-6539</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A modular, programmable, and high performance Power Gating strategy, called
cluster based tunable sleep transistor cell Power Gating, has been introduced
in the present paper with a few modifications. Furthermore, a detailed
comparison of its performance with some of the other conventional Power Gating
schemes; such as Cluster Based Sleep Transistor Design (CBSTD), Distributed
Sleep Transistor Network (DSTN) etc.; has also been presented here. Considering
the constraints of power consumption, performance, and the area overhead, while
doing the actual implementation of any Power Gating scheme, it becomes
important to deal with the various design issues like the proper sizing of the
sleep transistors (STs), controlling the voltage drop (IR drop) across the STs,
and obviously maintaining a desired performance with lower amount of delay
degradation. With this notion, we tried to find out an efficient Power Gating
strategy which can reduce the overall power consumption of any CMOS circuit by
virtue of reducing the standby mode leakage current. Taking the different
performance parameters into account, for an example circuit, which is actually
the conventional 4x4 multiplier design, we found that the modified tunable
sleep transistor cell Power Gating gives very much promising results. The
reported architecture of the 4x4 multiplier with the tunable sleep transistor
cell Power Gating, is designed using 45 nm technology and it consumes
1.3638x10-5 Watt of Average Power while being operated with the nominal case of
the bit configuration word, that is, 1000. ...........
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3221</identifier>
 <datestamp>2013-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3221</id><created>2013-10-11</created><authors><author><keyname>Kotagiri</keyname><forenames>Vamsi Sashank</forenames></author></authors><title>The 10-point and 12-point Number Theoretic Hilbert Transform</title><categories>cs.CR</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents 10-point and 12-point versions of the recently introduced
number theoretic Hilbert (NHT) transforms. Such transforms have applications in
signal processing and scrambling. Polymorphic solutions with respect to
different moduli for each of the two cases have been found. The multiplicity of
solutions for the same moduli increases their applicability to cryptography.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3225</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3225</id><created>2013-10-11</created><authors><author><keyname>Lloyd</keyname><forenames>Seth</forenames></author></authors><title>A Turing test for free will</title><categories>quant-ph cs.AI physics.hist-ph</categories><comments>20 pages, plain TeX</comments><journal-ref>Phil. Trans. Roy. Soc. A 28, 3597-3610 (2012)</journal-ref><doi>10.1098/rsta.2011.0331</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Before Alan Turing made his crucial contributions to the theory of
computation, he studied the question of whether quantum mechanics could throw
light on the nature of free will. This article investigates the roles of
quantum mechanics and computation in free will. Although quantum mechanics
implies that events are intrinsically unpredictable, the `pure stochasticity'
of quantum mechanics adds only randomness to decision making processes, not
freedom. By contrast, the theory of computation implies that even when our
decisions arise from a completely deterministic decision-making process, the
outcomes of that process can be intrinsically unpredictable, even to --
especially to -- ourselves. I argue that this intrinsic computational
unpredictability of the decision making process is what give rise to our
impression that we possess free will. Finally, I propose a `Turing test' for
free will: a decision maker who passes this test will tend to believe that he,
she, or it possesses free will, whether the world is deterministic or not.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3230</identifier>
 <datestamp>2013-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3230</id><created>2013-10-08</created><authors><author><keyname>Riemann</keyname><forenames>Ute</forenames></author></authors><title>Value-chain oriented identification of indicators to establish a
  comprehensive process improvement framework</title><categories>cs.OH</categories><comments>Process framework, value chain, business processes, indicators</comments><journal-ref>International Journal of Managing Value and Supply Chains (IJMVSC)
  Vol.4, No. 3, September 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The process development and optimization potential needs to be driven by the
individial coporate value chain. The identification of this specific value
chain and the related indicators is essential to limit the scope of any
analysis and optimization to the core business The process framework consisting
of clearly defined value chain, the related processes and the corresponding
indicators is a pre-requisite for a meaningful and efficient process analysis
and continuous process optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3233</identifier>
 <datestamp>2013-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3233</id><created>2013-10-09</created><authors><author><keyname>Du</keyname><forenames>Jia</forenames></author><author><keyname>Goh</keyname><forenames>Alvina</forenames></author><author><keyname>Qiu</keyname><forenames>Anqi</forenames></author></authors><title>Bayesian Estimation of White Matter Atlas from High Angular Resolution
  Diffusion Imaging</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a Bayesian probabilistic model to estimate the brain white matter
atlas from high angular resolution diffusion imaging (HARDI) data. This model
incorporates a shape prior of the white matter anatomy and the likelihood of
individual observed HARDI datasets. We first assume that the atlas is generated
from a known hyperatlas through a flow of diffeomorphisms and its shape prior
can be constructed based on the framework of large deformation diffeomorphic
metric mapping (LDDMM). LDDMM characterizes a nonlinear diffeomorphic shape
space in a linear space of initial momentum uniquely determining diffeomorphic
geodesic flows from the hyperatlas. Therefore, the shape prior of the HARDI
atlas can be modeled using a centered Gaussian random field (GRF) model of the
initial momentum. In order to construct the likelihood of observed HARDI
datasets, it is necessary to study the diffeomorphic transformation of
individual observations relative to the atlas and the probabilistic
distribution of orientation distribution functions (ODFs). To this end, we
construct the likelihood related to the transformation using the same
construction as discussed for the shape prior of the atlas. The probabilistic
distribution of ODFs is then constructed based on the ODF Riemannian manifold.
We assume that the observed ODFs are generated by an exponential map of random
tangent vectors at the deformed atlas ODF. Hence, the likelihood of the ODFs
can be modeled using a GRF of their tangent vectors in the ODF Riemannian
manifold. We solve for the maximum a posteriori using the
Expectation-Maximization algorithm and derive the corresponding update
equations. Finally, we illustrate the HARDI atlas constructed based on a
Chinese aging cohort of 94 adults and compare it with that generated by
averaging the coefficients of spherical harmonics of the ODF across subjects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3240</identifier>
 <datestamp>2013-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3240</id><created>2013-10-11</created><updated>2013-11-05</updated><authors><author><keyname>Candes</keyname><forenames>Emmanuel</forenames></author><author><keyname>Li</keyname><forenames>Xiaodong</forenames></author><author><keyname>Soltanolkotabi</keyname><forenames>Mahdi</forenames></author></authors><title>Phase Retrieval from Coded Diffraction Patterns</title><categories>cs.IT math.FA math.IT math.NA math.OC math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the question of recovering the phase of an object from
intensity-only measurements, a problem which naturally appears in X-ray
crystallography and related disciplines. We study a physically realistic setup
where one can modulate the signal of interest and then collect the intensity of
its diffraction pattern, each modulation thereby producing a sort of coded
diffraction pattern. We show that PhaseLift, a recent convex programming
technique, recovers the phase information exactly from a number of random
modulations, which is polylogarithmic in the number of unknowns. Numerical
experiments with noiseless and noisy data complement our theoretical analysis
and illustrate our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3248</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3248</id><created>2013-10-11</created><authors><author><keyname>Lv</keyname><forenames>Tiejun</forenames></author><author><keyname>Zhang</keyname><forenames>Zhang</forenames></author><author><keyname>Yang</keyname><forenames>Shaoshi</forenames></author></authors><title>A low complexity approach of combining cooperative diversity and
  multiuser diversity in multiuser cooperative networks</title><categories>cs.IT math.IT</categories><comments>10 pages, 7 figures, accepted by IEEE Transactions on Signal
  Processing, to be fully published in Nov. or Dec. 2013</comments><journal-ref>IEEE Transactions on Signal Processing, vol. 61, no. 24, pp.
  6247-6256, Dec. 2013</journal-ref><doi>10.1109/TSP.2013.2284484</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the scheduling scheme to combine cooperative
diversity (CD) and multiuser diversity (MUD) in multiuser cooperative networks
under the time resource allocation (TRA) framework in which the whole
transmission is divided into two phases: the broadcast phase and the relay
phase. The broadcast phase is for direct transmission whereas the relay phase
is for relay transmission. Based on this TRA framework, a user selection based
low complexity relay protocol (US-LCRP) is proposed to combine CD and MUD. In
each time slot (TS) of the broadcast phase, a &quot;best&quot; user is selected for
transmission in order to obtain MUD. In the relay phase, the relays forward the
messages of some specific users in a fixed order and then invoke the limited
feedback information to achieve CD. We demonstrate that the
diversity-multiplexing tradeoff (DMT) of the US-LCRP is superior to that of the
existing schemes, where more TSs are allocated for direct transmission in order
to jointly exploit CD and MUD. Our analytical and numerical results show that
the US-LCRP constitutes a more efficient resource utilization approach than the
existing schemes. Additionally, the US-LCRP can be implemented with low
complexity because only the direct links' channel state information (CSI) is
estimated during the whole transmission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3252</identifier>
 <datestamp>2013-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3252</id><created>2013-10-11</created><authors><author><keyname>Andoni</keyname><forenames>Alexandr</forenames></author><author><keyname>Gupta</keyname><forenames>Anupam</forenames></author><author><keyname>Krauthgamer</keyname><forenames>Robert</forenames></author></authors><title>Towards (1+\epsilon)-Approximate Flow Sparsifiers</title><categories>cs.DS math.CO</categories><comments>Full version of a paper accepted to SODA 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A useful approach to &quot;compress&quot; a large network $G$ is to represent it with a
{\em flow-sparsifier}, i.e., a small network $H$ that supports the same flows
as $G$, up to a factor $q \geq 1$ called the quality of sparsifier.
Specifically, we assume the network $G$ contains a set of $k$ terminals $T$,
shared with the network $H$, i.e., $T\subseteq V(G)\cap V(H)$, and we want $H$
to preserve all multicommodity flows that can be routed between the terminals
$T$. The challenge is to construct $H$ that is small.
  These questions have received a lot of attention in recent years, leading to
some known tradeoffs between the sparsifier's quality $q$ and its size
$|V(H)|$. Nevertheless, it remains an outstanding question whether every $G$
admits a flow-sparsifier $H$ with quality $q=1+\epsilon$, or even $q=O(1)$, and
size $|V(H)|\leq f(k,\epsilon)$ (in particular, independent of $|V(G)|$ and the
edge capacities). Making a first step in this direction, we present new
constructions for several scenarios:
  * Our main result is that for quasi-bipartite networks $G$, one can construct
a $(1+\epsilon)$-flow-sparsifier of size $\poly(k/\eps)$. In contrast, exact
($q=1$) sparsifiers for this family of networks are known to require size
$2^{\Omega(k)}$.
  * For networks $G$ of bounded treewidth $w$, we construct a flow-sparsifier
with quality $q=O(\log w / \log\log w)$ and size $O(w\cdot \poly(k))$.
  * For general networks $G$, we construct a {\em sketch} $sk(G)$, that stores
all the feasible multicommodity flows up to factor $q=1+\eps$, and its size
(storage requirement) is $f(k,\epsilon)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3257</identifier>
 <datestamp>2014-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3257</id><created>2013-10-11</created><updated>2014-02-17</updated><authors><author><keyname>Bremner</keyname><forenames>Murray</forenames></author><author><keyname>Hu</keyname><forenames>Jiaxiong</forenames></author><author><keyname>Oeding</keyname><forenames>Luke</forenames></author></authors><title>The 3 x 3 x 3 hyperdeterminant as a polynomial in the fundamental
  invariants for SL(3,C) x SL(3,C) x SL(3,C)</title><categories>math.AG cs.SC math.RT</categories><comments>10 pages, to appear in Mathematics in Computer Science (Special Issue
  on Computational Algebraic Geometry)</comments><msc-class>Primary 13A50, Secondary 15A72, 17B10</msc-class><doi>10.1007/s11786-014-0186-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We briefly review previous work on the invariant theory of 3 x 3 x 3 arrays.
We then recall how to generate arrays of arbitrary size m_1 x ... x m_k with
hyperdeterminant 0. Our main result is an explicit formula for the 3 x 3 x 3
hyperdeterminant as a polynomial in the fundamental invariants of degrees 6, 9
and 12 for the action of the Lie group SL(3,C) x SL(3,C) x SL(3,C). We apply
our calculations to Nurmiev's classification of normal forms for 3 x 3 x 3
arrays.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3262</identifier>
 <datestamp>2013-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3262</id><created>2013-10-11</created><authors><author><keyname>Chailloux</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Gutoski</keyname><forenames>Gus</forenames></author><author><keyname>Sikora</keyname><forenames>Jamie</forenames></author></authors><title>Optimal bounds for quantum weak oblivious transfer</title><categories>quant-ph cs.CR</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Oblivious transfer is a fundamental cryptographic primitive in which Bob
transfers one of two bits to Alice in such a way that Bob cannot know which of
the two bits Alice has learned. We present an optimal security bound for
quantum oblivious transfer protocols under a natural and demanding definition
of what it means for Alice to cheat. Our lower bound is a smooth tradeoff
between the probability B with which Bob can guess Alice's bit choice and the
probability A with which Alice can guess both of Bob's bits given that she
learns one of the bits with certainty. We prove that 2B + A is greater than or
equal to 2 in any quantum protocol for oblivious transfer, from which it
follows that one of the two parties must be able to cheat with probability at
least 2/3. We prove that this bound is optimal by exhibiting a family of
protocols whose cheating probabilities can be made arbitrarily close to any
point on the tradeoff curve.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3265</identifier>
 <datestamp>2014-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3265</id><created>2013-10-11</created><updated>2014-01-29</updated><authors><author><keyname>La Guardia</keyname><forenames>Giuliano Gadioli</forenames></author></authors><title>On Negacyclic MDS-Convolutional Codes</title><categories>quant-ph cs.IT math.IT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1212.4654.
  Accepted for publication in Linear Algebra and its Applications - LAA</comments><journal-ref>Linear Algebra and its Applications, 448 (2014) 85--96</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  New families of classical and quantum optimal negacyclic convolutional codes
are constructed in this paper. This optimality is in the sense that they attain
the classical (quantum) generalized Singleton bound. The constructions
presented in this paper are performed algebraically and not by computational
search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3268</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3268</id><created>2013-10-11</created><authors><author><keyname>Mishra</keyname><forenames>Umakant</forenames></author></authors><title>10 Inventions on collapsible keyboards: A TRIZ based analysis</title><categories>cs.HC</categories><comments>Published in TRIZsite Journal, March 2005, also available in
  http://papers.ssrn.com/abstract=932268. arXiv admin note: text overlap with
  arXiv:1310.3070</comments><doi>10.2139/ssrn.932268</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although a bigger keyboard is often comfortable to work with, they cannot be
fit into laptop and small size computer boxes. The portable and handheld
computers need small size keyboards. So there is a need to reduce the size of
the keyboard to fit into the laptop box. There are various mechanisms to reduce
the size of the keyboard; collapsible keyboard is one of them.
  Although different inventions intend to achieve the same objective of
reducing the keyboard size, they all differ in their mechanism. For example,
some invention uses a compression mechanism, some invention uses a folding
mechanism, and some invention uses a collapsing mechanism and so on.
  This article illustrates 10 inventions on collapsible keyboards from US
patent database. Each case is analyzed from a TRIZ perspective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3297</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3297</id><created>2013-10-11</created><authors><author><keyname>Bates</keyname><forenames>Daniel J.</forenames></author><author><keyname>Gross</keyname><forenames>Elizabeth</forenames></author><author><keyname>Leykin</keyname><forenames>Anton</forenames></author><author><keyname>Rodriguez</keyname><forenames>Jose Israel</forenames></author></authors><title>Bertini for Macaulay2</title><categories>math.AG cs.MS</categories><msc-class>65H10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerical algebraic geometry is the field of computational mathematics
concerning the numerical solution of polynomial systems of equations. Bertini,
a popular software package for computational applications of this field,
includes implementations of a variety of algorithms based on polynomial
homotopy continuation. The Macaulay2 package Bertini.m2 provides an interface
to Bertini, making it possible to access the core run modes of Bertini in
Macaulay2. With these run modes, users can find approximate solutions to
zero-dimensional systems and positive-dimensional systems, test numerically
whether a point lies on a variety, sample numerically from a variety, and
perform parameter homotopy runs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3307</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3307</id><created>2013-10-11</created><authors><author><keyname>Hernandez-Castro</keyname><forenames>Julio</forenames></author><author><keyname>Rossman</keyname><forenames>Jeremy</forenames></author></authors><title>Measuring Software Diversity, with Applications to Security</title><categories>cs.CR cs.SE</categories><comments>10 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we briefly introduce and discuss some of the diversity measures
used in Ecology. After a succinct description and analysis of the most relevant
ones, we single out the Shannon-Weiner index. We justify why it is the most
informative and relevant one for measuring software diversity. Then, we show
how it can be used for effectively assessing the diversity of various real
software ecosystems. We discover in the process a frequently overlooked
software monopoly, and its key security implications. We finally extract some
conclusions from the results obtained, focusing mostly on their security
implications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3308</identifier>
 <datestamp>2013-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3308</id><created>2013-10-11</created><updated>2013-10-17</updated><authors><author><keyname>Anderson</keyname><forenames>Eric</forenames></author><author><keyname>Li</keyname><forenames>Sihan</forenames></author><author><keyname>Xie</keyname><forenames>Tao</forenames></author></authors><title>A Preliminary Field Study of Game Programming on Mobile Devices</title><categories>cs.SE</categories><comments>arXiv:1309.5500</comments><report-no>PrMoTo/2013/03</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  TouchDevelop is a new programming environment that allows users to create
applications on mobile devices. Applications created with TouchDevelop have
continued to grow in popularity since TouchDevelop was first released to public
in 2011. This paper presents a field study of 31,699 applications, focusing on
different characteristics between 539 game scripts and all other non-game
applications, as well as what make some game applications more popular than
others to users. The study provides a list of findings on characteristics of
game scripts and also implications for improving end-user programming of game
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3309</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3309</id><created>2013-10-11</created><authors><author><keyname>Pokluda</keyname><forenames>Alexander</forenames></author></authors><title>Dynamic Resource Management using Operating System-Level Virtualization</title><categories>cs.DC</categories><comments>Undergraduate Honors Thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This thesis expands upon an existing system called Golondrina that performs
autonomic workload management among a cluster of hardware nodes running
operating system-level virtualization. Golondrina works by identifying
localized resource stress situations and attempting to dissipate them by
reallocating system resources and, if necessary, migrating or replicating
virtual machines. It is predicted that, using Golondrina, efficiency of similar
systems can be further improved by achieving greater resource utilization on
the hardware nodes while maintaining resource availability for each virtual
machine.
  The following topics are discussed: virtualization technologies and
associated challenges relating to resource management, the architecture and
design of Golondrina, intelligent resource reallocation based on predefined
policies, and preliminary results demonstrating the effects of a memory
resource management policy on the performance of a web application hosted in a
virtual environment.
  This research makes a significant contribution to the study of virtualized
data centres since currently no other system considers virtual machine
replication and dynamic memory reallocation as an approach to workload
management.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3311</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3311</id><created>2013-10-11</created><authors><author><keyname>Kang</keyname><forenames>Lucas</forenames></author></authors><title>Investigation of Rule 73 as Case Study of Class 4 Long-Distance Cellular
  Automata</title><categories>nlin.CG cs.CC nlin.PS</categories><comments>23 pages (including references and comments), 25 figures, independent
  research, to be published in nlin.CG, nlin.PS and cs.CC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cellular automata (CA) have been utilized for decades as discrete models of
many physical, mathematical, chemical, biological, and computing systems. The
most widely known form of CA, the elementary cellular automaton (ECA), has been
studied in particular due to its simple form and versatility. However, these
dynamic computation systems possess evolutionary rules dependent on a
neighborhood of adjacent cells, which limits their sampling radius and the
environments that they can be used in.
  The purpose of this study was to explore the complex nature of
one-dimensional CA in configurations other than that of the standard ECA.
Namely, &quot;long-distance cellular automata&quot; (LDCA), a construct that had been
described in the past, but never studied. I experimented with a class of LDCA
that used spaced sample cells unlike ECA, and were described by the notation
LDCA-x-y-n, where x and y represented the amount of spacing between the cell
and its left and right neighbors, and n denoted the length of the initial tape
for tapes of finite size. Some basic characteristics of ECA are explored in
this paper, such as seemingly universal behavior, the prevalence of complexity
with varying neighborhoods, and qualitative behavior as a function of x and y
spacing.
  Focusing mainly on purely Class 4 behavior in LDCA-1-2, I found that Rule 73
could potentially be Turing universal through the emulation of a cyclic tag
system, and revealed a connection between the mathematics of binary trees and
Eulerian numbers that might provide insight into unsolved problems in both
fields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3312</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3312</id><created>2013-10-11</created><authors><author><keyname>Syamsuddin</keyname><forenames>Irfan</forenames></author></authors><title>Multicriteria Evaluation and Sensitivity Analysis on Information
  Security</title><categories>cs.CY cs.CR</categories><journal-ref>International Journal of Computer Applications 69(24):22-25, May
  2013</journal-ref><doi>10.5120/12120-8242</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Information security plays a significant role in recent information society.
Increasing number and impact of cyber attacks on information assets have
resulted the increasing awareness among managers that attack on information is
actually attack on organization itself. Unfortunately, particular model for
information security evaluation for management levels is still not well
defined. In this study, decision analysis based on Ternary Analytic Hierarchy
Process (T-AHP) is proposed as a novel model to aid managers who responsible in
making strategic evaluation related to information security issues. In
addition, sensitivity analysis is applied to extend our analysis by using
several &quot;what-if&quot; scenarios in order to measure the consistency of the final
evaluation. Finally, we conclude that the final evaluation made by managers has
a significant consistency shown by sensitivity analysis results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3314</identifier>
 <datestamp>2013-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3314</id><created>2013-10-11</created><updated>2013-10-16</updated><authors><author><keyname>Ngo</keyname><forenames>Hung Q.</forenames></author><author><keyname>Re</keyname><forenames>Christopher</forenames></author><author><keyname>Rudra</keyname><forenames>Atri</forenames></author></authors><title>Skew Strikes Back: New Developments in the Theory of Join Algorithms</title><categories>cs.DB cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evaluating the relational join is one of the central algorithmic and most
well-studied problems in database systems. A staggering number of variants have
been considered including Block-Nested loop join, Hash-Join, Grace, Sort-merge
for discussions of more modern issues). Commercial database engines use finely
tuned join heuristics that take into account a wide variety of factors
including the selectivity of various predicates, memory, IO, etc. In spite of
this study of join queries, the textbook description of join processing is
suboptimal. This survey describes recent results on join algorithms that have
provable worst-case optimality runtime guarantees. We survey recent work and
provide a simpler and unified description of these algorithms that we hope is
useful for theory-minded readers, algorithm designers, and systems
implementors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3322</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3322</id><created>2013-10-11</created><authors><author><keyname>Elhoseiny</keyname><forenames>Mohamed</forenames></author><author><keyname>Faheem</keyname><forenames>Hossam</forenames></author><author><keyname>Nazmy</keyname><forenames>Taymour</forenames></author><author><keyname>Shaaban</keyname><forenames>Eman</forenames></author></authors><title>GPU-Framework for Teamwork Action Recognition</title><categories>cs.DC</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real time processing for teamwork action recognition is a challenge, due to
complex computational models to achieve high system performance. Hence, this
paper proposes a framework based on Graphical Processing Units (GPUs) to
achieve a significant speed up in the performance of role based activity
recognition of teamwork. The framework can be applied in various fields,
especially athletic and military applications. Furthermore, the framework can
be customized for many action recognition applications. The paper presents the
stages of the framework where GPUs are the main tool for performance
improvement. The speedup is achieved by performing video processing and Machine
learning algorithms on GPU. Video processing and machine learning algorithms
covers all computations involved in our framework. Video processing tasks on
involves GPU implementation of Motion detection, segmentation and object
tracking algorithms. In addition, our framework is integrated with GPUCV, a GPU
version of OpenCV functions. Machine learning tasks are supported under our
framework with GPU implementations of Support Vector Machine (SVM) for object
classification and feature discretization, Hidden Marcov Model (HMM) for
activity recognition phase, and ID3 algorithm for role recognition of team
members. The system was tested against UC-Teamwork dataset and speedup of 20X
has been achieved on NVidia 9500GT graphics card (32 500MHZ processors).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3333</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3333</id><created>2013-10-11</created><authors><author><keyname>Balasubramanian</keyname><forenames>Sriramkumar</forenames></author><author><keyname>Nagireddy</keyname><forenames>Raghuram Reddy</forenames></author></authors><title>Visualizing Bags of Vectors</title><categories>cs.IR cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The motivation of this work is two-fold - a) to compare between two different
modes of visualizing data that exists in a bag of vectors format b) to propose
a theoretical model that supports a new mode of visualizing data. Visualizing
high dimensional data can be achieved using Minimum Volume Embedding, but the
data has to exist in a format suitable for computing similarities while
preserving local distances. This paper compares the visualization between two
methods of representing data and also proposes a new method providing sample
visualizations for that method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3341</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3341</id><created>2013-10-12</created><authors><author><keyname>Rz{\ka}&#x17c;ewski</keyname><forenames>Pawe&#x142;</forenames></author></authors><title>Exact Algorithm for Graph Homomorphism and Locally Injective Graph
  Homomorphism</title><categories>cs.DM cs.DS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For graphs $G$ and $H$, a homomorphism from $G$ to $H$ is a function $\varphi
\colon V(G) \to V(H)$, which maps vertices adjacent in $G$ to adjacent vertices
of $H$. A homomorphism is locally injective if no two vertices with a common
neighbor are mapped to a single vertex in $H$. Many cases of graph homomorphism
and locally injective graph homomorphism are NP-complete, so there is little
hope to design polynomial-time algorithms for them. In this paper we present an
algorithm for graph homomorphism and locally injective homomorphism working in
time $\mathcal{O}^*((b + 2)^{|V(G)|})$, where $b$ is the bandwidth of the
complement of $H$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3351</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3351</id><created>2013-10-12</created><authors><author><keyname>Sugiyama</keyname><forenames>Ken-ichi</forenames></author></authors><title>An MDS code associated to an elliptic curve</title><categories>cs.IT math.IT</categories><msc-class>11T71, 14G50, 94B27</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We will construct an MDS(= the most distance separable) code $C$ which admits
a decomposition such that every factor is still MDS. An effective way of
decoding will be also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3353</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3353</id><created>2013-10-12</created><authors><author><keyname>Bellitto</keyname><forenames>Thomas</forenames></author><author><keyname>Marschall</keyname><forenames>Tobias</forenames></author><author><keyname>Sch&#xf6;nhuth</keyname><forenames>Alexander</forenames></author><author><keyname>Klau</keyname><forenames>Gunnar W.</forenames></author></authors><title>Next Generation Cluster Editing</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work aims at improving the quality of structural variant prediction from
the mapped reads of a sequenced genome. We suggest a new model based on cluster
editing in weighted graphs and introduce a new heuristic algorithm that allows
to solve this problem quickly and with a good approximation on the huge graphs
that arise from biological datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3356</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3356</id><created>2013-10-12</created><authors><author><keyname>Li</keyname><forenames>Feitian</forenames></author><author><keyname>Qiao</keyname><forenames>Fei</forenames></author><author><keyname>Wei</keyname><forenames>Qi</forenames></author><author><keyname>Yang</keyname><forenames>Huazhong</forenames></author></authors><title>A Novel Reconfigurable Computing Architecture for Image Signal
  Processing Using Circuit-Switched NoC and Synchronous Dataflow Model</title><categories>cs.AR</categories><comments>ISQED 2014,6 pages,7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a novel reconfigurable architecture is proposed for
multifunctional image signal processing systems. A circuit-switched NoC is used
to provide interconnection because the non-TMD links ensure fixed throughput,
which is a desirable behavior for computational intensive image processing
algorithms compared with packet-switched NoC. Image processing algorithms are
modeled as synchronous dataflow graphs which provide a unified model for
general computing procedure. An image processing system is considered as
several temporally mutually exclusive algorithms. Thus, their dataflow graph
representations could be considered as a group and a merging algorithm could be
applied to generate a union graph while eliminating spatial redundancy for area
consumption optimization. After the union graph have been mapped and routed on
the NoC, the reconfigurable system could be configured to any of its target
image processing algorithms by properly setting the NoC topology. Experiments
show the demo reconfigurable system with two image processing applications cost
26.4% less hardware resource, compared with the non-reconfigurable
implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3358</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3358</id><created>2013-10-12</created><updated>2013-11-03</updated><authors><author><keyname>Rigatos</keyname><forenames>Gerasimos G.</forenames></author></authors><title>A Kalman Filtering approach of improved precision for fault diagnosis in
  distributed parameter systems</title><categories>cs.SY</categories><comments>11 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Derivative-free nonlinear Kalman Filter is proposed for state estimation
and fault diagnosis in distributed parameter systems and particularly in
dynamical systems described by partial differential equations of the nonlinear
wave type. At a first stage, a nonlinear filtering approach for estimating the
dynamics of a 1D nonlinear wave equation, from measurements provided from a
small number of sensors is developed. It is shown that the numerical solution
of the associated partial differential equation results into a set of nonlinear
ordinary differential equations. With the application of diffeomorphism that is
based on differential flatness theory it is shown that an equivalent
description of the system is obtained in the linear canonical (Brunovsky) form.
This transformation enables to obtain local estimates about the state vector of
the system through the application of the standard Kalman Filter recursion. At
a second stage, the local statistical approach to fault diagnosis is used to
perform fault diagnosis for the distributed parameters system by processing
with elaborated statistical tools the differences (residuals) between the
output of the Kalman Filter and the measurements obtained from the distributed
parameter system. Optimal selection of the fault threshold is succeeded by
using the local statistical approach to fault diagnosis. The efficiency of the
proposed filtering approach for performing fault diagnosis in distributed
parameters systems is confirmed through simulation experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3360</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3360</id><created>2013-10-12</created><authors><author><keyname>Bongolan</keyname><forenames>Vena Pearl</forenames></author><author><keyname>Rongo</keyname><forenames>Rocco</forenames></author><author><keyname>Lupiano</keyname><forenames>Valeria</forenames></author><author><keyname>D'Ambrosio</keyname><forenames>Donato</forenames></author><author><keyname>Spataro</keyname><forenames>William</forenames></author><author><keyname>Iovine</keyname><forenames>Giulio</forenames></author></authors><title>A Probabilistic Approach to Risk Mapping for Mt. Etna</title><categories>cs.CE</categories><comments>Most recent presentation of related material was at the IMACS 2013
  World Congress, August 26-30, 2013, San Lorenzo de El Escorial, Spain</comments><msc-class>86-08</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We evaluate susceptibility to lava flows on Mt. Etna based on specially
designed die-toss experiments using probabilities for type, time and place of
activation from the volcano's 400-year recorded history and current studies on
its known fractures and fissures. The types of activations were forcast using a
table of probabilities for events, typed by duration and volume of ejecta.
Lengths of time were represented by the number of activations to expect within
a given time-frame, calculated assuming Poisson-distributed inter-arrival times
for activations. Locations of future activations were forecast with a
probability distribution function for activation probabilities. Most likely
scenarios for risk and resulting topography were generated for Etna's next
activation (average 7.76 years), the next 25, 50 and 100 years. Forecasts for
areas most likely affected are in good agreement with previous risk studies
made. Forecasts for risks of lava invasions, as well as future topographies
might be a first. Threats to lifelines are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3366</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3366</id><created>2013-10-12</created><authors><author><keyname>Egger</keyname><forenames>Jan</forenames></author></authors><title>PCG-Cut: Graph Driven Segmentation of the Prostate Central Gland</title><categories>cs.CV</categories><comments>6 pages, 6 figures, 2 tables, 52 references</comments><journal-ref>PLoS ONE 8(10): e76645 (2013)</journal-ref><doi>10.1371/journal.pone.0076645</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Prostate cancer is the most abundant cancer in men, with over 200,000
expected new cases and around 28,000 deaths in 2012 in the US alone. In this
study, the segmentation results for the prostate central gland (PCG) in MR
scans are presented. The aim of this research study is to apply a graph-based
algorithm to automated segmentation (i.e. delineation) of organ limits for the
prostate central gland. The ultimate goal is to apply automated segmentation
approach to facilitate efficient MR-guided biopsy and radiation treatment
planning. The automated segmentation algorithm used is graph-driven based on a
spherical template. Therefore, rays are sent through the surface points of a
polyhedron to sample the graph's nodes. After graph construction - which only
requires the center of the polyhedron defined by the user and located inside
the prostate center gland - the minimal cost closed set on the graph is
computed via a polynomial time s-t-cut, which results in the segmentation of
the prostate center gland's boundaries and volume. The algorithm has been
realized as a C++ modul within the medical research platform MeVisLab and the
ground truth of the central gland boundaries were manually extracted by
clinical experts (interventional radiologists) with several years of experience
in prostate treatment. For evaluation the automated segmentations of the
proposed scheme have been compared with the manual segmentations, yielding an
average Dice Similarity Coefficient (DSC) of 78.94 +/- 10.85%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3370</identifier>
 <datestamp>2014-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3370</id><created>2013-10-12</created><updated>2014-08-29</updated><authors><author><keyname>Kemman</keyname><forenames>Max</forenames></author><author><keyname>Scagliola</keyname><forenames>Stef</forenames></author><author><keyname>de Jong</keyname><forenames>Franciska</forenames></author><author><keyname>Ordelman</keyname><forenames>Roeland</forenames></author></authors><title>Talking With Scholars: Developing a Research Environment for Oral
  History Collections</title><categories>cs.DL</categories><comments>Demo paper for The 2nd International Workshop on Supporting Users
  Exploration of Digital Libraries (Malta, 26th September, 2013)</comments><doi>10.1007/978-3-319-08425-1_22</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scholars are yet to make optimal use of Oral History collections. For the
uptake of digital research tools in the daily working practice of researchers,
practices and conventions commonly adhered to in the subfields in the
humanities should be taken into account during development. To this end, in the
Oral History Today project a research tool for exploring Oral History
collections is developed in close collaboration with scholarly researchers.
This paper describes four stages of scholarly research and the first steps
undertaken to incorporate requirements of these stages in a digital research
environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3381</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3381</id><created>2013-10-12</created><updated>2014-05-13</updated><authors><author><keyname>Sen</keyname><forenames>Pinar</forenames></author><author><keyname>Aktas</keyname><forenames>Tugcan</forenames></author><author><keyname>Yilmaz</keyname><forenames>A. Ozgur</forenames></author></authors><title>A Low-Complexity Graph-Based LMMSE Receiver Designed for Colored Noise
  Induced by FTN-Signaling</title><categories>cs.IT math.IT</categories><comments>6 pages, 6 figures, IEEE Wireless Communications and Networking
  Conference 2014, Istanbul, Turkey</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a low complexity graph-based linear minimum mean square error
(LMMSE) equalizer which considers both the intersymbol interference (ISI) and
the effect of non-white noise inherent in Faster-than-Nyquist (FTN) signaling.
In order to incorporate the statistics of noise signal into the factor graph
over which the LMMSE algorithm is implemented, we suggest a method that models
it as an autoregressive (AR) process. Furthermore, we develop a new mechanism
for exchange of information between the proposed equalizer and the channel
decoder through turbo iterations. Based on these improvements, we show that the
proposed low complexity receiver structure performs close to the optimal
decoder operating in ISI-free ideal scenario without FTN signaling through
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3388</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3388</id><created>2013-10-12</created><authors><author><keyname>Kaminker</keyname><forenames>Tal</forenames></author><author><keyname>Sharir</keyname><forenames>Micha</forenames></author></authors><title>Finding the Largest Disk Containing a Query Point in Logarithmic Time
  with Linear Storage</title><categories>cs.CG</categories><comments>15 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let D be a set of n disks in the plane. We present a data structure of size
O(n) that can compute, for any query point q, the largest disk in D that
contains q, in O(log n) time. The structure can be constructed in O(n log^3 n)
time. The optimal storage and query time of the structure improve several
recent solutions by Augustine et al. and by Kaplan and Sharir.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3389</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3389</id><created>2013-10-12</created><authors><author><keyname>Peron</keyname><forenames>Thomas K. DM.</forenames></author><author><keyname>Ji</keyname><forenames>Peng</forenames></author><author><keyname>Rodrigues</keyname><forenames>Francisco A.</forenames></author><author><keyname>Kurths</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>Impact of order three cycles in complex network spectra</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>5 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The asymptotic behaviour of dynamical processes in networks can be expressed
as a function of spectral properties of the Adjacency and Laplacian matrices.
Although many theoretical results are known for the spectra of traditional
configuration models, networks generated through these models fail to describe
many topological features of real-world networks, in particular non-null values
for the clustering coefficient. Here we study the effects of cycles or order
three (triangles) in network spectra. By using recent advances in random matrix
theory, we determine the spectrum distribution of the network Adjacency matrix
as a function of the average number of triangles attached to each node for
networks without modular structure and degree-degree correlations. Furthermore
we show that cycles of order three have a weak influence on the Laplacian
eigenvalues, fact that explains the recent controversy on the dynamics of
clustered networks. Our findings can shed light in the study of how particular
kinds of subgraphs influence network dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3399</identifier>
 <datestamp>2013-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3399</id><created>2013-10-12</created><updated>2013-10-31</updated><authors><author><keyname>Prashanth</keyname><forenames>B U V</forenames></author><author><keyname>Sastry</keyname><forenames>P Narahari</forenames></author><author><keyname>Rajesh</keyname><forenames>V</forenames></author></authors><title>An Improved K-means Clustering Based Approach to Detect a DNA Structure
  in H&amp;E Image of Mouse Tissue Reacted with CD4-Green Antigen</title><categories>cs.CV</categories><comments>This paper has been withdrawn by the author to perform more
  comprehensive evaluations in the research work</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this manuscript we present the technique to detect and analyze the DNA
rich structure in Haemotoxylin &amp; Eosin (H&amp;E) image of a tissue treated with
anti CD4 green antigen. The detection of DNA rich structure can be considered
as a detection of blue nuclei present through the biomedical signal/image
processing technique performed on the image of the tissue obtained by the
Scanning Electron Microscope(SEM). Earlier the tissue treated with the anti CD4
green antigen, is stained with the H&amp;E staining solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3404</identifier>
 <datestamp>2013-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3404</id><created>2013-10-12</created><updated>2013-11-20</updated><authors><author><keyname>Kerneis</keyname><forenames>Gabriel</forenames></author><author><keyname>Shepherd</keyname><forenames>Charlie</forenames></author><author><keyname>Hajnoczi</keyname><forenames>Stefan</forenames></author></authors><title>QEMU/CPC: Static Analysis and CPS Conversion for Safe, Portable, and
  Efficient Coroutines</title><categories>cs.PL</categories><comments>12 pages</comments><acm-class>D.1.3</acm-class><journal-ref>Proceedings of the ACM SIGPLAN 2014 Workshop on Partial Evaluation
  and Program Manipulation, PEPM 2014, San Diego, CA, USA, January 20-21, 2014.
  ACM 2014</journal-ref><doi>10.1145/2543728.2543733</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coroutines and events are two common abstractions for writing concurrent
programs. Because coroutines are often more convenient, but events more
portable and efficient, it is natural to want to translate the former into the
latter. CPC is such a source-to-source translator for C programs, based on a
partial conversion into continuation-passing style (CPS conversion) of
functions annotated as cooperative.
  In this article, we study the application of the CPC translator to QEMU, an
open-source machine emulator which also uses annotated coroutine functions for
concurrency. We first propose a new type of annotations to identify functions
which never cooperate, and we introduce CoroCheck, a tool for the static
analysis and inference of cooperation annotations. Then, we improve the CPC
translator, defining CPS conversion as a calling convention for the C language,
with support for indirect calls to CPS-converted function through function
pointers. Finally, we apply CoroCheck and CPC to QEMU (750 000 lines of C
code), fixing hundreds of missing annotations and comparing performance of the
translated code with existing implementations of coroutines in QEMU.
  Our work shows the importance of static annotation checking to prevent actual
concurrency bugs, and demonstrates that CPS conversion is a flexible, portable,
and efficient compilation technique, even for very large programs written in an
imperative language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3407</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3407</id><created>2013-10-12</created><authors><author><keyname>Sorour</keyname><forenames>Sameh</forenames></author><author><keyname>Lostanlen</keyname><forenames>Yves</forenames></author><author><keyname>Valaee</keyname><forenames>Shahrokh</forenames></author></authors><title>Joint Indoor Localization and Radio Map Construction with Limited
  Deployment Load</title><categories>cs.NI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One major bottleneck in the practical implementation of received signal
strength (RSS) based indoor localization systems is the extensive deployment
efforts required to construct the radio maps through fingerprinting. In this
paper, we aim to design an indoor localization scheme that can be directly
employed without building a full fingerprinted radio map of the indoor
environment. By accumulating the information of localized RSSs, this scheme can
also simultaneously construct the radio map with limited calibration. To design
this scheme, we employ a source data set that possesses the same spatial
correlation of the RSSs in the indoor environment under study. The knowledge of
this data set is then transferred to a limited number of calibration
fingerprints and one or several RSS observations with unknown locations, in
order to perform direct localization of these observations using manifold
alignment. We test two different source data sets, namely a simulated radio
propagation map and the environments plan coordinates. For moving users, we
exploit the correlation of their observations to improve the localization
accuracy. The online testing in two indoor environments shows that the plan
coordinates achieve better results than the simulated radio maps, and a
negligible degradation with 70-85% reduction in calibration load.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3416</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3416</id><created>2013-10-12</created><authors><author><keyname>Daneshgaran</keyname><forenames>Fred</forenames></author><author><keyname>Mondin</keyname><forenames>Marina</forenames></author></authors><title>Impact of Interleaver Pruning on Properties of Underlying Permutations</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address the issue of pruning (i.e., shortening) a given
interleaver via truncation of the transposition vector of the mother
permutation and study its impact on the structural properties of the
permutation. This method of pruning allows for continuous un-interrupted data
flow regardless of the permutation length since the permutation engine is a
buffer whose leading element is swapped by other elements in the queue. The
principle goal of pruning is that of construction of variable length and hence
delay interleavers with application to iterative soft information processing
and concatenated codes, using the same structure (possibly in hardware) of the
interleaver and deinterleaver units. We address the issue of how pruning
impacts the spread of the permutation and also look at how pruning impacts
algebraically constructed permutations. We note that pruning via truncation of
the transposition vector of the permutation can have a catastrophic impact on
the permutation spread of algebraically constructed permutations. To remedy
this problem, we propose a novel lifting method whereby a subset of the points
in the permutation map leading to low spread of the pruned permutation are
identified and eliminated. Practical realization of this lifting is then
proposed via dummy symbol insertion in the input queue of the Finite State
Permuter (FSP), and subsequent removal of the dummy symbols at the FSP output.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3422</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3422</id><created>2013-10-12</created><updated>2014-04-13</updated><authors><author><keyname>Parand</keyname><forenames>K.</forenames></author><author><keyname>Zafarvahedian</keyname><forenames>Saeed</forenames></author><author><keyname>Hossayni</keyname><forenames>Sayyed A.</forenames></author></authors><title>GPU-acceleration of parallel unconditionally stable group explicit
  finite difference method</title><categories>cs.NA physics.comp-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graphics Processing Units (GPUs) are high performance co-processors
originally intended to improve the use and quality of computer graphics
applications. Since researchers and practitioners realized the potential of
using GPU for general purposes, their applications have been extended to other
fields, out of computer graphics scope. The main objective of this paper is to
evaluate the impact of using GPU in solution of the transient diffusion type
equation by parallel and stable group explicit finite difference method. To
accomplish that, GPU and CPU-based (multi-core) approaches were developed.
Moreover, we proposed an optimal synchronization arrangement for its
implementation pseudo-code. Also, the interrelation of GPU parallel programming
and initialization of the algorithm variables were discussed, using numerical
experiences. The GPU-approach results are faster than a much expensive parallel
8-thread CPU-based approach results. The GPU, used in this paper, is an
ordinary laptop GPU (GT 335M) and is accessible for everyone; therefor, the
results are expected to encourage all research society to use GPUs and improve
their research efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3423</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3423</id><created>2013-10-12</created><updated>2015-03-01</updated><authors><author><keyname>Kloster</keyname><forenames>Kyle</forenames></author><author><keyname>Gleich</keyname><forenames>David F.</forenames></author></authors><title>Sublinear Column-wise Actions of the Matrix Exponential on Social
  Networks</title><categories>cs.SI math.NA</categories><comments>41 pages. Updated version (11/20/13) published in the proceedings of
  WAW13. Update (1/19/14) fixes error in runtime bound. Update (5/3/2014)
  introduces two new algorithms. Update (3/1/15) accepted for publication in
  journal of Internet Math; generalizes power law degree distributions
  theorems. Codes available at
  http://www.cs.purdue.edu/homes/dgleich/codes/nexpokit</comments><acm-class>G.1.3; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider stochastic transition matrices from large social and information
networks. For these matrices, we describe and evaluate three fast methods to
estimate one column of the matrix exponential. The methods are designed to
exploit the properties inherent in social networks, such as a power-law degree
distribution. Using only this property, we prove that one of our algorithms has
a sublinear runtime. We present further experimental evidence showing that all
of them run quickly on social networks with billions of edges and accurately
identify the largest elements of the column.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3424</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3424</id><created>2013-10-12</created><authors><author><keyname>Rajasekharan</keyname><forenames>Jayaprakash</forenames></author><author><keyname>Koivunen</keyname><forenames>Visa</forenames></author></authors><title>Optimal Energy Consumption Model for Smart Grid Households with Energy
  Storage</title><categories>cs.OH</categories><comments>26 pages, 9 figures, 34 equations</comments><doi>10.1109/JSTSP.2014.2361315</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose to model the energy consumption of smart grid
households with energy storage systems as an intertemporal trading economy.
Intertemporal trade refers to transaction of goods across time when an agent,
at any time, is faced with the option of consuming or saving with the aim of
using the savings in the future or spending the savings from the past. Smart
homes define optimal consumption as either balancing/leveling consumption such
that the utility company is presented with a uniform demand or as minimizing
consumption costs by storing energy during off-peak time periods when prices
are lower and use the stored energy during peak time periods when prices are
higher. Due to the varying nature of energy requirements of household and
market energy prices over different time periods in a day, households face a
trade-off between consuming to meet their current energy requirements and/or
storing energy for future consumption and/or spending energy stored in the
past. These trade-offs or consumption preferences of the household are modeled
as utility functions using consumer theory. We introduce two different utility
functions, one for cost minimization and another for consumption
balancing/leveling, that are maximized subject to respective budget,
consumption, storage and savings constraints to solve for the optimum
consumption profile. The optimization problem of a household with energy
storage is formulated as a geometric program for consumption
balancing/leveling, while cost minimization is formulated as a linear
programming problem. Simulation results show that the proposed model achieves
extremely low peak to average ratio in the consumption balancing/leveling
scheme with about 8% reduction in consumption costs and the least possible
amount for electricity bill with about 12% reduction in consumption costs in
the cost minimization scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3438</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3438</id><created>2013-10-12</created><authors><author><keyname>Richt&#xe1;rik</keyname><forenames>Peter</forenames></author><author><keyname>Tak&#xe1;&#x10d;</keyname><forenames>Martin</forenames></author></authors><title>On Optimal Probabilities in Stochastic Coordinate Descent Methods</title><categories>stat.ML cs.DC math.OC</categories><comments>5 pages, 1 algorithm (`NSync), 2 theorems, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose and analyze a new parallel coordinate descent method---`NSync---in
which at each iteration a random subset of coordinates is updated, in parallel,
allowing for the subsets to be chosen non-uniformly. We derive convergence
rates under a strong convexity assumption, and comment on how to assign
probabilities to the sets to optimize the bound. The complexity and practical
performance of the method can outperform its uniform variant by an order of
magnitude. Surprisingly, the strategy of updating a single randomly selected
coordinate per iteration---with optimal probabilities---may require less
iterations, both in theory and practice, than the strategy of updating all
coordinates at every iteration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3447</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3447</id><created>2013-10-13</created><updated>2013-10-19</updated><authors><author><keyname>Liu</keyname><forenames>Jun</forenames></author><author><keyname>Huang</keyname><forenames>Ting-Zhu</forenames></author><author><keyname>Selesnick</keyname><forenames>Ivan W.</forenames></author><author><keyname>Lv</keyname><forenames>Xiao-Guang</forenames></author><author><keyname>Chen</keyname><forenames>Po-Yu</forenames></author></authors><title>Image Restoration using Total Variation with Overlapping Group Sparsity</title><categories>cs.CV math.NA</categories><comments>11 pages, 37 figures</comments><msc-class>46N10, 68U10, 94A08, 47A52</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image restoration is one of the most fundamental issues in imaging science.
Total variation (TV) regularization is widely used in image restoration
problems for its capability to preserve edges. In the literature, however, it
is also well known for producing staircase-like artifacts. Usually, the
high-order total variation (HTV) regularizer is an good option except its
over-smoothing property. In this work, we study a minimization problem where
the objective includes an usual $l_2$ data-fidelity term and an overlapping
group sparsity total variation regularizer which can avoid staircase effect and
allow edges preserving in the restored image. We also proposed a fast algorithm
for solving the corresponding minimization problem and compare our method with
the state-of-the-art TV based methods and HTV based method. The numerical
experiments illustrate the efficiency and effectiveness of the proposed method
in terms of PSNR, relative error and computing time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3452</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3452</id><created>2013-10-13</created><authors><author><keyname>Yan</keyname><forenames>Qiong</forenames></author><author><keyname>Xu</keyname><forenames>Li</forenames></author><author><keyname>Jia</keyname><forenames>Jiaya</forenames></author></authors><title>Dense Scattering Layer Removal</title><categories>cs.CV</categories><comments>10 pages, 10 figures, Siggraph Asia 2013 Technial Briefs</comments><acm-class>I.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new model, together with advanced optimization, to separate a
thick scattering media layer from a single natural image. It is able to handle
challenging underwater scenes and images taken in fog and sandstorm, both of
which are with significantly reduced visibility. Our method addresses the
critical issue -- this is, originally unnoticeable impurities will be greatly
magnified after removing the scattering media layer -- with transmission-aware
optimization. We introduce non-local structure-aware regularization to properly
constrain transmission estimation without introducing the halo artifacts. A
selective-neighbor criterion is presented to convert the unconventional
constrained optimization problem to an unconstrained one where the latter can
be efficiently solved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3454</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3454</id><created>2013-10-13</created><authors><author><keyname>Krishnamoorthy</keyname><forenames>Aravindh</forenames></author></authors><title>Linear Extended Whitening Filters</title><categories>cs.IT math.IT stat.AP</categories><comments>3pp, pre-print</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a class of linear whitening filters termed linear
extended whitening filters (EWFs) which are whitening filters that have
desirable secondary properties and can be used for simplifying algorithms, or
achieving desired side-effects on given secondary matrices, random vectors or
random processes. Further, we present an application of EWFs for simplification
of QR decomposition based ML detection algorithm in Wireless Communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3473</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3473</id><created>2013-10-13</created><authors><author><keyname>Jha</keyname><forenames>Rohit</forenames></author><author><keyname>Samuel</keyname><forenames>Alfy</forenames></author><author><keyname>Pawar</keyname><forenames>Ashmee</forenames></author><author><keyname>Kiruthika</keyname><forenames>M.</forenames></author></authors><title>A Domain-Specific Language for Discrete Mathematics</title><categories>cs.PL cs.DM</categories><journal-ref>International Journal of Computer Applications 70(15):6-19, May
  2013</journal-ref><doi>10.5120/12036-7257</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses a Domain Specific Language (DSL) that has been developed
to enable implementation of concepts of discrete mathematics. A library of data
types and functions provides functionality which is frequently required by
users. Covering the areas of Mathematical Logic, Set Theory, Functions, Graph
Theory, Number Theory, Linear Algebra and Combinatorics, the language's syntax
is close to the actual notation used in the specific fields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3481</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3481</id><created>2013-10-13</created><authors><author><keyname>Farzan</keyname><forenames>Azadeh</forenames></author><author><keyname>Kincaid</keyname><forenames>Zachary</forenames></author></authors><title>An Algebraic Framework for Compositional Program Analysis</title><categories>cs.PL</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of a program analysis is to compute an abstract meaning for a
program which approximates its dynamic behaviour. A compositional program
analysis accomplishes this task with a divide-and-conquer strategy: the meaning
of a program is computed by dividing it into sub-programs, computing their
meaning, and then combining the results. Compositional program analyses are
desirable because they can yield scalable (and easily parallelizable) program
analyses.
  This paper presents algebraic framework for designing, implementing, and
proving the correctness of compositional program analyses. A program analysis
in our framework defined by an algebraic structure equipped with sequencing,
choice, and iteration operations. From the analysis design perspective, a
particularly interesting consequence of this is that the meaning of a loop is
computed by applying the iteration operator to the loop body. This style of
compositional loop analysis can yield interesting ways of computing loop
invariants that cannot be defined iteratively. We identify a class of
algorithms, the so-called path-expression algorithms [Tarjan1981,Scholz2007],
which can be used to efficiently implement analyses in our framework. Lastly,
we develop a theory for proving the correctness of an analysis by establishing
an approximation relationship between an algebra defining a concrete semantics
and an algebra defining an analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3482</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3482</id><created>2013-10-13</created><authors><author><keyname>Ryabko</keyname><forenames>Boris</forenames></author></authors><title>Using Information Theory to Study the Efficiency and Capacity of Caching
  in the Computer Networks</title><categories>cs.IT cs.NI math.IT</categories><msc-class>68M10 Network design and communication</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays computer networks use different kind of memory whose speeds and
capacities vary widely. There exist methods of a so-called caching which are
intended to use the different kinds of memory in such a way that the frequently
used data are stored in the faster memory, wheres the infrequent ones are
stored in the slower memory. We address the problems of estimating the caching
efficiency and its capacity. We define the efficiency and capacity of the
caching and suggest a method for their estimation based on the analysis of
kinds of the accessible memory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3486</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3486</id><created>2013-10-13</created><authors><author><keyname>Dani</keyname><forenames>Varsha</forenames></author><author><keyname>King</keyname><forenames>Valerie</forenames></author><author><keyname>Movahedi</keyname><forenames>Mahnush</forenames></author><author><keyname>Saia</keyname><forenames>Jared</forenames></author></authors><title>Quorums Quicken Queries: Efficient Asynchronous Secure Multiparty
  Computation</title><categories>cs.DS cs.CR cs.DC</categories><comments>ICDCN version: 15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe an asynchronous algorithm to solve secure multiparty computation
(MPC) over n players, when strictly less than a 1/8 fraction of the players are
controlled by a static adversary. For any function f over a field that can be
computed by a circuit with m gates, our algorithm requires each player to send
a number of field elements and perform an amount of computation that is O (m/n
+ \sqrt{n}). This significantly improves over traditional algorithms, which
require each player to both send a number of messages and perform computation
that is {\Omega}(nm). Additionally, we define the threshold counting problem
and present a distributed algorithm to solve it in the asynchronous
communication model. Our algorithm is load balanced, with computation,
communication and latency complexity of O(log n), and may be of independent
interest to other applications with a load balancing goal in mind.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3487</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3487</id><created>2013-10-13</created><updated>2014-08-24</updated><authors><author><keyname>Blocq</keyname><forenames>Gideon</forenames></author><author><keyname>Orda</keyname><forenames>Ariel</forenames></author></authors><title>Worst-Case Coalitions in Routing Games</title><categories>cs.NI cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, there has been a growing interest in network scenarios where
selfish agents are able to communicate, bargain and form coalitions, i.e., play
a cooperative game, in order to overcome the inefficiency of noncooperative
equilibria. We study such coalitional games under worst-case conditions in the
fundamental load-balancing setting of routing over parallel links.
Specifically, we investigate fundamental solution concepts of the considered
cooperative game, namely the (Inner) Core and the min-max fair Nucleolus. Most
notably we prove that under certain conditions they contain the system optimum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3492</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3492</id><created>2013-10-13</created><authors><author><keyname>Zhang</keyname><forenames>Jiawei</forenames></author><author><keyname>Kong</keyname><forenames>Xiangnan</forenames></author><author><keyname>Yu</keyname><forenames>Philip S.</forenames></author></authors><title>Predicting Social Links for New Users across Aligned Heterogeneous
  Social Networks</title><categories>cs.SI cs.LG physics.soc-ph</categories><comments>11 pages, 10 figures, 4 tables</comments><msc-class>H.2.8 Database Management, Database Applications-Data Mining</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online social networks have gained great success in recent years and many of
them involve multiple kinds of nodes and complex relationships. Among these
relationships, social links among users are of great importance. Many existing
link prediction methods focus on predicting social links that will appear in
the future among all users based upon a snapshot of the social network. In
real-world social networks, many new users are joining in the service every
day. Predicting links for new users are more important. Different from
conventional link prediction problems, link prediction for new users are more
challenging due to the following reasons: (1) differences in information
distributions between new users and the existing active users (i.e., old
users); (2) lack of information from the new users in the network. We propose a
link prediction method called SCAN-PS (Supervised Cross Aligned Networks link
prediction with Personalized Sampling), to solve the link prediction problem
for new users with information transferred from both the existing active users
in the target network and other source networks through aligned accounts. We
proposed a within-target-network personalized sampling method to process the
existing active users' information in order to accommodate the differences in
information distributions before the intra-network knowledge transfer. SCAN-PS
can also exploit information in other source networks, where the user accounts
are aligned with the target network. In this way, SCAN-PS could solve the cold
start problem when information of these new users is total absent in the target
network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3498</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3498</id><created>2013-10-13</created><authors><author><keyname>Helbing</keyname><forenames>Dirk</forenames></author></authors><title>New Ways to Promote Sustainability and Social Well-Being in a Complex,
  Strongly Interdependent World: The FuturICT Approach</title><categories>cs.CY physics.soc-ph</categories><comments>For related work see http://www.soms.ethz.ch and
  http://www.futurict.eu</comments><journal-ref>This is the Epilogue of the Booklet by P. Ball, Why Society is a
  Complex Matter (Springer, Berlin, 2012), pp. 55-60</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  FuturICT is one of six proposals currently being considered for support
within the European Commission's Flagship Initiative (see Box 1). The vision of
the FuturICT project is to develop new science and new information and
communication systems that will promote social self-organization,
self-regulation, well-being, sustainability, and resilience. One of the main
aims of the approach is to increase individual opportunities for social,
economic and political participation, combined with the creation of collective
awareness of the impact that human actions have on our world. This requires us
to mine large datasets (&quot;Big Data&quot;) and to develop new methods and tools: a
Planetary Nervous System (PNS) to answer &quot;What is (the state of the world)...&quot;
questions, a Living Earth Simulator (LES) to study &quot;What ... if ...&quot; scenarios,
and a Global Participatory Platform (GPP) for social exploration and
interaction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3499</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3499</id><created>2013-10-13</created><authors><author><keyname>Pavlyshenko</keyname><forenames>Bohdan</forenames></author></authors><title>Forecasting of Events by Tweet Data Mining</title><categories>cs.SI cs.CL cs.CY</categories><comments>13 pages, 19 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the analysis of quantitative characteristics of frequent
sets and association rules in the posts of Twitter microblogs related to
different event discussions. For the analysis, we used a theory of frequent
sets, association rules and a theory of formal concept analysis. We revealed
the frequent sets and association rules which characterize the semantic
relations between the concepts of analyzed subjects. The support of some
frequent sets reaches its global maximum before the expected event but with
some time delay. Such frequent sets may be considered as predictive markers
that characterize the significance of expected events for blogosphere users. We
showed that the time dynamics of confidence in some revealed association rules
can also have predictive characteristics. Exceeding a certain threshold may be
a signal for corresponding reaction in the society within the time interval
between the maximum and the probable coming of an event. In this paper, we
considered two types of events: the Olympic tennis tournament final in London,
2012 and the prediction of Eurovision 2013 winner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3500</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3500</id><created>2013-10-13</created><authors><author><keyname>Pavlyshenko</keyname><forenames>Bohdan</forenames></author></authors><title>Can Twitter Predict Royal Baby's Name ?</title><categories>cs.SI cs.CL cs.CY</categories><comments>8 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze the existence of possible correlation between
public opinion of twitter users and the decision-making of persons who are
influential in the society. We carry out this analysis on the example of the
discussion of probable name of the British crown baby, born in July, 2013. In
our study, we use the methods of quantitative processing of natural language,
the theory of frequent sets, the algorithms of visual displaying of users'
communities. We also analyzed the time dynamics of keyword frequencies. The
analysis showed that the main predictable name was dominating in the spectrum
of names before the official announcement. Using the theories of frequent sets,
we showed that the full name consisting of three component names was the part
of top 5 by the value of support. It was revealed that the structure of
dynamically formed users' communities participating in the discussion is
determined by only a few leaders who influence significantly the viewpoints of
other users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3521</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3521</id><created>2013-10-13</created><authors><author><keyname>Gilles</keyname><forenames>Robert P.</forenames></author><author><keyname>Diamantaras</keyname><forenames>Dimitrios</forenames></author></authors><title>Platform Competition as Network Contestability</title><categories>cs.GT cs.SI physics.soc-ph</categories><comments>23 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent research in industrial organisation has investigated the essential
place that middlemen have in the networks that make up our global economy. In
this paper we attempt to understand how such middlemen compete with each other
through a game theoretic analysis using novel techniques from decision-making
under ambiguity. We model a purposely abstract and reduced model of one
middleman who pro- vides a two-sided platform, mediating surplus-creating
interactions between two users. The middleman evaluates uncertain outcomes
under positional ambiguity, taking into account the possibility of the
emergence of an alternative middleman offering intermediary services to the two
users. Surprisingly, we find many situations in which the middleman will
purposely extract maximal gains from her position. Only if there is relatively
low probability of devastating loss of business under competition, the
middleman will adopt a more competitive attitude and extract less from her
position.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3556</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3556</id><created>2013-10-13</created><updated>2013-12-14</updated><authors><author><keyname>Kundu</keyname><forenames>Abhisek</forenames></author><author><keyname>Nambirajan</keyname><forenames>Srinivas</forenames></author><author><keyname>Drineas</keyname><forenames>Petros</forenames></author></authors><title>Identifying Influential Entries in a Matrix</title><categories>cs.NA cs.LG stat.ML</categories><comments>There is a bug in the proof of Lemma 5, which we are currently
  working to fix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For any matrix A in R^(m x n) of rank \rho, we present a probability
distribution over the entries of A (the element-wise leverage scores of
equation (2)) that reveals the most influential entries in the matrix. From a
theoretical perspective, we prove that sampling at most s = O ((m + n) \rho^2
ln (m + n)) entries of the matrix (see eqn. (3) for the precise value of s)
with respect to these scores and solving the nuclear norm minimization problem
on the sampled entries, reconstructs A exactly. To the best of our knowledge,
these are the strongest theoretical guarantees on matrix completion without any
incoherence assumptions on the matrix A. From an experimental perspective, we
show that entries corresponding to high element-wise leverage scores reveal
structural properties of the data matrix that are of interest to domain
scientists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3564</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3564</id><created>2013-10-14</created><authors><author><keyname>Hasegawa</keyname><forenames>Satoshi</forenames></author><author><keyname>Anada</keyname><forenames>Hajime</forenames></author><author><keyname>Kanagawa</keyname><forenames>Shuya</forenames></author></authors><title>Pursuit Fractal Analysis of Time-Series Data</title><categories>cs.NA</categories><comments>5pages, 1 figure</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this study, we present a method to measure changes over time of fractal
dimension. We confirmed that our method can calculate the fractal dimension
with the same precision as conventional methods, and tracking performance of
our method is higher than that of the conventional methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3567</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3567</id><created>2013-10-14</created><updated>2015-05-05</updated><authors><author><keyname>Vaughan</keyname><forenames>Adam</forenames></author><author><keyname>Bohac</keyname><forenames>Stanislav V.</forenames></author></authors><title>An Extreme Learning Machine Approach to Predicting Near Chaotic HCCI
  Combustion Phasing in Real-Time</title><categories>cs.LG cs.CE</categories><comments>11 pages, 7 figures, minor revision (added implementation details and
  video link), submitted to Neural Networks</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fuel efficient Homogeneous Charge Compression Ignition (HCCI) engine
combustion timing predictions must contend with non-linear chemistry,
non-linear physics, period doubling bifurcation(s), turbulent mixing, model
parameters that can drift day-to-day, and air-fuel mixture state information
that cannot typically be resolved on a cycle-to-cycle basis, especially during
transients. In previous work, an abstract cycle-to-cycle mapping function
coupled with $\epsilon$-Support Vector Regression was shown to predict
experimentally observed cycle-to-cycle combustion timing over a wide range of
engine conditions, despite some of the aforementioned difficulties. The main
limitation of the previous approach was that a partially acausual randomly
sampled training dataset was used to train proof of concept offline
predictions. The objective of this paper is to address this limitation by
proposing a new online adaptive Extreme Learning Machine (ELM) extension named
Weighted Ring-ELM. This extension enables fully causal combustion timing
predictions at randomly chosen engine set points, and is shown to achieve
results that are as good as or better than the previous offline method. The
broader objective of this approach is to enable a new class of real-time model
predictive control strategies for high variability HCCI and, ultimately, to
bring HCCI's low engine-out NOx and reduced CO2 emissions to production
engines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3580</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3580</id><created>2013-10-14</created><updated>2013-11-28</updated><authors><author><keyname>Tang</keyname><forenames>Wanrong</forenames><affiliation>Angela</affiliation></author><author><keyname>Bi</keyname><forenames>Suzhi</forenames><affiliation>Angela</affiliation></author><author><keyname>Jun</keyname><forenames>Ying</forenames><affiliation>Angela</affiliation></author><author><keyname>Zhang</keyname></author></authors><title>Online Coordinated Charging Decision Algorithm for Electric Vehicles
  without Future Information</title><categories>cs.DS</categories><comments>12 pages, 7 figures</comments><acm-class>F.1.2; C.4; I.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The large-scale integration of plug-in electric vehicles (PEVs) to the power
grid spurs the need for efficient charging coordination mechanisms. It can be
shown that the optimal charging schedule smooths out the energy consumption
over time so as to minimize the total energy cost. In practice, however, it is
hard to smooth out the energy consumption perfectly, because the future PEV
charging demand is unknown at the moment when the charging rate of an existing
PEV needs to be determined. In this paper, we propose an Online cooRdinated
CHARging Decision (ORCHARD) algorithm, which minimizes the energy cost without
knowing the future information. Through rigorous proof, we show that ORCHARD is
strictly feasible in the sense that it guarantees to fulfill all charging
demands before due time. Meanwhile, it achieves the best known competitive
ratio of 2.39. To further reduce the computational complexity of the algorithm,
we propose a novel reduced-complexity algorithm to replace the standard convex
optimization techniques used in ORCHARD. Through extensive simulations, we show
that the average performance gap between ORCHARD and the offline optimal
solution, which utilizes the complete future information, is as small as 14%.
By setting a proper speeding factor, the average performance gap can be further
reduced to less than 6%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3584</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3584</id><created>2013-10-14</created><authors><author><keyname>Shahtouri</keyname><forenames>Saeid Montazeri</forenames></author><author><keyname>Ma</keyname><forenames>Richard T. B.</forenames></author></authors><title>Selection Policy: Fighting against Filter Effect in Network of Caches</title><categories>cs.NI</categories><comments>9 pages, 8 figures, submitted to Infocom 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many Information Centric Networking (ICN) proposals use a network of caches
to bring the contents closer to the consumers, reduce the load on producers and
decrease the unnecessary retransmission for ISPs. Nevertheless, the existing
cache management scheme for the network of caches obtain poor performance. The
main reason for performance degradation in a network of caches is the filter
effect of the replacement policy. A cache serves the requests that generate
cache-hits and forwards the requests that generate cache-misses. This filtering
changes the pattern of requests and leads to decreased hit ratios in the
subsequent caches. In this paper, we propose a coordinated caching scheme to
solve the filter effect problem by introducing the selection policy. This
policy manages a cache such that: i) the cache obtains a high hit ratio ii) the
missed requests from the cache can be used by subsequent caches to obtain a
high hit ratio. Our coordinated selection scheme achieves an overall hit ratio
of a network of caches equivalent to that of edge routers with big caches.
Moreover, our scheme decreases the average number of evictions per cache slot
by four order of magnitude compared to the LRU universal caching.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3593</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3593</id><created>2013-10-14</created><authors><author><keyname>Zhuang</keyname><forenames>Qian</forenames></author><author><keyname>Di</keyname><forenames>Zegnru</forenames></author><author><keyname>Wu</keyname><forenames>Jinshan</forenames></author></authors><title>Stability of Mixed-Strategy-Based Iterative Logit Quantal Response
  Dynamics in Game Theory</title><categories>physics.soc-ph cs.GT</categories><doi>10.1371/journal.pone.0105391</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using the Logit quantal response form as the response function in each step,
the original definition of static quantal response equilibrium (QRE) is
extended into an iterative evolution process. QREs remain as the fixed points
of the dynamic process. However, depending on whether such fixed points are the
long-term solutions of the dynamic process, they can be classified into stable
(SQREs) and unstable (USQREs) equilibriums. This extension resembles the
extension from static Nash equilibriums (NEs) to evolutionary stable solutions
in the framework of evolutionary game theory. The relation between SQREs and
other solution concepts of games, including NEs and QREs, is discussed. Using
experimental data from other published papers, we perform a preliminary
comparison between SQREs, NEs, QREs and the observed behavioral outcomes of
those experiments. For certain games, we determine that SQREs have better
predictive power than QREs and NEs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3595</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3595</id><created>2013-10-14</created><authors><author><keyname>Kundu</keyname><forenames>Atreyee</forenames></author><author><keyname>Chatterjee</keyname><forenames>Debasish</forenames></author></authors><title>Stabilizing discrete-time switched linear systems</title><categories>cs.SY</categories><comments>10 pages</comments><msc-class>93D20</msc-class><doi>10.1145/2562059.2562114</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article deals with stabilizing discrete-time switched linear systems.
Our contributions are threefold: Firstly, given a family of linear systems
possibly containing unstable dynamics, we propose a large class of switching
signals that stabilize a switched system generated by the switching signal and
the given family of systems. Secondly, given a switched system, a sufficient
condition for the existence of the proposed switching signal is derived by
expressing the switching signal as an infinite walk on a directed graph
representing the switched system. Thirdly, given a family of linear systems, we
propose an algorithmic technique to design a switching signal for stabilizing
the corresponding switched system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3607</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3607</id><created>2013-10-14</created><authors><author><keyname>Zimmermann</keyname><forenames>Albrecht</forenames></author><author><keyname>Moorthy</keyname><forenames>Sruthi</forenames></author><author><keyname>Shi</keyname><forenames>Zifan</forenames></author></authors><title>Predicting college basketball match outcomes using machine learning
  techniques: some results and lessons learned</title><categories>cs.LG stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most existing work on predicting NCAAB matches has been developed in a
statistical context. Trusting the capabilities of ML techniques, particularly
classification learners, to uncover the importance of features and learn their
relationships, we evaluated a number of different paradigms on this task. In
this paper, we summarize our work, pointing out that attributes seem to be more
important than models, and that there seems to be an upper limit to predictive
quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3609</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3609</id><created>2013-10-14</created><updated>2014-09-17</updated><authors><author><keyname>Legay</keyname><forenames>Axel</forenames></author><author><keyname>Sedwards</keyname><forenames>Sean</forenames></author><author><keyname>Traonouez</keyname><forenames>Louis-Marie</forenames></author></authors><title>Scalable Verification of Markov Decision Processes</title><categories>cs.DS cs.DC cs.LG cs.LO</categories><comments>V4: FMDS version, 12 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Markov decision processes (MDP) are useful to model concurrent process
optimisation problems, but verifying them with numerical methods is often
intractable. Existing approximative approaches do not scale well and are
limited to memoryless schedulers. Here we present the basis of scalable
verification for MDPSs, using an O(1) memory representation of
history-dependent schedulers. We thus facilitate scalable learning techniques
and the use of massively parallel verification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3623</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3623</id><created>2013-10-14</created><authors><author><keyname>Yang</keyname><forenames>Yiling</forenames></author><author><keyname>Huang</keyname><forenames>Yu</forenames></author><author><keyname>Ma</keyname><forenames>Xiaoxing</forenames></author><author><keyname>Lu</keyname><forenames>Jian</forenames></author></authors><title>Enabling Context-awareness by Predicate Detection in Asynchronous
  Pervasive Computing Environments</title><categories>cs.DC cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pervasive applications are involving more and more autonomous computing and
communicating devices, augmented with the abilities of sensing and controlling
the logical / physical environment. To enable context-awareness for such
applications, we are challenged by the intrinsic asynchrony among the context
collecting devices. To this end, we introduce the predicate detection theory
and propose the Predicate-Detection-based Context-Awareness (PD-CA) framework,
in which: a) logical time is used to explicitly cope with the asynchrony; b)
specification of predicates enables the applications to express contextual
properties of their concerns; c) online and incremental predicate detection
algorithms effectively enable context-awareness at runtime. Under the guidance
of the PD-CA framework, we present the design and implementation of the MIPA
middleware, which shields the applications from the burden of processing the
asynchronous contexts. We also demonstrate how PD-CA simplifies the development
of context-aware applications. Experimental evaluations show the performance of
MIPA in supporting context-aware applications despite of the asynchrony.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3637</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3637</id><created>2013-10-14</created><authors><author><keyname>Rohbanian</keyname><forenames>Mohammad Reza</forenames></author><author><keyname>Kharazmi</keyname><forenames>Mohammad Rafi</forenames></author><author><keyname>Keshavarz-Haddad</keyname><forenames>Alireza</forenames></author><author><keyname>Keshtgary</keyname><forenames>Manije</forenames></author></authors><title>Watchdog-LEACH: A new method based on LEACH protocol to Secure Clustered
  Wireless Sensor Networks</title><categories>cs.NI cs.CR</categories><journal-ref>ACSIJ Advances in Computer Science: an International Journal 2
  (2013) 105-117</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless sensor network comprises of small sensor nodes with limited
resources. Clustered networks have been proposed in many researches to reduce
the power consumption in sensor networks. LEACH is one of the most interested
techniques that offer an efficient way to minimize the power consumption in
sensor networks. However, due to the characteristics of restricted resources
and operation in a hostile environment, WSNs are subjected to numerous threats
and are vulnerable to attacks. This research proposes a solution that can be
applied on LEACH to increase the level of security. In Watchdog-LEACH, some
nodes are considered as watchdogs and some changes are applied on LEACH
protocol for intrusion detection. Watchdog-LEACH is able to protect against a
wide range of attacks and it provides security, energy efficiency and memory
efficiency. The result of simulation shows that in comparison to LEACH, the
energy overhead is about 2% so this method is practical and can be applied to
WSNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3652</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3652</id><created>2013-10-14</created><authors><author><keyname>Brown</keyname><forenames>Chlo&#xeb;</forenames></author><author><keyname>Efstratiou</keyname><forenames>Christos</forenames></author><author><keyname>Leontiadis</keyname><forenames>Ilias</forenames></author><author><keyname>Quercia</keyname><forenames>Daniele</forenames></author><author><keyname>Mascolo</keyname><forenames>Cecilia</forenames></author></authors><title>Tracking Serendipitous Interactions: How Individual Cultures Shape the
  Office</title><categories>cs.CY</categories><comments>10 pages, 4 figures. ACM CSCW 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many work environments, serendipitous interactions between members of
different groups may lead to enhanced productivity, collaboration and knowledge
dissemination. Two factors that may have an influence on such interactions are
cultural differences between individuals in highly multicultural workplaces,
and the layout and physical spaces of the workplace itself. In this work, we
investigate how these two factors may facilitate or hinder inter-group
interactions in the workplace. We analyze traces collected using wearable
electronic badges to capture face-to-face interactions and mobility patterns of
employees in a research laboratory in the UK. We observe that those who
interact with people of different roles tend to come from collectivist cultures
that value relationships and where people tend to be comfortable with social
hierarchies, and that some locations in particular are more likely to host
serendipitous interactions, knowledge that could be used by organizations to
enhance communication and productivity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3656</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3656</id><created>2013-10-14</created><updated>2015-06-24</updated><authors><author><keyname>Brengos</keyname><forenames>Tomasz</forenames><affiliation>Faculty of Mathematics and Information Sciences, Warsaw University of Technology</affiliation></author></authors><title>Weak bisimulation for coalgebras over order enriched monads</title><categories>cs.LO</categories><comments>44 pages</comments><proxy>LMCS</proxy><journal-ref>LMCS 11 (2:14) 2015</journal-ref><doi>10.2168/LMCS-11(2:14)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper introduces the notion of a weak bisimulation for coalgebras whose
type is a monad satisfying some extra properties. In the first part of the
paper we argue that systems with silent moves should be modelled
coalgebraically as coalgebras whose type is a monad. We show that the visible
and invisible part of the functor can be handled internally inside a monadic
structure. In the second part we introduce the notion of an ordered saturation
monad, study its properties, and show that it allows us to present two
approaches towards defining weak bisimulation for coalgebras and compare them.
We support the framework presented in this paper by two main examples of
models: labelled transition systems and simple Segala systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3673</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3673</id><created>2013-10-14</created><updated>2014-10-08</updated><authors><author><keyname>Allen</keyname><forenames>Sarah R.</forenames></author><author><keyname>Hellerstein</keyname><forenames>Lisa</forenames></author><author><keyname>Kletenik</keyname><forenames>Devorah</forenames></author><author><keyname>&#xdc;nl&#xfc;yurt</keyname><forenames>Tongu&#xe7;</forenames></author></authors><title>Evaluation of DNF Formulas</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic Boolean Function Evaluation (SBFE) is the problem of determining
the value of a given Boolean function $f$ on an unknown input $x$, when each
bit of $x_i$ of $x$ can only be determined by paying a given associated cost
$c_i$. Further, $x$ is drawn from a given product distribution: for each $x_i$,
$Prob[x_i=1] = p_i$, and the bits are independent. The goal is to minimize the
expected cost of evaluation. Stochastic Boolean Function Evaluation (SBFE) is
the problem of determining the value of a given Boolean function $f$ on an
unknown input $x$, when each bit of $x_i$ of $x$ can only be determined by
paying a given associated cost $c_i$. Further, $x$ is drawn from a given
product distribution: for each $x_i$, $Prob[x_i=1] = p_i$, and the bits are
independent. The goal is to minimize the expected cost of evaluation. In this
paper, we study the complexity of the SBFE problem for classes of DNF formulas.
We consider both exact and approximate versions of the problem for subclasses
of DNF, for arbitrary costs and product distributions, and for unit costs
and/or the uniform distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3674</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3674</id><created>2013-10-14</created><authors><author><keyname>Lagerkvist</keyname><forenames>Victor</forenames></author></authors><title>Weak Bases of Boolean Co-Clones</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Universal algebra and clone theory have proven to be a useful tool in the
study of constraint satisfaction problems since the complexity, up to logspace
reductions, is determined by the set of polymorphisms of the constraint
language. For classifications where primitive positive definitions are
unsuitable, such as size-preserving reductions, weaker closure operations may
be necessary. In this article we consider strong partial clones which can be
seen as a more fine-grained framework than Post's lattice where each clone
splits into an interval of strong partial clones. We investigate these
intervals and give simple relational descriptions, weak bases, of the largest
elements. The weak bases have a highly regular form and are in many cases
easily relatable to the smallest members in the intervals, which suggests that
the lattice of strong partial clones is considerably simpler than the full
lattice of partial clones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3692</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3692</id><created>2013-10-14</created><authors><author><keyname>Salge</keyname><forenames>Christoph</forenames></author><author><keyname>Polani</keyname><forenames>Daniel</forenames></author></authors><title>Changing the Environment based on Intrinsic Motivation</title><categories>nlin.AO cs.AI cs.IT math.IT</categories><comments>3 page, 1 figure, extended abstract of work presented at the Workshop
  for &quot;Guided Self-Organization&quot; 2013 (http://prokopenko.net/gso6.html)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the remarkable feats of intelligent life is that it restructures the
world it lives in for its own benefit. This extended abstract outlines how the
information-theoretic principle of empowerment, as an intrinsic motivation, can
be used to restructure the environment an agent lives in. We present a first
qualitative evaluation of how an agent in a 3d-gridworld builds a
staircase-like structure, which reflects the agent's embodiment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3695</identifier>
 <datestamp>2013-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3695</id><created>2013-10-14</created><updated>2013-11-06</updated><authors><author><keyname>Sandell</keyname><forenames>Magnus</forenames></author><author><keyname>Tosato</keyname><forenames>Filippo</forenames></author></authors><title>Lowest Density MDS Array Codes for Reliable Smart Meter Networks</title><categories>cs.IT math.IT</categories><comments>30 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce a lowest density MDS array code which is applied
to a Smart Meter network to introduce reliability. By treating the network as
distributed storage with multiple sources, information can be exchanged between
the nodes in the network allowing each node to store parity symbols relating to
data from other nodes. A lowest density MDS array code is then applied to make
the network robust against outages, ensuring low overhead and data transfers.
We show the minimum amount of overhead required to be able to recover from r
node erasures in an n node network and explicitly design an optimal array code
with lowest density. In contrast to existing codes, this one has no
restrictions on the number of nodes or erasures it can correct. Furthermore we
consider incomplete networks where all nodes are not connected to each other.
This limits the exchange of data for purposes of redundancy and we derive
conditions on the minimum node degree that allow lowest density MDS codes to
exist. We also present an explicit code design for incomplete networks that is
capable of correcting two node failures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3697</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3697</id><created>2013-10-14</created><authors><author><keyname>Tamar</keyname><forenames>Aviv</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author></authors><title>Variance Adjusted Actor Critic Algorithms</title><categories>stat.ML cs.LG cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an actor-critic framework for MDPs where the objective is the
variance-adjusted expected return. Our critic uses linear function
approximation, and we extend the concept of compatible features to the
variance-adjusted setting. We present an episodic actor-critic algorithm and
show that it converges almost surely to a locally optimal point of the
objective function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3713</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3713</id><created>2013-10-14</created><authors><author><keyname>Bauckhage</keyname><forenames>Christian</forenames></author></authors><title>Computing the Kullback-Leibler Divergence between two Weibull
  Distributions</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive a closed form solution for the Kullback-Leibler divergence between
two Weibull distributions. These notes are meant as reference material and
intended to provide a guided tour towards a result that is often mentioned but
seldom made explicit in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3716</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3716</id><created>2013-10-14</created><updated>2013-10-23</updated><authors><author><keyname>Sgrignoli</keyname><forenames>Paolo</forenames></author><author><keyname>Metulini</keyname><forenames>Rodolfo</forenames></author><author><keyname>Schiavo</keyname><forenames>Stefano</forenames></author><author><keyname>Riccaboni</keyname><forenames>Massimo</forenames></author></authors><title>The Relation Between Global Migration and Trade Networks</title><categories>physics.soc-ph cs.SI q-fin.GN</categories><comments>16 pages, 3 figures</comments><journal-ref>Physica A: Statistical Mechanics and its Applications, Volume 417,
  1 January 2015, Pages 245-260</journal-ref><doi>10.1016/j.physa.2014.09.037</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we develop a methodology to analyze and compare multiple global
networks. We focus our analysis on the relation between human migration and
trade. First, we identify the subset of products for which the presence of a
community of migrants significantly increases trade intensity. To assure
comparability across networks, we apply a hypergeometric filter to identify
links for which migration and trade intensity are both significantly higher
than expected. Next we develop an econometric methodology, inspired by spatial
econometrics, to measure the effect of migration on international trade while
controlling for network interdependencies. Overall, we find that migration
significantly boosts trade across sectors and we are able to identify product
categories for which this effect is particularly strong.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3717</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3717</id><created>2013-10-14</created><authors><author><keyname>Bahri</keyname><forenames>Anish</forenames></author><author><keyname>Sugumaran</keyname><forenames>V</forenames></author><author><keyname>Devasenapati</keyname><forenames>S Babu</forenames></author></authors><title>Misfire Detection in IC Engine using Kstar Algorithm</title><categories>cs.CV</categories><comments>12 Pages, 8 Figures, 4 Tables. International Journal of Research in
  Mechanical Engineering, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Misfire in an IC Engine continues to be a problem leading to reduced fuel
efficiency, increased power loss and emissions containing heavy concentration
of hydrocarbons. Misfiring creates a unique vibration pattern attributed to a
particular cylinder. Useful features can be extracted from these patterns and
can be analyzed to detect misfire. Statistical features from these vibration
signals were extracted. Out of these, useful features were identified using the
J48 decision tree algorithm and selected features were used for classification
using the Kstar algorithm. In this paper performance analysis of Kstar
algorithm is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3723</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3723</id><created>2013-10-14</created><authors><author><keyname>Quilbeuf</keyname><forenames>Jean</forenames></author><author><keyname>Igna</keyname><forenames>Georgeta</forenames></author><author><keyname>Bytschkow</keyname><forenames>Denis</forenames></author><author><keyname>Ruess</keyname><forenames>Harald</forenames></author></authors><title>Security policies for distributed systems</title><categories>cs.CR cs.DC</categories><comments>Submitted to POST14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A security policy specifies a security property as the maximal information
flow. A distributed system composed of interacting processes implicitly defines
an intransitive security policy by repudiating direct information flow between
processes that do not exchange messages directly. We show that implicitly
defined security policies in distributed systems are enforced, provided that
processes run in separation, and possible process communication on a technical
platform is restricted to specified message paths of the system. Furthermore,
we propose to further restrict the allowable information flow by adding filter
functions for controlling which messages may be transmitted between processes,
and we prove that locally checking filter functions is sufficient for ensuring
global security policies. Altogether, global intransitive security policies are
established by means of local verification conditions for the (trusted)
processes of the distributed system. Moreover, security policies may be
implemented securely on distributed integration platforms which ensure
partitioning. We illustrate our results with a smart grid case study, where we
use CTL model checking for discharging local verification conditions for each
process under consideration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3724</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3724</id><created>2013-10-14</created><authors><author><keyname>Costello,</keyname><forenames>Daniel J.</forenames><suffix>Jr.</suffix></author><author><keyname>Dolecek</keyname><forenames>Lara</forenames></author><author><keyname>Fuja</keyname><forenames>Thomas E.</forenames></author><author><keyname>Kliewer</keyname><forenames>Joerg</forenames></author><author><keyname>Mitchell</keyname><forenames>David G. M.</forenames></author><author><keyname>Smarandache</keyname><forenames>Roxana</forenames></author></authors><title>Spatially Coupled Sparse Codes on Graphs - Theory and Practice</title><categories>cs.IT math.IT</categories><comments>Submitted to the IEEE Communications Magazine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since the discovery of turbo codes 20 years ago and the subsequent
re-discovery of low-density parity-check codes a few years later, the field of
channel coding has experienced a number of major advances. Up until that time,
code designers were usually happy with performance that came within a few
decibels of the Shannon Limit, primarily due to implementation complexity
constraints, whereas the new coding techniques now allow performance within a
small fraction of a decibel of capacity with modest encoding and decoding
complexity. Due to these significant improvements, coding standards in
applications as varied as wireless mobile transmission, satellite TV, and deep
space communication are being updated to incorporate the new techniques. In
this paper, we review a particularly exciting new class of low-density
parity-check codes, called spatially-coupled codes, which promise excellent
performance over a broad range of channel conditions and decoded error rate
requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3741</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3741</id><created>2013-10-14</created><authors><author><keyname>Johansson</keyname><forenames>Fredrik</forenames></author></authors><title>Evaluating parametric holonomic sequences using rectangular splitting</title><categories>cs.SC</categories><comments>8 pages, 2 figures</comments><acm-class>I.1.2; F.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We adapt the rectangular splitting technique of Paterson and Stockmeyer to
the problem of evaluating terms in holonomic sequences that depend on a
parameter. This approach allows computing the $n$-th term in a recurrent
sequence of suitable type using $O(n^{1/2})$ &quot;expensive&quot; operations at the cost
of an increased number of &quot;cheap&quot; operations.
  Rectangular splitting has little overhead and can perform better than either
naive evaluation or asymptotically faster algorithms for ranges of $n$
encountered in applications. As an example, fast numerical evaluation of the
gamma function is investigated. Our work generalizes two previous algorithms of
Smith.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3770</identifier>
 <datestamp>2013-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3770</id><created>2013-10-14</created><updated>2013-11-05</updated><authors><author><keyname>Bannister</keyname><forenames>Michael J.</forenames></author><author><keyname>Devanny</keyname><forenames>William E.</forenames></author><author><keyname>Eppstein</keyname><forenames>David</forenames></author></authors><title>Small Superpatterns for Dominance Drawing</title><categories>cs.CG math.CO</categories><comments>ANALCO 2014, This version fixes an error in the leading constant of
  the 321-superpattern size</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We exploit the connection between dominance drawings of directed acyclic
graphs and permutations, in both directions, to provide improved bounds on the
size of universal point sets for certain types of dominance drawing and on
superpatterns for certain natural classes of permutations. In particular we
show that there exist universal point sets for dominance drawings of the Hasse
diagrams of width-two partial orders of size O(n^{3/2}), universal point sets
for dominance drawings of st-outerplanar graphs of size O(n\log n), and
universal point sets for dominance drawings of directed trees of size O(n^2).
We show that 321-avoiding permutations have superpatterns of size O(n^{3/2}),
riffle permutations (321-, 2143-, and 2413-avoiding permutations) have
superpatterns of size O(n), and the concatenations of sequences of riffles and
their inverses have superpatterns of size O(n\log n). Our analysis includes a
calculation of the leading constants in these bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3781</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3781</id><created>2013-10-14</created><authors><author><keyname>Gabora</keyname><forenames>Liane</forenames></author><author><keyname>Saberi</keyname><forenames>Maryam</forenames></author></authors><title>An Agent-based Model of the Cognitive Mechanisms Underlying the Origins
  of Creative Cultural Evolution</title><categories>cs.MA cs.AI</categories><comments>8 pages. arXiv admin note: text overlap with arXiv:1308.5032,
  arXiv:1005.1516, arXiv:1309.7407, arXiv:0911.2390, arXiv:0811.2551,
  arXiv:1310.0522</comments><journal-ref>(2011). Proceedings of the ACM Conference on Cognition &amp;
  Creativity (pp. 299-306). Nov 3-6. Georgia Institute of Technology, Atlanta,
  GA</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human culture is uniquely cumulative and open-ended. Using a computational
model of cultural evolution in which neural network based agents evolve ideas
for actions through invention and imitation, we tested the hypothesis that this
is due to the capacity for recursive recall. We compared runs in which agents
were limited to single-step actions to runs in which they used recursive recall
to chain simple actions into complex ones. Chaining resulted in higher cultural
diversity, open-ended generation of novelty, and no ceiling on the mean fitness
of actions. Both chaining and no-chaining runs exhibited convergence on optimal
actions, but without chaining this set was static while with chaining it was
ever-changing. Chaining increased the ability to capitalize on the capacity for
learning. These findings show that the recursive recall hypothesis provides a
computationally plausible explanation of why humans alone have evolved the
cultural means to transform this planet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3793</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3793</id><created>2013-10-14</created><updated>2015-06-01</updated><authors><author><keyname>Chung</keyname><forenames>Hye Won</forenames></author><author><keyname>Guha</keyname><forenames>Saikat</forenames></author><author><keyname>Zheng</keyname><forenames>Lizhong</forenames></author></authors><title>Superadditivity of Quantum Channel Coding Rate with Finite Blocklength
  Quantum Measurements</title><categories>cs.IT math.IT quant-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate superadditivity in the maximum achievable rate of reliable
classical communication over a quantum channel with a general pure-state
alphabet. We define superadditivity as the phenomenon that the maximum
accessible information per channel use strictly increases as the number of
channel outputs jointly measured at the receiver increases. We analyze this
trade-off between information rate and receiver complexity by considering the
capacity of the classical discrete memoryless superchannel induced under a
concatenated coding scheme, where the quantum joint detection measurement acts
exclusively on the length-$N$ inner codewords, but unfettered from any
assumption on the complexity of the classical outer code. We give a general
lower bound on the maximum accessible information per channel use for a
finite-length joint measurement, and we further refine the bound purely in
terms of $V$, the quantum version of channel dispersion, and $C$, the capacity
of the classical-quantum channel. We also observe a similar phenomenon of
superadditivity even in the channel capacity of a classical discrete memoryless
channel (DMC) in a concatenated coding scheme, due to a loss of information
from hard-decisions by the inner decoder over a finite blocklength, $N$.
Finally, we develop a unifying framework, within which the superadditivity in
capacity of the classical DMC and that of a pure-state classical-quantum
channel can both be expressed with a parameter $V/C^2$, a quantity that we show
is proportional to the inner-code measurement length $N$ that is sufficient to
achieve a given fraction $0 &lt; \alpha \le 1$ of the capacity $C$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3794</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3794</id><created>2013-10-14</created><updated>2013-11-04</updated><authors><author><keyname>Ji</keyname><forenames>Zhengfeng</forenames></author></authors><title>Binary Constraint System Games and Locally Commutative Reductions</title><categories>quant-ph cs.CC</categories><comments>21 pages, 6 figures; v2 contains an explicit proof of Lemma 4, a
  theorem on HORN-SAT* and several extended discussions</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A binary constraint system game is a two-player one-round non-local game
defined by a system of Boolean constraints. The game has a perfect quantum
strategy if and only if the constraint system has a quantum satisfying
assignment [R. Cleve and R. Mittal, arXiv:1209.2729]. We show that several
concepts including the quantum chromatic number and the Kochen-Specker sets
that arose from different contexts fit naturally in the binary constraint
system framework. The structure and complexity of the quantum satisfiability
problems for these constraint systems are investigated. Combined with a new
construct called the commutativity gadget for each problem, several classic
NP-hardness reductions are lifted to their corresponding quantum versions. We
also provide a simple parity constraint game that requires $\Omega(\sqrt{n})$
EPR pairs in perfect strategies where $n$ is the number of variables in the
constraint system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3805</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3805</id><created>2013-10-14</created><authors><author><keyname>Sur</keyname><forenames>Chiranjib</forenames></author><author><keyname>Shukla</keyname><forenames>Anupam</forenames></author></authors><title>Green Heron Swarm Optimization Algorithm - State-of-the-Art of a New
  Nature Inspired Discrete Meta-Heuristics</title><categories>cs.NE</categories><comments>20 pages, Pre-print copy, submitted to a peer reviewed journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many real world problems are NP-Hard problems are a very large part of them
can be represented as graph based problems. This makes graph theory a very
important and prevalent field of study. In this work a new bio-inspired
meta-heuristics called Green Heron Swarm Optimization (GHOSA) Algorithm is
being introduced which is inspired by the fishing skills of the bird. The
algorithm basically suited for graph based problems like combinatorial
optimization etc. However introduction of an adaptive mathematical variation
operator called Location Based Neighbour Influenced Variation (LBNIV) makes it
suitable for high dimensional continuous domain problems. The new algorithm is
being operated on the traditional benchmark equations and the results are
compared with Genetic Algorithm and Particle Swarm Optimization. The algorithm
is also operated on Travelling Salesman Problem, Quadratic Assignment Problem,
Knapsack Problem dataset. The procedure to operate the algorithm on the
Resource Constraint Shortest Path and road network optimization is also
discussed. The results clearly demarcates the GHOSA algorithm as an efficient
algorithm specially considering that the number of algorithms for the discrete
optimization is very low and robust and more explorative algorithm is required
in this age of social networking and mostly graph based problem scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3808</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3808</id><created>2013-10-14</created><authors><author><keyname>White</keyname><forenames>Howard D.</forenames></author><author><keyname>Mayr</keyname><forenames>Philipp</forenames></author></authors><title>Pennants for Descriptors</title><categories>cs.DL cs.IR</categories><comments>3 pages, 1 figure, paper presented at the NKOS workshop at TPDL 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new technique (called pennants) for displaying the descriptors
related to a descriptor across literatures, rather in a thesaurus. It has
definite implications for online searching and browsing. Pennants, named for
the flag they resemble, are a form of algorithmic prediction. Their cognitive
base is in relevance theory (RT) from linguistic pragmatics (Sperber &amp; Wilson
1995).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3809</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3809</id><created>2013-10-10</created><authors><author><keyname>Trei</keyname><forenames>Wilke</forenames></author></authors><title>Efficient Modular Arithmetic for SIMD Devices</title><categories>cs.CR cs.PF math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes several new improvements of modular arithmetic and how
to exploit them in order to gain more efficient implementations of commonly
used algorithms, especially in cryptographic applications. We further present a
new record for modular multiplications per second on a single desktop computer
as well as a new record for the ECM factoring algorithm. This new results allow
building personal computers which can handle more than 3 billion modular
multiplications per second for a 192 bit module at moderate costs using modern
graphic cards.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3843</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3843</id><created>2013-10-14</created><updated>2014-04-30</updated><authors><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Sanguinetti</keyname><forenames>Luca</forenames></author><author><keyname>Hoydis</keyname><forenames>Jakob</forenames></author><author><keyname>Debbah</keyname><forenames>M&#xe9;rouane</forenames></author></authors><title>Designing Multi-User MIMO for Energy Efficiency: When is Massive MIMO
  the Answer?</title><categories>cs.IT math.IT</categories><comments>Published at IEEE Wireless Communications and Networking Conference
  (WCNC 2014), 6 pages, 5 figures, 1 table. This version improves the visual
  presentation of Fig. 2 and corrects a typo in Lemma 2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Assume that a multi-user multiple-input multiple-output (MIMO) communication
system must be designed to cover a given area with maximal energy efficiency
(bit/Joule). What are the optimal values for the number of antennas, active
users, and transmit power? By using a new model that describes how these three
parameters affect the total energy efficiency of the system, this work provides
closed-form expressions for their optimal values and interactions. In sharp
contrast to common belief, the transmit power is found to increase (not
decrease) with the number of antennas. This implies that energy efficient
systems can operate at high signal-to-noise ratio (SNR) regimes in which the
use of interference-suppressing precoding schemes is essential. Numerical
results show that the maximal energy efficiency is achieved by a massive MIMO
setup wherein hundreds of antennas are deployed to serve relatively many users
using interference-suppressing regularized zero-forcing precoding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3849</identifier>
 <datestamp>2013-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3849</id><created>2013-10-11</created><authors><author><keyname>Mishra</keyname><forenames>Umakant</forenames></author></authors><title>10 Inventions on laptop keyboards -A study based on US patents</title><categories>cs.HC</categories><comments>Published in TRIZsite Journal, May 2005, also available at
  http://papers.ssrn.com/abstract=932274. arXiv admin note: substantial text
  overlap with arXiv:1307.5426, arXiv:1310.3268, arXiv:1310.3070</comments><doi>10.2139/ssrn.932274</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A desktop keyboard has several sections like character key section,
navigation key section, numeric key section, and function key section etc. each
consisting of several number of keys. However, a laptop computer does not have
so much of space to accommodate all these keys into the keyboard. There are
several considerations while designing a laptop keyboard.
  This article illustrates 10 inventions on keyboards for laptop and portable
computers. The inventions are selected from US patent database. The inventions
try to improve various aspects of a laptop keyboard, such as reducing size,
folding and concealing, ergonomic features, improving quality and reducing
cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3850</identifier>
 <datestamp>2013-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3850</id><created>2013-10-11</created><authors><author><keyname>Mishra</keyname><forenames>Umakant</forenames></author></authors><title>10 Inventions on modular keyboards: A TRIZ based analysis</title><categories>cs.HC</categories><comments>Published in TRIZsite Journal, March 2005, also available in
  http://papers.ssrn.com/abstract=932269. arXiv admin note: substantial text
  overlap with arXiv:1310.3070, arXiv:1310.3268, arXiv:1310.3849</comments><doi>10.2139/ssrn.932269</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As a standard keyboard is quite spacious many inventions try to use the space
of keyboard to use for various activities. A modular keyboard is designed in
such a way that the components of the keyboard can be attached and detached as
per the need.
  This article illustrates 10 inventions on modular keyboard from US patent
database. The objective of these inventions is to make a keyboard modular, so
that the same keyboard can be made smaller by detaching some of its components
and larger by attaching additional attachments. Some modular keyboards provide
slots for external attachments like mouse, telephone, speakers, joystick and
storage devices etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3875</identifier>
 <datestamp>2013-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3875</id><created>2013-10-14</created><authors><author><keyname>Li</keyname><forenames>Zhuchun</forenames></author><author><keyname>Ha</keyname><forenames>Seung-Yeal</forenames></author></authors><title>Cucker-Smale flocking with alternating leaders</title><categories>cs.MA</categories><comments>14pages</comments><msc-class>92D25, 92D50</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the emergent flocking behavior in a group of Cucker-Smale flocking
agents under rooted leadership with alternating leaders. It is well known that
the network topology regulates the emergent behaviors of flocks. All existing
results on the Cucker-Smale model with leader-follower topologies assume a
fixed leader during temporal evolution process. The rooted leadership is the
most general topology taking a leadership. Motivated by collective behaviors
observed in the flocks of birds, swarming fishes and potential engineering
applications, we consider the rooted leadership with alternating leaders; that
is, at each time slice there is a leader but it can be switched among the
agents from time to time. We will provide several sufficient conditions leading
to the asymptotic flocking among the Cucker-Smale agents under rooted
leadership with alternating leaders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3883</identifier>
 <datestamp>2014-08-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3883</id><created>2013-10-14</created><updated>2014-08-07</updated><authors><author><keyname>Haddad</keyname><forenames>Majed</forenames></author><author><keyname>Wiecek</keyname><forenames>Piotr</forenames></author><author><keyname>Habachi</keyname><forenames>Oussama</forenames></author><author><keyname>Hayel</keyname><forenames>Yezekael</forenames></author></authors><title>A Game Theoretic Analysis for Energy Efficient Heterogeneous Networks</title><categories>cs.GT cs.IT math.IT</categories><comments>7 pages, 3 figures, in Wiopt 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Smooth and green future extension/scalability (e.g., from sparse to dense,
from small-area dense to large-area dense, or from normal-dense to super-dense)
is an important issue in heterogeneous networks. In this paper, we study energy
efficiency of heterogeneous networks for both sparse and dense two-tier small
cell deployments. We formulate the problem as a hierarchical (Stackelberg) game
in which the macro cell is the leader whereas the small cell is the follower.
Both players want to strategically decide on their power allocation policies in
order to maximize the energy efficiency of their registered users. A backward
induction method has been used to obtain a closed-form expression of the
Stackelberg equilibrium. It is shown that the energy efficiency is maximized
when only one sub-band is exploited for the players of the game depending on
their fading channel gains. Simulation results are presented to show the
effectiveness of the proposed scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3892</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3892</id><created>2013-10-14</created><updated>2014-05-05</updated><authors><author><keyname>Price</keyname><forenames>Bradley S.</forenames></author><author><keyname>Geyer</keyname><forenames>Charles J.</forenames></author><author><keyname>Rothman</keyname><forenames>Adam J.</forenames></author></authors><title>Ridge Fusion in Statistical Learning</title><categories>stat.ML cs.LG stat.CO</categories><comments>24 pages and 9 tables, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a penalized likelihood method to jointly estimate multiple
precision matrices for use in quadratic discriminant analysis and model based
clustering. A ridge penalty and a ridge fusion penalty are used to introduce
shrinkage and promote similarity between precision matrix estimates. Block-wise
coordinate descent is used for optimization, and validation likelihood is used
for tuning parameter selection. Our method is applied in quadratic discriminant
analysis and semi-supervised model based clustering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3898</identifier>
 <datestamp>2013-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3898</id><created>2013-10-14</created><authors><author><keyname>Gall</keyname><forenames>Fran&#xe7;ois Le</forenames></author><author><keyname>Nishimura</keyname><forenames>Harumichi</forenames></author></authors><title>Quantum Algorithms for Matrix Products over Semirings</title><categories>quant-ph cs.DS</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we construct quantum algorithms for matrix products over
several algebraic structures called semirings, including the (max,min)-matrix
product, the distance matrix product and the Boolean matrix product. In
particular, we obtain the following results.
  We construct a quantum algorithm computing the product of two n x n matrices
over the (max,min) semiring with time complexity O(n^{2.473}). In comparison,
the best known classical algorithm for the same problem, by Duan and Pettie,
has complexity O(n^{2.687}). As an application, we obtain a O(n^{2.473})-time
quantum algorithm for computing the all-pairs bottleneck paths of a graph with
n vertices, while classically the best upper bound for this task is
O(n^{2.687}), again by Duan and Pettie.
  We construct a quantum algorithm computing the L most significant bits of
each entry of the distance product of two n x n matrices in time O(2^{0.64L}
n^{2.46}). In comparison, prior to the present work, the best known classical
algorithm for the same problem, by Vassilevska and Williams and Yuster, had
complexity O(2^{L}n^{2.69}). Our techniques lead to further improvements for
classical algorithms as well, reducing the classical complexity to
O(2^{0.96L}n^{2.69}), which gives a sublinear dependency on 2^L.
  The above two algorithms are the first quantum algorithms that perform better
than the $\tilde O(n^{5/2})$-time straightforward quantum algorithm based on
quantum search for matrix multiplication over these semirings. We also consider
the Boolean semiring, and construct a quantum algorithm computing the product
of two n x n Boolean matrices that outperforms the best known classical
algorithms for sparse matrices. For instance, if the input matrices have
O(n^{1.686...}) non-zero entries, then our algorithm has time complexity
O(n^{2.277}), while the best classical algorithm has complexity O(n^{2.373}).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3902</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3902</id><created>2013-10-14</created><updated>2015-05-29</updated><authors><author><keyname>Chen</keyname><forenames>Dajiang</forenames></author><author><keyname>Jiang</keyname><forenames>Shaoquan</forenames></author><author><keyname>Qin</keyname><forenames>Zhiguang</forenames></author></authors><title>Message Authentication Code over a Wiretap Channel</title><categories>cs.IT cs.CR math.IT</categories><comments>Formulation of model is changed</comments><journal-ref>ISIT 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Message Authentication Code (MAC) is a keyed function $f_K$ such that when
Alice, who shares the secret $K$ with Bob, sends $f_K(M)$ to the latter, Bob
will be assured of the integrity and authenticity of $M$. Traditionally, it is
assumed that the channel is noiseless. However, Maurer showed that in this case
an attacker can succeed with probability $2^{-\frac{H(K)}{\ell+1}}$ after
authenticating $\ell$ messages. In this paper, we consider the setting where
the channel is noisy. Specifically, Alice and Bob are connected by a discrete
memoryless channel (DMC) $W_1$ and a noiseless but insecure channel. In
addition, an attacker Oscar is connected with Alice through DMC $W_2$ and with
Bob through a noiseless channel. In this setting, we study the framework that
sends $M$ over the noiseless channel and the traditional MAC $f_K(M)$ over
channel $(W_1, W_2)$. We regard the noisy channel as an expensive resource and
define the authentication rate $\rho_{auth}$ as the ratio of message length to
the number $n$ of channel $W_1$ uses. The security of this framework depends on
the channel coding scheme for $f_K(M)$. A natural coding scheme is to use the
secrecy capacity achieving code of Csisz\'{a}r and K\&quot;{o}rner. Intuitively,
this is also the optimal strategy. However, we propose a coding scheme that
achieves a higher $\rho_{auth}.$ Our crucial point for this is that in the
secrecy capacity setting, Bob needs to recover $f_K(M)$ while in our coding
scheme this is not necessary. How to detect the attack without recovering
$f_K(M)$ is the main contribution of this work. We achieve this through random
coding techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3911</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3911</id><created>2013-10-14</created><updated>2015-01-22</updated><authors><author><keyname>Wang</keyname><forenames>Yongqing</forenames></author><author><keyname>Shen</keyname><forenames>Hua-Wei</forenames></author><author><keyname>Liu</keyname><forenames>Shenghua</forenames></author><author><keyname>Cheng</keyname><forenames>Xue-Qi</forenames></author></authors><title>Learning user-specific latent influence and susceptibility from
  information cascades</title><categories>cs.SI physics.soc-ph</categories><comments>from The 29th AAAI Conference on Artificial Intelligence (AAAI-2015)</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Predicting cascade dynamics has important implications for understanding
information propagation and launching viral marketing. Previous works mainly
adopt a pair-wise manner, modeling the propagation probability between pairs of
users using n^2 independent parameters for n users. Consequently, these models
suffer from severe overfitting problem, specially for pairs of users without
direct interactions, limiting their prediction accuracy. Here we propose to
model the cascade dynamics by learning two low-dimensional user-specific
vectors from observed cascades, capturing their influence and susceptibility
respectively. This model requires much less parameters and thus could combat
overfitting problem. Moreover, this model could naturally model
context-dependent factors like cumulative effect in information propagation.
Extensive experiments on synthetic dataset and a large-scale microblogging
dataset demonstrate that this model outperforms the existing pair-wise models
at predicting cascade dynamics, cascade size, and &quot;who will be retweeted&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3932</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3932</id><created>2013-10-15</created><authors><author><keyname>Holme</keyname><forenames>Petter</forenames></author></authors><title>Extinction times of epidemic outbreaks in networks</title><categories>q-bio.PE cs.SI physics.soc-ph</categories><doi>10.1371/journal.pone.0084429</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Susceptible-Infectious-Recovered (SIR) model of disease spreading, the
time to extinction of the epidemics happens at an intermediate value of the
per-contact transmission probability. Too contagious infections burn out fast
in the population. Infections that are not contagious enough die out before
they spread to a large fraction of people. We characterize how the maximal
extinction time in SIR simulations on networks depend on the network structure.
For example we find that the average distances in isolated components, weighted
by the component size, is a good predictor of the maximal time to extinction.
Furthermore, the transmission probability giving the longest outbreaks is
larger than, but otherwise seemingly independent of, the epidemic threshold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3939</identifier>
 <datestamp>2013-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3939</id><created>2013-10-15</created><authors><author><keyname>Sacca'</keyname><forenames>Domenico</forenames></author><author><keyname>Serra</keyname><forenames>Edoardo</forenames></author><author><keyname>Dicosta</keyname><forenames>Pietro</forenames></author><author><keyname>Piccolo</keyname><forenames>Antonio</forenames></author></authors><title>Multi-Sorted Inverse Frequent Itemsets Mining</title><categories>cs.DB</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The development of novel platforms and techniques for emerging &quot;Big Data&quot;
applications requires the availability of real-life datasets for data-driven
experiments, which are however out of reach for academic research in most cases
as they are typically proprietary. A possible solution is to use synthesized
datasets that reflect patterns of real ones in order to ensure high quality
experimental findings. A first step in this direction is to use inverse mining
techniques such as inverse frequent itemset mining (IFM) that consists of
generating a transactional database satisfying given support constraints on the
itemsets in an input set, that are typically the frequent ones. This paper
introduces an extension of IFM, called many-sorted IFM, where the schemes for
the datasets to be generated are those typical of Big Tables as required in
emerging big data applications, e.g., social network analytics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3945</identifier>
 <datestamp>2013-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3945</id><created>2013-10-15</created><authors><author><keyname>Ciancia</keyname><forenames>Vincenzo</forenames></author><author><keyname>Sammartino</keyname><forenames>Matteo</forenames></author></authors><title>A decidable class of (nominal) omega-regular languages over an infinite
  alphabet</title><categories>cs.FL cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a class of languages of infinite words over infinite alphabets, and
the corresponding automata. The automata used for recognition are a
generalisation of deterministic Muller automata to the setting of nominal sets.
Remarkably, the obtained languages are determined by their ultimately periodic
fragments, as in the classical case. Closure under complement, union and
intersection, and decidability of emptiness and equivalence are preserved by
the generalisation. This is shown by using finite representations of the
(otherwise infinite-state) defined class of automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3946</identifier>
 <datestamp>2013-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3946</id><created>2013-10-15</created><authors><author><keyname>Makki</keyname><forenames>Behrooz</forenames></author><author><keyname>Amat</keyname><forenames>Alexandre Graell i</forenames></author><author><keyname>Eriksson</keyname><forenames>Thomas</forenames></author></authors><title>On Noisy ARQ in Block-Fading Channels</title><categories>cs.IT math.IT stat.AP</categories><comments>Accepted for publication on IEEE Transactions on Vehicular Technology</comments><doi>10.1109/TVT.2013.2276371</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Assuming noisy feedback channels, this paper investigates the data
transmission efficiency and robustness of different automatic repeat request
(ARQ) schemes using adaptive power allocation. Considering different
block-fading channel assumptions, the long-term throughput, the delay-limited
throughput, the outage probability and the feedback load of different ARQ
protocols are studied. A closed-form expression for the power-limited
throughput optimization problem is obtained which is valid for different ARQ
protocols and feedback channel conditions. Furthermore, the paper presents
numerical investigations on the robustness of different ARQ protocols to
feedback errors. It is shown that many analytical assertions about the ARQ
protocols are valid both when the channel remains fixed during all
retransmission rounds and when it changes in each round (in)dependently. As
demonstrated, optimal power allocation is crucial for the performance of noisy
ARQ schemes when the goal is to minimize the outage probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3954</identifier>
 <datestamp>2013-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3954</id><created>2013-10-15</created><authors><author><keyname>Zeng</keyname><forenames>Jinshan</forenames></author><author><keyname>Lin</keyname><forenames>Shaobo</forenames></author><author><keyname>Xu</keyname><forenames>Zongben</forenames></author></authors><title>Sparse Solution of Underdetermined Linear Equations via Adaptively
  Iterative Thresholding</title><categories>cs.IT math.IT</categories><comments>33 pages, 1 figure</comments><msc-class>68W40</msc-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Finding the sparset solution of an underdetermined system of linear equations
$y=Ax$ has attracted considerable attention in recent years. Among a large
number of algorithms, iterative thresholding algorithms are recognized as one
of the most efficient and important classes of algorithms. This is mainly due
to their low computational complexities, especially for large scale
applications. The aim of this paper is to provide guarantees on the global
convergence of a wide class of iterative thresholding algorithms. Since the
thresholds of the considered algorithms are set adaptively at each iteration,
we call them adaptively iterative thresholding (AIT) algorithms. As the main
result, we show that as long as $A$ satisfies a certain coherence property, AIT
algorithms can find the correct support set within finite iterations, and then
converge to the original sparse solution exponentially fast once the correct
support set has been identified. Meanwhile, we also demonstrate that AIT
algorithms are robust to the algorithmic parameters. In addition, it should be
pointed out that most of the existing iterative thresholding algorithms such as
hard, soft, half and smoothly clipped absolute deviation (SCAD) algorithms are
included in the class of AIT algorithms studied in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3970</identifier>
 <datestamp>2013-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3970</id><created>2013-10-15</created><authors><author><keyname>Makki</keyname><forenames>Behrooz</forenames></author><author><keyname>Amat</keyname><forenames>Alexandre Graell i</forenames></author><author><keyname>Eriksson</keyname><forenames>Thomas</forenames></author></authors><title>Green Communication via Power-optimized HARQ Protocols</title><categories>cs.IT math.IT stat.AP</categories><comments>Accepted for publication on IEEE Transactions on Vehicular Technology</comments><doi>10.1109/TVT.2013.2274287</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, efficient use of energy has become an essential research topic for
green communication. This paper studies the effect of optimal power controllers
on the performance of delay-sensitive communication setups utilizing hybrid
automatic repeat request (HARQ). The results are obtained for repetition time
diversity (RTD) and incremental redundancy (INR) HARQ protocols. In all cases,
the optimal power allocation, minimizing the outage-limited average
transmission power, is obtained under both continuous and bursting
communication models. Also, we investigate the system throughput in different
conditions. The results indicate that the power efficiency is increased
substantially, if adaptive power allocation is utilized. For instance, assume
Rayleigh-fading channel, a maximum of two (re)transmission rounds with rates
$\{1,\frac{1}{2}\}$ nats-per-channel-use and an outage probability constraint
${10}^{-3}$. Then, compared to uniform power allocation, optimal power
allocation in RTD reduces the average power by 9 and 11 dB in the bursting and
continuous communication models, respectively. In INR, these values are
obtained to be 8 and 9 dB, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3971</identifier>
 <datestamp>2013-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3971</id><created>2013-10-15</created><authors><author><keyname>Birolo</keyname><forenames>Giovanni</forenames></author></authors><title>A Monadic Framework for Interactive Realizability</title><categories>cs.LO</categories><msc-class>03F03</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a new presentation of interactive realizability with a more explicit
syntax.
  Interactive realizability is a realizability semantics that extends the
Curry-Howard correspondence to (sub-)classical logic, more precisely to
first-order intuitionistic arithmetic (Heyting Arithmetic) extended by the law
of the excluded middle restricted to simply existential formulas, a system
motivated by its applications in proof mining.
  Monads can be used to structure functional programs by providing a clean and
modular way to include impure features in purely functional languages. We
express interactive realizers by means of an abstract framework that applies
the monadic approach used in functional programming to modified realizability,
in order to obtain more &quot;relaxed&quot; realizability notions that are suitable to
classical logic. In particular we use a combination of the state and exception
monads in order to capture the learning-from-mistakes nature of interactive
realizers at the syntactic level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3973</identifier>
 <datestamp>2013-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3973</id><created>2013-10-15</created><authors><author><keyname>Huang</keyname><forenames>Lirong</forenames></author><author><keyname>Hjalmarsson</keyname><forenames>H&#xe5;kan</forenames></author><author><keyname>Gerencs&#xe9;r</keyname><forenames>L&#xe1;szl&#xf3;</forenames></author></authors><title>Adaptive experiment design for LTI systems</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimal experiment design for parameter estimation is a research topic that
has been in the interest of various studies. A key problem in optimal input
design is that the optimal input depends on some unknown system parameters that
are to be identified. Adaptive design is one of the fundamental routes to
handle this problem. Although there exist a rich collection of results on
adaptive experiment design, there are few results that address these issues for
dynamic systems. This paper proposes an adaptive input design method for
general single-input single-output linear-time-invariant systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3975</identifier>
 <datestamp>2013-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3975</id><created>2013-10-15</created><authors><author><keyname>Makki</keyname><forenames>Behrooz</forenames></author><author><keyname>Amat</keyname><forenames>Alexandre Graell i</forenames></author><author><keyname>Eriksson</keyname><forenames>Thomas</forenames></author></authors><title>HARQ Feedback in Spectrum Sharing Networks</title><categories>cs.IT math.IT stat.AP</categories><comments>Published in IEEE Communications Letters</comments><journal-ref>&quot;HARQ Feedback in Spectrum Sharing Networks,&quot; 16(9), pp.
  1337-1340, Sept. 2012</journal-ref><doi>10.1109/LCOMM.2012.070512.112003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter studies the throughput and the outage probability of spectrum
sharing networks utilizing hybrid automatic repeat request (HARQ) feedback. We
focus on the repetition time diversity and the incremental redundancy HARQ
protocols where the results are obtained for both continuous and bursting
communication models. The channel data transmission efficiency is investigated
in the presence of both secondary user peak transmission power and primary user
received interference power constraints. Finally, we evaluate the effect of
secondary-primary channel state information imperfection on the performance of
the secondary channel. Simulation results show that, while the throughput is
not necessarily increased by HARQ, substantial outage probability reduction is
achieved in all conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3980</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3980</id><created>2013-10-15</created><updated>2014-07-11</updated><authors><author><keyname>Van Mieghem</keyname><forenames>Piet</forenames></author></authors><title>Decay towards the overall-healthy state in SIS epidemics on networks</title><categories>math.PR cond-mat.stat-mech cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The decay rate of SIS epidemics on the complete graph $K_{N}$ is computed
analytically, based on a new, algebraic method to compute the second largest
eigenvalue of a stochastic three-diagonal matrix up to arbitrary precision. The
latter problem has been addressed around 1950, mainly via the theory of
orthogonal polynomials and probability theory. The accurate determination of
the second largest eigenvalue, also called the \emph{decay parameter}, has been
an outstanding problem appearing in general birth-death processes and random
walks. Application of our general framework to SIS epidemics shows that the
maximum average lifetime of an SIS epidemics in any network with $N$ nodes is
not larger (but tight for $K_{N}$) than \[ E\left[ T\right]
\sim\frac{1}{\delta}\frac{\frac{\tau}{\tau_{c}}\sqrt{2\pi}% }{\left(
\frac{\tau}{\tau_{c}}-1\right) ^{2}}\frac{\exp\left( N\left\{
\log\frac{\tau}{\tau_{c}}+\frac{\tau_{c}}{\tau}-1\right\} \right) }{\sqrt
{N}}=O\left( e^{N\ln\frac{\tau}{\tau_{c}}}\right) \] for large $N$ and for an
effective infection rate $\tau=\frac{\beta}{\delta}$ above the epidemic
threshold $\tau_{c}$. Our order estimate of $E\left[ T\right] $ sharpens the
order estimate $E\left[ T\right] =O\left( e^{bN^{a}}\right) $ of Draief and
Massouli\'{e} \cite{Draief_Massoulie}. Combining the lower bound results of
Mountford \emph{et al.} \cite{Mountford2013} and our upper bound, we conclude
that for almost all graphs, the average time to absorption for $\tau&gt;\tau_{c}$
is $E\left[ T\right] =O\left( e^{c_{G}N}\right) $, where $c_{G}&gt;0$ depends on
the topological structure of the graph $G$ and $\tau$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.3990</identifier>
 <datestamp>2013-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.3990</id><created>2013-10-15</created><authors><author><keyname>Bhattacharjee</keyname><forenames>Subhasis</forenames></author></authors><title>Distributed Algorithm for Dynamic Data-Gathering in Sensor Network</title><categories>cs.DC cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In WSN, each sensor is responsible for sensing environmental conditions and
sending them to the one or more base stations. Battery-operated sensors are
severely constrained by the amount of energy that can be spend for transmitting
these sensed data. However, aggregation of data (including removal of redundant
data) at intermediate sensors and forwarding of aggregate data reduce overall
energy consumptions in WSN. In general, data gathering refers to the process of
periodic collection of sensed data from various sensors to one or more base
stations (BS). Energy efficient data gathering scheduling is essential for
improving the lifetime of WSN. In this paper, we propose a distributed
algorithm to compute data-gathering schedule that aim to improve the lifetime
of WSN by suitably selecting energy-efficient data-flow paths from various
sensors to the base station. For a multihop WSN with $n$ sensors, the proposed
algorithm first computes a schedule in $O(n^2)$ time steps, and then this
schedule is periodically updated based the residual energy and the feedback
received from the BS. The system performs approximately $\log(\mathcal{L})$
schedule updates where $\mathcal{L}$ is the expected lifetime of the system in
number of data-gathering rounds. Moreover, each updation process uses the
existing active schedule (data-flow path) - thus consuming only a small
fraction of a single data gathering round activity. Such an algorithm thus
could precisely incorporate the energy consumptions due to updates and related
activities. Moreover, our algorithm does not assume any global knowledge of the
topology or the positions of various sensors. Through simulation study, we
found that our proposed algorithm achieves significantly higher network
lifetime compared to existing data-flow schedules based on the Minimum Spanning
Tree (MST), the Shortest Path Tree (SPT), the Weighted Rooted Tree (WRT).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4000</identifier>
 <datestamp>2013-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4000</id><created>2013-10-15</created><authors><author><keyname>Adhatrao</keyname><forenames>Kalpesh</forenames></author><author><keyname>Gaykar</keyname><forenames>Aditya</forenames></author><author><keyname>Jha</keyname><forenames>Rohit</forenames></author><author><keyname>Honrao</keyname><forenames>Vipul</forenames></author></authors><title>A Secure Method for Signing in Using Quick Response Codes with Mobile
  Authentication</title><categories>cs.CR</categories><journal-ref>International Journal of Student Research in Technology and
  Management (IJSRTM), 1(1):1-11, March 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The emerging threats to user privacy over the internet are increasing at an
alarming rate. Signing in from an unreliable terminal into a web account may
result in compromising private details of a user such as username and password,
by means of keylogger software. Such software are capable of recording
keystrokes secretly, via covert channels without the knowledge of the user. In
this paper we propose a secure method for signing in using Quick Response (QR)
codes with mobile authentication. Through this method, the user can securely
sign-in into a web account by authenticating the user session on an unreliable
terminal browser, using a mobile device.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4019</identifier>
 <datestamp>2013-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4019</id><created>2013-10-15</created><authors><author><keyname>Carbone</keyname><forenames>Marco</forenames><affiliation>ITU, Denmark</affiliation></author><author><keyname>Lanese</keyname><forenames>Ivan</forenames><affiliation>University of Bologna/INRIA, Italy</affiliation></author><author><keyname>Lafuente</keyname><forenames>Alberto Lluch</forenames><affiliation>IMT Institute for Advanced Studies Lucca, Italy</affiliation></author><author><keyname>Sokolova</keyname><forenames>Ana</forenames><affiliation>University of Salzburg, Austria</affiliation></author></authors><title>Proceedings 6th Interaction and Concurrency Experience</title><categories>cs.PL cs.DC cs.LO</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 131, 2013</journal-ref><doi>10.4204/EPTCS.131</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of ICE 2013, the 6th Interaction and
Concurrency Experience workshop, which was held in Florence, Italy on the 6th
of June 2013 as a satellite event of DisCoTec 2013. The ICE procedure for paper
selection allows PC members to interact, anonymously, with authors. During the
review phase, each submitted paper is published on a Wiki and associated with a
discussion forum whose access is restricted to the authors and to all the PC
members not declaring a conflict of interests. The PC members post comments and
questions that the authors reply to. Each paper was reviewed by three PC
members, and altogether 6 papers were accepted for publication. We were proud
to host two invited talks, Davide Sangiorgi and Filippo Bonchi, whose abstracts
are included in this volume together with the regular papers. The workshop also
featured a brief announcement of an already published paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4023</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4023</id><created>2013-10-15</created><updated>2014-03-26</updated><authors><author><keyname>Chen</keyname><forenames>Yi</forenames></author><author><keyname>Wang</keyname><forenames>Xiao-long</forenames></author><author><keyname>Yuan</keyname><forenames>Bo</forenames></author><author><keyname>Tang</keyname><forenames>Bu-zhou</forenames></author></authors><title>Overlapping community detection in signed networks</title><categories>cs.SI physics.soc-ph</categories><comments>17 pages, 11 figures</comments><acm-class>I.5.3; H.2.8</acm-class><journal-ref>J. Stat. Mech. (2014) P03021</journal-ref><doi>10.1088/1742-5468/2014/03/P03021</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Complex networks considering both positive and negative links have gained
considerable attention during the past several years. Community detection is
one of the main challenges for complex network analysis. Most of the existing
algorithms for community detection in a signed network aim at providing a
hard-partition of the network where any node should belong to a community or
not. However, they cannot detect overlapping communities where a node is
allowed to belong to multiple communities. The overlapping communities widely
exist in many real world networks. In this paper, we propose a signed
probabilistic mixture (SPM) model for overlapping community detection in signed
networks. Compared with the existing models, the advantages of our methodology
are (i) providing soft-partition solutions for signed networks; (ii) providing
soft-memberships of nodes. Experiments on a number of signed networks show that
our SPM model: (i) can identify assortative structures or disassortative
structures as the same as other state-of-the-art models; (ii) can detect
overlapping communities; (iii) outperform other state-of-the-art models at
shedding light on the community detection in synthetic signed networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4038</identifier>
 <datestamp>2013-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4038</id><created>2013-10-15</created><authors><author><keyname>Perera</keyname><forenames>Charith</forenames></author><author><keyname>Jayaraman</keyname><forenames>Prem Prakash</forenames></author><author><keyname>Zaslavsky</keyname><forenames>Arkady</forenames></author><author><keyname>Christen</keyname><forenames>Peter</forenames></author><author><keyname>Georgakopoulos</keyname><forenames>Dimitrios</forenames></author></authors><title>MOSDEN: An Internet of Things Middleware for Resource Constrained Mobile
  Devices</title><categories>cs.NI cs.DC</categories><journal-ref>Proceedings of the 47th Hawaii International Conference on System
  Sciences (HICSS), Kona, Hawaii, USA, January, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Internet of Things (IoT) is part of Future Internet and will comprise
many billions of Internet Connected Objects (ICO) or `things' where things can
sense, communicate, compute and potentially actuate as well as have
intelligence, multi-modal interfaces, physical/ virtual identities and
attributes. Collecting data from these objects is an important task as it
allows software systems to understand the environment better. Many different
hardware devices may involve in the process of collecting and uploading sensor
data to the cloud where complex processing can occur. Further, we cannot expect
all these objects to be connected to the computers due to technical and
economical reasons. Therefore, we should be able to utilize resource
constrained devices to collect data from these ICOs. On the other hand, it is
critical to process the collected sensor data before sending them to the cloud
to make sure the sustainability of the infrastructure due to energy
constraints. This requires to move the sensor data processing tasks towards the
resource constrained computational devices (e.g. mobile phones). In this paper,
we propose Mobile Sensor Data Processing Engine (MOSDEN), an plug-in-based IoT
middleware for mobile devices, that allows to collect and process sensor data
without programming efforts. Our architecture also supports sensing as a
service model. We present the results of the evaluations that demonstrate its
suitability towards real world deployments. Our proposed middleware is built on
Android platform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4046</identifier>
 <datestamp>2013-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4046</id><created>2013-10-15</created><authors><author><keyname>Vabishchevich</keyname><forenames>Petr N.</forenames></author></authors><title>Explicit schemes for parabolic and hyperbolic equations</title><categories>cs.NA</categories><comments>12 pages</comments><msc-class>65J08, 65M06, 65M12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Standard explicit schemes for parabolic equations are not very convenient for
computing practice due to the fact that they have strong restrictions on a time
step. More promising explicit schemes are associated with explicit-implicit
splitting of the problem operator (Saul'yev asymmetric schemes, explicit
alternating direction (ADE) schemes, group explicit method). These schemes
belong to the class of unconditionally stable schemes, but they demonstrate bad
approximation properties. These explicit schemes are treated as schemes of the
alternating triangle method and can be considered as factorized schemes where
the problem operator is splitted into the sum of two operators that are adjoint
to each other. Here we propose a multilevel modification of the alternating
triangle method, which demonstrates better properties in terms of accuracy. We
also consider explicit schemes of the alternating triangle method for the
numerical solution of boundary value problems for hyperbolic equations of
second order. The study is based on the general theory of stability
(well-posedness) for operator-difference schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4050</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4050</id><created>2013-10-15</created><updated>2015-05-04</updated><authors><author><keyname>Bellini</keyname><forenames>Emanuele</forenames></author><author><keyname>Morgari</keyname><forenames>Guglielmo</forenames></author><author><keyname>Coppola</keyname><forenames>Marco</forenames></author></authors><title>An Extension of Cook's Elastic Cipher</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a block cipher of length L Cook's elastic cipher allows to encrypt
messages of variable length from L to 2L. Given some conditions on the key
schedule, Cook's elastic cipher is secure against any key recovery attack if
the underlying block cipher is, and it achieves complete diffusion in at most q
+ 1 rounds if the underlying block cipher achieves it in q rounds. We extend
Cook's construction inductively, obtaining an elastic cipher for any message
length greater than L with the same properties of security as Cook's elastic
cipher.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4052</identifier>
 <datestamp>2013-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4052</id><created>2013-10-15</created><authors><author><keyname>Jayaraman</keyname><forenames>Prem Prakash</forenames></author><author><keyname>Perera</keyname><forenames>Charith</forenames></author><author><keyname>Georgakopoulos</keyname><forenames>Dimitrios</forenames></author><author><keyname>Zaslavsky</keyname><forenames>Arkady</forenames></author></authors><title>Efficient Opportunistic Sensing using Mobile Collaborative Platform
  MOSDEN</title><categories>cs.NI cs.DC</categories><journal-ref>9th IEEE International Conference on Collaborative Computing:
  Networking, Applications and Worksharing (COLLABORATECOM), Austin, Texas,
  United States, October, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile devices are rapidly becoming the primary computing device in people's
lives. Application delivery platforms like Google Play, Apple App Store have
transformed mobile phones into intelligent computing devices by the means of
applications that can be downloaded and installed instantly. Many of these
applications take advantage of the plethora of sensors installed on the mobile
device to deliver enhanced user experience. The sensors on the smartphone
provide the opportunity to develop innovative mobile opportunistic sensing
applications in many sectors including healthcare, environmental monitoring and
transportation. In this paper, we present a collaborative mobile sensing
framework namely Mobile Sensor Data EngiNe (MOSDEN) that can operate on
smartphones capturing and sharing sensed data between multiple distributed
applications and users. MOSDEN follows a component-based design philosophy
promoting reuse for easy and quick opportunistic sensing application
deployments. MOSDEN separates the application-specific processing from the
sensing, storing and sharing. MOSDEN is scalable and requires minimal
development effort from the application developer. We have implemented our
framework on Android-based mobile platforms and evaluate its performance to
validate the feasibility and efficiency of MOSDEN to operate collaboratively in
mobile opportunistic sensing applications. Experimental outcomes and lessons
learnt conclude the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4060</identifier>
 <datestamp>2013-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4060</id><created>2013-10-15</created><authors><author><keyname>Bellini</keyname><forenames>Emanuele</forenames></author></authors><title>On the Griesmer Bound for Systematic Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalize the Griesmer bound in the case of systematic codes over a field
of size q greater than the distance d of the code. We also generalize the
Griesmer bound in the case of any systematic code of distance 2,3,4 and in the
case of binary systematic codes of distance up to 6.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4086</identifier>
 <datestamp>2013-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4086</id><created>2013-10-15</created><authors><author><keyname>Gabora</keyname><forenames>Liane</forenames></author><author><keyname>Chia</keyname><forenames>Wei Wen</forenames></author><author><keyname>Firouzi</keyname><forenames>Hadi</forenames></author></authors><title>A Computational Model of Two Cognitive Transitions Underlying Cultural
  Evolution</title><categories>cs.AI</categories><comments>arXiv admin note: text overlap with arXiv:1309.7407, arXiv:1308.5032,
  arXiv:1310.3781</comments><journal-ref>(2013). Proceedings of the Annual Meeting of the Cognitive Science
  Society. July 31-3, Berlin. Austin TX: Cognitive Science Society</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We tested the computational feasibility of the proposal that open-ended
cultural evolution was made possible by two cognitive transitions: (1) onset of
the capacity to chain thoughts together, followed by (2) onset of contextual
focus (CF): the capacity to shift between a divergent mode of thought conducive
to 'breaking out of a rut' and a convergent mode of thought conducive to minor
modifications. These transitions were simulated in EVOC, an agent-based model
of cultural evolution, in which the fitness of agents' actions increases as
agents invent ideas for new actions, and imitate the fittest of their
neighbors' actions. Both mean fitness and diversity of actions across the
society increased with chaining, and even more so with CF, as hypothesized. CF
was only effective when the fitness function changed, which supports its
hypothesized role in generating and refining ideas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4098</identifier>
 <datestamp>2013-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4098</id><created>2013-10-15</created><authors><author><keyname>Kempe</keyname><forenames>David</forenames></author><author><keyname>Lucier</keyname><forenames>Brendan</forenames></author></authors><title>User Satisfaction in Competitive Sponsored Search</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a model of competition between web search algorithms, and study
the impact of such competition on user welfare. In our model, search providers
compete for customers by strategically selecting which search results to
display in response to user queries. Customers, in turn, have private
preferences over search results and will tend to use search engines that are
more likely to display pages satisfying their demands.
  Our main question is whether competition between search engines increases the
overall welfare of the users (i.e., the likelihood that a user finds a page of
interest). When search engines derive utility only from customers to whom they
show relevant results, we show that they differentiate their results, and every
equilibrium of the resulting game achieves at least half of the welfare that
could be obtained by a social planner. This bound also applies whenever the
likelihood of selecting a given engine is a convex function of the probability
that a user's demand will be satisfied, which includes natural Markovian models
of user behavior.
  On the other hand, when search engines derive utility from all customers
(independent of search result relevance) and the customer demand functions are
not convex, there are instances in which the (unique) equilibrium involves no
differentiation between engines and a high degree of randomness in search
results. This can degrade social welfare by a factor of the square root of N
relative to the social optimum, where N is the number of webpages. These bad
equilibria persist even when search engines can extract only small (but
non-zero) expected revenue from dissatisfied users, and much higher revenue
from satisfied ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4106</identifier>
 <datestamp>2013-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4106</id><created>2013-10-15</created><authors><author><keyname>Miculan</keyname><forenames>Marino</forenames></author><author><keyname>Peressotti</keyname><forenames>Marco</forenames></author></authors><title>Weak bisimulations for labelled transition systems weighted over
  semirings</title><categories>cs.LO</categories><acm-class>F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Weighted labelled transition systems are LTSs whose transitions are given
weights drawn from a commutative monoid. WLTSs subsume a wide range of LTSs,
providing a general notion of strong (weighted) bisimulation. In this paper we
extend this framework towards other behavioural equivalences, by considering
semirings of weights. Taking advantage of this extra structure, we introduce a
general notion of weak weighted bisimulation. We show that weak weighted
bisimulation coincides with the usual weak bisimulations in the cases of
non-deterministic and fully-probabilistic systems; moreover, it naturally
provides a definition of weak bisimulation also for kinds of LTSs where this
notion is currently missing (such as, stochastic systems). Finally, we provide
a categorical account of the coalgebraic construction of weak weighted
bisimulation; this construction points out how to port our approach to other
equivalences based on different notion of observability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4113</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4113</id><created>2013-10-15</created><updated>2015-03-02</updated><authors><author><keyname>Dinur</keyname><forenames>Irit</forenames></author><author><keyname>Steurer</keyname><forenames>David</forenames></author><author><keyname>Vidick</keyname><forenames>Thomas</forenames></author></authors><title>A parallel repetition theorem for entangled projection games</title><categories>quant-ph cs.CC</categories><comments>30 pages. v2: improved exponent for expanding games; typos fixed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the behavior of the entangled value of two-player one-round
projection games under parallel repetition. We show that for any projection
game $G$ of entangled value 1-eps &lt; 1, the value of the $k$-fold repetition of
G goes to zero as O((1-eps^c)^k), for some universal constant c\geq 1.
Previously parallel repetition with an exponential decay in $k$ was only known
for the case of XOR and unique games. To prove the theorem we extend an
analytical framework recently introduced by Dinur and Steurer for the study of
the classical value of projection games under parallel repetition. Our proof,
as theirs, relies on the introduction of a simple relaxation of the entangled
value that is perfectly multiplicative. The main technical component of the
proof consists in showing that the relaxed value remains tightly connected to
the entangled value, thereby establishing the parallel repetition theorem. More
generally, we obtain results on the behavior of the entangled value under
products of arbitrary (not necessarily identical) projection games. Relating
our relaxed value to the entangled value is done by giving an algorithm for
converting a relaxed variant of quantum strategies that we call &quot;vector quantum
strategy&quot; to a quantum strategy. The algorithm is considerably simpler in case
the bipartite distribution of questions in the game has good expansion
properties. When this is not the case, rounding relies on a quantum analogue of
Holenstein's correlated sampling lemma which may be of independent interest.
Our &quot;quantum correlated sampling lemma&quot; generalizes results of van Dam and
Hayden on universal embezzlement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4125</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4125</id><created>2013-10-15</created><updated>2014-06-30</updated><authors><author><keyname>Fiorini</keyname><forenames>Samuel</forenames></author><author><keyname>Massar</keyname><forenames>Serge</forenames></author><author><keyname>Patra</keyname><forenames>Manas K.</forenames></author><author><keyname>Tiwary</keyname><forenames>Hans Raj</forenames></author></authors><title>Generalised probabilistic theories and conic extensions of polytopes</title><categories>quant-ph cs.DM</categories><comments>23 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generalized probabilistic theories (GPT) provide a general framework that
includes classical and quantum theories. It is described by a cone $C$ and its
dual $C^*$. We show that whether some one-way communication complexity problems
can be solved within a GPT is equivalent to the recently introduced cone
factorisation of the corresponding communication matrix $M$. We also prove an
analogue of Holevo's theorem: when the cone $C$ is contained in
$\mathbb{R}^{n}$, the classical capacity of the channel realised by sending GPT
states and measuring them is bounded by $\log n$.
  Polytopes and optimising functions over polytopes arise in many areas of
discrete mathematics. A conic extension of a polytope is the intersection of a
cone $C$ with an affine subspace whose projection onto the original space
yields the desired polytope. Extensions of polytopes can sometimes be much
simpler geometric objects than the polytope itself. The existence of a conic
extension of a polytope is equivalent to that of a cone factorisation of the
slack matrix of the polytope, on the same cone.
  We show that all $0/1$ polytopes whose vertices can be recognized by a
polynomial size circuit, which includes as a special case the travelling
salesman polytope and many other polytopes from combinatorial optimisation,
have small conic extension complexity when the cone is the completely positive
cone.
  Using recent exponential lower bounds on the linear extension complexity of
polytopes, this provides an exponential gap between the communication
complexity of GPT based on the completely positive cone and classical
communication complexity, and a conjectured exponential gap with quantum
communication complexity.
  Our work thus relates the communication complexity of generalisations of
quantum theory to questions of mainstream interest in the area of combinatorial
optimisation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4127</identifier>
 <datestamp>2014-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4127</id><created>2013-10-15</created><updated>2014-05-08</updated><authors><author><keyname>Gall</keyname><forenames>Fran&#xe7;ois Le</forenames></author><author><keyname>Nishimura</keyname><forenames>Harumichi</forenames></author><author><keyname>Tani</keyname><forenames>Seiichiro</forenames></author></authors><title>Quantum Algorithms for Finding Constant-sized Sub-hypergraphs</title><categories>quant-ph cs.CC cs.DS</categories><comments>18 pages; v2: changed title, added more backgrounds to the
  introduction, added another application</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a general framework to construct quantum algorithms that detect if
a $3$-uniform hypergraph given as input contains a sub-hypergraph isomorphic to
a prespecified constant-sized hypergraph. This framework is based on the
concept of nested quantum walks recently proposed by Jeffery, Kothari and
Magniez [SODA'13], and extends the methodology designed by Lee, Magniez and
Santha [SODA'13] for similar problems over graphs. As applications, we obtain a
quantum algorithm for finding a $4$-clique in a $3$-uniform hypergraph on $n$
vertices with query complexity $O(n^{1.883})$, and a quantum algorithm for
determining if a ternary operator over a set of size $n$ is associative with
query complexity $O(n^{2.113})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4128</identifier>
 <datestamp>2013-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4128</id><created>2013-10-15</created><authors><author><keyname>Adrovic</keyname><forenames>Danko</forenames></author><author><keyname>Verschelde</keyname><forenames>Jan</forenames></author></authors><title>A Polyhedral Method to Compute All Affine Solution Sets of Sparse
  Polynomial Systems</title><categories>cs.SC math.AG math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To compute solutions of sparse polynomial systems efficiently we have to
exploit the structure of their Newton polytopes. While the application of
polyhedral methods naturally excludes solutions with zero components, an
irreducible decomposition of a variety is typically understood in affine space,
including also those components with zero coordinates. We present a polyhedral
method to compute all affine solution sets of a polynomial system. The method
enumerates all factors contributing to a generalized permanent. Toric solution
sets are recovered as a special case of this enumeration. For sparse systems as
adjacent 2-by-2 minors our methods scale much better than the techniques from
numerical algebraic geometry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4136</identifier>
 <datestamp>2013-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4136</id><created>2013-10-15</created><authors><author><keyname>Teixeira</keyname><forenames>Thiago S. F. X.</forenames></author><author><keyname>Teodoro</keyname><forenames>George</forenames></author><author><keyname>Valle</keyname><forenames>Eduardo</forenames></author><author><keyname>Saltz</keyname><forenames>Joel H.</forenames></author></authors><title>Scalable Locality-Sensitive Hashing for Similarity Search in
  High-Dimensional, Large-Scale Multimedia Datasets</title><categories>cs.DC cs.DB cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Similarity search is critical for many database applications, including the
increasingly popular online services for Content-Based Multimedia Retrieval
(CBMR). These services, which include image search engines, must handle an
overwhelming volume of data, while keeping low response times. Thus,
scalability is imperative for similarity search in Web-scale applications, but
most existing methods are sequential and target shared-memory machines. Here we
address these issues with a distributed, efficient, and scalable index based on
Locality-Sensitive Hashing (LSH). LSH is one of the most efficient and popular
techniques for similarity search, but its poor referential locality properties
has made its implementation a challenging problem. Our solution is based on a
widely asynchronous dataflow parallelization with a number of optimizations
that include a hierarchical parallelization to decouple indexing and data
storage, locality-aware data partition strategies to reduce message passing,
and multi-probing to limit memory usage. The proposed parallelization attained
an efficiency of 90% in a distributed system with about 800 CPU cores. In
particular, the original locality-aware data partition reduced the number of
messages exchanged in 30%. Our parallel LSH was evaluated using the largest
public dataset for similarity search (to the best of our knowledge) with $10^9$
128-d SIFT descriptors extracted from Web images. This is two orders of
magnitude larger than datasets that previous LSH parallelizations could handle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4141</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4141</id><created>2013-10-15</created><updated>2014-04-29</updated><authors><author><keyname>Marenco</keyname><forenames>Javier</forenames></author><author><keyname>Mydlarz</keyname><forenames>Marcelo</forenames></author><author><keyname>Severin</keyname><forenames>Daniel</forenames></author></authors><title>Topological Additive Numbering of Directed Acyclic Graphs</title><categories>cs.CC cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose to study a problem that arises naturally from both Topological
Numbering of Directed Acyclic Graphs, and Additive Coloring (also known as
Lucky Labeling). Let $D$ be a digraph and $f$ a labeling of its vertices with
positive integers; denote by $S(v)$ the sum of labels over all neighbors of
each vertex $v$. The labeling $f$ is called \emph{topological additive
numbering} if $S(u) &lt; S(v)$ for each arc $(u,v)$ of the digraph. The problem
asks to find the minimum number $k$ for which $D$ has a topological additive
numbering with labels belonging to $\{ 1, \ldots, k \}$, denoted by
$\eta_t(D)$.
  We characterize when a digraph has topological additive numberings, give a
lower bound for $\eta_t(D)$, and provide an integer programming formulation for
our problem, characterizing when its coefficient matrix is totally unimodular.
We also present some families for which $\eta_t(D)$ can be computed in
polynomial time. Finally, we prove that this problem is \np-Hard even when its
input is restricted to planar bipartite digraphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4149</identifier>
 <datestamp>2013-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4149</id><created>2013-10-15</created><authors><author><keyname>Alvarado</keyname><forenames>Alex</forenames></author><author><keyname>Agrell</keyname><forenames>Erik</forenames></author></authors><title>Achievable Rates for Four-Dimensional Coded Modulation with a Bit-Wise
  Receiver</title><categories>cs.IT math.IT physics.optics</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study achievable rates for four-dimensional (4D) constellations for
spectrally efficient optical systems based on a (suboptimal) bit-wise receiver.
We show that PM-QPSK outperforms the best 4D constellation designed for uncoded
transmission by approximately 1 dB. Numerical results using LDPC codes validate
the analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4150</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4150</id><created>2013-10-15</created><authors><author><keyname>Kliuchnikov</keyname><forenames>Vadym</forenames></author><author><keyname>Bocharov</keyname><forenames>Alex</forenames></author><author><keyname>Svore</keyname><forenames>Krysta M.</forenames></author></authors><title>Asymptotically Optimal Topological Quantum Compiling</title><categories>quant-ph cs.ET</categories><comments>24 pages</comments><journal-ref>Phys. Rev. Lett. 112, 140504 (2014)</journal-ref><doi>10.1103/PhysRevLett.112.140504</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a topological quantum computer, universality is achieved by braiding and
quantum information is natively protected from small local errors. We address
the problem of compiling single-qubit quantum operations into braid
representations for non-abelian quasiparticles described by the Fibonacci anyon
model. We develop a probabilistically polynomial algorithm that outputs a braid
pattern to approximate a given single-qubit unitary to a desired precision. We
also classify the single-qubit unitaries that can be implemented exactly by a
Fibonacci anyon braid pattern and present an efficient algorithm to produce
their braid patterns. Our techniques produce braid patterns that meet the
uniform asymptotic lower bound on the compiled circuit depth and thus are
depth-optimal asymptotically. Our compiled circuits are significantly shorter
than those output by prior state-of-the-art methods, resulting in improvements
in depth by factors ranging from 20 to 1000 for precisions ranging between
$10^{-10}$ and $10^{-30}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4156</identifier>
 <datestamp>2013-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4156</id><created>2013-10-15</created><authors><author><keyname>Sun</keyname><forenames>Hong</forenames></author><author><keyname>De Roo</keyname><forenames>Jos</forenames></author><author><keyname>Twagirumukiza</keyname><forenames>Marc</forenames></author><author><keyname>Mels</keyname><forenames>Giovanni</forenames></author><author><keyname>Depraetere</keyname><forenames>Kristof</forenames></author><author><keyname>De Vloed</keyname><forenames>Boris</forenames></author><author><keyname>Colaert</keyname><forenames>Dirk</forenames></author></authors><title>Validation Rules for Assessing and Improving SKOS Mapping Quality</title><categories>cs.AI cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Simple Knowledge Organization System (SKOS) is popular for expressing
controlled vocabularies, such as taxonomies, classifications, etc., for their
use in Semantic Web applications. Using SKOS, concepts can be linked to other
concepts and organized into hierarchies inside a single terminology system.
Meanwhile, expressing mappings between concepts in different terminology
systems is also possible. This paper discusses potential quality issues in
using SKOS to express these terminology mappings. Problematic patterns are
defined and corresponding rules are developed to automatically detect
situations where the mappings either result in 'SKOS Vocabulary Hijacking' to
the source vocabularies or cause conflicts. An example of using the rules to
validate sample mappings between two clinical terminologies is given. The
validation rules, expressed in N3 format, are available as open source.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4162</identifier>
 <datestamp>2014-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4162</id><created>2013-10-15</created><updated>2014-02-21</updated><authors><author><keyname>Goonewardena</keyname><forenames>Mathew</forenames></author><author><keyname>Jin</keyname><forenames>Xin</forenames></author><author><keyname>Ajib</keyname><forenames>Wessam</forenames></author><author><keyname>Elbiaze</keyname><forenames>Halima</forenames></author></authors><title>Competition vs. Cooperation: A Game-Theoretic Decision Analysis for MIMO
  HetNets</title><categories>cs.GT cs.IT cs.NI math.IT</categories><comments>This paper has been accepted to be presented at IEEE ICC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of competition vs. cooperation in the
downlink, between base stations (BSs), of a multiple input multiple output
(MIMO) interference, heterogeneous wireless network (HetNet). This research
presents a scenario where a macrocell base station (MBS) and a cochannel
femtocell base station (FBS) each simultaneously serving their own user
equipment (UE), has to choose to act as individual systems or to cooperate in
coordinated multipoint transmission (CoMP). The paper employes both the
theories of non-cooperative and cooperative games in a unified procedure to
analyze the decision making process. The BSs of the competing system are
assumed to operate at the\emph{}maximum expected sum
rate\emph{}(MESR)\emph{}correlated equilibrium\emph{}(CE), which is compared
against the value of CoMP to establish the stability of the coalition. It is
proven that there exists a threshold geographical separation, $d_{\text{th}}$,
between the macrocell user equipment (MUE) and FBS, under which the region of
coordination is non-empty. Theoretical results are verified through
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4166</identifier>
 <datestamp>2013-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4166</id><created>2013-10-15</created><authors><author><keyname>Jiang</keyname><forenames>Luo-Luo</forenames></author><author><keyname>Perc</keyname><forenames>Matjaz</forenames></author></authors><title>Spreading of cooperative behaviour across interdependent groups</title><categories>physics.soc-ph cs.SI q-bio.PE</categories><comments>7 two-column pages, 5 figures; accepted for publication in Scientific
  Reports [related work available at http://arxiv.org/abs/1308.4969]</comments><journal-ref>Sci. Rep. 3 (2013) 2483</journal-ref><doi>10.1038/srep02483</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent empirical research has shown that links between groups reinforce
individuals within groups to adopt cooperative behaviour. Moreover, links
between networks may induce cascading failures, competitive percolation, or
contribute to efficient transportation. Here we show that there in fact exists
an intermediate fraction of links between groups that is optimal for the
evolution of cooperation in the prisoner's dilemma game. We consider individual
groups with regular, random, and scale-free topology, and study their different
combinations to reveal that an intermediate interdependence optimally
facilitates the spreading of cooperative behaviour between groups. Excessive
between-group links simply unify the two groups and make them act as one, while
too rare between-group links preclude a useful information flow between the two
groups. Interestingly, we find that between-group links are more likely to
connect two cooperators than in-group links, thus supporting the conclusion
that they are of paramount importance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4168</identifier>
 <datestamp>2013-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4168</id><created>2013-10-12</created><authors><author><keyname>Murali</keyname><forenames>Vidya N.</forenames></author><author><keyname>Threatt</keyname><forenames>Anthony L.</forenames></author><author><keyname>Manganelli</keyname><forenames>Joe</forenames></author><author><keyname>Yanik</keyname><forenames>Paul M.</forenames></author><author><keyname>Mohan</keyname><forenames>Sumod K.</forenames></author><author><keyname>Apte</keyname><forenames>Akshay A.</forenames></author><author><keyname>Ramachandran</keyname><forenames>Raghavendran</forenames></author><author><keyname>Smolentzov</keyname><forenames>Linnea</forenames></author><author><keyname>Brooks</keyname><forenames>Johnell</forenames></author><author><keyname>Walker</keyname><forenames>Ian D.</forenames></author><author><keyname>Green</keyname><forenames>Keith E.</forenames></author></authors><title>A Mobile Robotic Personal Nightstand with Integrated Perceptual
  Processes</title><categories>cs.RO</categories><comments>Submitted to AAAI 2010, IROS 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an intelligent interactive nightstand mounted on a mobile robot,
to aid the elderly in their homes using physical, tactile and visual percepts.
We show the integration of three different sensing modalities for controlling
the navigation of a robot mounted nightstand within the constrained environment
of a general purpose living room housing a single aging individual in need of
assistance and monitoring. A camera mounted on the ceiling of the room, gives a
top-down view of the obstacles, the person and the nightstand. Pressure sensors
mounted beneath the bed-stand of the individual provide physical perception of
the person's state. A proximity IR sensor on the nightstand acts as a tactile
interface along with a Wii Nunchuck (Nintendo) to control mundane operations on
the nightstand. Intelligence from these three modalities are combined to enable
path planning for the nightstand to approach the individual. With growing
emphasis on assistive technology for the aging individuals who are increasingly
electing to stay in their homes, we show how ubiquitous intelligence can be
brought inside homes to help monitor and provide care to an individual. Our
approach goes one step towards achieving pervasive intelligence by seamlessly
integrating different sensors embedded in the fabric of the environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4169</identifier>
 <datestamp>2014-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4169</id><created>2013-10-12</created><updated>2014-08-21</updated><authors><author><keyname>Gao</keyname><forenames>Yuan</forenames></author><author><keyname>Chen</keyname><forenames>Guanrong</forenames></author><author><keyname>Chan</keyname><forenames>Rosa H. M.</forenames></author></authors><title>Naming Game on Networks: Let Everyone be Both Speaker and Hearer</title><categories>cs.SI physics.soc-ph</categories><comments>11 pages, 6 figures</comments><journal-ref>Yuan Gao, Guanrong Chen, and Rosa H. M. Chan. Naming Game on
  Networks: Let Everyone be Both Speaker and Hearer. Sci. Rep. 4, 6149, (2014)</journal-ref><doi>10.1038/srep06149</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To investigate how consensus is reached on a large self-organized
peer-to-peer network, we extended the naming game model commonly used in
language and communication to Naming Game in Groups (NGG). Differing from other
existing naming game models, in NGG, everyone in the population (network) can
be both speaker and hearer simultaneously, which resembles in a closer manner
to real-life scenarios. Moreover, NGG allows the transmission (communication)
of multiple words (opinions) for multiple intra-group consensuses. The
communications among indirectly-connected nodes are also enabled in NGG. We
simulated and analyzed the consensus process in some typical network
topologies, including random-graph networks, small-world networks and
scale-free networks, to better understand how global convergence (consensus)
could be reached on one common word. The results are interpreted on group
negotiation of a peer-to-peer network, which shows that global consensus in the
population can be reached more rapidly when more opinions are permitted within
each group or when the negotiating groups in the population are larger in size.
The novel features and properties introduced by our model have demonstrated its
applicability in better investigating general consensus problems on
peer-to-peer networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4188</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4188</id><created>2013-10-15</created><updated>2014-11-21</updated><authors><author><keyname>Davison</keyname><forenames>P.</forenames></author><author><keyname>Leonard</keyname><forenames>N. E.</forenames></author><author><keyname>Olshevsky</keyname><forenames>A.</forenames></author><author><keyname>Schwemmer</keyname><forenames>M.</forenames></author></authors><title>Nonuniform Line Coverage from Noisy Scalar Measurements</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of distributed coverage control in a network of mobile
agents arranged on a line. The goal is to design distributed dynamics for the
agents to achieve optimal coverage positions with respect to a scalar density
field that measures the relative importance of each point on the line. Unlike
previous work, which has implicitly assumed the agents know this density field,
we only assume that each agent can access noisy samples of the field at points
close to its current location. We provide a simple randomized protocol wherein
every agent samples the scalar field at three nearby points at each step and
which guarantees convergence to the optimal positions. We further analyze the
convergence time of this protocol and show that, under suitable assumptions,
the squared distance to the optimal coverage configuration decays as $O(1/t)$
with the number of iterations $t$, where the constant scales polynomially with
the number of agents $n$. We illustrate these results with simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4201</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4201</id><created>2013-10-15</created><authors><author><keyname>Zhang</keyname><forenames>Hantian</forenames></author><author><keyname>Chang</keyname><forenames>Dong Eui</forenames></author><author><keyname>Cao</keyname><forenames>Qingjie</forenames></author></authors><title>Lyapunov-based Low-thrust Optimal Orbit Transfer: An approach in
  Cartesian coordinates</title><categories>math.OC cs.CE physics.class-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a simple approach to low-thrust optimal-fuel and
optimal-time transfer problems between two elliptic orbits using the Cartesian
coordinates system. In this case, an orbit is described by its specific angular
momentum and Laplace vectors with a free injection point. Trajectory
optimization with the pseudospectral method and nonlinear programming are
supported by the initial guess generated from the Chang-Chichka-Marsden
Lyapunov-based transfer controller. This approach successfully solves several
low-thrust optimal problems. Numerical results show that the Lyapunov-based
initial guess overcomes the difficulty in optimization caused by the strong
oscillation of variables in the Cartesian coordinates system. Furthermore, a
comparison of the results shows that obtaining the optimal transfer solution
through the polynomial approximation by utilizing Cartesian coordinates is
easier than using orbital elements, which normally produce strongly nonlinear
equations of motion. In this paper, the Earth's oblateness and shadow effect
are not taken into account.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4210</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4210</id><created>2013-10-15</created><updated>2014-02-05</updated><authors><author><keyname>Steeg</keyname><forenames>Greg Ver</forenames></author><author><keyname>Galstyan</keyname><forenames>Aram</forenames></author><author><keyname>Sha</keyname><forenames>Fei</forenames></author><author><keyname>DeDeo</keyname><forenames>Simon</forenames></author></authors><title>Demystifying Information-Theoretic Clustering</title><categories>cs.LG cs.IT math.IT physics.data-an stat.ML</categories><comments>Proceedings of The 31st International Conference on Machine Learning
  (ICML), 2014. 11 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel method for clustering data which is grounded in
information-theoretic principles and requires no parametric assumptions.
Previous attempts to use information theory to define clusters in an
assumption-free way are based on maximizing mutual information between data and
cluster labels. We demonstrate that this intuition suffers from a fundamental
conceptual flaw that causes clustering performance to deteriorate as the amount
of data increases. Instead, we return to the axiomatic foundations of
information theory to define a meaningful clustering measure based on the
notion of consistency under coarse-graining for finite data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4216</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4216</id><created>2013-10-15</created><updated>2013-11-05</updated><authors><author><keyname>Fachkha</keyname><forenames>Claude</forenames></author><author><keyname>Bou-Harb</keyname><forenames>Elias</forenames></author><author><keyname>Debbabi</keyname><forenames>Mourad</forenames></author></authors><title>Fingerprinting Internet DNS Amplification DDoS Activities</title><categories>cs.CR</categories><comments>5 pages, 2 figures</comments><doi>10.1109/NTMS.2014.6814019</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work proposes a novel approach to infer and characterize Internet-scale
DNS amplification DDoS attacks by leveraging the darknet space. Complementary
to the pioneer work on inferring Distributed Denial of Service (DDoS)
activities using darknet, this work shows that we can extract DDoS activities
without relying on backscattered analysis. The aim of this work is to extract
cyber security intelligence related to DNS Amplification DDoS activities such
as detection period, attack duration, intensity, packet size, rate and
geo-location in addition to various network-layer and flow-based insights. To
achieve this task, the proposed approach exploits certain DDoS parameters to
detect the attacks. We empirically evaluate the proposed approach using 720 GB
of real darknet data collected from a /13 address space during a recent three
months period. Our analysis reveals that the approach was successful in
inferring significant DNS amplification DDoS activities including the recent
prominent attack that targeted one of the largest anti-spam organizations.
Moreover, the analysis disclosed the mechanism of such DNS amplification DDoS
attacks. Further, the results uncover high-speed and stealthy attempts that
were never previously documented. The case study of the largest DDoS attack in
history lead to a better understanding of the nature and scale of this threat
and can generate inferences that could contribute in detecting, preventing,
assessing, mitigating and even attributing of DNS amplification DDoS
activities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4217</identifier>
 <datestamp>2013-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4217</id><created>2013-10-15</created><authors><author><keyname>Brunton</keyname><forenames>B. W.</forenames></author><author><keyname>Brunton</keyname><forenames>S. L.</forenames></author><author><keyname>Proctor</keyname><forenames>J. L.</forenames></author><author><keyname>Kutz</keyname><forenames>J. N.</forenames></author></authors><title>Optimal Sensor Placement and Enhanced Sparsity for Classification</title><categories>cs.CV</categories><comments>13 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of compressive sensing is efficient reconstruction of data from few
measurements, sometimes leading to a categorical decision. If only
classification is required, reconstruction can be circumvented and the
measurements needed are orders-of-magnitude sparser still. We define enhanced
sparsity as the reduction in number of measurements required for classification
over reconstruction. In this work, we exploit enhanced sparsity and learn
spatial sensor locations that optimally inform a categorical decision. The
algorithm solves an l1-minimization to find the fewest entries of the full
measurement vector that exactly reconstruct the discriminant vector in feature
space. Once the sensor locations have been identified from the training data,
subsequent test samples are classified with remarkable efficiency, achieving
performance comparable to that obtained by discrimination using the full image.
Sensor locations may be learned from full images, or from a random subsample of
pixels. For classification between more than two categories, we introduce a
coupling parameter whose value tunes the number of sensors selected, trading
accuracy for economy. We demonstrate the algorithm on example datasets from
image recognition using PCA for feature extraction and LDA for discrimination;
however, the method can be broadly applied to non-image data and adapted to
work with other methods for feature extraction and discrimination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4218</identifier>
 <datestamp>2013-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4218</id><created>2013-10-15</created><authors><author><keyname>Fazenda</keyname><forenames>Alvaro Luiz</forenames></author><author><keyname>Mendes</keyname><forenames>Celso L.</forenames></author><author><keyname>Kale</keyname><forenames>Laxmikant V.</forenames></author><author><keyname>Panetta</keyname><forenames>Jairo</forenames></author><author><keyname>Rodrigues</keyname><forenames>Eduardo Rocha</forenames></author></authors><title>Dynamic Load Balancing in GPU-Based Systems - Early Experiments</title><categories>cs.DC</categories><acm-class>D.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dynamic load-balancing framework in Charm++/AMPI, developed at the
University of Illinois, is based on using processor virtualization to allow
thread migration across processors. This framework has been successfully
applied to many scientific applications in the past, such as BRAMS, NAMD,
ChaNGa, and others. Most of these applications use only CPUs to perform their
operations. However, the use of GPUs to improve computational performance is
quickly getting massively disseminated in the high-performance computing
community. This paper aims to investigate how the same Charm++/AMPI framework
can be extended to balance load in a synthetic application inspired by the
BRAMS numerical forecast model, running mostly on GPUs rather than on CPUs.
Many major questions involving the use of GPUs with AMPI where handled in this
work, including: how to measure the GPU's load, how to use and share GPUs among
user-level threads, and what results are obtained when applying the mandatory
over-decomposition technique to a GPU-accelerated program.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4223</identifier>
 <datestamp>2013-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4223</id><created>2013-10-15</created><authors><author><keyname>Chitsaz</keyname><forenames>Hamidreza</forenames></author><author><keyname>Aminisharifabad</keyname><forenames>Mohammad</forenames></author></authors><title>Exact Learning of RNA Energy Parameters From Structure</title><categories>q-bio.BM cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of exact learning of parameters of a linear RNA
energy model from secondary structure data. A necessary and sufficient
condition for learnability of parameters is derived, which is based on
computing the convex hull of union of translated Newton polytopes of input
sequences. The set of learned energy parameters is characterized as the convex
cone generated by the normal vectors to those facets of the resulting polytope
that are incident to the origin. In practice, the sufficient condition may not
be satisfied by the entire training data set; hence, computing a maximal subset
of training data for which the sufficient condition is satisfied is often
desired. We show that problem is NP-hard in general for an arbitrary
dimensional feature space. Using a randomized greedy algorithm, we select a
subset of RNA STRAND v2.0 database that satisfies the sufficient condition for
separate A-U, C-G, G-U base pair counting model. The set of learned energy
parameters includes experimentally measured energies of A-U, C-G, and G-U
pairs; hence, our parameter set is in agreement with the Turner parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4227</identifier>
 <datestamp>2013-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4227</id><created>2013-10-15</created><authors><author><keyname>Orabona</keyname><forenames>Francesco</forenames></author><author><keyname>Hazan</keyname><forenames>Tamir</forenames></author><author><keyname>Sarwate</keyname><forenames>Anand D.</forenames></author><author><keyname>Jaakkola</keyname><forenames>Tommi</forenames></author></authors><title>On Measure Concentration of Random Maximum A-Posteriori Perturbations</title><categories>cs.LG math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The maximum a-posteriori (MAP) perturbation framework has emerged as a useful
approach for inference and learning in high dimensional complex models. By
maximizing a randomly perturbed potential function, MAP perturbations generate
unbiased samples from the Gibbs distribution. Unfortunately, the computational
cost of generating so many high-dimensional random variables can be
prohibitive. More efficient algorithms use sequential sampling strategies based
on the expected value of low dimensional MAP perturbations. This paper develops
new measure concentration inequalities that bound the number of samples needed
to estimate such expected values. Applying the general result to MAP
perturbations can yield a more efficient algorithm to approximate sampling from
the Gibbs distribution. The measure concentration result is of general interest
and may be applicable to other areas involving expected estimations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4231</identifier>
 <datestamp>2013-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4231</id><created>2013-10-15</created><authors><author><keyname>Mittal</keyname><forenames>Sparsh</forenames></author></authors><title>Dynamic cache reconfiguration based techniques for improving cache
  energy efficiency</title><categories>cs.AR</categories><comments>PhD thesis, dynamic cache reconfiguration</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern multicore processors are employing large last-level caches, for
example Intel's E7-8800 processor uses 24MB L3 cache. Further, with each CMOS
technology generation, leakage energy has been dramatically increasing and
hence, leakage energy is expected to become a major source of energy
dissipation, especially in last-level caches (LLCs). The conventional schemes
of cache energy saving either aim at saving dynamic energy or are based on
properties specific to first-level caches, and thus these schemes have limited
utility for last-level caches. Further, several other techniques require
offline profiling or per-application tuning and hence are not suitable for
product systems. In this research, we propose novel cache leakage energy saving
schemes for single-core and multicore systems; desktop, QoS, real-time and
server systems. We propose software-controlled, hardware-assisted techniques
which use dynamic cache reconfiguration to configure the cache to the most
energy efficient configuration while keeping the performance loss bounded. To
profile and test a large number of potential configurations, we utilize
low-overhead, micro-architecture components, which can be easily integrated
into modern processor chips. We adopt a system-wide approach to save energy to
ensure that cache reconfiguration does not increase energy consumption of other
components of the processor. We have compared our techniques with the
state-of-art techniques and have found that our techniques outperform them in
their energy efficiency. This research has important applications in improving
energy-efficiency of higher-end embedded, desktop, server processors and
multitasking systems. We have also proposed performance estimation approach for
efficient design space exploration and have implemented time-sampling based
simulation acceleration approach for full-system architectural simulators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4249</identifier>
 <datestamp>2014-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4249</id><created>2013-10-15</created><updated>2014-08-11</updated><authors><author><keyname>Berman</keyname><forenames>Gordon J.</forenames></author><author><keyname>Choi</keyname><forenames>Daniel M.</forenames></author><author><keyname>Bialek</keyname><forenames>William</forenames></author><author><keyname>Shaevitz</keyname><forenames>Joshua W.</forenames></author></authors><title>Mapping the stereotyped behaviour of freely-moving fruit flies</title><categories>q-bio.QM cs.CV physics.bio-ph stat.ML</categories><comments>21 pages, 17 figures. Email GJB (gberman@princeton.edu) to see
  supplementary movies, Journal of the Royal Society Interface, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most animals possess the ability to actuate a vast diversity of movements,
ostensibly constrained only by morphology and physics. In practice, however, a
frequent assumption in behavioral science is that most of an animal's
activities can be described in terms of a small set of stereotyped motifs. Here
we introduce a method for mapping the behavioral space of organisms, relying
only upon the underlying structure of postural movement data to organize and
classify behaviors. We find that six different drosophilid species each perform
a mix of non-stereotyped actions and over one hundred hierarchically-organized,
stereotyped behaviors. Moreover, we use this approach to compare these species'
behavioral spaces, systematically identifying subtle behavioral differences
between closely-related species.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4252</identifier>
 <datestamp>2013-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4252</id><created>2013-10-15</created><authors><author><keyname>Xie</keyname><forenames>Sihong</forenames></author><author><keyname>Kong</keyname><forenames>Xiangnan</forenames></author><author><keyname>Gao</keyname><forenames>Jing</forenames></author><author><keyname>Fan</keyname><forenames>Wei</forenames></author><author><keyname>Yu</keyname><forenames>Philip S.</forenames></author></authors><title>Multilabel Consensus Classification</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the era of big data, a large amount of noisy and incomplete data can be
collected from multiple sources for prediction tasks. Combining multiple models
or data sources helps to counteract the effects of low data quality and the
bias of any single model or data source, and thus can improve the robustness
and the performance of predictive models. Out of privacy, storage and bandwidth
considerations, in certain circumstances one has to combine the predictions
from multiple models or data sources to obtain the final predictions without
accessing the raw data. Consensus-based prediction combination algorithms are
effective for such situations. However, current research on prediction
combination focuses on the single label setting, where an instance can have one
and only one label. Nonetheless, data nowadays are usually multilabeled, such
that more than one label have to be predicted at the same time. Direct
applications of existing prediction combination methods to multilabel settings
can lead to degenerated performance. In this paper, we address the challenges
of combining predictions from multiple multilabel classifiers and propose two
novel algorithms, MLCM-r (MultiLabel Consensus Maximization for ranking) and
MLCM-a (MLCM for microAUC). These algorithms can capture label correlations
that are common in multilabel classifications, and optimize corresponding
performance metrics. Experimental results on popular multilabel classification
tasks verify the theoretical analysis and effectiveness of the proposed
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4261</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4261</id><created>2013-10-16</created><updated>2014-06-19</updated><authors><author><keyname>Guo</keyname><forenames>Han</forenames></author><author><keyname>Qiu</keyname><forenames>Chenlu</forenames></author><author><keyname>Vaswani</keyname><forenames>Namrata</forenames></author></authors><title>An Online Algorithm for Separating Sparse and Low-dimensional Signal
  Sequences from their Sum</title><categories>cs.IT math.IT</categories><comments>Accepted by IEEE Transactions on Signal Processing. The title has
  been changed from &quot;Practical ReProCS for Separating Sparse and
  Low-dimensional Signal Sequences from their Sum&quot; to &quot;An Online Algorithm for
  Separating Sparse and Low-dimensional Signal Sequences from their Sum&quot;</comments><doi>10.1109/TSP.2014.2331612</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper designs and evaluates a practical algorithm, called practical
recursive projected compressive sensing (Prac-ReProCS), for recovering a time
sequence of sparse vectors $S_t$ and a time sequence of dense vectors $L_t$
from their sum, $M_t:= S_t + L_t$, when any subsequence of the $L_t$'s lies in
a slowly changing low-dimensional subspace. A key application where this
problem occurs is in video layering where the goal is to separate a video
sequence into a slowly changing background sequence and a sparse foreground
sequence that consists of one or more moving regions/objects. Prac-ReProCS is a
practical modification of its theoretical counterpart which was analyzed in our
recent work. Experimental comparisons demonstrating the advantage of the
approach for both simulated and real videos are shown. Extension to the
undersampled case is also developed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4270</identifier>
 <datestamp>2013-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4270</id><created>2013-10-16</created><authors><author><keyname>Rana</keyname><forenames>Rajib</forenames></author><author><keyname>Chou</keyname><forenames>Chun Tung</forenames></author><author><keyname>Bulusu</keyname><forenames>Nirupama</forenames></author><author><keyname>Kanhere</keyname><forenames>Salil</forenames></author><author><keyname>Hu</keyname><forenames>Wen</forenames></author></authors><title>Ear-Phone: A Context-Aware Noise Mapping using Smart Phones</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A noise map facilitates the monitoring of environmental noise pollution in
urban areas. However, state-of-the-art techniques for rendering noise maps in
urban areas are expensive and rarely updated, as they rely on population and
traffic models rather than on real data. Smart phone based urban sensing can be
leveraged to create an open and inexpensive platform for rendering up-to- date
noise maps. In this paper, we present the design, implementation and
performance evaluation of an end-to-end, context-aware, noise mapping system
called Ear-Phone. Ear-Phone investigates the use of different interpolation and
regularization methods to address the fundamental problem of recovering the
noise map from incomplete and random samples obtained by crowdsourcing data
collection. Ear-Phone, implemented on Nokia N95, N97 and HP iPAQ, HTC One
mobile devices, also addresses the challenge of collecting accurate noise
pollution readings at a mobile device. A major challenge of using smart phones
as sensors is that even at the same location, the sensor reading may vary
depending on the phone orientation and user context (for example, whether the
user is carrying the phone in a bag or holding it in her palm). To address this
problem, Ear-Phone leverages context-aware sensing. We develop classifiers to
accurately determine the phone sensing context. Upon context discovery,
Ear-Phone automatically decides whether to sense or not. Ear-phone also
implements in-situ calibration which performs simple calibration that can be
carried out without any technical skills whatsoever required on the user's
part. Extensive simulations and outdoor experiments demonstrate that Ear-Phone
is a feasible platform to assess noise pollution, incurring reasonable system
resource consumption at mobile devices and providing high reconstruction
accuracy of the noise map.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4282</identifier>
 <datestamp>2013-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4282</id><created>2013-10-16</created><authors><author><keyname>Tang</keyname><forenames>Wenyuan</forenames></author><author><keyname>Jain</keyname><forenames>Rahul</forenames></author></authors><title>A Nash Equilibrium Need Not Exist in the Locational Marginal Pricing
  Mechanism</title><categories>cs.GT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Locational marginal pricing (LMP) is a widely employed method for pricing
electricity in the wholesale electricity market. Although it is well known that
the LMP mechanism is vulnerable to market manipulation, there is little
literature providing a systematic analysis of this phenomenon. In the first
part of this paper, we investigate the economic dispatch outcomes of the LMP
mechanism with strategic agents. We show via counterexamples, that contrary to
popular belief, a Nash equilibrium may not exist. And when it exists, the price
of anarchy may be arbitrarily large. We then provide two sufficient conditions
under either of which an efficient Nash equilibria exists. Last, we propose a
new market mechanism for electricity markets, the Power Network Second Price
(PNSP) mechanism that always induces an efficient Nash equilibrium. We briefly
address the extensions on the demand side.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4283</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4283</id><created>2013-10-16</created><updated>2014-06-13</updated><authors><author><keyname>Spiwack</keyname><forenames>Arnaud</forenames></author></authors><title>Abstract interpretation as anti-refinement</title><categories>cs.PL</categories><comments>Working paper</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This article shows a correspondence between abstract interpretation of
imperative programs and the refinement calculus: in the refinement calculus, an
abstract interpretation of a program is a specification which is a function.
  This correspondence can be used to guide the design of mechanically verified
static analyses, keeping the correctness proof well separated from the
heuristic parts of the algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4284</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4284</id><created>2013-10-16</created><updated>2014-04-15</updated><authors><author><keyname>Rana</keyname><forenames>Rajib</forenames></author><author><keyname>Hu</keyname><forenames>Wen</forenames></author><author><keyname>Chou</keyname><forenames>Chun Tung</forenames></author></authors><title>Signal Reconstruction from Rechargeable Wireless Sensor Networks using
  Sparse Random Projections</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to non-homogeneous spread of sunlight, sensing nodes possess non-uniform
energy budget in recharge- able Wireless Sensor Networks (WSNs). An
energy-aware workload distribution strategy is therefore nec- essary to achieve
good data accuracy subject to energy-neutral operation. Recently proposed
signal approx- imation strategies assume uniform sampling and fail to ensure
energy neutral operation in rechargeable wireless sensor networks. We propose
EAST (Energy Aware Sparse approximation Technique), which ap- proximates a
signal, by adapting sensor node sampling workload according to solar energy
availability. To the best of our knowledge, we are the first to propose sparse
approximation to model energy-aware workload distribution in rechargeable WSNs.
Experimental results, using data from an outdoor WSN deployment suggest that
EAST significantly improves the approximation accuracy offering approximately
50% higher sensor on-time. EAST requires the approximation error to be known
beforehand to determine the number of measure- ments. However, it is not always
possible to decide the accuracy a-priori. We improve EAST and propose EAST+,
which, given only the energy budget of the nodes, computes the optimal number
of measurements subject to the energy neutral operation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4288</identifier>
 <datestamp>2013-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4288</id><created>2013-10-16</created><authors><author><keyname>Tiruneh</keyname><forenames>Ababu Teklemariam</forenames></author></authors><title>Higher Order Aitken Extrapolation with Application to Converging and
  Diverging Gauss-Seidel Iterations</title><categories>math.NA cs.NA</categories><comments>32 pages ; 8 figures ; 9 tables</comments><msc-class>65Bxx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Aitken extrapolation normally applied to convergent fixed point iteration is
extended to extrapolate the solution of a divergent iteration. In addition,
higher order Aitken extrapolation is introduced that enables successive
decomposition of high Eigen values of the iteration matrix to enable
convergence. While extrapolation of a convergent fixed point iteration using a
geometric series sum is a known form of Aitken acceleration, it is shown in
this paper that the same formula can be used to estimate the solution of sets
of linear equations from diverging Gauss Seidel iterations. In both convergent
and divergent iterations, the ratios of differences among the consecutive
values of iteration eventually form a convergent or divergent series with a
factor equal to the largest Eigen value of the iteration matrix. Higher order
Aitken extrapolation is shown to eliminate the influence of dominant Eigen
values of the iteration matrix in successive order until the iteration is
determined by the lowest possible Eigen value. For the convergent part of the
Gauss Seidel iteration, further acceleration is made possible by coupling of
the extrapolation technique with the successive over relaxation method.
Application examples from both convergent and divergent iterations have been
provided. Coupling of the extrapolation with the successive over relaxation
technique is also illustrated for a steady state two dimensional heat flow
problem which was solved using MATLAB programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4290</identifier>
 <datestamp>2013-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4290</id><created>2013-10-16</created><authors><author><keyname>Rusu</keyname><forenames>Irena</forenames></author></authors><title>Extending Common Intervals Searching from Permutations to Sequences</title><categories>cs.DS</categories><comments>22 pages</comments><acm-class>F.2.3; G.2.1; I.5.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Common intervals have been defined as a modelisation of gene clusters in
genomes represented either as permutations or as sequences. Whereas optimal
algorithms for finding common intervals in permutations exist even for an
arbitrary number of permutations, in sequences no optimal algorithm has been
proposed yet even for only two sequences. Surprisingly enough, when sequences
are reduced to permutations, the existing algorithms perform far from the
optimum, showing that their performances are not dependent, as they should be,
on the structural complexity of the input sequences.
  In this paper, we propose to characterize the structure of a sequence by the
number $q$ of different dominating orders composing it (called the domination
number), and to use a recent algorithm for permutations in order to devise a
new algorithm for two sequences. Its running time is in
$O(q_1q_2p+q_1n_1+q_2n_2+N)$, where $n_1, n_2$ are the sizes of the two
sequences, $q_1,q_2$ are their respective domination numbers, $p$ is the
alphabet size and $N$ is the number of solutions to output. This algorithm
performs better as $q_1$ and/or $q_2$ reduce, and when the two sequences are
reduced to permutations (i.e. when $q_1=q_2=1$) it has the same running time as
the best algorithms for permutations. It is also the first algorithm for
sequences whose running time involves the parameter size of the solution. As a
counterpart, when $q_1$ and $q_2$ are of $O(n_1)$ and $O(n_2)$ respectively,
the algorithm is less efficient than other approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4291</identifier>
 <datestamp>2013-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4291</id><created>2013-10-16</created><authors><author><keyname>Singh</keyname><forenames>Ashutosh</forenames></author><author><keyname>Singh</keyname><forenames>Yatindra Nath</forenames></author></authors><title>Approaches toward Maintaining Bi-connectivity for Resilience in Overlaid
  Multicasting</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Application layer multicast (ALM) (also called Overlay Multicast),
multicast-related functionalities are moved to end-hosts. The key advantages,
overlays offers, are flexibility, adaptability and ease of deployment [1].
Application layer multicast builds a peer-to-peer (P2P) overlay topology
consisting of end-to-end unicast connections between end-hosts. End users self
organize themselves into logical overlay networks for efficient data delivery.
The major concern in designing ALM protocol is how to build and maintain a
topology, to route data efficiently and reliably. We propose here a two-fold
dynamic overlay tree construction and maintenance scheme in which a mesh-like
topology is first built, and on top of it, a single or multiple data delivery
tree(s) are built. While forming the mesh, it is ensured that two node disjoint
paths are maintained between every possible pair of nodes. The overlay tree is
built such that every host gets data feeds via two different paths. A fully
dynamic algorithm is run over the overlay topology to insert new links to
maintain biconnectivity while deleting the redundant links.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4301</identifier>
 <datestamp>2013-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4301</id><created>2013-10-16</created><authors><author><keyname>Chen</keyname><forenames>Xiaoming</forenames></author><author><keyname>Zhang</keyname><forenames>Zhaoyang</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author></authors><title>Adaptive Mode Selection in Multiuser MISO Cognitive Networks with
  Limited Cooperation and Feedback</title><categories>cs.IT math.IT</categories><comments>11 pages,6 figures, 4 tables IEEE Transactions on Vehicular
  Technology, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a multiuser MISO downlink cognitive network
coexisting with a primary network. With the purpose of exploiting the spatial
degree of freedom to counteract the inter-network interference and
intra-network (inter-user) interference simultaneously, we propose to perform
zero-forcing beamforming (ZFBF) at the multi-antenna cognitive base station
(BS) based on the instantaneous channel state information (CSI). The challenge
of designing ZFBF in cognitive networks lies in how to obtain the interference
CSI. To solve it, we introduce a limited inter-network cooperation protocol,
namely the quantized CSI conveyance from the primary receiver to the cognitive
BS via purchase. Clearly, the more the feedback amount, the better the
performance, but the higher the feedback cost. In order to achieve a balance
between the performance and feedback cost, we take the maximization of feedback
utility function, defined as the difference of average sum rate and feedback
cost while satisfying the interference constraint, as the optimization
objective, and derive the transmission mode and feedback amount joint
optimization scheme. Moreover, we quantitatively investigate the impact of CSI
feedback delay and obtain the corresponding optimization scheme. Furthermore,
through asymptotic analysis, we present some simple schemes. Finally, numerical
results confirm the effectiveness of our theoretical claims.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4303</identifier>
 <datestamp>2013-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4303</id><created>2013-10-16</created><authors><author><keyname>Gao</keyname><forenames>Zongsheng</forenames></author><author><keyname>Liu</keyname><forenames>Jun</forenames></author><author><keyname>Xu</keyname><forenames>Ke</forenames></author></authors><title>A novel weighting scheme for random $k$-SAT</title><categories>cs.DM</categories><comments>8 pages. arXiv admin note: text overlap with arXiv:cs/0305009 by
  other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a random $k$-CNF formula $F_{k}(n, rn)$ with $n$ variables and $rn$
clauses. For every truth assignment $\sigma\in \{0, 1\}^{n}$ and every clause
$c=\ell_{1}\vee\cdots\vee\ell_{k}$, let $d=d(\sigma, c)$ be the number of
satisfied literal occurrences in $c$ under $\sigma$. For fixed $\beta&gt;-1$ and
$\lambda&gt;0$, we take $\omega(\sigma, c)=0$, if $d=0$; $\omega(\sigma,
c)=\lambda(1+\beta)$, if $d=1$ and $\omega(\sigma, c)=\lambda^{d}$, if $d&gt;1$.
  Applying the above weighting scheme, we get that if $F_{k}(n, rn)$ is
unsatisfiable with probability tending to one as $n\rightarrow\infty$, then
$r\geq2.83, 8.09, 18.91, 40.81, 84.87$ for $k=3, 4, 5, 6$ and $7,$
respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4306</identifier>
 <datestamp>2013-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4306</id><created>2013-10-16</created><authors><author><keyname>Eberhart</keyname><forenames>Clovis</forenames><affiliation>LAMA</affiliation></author><author><keyname>Hirschowitz</keyname><forenames>Tom</forenames><affiliation>LAMA</affiliation></author><author><keyname>Seiller</keyname><forenames>Thomas</forenames><affiliation>LAMA, IRISA / INRIA Rennes</affiliation></author></authors><title>Fully-abstract concurrent games for pi</title><categories>cs.LO</categories><comments>20 pages, submitted</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a semantics for Milner's pi-calculus, with three main novelties.
First, it provides a fully-abstract model for fair testing equivalence, whereas
previous semantics covered variants of bisimilarity and the may and must
testing equivalences. Second, it is based on reduction semantics, whereas
previous semantics were based on labelled transition systems. Finally, it has a
strong game semantical flavor in the sense of Hyland-Ong and Nickau. Indeed,
our model may both be viewed as an innocent presheaf semantics and as a
concurrent game semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4310</identifier>
 <datestamp>2013-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4310</id><created>2013-10-16</created><authors><author><keyname>Chaudhuri</keyname><forenames>Kaustuv</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author><author><keyname>Despeyroux</keyname><forenames>Joelle</forenames></author></authors><title>A Hybrid Linear Logic for Constrained Transition Systems with
  Applications to Molecular Biology</title><categories>cs.LO</categories><proxy>ccsd</proxy><report-no>RR-402942</report-no><journal-ref>N&amp;deg; RR-402942 (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear implication can represent state transitions, but real transition
systems operate under temporal, stochastic or probabilistic constraints that
are not directly representable in ordinary linear logic. We propose a general
modal extension of intuitionistic linear logic where logical truth is indexed
by constraints and hybrid connectives combine constraint reasoning with logical
reasoning. The logic has a focused cut-free sequent calculus that can be used
to internalize the rules of particular constrained transition systems; we
illustrate this with an adequate encoding of the synchronous stochastic
pi-calculus. We also present some preliminary experiments of direct encoding of
biological systems in the logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4342</identifier>
 <datestamp>2013-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4342</id><created>2013-10-16</created><authors><author><keyname>Sree</keyname><forenames>Pokkuluri Kiran</forenames></author><author><keyname>Babuhor</keyname><forenames>Inampudi Ramesh</forenames></author><author><keyname>N3</keyname><forenames>SSSN Usha Devi</forenames></author></authors><title>An Extensive Report on Cellular Automata Based Artificial Immune System
  for Strengthening Automated Protein Prediction</title><categories>cs.AI cs.CE</categories><comments>arXiv admin note: text overlap with arXiv:0801.4312 by other authors</comments><journal-ref>Advances in Biomedical Engineering Research (ABER) Volume 1 Issue
  3, September 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Artificial Immune System (AIS-MACA) a novel computational intelligence
technique is can be used for strengthening the automated protein prediction
system with more adaptability and incorporating more parallelism to the system.
Most of the existing approaches are sequential which will classify the input
into four major classes and these are designed for similar sequences. AIS-MACA
is designed to identify ten classes from the sequences that share twilight zone
similarity and identity with the training sequences with mixed and hybrid
variations. This method also predicts three states (helix, strand, and coil)
for the secondary structure. Our comprehensive design considers 10 feature
selection methods and 4 classifiers to develop MACA (Multiple Attractor
Cellular Automata) based classifiers that are build for each of the ten
classes. We have tested the proposed classifier with twilight-zone and
1-high-similarity benchmark datasets with over three dozens of modern competing
predictors shows that AIS-MACA provides the best overall accuracy that ranges
between 80% and 89.8% depending on the dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4345</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4345</id><created>2013-10-16</created><updated>2015-09-09</updated><authors><author><keyname>Lagarias</keyname><forenames>Jeffrey C.</forenames></author><author><keyname>Luo</keyname><forenames>Yusheng</forenames></author></authors><title>Moser's Shadow Problem</title><categories>math.MG cs.CG</categories><comments>v1. 24 pages, two appendices; v2. 24 pages, revision changes notation
  in introduction, retains one appendix; v3. 28 pages, major revision,
  notations are changed</comments><msc-class>Primary: 52B10 Secondary: 51N15, 65D18, 68U05, 90C05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Moser's shadow problem asks to estimate the shadow function
$\mathfrak{s}_{b}(n)$ giving the minimal value with the property that for each
bounded convex polyhedron $P$ in $3$-space with $n$ vertices there is some
direction v (depending on P) such that when illuminated by parallel light rays
from infinity in direction v the polyhedron casts a shadow having at least
$\mathfrak{s}_{b}(n)$ vertices. A general version of the problem allows
unbounded polyhedra as well, and has associated shadow function
$\mathfrak{s}_{u}(n)$. This paper presents correct order of magnitude
asymptotic bounds on these functions. The bounded case has answer
$\mathfrak{b}(n) = \Theta \big( (\log n)/ (\log\log n)\big),$ a result
following from 1989 work of Chazelle, Edelsbrunner and Guibas. The unbounded
polyhedra case is shown to have the different asymptotic growth rate
$\mathfrak{u}(n) = \Theta \big(1\big)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4347</identifier>
 <datestamp>2013-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4347</id><created>2013-10-16</created><authors><author><keyname>Narasimhan</keyname><forenames>T. Lakshmi</forenames></author><author><keyname>Chockalingam</keyname><forenames>A.</forenames></author></authors><title>M-ary Detection and q-ary Decoding in Large-Scale MIMO: A Non-Binary
  Belief Propagation Approach</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a non-binary belief propagation approach (NB-BP)
for detection of $M$-ary modulation symbols and decoding of $q$-ary LDPC codes
in large-scale multiuser MIMO systems. We first propose a message passing based
symbol detection algorithm which computes vector messages using a scalar
Gaussian approximation of interference, which results in a total complexity of
just $O(KN\sqrt{M})$, where $K$ is the number of uplink users and $N$ is the
number of base station (BS) antennas. The proposed NB-BP detector does not need
to do a matrix inversion, which gives a complexity advantage over MMSE
detection. We then design optimized $q$-ary LDPC codes by matching the EXIT
charts of the proposed detector and the LDPC decoder. Simulation results show
that the proposed NB-BP detection-decoding approach using the optimized LDPC
codes achieve significantly better performance (by about 1 dB to 7 dB at
$10^{-5}$ coded BER for various system loading factors with number of users
ranging from 16 to 128 and number of BS antennas fixed at 128) compared to
using linear detectors (e.g., MMSE detector) and off-the-shelf $q$-ary
irregular LDPC codes. Also, even with estimated channel knowledge (e.g., with
MMSE channel estimate), the performance of the proposed NB-BP detector is
better than that of the MMSE detector.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4349</identifier>
 <datestamp>2013-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4349</id><created>2013-10-16</created><updated>2013-10-23</updated><authors><author><keyname>Bertram</keyname><forenames>Juliane</forenames></author><author><keyname>Hauck</keyname><forenames>Peter</forenames></author><author><keyname>Huber</keyname><forenames>Michael</forenames></author></authors><title>An Improved Majority-Logic Decoder Offering Massively Parallel Decoding
  for Real-Time Control in Embedded Systems</title><categories>cs.IT cs.AR cs.DM cs.ET math.IT</categories><comments>8 pages; to appear in &quot;IEEE Transactions on Communications&quot;</comments><msc-class>94B35, 68P30</msc-class><acm-class>E.4</acm-class><doi>10.1109/TCOMM.2013.13.130109</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an easy-to-implement hard-decision majority-logic decoding
algorithm for Reed-Muller codes RM(r,m) with m &gt;= 3, m/2 &gt;= r &gt;= 1. The
presented algorithm outperforms the best known majority-logic decoding
algorithms and offers highly parallel decoding. The result is of special
importance for safety- and time-critical applications in embedded systems. A
simple combinational circuit can perform the proposed decoding. In particular,
we show how our decoder for the three-error-correcting code RM(2,5) of
dimension 16 and length 32 can be realized on hardware level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4362</identifier>
 <datestamp>2013-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4362</id><created>2013-10-16</created><authors><author><keyname>Gillberg</keyname><forenames>Jussi</forenames></author><author><keyname>Marttinen</keyname><forenames>Pekka</forenames></author><author><keyname>Pirinen</keyname><forenames>Matti</forenames></author><author><keyname>Kangas</keyname><forenames>Antti J</forenames></author><author><keyname>Soininen</keyname><forenames>Pasi</forenames></author><author><keyname>J&#xe4;rvelin</keyname><forenames>Marjo-Riitta</forenames></author><author><keyname>Ala-Korpela</keyname><forenames>Mika</forenames></author><author><keyname>Kaski</keyname><forenames>Samuel</forenames></author></authors><title>Bayesian Information Sharing Between Noise And Regression Models
  Improves Prediction of Weak Effects</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the prediction of weak effects in a multiple-output regression
setup, when covariates are expected to explain a small amount, less than
$\approx 1%$, of the variance of the target variables. To facilitate the
prediction of the weak effects, we constrain our model structure by introducing
a novel Bayesian approach of sharing information between the regression model
and the noise model. Further reduction of the effective number of parameters is
achieved by introducing an infinite shrinkage prior and group sparsity in the
context of the Bayesian reduced rank regression, and using the Bayesian
infinite factor model as a flexible low-rank noise model. In our experiments
the model incorporating the novelties outperformed alternatives in genomic
prediction of rich phenotype data. In particular, the information sharing
between the noise and regression models led to significant improvement in
prediction accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4366</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4366</id><created>2013-10-16</created><authors><author><keyname>Nenova</keyname><forenames>Elena</forenames></author><author><keyname>Ignatov</keyname><forenames>Dmitry I.</forenames></author><author><keyname>Konstantinov</keyname><forenames>Andrey V.</forenames></author></authors><title>An FCA-based Boolean Matrix Factorisation for Collaborative Filtering</title><categories>cs.IR cs.DS stat.ML</categories><comments>http://ceur-ws.org/Vol-977/paper8.pdf</comments><msc-class>06B99, 03G10, 15B34</msc-class><acm-class>H.2.8; H.2.3</acm-class><journal-ref>In: C. Carpineto, A. Napoli, S.O. Kuznetsov (eds), FCA Meets IR
  2013, Vol. 977, CEUR Workshop Proceeding, 2013. P. 57-73</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new approach for Collaborative Filtering which is based on
Boolean Matrix Factorisation (BMF) and Formal Concept Analysis. In a series of
experiments on real data (Movielens dataset) we compare the approach with the
SVD- and NMF-based algorithms in terms of Mean Average Error (MAE). One of the
experimental consequences is that it is enough to have a binary-scaled rating
data to obtain almost the same quality in terms of MAE by BMF than for the
SVD-based algorithm in case of non-scaled data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4367</identifier>
 <datestamp>2013-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4367</id><created>2013-10-16</created><updated>2013-11-08</updated><authors><author><keyname>Je&#x17c;</keyname><forenames>Artur</forenames></author></authors><title>Context unification is in PSPACE</title><categories>cs.LO cs.FL</categories><comments>27 pages, submitted, small notation changes and small improvements
  over the previous text</comments><acm-class>F.4.1; F.4.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contexts are terms with one `hole', i.e. a place in which we can substitute
an argument. In context unification we are given an equation over terms with
variables representing contexts and ask about the satisfiability of this
equation. Context unification is a natural subvariant of second-order
unification, which is undecidable, and a generalization of word equations,
which are decidable, at the same time. It is the unique problem between those
two whose decidability is uncertain (for already almost two decades). In this
paper we show that the context unification is in PSPACE. The result holds under
a (usual) assumption that the first-order signature is finite.
  This result is obtained by an extension of the recompression technique,
recently developed by the author and used in particular to obtain a new PSPACE
algorithm for satisfiability of word equations, to context unification. The
recompression is based on performing simple compression rules (replacing pairs
of neighbouring function symbols), which are (conceptually) applied on the
solution of the context equation and modifying the equation in a way so that
such compression steps can be in fact performed directly on the equation,
without the knowledge of the actual solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4368</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4368</id><created>2013-10-16</created><updated>2014-12-09</updated><authors><author><keyname>Brandenberg</keyname><forenames>Ren&#xe9;</forenames></author><author><keyname>K&#xf6;nig</keyname><forenames>Stefan</forenames></author></authors><title>Sharpening Geometric Inequalities using Computable Symmetry Measures</title><categories>math.MG cs.CG</categories><comments>This is a preprint. The proper publication in final form is available
  at journals.cambridge.org, DOI 10.1112/S0025579314000291</comments><doi>10.1112/S0025579314000291</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many classical geometric inequalities on functionals of convex bodies depend
on the dimension of the ambient space. We show that this dimension dependence
may often be replaced (totally or partially) by different symmetry measures of
the convex body. Since these coefficients are bounded by the dimension but
possibly smaller, our inequalities sharpen the original ones. Since they can
often be computed efficiently, the improved bounds may also be used to obtain
better bounds in approximation algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4372</identifier>
 <datestamp>2014-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4372</id><created>2013-10-16</created><updated>2014-12-12</updated><authors><author><keyname>Jaume</keyname><forenames>Rafel</forenames></author><author><keyname>Rote</keyname><forenames>G&#xfc;nter</forenames></author></authors><title>Recursively-Regular Subdivisions and Applications</title><categories>cs.CG cs.DM math.CO</categories><comments>39 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalize regular subdivisions (polyhedral complexes resulting from the
projection of the lower faces of a polyhedron) introducing the class of
recursively-regular subdivisions. Informally speaking, a recursively-regular
subdivision is a subdivision that can be obtained by splitting some faces of a
regular subdivision by other regular subdivisions (and continue recursively).
We also define the \emph{finest regular coarsening} and the \emph{regularity
tree} of a polyhedral complex. We prove that recursively-regular subdivisions
are not necessarily connected by flips and that they are acyclic with respect
to the in-front relation. We show that the finest regular coarsening of a
subdivision can be efficiently computed, and that whether a subdivision is
recursively regular can be efficiently decided. As an application, we also
extend a theorem known since 1981 on illuminating space by cones and present
connections of recursive regularity to tensegrity theory and graph-embedding
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4377</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4377</id><created>2013-10-16</created><updated>2014-03-25</updated><authors><author><keyname>Peixoto</keyname><forenames>Tiago P.</forenames></author></authors><title>Hierarchical Block Structures and High-resolution Model Selection in
  Large Networks</title><categories>physics.data-an cond-mat.dis-nn cond-mat.stat-mech cs.SI physics.soc-ph stat.ML</categories><comments>18 pages, 9 figures + Supplemental Material</comments><journal-ref>Phys. Rev. X 4, 011047 (2014)</journal-ref><doi>10.1103/PhysRevX.4.011047</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Discovering and characterizing the large-scale topological features in
empirical networks are crucial steps in understanding how complex systems
function. However, most existing methods used to obtain the modular structure
of networks suffer from serious problems, such as being oblivious to the
statistical evidence supporting the discovered patterns, which results in the
inability to separate actual structure from noise. In addition to this, one
also observes a resolution limit on the size of communities, where smaller but
well-defined clusters are not detectable when the network becomes large. This
phenomenon occurs not only for the very popular approach of modularity
optimization, which lacks built-in statistical validation, but also for more
principled methods based on statistical inference and model selection, which do
incorporate statistical validation in a formally correct way. Here we construct
a nested generative model that, through a complete description of the entire
network hierarchy at multiple scales, is capable of avoiding this limitation,
and enables the detection of modular structure at levels far beyond those
possible with current approaches. Even with this increased resolution, the
method is based on the principle of parsimony, and is capable of separating
signal from noise, and thus will not lead to the identification of spurious
modules even on sparse networks. Furthermore, it fully generalizes other
approaches in that it is not restricted to purely assortative mixing patterns,
directed or undirected graphs, and ad hoc hierarchical structures such as
binary trees. Despite its general character, the approach is tractable, and can
be combined with advanced techniques of community detection to yield an
efficient algorithm that scales well for very large networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4378</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4378</id><created>2013-10-16</created><updated>2014-01-13</updated><authors><author><keyname>Peixoto</keyname><forenames>Tiago P.</forenames></author></authors><title>Efficient Monte Carlo and greedy heuristic for the inference of
  stochastic block models</title><categories>physics.data-an cond-mat.stat-mech cs.SI physics.comp-ph stat.ML</categories><comments>9 pages, 9 figures</comments><journal-ref>Phys. Rev. E 89, 012804 (2014)</journal-ref><doi>10.1103/PhysRevE.89.012804</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We present an efficient algorithm for the inference of stochastic block
models in large networks. The algorithm can be used as an optimized Markov
chain Monte Carlo (MCMC) method, with a fast mixing time and a much reduced
susceptibility to getting trapped in metastable states, or as a greedy
agglomerative heuristic, with an almost linear $O(N\ln^2N)$ complexity, where
$N$ is the number of nodes in the network, independent on the number of blocks
being inferred. We show that the heuristic is capable of delivering results
which are indistinguishable from the more exact and numerically expensive MCMC
method in many artificial and empirical networks, despite being much faster.
The method is entirely unbiased towards any specific mixing pattern, and in
particular it does not favor assortative community structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4389</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4389</id><created>2013-10-16</created><updated>2014-05-21</updated><authors><author><keyname>Cheng</keyname><forenames>Ming-Ming</forenames></author><author><keyname>Zheng</keyname><forenames>Shuai</forenames></author><author><keyname>Lin</keyname><forenames>Wen-Yan</forenames></author><author><keyname>Warrell</keyname><forenames>Jonathan</forenames></author><author><keyname>Vineet</keyname><forenames>Vibhav</forenames></author><author><keyname>Sturgess</keyname><forenames>Paul</forenames></author><author><keyname>Crook</keyname><forenames>Nigel</forenames></author><author><keyname>Mitra</keyname><forenames>Niloy</forenames></author><author><keyname>Torr</keyname><forenames>Philip</forenames></author></authors><title>ImageSpirit: Verbal Guided Image Parsing</title><categories>cs.GR cs.CV</categories><comments>http://mmcheng.net/imagespirit/</comments><acm-class>I.3.6; I.4.8</acm-class><journal-ref>ACM Transactions on Graphics, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Humans describe images in terms of nouns and adjectives while algorithms
operate on images represented as sets of pixels. Bridging this gap between how
humans would like to access images versus their typical representation is the
goal of image parsing, which involves assigning object and attribute labels to
pixel. In this paper we propose treating nouns as object labels and adjectives
as visual attribute labels. This allows us to formulate the image parsing
problem as one of jointly estimating per-pixel object and attribute labels from
a set of training images. We propose an efficient (interactive time) solution.
Using the extracted labels as handles, our system empowers a user to verbally
refine the results. This enables hands-free parsing of an image into pixel-wise
object/attribute labels that correspond to human semantics. Verbally selecting
objects of interests enables a novel and natural interaction modality that can
possibly be used to interact with new generation devices (e.g. smart phones,
Google Glass, living room devices). We demonstrate our system on a large number
of real-world images with varying complexity. To help understand the tradeoffs
compared to traditional mouse based interactions, results are reported for both
a large scale quantitative evaluation and a user study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4392</identifier>
 <datestamp>2013-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4392</id><created>2013-10-16</created><authors><author><keyname>Chekhchoukh</keyname><forenames>Abdessalem</forenames><affiliation>TIMC-IMAG, AGIM</affiliation></author><author><keyname>Goumidi</keyname><forenames>Malik</forenames><affiliation>TIMC-IMAG</affiliation></author><author><keyname>Vuillerme</keyname><forenames>Nicolas</forenames><affiliation>AGIM</affiliation></author><author><keyname>Payan</keyname><forenames>Yohan</forenames><affiliation>TIMC-IMAG</affiliation></author><author><keyname>Glade</keyname><forenames>Nicolas</forenames><affiliation>AGIM</affiliation></author></authors><title>Electrotactile vision substitution for 3D trajectory following</title><categories>cs.HC physics.med-ph</categories><proxy>ccsd</proxy><journal-ref>Conf Proc IEEE Eng Med Biol Soc 2013 (2013) 6413-6416</journal-ref><doi>10.1109/EMBC.2013.6611022</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Navigation for blind persons represents a challenge for researchers in vision
substitution. In this field, one of the used techniques to navigate is
guidance. In this study, we develop a new approach for 3D trajectory following
in which the requested task is to track a light path using computer input
devices (keyboard and mouse) or a rigid body handled in front of a stereoscopic
camera. The light path is visualized either on direct vision or by way of a
electro-stimulation device, the Tongue Display Unit, a 12x12 matrix of
electrodes. We improve our method by a series of experiments in which the
effect of the modality of perception and that of the input device. Preliminary
results indicated a close correlation between the stimulated and recorded
trajectories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4393</identifier>
 <datestamp>2014-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4393</id><created>2013-10-16</created><updated>2014-01-28</updated><authors><author><keyname>Boyer</keyname><forenames>Claire</forenames><affiliation>IMT</affiliation></author><author><keyname>Weiss</keyname><forenames>Pierre</forenames><affiliation>ITAV</affiliation></author><author><keyname>Bigot</keyname><forenames>J&#xe9;r&#xe9;mie</forenames><affiliation>DMIA</affiliation></author></authors><title>An algorithm for variable density sampling with block-constrained
  acquisition</title><categories>cs.IT math.IT math.OC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reducing acquisition time is of fundamental importance in various imaging
modalities. The concept of variable density sampling provides a nice framework
to achieve this. It was justified recently from a theoretical point of view in
the compressed sensing (CS) literature. Unfortunately, the sampling schemes
suggested by current CS theories may not be relevant since they do not take the
acquisition constraints into account (for example, continuity of the
acquisition trajectory in Magnetic Resonance Imaging - MRI). In this paper, we
propose a numerical method to perform variable density sampling with block
constraints. Our main contribution is to propose a new way to draw the blocks
in order to mimic CS strategies based on isolated measurements. The basic idea
is to minimize a tailored dissimilarity measure between a probability
distribution defined on the set of isolated measurements and a probability
distribution defined on a set of blocks of measurements. This problem turns out
to be convex and solvable in high dimension. Our second contribution is to
define an efficient minimization algorithm based on Nesterov's accelerated
gradient descent in metric spaces. We study carefully the choice of the metrics
and of the prox function. We show that the optimal choice may depend on the
type of blocks under consideration. Finally, we show that we can obtain better
MRI reconstruction results using our sampling schemes than standard strategies
such as equiangularly distributed radial lines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4399</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4399</id><created>2013-10-16</created><authors><author><keyname>De Meo</keyname><forenames>Pasquale</forenames></author><author><keyname>Ferrara</keyname><forenames>Emilio</forenames></author><author><keyname>Abel</keyname><forenames>Fabian</forenames></author><author><keyname>Aroyo</keyname><forenames>Lora</forenames></author><author><keyname>Houben</keyname><forenames>Geert-Jan</forenames></author></authors><title>Analyzing User Behavior across Social Sharing Environments</title><categories>cs.SI cs.CY physics.soc-ph</categories><journal-ref>ACM Transactions on Intelligent Systems and Technology, Vol. 5,
  No. 1, Article 1 (2013)</journal-ref><doi>10.1145/2535526</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we present an in-depth analysis of the user behaviors on
different Social Sharing systems. We consider three popular platforms, Flickr,
Delicious and StumbleUpon, and, by combining techniques from social network
analysis with techniques from semantic analysis, we characterize the tagging
behavior as well as the tendency to create friendship relationships of the
users of these platforms. The aim of our investigation is to see if (and how)
the features and goals of a given Social Sharing system reflect on the behavior
of its users and, moreover, if there exists a correlation between the social
and tagging behavior of the users. We report our findings in terms of the
characteristics of user profiles according to three different dimensions: (i)
intensity of user activities, (ii) tag-based characteristics of user profiles,
and (iii) semantic characteristics of user profiles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4411</identifier>
 <datestamp>2013-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4411</id><created>2013-10-16</created><authors><author><keyname>Gorman</keyname><forenames>Sean P.</forenames></author></authors><title>The Danger of a Big Data Episteme and the Need to Evolve GIS</title><categories>stat.CO cs.CY</categories><comments>AAG Conference 2013 - Los angeles, CA - More data, more problems?
  Geography and the future of 'big data'</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The emergence of &quot;Big Data&quot; as a dominant technology meme challenges
Geography's technical underpinnings, found in GIS, while engaging the
discipline in a conversation about the meme's impact on society. This allows
scholars to engage collaboratively from both a computationally quantitative and
critically qualitative perspective. For Geography there is an opportunity to
point out these shortcomings through critical appraisals of &quot;Big Data&quot; and its
reflection of society. Complimentarily this opens the door to developing
methodologies that will allow for a more realistic interpretation of &quot;Big Data&quot;
analysis in the context of an unfiltered societal view.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4412</identifier>
 <datestamp>2013-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4412</id><created>2013-10-16</created><authors><author><keyname>Xie</keyname><forenames>Nan</forenames></author><author><keyname>Weber</keyname><forenames>Steven</forenames></author></authors><title>Delay on broadcast erasure channels under random linear combinations</title><categories>cs.IT math.IT</categories><comments>57 pages, 8 figures, submitted October 1, 2013 to IEEE Transactions
  on Information Theory. Preliminary version presented at ITA 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a transmitter broadcasting random linear combinations (over a
field of size $d$) formed from a block of $c$ packets to a collection of $n$
receivers, where the channels between the transmitter and each receiver are
independent erasure channels with reception probabilities $\mathbf{q} =
(q_1,\ldots,q_n)$. We establish several properties of the random delay until
all $n$ receivers have recovered all $c$ packets, denoted $Y_{n:n}^{(c)}$.
First, we provide upper and lower bounds, exact expressions, and a recurrence
for the moments of $Y_{n:n}^{(c)}$. Second, we study the delay per packet
$Y_{n:n}^{(c)}/c$ as a function of $c$, including the asymptotic delay (as $c
\to \infty$), and monotonicity properties of the delay per packet (in $c$).
Third, we employ extreme value theory to investigate $Y_{n:n}^{(c)}$ as a
function of $n$ (as $n \to \infty$). Several results are new, some results are
extensions of existing results, and some results are proofs of known results
using new (probabilistic) proof techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4415</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4415</id><created>2013-10-16</created><updated>2014-02-18</updated><authors><author><keyname>Adamczyk</keyname><forenames>Marek</forenames></author><author><keyname>Sviridenko</keyname><forenames>Maxim</forenames></author><author><keyname>Ward</keyname><forenames>Justin</forenames></author></authors><title>Submodular Stochastic Probing on Matroids</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a stochastic probing problem we are given a universe $E$, where each
element $e \in E$ is active independently with probability $p_e$, and only a
probe of e can tell us whether it is active or not. On this universe we execute
a process that one by one probes elements --- if a probed element is active,
then we have to include it in the solution, which we gradually construct.
Throughout the process we need to obey inner constraints on the set of elements
taken into the solution, and outer constraints on the set of all probed
elements. This abstract model was presented by Gupta and Nagarajan (IPCO '13),
and provides a unified view of a number of problems. Thus far, all the results
falling under this general framework pertain mainly to the case in which we are
maximizing a linear objective function of the successfully probed elements. In
this paper we generalize the stochastic probing problem by considering a
monotone submodular objective function. We give a $(1 - 1/e)/(k_{in} +
k_{out}+1)$-approximation algorithm for the case in which we are given $k_{in}$
matroids as inner constraints and $k_{out}$ matroids as outer constraints.
Additionally, we obtain an improved $1/(k_{in} + k_{out})$-approximation
algorithm for linear objective functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4418</identifier>
 <datestamp>2013-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4418</id><created>2013-10-16</created><authors><author><keyname>Duchamp</keyname><forenames>G. H. E.</forenames></author><author><keyname>Hoang-Nghia</keyname><forenames>N.</forenames></author><author><keyname>Tanasa</keyname><forenames>A.</forenames></author></authors><title>A selection-quotient process for packed word Hopf algebra</title><categories>math.CO cs.DM hep-th</categories><comments>12 pages, conference proceedings, 5th International Conference on
  Algebraic Informatics, September 3-6, 2013</comments><journal-ref>Lecture Notes in Computer Science, volume 8080, Pages 223-234,
  2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we define a Hopf algebra structure on the vector space spanned
by packed words using a selection-quotient coproduct. We show that this algebra
is free on its irreducible packed words. Finally, we give some brief
explanations on the Maple codes we have used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4429</identifier>
 <datestamp>2013-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4429</id><created>2013-10-16</created><authors><author><keyname>Gu&#xe9;rin</keyname><forenames>Roch</forenames></author><author><keyname>de Oliveira</keyname><forenames>Jaudelice C.</forenames></author><author><keyname>Weber</keyname><forenames>Steven</forenames></author></authors><title>Adoption of bundled services with network externalities and correlated
  affinities</title><categories>cs.NI cs.GT</categories><comments>29 pages, 5 figures. Submitted on September 30, 2013 to the ACM
  Transactions on Internet Technology in response to Call for Papers for &quot;Theme
  Section on Pricing and Incentives in Networks and Systems&quot;. Preliminary work
  presented as a poster at the June 2013 W-PIN+NetEcon 2013 Joint Workshop on
  Pricing and Incentives in Networks and Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this paper is to develop a principled understanding of when it is
beneficial to bundle technologies or services whose value is heavily dependent
on the size of their user base, i.e., exhibits positive exernalities. Of
interest is how the joint distribution, and in particular the correlation, of
the values users assign to components of a bundle affect its odds of success.
The results offer insight and guidelines for deciding when bundling new
Internet technologies or services can help improve their overall adoption. In
particular, successful outcomes appear to require a minimum level of value
correlation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4456</identifier>
 <datestamp>2013-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4456</id><created>2013-10-16</created><authors><author><keyname>Webb</keyname><forenames>Stefan Douglas</forenames></author></authors><title>Inference, Sampling, and Learning in Copula Cumulative Distribution
  Networks</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The cumulative distribution network (CDN) is a recently developed class of
probabilistic graphical models (PGMs) permitting a copula factorization, in
which the CDF, rather than the density, is factored. Despite there being much
recent interest within the machine learning community about copula
representations, there has been scarce research into the CDN, its amalgamation
with copula theory, and no evaluation of its performance. Algorithms for
inference, sampling, and learning in these models are underdeveloped compared
those of other PGMs, hindering widerspread use.
  One advantage of the CDN is that it allows the factors to be parameterized as
copulae, combining the benefits of graphical models with those of copula
theory. In brief, the use of a copula parameterization enables greater
modelling flexibility by separating representation of the marginals from the
dependence structure, permitting more efficient and robust learning. Another
advantage is that the CDN permits the representation of implicit latent
variables, whose parameterization and connectivity are not required to be
specified. Unfortunately, that the model can encode only latent relationships
between variables severely limits its utility.
  In this thesis, we present inference, learning, and sampling for CDNs, and
further the state-of-the-art. First, we explain the basics of copula theory and
the representation of copula CDNs. Then, we discuss inference in the models,
and develop the first sampling algorithm. We explain standard learning methods,
propose an algorithm for learning from data missing completely at random
(MCAR), and develop a novel algorithm for learning models of arbitrary
treewidth and size. Properties of the models and algorithms are investigated
through Monte Carlo simulations. We conclude with further discussion of the
advantages and limitations of CDNs, and suggest future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4459</identifier>
 <datestamp>2013-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4459</id><created>2013-10-16</created><authors><author><keyname>Shtern</keyname><forenames>Alon</forenames></author><author><keyname>Kimmel</keyname><forenames>Ron</forenames></author></authors><title>Matching LBO eigenspace of non-rigid shapes via high order statistics</title><categories>cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fundamental tool in shape analysis is the virtual embedding of the
Riemannian manifold describing the geometry of a shape into Euclidean space.
Several methods have been proposed to embed isometric shapes in flat domains
while preserving distances measured on the manifold. Recently, attention has
been given to embedding shapes into the eigenspace of the Lapalce-Beltrami
operator. The Laplace-Beltrami eigenspace preserves the diffusion distance, and
is invariant under isometric transformations. However, Laplace-Beltrami
eigenfunctions computed independently for different shapes are often
incompatible with each other. Applications involving multiple shapes, such as
pointwise correspondence, would greatly benefit if their respective
eigenfunctions were somehow matched. Here, we introduce a statistical approach
for matching eigenfunctions. We consider the values of the eigenfunctions over
the manifold as sampling of random variables, and try to match their
multivariate distributions. Comparing distributions is done indirectly, using
high order statistics. We show that the permutation and sign ambiguities of low
order eigenfunctions, can be inferred by minimizing the difference of their
third order moments. The sign ambiguities of antisymmetric eigenfunctions can
be resolved by exploiting isometric invariant relations between the gradients
of the eigenfunctions and the surface normal. We present experiments
demonstrating the success of the proposed method applied to feature point
correspondence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4461</identifier>
 <datestamp>2014-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4461</id><created>2013-10-16</created><updated>2014-03-20</updated><authors><author><keyname>Merritt</keyname><forenames>Sears</forenames></author><author><keyname>Clauset</keyname><forenames>Aaron</forenames></author></authors><title>Scoring dynamics across professional team sports: tempo, balance and
  predictability</title><categories>stat.AP cs.CY physics.data-an physics.soc-ph</categories><comments>18 pages, 8 figures, 4 tables, 2 appendices</comments><journal-ref>EPJ Data Science 3, 4 (2014)</journal-ref><doi>10.1140/epjds29</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite growing interest in quantifying and modeling the scoring dynamics
within professional sports games, relative little is known about what patterns
or principles, if any, cut across different sports. Using a comprehensive data
set of scoring events in nearly a dozen consecutive seasons of college and
professional (American) football, professional hockey, and professional
basketball, we identify several common patterns in scoring dynamics. Across
these sports, scoring tempo---when scoring events occur---closely follows a
common Poisson process, with a sport-specific rate. Similarly, scoring
balance---how often a team wins an event---follows a common Bernoulli process,
with a parameter that effectively varies with the size of the lead. Combining
these processes within a generative model of gameplay, we find they both
reproduce the observed dynamics in all four sports and accurately predict game
outcomes. These results demonstrate common dynamical patterns underlying
within-game scoring dynamics across professional team sports, and suggest
specific mechanisms for driving them. We close with a brief discussion of the
implications of our results for several popular hypotheses about sports
dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4475</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4475</id><created>2013-10-09</created><authors><author><keyname>Khan</keyname><forenames>Muhammad Imran</forenames></author></authors><title>Network Parameters Impact on Dynamic Transmission Power Control in
  Vehicular Ad hoc Networks</title><categories>cs.NI</categories><comments>22 pages, International Journal of Next-Generation Networks (IJNGN)
  Vol.5, No.3, September 2013</comments><journal-ref>International Journal of Next-Generation Networks (IJNGN) Vol.5,
  No.3, pages 1-22, September 2013</journal-ref><doi>10.5121/ijngn.2013.5301</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In vehicular ad hoc networks, the dynamic change in transmission power is
very effective to increase the throughput of the wireless vehicular network and
decrease the delay of the message communication between vehicular nodes on the
highway. Transmission range is directly proportional to the transmission power
the moving node. If the transmission power will be high, the interference
increases that can cause higher delay in message reception at receiver end,
hence the performance of the network decreased. In this paper, it is analyzed
that how transmission power can be controlled by considering other different
parameter of the network such as; density, distance between moving nodes,
different types of messages dissemination with their priority, selection of an
antenna also affects on the transmission power. The dynamic control of
transmission power in VANETs serves also for the optimization of the resources
where it needs, can be decreased and increased depending on the circumstances
of the network. Different applications and events of different types also cause
changes in transmission power to enhance the reach-ability. The analysis in
this paper is comprised of density, distance with single hop and multi-hop
message broadcasting based dynamic transmission power control as well as
antenna selection and applications based. Some summarized tables are produced
according to the respective parameters of the vehicular network. At the end
some valuable observations are made and discussed in detail. This paper
concludes with a grand summary of all the protocols discussed in it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4485</identifier>
 <datestamp>2013-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4485</id><created>2013-10-15</created><authors><author><keyname>Liu</keyname><forenames>Juan</forenames></author><author><keyname>Zhang</keyname><forenames>Baochang</forenames></author><author><keyname>Shen</keyname><forenames>Linlin</forenames></author><author><keyname>Liu</keyname><forenames>Jianzhuang</forenames></author><author><keyname>Zhao</keyname><forenames>Jason</forenames></author></authors><title>The BeiHang Keystroke Dynamics Authentication System</title><categories>cs.CR cs.LG</categories><comments>25 pages,17 figures,5 tables</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Keystroke Dynamics is an important biometric solution for person
authentication. Based upon keystroke dynamics, this paper designs an embedded
password protection device, develops an online system, collects two public
databases for promoting the research on keystroke authentication, exploits the
Gabor filter bank to characterize keystroke dynamics, and provides benchmark
results of three popular classification algorithms, one-class support vector
machine, Gaussian classifier, and nearest neighbour classifier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4495</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4495</id><created>2013-10-16</created><authors><author><keyname>Sree</keyname><forenames>Pokkuluri Kiran</forenames></author><author><keyname>Babu</keyname><forenames>Inampudi Ramesh</forenames></author><author><keyname>Nedunuri</keyname><forenames>SSSN Usha Devi</forenames></author></authors><title>Multiple Attractor Cellular Automata (MACA) for Addressing Major
  Problems in Bioinformatics</title><categories>cs.CE cs.LG</categories><comments>arXiv admin note: text overlap with arXiv:1310.4342</comments><journal-ref>Review of Bioinformatics and Biometrics (RBB) Volume 2 Issue 3,
  September 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  CA has grown as potential classifier for addressing major problems in
bioinformatics. Lot of bioinformatics problems like predicting the protein
coding region, finding the promoter region, predicting the structure of protein
and many other problems in bioinformatics can be addressed through Cellular
Automata. Even though there are some prediction techniques addressing these
problems, the approximate accuracy level is very less. An automated procedure
was proposed with MACA (Multiple Attractor Cellular Automata) which can address
all these problems. The genetic algorithm is also used to find rules with good
fitness values. Extensive experiments are conducted for reporting the accuracy
of the proposed tool. The average accuracy of MACA when tested with ENCODE,
BG570, HMR195, Fickett and Tongue, ASP67 datasets is 78%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4502</identifier>
 <datestamp>2013-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4502</id><created>2013-10-16</created><authors><author><keyname>Warren</keyname><forenames>Michael S.</forenames></author></authors><title>2HOT: An Improved Parallel Hashed Oct-Tree N-Body Algorithm for
  Cosmological Simulation</title><categories>astro-ph.IM astro-ph.CO cs.DC</categories><comments>12 pages, 8 figures, 77 references; To appear in Proceedings of SC
  '13</comments><report-no>LA-UR-13-22336</report-no><doi>10.1145/2503210.2503220</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report on improvements made over the past two decades to our adaptive
treecode N-body method (HOT). A mathematical and computational approach to the
cosmological N-body problem is described, with performance and scalability
measured up to 256k ($2^{18}$) processors. We present error analysis and
scientific application results from a series of more than ten 69 billion
($4096^3$) particle cosmological simulations, accounting for $4 \times 10^{20}$
floating point operations. These results include the first simulations using
the new constraints on the standard model of cosmology from the Planck
satellite. Our simulations set a new standard for accuracy and scientific
throughput, while meeting or exceeding the computational efficiency of the
latest generation of hybrid TreePM N-body methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4541</identifier>
 <datestamp>2013-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4541</id><created>2013-10-16</created><authors><author><keyname>Cicconet</keyname><forenames>Marcelo</forenames></author><author><keyname>Geiger</keyname><forenames>Davi</forenames></author></authors><title>A Dynamic Programming Solution to the Monotonic Path of Minimal Cost in
  a 3-Rows Matrix</title><categories>cs.DS</categories><comments>1 page, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of finding the path of minimal cost going from left
to right in a 3-rows matrix, starting at the third row, and not going
downwards, where there is an additional cost related to not changing rows, such
that the higher the change in intensity within the row, the higher the cost of
not moving upwards.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4545</identifier>
 <datestamp>2013-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4545</id><created>2013-10-16</created><authors><author><keyname>Mahajan</keyname><forenames>Aditya</forenames></author><author><keyname>Mannan</keyname><forenames>Mehnaz</forenames></author></authors><title>Decentralized stochastic control</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decentralized stochastic control refers to the multi-stage optimization of a
dynamical system by multiple controllers that have access to different
information. Decentralization of information gives rise to new conceptual
challenges that require new solution approaches. In this expository paper, we
use the notion of an \emph{information-state} to explain the two commonly used
solution approaches to decentralized control: the person-by-person approach and
the common-information approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4546</identifier>
 <datestamp>2013-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4546</id><created>2013-10-16</created><authors><author><keyname>Mikolov</keyname><forenames>Tomas</forenames></author><author><keyname>Sutskever</keyname><forenames>Ilya</forenames></author><author><keyname>Chen</keyname><forenames>Kai</forenames></author><author><keyname>Corrado</keyname><forenames>Greg</forenames></author><author><keyname>Dean</keyname><forenames>Jeffrey</forenames></author></authors><title>Distributed Representations of Words and Phrases and their
  Compositionality</title><categories>cs.CL cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recently introduced continuous Skip-gram model is an efficient method for
learning high-quality distributed vector representations that capture a large
number of precise syntactic and semantic word relationships. In this paper we
present several extensions that improve both the quality of the vectors and the
training speed. By subsampling of the frequent words we obtain significant
speedup and also learn more regular word representations. We also describe a
simple alternative to the hierarchical softmax called negative sampling. An
inherent limitation of word representations is their indifference to word order
and their inability to represent idiomatic phrases. For example, the meanings
of &quot;Canada&quot; and &quot;Air&quot; cannot be easily combined to obtain &quot;Air Canada&quot;.
Motivated by this example, we present a simple method for finding phrases in
text, and show that learning good vector representations for millions of
phrases is possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4561</identifier>
 <datestamp>2013-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4561</id><created>2013-10-16</created><authors><author><keyname>Damian</keyname><forenames>Mirela</forenames></author><author><keyname>Demaine</keyname><forenames>Erik</forenames></author><author><keyname>Flatland</keyname><forenames>Robin</forenames></author></authors><title>Unfolding Orthogrids with Constant Refinement</title><categories>cs.CG</categories><comments>19 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a new class of orthogonal polyhedra, called orthogrids, that can be
unfolded without overlap with constant refinement of the gridded surface.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4571</identifier>
 <datestamp>2013-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4571</id><created>2013-10-16</created><authors><author><keyname>Baranov</keyname><forenames>Eduard</forenames><affiliation>Ecole Polytechnique F&#xe9;d&#xe9;rale de Lausanne, Switzerland</affiliation></author><author><keyname>Bliudze</keyname><forenames>Simon</forenames><affiliation>Ecole Polytechnique F&#xe9;d&#xe9;rale de Lausanne, Switzerland</affiliation></author></authors><title>Extended Connectors: Structuring Glue Operators in BIP</title><categories>cs.LO cs.PL cs.SE</categories><comments>In Proceedings ICE 2013, arXiv:1310.4019</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 131, 2013, pp. 20-35</journal-ref><doi>10.4204/EPTCS.131.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on a variation of the BIP operational semantics using the offer
predicate introduced in our previous work, we extend the algebras used to model
glue operators in BIP to encompass priorities. This extension uses the Algebra
of Causal Interaction Trees, T(P), as a pivot: existing transformations
automatically provide the extensions for the Algebra of Connectors. We then
extend the axiomatisation of T(P), since the equivalence induced by the new
operational semantics is weaker than that induced by the interaction semantics.
This extension leads to canonical normal forms for all structures and to a
simplification of the algorithm for the synthesis of connectors from Boolean
coordination constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4572</identifier>
 <datestamp>2013-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4572</id><created>2013-10-16</created><authors><author><keyname>Xu</keyname><forenames>Xian</forenames><affiliation>East China University of Science and Technology, Shanghai, China P.R.</affiliation></author></authors><title>On Context Bisimulation for Parameterized Higher-order Processes</title><categories>cs.LO cs.GT</categories><comments>In Proceedings ICE 2013, arXiv:1310.4019</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 131, 2013, pp. 37-51</journal-ref><doi>10.4204/EPTCS.131.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies context bisimulation for higher-order processes, in the
presence of parameterization (viz. abstraction). We show that the extension of
higher-order processes with process parameterization retains the
characterization of context bisimulation by a much simpler form of bisimulation
called normal bisimulation (viz. they are coincident), in which universal
quantifiers are eliminated; whereas it is not the same with name
parameterization. These results clarify further the bisimulation theory of
higher-order processes, and also shed light on the essential distinction
between the two kinds of parameterization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4573</identifier>
 <datestamp>2013-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4573</id><created>2013-10-16</created><authors><author><keyname>Lange</keyname><forenames>Julien</forenames><affiliation>University of Leicester, UK</affiliation></author><author><keyname>Scalas</keyname><forenames>Alceste</forenames><affiliation>University of Cagliari, Italy</affiliation></author></authors><title>Choreography Synthesis as Contract Agreement</title><categories>cs.LO cs.DC</categories><comments>In Proceedings ICE 2013, arXiv:1310.4019</comments><proxy>EPTCS</proxy><acm-class>D.3.1; D.3.2; F.3.1</acm-class><journal-ref>EPTCS 131, 2013, pp. 52-67</journal-ref><doi>10.4204/EPTCS.131.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a formal model for distributed systems, where each participant
advertises its requirements and obligations as behavioural contracts, and where
multiparty sessions are started when a set of contracts allows to synthesise a
choreography. Our framework is based on the CO2 calculus for contract-oriented
computing, and borrows concepts and results from the session type literature.
  It supports sessions where the number of participants is not determined
beforehand, and keeps CO2's ability to rule out participants that are culpable
if contracts are not fulfilled at runtime. We show that we have progress and
session fidelity in CO2, as a result of the honesty of participants - i.e.,
their ability to always adhere to their contracts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4574</identifier>
 <datestamp>2013-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4574</id><created>2013-10-16</created><authors><author><keyname>Poyias</keyname><forenames>Kyriakos</forenames><affiliation>University of Leicester, UK</affiliation></author><author><keyname>Tuosto</keyname><forenames>Emilio</forenames><affiliation>University of Leicester, UK</affiliation></author></authors><title>On Recovering from Run-time Misbehaviour in ADR</title><categories>cs.SE cs.DC cs.LO</categories><comments>In Proceedings ICE 2013, arXiv:1310.4019</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 131, 2013, pp. 68-84</journal-ref><doi>10.4204/EPTCS.131.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a monitoring mechanism for recording the evolution of systems
after certain computations, maintaining the history in a tree-like structure.
Technically, we develop the monitoring mechanism in a variant of ADR (after
Architectural Design Rewriting), a rule-based formal framework for modelling
the evolution of architectures of systems.
  The hierarchical nature of ADR allows us to take full advantage of the
tree-like structure of the monitoring mechanism. We exploit this mechanism to
formally define new rewriting mechanisms for ADR reconfiguration rules. Also,
by monitoring the evolution we propose a way of identifying which part of a
system has been affected when unexpected run-time behaviours emerge. Moreover,
we propose a methodology to suggest reconfigurations that could potentially
lead the system in a non-erroneous state.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4575</identifier>
 <datestamp>2013-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4575</id><created>2013-10-16</created><authors><author><keyname>Palmskog</keyname><forenames>Karl</forenames><affiliation>KTH Royal Institute of Technology</affiliation></author><author><keyname>Dam</keyname><forenames>Mads</forenames><affiliation>KTH Royal Institute of Technology</affiliation></author><author><keyname>Lundblad</keyname><forenames>Andreas</forenames><affiliation>KTH Royal Institute of Technology</affiliation></author><author><keyname>Jafari</keyname><forenames>Ali</forenames><affiliation>Reykjavik University</affiliation></author></authors><title>ABS-NET: Fully Decentralized Runtime Adaptation for Distributed Objects</title><categories>cs.DC cs.PL</categories><comments>In Proceedings ICE 2013, arXiv:1310.4019</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 131, 2013, pp. 85-100</journal-ref><doi>10.4204/EPTCS.131.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a formalized, fully decentralized runtime semantics for a core
subset of ABS, a language and framework for modelling distributed
object-oriented systems. The semantics incorporates an abstract graph
representation of a network infrastructure, with network endpoints represented
as graph nodes, and links as arcs with buffers, corresponding to OSI layer 2
interconnects. The key problem we wish to address is how to allocate
computational tasks to nodes so that certain performance objectives are met. To
this end, we use the semantics as a foundation for performing network-adaptive
task execution via object migration between nodes. Adaptability is analyzed in
terms of three Quality of Service objectives: node load, arc load and message
latency. We have implemented the key parts of our semantics in a simulator and
evaluated how well objectives are achieved for some application-relevant
choices of network topology, migration procedure and ABS program. The
evaluation suggests that it is feasible in a decentralized setting to
continually meet both the objective of a node-balanced task allocation and make
headway towards minimizing communication, and thus arc load and message
latency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4577</identifier>
 <datestamp>2013-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4577</id><created>2013-10-17</created><authors><author><keyname>Elices</keyname><forenames>Juan A.</forenames></author><author><keyname>Perez-Gonzalez</keyname><forenames>Fernando</forenames></author></authors><title>A highly optimized flow-correlation attack</title><categories>cs.CR cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deciding that two network flows are essentially the same is an important
problem in intrusion detection and in tracing anonymous connections. A stepping
stone or an anonymity network may try to prevent flow correlation by adding
chaff traffic, splitting the flow in several subflows or adding random delays.
A well-known attack for these types of systems is active watermarking. However,
active watermarking systems can be detected and an attacker can modify the flow
in such a way that the watermark is removed and can no longer be decoded. This
leads to the two basic features of our scheme: a highlyoptimized algorithm that
achieves very good performance and a passive analysis that is undetectable. We
propose a new passive analysis technique where detection is based on
Neyman-Pearson lemma. We correlate the inter-packet delays (IPDs) from both
flows. Then, we derive a modification to deal with stronger adversary models
that add chaff traffic, split the flows or add random delays. We empirically
validate the detectors with a simulator. Afterwards, we create a watermarkbased
version of our scheme to study the trade-off between performance and
detectability. Then, we compare the results with other state-of-the-art traffic
watermarking schemes in several scenarios concluding that our scheme
outperforms the rest. Finally, we present results using an implementation of
our method on live networks, showing that the conclusions can be extended to
real-world scenarios. Our scheme needs only tens of packets under normal
network interference and a few hundreds of packets when a number of
countermeasures are taken.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4579</identifier>
 <datestamp>2013-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4579</id><created>2013-10-17</created><authors><author><keyname>De</keyname><forenames>Abir</forenames></author><author><keyname>Ganguly</keyname><forenames>Niloy</forenames></author><author><keyname>Chakrabarti</keyname><forenames>Soumen</forenames></author></authors><title>Discriminative Link Prediction using Local Links, Node Features and
  Community Structure</title><categories>cs.LG cs.SI physics.soc-ph</categories><comments>10 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A link prediction (LP) algorithm is given a graph, and has to rank, for each
node, other nodes that are candidates for new linkage. LP is strongly motivated
by social search and recommendation applications. LP techniques often focus on
global properties (graph conductance, hitting or commute times, Katz score) or
local properties (Adamic-Adar and many variations, or node feature vectors),
but rarely combine these signals. Furthermore, neither of these extremes
exploit link densities at the intermediate level of communities. In this paper
we describe a discriminative LP algorithm that exploits two new signals. First,
a co-clustering algorithm provides community level link density estimates,
which are used to qualify observed links with a surprise value. Second, links
in the immediate neighborhood of the link to be predicted are not interpreted
at face value, but through a local model of node feature similarities. These
signals are combined into a discriminative link predictor. We evaluate the new
predictor using five diverse data sets that are standard in the literature. We
report on significant accuracy boosts compared to standard LP methods
(including Adamic-Adar and random walk). Apart from the new predictor, another
contribution is a rigorous protocol for benchmarking and reporting LP
algorithms, which reveals the regions of strengths and weaknesses of all the
predictors studied here, and establishes the new proposal as the most robust.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4581</identifier>
 <datestamp>2013-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4581</id><created>2013-10-17</created><authors><author><keyname>Berta</keyname><forenames>Mario</forenames></author></authors><title>Quantum Side Information: Uncertainty Relations, Extractors, Channel
  Simulations</title><categories>quant-ph cs.IT math-ph math.IT math.MP</categories><comments>PhD thesis, ETH Zurich. 214 pages, 13 figures, 1 table. Chapter 2 is
  based on arXiv:1107.5460 and arXiv:1308.4527 . Section 3.1 is based on
  arXiv:1302.5902 and Section 3.2 is a preliminary version of arXiv:1308.4527
  (you better read arXiv:1308.4527). Chapter 4 is (partly) based on
  arXiv:1012.6044 and arXiv:1111.2026 . Chapter 5 is based on arXiv:0912.3805,
  arXiv:1108.5357 and arXiv:1301.1594</comments><report-no>Diss. ETH No. 21180</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the first part of this thesis, we discuss the algebraic approach to
classical and quantum physics and develop information theoretic concepts within
this setup.
  In the second part, we discuss the uncertainty principle in quantum
mechanics. The principle states that even if we have full classical information
about the state of a quantum system, it is impossible to deterministically
predict the outcomes of all possible measurements. In comparison, the
perspective of a quantum observer allows to have quantum information about the
state of a quantum system. This then leads to an interplay between uncertainty
and quantum correlations. We provide an information theoretic analysis by
discussing entropic uncertainty relations with quantum side information.
  In the third part, we discuss the concept of randomness extractors. Classical
and quantum randomness are an essential resource in information theory,
cryptography, and computation. However, most sources of randomness exhibit only
weak forms of unpredictability, and the goal of randomness extraction is to
convert such weak randomness into (almost) perfect randomness. We discuss
various constructions for classical and quantum randomness extractors, and we
examine especially the performance of these constructions relative to an
observer with quantum side information.
  In the fourth part, we discuss channel simulations. Shannon's noisy channel
theorem can be understood as the use of a noisy channel to simulate a noiseless
one. Channel simulations as we want to consider them here are about the reverse
problem: simulating noisy channels from noiseless ones. Starting from the
purely classical case (the classical reverse Shannon theorem), we develop
various kinds of quantum channel simulation results. We achieve this by using
classical and quantum randomness extractors that also work with respect to
quantum side information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4583</identifier>
 <datestamp>2013-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4583</id><created>2013-10-17</created><authors><author><keyname>Yang</keyname><forenames>Ying</forenames></author><author><keyname>Moretti</keyname><forenames>Marco</forenames></author><author><keyname>Dong</keyname><forenames>Wenxiang</forenames></author><author><keyname>Wang</keyname><forenames>Weidong</forenames></author></authors><title>Low complexity resource allocation for load minimization in OFDMA
  wireless networks</title><categories>cs.IT cs.NI math.IT</categories><comments>24 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To cope with the ever increasing demand for bandwidth, future wireless
networks will be designed with reuse distance equal to one. This scenario
requires the implementation of techniques able to manage the strong multiple
access interference each cell generates towards its neighbor cells. In
particular, low complexity and reduced feedback are important requirements for
practical algorithms. In this paper we study an allocation problem for OFDMA
networks formulated with the objective of minimizing the load of each cell in
the system subject to the constraint that each user meets its target rate. We
decompose resource allocation into two sub-problems: channel allocation under
deterministic power assignment and continuous power assignment optimization.
Channel allocation is formulated as the problem of finding the maximum weighted
independent set (MWIS) in graph theory. In addition, we propose a minimal
weighted-degree greedy (MWDG) algorithm of which the approximation factor is
analyzed. For power allocation, an iterative power reassignment algorithm
(DPRA) is proposed. The control information requested to perform the allocation
is limited and the computational burden is shared between the base station and
the user equipments. Simulations have been carried out under constant bit rate
traffic model and the results have been compared with other allocation schemes
of similar complexity. MWDG has excellent performance and outperforms all other
techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4588</identifier>
 <datestamp>2013-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4588</id><created>2013-10-17</created><authors><author><keyname>Brand</keyname><forenames>Michael</forenames></author></authors><title>Arbitrary Sequence RAMs</title><categories>cs.CC cs.FL</categories><comments>9 pages</comments><msc-class>68Q05, 68Q10, g8Q15</msc-class><acm-class>F.1.1; F.1.2; F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is known that in some cases a Random Access Machine (RAM) benefits from
having an additional input that is an arbitrary number, satisfying only the
criterion of being sufficiently large. This is known as the ARAM model. We
introduce a new type of RAM, which we refer to as the Arbitrary Sequence RAM
(ASRAM), that generalises the ARAM by allowing the generation of additional
arbitrary large numbers at will during execution time. We characterise the
power contribution of this ability under several RAM variants.
  In particular, we demonstrate that an arithmetic ASRAM is more powerful than
an arithmetic ARAM, that a sufficiently equipped ASRAM can recognise any
language in the arithmetic hierarchy in constant time (and more, if it is given
more time), and that, on the other hand, in some cases the ASRAM is no more
powerful than its underlying RAM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4595</identifier>
 <datestamp>2013-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4595</id><created>2013-10-17</created><authors><author><keyname>Censor-Hillel</keyname><forenames>Keren</forenames><affiliation>Technion</affiliation></author><author><keyname>King</keyname><forenames>Valerie</forenames><affiliation>University of Victoria</affiliation></author></authors><title>Proceedings Ninth International Workshop on Foundations of Mobile
  Computing</title><categories>cs.DC cs.DS cs.NI</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 132, 2013</journal-ref><doi>10.4204/EPTCS.132</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile communication has become a vigorous field of research in computer
science, due to the wide spreading of mobile technologies, applications and
services. The intertwining of communication, computation and mobility
constantly poses new challenges to algorithmic design in this area. The
Foundations of Mobile Computing (FOMC) workshop is dedicated to all aspects
that cover contributions both in the design and analysis of
discrete/distributed algorithms, and in the system modeling of mobile, wireless
and similarly dynamic networks. It aims to bring together the practitioners and
theoreticians of the field to foster cooperation between research in mobile
computing and algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4596</identifier>
 <datestamp>2013-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4596</id><created>2013-10-17</created><authors><author><keyname>Khafagy</keyname><forenames>Mohammad</forenames></author><author><keyname>Ismail</keyname><forenames>Amr</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author><author><keyname>Aissa</keyname><forenames>Sonia</forenames></author></authors><title>Energy-Efficient Cooperative Protocols for Full-Duplex Relay Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, energy-efficient cooperative protocols are studied for
full-duplex relaying (FDR) with loopback interference. In these protocols,
relay assistance is only sought under certain conditions on the different link
outages to ensure effective cooperation. Recently, an energy-efficient
selective decode-and-forward protocol was proposed for FDR, and was shown to
outperform existing schemes in terms of outage. Here, we propose an incremental
selective decode-and-forward protocol that offers additional power savings,
while keeping the same outage performance. We compare the performance of the
two protocols in terms of the end-to-end signal-to-noise ratio cumulative
distribution function via closed-form expressions. Finally, we corroborate our
theoretical results with simulation, and show the relative relay power savings
in comparison to non-selective cooperation in which the relay cooperates
regardless of channel conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4613</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4613</id><created>2013-10-17</created><updated>2015-09-02</updated><authors><author><keyname>Goaoc</keyname><forenames>Xavier</forenames></author><author><keyname>Pat&#xe1;k</keyname><forenames>Pavel</forenames></author><author><keyname>Safernov&#xe1;</keyname><forenames>Zuzana</forenames></author><author><keyname>Tancer</keyname><forenames>Martin</forenames></author><author><keyname>Wagner</keyname><forenames>Uli</forenames></author></authors><title>Bounding Helly numbers via Betti numbers</title><categories>math.CO cs.CG cs.DM math.AT</categories><comments>28 pages, 8 figures</comments><msc-class>Primary 52A35, secondary 05E45, 55S91, 05D10, 57Q35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that very weak topological assumptions are enough to ensure the
existence of a Helly-type theorem. More precisely, we show that for any
non-negative integers $b$ and $d$ there exists an integer $h(b,d)$ such that
the following holds. If $\mathcal{F}$ is a finite family of subsets of
$\mathbb{R}^d$ such that $\tilde\beta_i\{\bigcap\mathcal{G}\} \le b$ for any
$\mathcal{G} \subsetneq \mathcal{F}$ and every $0 \le i \le \lceil d/2
\rceil-1$ then $\mathcal{F}$ has Helly number at most $h(b,d)$. Here
$\tilde\beta_i$ denotes the reduced $\mathbb{Z}_2$-Betti numbers (with singular
homology). These topological conditions are sharp: not controlling any of these
$\lceil d/2 \rceil$ first Betti numbers allow for families with unbounded Helly
number.
  Our proofs combine homological non-embeddability results with a Ramsey-based
approach to build, given an arbitrary simplicial complex $K$, some well-behaved
chain map $C_*(K) \to C_*(\mathbb{R}^d)$. Both techniques are of independent
interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4632</identifier>
 <datestamp>2013-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4632</id><created>2013-10-17</created><authors><author><keyname>Di Marco</keyname><forenames>Piergiuseppe</forenames></author><author><keyname>Athanasiou</keyname><forenames>George</forenames></author><author><keyname>Mekikis</keyname><forenames>Prodromos-Vasileios</forenames></author><author><keyname>Fischione</keyname><forenames>Carlo</forenames></author></authors><title>MAC-aware Routing Metrics for the Internet of Things</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless medium access control (MAC) and routing protocols are fundamental
building blocks of the Internet of Things (IoT). As new IoT networking
standards are being proposed and different existing solutions patched,
evaluating the end-to-end performance of the network becomes challenging.
Specific solutions designed to be beneficial, when stacked may have detrimental
effects on the overall network performance. In this paper, an analysis of MAC
and routing protocols for IoT is provided with focus on the IEEE 802.15.4 MAC
and the IETF RPL standards. It is shown that existing routing metrics do not
account for the complex interactions between MAC and routing, and thus novel
metrics are proposed. This enables a protocol selection mechanism for selecting
the routing option and adapting the MAC parameters, given specific performance
constraints. Extensive analytical and experimental results show that the
behavior of the MAC protocol can hurt the performance of the routing protocol
and vice versa, unless these two are carefully optimized together by the
proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4633</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4633</id><created>2013-10-17</created><authors><author><keyname>Rohr</keyname><forenames>Rudolf P.</forenames></author><author><keyname>Naisbit</keyname><forenames>Russel E.</forenames></author><author><keyname>Mazza</keyname><forenames>Christian</forenames></author><author><keyname>Bersier</keyname><forenames>Louix-F&#xe9;lix</forenames></author></authors><title>Matching-centrality decomposition and the forecasting of new links in
  networks</title><categories>physics.soc-ph cs.SI q-bio.PE</categories><journal-ref>Proceedings of the Royal Society B: Biological Sciences (2016),
  283:20152702</journal-ref><doi>10.1098/rspb.2015.2702</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Networks play a prominent role in the study of complex systems of interacting
entities in biology, sociology, and economics. Despite this diversity, we
demonstrate here that a statistical model decomposing networks into matching
and centrality components provides a comprehensive and unifying quantification
of their architecture. First we show, for a diverse set of networks, that this
decomposition provides an extremely tight fit to observed networks.
Consequently, the model allows very accurate prediction of missing links in
partially known networks. Second, when node characteristics are known, we show
how the matching-centrality decomposition can be related to this external
information. Consequently, it offers a simple and versatile tool to explore how
node characteristics explain network architecture. Finally, we demonstrate the
efficiency and flexibility of the model to forecast the links that a novel node
would create if it were to join an existing network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4638</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4638</id><created>2013-10-17</created><updated>2013-10-28</updated><authors><author><keyname>Csirmaz</keyname><forenames>Laszlo</forenames></author></authors><title>Using multiobjective optimization to map the entropy region of four
  random variables</title><categories>cs.IT math.IT math.OC</categories><msc-class>90C60, 90C05, 94A17, 90C29</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Presently the only available method of exploring the 15-dimensional entropy
region formed by the entropies of four random variables is the one of Zhang and
Yeung from 1998. It is argued that their method is equivalent to solving linear
multiobjective optimization problems. Benson's outer approximation algorithm is
a fundamental tool for solving these optimization problems. An improved version
of Benson's algorithm is described which requires solving one scalar linear
program in each iteration rather than two or three as in previous versions.
During the algorithm design special care was taken for numerical stability. The
implemented algorithm was used to check previous statements about the entropy
region, and to gain new information on that region. The experimental results
demonstrate the viability of the method for determining the extremal set of
medium size, numerically ill-posed optimization problems. With growing problem
size two limitations of Benson's algorithm have been observed: the inefficiency
of the scalar LP solver on one hand and the unexpectedly large number of
intermediate vertices on the other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4640</identifier>
 <datestamp>2013-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4640</id><created>2013-10-17</created><authors><author><keyname>Csirmaz</keyname><forenames>Laszlo</forenames></author></authors><title>Secret sharing on the $d$-dimensional cube</title><categories>cs.CR math.CO</categories><msc-class>94A60, 94A17, 52B40, 68R10</msc-class><doi>10.1007/s10623-013-9888-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that for $d&gt;1$ the best information ratio of the perfect secret
sharing scheme based on the edge set of the $d$-dimensional cube is exactly
$d/2$. Using the technique developed, we also prove that the information ratio
of the infinite $d$-dimensional lattice is $d$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4645</identifier>
 <datestamp>2013-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4645</id><created>2013-10-17</created><authors><author><keyname>Lowery</keyname><forenames>Bradley R.</forenames></author><author><keyname>Langou</keyname><forenames>Julien</forenames></author></authors><title>A Greedy Algorithm for Optimally Pipelining a Reduction</title><categories>cs.DC</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collective communications are ubiquitous in parallel applications. We present
two new algorithms for performing a reduction. The operation associated with
our reduction needs to be associative and commutative. The two algorithms are
developed under two different communication models (unidirectional and
bidirectional). Both algorithms use a greedy scheduling scheme. For a
unidirectional, fully connected network, we prove that our greedy algorithm is
optimal when some realistic assumptions are respected. Previous algorithms fit
the same assumptions and are only appropriate for some given configurations.
Our algorithm is optimal for all configurations. We note that there are some
configuration where our greedy algorithm significantly outperform any existing
algorithms. This result represents a contribution to the state-of-the art. For
a bidirectional, fully connected network, we present a different greedy
algorithm. We verify by experimental simulations that our algorithm matches the
time complexity of an optimal broadcast (with addition of the computation).
Beside reversing an optimal broadcast algorithm, the greedy algorithm is the
first known reduction algorithm to experimentally attain this time complexity.
Simulations show that this greedy algorithm performs well in practice,
outperforming any state-of-the-art reduction algorithms. Positive experiments
on a parallel distributed machine are also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4647</identifier>
 <datestamp>2013-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4647</id><created>2013-10-17</created><authors><author><keyname>Jagtap</keyname><forenames>Sudhir B</forenames></author><author><keyname>G</keyname><forenames>Kodge B.</forenames></author></authors><title>Census Data Mining and Data Analysis using WEKA</title><categories>cs.DB cs.CY</categories><comments>06 pages, 03 figures. International Conference in Emerging Trends in
  Science, Technology and Management-2013, Singapore</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Data mining (also known as knowledge discovery from databases) is the process
of extraction of hidden, previously unknown and potentially useful information
from databases. The outcome of the extracted data can be analyzed for the
future planning and development perspectives. In this paper, we have made an
attempt to demonstrate how one can extract the local (district) level census,
socio-economic and population related other data for knowledge discovery and
their analysis using the powerful data mining tool Weka.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4652</identifier>
 <datestamp>2013-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4652</id><created>2013-10-17</created><authors><author><keyname>Csirmaz</keyname><forenames>Laszlo</forenames></author></authors><title>Gruppen secret sharing, or, how to share several secrets if you must?</title><categories>cs.CR</categories><msc-class>94A62, 90C25, 05B35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Each member of an $n$-person team has a secret, say a password. The $k$ out
of $n$ gruppen secret sharing requires that any group of $k$ members should be
able to recover the secrets of the other $n-k$ members, while any group of
$k-1$ or less members should have no information on the secret of other team
member even if other secrets leak out. We prove that when all secrets are
chosen independently and have size $s$, then each team member must have a share
of size at least $(n-k)s$, and we present a scheme which achieves this bound
when $s$ is large enough. This result shows a significant saving over $n$
independent applications of Shamir's $k$ out of $n-1$ threshold schemes which
assigns shares of size $(n-1)s$ to each team member independently of $k$.
  We also show how to set up such a scheme without any trusted dealer, and how
the secrets can be recovered, possibly multiple times, without leaking
information. We also discuss how our scheme fits to the much-investigated
multiple secret sharing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4656</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4656</id><created>2013-10-17</created><authors><author><keyname>Miyauchi</keyname><forenames>Atsushi</forenames></author><author><keyname>Sukegawa</keyname><forenames>Noriyoshi</forenames></author></authors><title>Maximizing Barber's bipartite modularity is also hard</title><categories>cs.SI cs.CC physics.soc-ph</categories><comments>18 pages, 1 figure</comments><journal-ref>Optimization Letters 9, 897-913 (2015)</journal-ref><doi>10.1007/s11590-014-0818-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modularity introduced by Newman and Girvan [Phys. Rev. E 69, 026113 (2004)]
is a quality function for community detection. Numerous methods for modularity
maximization have been developed so far. In 2007, Barber [Phys. Rev. E 76,
066102 (2007)] introduced a variant of modularity called bipartite modularity
which is appropriate for bipartite networks. Although maximizing the standard
modularity is known to be NP-hard, the computational complexity of maximizing
bipartite modularity has yet to be revealed. In this study, we prove that
maximizing bipartite modularity is also NP-hard. More specifically, we show the
NP-completeness of its decision version by constructing a reduction from a
classical partitioning problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4661</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4661</id><created>2013-10-17</created><updated>2015-02-02</updated><authors><author><keyname>Collier</keyname><forenames>Olivier</forenames></author><author><keyname>Dalalyan</keyname><forenames>Arnak S.</forenames></author></authors><title>Minimax rates in permutation estimation for feature matching</title><categories>math.ST cs.LG stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of matching two sets of features appears in various tasks of
computer vision and can be often formalized as a problem of permutation
estimation. We address this problem from a statistical point of view and
provide a theoretical analysis of the accuracy of several natural estimators.
To this end, the minimax rate of separation is investigated and its expression
is obtained as a function of the sample size, noise level and dimension. We
consider the cases of homoscedastic and heteroscedastic noise and establish, in
each case, tight upper bounds on the separation distance of several estimators.
These upper bounds are shown to be unimprovable both in the homoscedastic and
heteroscedastic settings. Interestingly, these bounds demonstrate that a phase
transition occurs when the dimension $d$ of the features is of the order of the
logarithm of the number of features $n$. For $d=O(\log n)$, the rate is
dimension free and equals $\sigma (\log n)^{1/2}$, where $\sigma$ is the noise
level. In contrast, when $d$ is larger than $c\log n$ for some constant $c&gt;0$,
the minimax rate increases with $d$ and is of the order $\sigma(d\log
n)^{1/4}$. We also discuss the computational aspects of the estimators and
provide empirical evidence of their consistency on synthetic data. Finally, we
show that our results extend to more general matching criteria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4664</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4664</id><created>2013-10-17</created><updated>2014-09-17</updated><authors><author><keyname>Bayramli</keyname><forenames>Burak</forenames></author></authors><title>SVD Factorization for Tall-and-Fat Matrices on Map/Reduce Architectures</title><categories>cs.DC</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We demonstrate an implementation for an approximate rank-k SVD factorization,
combining well-known randomized projection techniques with previously
implemented map/reduce solutions in order to compute steps of the random
projection based SVD procedure, such QR and SVD. We structure the problem in a
way that it reduces to Cholesky and SVD factorizations on $k \times k$ matrices
computed on a single machine, greatly easing the computability of the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4702</identifier>
 <datestamp>2013-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4702</id><created>2013-10-02</created><authors><author><keyname>Resch</keyname><forenames>Jason</forenames></author><author><keyname>Volvovski</keyname><forenames>Ilya</forenames></author></authors><title>Reliability Models for Highly Fault-tolerant Storage Systems</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We found that a reliability model commonly used to estimate
Mean-Time-To-Data-Loss (MTTDL), while suitable for modeling RAID 0 and RAID 5,
fails to accurately model systems having a fault-tolerance greater than 1.
Therefore, to model the reliability of RAID 6, Triple-Replication, or k-of-n
systems requires an alternate technique. In this paper, we explore some
alternatives, and evaluate their efficacy by comparing their predictions to
simulations. Our main result is a new formula which more accurately models
storage system reliability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4707</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4707</id><created>2013-10-17</created><authors><author><keyname>Zhang</keyname><forenames>Zi-Ke</forenames></author><author><keyname>Zhang</keyname><forenames>Chu-Xu</forenames></author><author><keyname>Han</keyname><forenames>Xiao-Pu</forenames></author><author><keyname>Liu</keyname><forenames>Chuang</forenames></author></authors><title>Emergence of Blind Areas in Information Spreading</title><categories>physics.soc-ph cs.SI</categories><doi>10.1371/journal.pone.0095785</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, contagion-based (disease, information, etc.) spreading on social
networks has been extensively studied. In this paper, other than traditional
full interaction, we propose a partial interaction based spreading model,
considering that the informed individuals would transmit information to only a
certain fraction of their neighbors due to the transmission ability in
real-world social networks. Simulation results on three representative networks
(BA, ER, WS) indicate that the spreading efficiency is highly correlated with
the network heterogeneity. In addition, a special phenomenon, namely
\emph{Information Blind Areas} where the network is separated by several
information-unreachable clusters, will emerge from the spreading process.
Furthermore, we also find that the size distribution of such information blind
areas obeys power-law-like distribution, which has very similar exponent with
that of site percolation. Detailed analyses show that the critical value is
decreasing along with the network heterogeneity for the spreading process,
which is complete the contrary to that of random selection. Moreover, the
critical value in the latter process is also larger that of the former for the
same network. Those findings might shed some lights in in-depth understanding
the effect of network properties on information spreading.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4713</identifier>
 <datestamp>2013-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4713</id><created>2013-10-17</created><authors><author><keyname>Chen</keyname><forenames>Junzhou</forenames></author><author><keyname>Wong</keyname><forenames>Kin Hong</forenames></author></authors><title>Calibration of an Articulated Camera System with Scale Factor Estimation</title><categories>cs.CV cs.CG</categories><comments>15 pages, 15 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Multiple Camera Systems (MCS) have been widely used in many vision
applications and attracted much attention recently. There are two principle
types of MCS, one is the Rigid Multiple Camera System (RMCS); the other is the
Articulated Camera System (ACS). In a RMCS, the relative poses (relative 3-D
position and orientation) between the cameras are invariant. While, in an ACS,
the cameras are articulated through movable joints, the relative pose between
them may change. Therefore, through calibration of an ACS we want to find not
only the relative poses between the cameras but also the positions of the
joints in the ACS.
  In this paper, we developed calibration algorithms for the ACS using a simple
constraint: the joint is fixed relative to the cameras connected with it during
the transformations of the ACS. When the transformations of the cameras in an
ACS can be estimated relative to the same coordinate system, the positions of
the joints in the ACS can be calculated by solving linear equations. However,
in a non-overlapping view ACS, only the ego-transformations of the cameras and
can be estimated. We proposed a two-steps method to deal with this problem. In
both methods, the ACS is assumed to have performed general transformations in a
static environment. The efficiency and robustness of the proposed methods are
tested by simulation and real experiments. In the real experiment, the
intrinsic and extrinsic parameters of the ACS are obtained simultaneously by
our calibration procedure using the same image sequences, no extra data
capturing step is required. The corresponding trajectory is recovered and
illustrated using the calibration results of the ACS. Since the estimated
translations of different cameras in an ACS may scaled by different scale
factors, a scale factor estimation algorithm is also proposed. To our
knowledge, we are the first to study the calibration of ACS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4716</identifier>
 <datestamp>2013-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4716</id><created>2013-10-17</created><authors><author><keyname>Papachristodoulou</keyname><forenames>Antonis</forenames></author><author><keyname>Anderson</keyname><forenames>James</forenames></author><author><keyname>Valmorbida</keyname><forenames>Giorgio</forenames></author><author><keyname>Prajna</keyname><forenames>Stephen</forenames></author><author><keyname>Seiler</keyname><forenames>Pete</forenames></author><author><keyname>Parrilo</keyname><forenames>Pablo</forenames></author></authors><title>SOSTOOLS Version 3.00 Sum of Squares Optimization Toolbox for MATLAB</title><categories>math.OC cs.MS cs.SY</categories><comments>64 pages, 3 figures, &quot;software available from
  http://sysos.eng.ox.ac.uk/sostools/ &quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  SOSTOOLS v3.00 is the latest release of the freely available MATLAB toolbox
for formulating and solving sum of squares (SOS) optimization problems. Such
problems arise naturally in the analysis and control of nonlinear dynamical
systems, but also in other areas such as combinatorial optimization. Highlights
of the new release include the ability to create polynomial matrices and
formulate polynomial matrix inequalities, compatibility with MuPAD, the new
MATLAB symbolic engine, as well as the multipoly toolbox v2.01. SOSTOOLS v3.00
can interface with five semidefinite programming solvers, and includes ten
demonstration examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4734</identifier>
 <datestamp>2013-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4734</id><created>2013-10-17</created><authors><author><keyname>Brim</keyname><forenames>Lubos</forenames></author><author><keyname>Ceska</keyname><forenames>Milan</forenames></author><author><keyname>Drazan</keyname><forenames>Sven</forenames></author><author><keyname>Safranek</keyname><forenames>David</forenames></author></authors><title>On Robustness Analysis of Stochastic Biochemical Systems by
  Probabilistic Model Checking</title><categories>cs.NA cs.CE cs.SY</categories><comments>43 pages, 15 figures, technical report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report proposes a novel framework for a rigorous robustness analysis of
stochastic biochemical systems. The technique is based on probabilistic model
checking. We adapt the general definition of robustness introduced by Kitano to
the class of stochastic systems modelled as continuous time Markov Chains in
order to extensively analyse and compare robustness of biological models with
uncertain parameters. The framework utilises novel computational methods that
enable to effectively evaluate the robustness of models with respect to
quantitative temporal properties and parameters such as reaction rate constants
and initial conditions.
  The framework is applied to gene regulation as an example of a central
biological mechanism where intrinsic and extrinsic stochasticity plays crucial
role due to low numbers of DNA and RNA molecules. Using our methods we have
obtained a comprehensive and precise analysis of stochastic dynamics under
parameter uncertainty. Furthermore, we apply our framework to compare several
variants of two-component signalling networks from the perspective of
robustness with respect to intrinsic noise caused by low populations of
signalling components. We succeeded to extend previous studies performed on
deterministic models (ODE) and show that stochasticity may significantly affect
obtained predictions. Our case studies demonstrate that the framework can
provide deeper insight into the role of key parameters in maintaining the
system functionality and thus it significantly contributes to formal methods in
computational systems biology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4743</identifier>
 <datestamp>2013-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4743</id><created>2013-10-17</created><authors><author><keyname>Rao</keyname><forenames>M.</forenames></author><author><keyname>Rigo</keyname><forenames>M.</forenames></author><author><keyname>Salimov</keyname><forenames>P.</forenames></author></authors><title>Avoiding 2-binomial squares and cubes</title><categories>cs.FL math.CO</categories><msc-class>68R15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two finite words $u,v$ are 2-binomially equivalent if, for all words $x$ of
length at most 2, the number of occurrences of $x$ as a (scattered) subword of
$u$ is equal to the number of occurrences of $x$ in $v$. This notion is a
refinement of the usual abelian equivalence. A 2-binomial square is a word $uv$
where $u$ and $v$ are 2-binomially equivalent.
  In this paper, considering pure morphic words, we prove that 2-binomial
squares (resp. cubes) are avoidable over a 3-letter (resp. 2-letter) alphabet.
The sizes of the alphabets are optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4753</identifier>
 <datestamp>2013-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4753</id><created>2013-10-17</created><updated>2013-10-17</updated><authors><author><keyname>Gabora</keyname><forenames>Liane</forenames></author><author><keyname>Firouzi</keyname><forenames>Hadi</forenames></author></authors><title>Society Functions Best with an Intermediate Level of Creativity</title><categories>cs.MA q-bio.NC</categories><comments>6 pages. arXiv admin note: text overlap with arXiv:1310.4086,
  arXiv:1310.3781</comments><journal-ref>Proceedings of the Annual Meeting of the Cognitive Science Society
  (pp. 1578-1583). August 1-4, Sapporo Japan. Houston TX: Cognitive Science
  Society (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a society, a proportion of the individuals can benefit from creativity
without being creative themselves by copying the creators. This paper uses an
agent-based model of cultural evolution to investigate how society is affected
by different levels of individual creativity. We performed a time series
analysis of the mean fitness of ideas across the artificial society varying
both the percentage of creators, C, and how creative they are, p using two
discounting methods. Both analyses revealed a valley in the adaptive landscape,
indicating a tradeoff between C and p. The results suggest that excess
creativity at the individual level can be detrimental at the level of the
society because creators invest in unproven ideas at the expense of propagating
proven ideas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4756</identifier>
 <datestamp>2013-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4756</id><created>2013-10-17</created><authors><author><keyname>Wotzlaw</keyname><forenames>Andreas</forenames></author><author><keyname>van der Grinten</keyname><forenames>Alexander</forenames></author><author><keyname>Speckenmeyer</keyname><forenames>Ewald</forenames></author></authors><title>Effectiveness of pre- and inprocessing for CDCL-based SAT solving</title><categories>cs.LO cs.AI</categories><comments>9 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Applying pre- and inprocessing techniques to simplify CNF formulas both
before and during search can considerably improve the performance of modern SAT
solvers. These algorithms mostly aim at reducing the number of clauses,
literals, and variables in the formula. However, to be worthwhile, it is
necessary that their additional runtime does not exceed the runtime saved
during the subsequent SAT solver execution. In this paper we investigate the
efficiency and the practicability of selected simplification algorithms for
CDCL-based SAT solving. We first analyze them by means of their expected impact
on the CNF formula and SAT solving at all. While testing them on real-world and
combinatorial SAT instances, we show which techniques and combinations of them
yield a desirable speedup and which ones should be avoided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4759</identifier>
 <datestamp>2013-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4759</id><created>2013-10-17</created><authors><author><keyname>G&#xf6;ring</keyname><forenames>Christoph</forenames></author><author><keyname>Freytag</keyname><forenames>Alexander</forenames></author><author><keyname>Rodner</keyname><forenames>Erik</forenames></author><author><keyname>Denzler</keyname><forenames>Joachim</forenames></author></authors><title>Fine-grained Categorization -- Short Summary of our Entry for the
  ImageNet Challenge 2012</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we tackle the problem of visual categorization of dog breeds,
which is a surprisingly challenging task due to simultaneously present low
interclass distances and high intra-class variances. Our approach combines
several techniques well known in our community but often not utilized for
fine-grained recognition:
  (1) automatic segmentation, (2) efficient part detection, and (3) combination
of multiple features. In particular, we demonstrate that a simple head detector
embedded in an off-the-shelf recognition pipeline can improve recognition
accuracy quite significantly, highlighting the importance of part features for
fine-grained recognition tasks. Using our approach, we achieved a 24.59% mean
average precision performance on the Stanford dog dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4761</identifier>
 <datestamp>2013-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4761</id><created>2013-10-17</created><authors><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Andreopoulos</keyname><forenames>Yiannis</forenames></author><author><keyname>Wassell</keyname><forenames>Ian J.</forenames></author><author><keyname>Rodrigues</keyname><forenames>Miguel R. D.</forenames></author></authors><title>Towards Energy Neutrality in Energy Harvesting Wireless Sensor Networks:
  A Case for Distributed Compressive Sensing?</title><categories>cs.IT cs.NI math.IT</categories><comments>6 pages. This work will be presented at the 2013 IEEE Global
  Communications Conference (GLOBECOM), Atlanta, US, December 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper advocates the use of the emerging distributed compressive sensing
(DCS) paradigm in order to deploy energy harvesting (EH) wireless sensor
networks (WSN) with practical network lifetime and data gathering rates that
are substantially higher than the state-of-the-art. In particular, we argue
that there are two fundamental mechanisms in an EH WSN: i) the energy diversity
associated with the EH process that entails that the harvested energy can vary
from sensor node to sensor node, and ii) the sensing diversity associated with
the DCS process that entails that the energy consumption can also vary across
the sensor nodes without compromising data recovery. We also argue that such
mechanisms offer the means to match closely the energy demand to the energy
supply in order to unlock the possibility for energy-neutral WSNs that leverage
EH capability. A number of analytic and simulation results are presented in
order to illustrate the potential of the approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4774</identifier>
 <datestamp>2013-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4774</id><created>2013-10-17</created><authors><author><keyname>Bhute</keyname><forenames>Avinash N</forenames></author><author><keyname>Meshram</keyname><forenames>B. B.</forenames></author></authors><title>IntelligentWeb Agent for Search Engines</title><categories>cs.IR</categories><comments>5 pages. International Conference on Trends and Advances in
  Computation and Engineering TRACE -2010. arXiv admin note: text overlap with
  arXiv:1205.2891 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we review studies of the growth of the Internet and
technologies that are useful for information search and retrieval on the Web.
Search engines are retrieve the efficient information. We collected data on the
Internet from several different sources, e.g., current as well as projected
number of users, hosts, and Web sites. The trends cited by the sources are
consistent and point to exponential growth in the past and in the coming
decade. Hence it is not surprising that about 85% of Internet users surveyed
claim using search engines and search services to find specific information and
users are not satisfied with the performance of the current generation of
search engines; the slow retrieval speed, communication delays, and poor
quality of retrieved results. Web agents, programs acting autonomously on some
task, are already present in the form of spiders, crawler, and robots. Agents
offer substantial benefits and hazards, and because of this, their development
must involve attention to technical details. This paper illustrates the
different types of agents,crawlers, robots,etc for mining the contents of web
in a methodical, automated manner, also discusses the use of crawler to gather
specific types of information from Web pages, such as harvesting e-mail
addresses
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4777</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4777</id><created>2013-10-17</created><updated>2014-01-20</updated><authors><author><keyname>Park</keyname><forenames>Jihong</forenames></author><author><keyname>Kim</keyname><forenames>Seong-Lyun</forenames></author></authors><title>Content-Specific Broadcast Cellular Networks based on User Demand
  Prediction: A Revenue Perspective</title><categories>cs.NI</categories><comments>6 pages; This paper will appear in the Proc. of IEEE WCNC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Long Term Evolution (LTE) broadcast is a promising solution to cope with
exponentially increasing user traffic by broadcasting common user requests over
the same frequency channels. In this paper, we propose a novel network
framework provisioning broadcast and unicast services simultaneously. For each
serving file to users, a cellular base station determines either to broadcast
or unicast the file based on user demand prediction examining the file's
content specific characteristics such as: file size, delay tolerance, price
sensitivity. In a network operator's revenue maximization perspective while not
inflicting any user payoff degradation, we jointly optimize resource
allocation, pricing, and file scheduling. In accordance with the state of the
art LTE specifications, the proposed network demonstrates up to 32% increase in
revenue for a single cell and more than a 7-fold increase for a 7 cell
coordinated LTE broadcast network, compared to the conventional unicast
cellular networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4780</identifier>
 <datestamp>2014-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4780</id><created>2013-10-17</created><updated>2014-04-11</updated><authors><author><keyname>Cheney</keyname><forenames>James</forenames></author><author><keyname>Lindley</keyname><forenames>Sam</forenames></author><author><keyname>Radanne</keyname><forenames>Gabriel</forenames></author><author><keyname>Wadler</keyname><forenames>Philip</forenames></author></authors><title>Effective Quotation: relating approaches to language-integrated query</title><categories>cs.PL</categories><comments>Proceedings of the ACM SIGPLAN 2014 Workshop on Partial Evaluation
  and Program Manipulation, January 20-21, 2014, San Diego, CA, USA. Copyright
  is held by the owner/author(s). Publication rights licensed to ACM</comments><acm-class>D.3.1; D.3.2; H.2.3</acm-class><doi>10.1145/2543728.2543738</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Language-integrated query techniques have been explored in a number of
different language designs. We consider two different, type-safe approaches
employed by Links and F#. Both approaches provide rich dynamic query generation
capabilities, and thus amount to a form of heterogeneous staged computation,
but to date there has been no formal investigation of their relative
expressiveness. We present two core calculi Eff and Quot, respectively
capturing the essential aspects of language-integrated querying using effects
in Links and quotation in LINQ. We show via translations from Eff to Quot and
back that the two approaches are equivalent in expressiveness. Based on the
translation from Eff to Quot, we extend a simple Links compiler to handle
queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4802</identifier>
 <datestamp>2013-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4802</id><created>2013-10-16</created><authors><author><keyname>Martinez-Palau</keyname><forenames>Xavier</forenames></author><author><keyname>Dominguez-Sal</keyname><forenames>David</forenames></author><author><keyname>Akbarinia</keyname><forenames>Reza</forenames></author><author><keyname>Valduriez</keyname><forenames>Patrick</forenames></author><author><keyname>Larriba-Pey</keyname><forenames>Josep Llu&#xed;s</forenames></author></authors><title>On Demand Memory Specialization for Distributed Graph Databases</title><categories>cs.DB cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose the DN-tree that is a data structure to build lossy
summaries of the frequent data access patterns of the queries in a distributed
graph data management system. These compact representations allow us an
efficient communication of the data structure in distributed systems. We
exploit this data structure with a new \textit{Dynamic Data Partitioning}
strategy (DYDAP) that assigns the portions of the graph according to historical
data access patterns, and guarantees a small network communication and a
computational load balance in distributed graph queries. This method is able to
adapt dynamically to new workloads and evolve when the query distribution
changes. Our experiments show that DYDAP yields a throughput up to an order of
magnitude higher than previous methods based on cache specialization, in a
variety of scenarios, and the average response time of the system is divided by
two.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4807</identifier>
 <datestamp>2013-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4807</id><created>2013-10-16</created><authors><author><keyname>Chen</keyname><forenames>Sheng-Gwo</forenames></author><author><keyname>Chi</keyname><forenames>Mei-Hsiu</forenames></author><author><keyname>Wu</keyname><forenames>Jyh-Yang</forenames></author></authors><title>High-order algorithms for solving eigenproblems over discrete surfaces</title><categories>cs.NA math.NA physics.comp-ph</categories><comments>29pages, 19figures, 2tables. arXiv admin note: text overlap with
  arXiv:1004.3486</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The eigenvalue problem of the Laplace-Beltrami operators on curved surfaces
plays an essential role in the convergence analysis of the numerical
simulations of some important geometric partial differential equations which
involve this operator. In this note we shall combine the local tangential
lifting (LTL) method with the configuration equation to develop a new effective
and convergent algorithm to solve the eigenvalue problems of the
Laplace-Beltrami operators acting on functions over discrete surfaces. The
convergence rates of our algorithms of discrete Laplace-Beltrami operators over
surfaces is $O(r^n)$, $n \geq 1$, where $r$ represents the size of the mesh of
discretization of the surface. The problem of high-order accuracies will also
be discussed and used to compute geometric invariants of the underlying
surfaces. Some convergence tests and eigenvalue computations on the sphere,
tori and a dumbbell are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4822</identifier>
 <datestamp>2014-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4822</id><created>2013-10-17</created><updated>2014-01-31</updated><authors><author><keyname>Escalante</keyname><forenames>Hugo Jair</forenames></author><author><keyname>Guyon</keyname><forenames>Isabelle</forenames></author><author><keyname>Athitsos</keyname><forenames>Vassilis</forenames></author><author><keyname>Jangyodsuk</keyname><forenames>Pat</forenames></author><author><keyname>Wan</keyname><forenames>Jun</forenames></author></authors><title>Principal motion components for gesture recognition using a
  single-example</title><categories>cs.CV</categories><msc-class>68T45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces principal motion components (PMC), a new method for
one-shot gesture recognition. In the considered scenario a single
training-video is available for each gesture to be recognized, which limits the
application of traditional techniques (e.g., HMMs). In PMC, a 2D map of motion
energy is obtained per each pair of consecutive frames in a video. Motion maps
associated to a video are processed to obtain a PCA model, which is used for
recognition under a reconstruction-error approach. The main benefits of the
proposed approach are its simplicity, easiness of implementation, competitive
performance and efficiency. We report experimental results in one-shot gesture
recognition using the ChaLearn Gesture Dataset; a benchmark comprising more
than 50,000 gestures, recorded as both RGB and depth video with a Kinect
camera. Results obtained with PMC are competitive with alternative methods
proposed for the same data set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4843</identifier>
 <datestamp>2013-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4843</id><created>2013-10-17</created><authors><author><keyname>Erd&#x151;s</keyname><forenames>D&#xf3;ra</forenames></author><author><keyname>Miettinen</keyname><forenames>Pauli</forenames></author></authors><title>Scalable Boolean Tensor Factorizations using Random Walks</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tensors are becoming increasingly common in data mining, and consequently,
tensor factorizations are becoming more and more important tools for data
miners. When the data is binary, it is natural to ask if we can factorize it
into binary factors while simultaneously making sure that the reconstructed
tensor is still binary. Such factorizations, called Boolean tensor
factorizations, can provide improved interpretability and find Boolean
structure that is hard to express using normal factorizations. Unfortunately
the algorithms for computing Boolean tensor factorizations do not usually scale
well. In this paper we present a novel algorithm for finding Boolean CP and
Tucker decompositions of large and sparse binary tensors. In our experimental
evaluation we show that our algorithm can handle large tensors and accurately
reconstructs the latent Boolean structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4849</identifier>
 <datestamp>2015-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4849</id><created>2013-10-17</created><updated>2015-03-06</updated><authors><author><keyname>Waegeman</keyname><forenames>Willem</forenames></author><author><keyname>Dembczynski</keyname><forenames>Krzysztof</forenames></author><author><keyname>Jachnik</keyname><forenames>Arkadiusz</forenames></author><author><keyname>Cheng</keyname><forenames>Weiwei</forenames></author><author><keyname>Hullermeier</keyname><forenames>Eyke</forenames></author></authors><title>On the Bayes-optimality of F-measure maximizers</title><categories>stat.ML cs.LG</categories><journal-ref>JMLR 15 (2014) 3333-3388</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The F-measure, which has originally been introduced in information retrieval,
is nowadays routinely used as a performance metric for problems such as binary
classification, multi-label classification, and structured output prediction.
Optimizing this measure is a statistically and computationally challenging
problem, since no closed-form solution exists. Adopting a decision-theoretic
perspective, this article provides a formal and experimental analysis of
different approaches for maximizing the F-measure. We start with a Bayes-risk
analysis of related loss functions, such as Hamming loss and subset zero-one
loss, showing that optimizing such losses as a surrogate of the F-measure leads
to a high worst-case regret. Subsequently, we perform a similar type of
analysis for F-measure maximizing algorithms, showing that such algorithms are
approximate, while relying on additional assumptions regarding the statistical
distribution of the binary response variables. Furthermore, we present a new
algorithm which is not only computationally efficient but also Bayes-optimal,
regardless of the underlying distribution. To this end, the algorithm requires
only a quadratic (with respect to the number of binary responses) number of
parameters of the joint distribution. We illustrate the practical performance
of all analyzed methods by means of experiments with multi-label classification
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4852</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4852</id><created>2013-10-17</created><authors><author><keyname>Brough</keyname><forenames>Tara</forenames></author><author><keyname>Cain</keyname><forenames>Alan J.</forenames></author></authors><title>Automaton semigroup constructions</title><categories>math.GR cs.FL</categories><comments>13 pages; 2 figures</comments><msc-class>20M35, 68Q45</msc-class><journal-ref>Semigroup Forum (2015) 90:763-774</journal-ref><doi>10.1007/s00233-014-9632-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to investigate whether the class of automaton
semigroups is closed under certain semigroup constructions. We prove that the
free product of two automaton semigroups that contain left identities is again
an automaton semigroup. We also show that the class of automaton semigroups is
closed under the combined operation of 'free product followed by adjoining an
identity'. We present an example of a free product of finite semigroups that we
conjecture is not an automaton semigroup. Turning to wreath products, we
consider two slight generalizations of the concept of an automaton semigroup,
and show that a wreath product of an automaton monoid and a finite monoid
arises as a generalized automaton semigroup in both senses. We also suggest a
potential counterexample that would show that a wreath product of an automaton
monoid and a finite monoid is not a necessarily an automaton monoid in the
usual sense.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4856</identifier>
 <datestamp>2013-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4856</id><created>2013-10-17</created><authors><author><keyname>Klimann</keyname><forenames>Ines</forenames></author><author><keyname>Mairesse</keyname><forenames>Jean</forenames></author><author><keyname>Picantin</keyname><forenames>Matthieu</forenames></author></authors><title>Implementing Computations in Automaton (Semi)groups</title><categories>cs.FL math.GR</categories><comments>12 pages, 4 figures, 3 tables, CIAA 2012</comments><journal-ref>CIAA 2012, LNCS 7381, pp.240-252, 2012. (N. Moreira and R.Reis
  Eds.)</journal-ref><doi>10.1142/S021819671250052X</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  We consider the growth, order, and finiteness problems for automaton
(semi)groups. We propose new implementations and compare them with the existing
ones. As a result of extensive experimentations, we propose some conjectures on
the order of finite automaton (semi)groups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4859</identifier>
 <datestamp>2013-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4859</id><created>2013-10-17</created><authors><author><keyname>Ali</keyname><forenames>Omar</forenames></author><author><keyname>Ayoub</keyname><forenames>Mahmoud F.</forenames></author><author><keyname>Youssef</keyname><forenames>Moustafa</forenames></author></authors><title>Practical Provably Secure Multi-node Communication</title><categories>cs.CR</categories><comments>Proceedings of the IEEE International Conference on Computing,
  Networking and Communications (ICNC 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a practical and provably-secure multimode communication scheme in
the presence of a passive eavesdropper. The scheme is based on a random
scheduling approach that hides the identity of the transmitter from the
eavesdropper. This random scheduling leads to ambiguity at the eavesdropper
with regard to the origin of the transmitted frame. We present the details of
the technique and analyze it to quantify the secrecy-fairness-overhead
trade-off. Implementation of the scheme over Crossbow Telosb motes, equipped
with CC2420 radio chips, shows that the scheme can achieve significant secrecy
gain with vanishing outage probability. In addition, it has significant
overhead advantage over direct extensions to two-nodes schemes. The technique
also has the advantage of allowing inactive nodes to leverage sleep mode to
further save energy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4874</identifier>
 <datestamp>2013-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4874</id><created>2013-10-17</created><authors><author><keyname>Wang</keyname><forenames>Chenlan</forenames></author><author><keyname>Doan</keyname><forenames>Xuan Vinh</forenames></author><author><keyname>Chen</keyname><forenames>Bo</forenames></author></authors><title>Price of Anarchy for Non-atomic Congestion Games with Stochastic Demands</title><categories>cs.GT math.OC</categories><comments>31 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalize the notions of user equilibrium and system optimum to
non-atomic congestion games with stochastic demands. We establish upper bounds
on the price of anarchy for three different settings of link cost functions and
demand distributions, namely, (a) affine cost functions and general
distributions, (b) polynomial cost functions and general positive-valued
distributions, and (c) polynomial cost functions and the normal distributions.
All the upper bounds are tight in some special cases, including the case of
deterministic demands.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4880</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4880</id><created>2013-10-17</created><updated>2015-11-30</updated><authors><author><keyname>Rana</keyname><forenames>Rajib</forenames></author><author><keyname>Austin</keyname><forenames>Daniel</forenames></author><author><keyname>Jacobs</keyname><forenames>Peter G.</forenames></author><author><keyname>Karunanithi</keyname><forenames>Mohanraj</forenames></author><author><keyname>Kaye</keyname><forenames>Jeffrey</forenames></author></authors><title>Gait Velocity Estimation using time interleaved between Consecutive
  Passive IR Sensor Activations</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gait velocity has been consistently shown to be an important indicator and
predictor of health status, especially in older adults. It is often assessed
clinically, but the assessments occur infrequently and do not allow optimal
detection of key health changes when they occur. In this paper, we show that
the time gap between activations of a pair of Passive Infrared (PIR) motion
sensors installed in the consecutively visited room pair carry rich latent
information about a person's gait velocity. We name this time gap transition
time and show that despite a six second refractory period of the PIR sensors,
transition time can be used to obtain an accurate representation of gait
velocity.
  Using a Support Vector Regression (SVR) approach to model the relationship
between transition time and gait velocity, we show that gait velocity can be
estimated with an average error less than 2.5 cm/sec. This is demonstrated with
data collected over a 5 year period from 74 older adults monitored in their own
homes.
  This method is simple and cost effective and has advantages over competing
approaches such as: obtaining 20 to 100x more gait velocity measurements per
day and offering the fusion of location-specific information with time stamped
gait estimates. These advantages allow stable estimates of gait parameters
(maximum or average speed, variability) at shorter time scales than current
approaches. This also provides a pervasive in-home method for context-aware
gait velocity sensing that allows for monitoring of gait trajectories in space
and time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4891</identifier>
 <datestamp>2013-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4891</id><created>2013-10-17</created><authors><author><keyname>Harandi</keyname><forenames>Mehrtash</forenames></author><author><keyname>Sanderson</keyname><forenames>Conrad</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Lovell</keyname><forenames>Brian C.</forenames></author></authors><title>Dictionary Learning and Sparse Coding on Grassmann Manifolds: An
  Extrinsic Solution</title><categories>cs.CV</categories><comments>9 pages. Appearing in Int. Conf. Computer Vision, 2013, Australia</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Recent advances in computer vision and machine learning suggest that a wide
range of problems can be addressed more appropriately by considering
non-Euclidean geometry. In this paper we explore sparse dictionary learning
over the space of linear subspaces, which form Riemannian structures known as
Grassmann manifolds. To this end, we propose to embed Grassmann manifolds into
the space of symmetric matrices by an isometric mapping, which enables us to
devise a closed-form solution for updating a Grassmann dictionary, atom by
atom. Furthermore, to handle non-linearity in data, we propose a kernelised
version of the dictionary learning algorithm. Experiments on several
classification tasks (face recognition, action recognition, dynamic texture
classification) show that the proposed approach achieves considerable
improvements in discrimination accuracy, in comparison to state-of-the-art
methods such as kernelised Affine Hull Method and graph-embedding Grassmann
discriminant analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4894</identifier>
 <datestamp>2013-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4894</id><created>2013-10-17</created><authors><author><keyname>Preciado</keyname><forenames>Victor M.</forenames></author><author><keyname>Zargham</keyname><forenames>Michael</forenames></author><author><keyname>Sun</keyname><forenames>David</forenames></author></authors><title>Traffic Control for Network Protection Against Spreading Processes</title><categories>cs.SY cs.SI math.OC</categories><comments>arXiv admin note: text overlap with arXiv:1309.6270</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Epidemic outbreaks in human populations are facilitated by the underlying
transportation network. We consider strategies for containing a viral spreading
process by optimally allocating a limited budget to three types of protection
resources: (i) Traffic control resources, (ii), preventative resources and
(iii) corrective resources. Traffic control resources are employed to impose
restrictions on the traffic flowing across directed edges in the transportation
network. Preventative resources are allocated to nodes to reduce the
probability of infection at that node (e.g. vaccines), and corrective resources
are allocated to nodes to increase the recovery rate at that node (e.g.
antidotes). We assume these resources have monetary costs associated with them,
from which we formalize an optimal budget allocation problem which maximizes
containment of the infection. We present a polynomial time solution to the
optimal budget allocation problem using Geometric Programming (GP) for an
arbitrary weighted and directed contact network and a large class of resource
cost functions. We illustrate our approach by designing optimal traffic control
strategies to contain an epidemic outbreak that propagates through a real-world
air transportation network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4896</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4896</id><created>2013-10-17</created><authors><author><keyname>Ilic</keyname><forenames>Velimir M.</forenames></author><author><keyname>Stankovic</keyname><forenames>Miomir S.</forenames></author></authors><title>A unified characterization of generalized information and certainty
  measures</title><categories>cs.IT math.IT</categories><doi>10.1016/j.physa.2014.07.061</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the axiomatic characterization of information and
certainty measures in a unified way. We present the general axiomatic system
which captures the common properties of a large number of the measures
previously considered by numerous authors. We provide the corresponding
characterization theorems and define a new generalized measure called the
Inforcer, which is the quasi-linear mean of the function associated to the
event probability following the general composition law. In particular, we pay
attention to the polynomial composition and the corresponding polynomially
composable Inforcer measure. The most common measures appearing in literature
can be obtained by specific choice of parameters appearing in our generic
measures and they are listed in tables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4899</identifier>
 <datestamp>2013-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4899</id><created>2013-10-17</created><authors><author><keyname>Wu</keyname><forenames>Zhengwei</forenames></author><author><keyname>Preciado</keyname><forenames>Victor M.</forenames></author></authors><title>Laplacian Spectral Properties of Graphs from Random Local Samples</title><categories>cs.SI cs.DM math.OC</categories><comments>arXiv admin note: text overlap with arXiv:1209.0341</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Laplacian eigenvalues of a network play an important role in the analysis
of many structural and dynamical network problems. In this paper, we study the
relationship between the eigenvalue spectrum of the normalized Laplacian matrix
and the structure of `local' subgraphs of the network. We call a subgraph
\emph{local} when it is induced by the set of nodes obtained from a
breath-first search (BFS) of radius $r$ around a node. In this paper, we
propose techniques to estimate spectral properties of the normalized Laplacian
matrix from a random collection of induced local subgraphs. In particular, we
provide an algorithm to estimate the spectral moments of the normalized
Laplacian matrix (the power-sums of its eigenvalues). Moreover, we propose a
technique, based on convex optimization, to compute upper and lower bounds on
the spectral radius of the normalized Laplacian matrix from local subgraphs. We
illustrate our results studying the normalized Laplacian spectrum of a
large-scale online social network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4900</identifier>
 <datestamp>2013-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4900</id><created>2013-10-17</created><authors><author><keyname>Otsuki</keyname><forenames>Akira</forenames></author><author><keyname>Kawamura</keyname><forenames>Masayoshi</forenames></author></authors><title>GV-Index:Scientific Contribution Rating Index That Takes into Account
  the Growth Degree of Research Area and Variance Values of the Publication
  Year of Cited Paper</title><categories>cs.DL</categories><comments>11 pages, 9 figures, 8 tables</comments><journal-ref>International Journal of Data Mining &amp; Knowledge Management
  Process (IJDKP) Vol.3, No.5, September 2013</journal-ref><doi>10.5121/ijdkp.2013.3501</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  There are a wide variety of scientific contribution rating indices including
the impact factor and h-index. These are used for quantitative analyses on
research papers published in the past, and therefore unable to incorporate in
the assessment the growth, or deterioration, of the research area: whether the
research area of a particular paper is in decline or conversely in a growing
trend. Other hand, the use of the conventional rating indices may result in
higher rates for papers that are hardly referenced nowadays in other papers
although frequently cited in the past. This study proposes a new type of
scientific contribution ranking index, &quot;Growing Degree of Research Area and
Variance Values Index (GV-Index)&quot;. The GV-Index is computed by a principal
component analysis based on an estimated value obtained by PageRank Algorithm,
which takes into account the growing degree of the research area and its
variance. We also propose visualization system of a scientist's network using
the GV-Index.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4904</identifier>
 <datestamp>2013-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4904</id><created>2013-10-17</created><authors><author><keyname>Otsuki</keyname><forenames>Akira</forenames></author></authors><title>Dynamic Extraction of Key Paper from the Cluster Using Variance Values
  of Cited Literature</title><categories>cs.DL</categories><comments>12 pages, 8 figures, 3 tables</comments><journal-ref>International Journal of Data Mining &amp; Knowledge Management
  Process (IJDKP) Vol.3, No.5, September 2013</journal-ref><doi>10.5121/ijdkp.2013.3506</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  When looking into recent research trends in the field of academic landscape,
citation network analysis is common and automated clustering of many academic
papers has been achieved by making good use of various techniques. However,
specifying the features of each area identified by automated clustering or
dynamically extracted key papers in each research area has not yet been
achieved. In this study, therefore, we propose a method for dynamically
specifying the key papers in each area identified by clustering. We will
investigate variance values of the publication year of the cited literature and
calculate each cited paper's importance by applying the variance values to the
PageRank algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4906</identifier>
 <datestamp>2013-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4906</id><created>2013-10-18</created><authors><author><keyname>Sharma</keyname><forenames>Gokarna</forenames><affiliation>Louisiana State University</affiliation></author><author><keyname>Busch</keyname><forenames>Costas</forenames><affiliation>Louisiana State University</affiliation></author></authors><title>Distributed Queuing in Dynamic Networks</title><categories>cs.DC</categories><comments>In Proceedings FOMC 2013, arXiv:1310.4595</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 132, 2013, pp. 1-19</journal-ref><doi>10.4204/EPTCS.132.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of forming a distributed queue in the adversarial
dynamic network model of Kuhn, Lynch, and Oshman (STOC 2010) in which the
network topology changes from round to round but the network stays connected.
This is a synchronous model in which network nodes are assumed to be fixed, the
communication links for each round are chosen by an adversary, and nodes do not
know who their neighbors are for the current round before they broadcast their
messages. Queue requests may arrive over rounds at arbitrary nodes and the goal
is to eventually enqueue them in a distributed queue. We present two algorithms
that give a total distributed ordering of queue requests in this model. We
measure the performance of our algorithms through round complexity, which is
the total number of rounds needed to solve the distributed queuing problem. We
show that in 1-interval connected graphs, where the communication links change
arbitrarily between every round, it is possible to solve the distributed
queueing problem in O(nk) rounds using O(log n) size messages, where n is the
number of nodes in the network and k &lt;= n is the number of queue requests.
Further, we show that for more stable graphs, e.g. T-interval connected graphs
where the communication links change in every T rounds, the distributed queuing
problem can be solved in O(n+ (nk/min(alpha,T))) rounds using the same O(log n)
size messages, where alpha &gt; 0 is the concurrency level parameter that captures
the minimum number of active queue requests in the system in any round. These
results hold in any arbitrary (sequential, one-shot concurrent, or dynamic)
arrival of k queue requests in the system. Moreover, our algorithms ensure
correctness in the sense that each queue request is eventually enqueued in the
distributed queue after it is issued and each queue request is enqueued exactly
once. We also provide an impossibility result for this distributed queuing
problem in this model. To the best of our knowledge, these are the first
solutions to the distributed queuing problem in adversarial dynamic networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4907</identifier>
 <datestamp>2013-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4907</id><created>2013-10-18</created><authors><author><keyname>Levin</keyname><forenames>Liron</forenames><affiliation>Communication Systems Engineering Department, Ben-Gurion University of the Negev, Israel</affiliation></author><author><keyname>Kowalski</keyname><forenames>Dariusz R.</forenames><affiliation>Department of Computer Science, University of Liverpool, UK</affiliation></author><author><keyname>Segal</keyname><forenames>Michael</forenames><affiliation>Communication Systems Engineering Department, Ben-Gurion University of the Negev, Israel</affiliation></author></authors><title>Message and time efficient multi-broadcast schemes</title><categories>cs.DC cs.NI</categories><comments>In Proceedings FOMC 2013, arXiv:1310.4595</comments><proxy>EPTCS</proxy><acm-class>C.2.4</acm-class><journal-ref>EPTCS 132, 2013, pp. 21-37</journal-ref><doi>10.4204/EPTCS.132.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider message and time efficient broadcasting and multi-broadcasting in
wireless ad-hoc networks, where a subset of nodes, each with a unique rumor,
wish to broadcast their rumors to all destinations while minimizing the total
number of transmissions and total time until all rumors arrive to their
destination. Under centralized settings, we introduce a novel approximation
algorithm that provides almost optimal results with respect to the number of
transmissions and total time, separately. Later on, we show how to efficiently
implement this algorithm under distributed settings, where the nodes have only
local information about their surroundings. In addition, we show multiple
approximation techniques based on the network collision detection capabilities
and explain how to calibrate the algorithms' parameters to produce optimal
results for time and messages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4908</identifier>
 <datestamp>2013-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4908</id><created>2013-10-18</created><authors><author><keyname>Augustine</keyname><forenames>John</forenames><affiliation>Indian Institute of Technology Madras</affiliation></author><author><keyname>Kulkarni</keyname><forenames>Tejas</forenames><affiliation>Indian Institute of Technology Madras</affiliation></author><author><keyname>Nakhe</keyname><forenames>Paresh</forenames><affiliation>Indian Institute of Technology Madras</affiliation></author><author><keyname>Robinson</keyname><forenames>Peter</forenames><affiliation>Nanyang Technological University</affiliation></author></authors><title>Robust Leader Election in a Fast-Changing World</title><categories>cs.DC cs.DS</categories><comments>In Proceedings FOMC 2013, arXiv:1310.4595</comments><proxy>EPTCS</proxy><acm-class>C.2.4; F.2.2</acm-class><journal-ref>EPTCS 132, 2013, pp. 38-49</journal-ref><doi>10.4204/EPTCS.132.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of electing a leader among nodes in a highly dynamic
network where the adversary has unbounded capacity to insert and remove nodes
(including the leader) from the network and change connectivity at will. We
present a randomized Las Vegas algorithm that (re)elects a leader in O(D\log n)
rounds with high probability, where D is a bound on the dynamic diameter of the
network and n is the maximum number of nodes in the network at any point in
time. We assume a model of broadcast-based communication where a node can send
only 1 message of O(\log n) bits per round and is not aware of the receivers in
advance. Thus, our results also apply to mobile wireless ad-hoc networks,
improving over the optimal (for deterministic algorithms) O(Dn) solution
presented at FOMC 2011. We show that our algorithm is optimal by proving that
any randomized Las Vegas algorithm takes at least omega(D\log n) rounds to
elect a leader with high probability, which shows that our algorithm yields the
best possible (up to constants) termination time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4909</identifier>
 <datestamp>2013-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4909</id><created>2013-10-18</created><authors><author><keyname>Elayidom</keyname><forenames>M. Sudheep</forenames></author><author><keyname>Jose</keyname><forenames>Chinchu</forenames></author><author><keyname>Puthussery</keyname><forenames>Anitta</forenames></author><author><keyname>Sasi</keyname><forenames>Neenu K</forenames></author></authors><title>Text Classification For Authorship Attribution Analysis</title><categories>cs.DL cs.CL cs.LG</categories><comments>10 pages</comments><journal-ref>Advanced Computing: An International Journal (ACIJ), Vol.4, No.5,
  September 2013</journal-ref><doi>10.5121/acij.2013.4501</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Authorship attribution mainly deals with undecided authorship of literary
texts. Authorship attribution is useful in resolving issues like uncertain
authorship, recognize authorship of unknown texts, spot plagiarism so on.
Statistical methods can be used to set apart the approach of an author
numerically. The basic methodologies that are made use in computational
stylometry are word length, sentence length, vocabulary affluence, frequencies
etc. Each author has an inborn style of writing, which is particular to
himself. Statistical quantitative techniques can be used to differentiate the
approach of an author in a numerical way. The problem can be broken down into
three sub problems as author identification, author characterization and
similarity detection. The steps involved are pre-processing, extracting
features, classification and author identification. For this different
classifiers can be used. Here fuzzy learning classifier and SVM are used. After
author identification the SVM was found to have more accuracy than Fuzzy
classifier. Later combined the classifiers to obtain a better accuracy when
compared to individual SVM and fuzzy classifier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4914</identifier>
 <datestamp>2013-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4914</id><created>2013-10-18</created><authors><author><keyname>Rossi</keyname><forenames>Fabrice</forenames><affiliation>SAMM</affiliation></author><author><keyname>Latouche</keyname><forenames>Pierre</forenames><affiliation>SAMM</affiliation></author></authors><title>Activity date estimation in timestamped interaction networks</title><categories>math.ST cs.SI stat.TH</categories><comments>21-th European Symposium on Artificial Neural Networks, Computational
  Intelligence and Machine Learning (ESANN 2013), Bruges : Belgium (2013)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose in this paper a new generative model for graphs that uses a latent
space approach to explain timestamped interactions. The model is designed to
provide global estimates of activity dates in historical networks where only
the interaction dates between agents are known with reasonable precision.
Experimental results show that the model provides better results than local
averages in dense enough networks
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4919</identifier>
 <datestamp>2013-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4919</id><created>2013-10-18</created><authors><author><keyname>Rawat</keyname><forenames>Veena</forenames></author></authors><title>Reducing Failure Probability of cloud storage services using
  Multi-Clouds</title><categories>cs.DC</categories><comments>51 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Any information is valuable as long as it has related data. If related data
are not put together, the information is meaningless as unrelated data has no
value. The mapped information is required only by authenticated users. So there
is no necessity to store related information together. If the relations of a
database are fragmented into chunks and these chunks are stored at different
cloud service providers, it could prevent from any privacy breach and the data
stored will be secure. It would also reduce the data transfer costs as the
entire data is not always required, for e.g. during updates. Also, instead of
storage of chunks at a single CSP, if each chunk or fragment is stored at
multiple CSPs it ensures availability and also permits concurrent access.
Additionally, it would prevent financial loss during cloud outages and also
prevent data lock-in. Replicating data chunks at multiple clouds situated at
geographically different locations would also have an additional decrease in
response time. The work attempts to select multiple cloud service providers
within a given budget so as to ensure maximum availability of data. The entire
data can be stored at each of the data centers selected depending on the budget
when there is no security or privacy issue. Data can also be stored in chunks
by replicating each data chunk at two or more cloud service providers.
Different chunks can be replicated at different service providers. The work
also attempts to select various cloud service providers to ensure maximum valid
data chunks within a given budget.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4935</identifier>
 <datestamp>2013-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4935</id><created>2013-10-18</created><authors><author><keyname>Jurdzinski</keyname><forenames>Tomasz</forenames></author><author><keyname>Kowalski</keyname><forenames>Dariusz R.</forenames></author><author><keyname>Lorys</keyname><forenames>Krzysztof</forenames></author></authors><title>Online Packet Scheduling under Adversarial Jamming</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of scheduling packets of different lengths via a
directed communication link prone to jamming errors. Dynamic packet arrivals
and errors are modelled by an adversary. We focus on estimating relative
throughput of online scheduling algorithms, that is, the ratio between the
throughputs achieved by the algorithm and the best scheduling for the same
arrival and error patterns. This framework allows more accurate analysis of
performance of online scheduling algorithms, even in worst-case arrival and
error scenarios. We design an online algorithm for scheduling packets of
arbitrary lengths, achieving optimal relative throughput in the range (1/3,1/2]
(the exact value depends on packet lengths). In other words, for any arrival
and jamming patterns, our solution gives throughput which is no more than c
times worse than the best possible scheduling for these patters, where c in [2;
3) is the inverse of relative throughput. Another algorithm we design makes use
of additional resources in order to achieve relative throughput 1, that is, it
achieves at least as high throughput as the best schedule without such
resources, for any arrival and jamming patterns. More precisely, we show that
if the algorithm can run with double speed, i.e., with twice higher frequency,
then its relative throughput is 1. This demonstrates that throughput of the
best online scheduling algorithms scales well with resource augmentation.
  Keywords: Packet scheduling, Dynamic packet arrivals, Adversarial jamming,
Online algorithms, Relative throughput, Resource augmentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4938</identifier>
 <datestamp>2013-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4938</id><created>2013-10-18</created><authors><author><keyname>Wotzlaw</keyname><forenames>Andreas</forenames></author><author><keyname>Coote</keyname><forenames>Ravi</forenames></author></authors><title>A Logic-based Approach for Recognizing Textual Entailment Supported by
  Ontological Background Knowledge</title><categories>cs.CL cs.AI cs.LO</categories><comments>25 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the architecture and the evaluation of a new system for
recognizing textual entailment (RTE). In RTE we want to identify automatically
the type of a logical relation between two input texts. In particular, we are
interested in proving the existence of an entailment between them. We conceive
our system as a modular environment allowing for a high-coverage syntactic and
semantic text analysis combined with logical inference. For the syntactic and
semantic analysis we combine a deep semantic analysis with a shallow one
supported by statistical models in order to increase the quality and the
accuracy of results. For RTE we use logical inference of first-order employing
model-theoretic techniques and automated reasoning tools. The inference is
supported with problem-relevant background knowledge extracted automatically
and on demand from external sources like, e.g., WordNet, YAGO, and OpenCyc, or
other, more experimental sources with, e.g., manually defined presupposition
resolutions, or with axiomatized general and common sense knowledge. The
results show that fine-grained and consistent knowledge coming from diverse
sources is a necessary condition determining the correctness and traceability
of results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4939</identifier>
 <datestamp>2013-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4939</id><created>2013-10-18</created><authors><author><keyname>Merhav</keyname><forenames>Neri</forenames></author></authors><title>Asymptotically optimal decision rules for joint detection and source
  coding</title><categories>cs.IT math.IT</categories><comments>19 pages; submitted to IEEE Trans. on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of joint detection and lossless source coding is considered. We
derive asymptotically optimal decision rules for deciding whether or not a
sequence of observations has emerged from a desired information source, and to
compress it if has. In particular, our decision rules asymptotically minimize
the cost of compression in the case that the data has been classified as
`desirable', subject to given constraints on the two kinds of the probability
of error. In another version of this performance criterion, the constraint on
the false alarm probability is replaced by the a constraint on the cost of
compression in the false alarm event. We then analyze the asymptotic
performance of these decision rules and demonstrate that they may exhibit
certain phase transitions. We also derive universal decision rules for the case
where the underlying sources (under either hypothesis or both) are unknown, and
training sequences from each source may or may not be available. Finally, we
discuss how our framework can be extended in several directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4943</identifier>
 <datestamp>2013-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4943</id><created>2013-10-18</created><authors><author><keyname>Wei</keyname><forenames>Peng</forenames></author><author><keyname>Dan</keyname><forenames>Lilin</forenames></author><author><keyname>Xiao</keyname><forenames>Yue</forenames></author><author><keyname>Xiang</keyname><forenames>Wei</forenames></author><author><keyname>Li</keyname><forenames>Shaoqian</forenames></author></authors><title>N-continuous OFDM: System Optimization and Performance Analysis</title><categories>cs.IT math.IT</categories><comments>14pages, 7 figures</comments><msc-class>94A14</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  N-continuous orthogonal frequency division multiplexing (NC-OFDM) is a
promising technique to achieve significant sidelobe suppression of baseband
OFDM signals. However, the high complexity limits its application. Based on
conventional NC-OFDM, in this paper, a new technique, called time-domain
N-continuous OFDM (TD-NC-OFDM), is proposed to transfer the original
frequency-domain processing to the time domain, by the linear combination of a
novel basis set to smooth the consecutive OFDM symbols and their high-order
derivatives. We prove that TD-NC-OFDM is an equivalent to conventional one
while consuming much lower complexity. Furthermore, via the time-domain
structure, a closed-form spectral expression of NC-OFDM signals is derived and
a compact upper bound of sidelobe decaying is derived. This paper also
investigates the impact of the TD-NC-OFDM technique on received
signal-to-interference-plus-noise ratio (SINR) and provides a closed-form
analytical expression. Theoretical analyses and simulation results show that
TD-NC-OFDM can prohibitively suppress the sidelobe with much lower complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4945</identifier>
 <datestamp>2014-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4945</id><created>2013-10-18</created><updated>2014-02-20</updated><authors><author><keyname>Zeng</keyname><forenames>Xiangrong</forenames></author><author><keyname>Figueiredo</keyname><forenames>M&#xe1;rio A. T.</forenames></author></authors><title>A novel sparsity and clustering regularization</title><categories>cs.LG cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel SPARsity and Clustering (SPARC) regularizer, which is a
modified version of the previous octagonal shrinkage and clustering algorithm
for regression (OSCAR), where, the proposed regularizer consists of a
$K$-sparse constraint and a pair-wise $\ell_{\infty}$ norm restricted on the
$K$ largest components in magnitude. The proposed regularizer is able to
separably enforce $K$-sparsity and encourage the non-zeros to be equal in
magnitude. Moreover, it can accurately group the features without shrinking
their magnitude. In fact, SPARC is closely related to OSCAR, so that the
proximity operator of the former can be efficiently computed based on that of
the latter, allowing using proximal splitting algorithms to solve problems with
SPARC regularization. Experiments on synthetic data and with benchmark breast
cancer data show that SPARC is a competitive group-sparsity inducing
regularizer for regression and classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4949</identifier>
 <datestamp>2013-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4949</id><created>2013-10-18</created><authors><author><keyname>Shukla</keyname><forenames>Ravi Shankar</forenames></author><author><keyname>Tyagi</keyname><forenames>Neeraj</forenames></author></authors><title>Global mibility and handover management for heterogeneous network in
  VANET</title><categories>cs.NI</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Now a days Vehicular Ad hoc Network is an emerging technology. Mobility
management is one of the most challenging research issues for Vehicular Ad hoc
Network to support variety of intelligent transportation system applications.
Vehicular Ad hoc Networks are getting importance for inter-vehicle
communication, because they allow the communication among vehicles without any
infrastructure, configuration effort, and without the high costs of cellular
networks. Besides local data exchange, vehicular applications may be used to
accessing Internet services. The access is provided by Internet gateways
located on the site of roadside. However, the Internet integration requires a
respective mobility support of the vehicular ad hoc network. In this paper we
will study about the network mobility approach in vehicular ad hoc network; the
model will describe the movement of vehicles from one network to other network.
The proposed handover scheme reduces the handover latency, packet loss
signaling overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4954</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4954</id><created>2013-10-18</created><updated>2013-10-21</updated><authors><author><keyname>&#xc1;lvarez-Garc&#xed;a</keyname><forenames>Sandra</forenames></author><author><keyname>Brisaboa</keyname><forenames>Nieves R.</forenames></author><author><keyname>Fern&#xe1;ndez</keyname><forenames>Javier D.</forenames></author><author><keyname>Mart&#xed;nez-Prieto</keyname><forenames>Miguel A.</forenames></author><author><keyname>Navarro</keyname><forenames>Gonzalo</forenames></author></authors><title>Compressed Vertical Partitioning for Full-In-Memory RDF Management</title><categories>cs.DB cs.DS cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Web of Data has been gaining momentum and this leads to increasingly
publish more semi-structured datasets following the RDF model, based on atomic
triple units of subject, predicate, and object. Although it is a simple model,
compression methods become necessary because datasets are increasingly larger
and various scalability issues arise around their organization and storage.
This requirement is more restrictive in RDF stores because efficient SPARQL
resolution on the compressed RDF datasets is also required.
  This article introduces a novel RDF indexing technique (called k2-triples)
supporting efficient SPARQL resolution in compressed space. k2-triples, uses
the predicate to vertically partition the dataset into disjoint subsets of
pairs (subject, object), one per predicate. These subsets are represented as
binary matrices in which 1-bits mean that the corresponding triple exists in
the dataset. This model results in very sparse matrices, which are efficiently
compressed using k2-trees. We enhance this model with two compact indexes
listing the predicates related to each different subject and object, in order
to address the specific weaknesses of vertically partitioned representations.
The resulting technique not only achieves by far the most compressed
representations, but also the best overall performance for RDF retrieval in our
experiments. Our approach uses up to 10 times less space than a state of the
art baseline, and outperforms its performance by several order of magnitude on
the most basic query patterns. In addition, we optimize traditional join
algorithms on k2-triples and define a novel one leveraging its specific
features. Our experimental results show that our technique overcomes
traditional vertical partitioning for join resolution, reporting the best
numbers for joins in which the non-joined nodes are provided, and being
competitive in the majority of the cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4966</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4966</id><created>2013-10-18</created><updated>2014-01-12</updated><authors><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author><author><keyname>de Moya-Aneg&#xf3;n</keyname><forenames>F&#xe9;lix</forenames></author><author><keyname>Guerrero-Bote</keyname><forenames>Vicente P.</forenames></author></authors><title>Journal Maps, Interactive Overlays, and the Measurement of
  Interdisciplinarity on the Basis of Scopus Data (1996-2012)</title><categories>cs.DL</categories><comments>accepted for publication in the Journal of the Association for
  Information Science and Technology (JASIST)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using Scopus data, we construct a global map of science based on aggregated
journal-journal citations from 1996-2012 (N of journals = 20,554). This base
map enables users to overlay downloads from Scopus interactively. Using a
single year (e.g., 2012), results can be compared with mappings based on the
Journal Citation Reports at the Web-of-Science (N = 10,936). The Scopus maps
are more detailed at both the local and global levels because of their greater
coverage, including, for example, the arts and humanities. The base maps can be
interactively overlaid with journal distributions in sets downloaded from
Scopus, for example, for the purpose of portfolio analysis. Rao-Stirling
diversity can be used as a measure of interdisciplinarity in the sets under
study. Maps at the global and the local level, however, can be very different
because of the different levels of aggregation involved. Two journals, for
example, can both belong to the humanities in the global map, but participate
in different specialty structures locally. The base map and interactive tools
are available online (with instructions) at
http://www.leydesdorff.net/scopus_ovl.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4975</identifier>
 <datestamp>2014-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4975</id><created>2013-10-18</created><updated>2014-08-14</updated><authors><author><keyname>Javarone</keyname><forenames>Marco Alberto</forenames></author></authors><title>Competitive dynamics of lexical innovations in multi-layer networks</title><categories>physics.soc-ph cs.SI</categories><comments>23 pages, 19 figures, 1 table</comments><journal-ref>Int. J. Mod. Phys. C (2014)</journal-ref><doi>10.1142/S012918311450048X</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the introduction of lexical innovations into a community of language
users. Lexical innovations, i.e., new terms added to people's vocabulary, play
an important role in the process of language evolution. Nowadays, information
is spread through a variety of networks, including, among others, online and
offline social networks and the World Wide Web. The entire system, comprising
networks of different nature, can be represented as a multi-layer network. In
this context, lexical innovations diffusion occurs in a peculiar fashion. In
particular, a lexical innovation can undergo three different processes: its
original meaning is accepted; its meaning can be changed or misunderstood
(e.g., when not properly explained), hence more than one meaning can emerge in
the population; lastly, in the case of a loan word, it can be translated into
the population language (i.e., defining a new lexical innovation or using a
synonym) or into a dialect spoken by part of the population. Therefore, lexical
innovations cannot be considered simply as information. We develop a model for
analyzing this scenario using a multi-layer network comprising a social network
and a media network. The latter represents the set of all information systems
of a society, e.g., television, the World Wide Web and radio. Furthermore, we
identify temporal directed edges between the nodes of these two networks. In
particular, at each time step, nodes of the media network can be connected to
randomly chosen nodes of the social network and vice versa. In so doing,
information spreads through the whole system and people can share a lexical
innovation with their neighbors or, in the event they work as reporters, by
using media nodes. Lastly, we use the concept of &quot;linguistic sign&quot; to model
lexical innovations, showing its fundamental role in the study of these
dynamics. Many numerical simulations have been performed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4977</identifier>
 <datestamp>2013-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4977</id><created>2013-10-18</created><authors><author><keyname>Signoretto</keyname><forenames>Marco</forenames></author><author><keyname>De Lathauwer</keyname><forenames>Lieven</forenames></author><author><keyname>Suykens</keyname><forenames>Johan A. K.</forenames></author></authors><title>Learning Tensors in Reproducing Kernel Hilbert Spaces with Multilinear
  Spectral Penalties</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a general framework to learn functions in tensor product
reproducing kernel Hilbert spaces (TP-RKHSs). The methodology is based on a
novel representer theorem suitable for existing as well as new spectral
penalties for tensors. When the functions in the TP-RKHS are defined on the
Cartesian product of finite discrete sets, in particular, our main problem
formulation admits as a special case existing tensor completion problems. Other
special cases include transfer learning with multimodal side information and
multilinear multitask learning. For the latter case, our kernel-based view is
instrumental to derive nonlinear extensions of existing model classes. We give
a novel algorithm and show in experiments the usefulness of the proposed
extensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4986</identifier>
 <datestamp>2013-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4986</id><created>2013-10-18</created><updated>2013-10-23</updated><authors><author><keyname>Cerutti</keyname><forenames>Federico</forenames></author><author><keyname>Dunne</keyname><forenames>Paul E.</forenames></author><author><keyname>Giacomin</keyname><forenames>Massimiliano</forenames></author><author><keyname>Vallati</keyname><forenames>Mauro</forenames></author></authors><title>Computing Preferred Extensions in Abstract Argumentation: a SAT-based
  Approach</title><categories>cs.AI</categories><comments>Preprint of TAFA'13 post proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel SAT-based approach for the computation of
extensions in abstract argumentation, with focus on preferred semantics, and an
empirical evaluation of its performances. The approach is based on the idea of
reducing the problem of computing complete extensions to a SAT problem and then
using a depth-first search method to derive preferred extensions. The proposed
approach has been tested using two distinct SAT solvers and compared with three
state-of-the-art systems for preferred extension computation. It turns out that
the proposed approach delivers significantly better performances in the large
majority of the considered cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4989</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4989</id><created>2013-10-18</created><updated>2013-10-21</updated><authors><author><keyname>Feldt</keyname><forenames>Robert</forenames></author></authors><title>Do System Test Cases Grow Old?</title><categories>cs.SE</categories><comments>Updated with nicer figs without border around them</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Companies increasingly use either manual or automated system testing to
ensure the quality of their software products. As a system evolves and is
extended with new features the test suite also typically grows as new test
cases are added. To ensure software quality throughout this process the test
suite is continously executed, often on a daily basis. It seems likely that
newly added tests would be more likely to fail than older tests but this has
not been investigated in any detail on large-scale, industrial software
systems. Also it is not clear which methods should be used to conduct such an
analysis. This paper proposes three main concepts that can be used to
investigate aging effects in the use and failure behavior of system test cases:
test case activation curves, test case hazard curves, and test case half-life.
To evaluate these concepts and the type of analysis they enable we apply them
on an industrial software system containing more than one million lines of
code. The data sets comes from a total of 1,620 system test cases executed a
total of more than half a million times over a time period of two and a half
years. For the investigated system we find that system test cases stay active
as they age but really do grow old; they go through an infant mortality phase
with higher failure rates which then decline over time. The test case half-life
is between 5 to 12 months for the two studied data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.4993</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.4993</id><created>2013-10-18</created><updated>2014-02-11</updated><authors><author><keyname>Ram</keyname><forenames>B Hari</forenames></author><author><keyname>Giridhar</keyname><forenames>K</forenames></author></authors><title>Fractional Interference Alignment: An Interference Alignment Scheme for
  Finite Alphabet Signals</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interference Alignment (IA) is a transmission scheme which achieves 1/2
Degrees-of-Freedom (DoF) per transmit-antenna per user. The constraints imposed
on the scheme are based on the linear receiver since conventional IA assumes
Gaussian signaling. However, when the transmitters employ Finite Alphabet (FA)
signaling, neither the conventional IA precoders nor the linear receiver are
optimal structures. Therefore, a novel Fractional Interference Alignment (FIA)
scheme is introduced when FA signals are used, where the alignment constraints
are now based on the non-linear, minimum distance (MD) detector. Since DoF is
defined only as signal-to-noise ratio tends to infinity, we introduce a new
metric called SpAC (number of Symbols transmitted-per-transmit
Antenna-per-Channel use) for analyzing the FIA scheme. The maximum SpAC is one,
and the FIA achieves any value of SpAC in the range [0,1]. The key motivation
for this work is that numerical simulations with FA signals and MD detector for
fixed SpAC (=1/2, as in IA) over a set of optimization problems, like
minimizing bit error rate or maximizing the mutual information, achieves a
significantly better error rate performance when compared to the existing
algorithms that minimize mean square error or maximize signal-to-interference
plus noise ratio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5007</identifier>
 <datestamp>2013-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5007</id><created>2013-10-17</created><authors><author><keyname>Xu</keyname><forenames>Tianbing</forenames></author><author><keyname>Gao</keyname><forenames>Jianfeng</forenames></author><author><keyname>Xiao</keyname><forenames>Lin</forenames></author><author><keyname>Regan</keyname><forenames>Amelia</forenames></author></authors><title>Online Classification Using a Voted RDA Method</title><categories>cs.LG stat.ML</categories><comments>23 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a voted dual averaging method for online classification problems
with explicit regularization. This method employs the update rule of the
regularized dual averaging (RDA) method, but only on the subsequence of
training examples where a classification error is made. We derive a bound on
the number of mistakes made by this method on the training set, as well as its
generalization error rate. We also introduce the concept of relative strength
of regularization, and show how it affects the mistake bound and generalization
performance. We experimented with the method using $\ell_1$ regularization on a
large-scale natural language processing task, and obtained state-of-the-art
classification performance with fairly sparse models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5008</identifier>
 <datestamp>2013-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5008</id><created>2013-10-17</created><authors><author><keyname>Xu</keyname><forenames>Tianbing</forenames></author><author><keyname>Yu</keyname><forenames>Yaming</forenames></author><author><keyname>Turner</keyname><forenames>John</forenames></author><author><keyname>Regan</keyname><forenames>Amelia</forenames></author></authors><title>Thompson Sampling in Dynamic Systems for Contextual Bandit Problems</title><categories>cs.LG</categories><comments>22 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the multiarm bandit problems in the timevarying dynamic system
for rich structural features. For the nonlinear dynamic model, we propose the
approximate inference for the posterior distributions based on Laplace
Approximation. For the context bandit problems, Thompson Sampling is adopted
based on the underlying posterior distributions of the parameters. More
specifically, we introduce the discount decays on the previous samples impact
and analyze the different decay rates with the underlying sample dynamics.
Consequently, the exploration and exploitation is adaptively tradeoff according
to the dynamics in the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5011</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5011</id><created>2013-10-18</created><updated>2015-01-13</updated><authors><author><keyname>Bergstra</keyname><forenames>Jan A.</forenames></author><author><keyname>Bethke</keyname><forenames>Inge</forenames></author><author><keyname>Ponse</keyname><forenames>Alban</forenames></author></authors><title>Equations for formally real meadows</title><categories>math.RA cs.LO</categories><comments>24 pages, 14 tables, revised, new Theorem 3.7</comments><msc-class>12D15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the signatures $\Sigma_m=(0,1,-,+, \cdot, \ ^{-1})$ of meadows
and $(\Sigma_m, {\mathbf s})$ of signed meadows. We give two complete
axiomatizations of the equational theories of the real numbers with respect to
these signatures. In the first case, we extend the axiomatization of
zero-totalized fields by a single axiom scheme expressing formal realness; the
second axiomatization presupposes an ordering. We apply these completeness
results in order to obtain complete axiomatizations of the complex numbers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5022</identifier>
 <datestamp>2013-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5022</id><created>2013-10-18</created><authors><author><keyname>Wawrzyniak</keyname><forenames>Karol</forenames></author><author><keyname>Orynczak</keyname><forenames>Grzegorz</forenames></author><author><keyname>Klos</keyname><forenames>Michal</forenames></author><author><keyname>Goska</keyname><forenames>Aneta</forenames></author><author><keyname>Jakubek</keyname><forenames>Marcin</forenames></author></authors><title>Division of the Energy Market into Zones in Variable Weather Conditions
  using Locational Marginal Prices</title><categories>cs.CE cs.CY cs.SY</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adopting a zonal structure of electricity market requires specification of
zones' borders. One of the approaches to identify zones is based on clustering
of Locational Marginal Prices (LMP). The purpose of the paper is twofold: (i)
we extend the LMP methodology by taking into account variable weather
conditions and (ii) we point out some weaknesses of the method and suggest
their potential solutions. The offered extension comprises simulations based on
the Optimal Power Flow (OPF) algorithm and twofold clustering method. First,
LMP are calculated by OPF for each of scenario representing different weather
conditions. Second, hierarchical clustering based on Ward's criterion is used
on each realization of the prices separately. Then, another clustering method,
i.e. consensus clustering, is used to aggregate the results from all
simulations and to find the global division into zones. The offered method of
aggregation is not limited only to LMP methodology and is universal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5025</identifier>
 <datestamp>2013-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5025</id><created>2013-10-18</created><authors><author><keyname>Wawrzyniak</keyname><forenames>Karol</forenames></author><author><keyname>Klos</keyname><forenames>Michal</forenames></author><author><keyname>Orynczak</keyname><forenames>Grzegorz</forenames></author><author><keyname>Jakubek</keyname><forenames>Marcin</forenames></author></authors><title>The Optimal Division of the Energy Market into Zones: Comparison of Two
  Methodologies under Variable Wind Conditions</title><categories>cs.CE cs.CY cs.SY</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We compare two competing methodologies of market zones identification under
the criterion of social welfare maximization: (i) consensus clustering of
Locational Marginal Prices over different wind scenarios and (ii) congestion
contribution identification with congested lines identified across variable
wind generation outputs. We test the division of market into zones based on
each of the two methodologies using a welfare criterion, i.e., comparing the
cost of supplying energy on uniform market (including readjustments made on a
balancing market to overcome the congestion) with cost on k-zone market. A
division which maximizes the welfare is considered as the optimum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5032</identifier>
 <datestamp>2013-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5032</id><created>2013-10-18</created><authors><author><keyname>Cervelle</keyname><forenames>Julien</forenames></author><author><keyname>Dennunzio</keyname><forenames>Alberto</forenames></author><author><keyname>Formenti</keyname><forenames>Enrico</forenames></author><author><keyname>Provillard</keyname><forenames>Julien</forenames></author></authors><title>Acceptance conditions for omega-languages and the Borel hierarchy</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates acceptance conditions for finite automata recognizing
omega-regular languages. As a first result, we show that, under any acceptance
condition that can be defined in the MSO logic, a finite automaton can
recognize at most omega-regular languages. Starting from this, the paper aims
at classifying acceptance conditions according to their expressive power and at
finding the exact position of the classes of omega-languages they induced
according to the Borel hierarchy. A new interesting acceptance condition is
introduced and fully characterized. A step forward is also made in the
understanding of the expressive power of (fin, =).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5034</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5034</id><created>2013-10-18</created><updated>2014-07-02</updated><authors><author><keyname>Bl&#xf6;mer</keyname><forenames>Johannes</forenames></author><author><keyname>Bujna</keyname><forenames>Kathrin</forenames></author><author><keyname>Kuntze</keyname><forenames>Daniel</forenames></author></authors><title>A Theoretical and Experimental Comparison of the EM and SEM Algorithm</title><categories>cs.LG stat.ML</categories><comments>This paper is a preprint of a paper submitted to and accepted for
  publication in ICPR 2014 and is subject to IEEE copyright</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we provide a new analysis of the SEM algorithm. Unlike previous
work, we focus on the analysis of a single run of the algorithm. First, we
discuss the algorithm for general mixture distributions. Second, we consider
Gaussian mixture models and show that with high probability the update
equations of the EM algorithm and its stochastic variant are almost the same,
given that the input set is sufficiently large. Our experiments confirm that
this still holds for a large number of successive update steps. In particular,
for Gaussian mixture models, we show that the stochastic variant runs nearly
twice as fast.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5035</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5035</id><created>2013-10-18</created><updated>2014-05-28</updated><authors><author><keyname>Lin</keyname><forenames>Zhouchen</forenames></author><author><keyname>Liu</keyname><forenames>Risheng</forenames></author><author><keyname>Li</keyname><forenames>Huan</forenames></author></authors><title>Linearized Alternating Direction Method with Parallel Splitting and
  Adaptive Penalty for Separable Convex Programs in Machine Learning</title><categories>cs.NA cs.LG math.OC stat.ML</categories><comments>Preliminary version published on Asian Conference on Machine Learning
  2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many problems in machine learning and other fields can be (re)for-mulated as
linearly constrained separable convex programs. In most of the cases, there are
multiple blocks of variables. However, the traditional alternating direction
method (ADM) and its linearized version (LADM, obtained by linearizing the
quadratic penalty term) are for the two-block case and cannot be naively
generalized to solve the multi-block case. So there is great demand on
extending the ADM based methods for the multi-block case. In this paper, we
propose LADM with parallel splitting and adaptive penalty (LADMPSAP) to solve
multi-block separable convex programs efficiently. When all the component
objective functions have bounded subgradients, we obtain convergence results
that are stronger than those of ADM and LADM, e.g., allowing the penalty
parameter to be unbounded and proving the sufficient and necessary conditions}
for global convergence. We further propose a simple optimality measure and
reveal the convergence rate of LADMPSAP in an ergodic sense. For programs with
extra convex set constraints, with refined parameter estimation we devise a
practical version of LADMPSAP for faster convergence. Finally, we generalize
LADMPSAP to handle programs with more difficult objective functions by
linearizing part of the objective function as well. LADMPSAP is particularly
suitable for sparse representation and low-rank recovery problems because its
subproblems have closed form solutions and the sparsity and low-rankness of the
iterates can be preserved during the iteration. It is also highly
parallelizable and hence fits for parallel or distributed computing. Numerical
experiments testify to the advantages of LADMPSAP in speed and numerical
accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5037</identifier>
 <datestamp>2014-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5037</id><created>2013-10-18</created><authors><author><keyname>Beerenwinkel</keyname><forenames>Niko</forenames></author><author><keyname>Beretta</keyname><forenames>Stefano</forenames></author><author><keyname>Bonizzoni</keyname><forenames>Paola</forenames></author><author><keyname>Dondi</keyname><forenames>Riccardo</forenames></author><author><keyname>Pirola</keyname><forenames>Yuri</forenames></author></authors><title>Covering Pairs in Directed Acyclic Graphs</title><categories>cs.DS cs.CC</categories><journal-ref>Proc. of Language and Automata Theory and Applications (LATA
  2014), LNCS Vol. 8370, 2014, pp 126-137</journal-ref><doi>10.1007/978-3-319-04921-2_10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Minimum Path Cover problem on directed acyclic graphs (DAGs) is a
classical problem that provides a clear and simple mathematical formulation for
several applications in different areas and that has an efficient algorithmic
solution. In this paper, we study the computational complexity of two
constrained variants of Minimum Path Cover motivated by the recent introduction
of next-generation sequencing technologies in bioinformatics. The first problem
(MinPCRP), given a DAG and a set of pairs of vertices, asks for a minimum
cardinality set of paths &quot;covering&quot; all the vertices such that both vertices of
each pair belong to the same path. For this problem, we show that, while it is
NP-hard to compute if there exists a solution consisting of at most three
paths, it is possible to decide in polynomial time whether a solution
consisting of at most two paths exists. The second problem (MaxRPSP), given a
DAG and a set of pairs of vertices, asks for a path containing the maximum
number of the given pairs of vertices. We show its NP-hardness and also its
W[1]-hardness when parametrized by the number of covered pairs. On the positive
side, we give a fixed-parameter algorithm when the parameter is the maximum
overlapping degree, a natural parameter in the bioinformatics applications of
the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5042</identifier>
 <datestamp>2013-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5042</id><created>2013-10-18</created><authors><author><keyname>Turney</keyname><forenames>Peter D.</forenames></author></authors><title>Distributional semantics beyond words: Supervised learning of analogy
  and paraphrase</title><categories>cs.LG cs.AI cs.CL cs.IR</categories><acm-class>H.3.1; I.2.6; I.2.7</acm-class><journal-ref>Transactions of the Association for Computational Linguistics
  (TACL), (2013), 1, 353-366</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There have been several efforts to extend distributional semantics beyond
individual words, to measure the similarity of word pairs, phrases, and
sentences (briefly, tuples; ordered sets of words, contiguous or
noncontiguous). One way to extend beyond words is to compare two tuples using a
function that combines pairwise similarities between the component words in the
tuples. A strength of this approach is that it works with both relational
similarity (analogy) and compositional similarity (paraphrase). However, past
work required hand-coding the combination function for different tasks. The
main contribution of this paper is that combination functions are generated by
supervised learning. We achieve state-of-the-art results in measuring
relational similarity between word pairs (SAT analogies and SemEval~2012 Task
2) and measuring compositional similarity between noun-modifier phrases and
unigrams (multiple-choice paraphrase questions).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5043</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5043</id><created>2013-10-18</created><updated>2014-07-01</updated><authors><author><keyname>Kufleitner</keyname><forenames>Manfred</forenames></author><author><keyname>Walter</keyname><forenames>Tobias</forenames></author></authors><title>One Quantifier Alternation in First-Order Logic with Modular Predicates</title><categories>cs.FL cs.LO</categories><acm-class>F.4.1; F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adding modular predicates yields a generalization of first-order logic FO
over words. The expressive power of FO[&lt;,MOD] with order comparison $x&lt;y$ and
predicates for $x \equiv i \mod n$ has been investigated by Barrington,
Compton, Straubing and Therien. The study of FO[&lt;,MOD]-fragments was initiated
by Chaubard, Pin and Straubing. More recently, Dartois and Paperman showed that
definability in the two-variable fragment FO2[&lt;,MOD] is decidable. In this
paper we continue this line of work.
  We give an effective algebraic characterization of the word languages in
Sigma2[&lt;,MOD]. The fragment Sigma2 consists of first-order formulas in prenex
normal form with two blocks of quantifiers starting with an existential block.
In addition we show that Delta2[&lt;,MOD], the largest subclass of Sigma2[&lt;,MOD]
which is closed under negation, has the same expressive power as two-variable
logic FO2[&lt;,MOD]. This generalizes the result FO2[&lt;] = Delta2[&lt;] of Therien and
Wilke to modular predicates. As a byproduct, we obtain another decidable
characterization of FO2[&lt;,MOD].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5045</identifier>
 <datestamp>2014-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5045</id><created>2013-10-18</created><updated>2014-04-04</updated><authors><author><keyname>Demirel</keyname><forenames>&#xd6;mer</forenames></author><author><keyname>Smal</keyname><forenames>Ihor</forenames></author><author><keyname>Niessen</keyname><forenames>Wiro</forenames></author><author><keyname>Meijering</keyname><forenames>Erik</forenames></author><author><keyname>Sbalzarini</keyname><forenames>Ivo F.</forenames></author></authors><title>PPF - A Parallel Particle Filtering Library</title><categories>cs.DC stat.CO</categories><comments>8 pages, 8 figures; will appear in the proceedings of the IET Data
  Fusion &amp; Target Tracking Conference 2014</comments><acm-class>C.1.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the parallel particle filtering (PPF) software library, which
enables hybrid shared-memory/distributed-memory parallelization of particle
filtering (PF) algorithms combining the Message Passing Interface (MPI) with
multithreading for multi-level parallelism. The library is implemented in Java
and relies on OpenMPI's Java bindings for inter-process communication. It
includes dynamic load balancing, multi-thread balancing, and several
algorithmic improvements for PF, such as input-space domain decomposition. The
PPF library hides the difficulties of efficient parallel programming of PF
algorithms and provides application developers with the necessary tools for
parallel implementation of PF methods. We demonstrate the capabilities of the
PPF library using two distributed PF algorithms in two scenarios with different
numbers of particles. The PPF library runs a 38 million particle problem,
corresponding to more than 1.86 GB of particle data, on 192 cores with 67%
parallel efficiency. To the best of our knowledge, the PPF library is the first
open-source software that offers a parallel framework for PF applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5047</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5047</id><created>2013-10-18</created><authors><author><keyname>Ritchie</keyname><forenames>Martin</forenames></author><author><keyname>Berthouze</keyname><forenames>Luc</forenames></author><author><keyname>House</keyname><forenames>Thomas</forenames></author><author><keyname>Kiss</keyname><forenames>Istvan Z.</forenames></author></authors><title>Higher-order structure and epidemic dynamics in clustered networks</title><categories>physics.soc-ph cs.SI q-bio.PE</categories><journal-ref>Journal Theoretical Biology 2014, 348:21-32</journal-ref><doi>10.1016/j.jtbi.2014.01.025</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clustering is typically measured by the ratio of triangles to all triples,
open or closed. Generating clustered networks, and how clustering affects
dynamics on networks, is reasonably well understood for certain classes of
networks \cite{vmclust, karrerclust2010}, e.g., networks composed of lines and
non-overlapping triangles. In this paper we show that it is possible to
generate networks which, despite having the same degree distribution and equal
clustering, exhibit different higher-order structure, specifically, overlapping
triangles and other order-four (a closed network motif composed of four nodes)
structures. To distinguish and quantify these additional structural features,
we develop a new network metric capable of measuring order-four structure
which, when used alongside traditional network metrics, allows us to more
accurately describe a network's topology. Three network generation algorithms
are considered: a modified configuration model and two rewiring algorithms. By
generating homogeneous networks with equal clustering we study and quantify
their structural differences, and using SIS (Susceptible-Infected-Susceptible)
and SIR (Susceptible-Infected-Recovered) dynamics we investigate
computationally how differences in higher-order structure impact on epidemic
threshold, final epidemic or prevalence levels and time evolution of epidemics.
Our results suggest that characterising and measuring higher-order network
structure is needed to advance our understanding of the impact of network
topology on dynamics unfolding on the networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5059</identifier>
 <datestamp>2014-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5059</id><created>2013-10-18</created><authors><author><keyname>Gittsovich</keyname><forenames>Oleg</forenames></author><author><keyname>Beaudry</keyname><forenames>Normand J.</forenames></author><author><keyname>Narasimhachar</keyname><forenames>Varun</forenames></author><author><keyname>Alvarez</keyname><forenames>Ruben Romero</forenames></author><author><keyname>Moroder</keyname><forenames>Tobias</forenames></author><author><keyname>L&#xfc;tkenhaus</keyname><forenames>Norbert</forenames></author></authors><title>Squashing model for detectors and applications to quantum key
  distribution protocols</title><categories>quant-ph cs.CR cs.IT math.IT</categories><comments>24 pages main text, 12 figures, 4 pages appendix, 1 figure</comments><journal-ref>Phys. Rev. A 89, 012325 (2014)</journal-ref><doi>10.1103/PhysRevA.89.012325</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a framework that allows a description of measurements in Hilbert
spaces that are smaller than their natural representation. This description,
which we call a &quot;squashing model&quot;, consists of a squashing map that maps the
input states of the measurement from the original Hilbert space to the smaller
one, followed by a targeted prescribed measurement on the smaller Hilbert
space. This framework has applications in quantum key distribution, but also in
other cryptographic tasks, as it greatly simplifies the theoretical analysis
under adversarial conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5061</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5061</id><created>2013-10-18</created><authors><author><keyname>Demirarslan</keyname><forenames>Pinar Celebi</forenames></author><author><keyname>Soprunov</keyname><forenames>Ivan</forenames></author></authors><title>On dual toric complete intersection codes</title><categories>math.AG cs.IT math.IT</categories><comments>20 pages, 6 figures</comments><msc-class>Primary 14M25, 14G50, Secondary 52B20</msc-class><journal-ref>Finite Fields and Their Applications (2015) pp. 118-136</journal-ref><doi>10.1016/j.ffa.2014.12.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study duality for evaluation codes on intersections of d
hypersurfaces with given d-dimensional Newton polytopes, so called toric
complete intersection codes. In particular, we give a condition for such a code
to be quasi-self-dual. In the case of d=2 it reduces to a combinatorial
condition on the Newton polygons. This allows us to give an explicit
construction of dual and quasi-self-dual toric complete intersection codes. We
provide a list of examples over the field of 16 elements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5062</identifier>
 <datestamp>2013-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5062</id><created>2013-10-18</created><authors><author><keyname>Rudolph-Lilith</keyname><forenames>Michelle</forenames></author><author><keyname>Muller</keyname><forenames>Lyle E.</forenames></author></authors><title>Aspects of randomness in neural graph structures</title><categories>physics.soc-ph cs.SI q-bio.NC</categories><comments>19 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the past two decades, significant advances have been made in understanding
the structural and functional properties of biological networks, via
graph-theoretic analysis. In general, most graph-theoretic studies are
conducted in the presence of serious uncertainties, such as major undersampling
of the experimental data. In the specific case of neural systems, however, a
few moderately robust experimental reconstructions do exist, and these have
long served as fundamental prototypes for studying connectivity patterns in the
nervous system. In this paper, we provide a comparative analysis of these
&quot;historical&quot; graphs, both in (unmodified) directed and (often symmetrized)
undirected forms, and focus on simple structural characterizations of their
connectivity. We find that in most measures the networks studied are captured
by simple random graph models; in a few key measures, however, we observe a
marked departure from the random graph prediction. Our results suggest that the
mechanism of graph formation in the networks studied is not well-captured by
existing abstract graph models, such as the small-world or scale-free graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5082</identifier>
 <datestamp>2013-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5082</id><created>2013-10-18</created><authors><author><keyname>Camps-Valls</keyname><forenames>Gustavo</forenames></author><author><keyname>Guti&#xe9;rrez</keyname><forenames>Juan</forenames></author><author><keyname>G&#xf3;mez-P&#xe9;rez</keyname><forenames>Gabriel</forenames></author><author><keyname>Malo</keyname><forenames>Jes&#xfa;s</forenames></author></authors><title>On the Suitable Domain for SVM Training in Image Coding</title><categories>cs.CV cs.LG stat.ML</categories><journal-ref>Journal of Machine Learning Research, JMLR, 9(1), 49-66, 2008</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conventional SVM-based image coding methods are founded on independently
restricting the distortion in every image coefficient at some particular image
representation. Geometrically, this implies allowing arbitrary signal
distortions in an $n$-dimensional rectangle defined by the
$\varepsilon$-insensitivity zone in each dimension of the selected image
representation domain. Unfortunately, not every image representation domain is
well-suited for such a simple, scalar-wise, approach because statistical and/or
perceptual interactions between the coefficients may exist. These interactions
imply that scalar approaches may induce distortions that do not follow the
image statistics and/or are perceptually annoying. Taking into account these
relations would imply using non-rectangular $\varepsilon$-insensitivity regions
(allowing coupled distortions in different coefficients), which is beyond the
conventional SVM formulation.
  In this paper, we report a condition on the suitable domain for developing
efficient SVM image coding schemes. We analytically demonstrate that no linear
domain fulfills this condition because of the statistical and perceptual
inter-coefficient relations that exist in these domains. This theoretical
result is experimentally confirmed by comparing SVM learning in previously
reported linear domains and in a recently proposed non-linear perceptual domain
that simultaneously reduces the statistical and perceptual relations (so it is
closer to fulfilling the proposed condition). These results highlight the
relevance of an appropriate choice of the image representation before SVM
learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5089</identifier>
 <datestamp>2013-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5089</id><created>2013-10-18</created><authors><author><keyname>Arenas-Garc&#xed;a</keyname><forenames>Jer&#xf3;nimo</forenames></author><author><keyname>Petersen</keyname><forenames>Kaare Brandt</forenames></author><author><keyname>Camps-Valls</keyname><forenames>Gustavo</forenames></author><author><keyname>Hansen</keyname><forenames>Lars Kai</forenames></author></authors><title>Kernel Multivariate Analysis Framework for Supervised Subspace Learning:
  A Tutorial on Linear and Kernel Multivariate Methods</title><categories>stat.ML cs.LG</categories><journal-ref>IEEE Signal Processing Magazine, 30(4), 16-29, 2013</journal-ref><doi>10.1109/MSP.2013.2250591</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature extraction and dimensionality reduction are important tasks in many
fields of science dealing with signal processing and analysis. The relevance of
these techniques is increasing as current sensory devices are developed with
ever higher resolution, and problems involving multimodal data sources become
more common. A plethora of feature extraction methods are available in the
literature collectively grouped under the field of Multivariate Analysis (MVA).
This paper provides a uniform treatment of several methods: Principal Component
Analysis (PCA), Partial Least Squares (PLS), Canonical Correlation Analysis
(CCA) and Orthonormalized PLS (OPLS), as well as their non-linear extensions
derived by means of the theory of reproducing kernel Hilbert spaces. We also
review their connections to other methods for classification and statistical
dependence estimation, and introduce some recent developments to deal with the
extreme cases of large-scale and low-sized problems. To illustrate the wide
applicability of these methods in both classification and regression problems,
we analyze their performance in a benchmark of publicly available data sets,
and pay special attention to specific real applications involving audio
processing for music genre prediction and hyperspectral satellite images for
Earth and climate monitoring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5095</identifier>
 <datestamp>2013-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5095</id><created>2013-10-18</created><authors><author><keyname>Riedel</keyname><forenames>Martin</forenames><affiliation>SAMM</affiliation></author><author><keyname>K&#xe4;stner</keyname><forenames>Marika</forenames><affiliation>SAMM</affiliation></author><author><keyname>Rossi</keyname><forenames>Fabrice</forenames><affiliation>SAMM</affiliation></author><author><keyname>Villmann</keyname><forenames>Thomas</forenames></author></authors><title>Regularization in Relevance Learning Vector Quantization Using l one
  Norms</title><categories>stat.ML cs.LG</categories><proxy>ccsd</proxy><journal-ref>21-th European Symposium on Artificial Neural Networks,
  Computational Intelligence and Machine Learning (ESANN 2013), Bruges :
  Belgium (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose in this contribution a method for l one regularization in
prototype based relevance learning vector quantization (LVQ) for sparse
relevance profiles. Sparse relevance profiles in hyperspectral data analysis
fade down those spectral bands which are not necessary for classification. In
particular, we consider the sparsity in the relevance profile enforced by LASSO
optimization. The latter one is obtained by a gradient learning scheme using a
differentiable parametrized approximation of the $l_{1}$-norm, which has an
upper error bound. We extend this regularization idea also to the matrix
learning variant of LVQ as the natural generalization of relevance learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5096</identifier>
 <datestamp>2013-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5096</id><created>2013-10-18</created><authors><author><keyname>Li</keyname><forenames>Zhong-Lin Han Yu-Jian</forenames></author><author><keyname>Wang</keyname><forenames>Bing-Hong</forenames></author></authors><title>Opinion Dynamic with agents immigration</title><categories>physics.soc-ph cs.SI</categories><comments>this work was finished in (Dated: April 22, 2011)</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We propose a strategy for achieving maximum cooperation in evolutionary games
on complex networks. Each individual is assigned a weight that is proportional
to the power of its degree, where the exponent alpha is an adjustable parameter
that controls the level of diversity among individuals in the network. During
the evolution, every individual chooses one of its neighbors as a reference
with a probability proportional to the weight of the neighbor, and updates its
strategy depending on their payoff difference. It is found that there exists an
optimal value of alpha, for which the level of cooperation reaches maximum.
This phenomenon indicates that, although high-degree individuals play a
prominent role in maintaining the cooperation, too strong influences from the
hubs may counterintuitively inhibit the diffusion of cooperation. We provide a
physical theory, aided by numerical computations, to explain the emergence of
the optimal cooperation. Other pertinent quantities such as the payoff, the
cooperator density as a function of the degree, and the payoff distribution,
are also investigated. Our results suggest that, in order to achieve strong
cooperation on a complex network, individuals should learn more frequently from
neighbors with higher degrees, but only to certain extent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5107</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5107</id><created>2013-10-18</created><authors><author><keyname>Camps-Valls</keyname><forenames>Gustavo</forenames></author><author><keyname>Tuia</keyname><forenames>Devis</forenames></author><author><keyname>Bruzzone</keyname><forenames>Lorenzo</forenames></author><author><keyname>Benediktsson</keyname><forenames>J&#xf3;n Atli</forenames></author></authors><title>Advances in Hyperspectral Image Classification: Earth monitoring with
  statistical learning methods</title><categories>cs.CV</categories><comments>IEEE Signal Processing Magazine, 2013</comments><doi>10.1109/MSP.2013.2279179</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hyperspectral images show similar statistical properties to natural grayscale
or color photographic images. However, the classification of hyperspectral
images is more challenging because of the very high dimensionality of the
pixels and the small number of labeled examples typically available for
learning. These peculiarities lead to particular signal processing problems,
mainly characterized by indetermination and complex manifolds. The framework of
statistical learning has gained popularity in the last decade. New methods have
been presented to account for the spatial homogeneity of images, to include
user's interaction via active learning, to take advantage of the manifold
structure with semisupervised learning, to extract and encode invariances, or
to adapt classifiers and image representations to unseen yet similar scenes.
This tutuorial reviews the main advances for hyperspectral remote sensing image
classification through illustrative examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5111</identifier>
 <datestamp>2014-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5111</id><created>2013-10-18</created><updated>2014-03-06</updated><authors><author><keyname>Lahiri</keyname><forenames>Shibamouli</forenames></author></authors><title>Complexity of Word Collocation Networks: A Preliminary Structural
  Analysis</title><categories>cs.SI physics.soc-ph</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we explore complex network properties of word collocation
networks (Ferret, 2002) from four different genres. Each document of a
particular genre was converted into a network of words with word collocations
as edges. We analyzed graphically and statistically how the global properties
of these networks varied across different genres, and among different network
types within the same genre. Our results indicate that the distributions of
network properties are visually similar but statistically apart across
different genres, and interesting variations emerge when we consider different
network types within a single genre. We further investigate how the global
properties change as we add more and more collocation edges to the graph of one
particular genre, and observe that except for the number of vertices and the
size of the largest connected component, network properties change in phases,
via jumps and drops.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5114</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5114</id><created>2013-10-18</created><updated>2013-12-10</updated><authors><author><keyname>Gueudr&#xe9;</keyname><forenames>Thomas</forenames></author><author><keyname>Dobrinevski</keyname><forenames>Alexander</forenames></author><author><keyname>Bouchaud</keyname><forenames>Jean-Philippe</forenames></author></authors><title>Explore or exploit? A generic model and an exactly solvable case</title><categories>cond-mat.dis-nn cs.LG physics.soc-ph q-fin.GN</categories><comments>5 pages 2 figures</comments><doi>10.1103/PhysRevLett.112.050602</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding a good compromise between the exploitation of known resources and the
exploration of unknown, but potentially more profitable choices, is a general
problem, which arises in many different scientific disciplines. We propose a
stylized model for these exploration-exploitation situations, including
population or economic growth, portfolio optimisation, evolutionary dynamics,
or the problem of optimal pinning of vortices or dislocations in disordered
materials. We find the exact growth rate of this model for tree-like geometries
and prove the existence of an optimal migration rate in this case. Numerical
simulations in the one-dimensional case confirm the generic existence of an
optimum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5124</identifier>
 <datestamp>2014-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5124</id><created>2013-10-18</created><authors><author><keyname>Cheng</keyname><forenames>Qi</forenames></author><author><keyname>Wan</keyname><forenames>Daqing</forenames></author><author><keyname>Zhuang</keyname><forenames>Jincheng</forenames></author></authors><title>Traps to the BGJT-Algorithm for Discrete Logarithms</title><categories>math.NT cs.CR</categories><msc-class>11Y16</msc-class><doi>10.1112/S1461157014000242</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the recent breakthrough paper by Barbulescu, Gaudry, Joux and Thom{\'e}, a
quasi-polynomial time algorithm (QPA) is proposed for the discrete logarithm
problem over finite fields of small characteristic. The time complexity
analysis of the algorithm is based on several heuristics presented in their
paper. We show that some of the heuristics are problematic in their original
forms, in particular, when the field is not a Kummer extension. We believe that
the basic idea behind the new approach should still work, and propose a fix to
the algorithm in non-Kummer cases, without altering the quasi-polynomial time
complexity. The modified algorithm is also heuristic. Further study is required
in order to fully understand the effectiveness of the new approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5125</identifier>
 <datestamp>2013-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5125</id><created>2013-10-18</created><updated>2013-10-29</updated><authors><author><keyname>S</keyname><forenames>Nischal</forenames></author><author><keyname>Sharma</keyname><forenames>Vinod</forenames></author></authors><title>A Joint Uplink/Downlink Opportunistic Scheduling Scheme for
  Infrastructure WLANs</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a combined uplink/downlink opportunistic scheduling algorithm for
infrastructure WLANs. In the presence of both uplink and downlink flows, an
infrastructure WLAN suffers from the uplink/downlink unfairness problem which
severely decreases the throughput of the access point (AP). We resolve the
unfairness by maintaining a separate queue and a backoff timer for each
associated mobile station (STA) at the AP. We also increase the system
throughput by making the backoff time a function of the channel gains. This
reduces the collision probability also. We theoretically analyze the
performance of the system under symmetric statistics for all users and validate
the analysis by extensive simulations. Simulation results show increase in
system throughput by over 40% compared to the 802.11 MAC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5139</identifier>
 <datestamp>2013-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5139</id><created>2013-10-18</created><authors><author><keyname>Szolnoki</keyname><forenames>Attila</forenames></author><author><keyname>Perc</keyname><forenames>Matjaz</forenames></author></authors><title>Correlation of positive and negative reciprocity fails to confer an
  evolutionary advantage: Phase transitions to elementary strategies</title><categories>physics.soc-ph cs.GT q-bio.PE</categories><comments>10 two-column pages, 10 figures; accepted for publication in Physical
  Review X</comments><journal-ref>Phys. Rev. X 3 (2013) 041021</journal-ref><doi>10.1103/PhysRevX.3.041021</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Economic experiments reveal that humans value cooperation and fairness.
Punishing unfair behavior is therefore common, and according to the theory of
strong reciprocity, it is also directly related to rewarding cooperative
behavior. However, empirical data fail to confirm that positive and negative
reciprocity are correlated. Inspired by this disagreement, we determine whether
the combined application of reward and punishment is evolutionary advantageous.
We study a spatial public goods game, where in addition to the three elementary
strategies of defection, rewarding and punishment, a fourth strategy combining
the later two competes for space. We find rich dynamical behavior that gives
rise to intricate phase diagrams where continuous and discontinuous phase
transitions occur in succession. Indirect territorial competition, spontaneous
emergence of cyclic dominance, as well as divergent fluctuations of
oscillations that terminate in an absorbing phase are observed. Yet despite the
high complexity of solutions, the combined strategy can survive only in very
narrow and unrealistic parameter regions. Elementary strategies, either in pure
or mixed phases, are much more common and likely to prevail. Our results
highlight the importance of patterns and structure in human cooperation, which
should be considered in future experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5141</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5141</id><created>2013-10-17</created><authors><author><keyname>Letzter</keyname><forenames>Shoham</forenames></author></authors><title>The Property of Having a $k$-Regular Subgraph Has a Sharp Threshold</title><categories>math.PR cs.DM math.CO</categories><journal-ref>Random Struct. Algorithms 42(4): 509-519 (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the property of containing a $k$-regular subgraph in the random
graph model $G(n,p)$ has a sharp threshold for $k\ge3$. We also show how to use
similar methods to obtain an easy prove for the (known fact of) sharpness of
having a non empty $k$-core for $k\ge3$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5142</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5142</id><created>2013-10-18</created><authors><author><keyname>Jung</keyname><forenames>Hyun Joon</forenames></author><author><keyname>Lease</keyname><forenames>Matthew</forenames></author></authors><title>Crowdsourced Task Routing via Matrix Factorization</title><categories>cs.CY cs.IR</categories><comments>10 pages, 7 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We describe methods to predict a crowd worker's accuracy on new tasks based
on his accuracy on past tasks. Such prediction provides a foundation for
identifying the best workers to route work to in order to maximize accuracy on
the new task. Our key insight is to model similarity of past tasks to the
target task such that past task accuracies can be optimally integrated to
predict target task accuracy. We describe two matrix factorization (MF)
approaches from collaborative filtering which not only exploit such task
similarity, but are known to be robust to sparse data. Experiments on synthetic
and real-world datasets provide feasibility assessment and comparative
evaluation of MF approaches vs. two baseline methods. Across a range of data
scales and task similarity conditions, we evaluate: 1) prediction error over
all workers; and 2) how well each method predicts the best workers to use for
each task. Results show the benefit of task routing over random assignment, the
strength of probabilistic MF over baseline methods, and the robustness of
methods under different conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5163</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5163</id><created>2013-10-18</created><updated>2013-10-21</updated><authors><author><keyname>Young</keyname><forenames>George Forrest</forenames></author><author><keyname>Scardovi</keyname><forenames>Luca</forenames></author><author><keyname>Leonard</keyname><forenames>Naomi Ehrich</forenames></author></authors><title>A New Notion of Effective Resistance for Directed Graphs-Part I:
  Definition and Properties</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The graphical notion of effective resistance has found wide-ranging
applications in many areas of pure mathematics, applied mathematics and control
theory. By the nature of its construction, effective resistance can only be
computed in undirected graphs and yet in several areas of its application,
directed graphs arise as naturally (or more naturally) than undirected ones. In
part I of this work, we propose a generalization of effective resistance to
directed graphs that preserves its control-theoretic properties in relation to
consensus-type dynamics. We proceed to analyze the dependence of our algebraic
definition on the structural properties of the graph and the relationship
between our construction and a graphical distance. The results make possible
the calculation of effective resistance between any two nodes in any directed
graph and provide a solid foundation for the application of effective
resistance to problems involving directed graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5168</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5168</id><created>2013-10-18</created><updated>2013-10-21</updated><authors><author><keyname>Young</keyname><forenames>George Forrest</forenames></author><author><keyname>Scardovi</keyname><forenames>Luca</forenames></author><author><keyname>Leonard</keyname><forenames>Naomi Ehrich</forenames></author></authors><title>A New Notion of Effective Resistance for Directed Graphs-Part II:
  Computing Resistances</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Part I of this work we defined a generalization of the concept of
effective resistance to directed graphs, and we explored some of the properties
of this new definition. Here, we use the theory developed in Part I to compute
effective resistances in some prototypical directed graphs. This exploration
highlights cases where our notion of effective resistance for directed graphs
behaves analogously to our experience from undirected graphs, as well as cases
where it behaves in unexpected ways.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5182</identifier>
 <datestamp>2014-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5182</id><created>2013-10-18</created><updated>2014-06-04</updated><authors><author><keyname>Gramacy</keyname><forenames>Robert B.</forenames></author><author><keyname>Niemi</keyname><forenames>Jarad</forenames></author><author><keyname>Weiss</keyname><forenames>Robin M.</forenames></author></authors><title>Massively parallel approximate Gaussian process regression</title><categories>stat.CO cs.DC</categories><comments>24 pages, 6 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore how the big-three computing paradigms -- symmetric multi-processor
(SMC), graphical processing units (GPUs), and cluster computing -- can together
be brought to bare on large-data Gaussian processes (GP) regression problems
via a careful implementation of a newly developed local approximation scheme.
Our methodological contribution focuses primarily on GPU computation, as this
requires the most care and also provides the largest performance boost.
However, in our empirical work we study the relative merits of all three
paradigms to determine how best to combine them. The paper concludes with two
case studies. One is a real data fluid-dynamics computer experiment which
benefits from the local nature of our approximation; the second is a synthetic
data example designed to find the largest design for which (accurate) GP
emulation can performed on a commensurate predictive set under an hour.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5187</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5187</id><created>2013-10-18</created><authors><author><keyname>Halbawi</keyname><forenames>Wael</forenames></author><author><keyname>Ho</keyname><forenames>Tracey</forenames></author><author><keyname>Yao</keyname><forenames>Hongyi</forenames></author><author><keyname>Duursma</keyname><forenames>Iwan</forenames></author></authors><title>Distributed Reed-Solomon Codes for Simple Multiple Access Networks</title><categories>cs.IT math.IT</categories><comments>12 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a simple multiple access network in which a destination node
receives information from multiple sources via a set of relay nodes. Each relay
node has access to a subset of the sources, and is connected to the destination
by a unit capacity link. We also assume that $z$ of the relay nodes are
adversarial. We propose a computationally efficient distributed coding scheme
and show that it achieves the full capacity region for up to three sources.
Specifically, the relay nodes encode in a distributed fashion such that the
overall codewords received at the destination are codewords from a single
Reed-Solomon code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5199</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5199</id><created>2013-10-18</created><updated>2014-02-20</updated><authors><author><keyname>Rungger</keyname><forenames>Matthias</forenames></author><author><keyname>Tabuada</keyname><forenames>Paulo</forenames></author></authors><title>A Notion of Robustness for Cyber-Physical Systems</title><categories>cs.SY math.OC</categories><acm-class>I.2.8; I.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robustness as a system property describes the degree to which a system is
able to function correctly in the presence of disturbances, i.e., unforeseen or
erroneous inputs. In this paper, we introduce a notion of robustness termed
input-output dynamical stability for cyber-physical systems (CPS) which merges
existing notions of robustness for continuous systems and discrete systems. The
notion captures two intuitive aims of robustness: bounded disturbances have
bounded effects and the consequences of a sporadic disturbance disappear over
time. We present a design methodology for robust CPS which is based on an
abstraction and refinement process. We suggest several novel notions of
simulation relations to ensure the soundness of the approach. In addition, we
show how such simulation relations can be constructed compositionally. The
different concepts and results are illustrated throughout the paper with
examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5202</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5202</id><created>2013-10-19</created><updated>2015-10-20</updated><authors><author><keyname>Arslan</keyname><forenames>Omur</forenames></author><author><keyname>Guralnik</keyname><forenames>Dan P.</forenames></author><author><keyname>Koditschek</keyname><forenames>Daniel E.</forenames></author></authors><title>Discriminative Measures for Comparison of Phylogenetic Trees</title><categories>q-bio.PE cs.CE cs.CG</categories><comments>24 pages, 7 figures, 1 table, a new graph-theoretic formulation of
  the NNI navigation dissimilarity</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce and study three new measures for efficient
discriminative comparison of phylogenetic trees. The NNI navigation
dissimilarity $d_{nav}$ counts the steps along a &quot;combing&quot; of the Nearest
Neighbor Interchange (NNI) graph of binary hierarchies, providing an efficient
approximation to the (NP-hard) NNI distance in terms of &quot;edit length&quot;. At the
same time, a closed form formula for $d_{nav}$ presents it as a weighted count
of pairwise incompatibilities between clusters, lending it the character of an
edge dissimilarity measure as well. A relaxation of this formula to a simple
count yields another measure on all trees --- the crossing dissimilarity
$d_{CM}$. Both dissimilarities are symmetric and positive definite (vanish only
between identical trees) on binary hierarchies but they fail to satisfy the
triangle inequality. Nevertheless, both are bounded below by the widely used
Robinson-Foulds metric and bounded above by a closely related true metric, the
cluster-cardinality metric $d_{CC}$. We show that each of the three proposed
new dissimilarities is computable in time $O(n^2)$ in the number of leaves $n$,
and conclude the paper with a brief numerical exploration of the distribution
over tree space of these dissimilarities in comparison with the Robinson-Foulds
metric and the more recently introduced matching-split distance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5205</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5205</id><created>2013-10-19</created><authors><author><keyname>Dong</keyname><forenames>Gaogao</forenames></author><author><keyname>Tian</keyname><forenames>Lixin</forenames></author><author><keyname>Du</keyname><forenames>Ruijin</forenames></author><author><keyname>Stanley</keyname><forenames>H. Eugene</forenames></author></authors><title>Robustness of Network of Networks with Interdependent and Interconnected
  links</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robustness of network of networks (NON) has been studied only for dependency
coupling (J.X. Gao et. al., Nature Physics, 2012) and only for connectivity
coupling (E.A. Leicht and R.M. D Souza, arxiv:0907.0894). The case of network
of n networks with both interdependent and interconnected links is more
complicated, and also more closely to real-life coupled network systems. Here
we develop a framework to study analytically and numerically the robustness of
this system. For the case of starlike network of n ER networks, we find that
the system undergoes from second order to first order phase transition as
coupling strength q increases. We find that increasing intra-connectivity links
or inter-connectivity links can increase the robustness of the system, while
the interdependency links decrease its robustness. Especially when q=1, we find
exact analytical solutions of the giant component and the first order
transition point. Understanding the robustness of network of networks with
interdependent and interconnected links is helpful to design resilient
infrastructures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5207</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5207</id><created>2013-10-19</created><updated>2015-09-21</updated><authors><author><keyname>Shankar</keyname><forenames>Varun</forenames></author><author><keyname>Wright</keyname><forenames>Grady B.</forenames></author><author><keyname>Fogelson</keyname><forenames>Aaron L.</forenames></author><author><keyname>Kirby</keyname><forenames>Robert M.</forenames></author></authors><title>A Radial Basis Function (RBF)-Finite Difference Method for the
  Simulation of Reaction-Diffusion Equations on Stationary Platelets within the
  Augmented Forcing Method</title><categories>math.NA cs.CE cs.NA q-bio.QM</categories><comments>27 pages, 7 figures, 9 tables</comments><journal-ref>International Journal for Numerical Methods in Fluids, Volume 75,
  1,1-22, 2014</journal-ref><doi>10.1002/fld.3880</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a computational method for solving the coupled problem of chemical
transport in a fluid (blood) with binding/unbinding of the chemical to/from
cellular (platelet) surfaces in contact with the fluid, and with transport of
the chemical on the cellular surfaces. The overall framework is the Augmented
Forcing Point Method (AFM) (\emph{L. Yao and A.L. Fogelson, Simulations of
chemical transport and reaction in a suspension of cells I: An augmented
forcing point method for the stationary case, IJNMF (2012) 69, 1736-52.}) for
solving fluid-phase transport in a region outside of a collection of cells
suspended in the fluid. We introduce a novel Radial Basis Function-Finite
Difference (RBF-FD) method to solve reaction-diffusion equations on the surface
of each of a collection of 2D stationary platelets suspended in blood.
Parametric RBFs are used to represent the geometry of the platelets and give
accurate geometric information needed for the RBF-FD method. Symmetric
Hermite-RBF interpolants are used for enforcing the boundary conditions on the
fluid-phase chemical concentration, and their use removes a significant
limitation of the original AFM. The efficacy of the new methods are shown
through a series of numerical experiments; in particular, second order
convergence for the coupled problem is demonstrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5220</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5220</id><created>2013-10-19</created><authors><author><keyname>Sehra</keyname><forenames>Sumeet Kaur</forenames></author><author><keyname>Brar</keyname><forenames>Dr. Yadwinder Singh</forenames></author><author><keyname>Kaur</keyname><forenames>Dr. Navdeep</forenames></author></authors><title>Multi criteria decision making approach for selecting effort estimation
  model</title><categories>cs.SE</categories><journal-ref>International Journal of Computer Applications 39(1):10-17,
  February 2012</journal-ref><doi>10.5120/4783-6989</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Effort Estimation has always been a challenging task for the Project
managers. Many researchers have tried to help them by creating different types
of models. This has been already proved that none is successful for all types
of projects and every type of environment. Analytic Hierarchy Process has been
identified as the tool that would help in Multi Criteria Decision Making.
Researchers have identified that Analytic Hierarchy Process can be used for the
comparison of effort estimation of different models and techniques. But the
problem with traditional Analytic Hierarchy Process is its inability to deal
with the imprecision and subjectivity in the pairwise comparison process. The
motive of this paper is to propose Fuzzy Analytic Hierarchy Process, which can
be used to rectify the subjectivity and imprecision of Analytic Hierarchy
Process and can be used for selecting the type of Model best suited for
estimating the effort for a given problem type or environment. Instead of
single crisp value, Fuzzy Analytic Hierarchy Process uses a range of values to
incorporate decision maker uncertainty. From this range, decision maker can
select the value that reflects his confidence and also he can specify his
attitude like optimistic, pessimistic or moderate. In this work, the comparison
of Analytic Hierarchy Process and Fuzzy Analytic Hierarchy Process is concluded
using a case study of selection of effort estimation model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5221</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5221</id><created>2013-10-19</created><authors><author><keyname>Sehra</keyname><forenames>Sumeet Kaur</forenames></author><author><keyname>Brar</keyname><forenames>Yadwinder Singh</forenames></author><author><keyname>Kaur</keyname><forenames>Navdeep</forenames></author></authors><title>Soft computing techniques for software effort estimation</title><categories>cs.SE</categories><journal-ref>International Journal of Advanced Computer and Mathematical
  Sciences Vol.2(3), November, 2011. pp:10-17</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The effort invested in a software project is probably one of the most
important and most analyzed variables in recent years in the process of project
management. The limitation of algorithmic effort prediction models is their
inability to cope with uncertainties and imprecision surrounding software
projects at the early development stage. More recently attention has turned to
a variety of machine learning methods, and soft computing in particular to
predict software development effort. Soft computing is a consortium of
methodologies centering in fuzzy logic, artificial neural networks, and
evolutionary computation. It is important, to mention here, that these
methodologies are complementary and synergistic, rather than competitive. They
provide in one form or another flexible information processing capability for
handling real life ambiguous situations. These methodologies are currently used
for reliable and accurate estimate of software development effort, which has
always been a challenge for both the software industry and academia. The aim of
this study is to analyze soft computing techniques in the existing models and
to provide in depth review of software and project estimation techniques
existing in industry and literature based on the different test datasets along
with their strength and weaknesses
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5222</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5222</id><created>2013-10-19</created><authors><author><keyname>Sehra</keyname><forenames>Sumeet Kaur</forenames></author><author><keyname>Kaur</keyname><forenames>Jasneet</forenames></author><author><keyname>Sehra</keyname><forenames>Sukhjit Singh</forenames></author></authors><title>Effect of data preprocessing on software effort estimation</title><categories>cs.SE</categories><journal-ref>International Journal of Computer Applications 69(25):29-32, May
  2013</journal-ref><doi>10.5120/12130-8506</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software effort estimation requires high accuracy, but accurate estimations
are difficult to achieve. Increasingly, data mining is used to improve an
organization's software process quality, e. g. the accuracy of effort
estimations . There are a large number of different method combination exists
for software effort estimation, selecting the most suitable combination becomes
the subject of research in this paper. In this study, three simple
preprocessors are taken (none, norm, log) and effort is measured using COCOMO
model. Then results obtained from different preprocessors are compared and norm
preprocessor proves to be more accurate as compared to other preprocessors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5225</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5225</id><created>2013-10-19</created><authors><author><keyname>Ilyas</keyname><forenames>Muhammad</forenames></author><author><keyname>Yamada</keyname><forenames>Mieko</forenames></author></authors><title>Generalized Extended Hamming Codes over Galois Ring of Characteristic
  $2^{n}$</title><categories>cs.IT math.IT</categories><comments>12 pages, International Symposium on Computational Science 2011
  Selected Paper for Gakuto International Series Mathematical Science and
  Application Volume 34, 2011</comments><msc-class>94B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce generalized extended Hamming codes over Galois
rings $GR(2^n,m)$ of characteristic $2^n$ with extension degree $m$.
Furthermore we prove that the minimum Hamming weight of generalized extended
Hamming codes over $GR(2^n,m)$ is 4 and the minimum Lee weight of generalized
extended Hamming codes over $GR(8,m)$ is 6 for all $m \geq 3$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5230</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5230</id><created>2013-10-19</created><authors><author><keyname>Bauwens</keyname><forenames>Bruno</forenames></author></authors><title>Prefix and plain Kolmogorov complexity characterizations of
  2-randomness: simple proofs</title><categories>cs.IT math.IT</categories><comments>16 pages, 2 figures</comments><msc-class>03D32</msc-class><acm-class>F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Joseph Miller [16] and independently Andre Nies, Frank Stephan and Sebastiaan
Terwijn [18] gave a complexity characterization of 2-random sequences in terms
of plain Kolmogorov complexity C: they are sequences that have infinitely many
initial segments with O(1)-maximal plain complexity (among the strings of the
same length). Later Miller [17] showed that prefix complexity K can also be
used in a similar way: a sequence is 2-random if and only if it has infinitely
many initial segments with O(1)-maximal prefix complexity (which is n + K (n)
for strings of length n). The known proofs of these results are quite involved;
in this paper we provide simple direct proofs for both of them.
  In [16] Miller also gave a quantitative version of the first result: the
0'-randomness deficiency of a sequence {\omega} equals lim inf [n - C
({\omega}1 . . . {\omega}n)] + O(1). (Our simplified proof can also be used to
prove this.) We show (and this seems to be a new result) that a similar
quantitative result is also true for prefix complexity: 0'-randomness
deficiency equals lim inf [n + K (n) -- K ({\omega}1 . . . {\omega}n)] + O(1).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5231</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5231</id><created>2013-10-19</created><updated>2015-01-16</updated><authors><author><keyname>Jain</keyname><forenames>Anugrah</forenames></author></authors><title>Fault Tolerant Synthesis of Reversible Circuits</title><categories>cs.ET quant-ph</categories><comments>This paper has been withdrawn by the author. Because of having some
  serious errors this paper may mislead the readers</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reversible computing has emerged as a possible low cost alternative to
conventional computing in terms of speed, power consumption and computing
capability. In order to achieve reliable circuits in reversible computing,
provision for fault tolerance is necessary. A number of fault models, fault
tolerant techniques (such as parity-preserving) and testing approaches have
proposed in literature. This dissertation exploits parity-preserving
characteristics of two reversible gates which provide low cost
parity-preserving based fault tolerance. In order to extend online testability
of reversible circuits, the substitution of Peres gate has been presented. The
online testing capabilities of MCF including Swap and Fredkin gates were also
identifies. Finally a tool was developed to implement all above substitutions
and converting any reversible circuit to parity-preserving based fault tolerant
circuit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5242</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5242</id><created>2013-10-19</created><authors><author><keyname>D'Angeli</keyname><forenames>Daniele</forenames></author><author><keyname>Rodaro</keyname><forenames>Emanuele</forenames></author></authors><title>Groups and Semigroups Defined by Colorings of Synchronizing Automata</title><categories>math.GR cs.FL math.CO</categories><msc-class>20E08, 05E99, 20M35, 20M05</msc-class><journal-ref>International Journal of Algebra and Computation (2014), Volume
  24(06), pp. 773-793</journal-ref><doi>10.1142/S0218196714500337</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we combine the algebraic properties of Mealy machines
generating self-similar groups and the combinatorial properties of the
corresponding deterministic finite automata (DFA). In particular, we relate
bounded automata to finitely generated synchronizing automata and characterize
finite automata groups in terms of nilpotency of the corresponding DFA.
Moreover, we present a decidable sufficient condition to have free semigroups
in an automaton group. A series of examples and applications is widely
discussed, in particular we show a way to color the De Bruijn automata into
Mealy automata whose associated semigroups are free, and we present some
structural results related to the associated groups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5245</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5245</id><created>2013-10-19</created><updated>2014-04-09</updated><authors><author><keyname>Avraham</keyname><forenames>Rinat Ben</forenames></author><author><keyname>Filtser</keyname><forenames>Omrit</forenames></author><author><keyname>Kaplan</keyname><forenames>Haim</forenames></author><author><keyname>Katz</keyname><forenames>Matthew J.</forenames></author><author><keyname>Sharir</keyname><forenames>Micha</forenames></author></authors><title>The Discrete and Semi-continuous Fr\'echet Distance with Shortcuts via
  Approximate Distance Counting and Selection</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The \emph{Fr\'echet distance} is a well studied similarity measures between
curves. The \emph{discrete Fr\'echet distance} is an analogous similarity
measure, defined for a sequence $A$ of $m$ points and a sequence $B$ of $n$
points, where the points are usually sampled from input curves. In this paper
we consider a variant, called the \emph{discrete Fr\'echet distance with
shortcuts}, which captures the similarity between (sampled) curves in the
presence of outliers. For the \emph{two-sided} case, where shortcuts are
allowed in both curves, we give an $O((m^{2/3}n^{2/3}+m+n)\log^3 (m+n))$-time
algorithm for computing this distance. When shortcuts are allowed only in one
noise-containing curve, we give an even faster randomized algorithm that runs
in $O((m+n)^{6/5+\eps})$ expected time, for any $\eps&gt;0$.
  Our techniques are novel and may find further applications. One of the main
new technical results is: Given two sets of points $A$ and $B$ and an interval
$I$, we develop an algorithm that decides whether the number of pairs $(x,y)\in
A\times B$ whose distance ${\rm dist}(x,y)$ is in $I$, is less than some given
threshold $L$. The running time of this algorithm decreases as $L$ increases.
In case there are more than $L$ pairs of points whose distance is in $I$, we
can get a small sample of pairs that contains a pair at approximate median
distance (i.e., we can approximately &quot;bisect&quot; $I$). We combine this procedure
with additional ideas to search, with a small overhead, for the optimal
one-sided Fr\'echet distance with shortcuts, using a very fast decision
procedure. We also show how to apply this technique for approximating distance
selection (with respect to rank), and for computing the semi-continuous
Fr\'echet distance with one-sided shortcuts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5246</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5246</id><created>2013-10-19</created><updated>2013-11-17</updated><authors><author><keyname>Myasnikov</keyname><forenames>Alexei</forenames></author><author><keyname>Nikolaev</keyname><forenames>Andrey</forenames></author><author><keyname>Ushakov</keyname><forenames>Alexander</forenames></author></authors><title>The Post correspondence problem in groups</title><categories>math.GR cs.CC math.CO</categories><comments>13 pages</comments><msc-class>03D15, 20F65, 20F10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalize the classical Post correspondence problem ($\mathbf{PCP}_n$)
and its non-homogeneous variation ($\mathbf{GPCP}_n$) to non-commutative groups
and study the computational complexity of these new problems. We observe that
$\mathbf{PCP}_n$ is closely related to the equalizer problem in groups, while
$\mathbf{GPCP}_n$ is connected to the double twisted conjugacy problem for
endomorphisms. Furthermore, it is shown that one of the strongest forms of the
word problem in a group $G$ (we call it the {\em hereditary word problem}) can
be reduced to $\mathbf{GPCP}_n$ in $G$ in polynomial time.
  The main results are that $\mathbf{PCP}_n$ is decidable in a finitely
generated nilpotent group in polynomial time, while $\mathbf{GPCP}_n$ is
undecidable in any group containing free non-abelian subgroup (though the
argument is very different from the classical case of free semigroups). We show
that the double endomorphism twisted conjugacy problem is undecidable in free
groups of sufficiently large finite rank. We also consider the bounded
$\mathbf{PCP}$ and observe that it is in $\mathbf{NP}$ for any group with
$\mathbf{P}$-time decidable word problem, meanwhile it is $\mathbf{NP}$-hard in
any group containing free non-abelian subgroup. In particular, the bounded
$\mathbf{PCP}$ is $\mathbf{NP}$-complete in non-elementary hyperbolic groups
and non-abelian right angle Artin groups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5249</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5249</id><created>2013-10-19</created><authors><author><keyname>Mahrsi</keyname><forenames>Mohamed Khalil El</forenames><affiliation>LTCI, SAMM</affiliation></author><author><keyname>Rossi</keyname><forenames>Fabrice</forenames><affiliation>SAMM</affiliation></author></authors><title>Graph-Based Approaches to Clustering Network-Constrained Trajectory Data</title><categories>cs.LG</categories><proxy>ccsd</proxy><report-no>NFMCP2013</report-no><journal-ref>New Frontiers in Mining Complex Patterns, Appice, Annalisa and
  Ceci, Michelangelo and Loglisci, Corrado and Manco, Giuseppe and Masciari,
  Elio and Ras, Zbigniew (Ed.) (2013) 124-137</journal-ref><doi>10.1007/978-3-642-37382-4_9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clustering trajectory data attracted considerable attention in the last few
years. Most of prior work assumed that moving objects can move freely in an
euclidean space and did not consider the eventual presence of an underlying
road network and its influence on evaluating the similarity between
trajectories. In this paper, we present an approach to clustering such
network-constrained trajectory data. More precisely we aim at discovering
groups of road segments that are often travelled by the same trajectories. To
achieve this end, we model the interactions between segments w.r.t. their
similarity as a weighted graph to which we apply a community detection
algorithm to discover meaningful clusters. We showcase our proposition through
experimental results obtained on synthetic datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5250</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5250</id><created>2013-10-19</created><authors><author><keyname>Smith</keyname><forenames>Benjamin</forenames><affiliation>INRIA Saclay - Ile de France, LIX</affiliation></author></authors><title>Easy scalar decompositions for efficient scalar multiplication on
  elliptic curves and genus 2 Jacobians</title><categories>math.NT cs.CR</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The first step in elliptic curve scalar multiplication algorithms based on
scalar decompositions using efficient endomorphisms-including
Gallant-Lambert-Vanstone (GLV) and Galbraith-Lin-Scott (GLS) multiplication, as
well as higher-dimensional and higher-genus constructions-is to produce a short
basis of a certain integer lattice involving the eigenvalues of the
endomorphisms. The shorter the basis vectors, the shorter the decomposed scalar
coefficients, and the faster the resulting scalar multiplication. Typically,
knowledge of the eigenvalues allows us to write down a long basis, which we
then reduce using the Euclidean algorithm, Gauss reduction, LLL, or even a more
specialized algorithm. In this work, we use elementary facts about quadratic
rings to immediately write down a short basis of the lattice for the GLV, GLS,
GLV+GLS, and Q-curve constructions on elliptic curves, and for genus 2 real
multiplication constructions. We do not pretend that this represents a
significant optimization in scalar multiplication, since the lattice reduction
step is always an offline precomputation---but it does give a better insight
into the structure of scalar decompositions. In any case, it is always more
convenient to use a ready-made short basis than it is to compute a new one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5251</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5251</id><created>2013-10-19</created><updated>2014-05-15</updated><authors><author><keyname>Chepuri</keyname><forenames>Sundeep Prabhakar</forenames></author><author><keyname>Leus</keyname><forenames>Geert</forenames></author></authors><title>Sparsity-Promoting Sensor Selection for Non-linear Measurement Models</title><categories>cs.IT math.IT</categories><comments>13 pages, submitted to TSP (revised Mar. 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sensor selection is an important design problem in large-scale sensor
networks. Sensor selection can be interpreted as the problem of selecting the
best subset of sensors that guarantees a certain estimation performance. We
focus on observations that are related to a general non-linear model. The
proposed framework is valid as long as the observations are independent, and
its likelihood satisfies the regularity conditions. We use several functions of
the Cram\'er-Rao bound (CRB) as a performance measure. We formulate the sensor
selection problem as the design of a selection vector, which in its original
form is a nonconvex l0-(quasi) norm optimization problem. We present relaxed
sensor selection solvers that can be efficiently solved in polynomial time. We
also propose a projected subgradient algorithm that is attractive for
large-scale problems and also show how the algorithm can be easily distributed.
The proposed framework is illustrated with a number of examples related to
sensor placement design for localization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5254</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5254</id><created>2013-10-19</created><authors><author><keyname>Bukhari</keyname><forenames>Syed Ijaz Ahmad</forenames></author></authors><title>Real Time Data Warehouse</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data Warehouse (DW) is an essential part of Business Intelligence. DW emerged
as a fast growing reporting and analysis technique in early 1980s. Today, it
has almost replaced relational databases. However, with passage of time, static
and historic data of DWs could not produce Real Time reporting and analysis,
thus giving a way to emerge the Idea of Real Time Data Warehouse (RTDW).
Although, there are problems with RTDWs, but with advancement in technology and
researchers focus, RTDWs will be able to generate real time reports, analysis
and forecasting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5255</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5255</id><created>2013-10-19</created><authors><author><keyname>Beaumont</keyname><forenames>Olivier</forenames><affiliation>INRIA Bordeaux - Sud-Ouest, LaBRI</affiliation></author><author><keyname>Eyraud-Dubois</keyname><forenames>Lionel</forenames><affiliation>INRIA Bordeaux - Sud-Ouest, LaBRI</affiliation></author><author><keyname>Renaud-Goud</keyname><forenames>Paul</forenames><affiliation>INRIA Bordeaux - Sud-Ouest, LaBRI</affiliation></author></authors><title>Efficient and Robust Allocation Algorithms in Clouds under Memory
  Constraints</title><categories>cs.DC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider robust resource allocation of services in Clouds. More
specifically, we consider the case of a large public or private Cloud platform
that runs a relatively small set of large and independent services. These
services are characterized by their demand along several dimensions (CPU,
memory,...) and by their quality of service requirements, that have been
defined through an SLA in the case of a public Cloud or fixed by the
administrator in the case of a private Cloud. This quality of service defines
the required robustness of the service, by setting an upper limit on the
probability that the provider fails to allocate the required quantity of
resources. This maximum probability of failure can be transparently turned into
a pair (price,penalty). Failures can indeed hit the platform, and resilience is
provided through service replication. Our contribution is two-fold. First, we
propose a resource allocation strategy whose complexity is logarithmic in the
number of resources, what makes it very efficient for large platforms. Second,
we propose an efficient algorithm based on rare events detection techniques in
order to estimate the robustness of an allocation, a problem that has been
proven to be P-complete. Finally, we provide an analysis of the proposed
strategy through an extensive set of simulations, both in terms of the overall
number of allocated resources and in terms of time necessary to compute the
allocation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5288</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5288</id><created>2013-10-19</created><updated>2013-12-31</updated><authors><author><keyname>Wilson</keyname><forenames>Andrew Gordon</forenames></author><author><keyname>Gilboa</keyname><forenames>Elad</forenames></author><author><keyname>Nehorai</keyname><forenames>Arye</forenames></author><author><keyname>Cunningham</keyname><forenames>John P.</forenames></author></authors><title>GPatt: Fast Multidimensional Pattern Extrapolation with Gaussian
  Processes</title><categories>stat.ML cs.AI cs.LG stat.ME</categories><comments>13 Pages, 9 Figures, 1 Table. Submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gaussian processes are typically used for smoothing and interpolation on
small datasets. We introduce a new Bayesian nonparametric framework -- GPatt --
enabling automatic pattern extrapolation with Gaussian processes on large
multidimensional datasets. GPatt unifies and extends highly expressive kernels
and fast exact inference techniques. Without human intervention -- no hand
crafting of kernel features, and no sophisticated initialisation procedures --
we show that GPatt can solve large scale pattern extrapolation, inpainting, and
kernel discovery problems, including a problem with 383400 training points. We
find that GPatt significantly outperforms popular alternative scalable Gaussian
process methods in speed and accuracy. Moreover, we discover profound
differences between each of these methods, suggesting expressive kernels,
nonparametric representations, and exact inference are useful for modelling
large scale multidimensional patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5306</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5306</id><created>2013-10-20</created><authors><author><keyname>Papaioannnou</keyname><forenames>Panagiotis</forenames></author><author><keyname>Russo</keyname><forenames>Lucia</forenames></author><author><keyname>Papaioannou</keyname><forenames>George</forenames></author><author><keyname>Siettos</keyname><forenames>Constantinos</forenames></author></authors><title>Can social microblogging be used to forecast intraday exchange rates?</title><categories>cs.SI cs.CE q-fin.ST</categories><comments>This is a prior version of the paper published at NETNOMICS. The
  final publication is available at
  http://www.springer.com/economics/economic+theory/journal/11066</comments><journal-ref>Netnomics, 14, 47-68 (2013)</journal-ref><doi>10.1007/s11066-013-9079-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Efficient Market Hypothesis (EMH) is widely accepted to hold true under
certain assumptions. One of its implications is that the prediction of stock
prices at least in the short run cannot outperform the random walk model. Yet,
recently many studies stressing the psychological and social dimension of
financial behavior have challenged the validity of the EMH. Towards this aim,
over the last few years, internet-based communication platforms and search
engines have been used to extract early indicators of social and economic
trends. Here, we used Twitter's social networking platform to model and
forecast the EUR/USD exchange rate in a high-frequency intradaily trading
scale. Using time series and trading simulations analysis, we provide some
evidence that the information provided in social microblogging platforms such
as Twitter can in certain cases enhance the forecasting efficiency regarding
the very short (intradaily) forex.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5347</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5347</id><created>2013-10-20</created><authors><author><keyname>Park</keyname><forenames>Il Memming</forenames></author><author><keyname>Seth</keyname><forenames>Sohan</forenames></author><author><keyname>Van Vaerenbergh</keyname><forenames>Steven</forenames></author></authors><title>Bayesian Extensions of Kernel Least Mean Squares</title><categories>stat.ML cs.LG</categories><comments>7 pages, 4 fiures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The kernel least mean squares (KLMS) algorithm is a computationally efficient
nonlinear adaptive filtering method that &quot;kernelizes&quot; the celebrated (linear)
least mean squares algorithm. We demonstrate that the least mean squares
algorithm is closely related to the Kalman filtering, and thus, the KLMS can be
interpreted as an approximate Bayesian filtering method. This allows us to
systematically develop extensions of the KLMS by modifying the underlying
state-space and observation models. The resulting extensions introduce many
desirable properties such as &quot;forgetting&quot;, and the ability to learn from
discrete data, while retaining the computational simplicity and time complexity
of the original algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5356</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5356</id><created>2013-10-20</created><authors><author><keyname>Sznajd-Weron</keyname><forenames>Katarzyna</forenames></author><author><keyname>Szwabinski</keyname><forenames>Janusz</forenames></author><author><keyname>Weron</keyname><forenames>Rafal</forenames></author><author><keyname>Weron</keyname><forenames>Tomasz</forenames></author></authors><title>Rewiring the network. What helps an innovation to diffuse?</title><categories>physics.soc-ph cs.SI</categories><doi>10.1088/1742-5468/2014/03/P03007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fundamental question related to innovation diffusion is how the social
network structure influences the process. Empirical evidence regarding
real-world influence networks is very limited. On the other hand, agent-based
modeling literature reports different and at times seemingly contradictory
results. In this paper we study innovation diffusion processes for a range of
Watts-Strogatz networks in an attempt to shed more light on this problem. Using
the so-called Sznajd model as the backbone of opinion dynamics, we find that
the published results are in fact consistent and allow to predict the role of
network topology in various situations. In particular, the diffusion of
innovation is easier on more regular graphs, i.e. with a higher clustering
coefficient. Moreover, in the case of uncertainty - which is particularly high
for innovations connected to public health programs or ecological campaigns - a
more clustered network will help the diffusion. On the other hand, when social
influence is less important (i.e. in the case of perfect information), a
shorter path will help the innovation to spread in the society and - as a
result - the diffusion will be easiest on a random graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5359</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5359</id><created>2013-10-20</created><updated>2014-05-12</updated><authors><author><keyname>Barg</keyname><forenames>Alexander</forenames></author><author><keyname>Skriganov</keyname><forenames>Maxim</forenames></author></authors><title>Association schemes on general measure spaces and zero-dimensional
  Abelian groups</title><categories>math.FA cs.IT math.CO math.IT</categories><comments>80 pages, no figures. This revision contains several minor
  corrections; we also added a short Sect. 6-B on the non-polynomiality of
  metric schemes with a non-Archimedean metric</comments><msc-class>05E30, 22B05, 42C40, 43A70, 94B60</msc-class><journal-ref>Advances in Mathematics, vol. 281 (2015), pp. 142-247</journal-ref><doi>10.1016/j.aim.2015.04.026</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Association schemes form one of the main objects of algebraic combinatorics,
classically defined on finite sets. In this paper we define association schemes
on arbitrary, possibly uncountable sets with a measure. We study operator
realizations of the adjacency algebras of schemes and derive simple properties
of these algebras. To develop a theory of general association schemes, we focus
on schemes on topological Abelian groups where we can employ duality theory and
the machinery of harmonic analysis. We construct translation association
schemes on such groups using the language of spectrally dual partitions. Such
partitions are shown to arise naturally on topological zero-dimensional Abelian
groups, for instance, Cantor-type groups or the groups of p-adic numbers. This
enables us to construct large classes of dual pairs of association schemes on
zero-dimensional groups with respect to their Haar measure, and to compute
their eigenvalues and intersection numbers. We also derive properties of
infinite metric schemes, connecting them with the properties of the
non-Archimedean metric on the group.
  Pursuing the connection between schemes on zero-dimensional groups and
harmonic analysis, we show that the eigenvalues have a natural interpretation
in terms of Littlewood-Paley wavelet bases, and in the (equivalent) language of
martingale theory. For a class of nonmetric schemes constructed in the paper,
the eigenvalues coincide with values of orthogonal functions on
zero-dimensional groups. We observe that these functions, which we call
Haar-like bases, have the properties of wavelets on the group, including in
some special cases the self-similarity property. This establishes a seemingly
new link between algebraic combinatorics and harmonic analysis.
  We conclude the paper by studying some analogs of problems of classical
coding theory related to the theory of association schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5363</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5363</id><created>2013-10-20</created><updated>2014-05-21</updated><authors><author><keyname>Tyszka</keyname><forenames>Apoloniusz</forenames></author></authors><title>MuPAD codes which implement limit-computable functions that cannot be
  bounded by any computable function</title><categories>cs.CC cs.LO</categories><comments>16 pages, Theorem 1 strengthened</comments><msc-class>03D25, 11U05, 68Q05</msc-class><journal-ref>Annals of Computer Science and Information Systems, vol. 2, pp.
  623-629 (2014)</journal-ref><doi>10.15439/2014F91</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a positive integer n, let f(n) denote the smallest non-negative integer b
such that for each system S \subseteq {x_k=1,x_i+x_j=x_k,x_i*x_j=x_k: i,j,k \in
{1,...,n}} with a solution in non-negative integers x_1,...,x_n, there exists a
solution of S in {0,...,b}^n. We prove that the function f is strictly
increasing and dominates all computable functions. We present an infinite loop
in MuPAD which takes as input a positive integer n and returns a non-negative
integer on each iteration. Let g(n,m) denote the number returned on the m-th
iteration, if n is taken as input. Then, g(n,m) \leq m-1, 0=g(n,1)&lt;1=g(n,2)
\leq g(n,3) \leq g(n,4) \leq ... and
g(n,f(n))&lt;f(n)=g(n,f(n)+1)=g(n,f(n)+2)=g(n,f(n)+3)=....
  A MuPAD code constructed on another principle contains a repeat-until loop
and implements a limit-computable function \xi: N--&gt;N that cannot be bounded by
any computable function. This code takes as input a non-negative integer n,
immediately returns 0, and computes a system S of polynomial equations. If the
loop terminates for S, then the next instruction is executed and returns
\xi(n).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5367</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5367</id><created>2013-10-20</created><authors><author><keyname>Talwar</keyname><forenames>Kunal</forenames></author><author><keyname>Wieder</keyname><forenames>Udi</forenames></author></authors><title>Balanced Allocations: A Simple Proof for the Heavily Loaded Case</title><categories>cs.DM cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a relatively simple proof that the expected gap between the
maximum load and the average load in the two choice process is bounded by
$(1+o(1))\log \log n$, irrespective of the number of balls thrown. The theorem
was first proven by Berenbrink et al. Their proof uses heavy machinery from
Markov-Chain theory and some of the calculations are done using computers. In
this manuscript we provide a significantly simpler proof that is not aided by
computers and is self contained. The simplification comes at a cost of weaker
bounds on the low order terms and a weaker tail bound for the probability of
deviating from the expectation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5372</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5372</id><created>2013-10-20</created><authors><author><keyname>Sattath</keyname><forenames>Or</forenames></author></authors><title>An Almost Sudden Jump in Quantum Complexity</title><categories>quant-ph cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Quantum Satisfiability problem (QSAT) is the generalization of the
canonical NP-complete problem - Boolean Satisfiability. (k,s)-QSAT is the
following variant of the problem: given a set of projectors of rank 1, acting
non-trivially on k qubits out of n qubits, such that each qubit appears in at
most s projectors, decide whether there exists a quantum state in the null
space of all the projectors. Let f*(k) be the maximal integer s such that every
(k,s)-QSAT instance is satisfiable. Deciding (k,f*(k))-QSAT is computationally
easy: by definition the answer is &quot;satisfiable&quot;. But, by relaxing the
conditions slightly, we show that (k,f*(k)+2)-QSAT is QMA_1-hard, for k &gt;=15.
This is a quantum analogue of a classical result by Kratochv\'il et al.
[KST93]. We use the term &quot;an almost sudden jump&quot; to stress that the complexity
of (k,f*(k)+1)-QSAT is open, where the jump in the classical complexity is
known to be sudden.
  We present an implication of this finding to the quantum PCP conjecture,
arguably one of the most important open problems in the field of Hamiltonian
complexity. Our implications impose constraints on one possible way to refute
the quantum PCP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5376</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5376</id><created>2013-10-20</created><authors><author><keyname>Leslie</keyname><forenames>Martin</forenames></author></authors><title>Hypermap-Homology Quantum Codes (Ph.D. thesis)</title><categories>cs.IT math.IT quant-ph</categories><comments>Author's Ph.D. thesis (University of Arizona, May 2013). Adviser:
  Marek Rychlik</comments><msc-class>68P30 68Q12 05C65 05C10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new type of sparse CSS quantum error correcting code based on
the homology of hypermaps. Sparse quantum error correcting codes are of
interest in the building of quantum computers due to their ease of
implementation and the possibility of developing fast decoders for them. Codes
based on the homology of embeddings of graphs, such as Kitaev's toric code,
have been discussed widely in the literature and our class of codes generalize
these. We use embedded hypergraphs, which are a generalization of graphs that
can have edges connected to more than two vertices. We develop theorems and
examples of our hypermap-homology codes, especially in the case that we choose
a special type of basis in our homology chain complex. In particular the most
straightforward generalization of the $m \times m$ toric code to
hypermap-homology codes gives us a $[(3/2)m^2,2,m]$ code as compared to the
toric code which is a $[2m^2,2,m]$ code. Thus we can protect the same amount of
quantum information, with the same error-correcting capability, using less
physical qubits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5380</identifier>
 <datestamp>2014-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5380</id><created>2013-10-20</created><updated>2014-02-21</updated><authors><author><keyname>Burckel</keyname><forenames>Serge</forenames></author><author><keyname>Gioan</keyname><forenames>Emeric</forenames></author><author><keyname>Thom&#xe9;</keyname><forenames>Emmanuel</forenames></author></authors><title>Computation with No Memory, and Rearrangeable Multicast Networks</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the computation of mappings from a set S^n to itself with &quot;in
situ programs&quot;, that is using no extra variables than the input, and performing
modifications of one component at a time, hence using no memory. In this paper,
we survey this problem introduced in previous papers by the authors, we detail
its close relation with rearrangeable multicast networks, and we provide new
results for both viewpoints.
  A bijective mapping can be computed by 2n - 1 component modifications, that
is by a program of length 2n - 1, a result equivalent to the rearrangeability
of the concatenation of two reversed butterfly networks. For a general
arbitrary mapping, we give two methods to build a program with maximal length
4n-3. Equivalently, this yields rearrangeable multicast routing methods for the
network formed by four successive butterflies with alternating reversions. The
first method is available for any set S and practically equivalent to a known
method in network theory. The second method, a refinement of the first,
described when |S| is a power of 2, is new and allows more flexibility than the
known method.
  For a linear mapping, when S is any field, or a quotient of an Euclidean
domain (e.g. Z/sZ for any integer s), we build a program with maximal length 2n
- 1. In this case the assignments are also linear, thereby particularly
efficient from the algorithmic viewpoint, and giving moreover directly a
program for the inverse when it exists. This yields also a new result on matrix
decompositions, and a new result on the multicast properties of two successive
reversed butterflies. Results of this flavour were known only for the boolean
field Z/2Z.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5393</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5393</id><created>2013-10-20</created><authors><author><keyname>Xiao</keyname><forenames>Fanyi</forenames></author><author><keyname>Luo</keyname><forenames>Ruikun</forenames></author><author><keyname>Yu</keyname><forenames>Zhiding</forenames></author></authors><title>Multi-Task Regularization with Covariance Dictionary for Linear
  Classifiers</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a multi-task linear classifier learning problem
called D-SVM (Dictionary SVM). D-SVM uses a dictionary of parameter covariance
shared by all tasks to do multi-task knowledge transfer among different tasks.
We formally define the learning problem of D-SVM and show two interpretations
of this problem, from both the probabilistic and kernel perspectives. From the
probabilistic perspective, we show that our learning formulation is actually a
MAP estimation on all optimization variables. We also show its equivalence to a
multiple kernel learning problem in which one is trying to find a re-weighting
kernel for features from a dictionary of basis (despite the fact that only
linear classifiers are learned). Finally, we describe an alternative
optimization scheme to minimize the objective function and present empirical
studies to valid our algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5407</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5407</id><created>2013-10-20</created><authors><author><keyname>Sarma</keyname><forenames>Atish Das</forenames></author><author><keyname>Molla</keyname><forenames>Anisur Rahaman</forenames></author><author><keyname>Pandurangan</keyname><forenames>Gopal</forenames></author></authors><title>Distributed Computation of Sparse Cuts</title><categories>cs.DC cs.DS</categories><comments>19 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding sparse cuts is an important tool in analyzing large-scale distributed
networks such as the Internet and Peer-to-Peer networks, as well as large-scale
graphs such as the web graph, online social communities, and VLSI circuits. In
distributed communication networks, they are useful for topology maintenance
and for designing better search and routing algorithms.
  In this paper, we focus on developing fast distributed algorithms for
computing sparse cuts in networks. Given an undirected $n$-node network $G$
with conductance $\phi$, the goal is to find a cut set whose conductance is
close to $\phi$. We present two distributed algorithms that find a cut set with
sparsity $\tilde O(\sqrt{\phi})$ ($\tilde{O}$ hides $\polylog{n}$ factors).
Both our algorithms work in the CONGEST distributed computing model and output
a cut of conductance at most $\tilde O(\sqrt{\phi})$ with high probability, in
$\tilde O(\frac{1}{b}(\frac{1}{\phi} + n))$ rounds, where $b$ is balance of the
cut of given conductance. In particular, to find a sparse cut of constant
balance, our algorithms take $\tilde O(\frac{1}{\phi} + n)$ rounds.
  Our algorithms can also be used to output a {\em local} cluster, i.e., a
subset of vertices near a given source node, and whose conductance is within a
quadratic factor of the best possible cluster around the specified node. Both
our distributed algorithm can work without knowledge of the optimal $\phi$
value and hence can be used to find approximate conductance values both
globally and with respect to a given source node. We also give a lower bound on
the time needed for any distributed algorithm to compute any non-trivial sparse
cut --- any distributed approximation algorithm (for any non-trivial
approximation ratio) for computing sparsest cut will take $\tilde
\Omega(\sqrt{n} + D)$ rounds, where $D$ is the diameter of the graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5409</identifier>
 <datestamp>2014-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5409</id><created>2013-10-20</created><updated>2014-02-26</updated><authors><author><keyname>Liu</keyname><forenames>Chao</forenames></author><author><keyname>Xi</keyname><forenames>Feng</forenames></author><author><keyname>Chen</keyname><forenames>Shengyao</forenames></author><author><keyname>Liu</keyname><forenames>Zhong</forenames></author></authors><title>Pulse-Doppler Signal Processing with Quadrature Compressive Sampling</title><categories>cs.IT math.IT</categories><comments>37 pages, 18 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quadrature compressive sampling (QuadCS) is a newly introduced sub-Nyquist
sampling for acquiring inphase and quadrature (I/Q) components of
radio-frequency signals. For applications to pulse-Doppler radars, the QuadCS
outputs can be arranged in 2-dimensional data similar to that by Nyquist
sampling. This paper develops a compressive sampling pulse-Doppler (CoSaPD)
processing scheme from the sub-Nyquist samples. The CoSaPD scheme follows
Doppler estimation/detection and range estimation and is conducted on the
sub-Nyquist samples without recovering the Nyquist samples. The Doppler
estimation is realized through spectrum analyzer as in classic processing. The
detection is done on the Doppler bin data. The range estimation is performed
through sparse recovery algorithms on the detected targets and thus the
computational load is reduced. The detection threshold can be set at a low
value for improving detection probability and then the introduced false targets
are removed in the range estimation stage through inherent detection
characteristic in the recovery algorithms. Simulation results confirm our
findings. The CoSaPD scheme with the data at one eighth the Nyquist rate and
for SNR above -25dB can achieve performance of the classic processing with
Nyquist samples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5411</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5411</id><created>2013-10-20</created><updated>2013-10-22</updated><authors><author><keyname>Israni</keyname><forenames>Pankaj Kumar</forenames></author><author><keyname>Jain</keyname><forenames>S. C.</forenames></author></authors><title>Digital Circuits Implementation On Rpga Simulator</title><categories>cs.ET</categories><comments>72 pages, dissertation report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For Reversible computing, Target technology is yet to become available.
Adequate tools are not yet developed for reversible technology. Simulation is
still under development. Classical logic synthesis methods and simulation tools
can not be used in reversible computing. Because these work on irreversible
logic blocks but in reversible computing reversible logic blocks are used for
design and implementation. So, we need a simulation platform to analysis and
development in this area. So, we worked on this area and developed a GUI based
simulator. In the Dissertation work, we undertook simulation of RPGA,
reversible alternative to FPGA. We developed a RPGA simulator in our project.
RPGA simulator combines the technology of PLD/FPGA/SYSTOLIC array. Our RPGA
simulator, implements a given symmetric reversible Circuit on a RPGA structure.
We also worked on RPGA structure gates (picton and kerntopf gates) to develop a
better RPGA structure. Stepwise execution on RPGA structure is also performed
in RPGA simulator. We also designed new algorithms for Truth Table generation
and symmetry analysis. The dissertation work aims to develop entire simulator
is GUI based and easy to learn that makes it user friendly. User can easily
view all simulation results in GUI of our RPGA simulator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5420</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5420</id><created>2013-10-21</created><authors><author><keyname>Firouzbakht</keyname><forenames>Koorosh</forenames></author><author><keyname>Noubir</keyname><forenames>Guevara</forenames></author><author><keyname>Salehi</keyname><forenames>Masoud</forenames></author></authors><title>On the Performance of Adaptive Packetized Wireless Communication Links
  under Jamming</title><categories>cs.IT math.IT</categories><journal-ref>Wireless Communications, IEEE Transactions on, Volume:13, Issue: 7
  (2014)</journal-ref><doi>10.1109/TWC.2014.2314105</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We employ a game theoretic approach to formulate communication between two
nodes over a wireless link in the presence of an adversary. We define a
constrained, two-player, zero-sum game between a transmitter/receiver pair with
adaptive transmission parameters and an adversary with average and maximum
power constraints. In this model, the transmitter's goal is to maximize the
achievable expected performance of the communication link, defined by a utility
function, while the jammer's goal is to minimize the same utility function.
Inspired by capacity/rate as a performance measure, we define a general utility
function and a payoff matrix which may be applied to a variety of jamming
problems. We show the existence of a threshold such that if the jammer's
average power exceeds this threshold, the expected payoff of the transmitter at
Nash Equilibrium (NE) is the same as the case when the jammer uses its maximum
allowable power all the time. We provide analytical and numerical results for
transmitter and jammer optimal strategies and a closed form expression for the
expected value of the game at the NE. As a special case, we investigate the
maximum achievable transmission rate of a rate-adaptive, packetized, wireless
AWGN communication link under different jamming scenarios and show that
randomization can significantly assist a smart jammer with limited average
power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5422</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5422</id><created>2013-10-21</created><updated>2014-09-26</updated><authors><author><keyname>Fukunaga</keyname><forenames>Takuro</forenames></author></authors><title>Spider covers for prize-collecting network activation problem</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the network activation problem, each edge in a graph is associated with an
activation function, that decides whether the edge is activated from
node-weights assigned to its end-nodes. The feasible solutions of the problem
are the node-weights such that the activated edges form graphs of required
connectivity, and the objective is to find a feasible solution minimizing its
total weight. In this paper, we consider a prize-collecting version of the
network activation problem, and present first non- trivial approximation
algorithms. Our algorithms are based on a new LP relaxation of the problem.
They round optimal solutions for the relaxation by repeatedly computing
node-weights activating subgraphs called spiders, which are known to be useful
for approximating the network activation problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5426</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5426</id><created>2013-10-21</created><updated>2013-10-25</updated><authors><author><keyname>Sparks</keyname><forenames>Evan R.</forenames></author><author><keyname>Talwalkar</keyname><forenames>Ameet</forenames></author><author><keyname>Smith</keyname><forenames>Virginia</forenames></author><author><keyname>Kottalam</keyname><forenames>Jey</forenames></author><author><keyname>Pan</keyname><forenames>Xinghao</forenames></author><author><keyname>Gonzalez</keyname><forenames>Joseph</forenames></author><author><keyname>Franklin</keyname><forenames>Michael J.</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author><author><keyname>Kraska</keyname><forenames>Tim</forenames></author></authors><title>MLI: An API for Distributed Machine Learning</title><categories>cs.LG cs.DC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MLI is an Application Programming Interface designed to address the
challenges of building Machine Learn- ing algorithms in a distributed setting
based on data-centric computing. Its primary goal is to simplify the
development of high-performance, scalable, distributed algorithms. Our initial
results show that, relative to existing systems, this interface can be used to
build distributed implementations of a wide variety of common Machine Learning
algorithms with minimal complexity and highly competitive performance and
scalability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5430</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5430</id><created>2013-10-21</created><authors><author><keyname>Bevilacqua</keyname><forenames>Glenn S.</forenames></author><author><keyname>Clare</keyname><forenames>Shealen</forenames></author><author><keyname>Goyal</keyname><forenames>Amit</forenames></author><author><keyname>Lakshmanan</keyname><forenames>Laks V. S.</forenames></author></authors><title>Validating Network Value of Influencers by means of Explanations</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, there has been significant interest in social influence analysis.
One of the central problems in this area is the problem of identifying
influencers, such that by convincing these users to perform a certain action
(like buying a new product), a large number of other users get influenced to
follow the action. The client of such an application is a marketer who would
target these influencers for marketing a given new product, say by providing
free samples or discounts. It is natural that before committing resources for
targeting an influencer the marketer would be interested in validating the
influence (or network value) of influencers returned. This requires digging
deeper into such analytical questions as: who are their followers, on what
actions (or products) they are influential, etc. However, the current
approaches to identifying influencers largely work as a black box in this
respect. The goal of this paper is to open up the black box, address these
questions and provide informative and crisp explanations for validating the
network value of influencers.
  We formulate the problem of providing explanations (called PROXI) as a
discrete optimization problem of feature selection. We show that PROXI is not
only NP-hard to solve exactly, it is NP-hard to approximate within any
reasonable factor. Nevertheless, we show interesting properties of the
objective function and develop an intuitive greedy heuristic. We perform
detailed experimental analysis on two real world datasets - Twitter and
Flixster, and show that our approach is useful in generating concise and
insightful explanations of the influence distribution of users and that our
greedy algorithm is effective and efficient with respect to several baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5439</identifier>
 <datestamp>2014-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5439</id><created>2013-10-21</created><updated>2014-06-20</updated><authors><author><keyname>Prajapati</keyname><forenames>Monica</forenames></author><author><keyname>Sharma</keyname><forenames>Anuj</forenames></author></authors><title>Role OF Web 2.0 IN E-Governance</title><categories>cs.CY</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in equation 1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This research paper conducts a survey of the various government organizations
that are using Web 2.0 to enhance their functionality, interact with potential
audience and to reach out to a huge customer base. The aim of the paper is to
study the role of Web 2.0 in E-Governance and the various benefits that has
been achieved through it. This research paper is based on qualitative case
study method. We have reviewed existing state-of-the-art Web 2.0 applications
and E-Governance initiatives related with Web 2.0. We have included case
studies to support the proposition that use of Web 2.0 can enhance the
participation of citizens in E-Governance initiatives in an interactive manner.
This paper proposes that many of the government organizations can exploit the
benefits of Web 2.0 and hence can leverage its flexibility and interactive
nature. The study concludes that by establishing links with the social
networks, it is possible to reach a much wider audience easily and cost
effectively. The use of Web 2.0 applications can also bring out the
transparency in the government affairs. The Number of government organizations
embracing this new technology is on increase. The paper had surveyed few
organizations which are using Web 2.0 tools to enhance their operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5446</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5446</id><created>2013-10-21</created><authors><author><keyname>Mehani</keyname><forenames>Olivier</forenames></author><author><keyname>Boreli</keyname><forenames>Roksana</forenames></author><author><keyname>Jourjon</keyname><forenames>Guillaume</forenames></author><author><keyname>Ernst</keyname><forenames>Thierry</forenames></author></authors><title>Rate Control Adaptation for Heterogeneous Handovers</title><categories>cs.NI</categories><comments>Also available from http://www.nicta.com.au/pub?id=6163</comments><report-no>NICTA TR-6163</report-no><msc-class>20.010 20.070 20.100</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present enhancements to the TCP-Friendly Rate Control mechanism (TFRC)
designed to better handle the intermittent connectivity occurring in mobility
situations. Our aim is to quickly adapt to new network conditions and better
support real-time applications for which the user-perceived quality depends on
the immediate transmission rate. We propose to suspend the transmission before
disconnections occur, in a way inspired by Freeze-TCP, and extend the solution
by probing the network after reconnecting to enable full use of the newly
available capacity.
  We first introduce a numerical model of TFRC's performance after a network
handover and use it to evaluate the potential performance gains for realistic
network parameters. We then describe a set of additions to TFRC to achieve
these gains. Implementations within the Datagram Congestion Control Protocol
(DCCP) for ns -2 and Linux have been adapted to support these enhancements.
Comparisons of experimental results for the original and modified DCCP are
presented for a number of example mobility scenarios.
  We thus show how the proposed modifications enable faster recovery after
disconnected periods as well as significantly improved adjustments to the newly
available network conditions and an improvement in the quality of experience
(QoE) for video-streaming applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5463</identifier>
 <datestamp>2014-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5463</id><created>2013-10-21</created><updated>2014-08-04</updated><authors><author><keyname>Imran</keyname><forenames>Muhammad</forenames></author><author><keyname>Lykourentzou</keyname><forenames>Ioanna</forenames></author><author><keyname>Naudet</keyname><forenames>Yannick</forenames></author><author><keyname>Castillo</keyname><forenames>Carlos</forenames></author></authors><title>Engineering Crowdsourced Stream Processing Systems</title><categories>cs.DB cs.AI cs.SE</categories><acm-class>H.4; D.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A crowdsourced stream processing system (CSP) is a system that incorporates
crowdsourced tasks in the processing of a data stream. This can be seen as
enabling crowdsourcing work to be applied on a sample of large-scale data at
high speed, or equivalently, enabling stream processing to employ human
intelligence. It also leads to a substantial expansion of the capabilities of
data processing systems. Engineering a CSP system requires the combination of
human and machine computation elements. From a general systems theory
perspective, this means taking into account inherited as well as emerging
properties from both these elements. In this paper, we position CSP systems
within a broader taxonomy, outline a series of design principles and evaluation
metrics, present an extensible framework for their design, and describe several
design patterns. We showcase the capabilities of CSP systems by performing a
case study that applies our proposed framework to the design and analysis of a
real system (AIDR) that classifies social media messages during time-critical
crisis events. Results show that compared to a pure stream processing system,
AIDR can achieve a higher data classification accuracy, while compared to a
pure crowdsourcing solution, the system makes better use of human workers by
requiring much less manual work effort.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5468</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5468</id><created>2013-10-21</created><authors><author><keyname>Mahmoudi</keyname><forenames>Hamed</forenames></author><author><keyname>Rodolakis</keyname><forenames>Georgios</forenames></author><author><keyname>Georgiadis</keyname><forenames>Leonidas</forenames></author><author><keyname>Saad</keyname><forenames>David</forenames></author></authors><title>Message-Passing Algorithms for Optimal Utilization of Cognitive Radio
  Networks</title><categories>cs.IT cond-mat.stat-mech cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cognitive Radio has been proposed as a key technology to significantly
improve spectrum usage in wireless networks by enabling unlicensed users to
access unused resource. We present new algorithms that are needed for the
implementation of opportunistic scheduling policies that maximize the
throughput utilization of resources by secondary users, under maximum
interference constraints imposed by existing primary users. Our approach is
based on the Belief Propagation (BP) algorithm, which is advantageous due to
its simplicity and potential for distributed implementation. We examine
convergence properties and evaluate the performance of the proposed BP
algorithms via simulations and demonstrate that the results compare favorably
with a benchmark greedy strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5469</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5469</id><created>2013-10-21</created><authors><author><keyname>Cochefert</keyname><forenames>Manfred</forenames></author><author><keyname>Couturier</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author><author><keyname>Golovach</keyname><forenames>Petr A.</forenames></author><author><keyname>Kratsch</keyname><forenames>Dieter</forenames></author><author><keyname>Paulusma</keyname><forenames>Dani&#xeb;l</forenames></author></authors><title>Parameterized Algorithms for Finding Square Roots</title><categories>cs.DS</categories><acm-class>F.2.2; G.2.1; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the following two problems are fixed-parameter tractable with
parameter k: testing whether a connected n-vertex graph with m edges has a
square root with at most n-1+k edges and testing whether such a graph has a
square root with at least m-k edges. Our first result implies that squares of
graphs obtained from trees by adding at most k edges can be recognized in
polynomial time for every fixed k&gt;=0; previously this result was known only for
k=0. Our second result is equivalent to stating that deciding whether a graph
can be modified into a square root of itself by at most k edge deletions is
fixed-parameter tractable with parameter k.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5472</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5472</id><created>2013-10-21</created><authors><author><keyname>Ali</keyname><forenames>Syed Asif</forenames></author><author><keyname>Soomro</keyname><forenames>Safeeulah</forenames></author><author><keyname>Memon</keyname><forenames>Abdul Ghafoor</forenames></author><author><keyname>Ahmed</keyname><forenames>Mashooque</forenames></author></authors><title>Interactive Employment Model to Assimilate the Deaf persons in workplace
  by using ICT</title><categories>cs.CY</categories><journal-ref>Sindh Univ. Res. Jour. (Sci. Ser.) Vol. 45 (2) 263-266 (2013)</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The rate of disability is increase day by day all over the world .There are
various type of Disabilities but the deaf persons are on second number among
all types of disabilities.. In most of the countries disabled persons are
supposed to be social liability on their family and in the society as awhile.
Now days in developing countries it is difficult for normal persons to get
suitable jobs. Whenever we are talking about disabled, it is more difficult to
assimilate deaf persons in workplace. Threats of unemployment of disabled
person are almost double that of people without disabilities. Number of able
deaf persons is unable to get suitable job due to several reasons like lack of
facilities for deaf persons and lack of awareness from normal persons side
which create barrier in searching job for deaf persons. This research work
emphasis on the special need and training required for the deaf individual sand
to make and train them how to move in workplace with different social and
technical barrier. In recent era technology play important role in each and
every part of life. Using the facility of Information and communication
Technology we can easily assimilate deaf in workplace. The proposed model helps
deaf persons to adjust in their jobs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5474</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5474</id><created>2013-10-21</created><authors><author><keyname>Ali</keyname><forenames>Syed Asif</forenames></author><author><keyname>Soomro</keyname><forenames>Safeeullah</forenames></author><author><keyname>Memon</keyname><forenames>Abdul Ghafoor</forenames></author><author><keyname>Baqi</keyname><forenames>Abdul</forenames></author></authors><title>Implementation of Automata Theory to Improve the Learning Disability</title><categories>cs.CY</categories><journal-ref>Sindh Univ. Res. Jour. (Sci. Ser.) Vol. 45 (1):1193-196 (2013)</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  There are various types of disability egress in world like blindness,
deafness, and Physical disabilities. It is quite difficult to deal with people
with disability. Learning disability (LD) is types of disability totally
different from general disability. To deal children with learning disability is
difficult for both parents and teacher. As parent deal with only single child
so it bit easy. But teacher deals with different students at a time so its more
difficult to deal with group of students with learning disability. If there is
more students with learning disability so it is necessary that first all
identify the type of learning disability in group of students. Some students
have learning disability of mathematics; some have learning disability of other
subjects. By using theory of Automata it easy to analysis the level of
disability among all students then deal with them accordingly. For these
purpose deterministic automata is the best practice. Teacher deals with
deterministic students in class and check there response. In this research
deterministic automata is use to facilitated the teacher which help teacher in
identification of students with learning disability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5476</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5476</id><created>2013-10-21</created><authors><author><keyname>Trujillo-Rasua</keyname><forenames>Rolando</forenames></author></authors><title>Privacy in RFID and mobile objects</title><categories>cs.CR</categories><comments>PhD Thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Radio Frequency Identification (RFID) is a technology aimed at eficiently
identifying and tracking goods and assets. Such identification may be performed
without requiring line-of-sight alignment or physical contact between the RFID
tag and the RFID reader, whilst tracking is naturally achieved due to the short
interrogation field of RFID readers. That is why the reduction in price of the
RFID tags has been accompanied with an increasing attention paid to this
technology. However, since tags are resource-constrained devices sending
identification data wirelessly, designing secure and private RFID
identification protocols is a challenging task. This scenario is even more
complex when scalability must be met by those protocols.
  Assuming the existence of a lightweight, secure, private and scalable RFID
identification protocol, there exist other concerns surrounding the RFID
technology. Some of them arise from the technology itself, such as distance
checking, but others are related to the potential of RFID systems to gather
huge amount of tracking data. Publishing and mining such moving objects data is
essential to improve efficiency of supervisory control, assets management and
localisation, transportation, etc. However, obvious privacy threats arise if an
individual can be linked with some of those published trajectories. The present
dissertation contributes to the design of algorithms and protocols aimed at
dealing with the issues explained above. First, we propose a set of protocols
and heuristics based on a distributed architecture that improve the e?ciency of
the identification process without compromising privacy or security. Moreover,
we present a novel distance-bounding protocol based on graphs that is extremely
low-resource consuming. Finally, we present two trajectory anonymisation
methods aimed at preserving the individuals' privacy when their trajectories
are released.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5478</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5478</id><created>2013-10-21</created><authors><author><keyname>Khan</keyname><forenames>Adnan Alam</forenames></author><author><keyname>Soomro</keyname><forenames>Safeeullah</forenames></author><author><keyname>Memon</keyname><forenames>Abdul Ghafoor</forenames></author></authors><title>Flickers Forecasting In CRT Using Stochastic Analysis</title><categories>cs.OH</categories><journal-ref>Sindh Univ. Res. Jour. (Sci. Ser.) Vol.44 (3) 473-478 (2012)</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Videos are composed of sequence of interrelated frames. There is a minute
difference among frames. Flicker is an error which is found in every video. It
is like a checker box in a video, there are several reasons behind flickers
generation, one of the main reasons is refresh rate of the monitor and second
reason is number of frames per second in a video. The main objective of this
study is to propose and develop a framework that identifies flicker location
and minimizes the flickers rate. Analysis shows that flickers can be minimize
by adjusting the persistence of pixel and higher refresh rate of CRT monitor.
Further we have compared different isotopes of phosphorous pixels and generate
its graphs. This paper highlighted the cause of flicker and its avoidance
.Statistical research proves that proposed algorithm improves the video quality
and reduce flickers ratio up to 90%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5479</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5479</id><created>2013-10-21</created><authors><author><keyname>M&#xfc;ller</keyname><forenames>Ralf R.</forenames></author><author><keyname>Alfano</keyname><forenames>Giusi</forenames></author><author><keyname>Zaidel</keyname><forenames>Benjamin M.</forenames></author><author><keyname>de Miguel</keyname><forenames>Rodrigo</forenames></author></authors><title>Applications of Large Random Matrices in Communications Engineering</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:0706.1169 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work gives an overview of analytic tools for the design, analysis, and
modelling of communication systems which can be described by linear vector
channels such as y = Hx+z where the number of components in each vector is
large. Tools from probability theory, operator algebra, and statistical physics
are reviewed. The survey of analytical tools is complemented by examples of
applications in communications engineering. Asymptotic eigenvalue distributions
of many classes of random matrices are given. The treatment includes the
problem of moments and the introduction of the Stieltjes transform. Free
probability theory, which evolved from non-commutative operator algebras, is
explained from a probabilistic point of view in order to better fit the
engineering community. For that purpose freeness is defined without reference
to non-commutative algebras. The treatment includes additive and multiplicative
free convolution, the R-transform, the S-transform, and the free central limit
theorem. The replica method developed in statistical physics for the purpose of
analyzing spin glasses is reviewed from the viewpoint of its applications in
communications engineering. Correspondences between free energy and mutual
information as well as energy functions and detector metrics are established.
These analytic tools are applied to the design and the analysis of linear
multiuser detectors, the modelling of scattering in communication channels with
dual antennas arrays, and the analysis of optimal detection for communication
via code-division multiple-access and/or dual antenna array channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5485</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5485</id><created>2013-10-21</created><updated>2014-04-29</updated><authors><author><keyname>Sun</keyname><forenames>Jiajun</forenames></author></authors><title>Behavior-Based online Incentive Mechanism for Crowd Sensing with Budget
  Constraints</title><categories>cs.GT cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crowd sensing is a new paradigm which leverages the ubiquity of
sensor-equipped mobile devices to collect data. To achieve good quality for
crowd sensing, incentive mechanisms are indispensable to attract more
participants. Most of existing mechanisms focus on the expected utility prior
to sensing, ignoring the risk of low quality solution and privacy leakage.
Traditional incentive mechanisms such as the Vickrey-Clarke-Groves (VCG)
mechanism and its variants are not applicable here. In this paper, to address
these challenges, we propose a behavior based incentive mechanism for crowd
sensing applications with budget constraints by applying sequential all-pay
auctions in mobile social networks (MSNs), not only to consider the effects of
extensive user participation, but also to maximize high quality of the context
based sensing content submission for crowd sensing platform under the budget
constraints, where users arrive in a sequential order. Through an extensive
simulation, results indicate that incentive mechanisms in our proposed
framework outperform the best existing solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5488</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5488</id><created>2013-10-21</created><authors><author><keyname>Pessemier</keyname><forenames>Wim</forenames></author><author><keyname>Raskin</keyname><forenames>Gert</forenames></author><author><keyname>Van Winckel</keyname><forenames>Hans</forenames></author><author><keyname>Deconinck</keyname><forenames>Geert</forenames></author><author><keyname>Saey</keyname><forenames>Philippe</forenames></author></authors><title>A practical approach to ontology-enabled control systems for
  astronomical instrumentation</title><categories>astro-ph.IM cs.AI cs.SE</categories><comments>Proceedings of ICALEPCS 2013</comments><msc-class>85</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Even though modern service-oriented and data-oriented architectures promise
to deliver loosely coupled control systems, they are inherently brittle as they
commonly depend on a priori agreed interfaces and data models. At the same
time, the Semantic Web and a whole set of accompanying standards and tools are
emerging, advocating ontologies as the basis for knowledge exchange. In this
paper we aim to identify a number of key ideas from the myriad of
knowledge-based practices that can readily be implemented by control systems
today. We demonstrate with a practical example (a three-channel imager for the
Mercator Telescope) how ontologies developed in the Web Ontology Language (OWL)
can serve as a meta-model for our instrument, covering as many engineering
aspects of the project as needed. We show how a concrete system model can be
built on top of this meta-model via a set of Domain Specific Languages (DSLs),
supporting both formal verification and the generation of software and
documentation artifacts. Finally we reason how the available semantics can be
exposed at run-time by adding a &quot;semantic layer&quot; that can be browsed, queried,
monitored etc. by any OPC UA-enabled client.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5493</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5493</id><created>2013-10-21</created><authors><author><keyname>Bonamy</keyname><forenames>Marthe</forenames></author><author><keyname>Bousquet</keyname><forenames>Nicolas</forenames></author></authors><title>Brooks' theorem on powers of graphs</title><categories>cs.DM math.CO</categories><comments>7 pages, no figure, submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that for $k\geq 3$, the bound given by Brooks' theorem on the
chromatic number of $k$-th powers of graphs of maximum degree $\Delta \geq 3$
can be lowered by 1, even in the case of online list coloring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5497</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5497</id><created>2013-10-21</created><authors><author><keyname>Promayon</keyname><forenames>Emmanuel</forenames><affiliation>TIMC-IMAG</affiliation></author><author><keyname>Fouard</keyname><forenames>Celine</forenames><affiliation>TIMC-IMAG</affiliation></author><author><keyname>Bailet</keyname><forenames>Mathieu</forenames><affiliation>TIMC-IMAG</affiliation></author><author><keyname>Deram</keyname><forenames>Aurelien</forenames><affiliation>TIMC-IMAG</affiliation></author><author><keyname>Fiard</keyname><forenames>Gaelle</forenames><affiliation>TIMC-IMAG</affiliation></author><author><keyname>Hungr</keyname><forenames>Nikolai</forenames><affiliation>TIMC-IMAG</affiliation></author><author><keyname>Luboz</keyname><forenames>Vincent</forenames><affiliation>TIMC-IMAG</affiliation></author><author><keyname>Payan</keyname><forenames>Yohan</forenames><affiliation>TIMC-IMAG</affiliation></author><author><keyname>Sarrazin</keyname><forenames>Johan</forenames><affiliation>TIMC-IMAG</affiliation></author><author><keyname>Saubat</keyname><forenames>Nicolas</forenames><affiliation>TIMC-IMAG</affiliation></author><author><keyname>Selmi</keyname><forenames>Sonia Yuki</forenames><affiliation>TIMC-IMAG</affiliation></author><author><keyname>Voros</keyname><forenames>Sandrine</forenames><affiliation>TIMC-IMAG</affiliation></author><author><keyname>Cinquin</keyname><forenames>Philippe</forenames><affiliation>TIMC-IMAG</affiliation></author><author><keyname>Troccaz</keyname><forenames>Jocelyne</forenames><affiliation>TIMC-IMAG</affiliation></author></authors><title>Using CamiTK for rapid prototyping of interactive Computer Assisted
  Medical Intervention applications</title><categories>cs.OH</categories><proxy>ccsd</proxy><journal-ref>Conference proceedings : Annual International Conference of the
  IEEE Engineering in Medicine and Biology Society. 2013 (2013) 4933-6</journal-ref><doi>10.1109/EMBC.2013.6610654</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computer Assisted Medical Intervention (CAMI hereafter) is a complex
multi-disciplinary field. CAMI research requires the collaboration of experts
in several fields as diverse as medicine, computer science, mathematics,
instrumentation, signal processing, mechanics, modeling, automatics, optics,
etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5515</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5515</id><created>2013-10-21</created><updated>2014-04-20</updated><authors><author><keyname>Buzaglo</keyname><forenames>Sarit</forenames></author><author><keyname>Etzion</keyname><forenames>Tuvi</forenames></author></authors><title>Perfect Permutation Codes with the Kendall's $\tau$-Metric</title><categories>cs.IT math.IT</categories><comments>to be presented in ISIT2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rank modulation scheme has been proposed for efficient writing and
storing data in non-volatile memory storage. Error-correction in the rank
modulation scheme is done by considering permutation codes. In this paper we
consider codes in the set of all permutations on $n$ elements, $S_n$, using the
Kendall's $\tau$-metric. We prove that there are no perfect
single-error-correcting codes in $S_n$, where $n&gt;4$ is a prime or $4\leq n\leq
10$. We also prove that if such a code exists for $n$ which is not a prime then
the code should have some uniform structure. We define some variations of the
Kendall's $\tau$-metric and consider the related codes and specifically we
prove the existence of a perfect single-error-correcting code in $S_5$.
Finally, we examine the existence problem of diameter perfect codes in $S_n$
and obtain a new upper bound on the size of a code in $S_n$ with even minimum
Kendall's $\tau$-distance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5516</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5516</id><created>2013-10-21</created><authors><author><keyname>Duchamp</keyname><forenames>G. H. E.</forenames></author><author><keyname>Hoang-Nghia</keyname><forenames>N.</forenames></author><author><keyname>Krajewski</keyname><forenames>T.</forenames></author><author><keyname>Tanasa</keyname><forenames>A.</forenames></author></authors><title>Renormalization group-like proof of the universality of the Tutte
  polynomial for matroids</title><categories>math.CO cs.DM hep-th</categories><comments>12 pages, 3 figures, conference proceedings, 25th International
  Conference on Formal Power Series and Algebraic Combinatorics, Paris, France,
  June 2013</comments><journal-ref>Discrete Mathematics &amp; Theoretical Computer Science Proceedings
  FPSAC 2013, pag. 397-408 (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we give a new proof of the universality of the Tutte polynomial
for matroids. This proof uses appropriate characters of Hopf algebra of
matroids, algebra introduced by Schmitt (1994). We show that these Hopf algebra
characters are solutions of some differential equations which are of the same
type as the differential equations used to describe the renormalization group
flow in quantum field theory. This approach allows us to also prove, in a
different way, a matroid Tutte polynomial convolution formula published by
Kook, Reiner and Stanton (1999). This FPSAC contribution is an extended
abstract.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5520</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5520</id><created>2013-10-21</created><authors><author><keyname>Sasaki</keyname><forenames>Tatsuya</forenames></author><author><keyname>Uchida</keyname><forenames>Satoshi</forenames></author></authors><title>Rewards and the evolution of cooperation in public good games</title><categories>physics.soc-ph cs.GT nlin.AO q-bio.PE</categories><comments>9 pages, 2 figures</comments><journal-ref>Biology Letters, 2014, 10(1), 20130903</journal-ref><doi>10.1098/rsbl.2013.0903</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Properly coordinating cooperation is relevant for resolving public good
problems such as clean energy and environmental protection. However, little is
known about how individuals can coordinate themselves for a certain level of
cooperation in large populations of strangers. In a typical situation, a
consensus-building process hardly succeeds due to lack of face and standing.
The evolution of cooperation in this type of situation is studied using
threshold public good games in which cooperation prevails when it is initially
sufficient, or otherwise, it perishes. While punishment is a powerful tool to
shape human behaviours, institutional punishment is often too costly to start
with only a few contributors, which is another coordination problem. Here we
show that whatever the initial conditions, reward funds based on voluntary
contribution can evolve. The voluntary reward paves the way for effectively
overcoming the coordination problem and efficiently transforms freeloaders to
cooperators with a perceived small risk of collective failure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5534</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5534</id><created>2013-10-21</created><updated>2014-01-21</updated><authors><author><keyname>Farokhi</keyname><forenames>Farhad</forenames></author><author><keyname>Johansson</keyname><forenames>Karl H.</forenames></author></authors><title>A Study of Truck Platooning Incentives Using a Congestion Game</title><categories>cs.GT cs.SY math.OC</categories><comments>Updated Introduction; Improved Literature Review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce an atomic congestion game with two types of agents, cars and
trucks, to model the traffic flow on a road over various time intervals of the
day. Cars maximize their utility by finding a trade-off between the time they
choose to use the road, the average velocity of the flow at that time, and the
dynamic congestion tax that they pay for using the road. In addition to these
terms, the trucks have an incentive for using the road at the same time as
their peers because they have platooning capabilities, which allow them to save
fuel. The dynamics and equilibria of this game-theoretic model for the
interaction between car traffic and truck platooning incentives are
investigated. We use traffic data from Stockholm to validate parts of the
modeling assumptions and extract reasonable parameters for the simulations. We
use joint strategy fictitious play and average strategy fictitious play to
learn a pure strategy Nash equilibrium of this game. We perform a comprehensive
simulation study to understand the influence of various factors, such as the
drivers' value of time and the percentage of the trucks that are equipped with
platooning devices, on the properties of the Nash equilibrium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5538</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5538</id><created>2013-10-21</created><authors><author><keyname>Saraswat</keyname><forenames>Vijay</forenames></author><author><keyname>Jagadeesan</keyname><forenames>Radha</forenames></author><author><keyname>Gupta</keyname><forenames>Vineet</forenames></author></authors><title>TCC, with History</title><categories>cs.PL</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern computer systems are awash in a sea of asynchronous events. There is
an increasing need for a declarative language that can permit business users to
specify complex event-processing rules. Such rules should be able to correlate
different event streams, detect absence of events (negative information),
permit aggregations over sliding windows, specify dependent sliding windows
etc. For instance it should be possible to precisely state a rule such as
&quot;Every seventh trading session that DowJones has risen consecutively, and IBM's
stock is off 3% over its average in this period, evaluate IBM position&quot;,
&quot;Declare the sensor as faulty if no reading has been received for 500 ms&quot;, etc.
Further, the language should be implementable efficiently in an event-driven
fashion.
  We propose the Timed (Default) Concurrent Constraint, TCC, programming
framework as a foundation for such complex event processing. While very rich,
the TCC framework &quot;forgets&quot; information from one instant to the next. We make
two extensions. First, we extend the TCC model to carry the store from previous
time instants as &quot;past&quot; information in the current time instant. This permits
rules to to be written with rich queries over the past. Second, we show that
many of the powerful properties of the agent language can be folded into the
query language by permitting agents and queries to be defined mutually
recursively, building on the testing interpretation of intuitionistic logic
described in RCC \cite{radha-fsttcs05}. We show that this permits queries to
move &quot;back and forth&quot; in the past, e.g.{} &quot;Order a review if the last time that
IBM stock price dropped by 10% in a day, there was more than 20% increase in
trading volume for Oracle the following day.&quot;
  We provide a formal semantics for TCC + Histories and establish some basic
properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5540</identifier>
 <datestamp>2013-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5540</id><created>2013-10-21</created><updated>2013-11-12</updated><authors><author><keyname>Fiedor</keyname><forenames>Pawe&#x142;</forenames></author></authors><title>Frequency Effects on Predictability of Stock Returns</title><categories>q-fin.ST cs.IT math.IT</categories><comments>8 pages, 16 figures, submitted for possible publication to
  Computational Intelligence for Financial Engineering and Economics 2014
  conference</comments><msc-class>94A17</msc-class><acm-class>E.4; H.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose that predictability is a prerequisite for profitability on
financial markets. We look at ways to measure predictability of price changes
using information theoretic approach and employ them on all historical data
available for NYSE 100 stocks. This allows us to determine whether frequency of
sampling price changes affects the predictability of those. We also relations
between price changes predictability and the deviation of the price formation
processes from iid as well as the stock's sector. We also briefly comment on
the complicated relationship between predictability of price changes and the
profitability of algorithmic trading.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5541</identifier>
 <datestamp>2014-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5541</id><created>2013-10-21</created><updated>2014-02-03</updated><authors><author><keyname>Demirel</keyname><forenames>&#xd6;mer</forenames></author><author><keyname>Smal</keyname><forenames>Ihor</forenames></author><author><keyname>Niessen</keyname><forenames>Wiro J.</forenames></author><author><keyname>Meijering</keyname><forenames>Erik</forenames></author><author><keyname>Sbalzarini</keyname><forenames>Ivo F.</forenames></author></authors><title>Piecewise Constant Sequential Importance Sampling for Fast Particle
  Filtering</title><categories>stat.CO cs.CC</categories><comments>8 pages; will appear in the proceedings of the IET Data Fusion &amp;
  Target Tracking Conference 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Particle filters are key algorithms for object tracking under non-linear,
non-Gaussian dynamics. The high computational cost of particle filters,
however, hampers their applicability in cases where the likelihood model is
costly to evaluate, or where large numbers of particles are required to
represent the posterior. We introduce the approximate sequential importance
sampling/resampling (ASIR) algorithm, which aims at reducing the cost of
traditional particle filters by approximating the likelihood with a mixture of
uniform distributions over pre-defined cells or bins. The particles in each bin
are represented by a dummy particle at the center of mass of the original
particle distribution and with a state vector that is the average of the states
of all particles in the same bin. The likelihood is only evaluated for the
dummy particles, and the resulting weight is identically assigned to all
particles in the bin. We derive upper bounds on the approximation error of the
so-obtained piecewise constant function representation, and analyze how bin
size affects tracking accuracy and runtime. Further, we show numerically that
the ASIR approximation error converges to that of sequential importance
sampling/resampling (SIR) as the bin size is decreased. We present a set of
numerical experiments from the field of biological image processing and
tracking that demonstrate ASIR's capabilities. Overall, we consider ASIR a
promising candidate for simple, fast particle filtering in generic
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5542</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5542</id><created>2013-10-21</created><authors><author><keyname>Kadyrov</keyname><forenames>Alexander</forenames></author><author><keyname>Yu</keyname><forenames>Hui</forenames></author><author><keyname>Liu</keyname><forenames>Honghai</forenames></author></authors><title>Ship Detection and Segmentation using Image Correlation</title><categories>cs.CV</categories><comments>8 pages, to be published in proc. of conference IEEE SMC 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There have been intensive research interests in ship detection and
segmentation due to high demands on a wide range of civil applications in the
last two decades. However, existing approaches, which are mainly based on
statistical properties of images, fail to detect smaller ships and boats.
Specifically, known techniques are not robust enough in view of inevitable
small geometric and photometric changes in images consisting of ships. In this
paper a novel approach for ship detection is proposed based on correlation of
maritime images. The idea comes from the observation that a fine pattern of the
sea surface changes considerably from time to time whereas the ship appearance
basically keeps unchanged. We want to examine whether the images have a common
unaltered part, a ship in this case. To this end, we developed a method -
Focused Correlation (FC) to achieve robustness to geometric distortions of the
image content. Various experiments have been conducted to evaluate the
effectiveness of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5551</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5551</id><created>2013-10-18</created><authors><author><keyname>Heinle</keyname><forenames>Albert</forenames></author><author><keyname>Levandovskyy</keyname><forenames>Viktor</forenames></author><author><keyname>Nareike</keyname><forenames>Andreas</forenames></author></authors><title>SymbolicData:SDEval - Benchmarking for Everyone</title><categories>cs.SC cs.MS cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we will present SDeval, a software project that contains tools
for creating and running benchmarks with a focus on problems in computer
algebra. It is built on top of the Symbolic Data project, able to translate
problems in the database into executable code for various computer algebra
systems. The included tools are designed to be very flexible to use and to
extend, such that they can be utilized even in contexts of other communities.
With the presentation of SDEval, we will also address particularities of
benchmarking in the field of computer algebra. Furthermore, with SDEval, we
provide a feasible and automatizable way of reproducing benchmarks published in
current research works, which appears to be a difficult task in general due to
the customizability of the available programs. We will simultaneously present
the current developments in the Symbolic Data project.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5553</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5553</id><created>2013-10-21</created><authors><author><keyname>N&#xf6;tzel</keyname><forenames>J.</forenames></author></authors><title>Hypothesis Testing on Invariant Subspaces of the Symmetric Group, Part I
  - Quantum Sanov's Theorem and Arbitrarily Varying Sources</title><categories>quant-ph cs.IT math-ph math.IT math.MP math.RT</categories><comments>20 pages, no figures</comments><doi>10.1088/1751-8113/47/23/235303</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report a proof of the quantum Sanov Theorem by elementary application of
basic facts about representations of the symmetric group, together with a
complete characterization of the optimal error exponent in a situation where
the null hypothesis is given by an arbitrarily varying quantum source instead.
Our approach differs from previous ones in two points: First, it supports a
reasoning inspired by the method of types. Second, the measurement scheme we
propose to distinguish the two alternatives not only does that job
asymptotically perfect, but also yields additional information about the null
hypothesis. An example of that is given. The measurement is composed of
projections onto permutation-invariant subspaces, thus providing a direct link
between one of the most basic tasks in quantum information on the one hand side
and fundamental objects in representation theory on the other. We additionally
connect to representation theory by proving a relation between Kostka numbers
and quantum states, and to state estimation via a generalization of a
well-known spectral estimation theorem to non-i.i.d. sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5557</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5557</id><created>2013-10-21</created><authors><author><keyname>Bradai</keyname><forenames>Abbas</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Ahmed</keyname><forenames>Toufik</forenames><affiliation>LaBRI</affiliation></author></authors><title>On the Optimal Scheduling in Pull-based Real-Time P2P Streaming Systems:
  Layered and Non-Layered Streaming</title><categories>cs.NI</categories><comments>IEEE ICC 2012, Ottawa : Canada (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During the last decade, we witnessed a rapid growth in deployment of
pull-based P2P streaming applications. In these applications, each node selects
some other nodes as its neighbors and requests streaming data from them. This
scheme allows eliminating data redundancy and recovering from data loss, but it
pushes the complexity to the receiver node side. In this paper, we
theoretically study the scheduling problem in Pull-based P2P video streaming
and we model it as an assignment problem. Then, we propose AsSched, new
scheduling algorithm for layered streaming, in order to optimize the throughput
and the delivery ratio of the system. In second time, we derive an optimal
algorithm (NAsSched) for non layered streaming. The results of simulations show
that our algorithms significantly outperform classic scheduling strategies
especially in stern bandwidth constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5558</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5558</id><created>2013-10-18</created><updated>2013-11-12</updated><authors><author><keyname>Balaguer</keyname><forenames>Sandie</forenames><affiliation>LSV, ENS Cachan, Inria, CNRS</affiliation></author><author><keyname>Chatain</keyname><forenames>Thomas</forenames><affiliation>LSV, ENS Cachan, Inria, CNRS</affiliation></author></authors><title>Avoiding Shared Clocks in Networks of Timed Automata</title><categories>cs.FL</categories><comments>Article accepted to Logical Methods in Computer Science. Number:
  LMCS-2013-837 Special issue: 23rd International Conference on Concurrency
  Theory (CONCUR 2012)</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 4 (November
  14, 2013) lmcs:933</journal-ref><doi>10.2168/LMCS-9(4:13)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Networks of timed automata (NTA) are widely used to model distributed
real-time systems. Quite often in the literature, the automata are allowed to
share clocks, i.e. transitions of one automaton may be guarded by conditions on
the value of clocks reset by another automaton. This is a problem when one
considers implementing such model in a distributed architecture, since reading
clocks a priori requires communications which are not explicitly described in
the model. We focus on the following question: given an NTA A1 || A2 where A2
reads some clocks reset by A1, does there exist an NTA A'1 || A'2 without
shared clocks with the same behavior as the initial NTA? For this, we allow the
automata to exchange information during synchronizations only, in particular by
copying the value of their neighbor's clocks. We discuss a formalization of the
problem and define an appropriate behavioural equivalence. Then we give a
criterion using the notion of contextual timed transition system, which
represents the behavior of A2 when in parallel with A1. Finally, we effectively
build A'1 || A'2 when it exists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5564</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5564</id><created>2013-10-16</created><authors><author><keyname>Derrien</keyname><forenames>Alban</forenames><affiliation>LINA, INRIA - LINA</affiliation></author><author><keyname>Petit</keyname><forenames>Thierry</forenames><affiliation>LINA, INRIA - LINA</affiliation></author></authors><title>The Energetic Reasoning Checker Revisited</title><categories>cs.OH</categories><comments>CP Doctoral Program 2013, Uppsala : Sweden (2013)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energetic Reasoning (ER) is a powerful filtering algorithm for the Cumulative
constraint. Unfortunately, ER is generally too costly to be used in practice.
One reason of its bad behavior is that many intervals are considered as
relevant by the checker of ER, although most of them should be ignored. In this
paper, we provide a sharp characterization that allows to reduce the number of
intervals by a factor seven. Our experiments show that associating this checker
with a Time-Table filtering algorithm leads to promising results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5568</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5568</id><created>2013-10-21</created><authors><author><keyname>Bull</keyname><forenames>Larry</forenames></author></authors><title>Towards Application of the RBNK Model</title><categories>cs.CE cs.NE</categories><comments>arXiv admin note: substantial text overlap with arXiv:1306.4793,
  arXiv:1303.7220</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The computational modeling of genetic regulatory networks is now common
place, either by fitting a system to experimental data or by exploring the
behaviour of abstract systems with the aim of identifying underlying
principles. This paper presents an approach to the latter, considering the
response to environmental changes of a well-known model placed upon tunable
fitness landscapes. The effects on genome size and gene connectivity are
explored.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5569</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5569</id><created>2013-10-21</created><updated>2016-02-25</updated><authors><author><keyname>Yeh</keyname><forenames>Edmund</forenames></author><author><keyname>Ho</keyname><forenames>Tracey</forenames></author><author><keyname>Cui</keyname><forenames>Ying</forenames></author><author><keyname>Liu</keyname><forenames>Ran</forenames></author><author><keyname>Burd</keyname><forenames>Michael</forenames></author><author><keyname>Leong</keyname><forenames>Derek</forenames></author></authors><title>Forwarding, Caching and Congestion Control in Named Data Networks</title><categories>cs.NI</categories><comments>This version of the paper contains a new section on congestion
  control for NDN networks. Previous version of the paper appeared in
  Proceedings of ACM Conference on Information-Centric Networking (ICN), Paris,
  France, September 24-26, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Emerging information-centric networking architectures seek to optimally
utilize both bandwidth and storage for efficient content distribution. This
highlights the need for joint design of traffic engineering and caching
strategies, in order to optimize network performance in view of both current
traffic loads and future traffic demands. We present a systematic framework for
joint dynamic interest request forwarding and dynamic cache placement and
eviction, within the context of the Named Data Networking (NDN) architecture.
The framework employs a virtual control plane which operates on the user demand
rate for data objects in the network, and an actual plane which handles
Interest Packets and Data Packets. We develop distributed algorithms within the
virtual plane to achieve network load balancing through dynamic forwarding and
caching, thereby maximizing the user demand rate that the NDN network can
satisfy. Next, we show that congestion control can be optimally combined with
forwarding and caching within this framework to maximize user utilities subject
to network stability. Numerical experiments within a number of network settings
demonstrate the superior performance of the resulting algorithms for the actual
plane in terms of high user utilities, low user delay, and high rate of cache
hits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5572</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5572</id><created>2013-10-21</created><updated>2013-11-13</updated><authors><author><keyname>Rettinger</keyname><forenames>Robert</forenames><affiliation>University of Hagen</affiliation></author><author><keyname>Weihrauch</keyname><forenames>Klaus</forenames><affiliation>University of Hagen</affiliation></author></authors><title>Products of effective topological spaces and a uniformly computable
  Tychonoff Theorem</title><categories>cs.LO math.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 4 (November
  14, 2013) lmcs:1053</journal-ref><doi>10.2168/LMCS-9(4:14)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article is a fundamental study in computable analysis. In the framework
of Type-2 effectivity, TTE, we investigate computability aspects on finite and
infinite products of effective topological spaces. For obtaining uniform
results we introduce natural multi-representations of the class of all
effective topological spaces, of their points, of their subsets and of their
compact subsets. We show that the binary, finite and countable product
operations on effective topological spaces are computable. For spaces with
non-empty base sets the factors can be retrieved from the products. We study
computability of the product operations on points, on arbitrary subsets and on
compact subsets. For the case of compact sets the results are uniformly
computable versions of Tychonoff's Theorem (stating that every Cartesian
product of compact spaces is compact) for both, the cover multi-representation
and the &quot;minimal cover&quot; multi-representation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5573</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5573</id><created>2013-10-21</created><authors><author><keyname>Sandu</keyname><forenames>Adrian</forenames></author><author><keyname>Guenther</keyname><forenames>Michael</forenames></author></authors><title>A class of generalized additive Runge-Kutta methods</title><categories>cs.NA math.NA</categories><report-no>Computational Science Laboratory CSL-TR-5/2013</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work generalizes the additively partitioned Runge-Kutta methods by
allowing for different stage values as arguments of different components of the
right hand side. An order conditions theory is developed for the new family of
generalized additive methods, and stability and monotonicity investigations are
carried out. The paper discusses the construction and properties of
implicit-explicit and implicit-implicit,methods in the new framework. The new
family, named GARK, introduces additional flexibility when compared to
traditional partitioned Runge-Kutta methods, and therefore offers additional
opportunities for the development of flexible solvers for systems with multiple
scales, or driven by multiple physical processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5576</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5576</id><created>2013-10-21</created><authors><author><keyname>Bonnet</keyname><forenames>Edouard</forenames></author><author><keyname>Paschos</keyname><forenames>Vangelis Th.</forenames></author></authors><title>Parameterized (in)approximability of subset problems</title><categories>cs.CC</categories><comments>7 pages</comments><msc-class>68Q17</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss approximability and inapproximability in FPT-time for a large
class of subset problems where a feasible solution $S$ is a subset of the input
data and the value of $S$ is $|S|$. The class handled encompasses many
well-known graph, set, or satisfiability problems such as Dominating Set,
Vertex Cover, Set Cover, Independent Set, Feedback Vertex Set, etc. In a first
time, we introduce the notion of intersective approximability that generalizes
the one of safe approximability and show strong parameterized inapproximability
results for many of the subset problems handled. Then, we study approximability
of these problems with respect to the dual parameter $n-k$ where $n$ is the
size of the instance and $k$ the standard parameter. More precisely, we show
that under such a parameterization, many of these problems, while
W[$\cdot$]-hard, admit parameterized approximation schemata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5590</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5590</id><created>2013-10-21</created><authors><author><keyname>Habi&#x10d;</keyname><forenames>Miha E.</forenames></author></authors><title>Cardinal-Recognizing Infinite Time Turing Machines</title><categories>math.LO cs.FL cs.LO</categories><comments>10 pages, presented at 2013 Computability in Europe conference</comments><msc-class>03D60</msc-class><journal-ref>The Nature of Computation. Logic, Algorithms, Applications. 9th
  Conference on Computability in Europe, CiE 2013, Milan, Italy. pp 231-240</journal-ref><doi>10.1007/978-3-642-39053-1_27</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a model of infinitary computation which enhances the infinite
time Turing machine model slightly but in a natural way by giving the machines
the capability of detecting cardinal stages of computation. The computational
strength with respect to ITTMs is determined to be precisely that of the strong
halting problem and the nature of the new characteristic ordinals (clockable,
writable, etc.) is explored.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5597</identifier>
 <datestamp>2013-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5597</id><created>2013-10-21</created><updated>2013-10-25</updated><authors><author><keyname>Couto</keyname><forenames>Francisco M</forenames></author></authors><title>CIDS country rankings: comparing documents and citations of USA, UK and
  China top researchers</title><categories>cs.DL cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This technical report presents a bibliometric analysis of the top 30 cited
researchers from USA, UK and China. The analysis is based on Google Scholar
data using CIDS. The researchers were identified using their email suffix: edu,
uk and cn. This na\&quot;{i}ve approach was able to produce rankings consistent with
the SCImago country rankings using mininal resources in a fully automated way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5603</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5603</id><created>2013-10-21</created><authors><author><keyname>Yan</keyname><forenames>Jie</forenames></author><author><keyname>Tan</keyname><forenames>Guangming</forenames></author><author><keyname>Sun</keyname><forenames>Ninghui</forenames></author></authors><title>GRE: A Graph Runtime Engine for Large-Scale Distributed Graph-Parallel
  Applications</title><categories>cs.DC</categories><comments>12 pages, also submitted to PVLDB</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Large-scale distributed graph-parallel computing is challenging. On one hand,
due to the irregular computation pattern and lack of locality, it is hard to
express parallelism efficiently. On the other hand, due to the scale-free
nature, real-world graphs are hard to partition in balance with low cut. To
address these challenges, several graph-parallel frameworks including Pregel
and GraphLab (PowerGraph) have been developed recently. In this paper, we
present an alternative framework, Graph Runtime Engine (GRE). While retaining
the vertex-centric programming model, GRE proposes two new abstractions: 1) a
Scatter-Combine computation model based on active message to exploit massive
fined-grained edge-level parallelism, and 2) a Agent-Graph data model based on
vertex factorization to partition and represent directed graphs. GRE is
implemented on commercial off-the-shelf multi-core cluster. We experimentally
evaluate GRE with three benchmark programs (PageRank, Single Source Shortest
Path and Connected Components) on real-world and synthetic graphs of millions
billion of vertices. Compared to PowerGraph, GRE shows 2.5~17 times better
performance on 8~16 machines (192 cores). Specifically, the PageRank in GRE is
the fastest when comparing to counterparts of other frameworks (PowerGraph,
Spark,Twister) reported in public literatures. Besides, GRE significantly
optimizes memory usage so that it can process a large graph of 1 billion
vertices and 17 billion edges on our cluster with totally 768GB memory, while
PowerGraph can only process less than half of this graph scale.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5619</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5619</id><created>2013-10-21</created><authors><author><keyname>Dongre</keyname><forenames>Vikas J.</forenames></author><author><keyname>Mankar</keyname><forenames>Vijay H.</forenames></author></authors><title>Devnagari Handwritten Numeral Recognition using Geometric Features and
  Statistical Combination Classifier</title><categories>cs.CV</categories><comments>8 pages, 6 figures, 1 table, journal paper</comments><journal-ref>International Journal on Computer Science and Engineering (IJCSE)
  ISSN : 0975-3397, Vol. 5 No. 10 pp. 856-863, Oct 2013</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper presents a Devnagari Numerical recognition method based on
statistical discriminant functions. 17 geometric features based on pixel
connectivity, lines, line directions, holes, image area, perimeter,
eccentricity, solidity, orientation etc. are used for representing the
numerals. Five discriminant functions viz. Linear, Quadratic, Diaglinear,
Diagquadratic and Mahalanobis distance are used for classification. 1500
handwritten numerals are used for training. Another 1500 handwritten numerals
are used for testing. Experimental results show that Linear, Quadratic and
Mahalanobis discriminant functions provide better results. Results of these
three Discriminants are fed to a majority voting type Combination classifier.
It is found that Combination classifier offers better results over individual
classifiers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5620</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5620</id><created>2013-10-21</created><updated>2015-04-22</updated><authors><author><keyname>Zamora-Martinez</keyname><forenames>Francisco</forenames></author><author><keyname>Romeu</keyname><forenames>Pablo</forenames></author><author><keyname>Botella-Rocamora</keyname><forenames>Paloma</forenames></author><author><keyname>Pardo</keyname><forenames>Juan</forenames></author></authors><title>Towards Energy Efficiency: Forecasting Indoor Temperature via
  Multivariate Analysis</title><categories>cs.SY</categories><comments>22 pages, Published 9 September 2013 at MDPI's journal &quot;Energies&quot;,
  Special Issue &quot;Energy Efficient Building Design 2013&quot;</comments><journal-ref>Energies. 2013; 6(9):4639-4659</journal-ref><doi>10.3390/en6094639</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The small medium large system (SMLSystem) is a house built at the Universidad
CEU Cardenal Herrera (CEU-UCH) for participation in the Solar Decathlon 2013
competition. Several technologies have been integrated to reduce power
consumption. One of these is a forecasting system based on artificial neural
networks (ANNs), which is able to predict indoor temperature in the near future
using captured data by a complex monitoring system as the input. A study of the
impact on forecasting performance of different covariate combinations is
presented in this paper. Additionally, a comparison of ANNs with the standard
statistical forecasting methods is shown. The research in this paper has been
focused on forecasting the indoor temperature of a house, as it is directly
related to HVAC---heating, ventilation and air conditioning---system
consumption. HVAC systems at the SMLSystem house represent 53.9% of the overall
power consumption. The energy used to maintain temperature was measured to be
30--38.9% of the energy needed to lower it. Hence, these forecasting measures
allow the house to adapt itself to future temperature conditions by using home
automation in an energy-efficient manner. Experimental results show a high
forecasting accuracy and therefore, they might be used to efficiently control
an HVAC system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5624</identifier>
 <datestamp>2014-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5624</id><created>2013-10-21</created><updated>2014-05-28</updated><authors><author><keyname>Frahm</keyname><forenames>Klaus M.</forenames></author><author><keyname>Eom</keyname><forenames>Young-Ho</forenames></author><author><keyname>Shepelyansky</keyname><forenames>Dima L.</forenames></author></authors><title>Google matrix of the citation network of Physical Review</title><categories>physics.soc-ph cs.DL cs.SI</categories><comments>25 pages. 17 figures. Published in Phys. Rev. E</comments><journal-ref>Phys. Rev. E 89, 052814 (2014)</journal-ref><doi>10.1103/PhysRevE.89.052814</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the statistical properties of spectrum and eigenstates of the Google
matrix of the citation network of Physical Review for the period 1893 - 2009.
The main fraction of complex eigenvalues with largest modulus is determined
numerically by different methods based on high precision computations with up
to $p=16384$ binary digits that allows to resolve hard numerical problems for
small eigenvalues. The nearly nilpotent matrix structure allows to obtain a
semi-analytical computation of eigenvalues. We find that the spectrum is
characterized by the fractal Weyl law with a fractal dimension $d_f \approx 1$.
It is found that the majority of eigenvectors are located in a localized phase.
The statistical distribution of articles in the PageRank-CheiRank plane is
established providing a better understanding of information flows on the
network. The concept of ImpactRank is proposed to determine an influence domain
of a given article. We also discuss the properties of random matrix models of
Perron-Frobenius operators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5647</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5647</id><created>2013-10-21</created><authors><author><keyname>Agarwal</keyname><forenames>Pankaj</forenames></author><author><keyname>Har-Peled</keyname><forenames>Sariel</forenames></author><author><keyname>Kaplan</keyname><forenames>Haim</forenames></author><author><keyname>Sharir</keyname><forenames>Micha</forenames></author></authors><title>Union of Random Minkowski Sums and Network Vulnerability Analysis</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\mathcal{C}=\{C_1,\ldots,C_n\}$ be a set of $n$ pairwise-disjoint convex
sets of constant description complexity, and let $\pi$ be a probability density
function (pdf for short) over the non-negative reals. For each $i$, let $K_i$
be the Minkowski sum of $C_i$ with a disk of radius $r_i$, where each $r_i$ is
a random non-negative number drawn independently from the distribution
determined by $\pi$. We show that the expected complexity of the union of $K_1,
\ldots, K_n$ is $O(n^{1+\varepsilon})$ for any $\varepsilon &gt; 0$; here the
constant of proportionality depends on $\varepsilon$ and on the description
complexity of the sets in $\mathcal{C}$, but not on $\pi$. If each $C_i$ is a
convex polygon with at most $s$ vertices, then we show that the expected
complexity of the union is $O(s^2n\log n)$.
  Our bounds hold in the stronger model in which we are given an arbitrary
multi-set $R=\{r_1,\ldots,r_n\}$ of expansion radii, each a non-negative real
number. We assign them to the members of $\mathcal{C}$ by a random permutation,
where all permutations are equally likely to be chosen; the expectations are
now with respect to these permutations.
  We also present an application of our results to a problem that arises in
analyzing the vulnerability of a network to a physical attack. %
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5653</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5653</id><created>2013-10-09</created><authors><author><keyname>Razafindradina</keyname><forenames>Henri Bruno</forenames></author><author><keyname>Karim</keyname><forenames>Attoumani Mohamed</forenames></author></authors><title>Blind and robust images watermarking based on wavelet and edge insertion</title><categories>cs.MM</categories><comments>8 pages</comments><msc-class>Computer Science</msc-class><acm-class>K.6.5; C.2.0; D.4.6</acm-class><journal-ref>IJCIS, Vol.3, No. 3, September 2013, pp 23-30</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper gives a new scheme of watermarking technique related to insert the
mark by adding edge in HH sub-band of the host image after wavelet
decomposition. Contrary to most of the watermarking algorithms in wavelet
domain, our method is blind and results show that it is robust against the JPEG
and GIF compression, histogram and spectrum spreading, noise adding and small
rotation. Its robustness against compression is better than others watermarking
algorithms reported in the literature. The algorithm is flexible because its
capacity or robustness can be improved by modifying some parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5656</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5656</id><created>2013-10-21</created><updated>2013-11-21</updated><authors><author><keyname>Skordev</keyname><forenames>Dimiter</forenames><affiliation>Sofia University, Faculty of Mathematics and Informatics, Sofia, Bulgaria</affiliation></author></authors><title>Approximation systems for functions in topological and in metric spaces</title><categories>cs.LO math.LO</categories><comments>21 pages, published in Logical Methods in Computer Science</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 4 (November
  20, 2013) lmcs:890</journal-ref><doi>10.2168/LMCS-9(4:15)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A notable feature of the TTE approach to computability is the representation
of the argument values and the corresponding function values by means of
infinitistic names. Two ways to eliminate the using of such names in certain
cases are indicated in the paper. The first one is intended for the case of
topological spaces with selected indexed denumerable bases. Suppose a partial
function is given from one such space into another one whose selected base has
a recursively enumerable index set, and suppose that the intersection of base
open sets in the first space is computable in the sense of Weihrauch-Grubba.
Then the ordinary TTE computability of the function is characterized by the
existence of an appropriate recursively enumerable relation between indices of
base sets containing the argument value and indices of base sets containing the
corresponding function value.This result can be regarded as an improvement of a
result of Korovina and Kudinov. The second way is applicable to metric spaces
with selected indexed denumerable dense subsets. If a partial function is given
from one such space into another one, then, under a semi-computability
assumption concerning these spaces, the ordinary TTE computability of the
function is characterized by the existence of an appropriate recursively
enumerable set of quadruples. Any of them consists of an index of element from
the selected dense subset in the first space, a natural number encoding a
rational bound for the distance between this element and the argument value, an
index of element from the selected dense subset in the second space and a
natural number encoding a rational bound for the distance between this element
and the function value. One of the examples in the paper indicates that the
computability of real functions can be characterized in a simple way by using
the first way of elimination of the infinitistic names.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5660</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5660</id><created>2013-10-21</created><authors><author><keyname>Talebi</keyname><forenames>M. Sadegh</forenames></author></authors><title>Uncoupled Learning Rules for Seeking Equilibria in Repeated Plays: An
  Overview</title><categories>cs.GT</categories><comments>Working paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, we consider repeated play of a finite game using learning rules
whose period-by-period behavior probabilities or empirical distributions
converge to some notion of equilibria of the stage game. Our primary focus is
on uncoupled and completely uncoupled learning rules. While the former relies
on players being aware of only their own payoff functions and able to monitor
the action taken by the others, the latter assumes that players only know their
own past realized payoffs. We highlight the border between possible and
impossible results using these rules. We also overview several uncoupled and
completely uncoupled learning rules, most of which leverage notions of regret
as the solution concept to seek payoff-improving action profiles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5664</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5664</id><created>2013-10-21</created><authors><author><keyname>Aharonov</keyname><forenames>Dorit</forenames></author><author><keyname>Eldar</keyname><forenames>Lior</forenames></author></authors><title>Quantum Locally Testable Codes</title><categories>quant-ph cs.CC math-ph math.MP</categories><comments>Some of the results presented here appeared in an initial form in our
  quant-ph submission arXiv:1301.3407. This is a much extended and improved
  version. 30 pages, no figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We initiate the study of quantum Locally Testable Codes (qLTCs). We provide a
definition together with a simplification, denoted sLTCs, for the special case
of stabilizer codes, together with some basic results using those definitions.
The most crucial parameter of such codes is their soundness, $R(\delta)$,
namely, the probability that a randomly chosen constraint is violated as a
function of the distance of a word from the code ($\delta$, the relative
distance from the code, is called the proximity). We then proceed to study
limitations on qLTCs. In our first main result we prove a surprising,
inherently quantum, property of sLTCs: for small values of proximity, the
better the small-set expansion of the interaction graph of the constraints, the
less sound the qLTC becomes. This phenomenon, which can be attributed to
monogamy of entanglement, stands in sharp contrast to the classical setting.
The complementary, more intuitive, result also holds: an upper bound on the
soundness when the code is defined on poor small-set expanders (a bound which
turns out to be far more difficult to show in the quantum case). Together we
arrive at a quantum upper-bound on the soundness of stabilizer qLTCs set on any
graph, which does not hold in the classical case. Many open questions are
raised regarding what possible parameters are achievable for qLTCs. In the
appendix we also define a quantum analogue of PCPs of proximity (PCPPs) and
point out that the result of Ben-Sasson et. al. by which PCPPs imply LTCs with
related parameters, carries over to the sLTCs. This creates a first link
between qLTCs and quantum PCPs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5665</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5665</id><created>2013-10-21</created><updated>2014-12-02</updated><authors><author><keyname>Mohri</keyname><forenames>Mehryar</forenames></author><author><keyname>Medina</keyname><forenames>Andres Mu&#xf1;oz</forenames></author></authors><title>Learning Theory and Algorithms for Revenue Optimization in Second-Price
  Auctions with Reserve</title><categories>cs.LG</categories><comments>Accepted at ICML 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Second-price auctions with reserve play a critical role for modern search
engine and popular online sites since the revenue of these companies often
directly de- pends on the outcome of such auctions. The choice of the reserve
price is the main mechanism through which the auction revenue can be influenced
in these electronic markets. We cast the problem of selecting the reserve price
to optimize revenue as a learning problem and present a full theoretical
analysis dealing with the complex properties of the corresponding loss
function. We further give novel algorithms for solving this problem and report
the results of several experiments in both synthetic and real data
demonstrating their effectiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5670</identifier>
 <datestamp>2014-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5670</id><created>2013-10-21</created><updated>2014-09-09</updated><authors><author><keyname>Terasawa</keyname><forenames>Yoshihiro</forenames></author></authors><title>A simple authentication by common strings</title><categories>cs.CR</categories><comments>in Japanese</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It was the problem of finding the minimum value of the sum of the distances
of the path through all cities Overview TSP. We propose an authentication with
the problem that the deformation sum of the distances of the path to be a
constant value. In this document, it is intended to construct an authentication
function robust implementation is easy and the Blog. After it was shown that
the first, to determine the replacement group and path are the same, we propose
the authentication method to consider the deformation of the traveling salesman
problem in the directed graph, using a sequence of bytes. Instead of providing
illumination mathematically rigorous, describes a verifiable algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5684</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5684</id><created>2013-10-21</created><updated>2015-08-27</updated><authors><author><keyname>Pudl&#xe1;k</keyname><forenames>Pavel</forenames></author></authors><title>Linear tree codes and the problem of explicit constructions</title><categories>cs.IT math.IT</categories><msc-class>94B60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We reduce the problem of constructing asymptotically good tree codes to the
construction of triangular totally nonsingular matrices over fields with
polynomially many elements. We show a connection of this problem to Birkhoff
interpolation in finite fields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5695</identifier>
 <datestamp>2014-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5695</id><created>2013-10-20</created><updated>2014-08-18</updated><authors><author><keyname>Sun</keyname><forenames>Jiajun</forenames></author></authors><title>Collection-behavior based Multi-parameter Posted Pricing Mechanism for
  Crowd Sensing</title><categories>cs.GT cs.NI</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in Figure 3</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crowd sensing is a new paradigm which leverages a large number of
sensor-equipped mobile phones to collect sensing data. To attract more
participants to provide good quality, bidding mechanisms that solicit the
Vickrey-Clarke-Groves (VCG) mechanism and its variants are natural fits for
crowd sensing applications in mobile social networks. However, in practical
continuous crowd sensing applications, where bids cannot be solicited and only
posted pricing mechanisms can be implemented. Obviously, these mechanisms for
continuous crowd sensing are not applicable. To tackle the issue, we propose a
collection-behavior based multi-parameter posted pricing mechanism, not only to
consider extensive user participating and sensing data submission quality under
given budget constraints by applying all-pay auctions and posted pricing
mechanisms, but also to maximize the coverage utilities by applying crowd
aversion. Simulation results indicate that incentive mechanisms in our proposed
framework outperform the best existing solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5697</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5697</id><created>2013-10-21</created><authors><author><keyname>Gupta</keyname><forenames>Hitesh</forenames></author><author><keyname>Jain</keyname><forenames>Dr. S. C.</forenames></author></authors><title>Multivalued Logic Circuit Design for Binary Logic Interface</title><categories>cs.OH</categories><comments>72 pages,Dissertation Report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Binary logic and devices have been in used since inception with advancement
and technology and millennium gate design era. The development in binary logic
has become tedious and cumbersome. Multivalued logic enables significant more
information to be packed within a single digit. The design and development of
logic circuit becomes very compact and easier. Attempts are being made to
fabricate multivalued logic based devices. Since present devices can be
implemented only in binary system,it is necessary to evolve a system that can
built the circuit in multivalued logic system and convert in binary logic
system. In multivalued logic system logic gates differ in different logic
system, a quaternary has become mature in terms of logic algebra and gates.
Hence logic design based on above system can be done using standard procedure.
In this dissertation a logic circuit design entry based on multivalued logic
system has been taken up that can provide the ease of circuit design in
multivalued system and output as binary valued circuit. The named &quot;MVL-DEV&quot;
offers editing, storage and conversion into binary facility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5698</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5698</id><created>2013-10-21</created><authors><author><keyname>Guisado-G&#xe1;mez</keyname><forenames>Joan</forenames></author><author><keyname>Dominguez-Sal</keyname><forenames>David</forenames></author><author><keyname>Larriba-Pey</keyname><forenames>Josep-LLuis</forenames></author></authors><title>Massive Query Expansion by Exploiting Graph Knowledge Bases</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Keyword based search engines have problems with term ambiguity and vocabulary
mismatch. In this paper, we propose a query expansion technique that enriches
queries expressed as keywords and short natural language descriptions. We
present a new massive query expansion strategy that enriches queries using a
knowledge base by identifying the query concepts, and adding relevant synonyms
and semantically related terms. We propose two approaches: (i) lexical
expansion that locates the relevant concepts in the knowledge base; and, (ii)
topological expansion that analyzes the network of relations among the
concepts, and suggests semantically related terms by path and community
analysis of the knowledge graph. We perform our expansions by using two
versions of the Wikipedia as knowledge base, concluding that the combination of
both lexical and topological expansion provides improvements of the system's
precision up to more than 27%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5714</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5714</id><created>2013-10-21</created><authors><author><keyname>Lauria</keyname><forenames>Massimo</forenames></author></authors><title>Short $\mathsf{Res}^*(\mathsf{polylog})$ refutations if and only if
  narrow $\mathsf{Res}$ refutations</title><categories>cs.CC cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note we show that any $k$-CNF which can be refuted by a
quasi-polynomial $\mathsf{Res}^*(\mathsf{polylog})$ refutation has a &quot;narrow&quot;
refutation in $\mathsf{Res}$ (i.e., of poly-logarithmic width). We also show
the converse implication: a narrow Resolution refutation can be simulated by a
short $\mathsf{Res}^*(\mathsf{polylog})$ refutation.
  The author does not claim priority on this result. The technical part of this
note bears similarity with the relation between $d$-depth Frege refutations and
tree-like $d+1$-depth Frege refutations outlined in (Kraj\'i\v{c}ek 1994,
Journal of Symbolic Logic 59, 73). Part of it had already been specialized to
$\mathsf{Res}$ and $\mathsf{Res}(k)$ in (Esteban et al. 2004, Theor. Comput.
Sci. 321, 347).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5715</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5715</id><created>2013-10-21</created><updated>2015-01-16</updated><authors><author><keyname>Needell</keyname><forenames>Deanna</forenames></author><author><keyname>Srebro</keyname><forenames>Nathan</forenames></author><author><keyname>Ward</keyname><forenames>Rachel</forenames></author></authors><title>Stochastic Gradient Descent, Weighted Sampling, and the Randomized
  Kaczmarz algorithm</title><categories>math.NA cs.CV cs.LG math.OC stat.ML</categories><comments>22 pages, 6 figures</comments><msc-class>65B99, 52A99, 60G99, 62L20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We obtain an improved finite-sample guarantee on the linear convergence of
stochastic gradient descent for smooth and strongly convex objectives,
improving from a quadratic dependence on the conditioning $(L/\mu)^2$ (where
$L$ is a bound on the smoothness and $\mu$ on the strong convexity) to a linear
dependence on $L/\mu$. Furthermore, we show how reweighting the sampling
distribution (i.e. importance sampling) is necessary in order to further
improve convergence, and obtain a linear dependence in the average smoothness,
dominating previous results. We also discuss importance sampling for SGD more
broadly and show how it can improve convergence also in other scenarios. Our
results are based on a connection we make between SGD and the randomized
Kaczmarz algorithm, which allows us to transfer ideas between the separate
bodies of literature studying each of the two methods. In particular, we recast
the randomized Kaczmarz algorithm as an instance of SGD, and apply our results
to prove its exponential convergence, but to the solution of a weighted least
squares problem rather than the original least squares problem. We then present
a modified Kaczmarz algorithm with partially biased sampling which does
converge to the original least squares solution with the same exponential
convergence rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5720</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5720</id><created>2013-10-21</created><authors><author><keyname>Kornbluth</keyname><forenames>Yosef</forenames></author><author><keyname>Lowinger</keyname><forenames>Steven</forenames></author><author><keyname>Cwilich</keyname><forenames>Gabriel</forenames></author><author><keyname>Buldyrev</keyname><forenames>Sergey V.</forenames></author></authors><title>Cascading Failures in Networks with Proximate Dependent Nodes</title><categories>physics.soc-ph cs.SI physics.data-an</categories><journal-ref>Phys. Rev. E 89, 032808 (2014)</journal-ref><doi>10.1103/PhysRevE.89.032808</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the mutual percolation of a system composed of two interdependent
random regular networks. We introduce a notion of distance to explore the
effects of the proximity of interdependent nodes on the cascade of failures
after an initial attack. We find a non-trivial relation between the nature of
the transition through which the networks disintegrate and the parameters of
the system, which are the degree of the nodes and the maximum distance between
interdependent nodes. We explain this relation by solving the problem
analytically for the relevant set of cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5738</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5738</id><created>2013-10-21</created><authors><author><keyname>Hutter</keyname><forenames>Frank</forenames></author><author><keyname>Osborne</keyname><forenames>Michael A.</forenames></author></authors><title>A Kernel for Hierarchical Parameter Spaces</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a family of kernels for mixed continuous/discrete hierarchical
parameter spaces and show that they are positive definite.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5746</identifier>
 <datestamp>2013-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5746</id><created>2013-10-21</created><updated>2013-11-07</updated><authors><author><keyname>Gwynne</keyname><forenames>Matthew</forenames></author><author><keyname>Kullmann</keyname><forenames>Oliver</forenames></author></authors><title>Trading inference effort versus size in CNF Knowledge Compilation</title><categories>cs.CC</categories><comments>43 pages, second version with literature updates. Proceeds with the
  separation results from the discontinued arXiv:1302.4421</comments><msc-class>94C10</msc-class><acm-class>F.2.3; I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowledge Compilation (KC) studies compilation of boolean functions f into
some formalism F, which allows to answer all queries of a certain kind in
polynomial time. Due to its relevance for SAT solving, we concentrate on the
query type &quot;clausal entailment&quot; (CE), i.e., whether a clause C follows from f
or not, and we consider subclasses of CNF, i.e., clause-sets F with special
properties. In this report we do not allow auxiliary variables (except of the
Outlook), and thus F needs to be equivalent to f.
  We consider the hierarchies UC_k &lt;= WC_k, which were introduced by the
authors in 2012. Each level allows CE queries. The first two levels are
well-known classes for KC. Namely UC_0 = WC_0 is the same as PI as studied in
KC, that is, f is represented by the set of all prime implicates, while UC_1 =
WC_1 is the same as UC, the class of unit-refutation complete clause-sets
introduced by del Val 1994. We show that for each k there are (sequences of)
boolean functions with polysize representations in UC_{k+1}, but with an
exponential lower bound on representations in WC_k. Such a separation was
previously only know for k=0. We also consider PC &lt; UC, the class of
propagation-complete clause-sets. We show that there are (sequences of) boolean
functions with polysize representations in UC, while there is an exponential
lower bound for representations in PC. These separations are steps towards a
general conjecture determining the representation power of the hierarchies PC_k
&lt; UC_k &lt;= WC_k. The strong form of this conjecture also allows auxiliary
variables, as discussed in depth in the Outlook.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5747</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5747</id><created>2013-10-21</created><updated>2014-02-18</updated><authors><author><keyname>Melliti</keyname><forenames>Tarek</forenames></author><author><keyname>Noual</keyname><forenames>Mathilde</forenames></author><author><keyname>Regnault</keyname><forenames>Damien</forenames></author><author><keyname>Sen&#xe9;</keyname><forenames>Sylvain</forenames></author><author><keyname>Sobieraj</keyname><forenames>J&#xe9;r&#xe9;my</forenames></author></authors><title>Full characterisation of attractors of two intersected asynchronous
  Boolean automata cycles</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The understanding of Boolean automata networks dynamics takes an important
place in various domains of computer science such as computability, complexity
and discrete dynamical systems. In this paper, we make a step further in this
understanding by focusing on their cycles, whose necessity in networks is known
as the brick of their complexity. We present new results that provide a
characterisation of the transient and asymptotic dynamics, i.e. of the
computational abilities, of asynchronous Boolean automata networks composed of
two cycles that intersect at one automaton, the so-called double-cycles. To do
so, we introduce an efficient formalism inspired by algorithms to define long
sequences of updates, that allows a better description of their dynamics than
previous works in this area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5748</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5748</id><created>2013-10-21</created><updated>2014-08-25</updated><authors><author><keyname>&#x160;ulc</keyname><forenames>Petr</forenames></author><author><keyname>Backhaus</keyname><forenames>Scott</forenames></author><author><keyname>Chertkov</keyname><forenames>Michael</forenames></author></authors><title>Optimal Distributed Control of Reactive Power via the Alternating
  Direction Method of Multipliers</title><categories>math.OC cs.SY</categories><journal-ref>IEEE Transactions on Energy Conversion 29 (2014), 968 - 977</journal-ref><doi>10.1109/TEC.2014.2363196</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We formulate the control of reactive power generation by photovoltaic
inverters in a power distribution circuit as a constrained optimization that
aims to minimize reactive power losses subject to finite inverter capacity and
upper and lower voltage limits at all nodes in the circuit. When voltage
variations along the circuit are small and losses of both real and reactive
powers are small compared to the respective flows, the resulting optimization
problem is convex. Moreover, the cost function is separable enabling a
distributed, on-line implementation with node-local computations using only
local measurements augmented with limited information from the neighboring
nodes communicated over cyber channels. Such an approach lies between the fully
centralized and local policy approaches previously considered. We explore
protocols based on the dual ascent method and on the Alternating Direction
Method of Multipliers (ADMM) and find that the ADMM protocol performs
significantly better.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5755</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5755</id><created>2013-10-21</created><authors><author><keyname>Egger</keyname><forenames>Jan</forenames></author><author><keyname>Bauer</keyname><forenames>Miriam H. A.</forenames></author><author><keyname>Gro&#xdf;kopf</keyname><forenames>Stefan</forenames></author><author><keyname>Biermann</keyname><forenames>Christina</forenames></author><author><keyname>Freisleben</keyname><forenames>Bernd</forenames></author><author><keyname>Nimsky</keyname><forenames>Christopher</forenames></author></authors><title>Determination, Calculation and Representation of the Upper and Lower
  Sealing Zones During Virtual Stenting of Aneurysms</title><categories>cs.CV physics.med-ph q-bio.TO</categories><comments>4 pages, 2 figures, 10 references</comments><journal-ref>Int J CARS, Vol. 5, Suppl. 1, pp. 13-14, Springer Press, June 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this contribution, a novel method for stent simulation in preoperative
computed tomography angiography (CTA) acquisitions of patients is presented
where the sealing zones are automatically calculated and visualized. The method
is eligible for non-bifurcated and bifurcated stents (Y-stents). Results of the
proposed stent simulation with an automatic calculation of the sealing zones
for specific diseases (abdominal aortic aneurysms (AAA), thoracic aortic
aneurysms (TAA), iliac aneurysms) are presented. The contribution is organized
as follows. Section 2 presents the proposed approach. In Section 3,
experimental results are discussed. Section 4 concludes the contribution and
outlines areas for future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5767</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5767</id><created>2013-10-21</created><authors><author><keyname>Li</keyname><forenames>Xi</forenames></author><author><keyname>Li</keyname><forenames>Yao</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Dick</keyname><forenames>Anthony</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author></authors><title>Contextual Hypergraph Modelling for Salient Object Detection</title><categories>cs.CV</categories><comments>Appearing in Proc. Int. Conf. Computer Vision 2013, Sydney, Australia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Salient object detection aims to locate objects that capture human attention
within images. Previous approaches often pose this as a problem of image
contrast analysis. In this work, we model an image as a hypergraph that
utilizes a set of hyperedges to capture the contextual properties of image
pixels or regions. As a result, the problem of salient object detection becomes
one of finding salient vertices and hyperedges in the hypergraph. The main
advantage of hypergraph modeling is that it takes into account each pixel's (or
region's) affinity with its neighborhood as well as its separation from image
background. Furthermore, we propose an alternative approach based on
center-versus-surround contextual contrast analysis, which performs salient
object detection by optimizing a cost-sensitive support vector machine (SVM)
objective function. Experimental results on four challenging datasets
demonstrate the effectiveness of the proposed approaches against the
state-of-the-art approaches to salient object detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5770</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5770</id><created>2013-10-21</created><updated>2014-04-26</updated><authors><author><keyname>Saldi</keyname><forenames>Naci</forenames></author><author><keyname>Linder</keyname><forenames>Tam&#xe1;s</forenames></author><author><keyname>Y&#xfc;ksel</keyname><forenames>Serdar</forenames></author></authors><title>Quantized Stationary Control Policies in Markov Decision Processes</title><categories>math.OC cs.SY</categories><comments>21 pages</comments><msc-class>93E20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a large class of Markov Decision Processes, stationary (possibly
randomized) policies are globally optimal. However, in Borel state and action
spaces, the computation and implementation of even such stationary policies are
known to be prohibitive. In addition, networked control applications require
remote controllers to transmit action commands to an actuator with low
information rate. These two problems motivate the study of approximating
optimal policies by quantized (discretized) policies. To this end, we introduce
deterministic stationary quantizer policies and show that such policies can
approximate optimal deterministic stationary policies with arbitrary precision
under mild technical conditions, thus demonstrating that one can search for
$\varepsilon$-optimal policies within the class of quantized control policies.
We also derive explicit bounds on the approximation error in terms of the rate
of the approximating quantizers. We extend all these approximation results to
randomized policies. These findings pave the way toward applications in optimal
design of networked control systems where controller actions need to be
quantized, as well as for new computational methods for generating
approximately optimal decision policies in general (Polish) state and action
spaces for both discounted cost and average cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5777</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5777</id><created>2013-10-21</created><authors><author><keyname>Wang</keyname><forenames>Xianwen</forenames></author><author><keyname>Peng</keyname><forenames>Lian</forenames></author><author><keyname>Zhang</keyname><forenames>Chunbo</forenames></author><author><keyname>Xu</keyname><forenames>Shenmeng</forenames></author><author><keyname>Wang</keyname><forenames>Zhi</forenames></author><author><keyname>Wang</keyname><forenames>Chuanli</forenames></author><author><keyname>Wang</keyname><forenames>Xianbing</forenames></author></authors><title>Exploring Scientists' Working Timetable: A Global Survey</title><categories>cs.DL cs.IR physics.soc-ph</categories><journal-ref>Journal of Informetrics. 2013, 7(3):665-675</journal-ref><doi>10.1016/j.joi.2013.04.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In our previous study (Wang et al., 2012), we analyzed scientists' working
timetable of 3 countries, using realtime downloading data of scientific
literatures. In this paper, we make a through analysis about global scientists'
working habits. Top 30 countries/territories from Europe, Asia, Australia,
North America, Latin America and Africa are selected as representatives and
analyzed in detail. Regional differences for scientists' working habits exists
in different countries. Besides different working cultures, social factors
could affect scientists' research activities and working patterns.
Nevertheless, a common conclusion is that scientists today are often working
overtime. Although scientists may feel engaged and fulfilled about their hard
working, working too much still warns us to reconsider the work - life balance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5781</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5781</id><created>2013-10-21</created><authors><author><keyname>Flannery</keyname><forenames>Madison</forenames></author><author><keyname>Fenn</keyname><forenames>Shannon</forenames></author><author><keyname>Budden</keyname><forenames>David</forenames></author></authors><title>RANSAC: Identification of Higher-Order Geometric Features and
  Applications in Humanoid Robot Soccer</title><categories>cs.RO cs.AI cs.CV</categories><comments>8 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability for an autonomous agent to self-localise is directly proportional
to the accuracy and precision with which it can perceive salient features
within its local environment. The identification of such features by
recognising geometric profile allows robustness against lighting variations,
which is necessary in most industrial robotics applications. This paper details
a framework by which the random sample consensus (RANSAC) algorithm, often
applied to parameter fitting in linear models, can be extended to identify
higher-order geometric features. Goalpost identification within humanoid robot
soccer is investigated as an application, with the developed system yielding an
order-of-magnitude improvement in classification performance relative to a
traditional histogramming methodology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5786</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5786</id><created>2013-10-21</created><authors><author><keyname>Cai</keyname><forenames>Leizhen</forenames></author><author><keyname>Guo</keyname><forenames>Chengwei</forenames></author></authors><title>Contracting Graphs to Split Graphs and Threshold Graphs</title><categories>cs.DS</categories><comments>14 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the parameterized complexity of Split Contraction and Threshold
Contraction. In these problems we are given a graph G and an integer k and
asked whether G can be modified into a split graph or a threshold graph,
respectively, by contracting at most k edges. We present an FPT algorithm for
Split Contraction, and prove that Threshold Contraction on split graphs, i.e.,
contracting an input split graph to a threshold graph, is FPT when
parameterized by the number of contractions. To give a complete picture, we
show that these two problems admit no polynomial kernels unless NP\subseteq
coNP/poly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5791</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5791</id><created>2013-10-21</created><updated>2014-12-09</updated><authors><author><keyname>Cai</keyname><forenames>T. Tony</forenames></author><author><keyname>Zhang</keyname><forenames>Anru</forenames></author></authors><title>ROP: Matrix recovery via rank-one projections</title><categories>math.ST cs.IT math.IT stat.ME stat.ML stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/14-AOS1267 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS1267</report-no><journal-ref>Annals of Statistics 2015, Vol. 43, No. 1, 102-138</journal-ref><doi>10.1214/14-AOS1267</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Estimation of low-rank matrices is of significant interest in a range of
contemporary applications. In this paper, we introduce a rank-one projection
model for low-rank matrix recovery and propose a constrained nuclear norm
minimization method for stable recovery of low-rank matrices in the noisy case.
The procedure is adaptive to the rank and robust against small perturbations.
Both upper and lower bounds for the estimation accuracy under the Frobenius
norm loss are obtained. The proposed estimator is shown to be rate-optimal
under certain conditions. The estimator is easy to implement via convex
programming and performs well numerically. The techniques and main results
developed in the paper also have implications to other related statistical
problems. An application to estimation of spiked covariance matrices from
one-dimensional random projections is considered. The results demonstrate that
it is still possible to accurately estimate the covariance matrix of a
high-dimensional distribution based only on one-dimensional projections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5793</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5793</id><created>2013-10-21</created><authors><author><keyname>Mulay</keyname><forenames>Snehal</forenames></author><author><keyname>Dhekne</keyname><forenames>Chinmay</forenames></author><author><keyname>Bapat</keyname><forenames>Rucha</forenames></author><author><keyname>Budukh</keyname><forenames>Tanmay</forenames></author><author><keyname>Gadgil</keyname><forenames>Soham</forenames></author></authors><title>Intelligent City Traffic Management and Public Transportation System</title><categories>cs.AI cs.CY</categories><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 10,
  Issue 3, No 1, May 2013 ISSN (Print): 1694-0814 | ISSN (Online): 1694-0784
  Reference: IJCSI-10-3-1-46-50</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intelligent Transportation System in case of cities is controlling traffic
congestion and regulating the traffic flow. This paper presents three modules
that will help in managing city traffic issues and ultimately gives advanced
development in transportation system. First module, Congestion Detection and
Management will provide user real time information about congestion on the road
towards his destination, Second module, Intelligent Public Transport System
will provide user real time public transport information,i.e, local buses, and
the third module, Signal Synchronization will help in controlling congestion at
signals, with real time adjustments of signal timers according to the
congestion. All the information that user is getting about the traffic or
public transportation will be provided on users day to day device that is
mobile through Android application or SMS. Moreover, communication can also be
done via Website for Clients having internet access. And all these modules will
be fully automated without any human intervention at server side.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5794</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5794</id><created>2013-10-22</created><authors><author><keyname>Soomro</keyname><forenames>Safeeullah</forenames></author><author><keyname>Khan</keyname><forenames>Adnan Alam</forenames></author><author><keyname>Memon</keyname><forenames>Abdul Ghafoor</forenames></author><author><keyname>Iftikhar</keyname><forenames>Asim</forenames></author><author><keyname>Mujeeb-u-Rehman</keyname><forenames>Maree</forenames></author></authors><title>An Operational Approach For Wimax At Ultra High Bandwidth With Spectrum
  60 Ghz</title><categories>cs.NI</categories><journal-ref>Sindh Univ. Res. Jour. (Sci. Ser.) Vol.44 (3) 535-540 (2012)</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  WiMax is a promising network of today industry. It provides P2P and P2MP
point to multipoint broadband services up to thirty miles. Its operational
frequency range is 10 GHz to 60 GHz. It provides data rate of 75Mbps per
channel; with an end-to-end encryption called CCMP (Counter Mode with Cipher
Block Chaining Message Authentication Code Protocol). CCMP is an Advanced
Encryption Standard AES based encryption method, which delivers secure
communication. Telecom industry seeks secure, cheaper, wireless metro area
network, that full fill the today internet demand in most efficient way. Our
research explores the new dimension of WiMax with HEMT High Electron Mobility
Transistor using un-licensed Band. The Aim of this paper is to simulate 60GHz
unlicensed WiMax band in Matlab. This research explores millimeter waves,
related electronics, mathematics of WiMax and simulated graph comparison. It
determines available capacity verses coverage area and transmission bit error
probability. Further it highlights the ideal modulation condition in different
terrains. Research proofs that WiMax will be the future promising wireless
network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5796</identifier>
 <datestamp>2014-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5796</id><created>2013-10-22</created><updated>2014-10-22</updated><authors><author><keyname>Cortes</keyname><forenames>Corinna</forenames></author><author><keyname>Greenberg</keyname><forenames>Spencer</forenames></author><author><keyname>Mohri</keyname><forenames>Mehryar</forenames></author></authors><title>Relative Deviation Learning Bounds and Generalization with Unbounded
  Loss Functions</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an extensive analysis of relative deviation bounds, including
detailed proofs of two-sided inequalities and their implications. We also give
detailed proofs of two-sided generalization bounds that hold in the general
case of unbounded loss functions, under the assumption that a moment of the
loss is bounded. These bounds are useful in the analysis of importance
weighting and other learning tasks such as unbounded regression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5805</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5805</id><created>2013-10-22</created><authors><author><keyname>Lazzez</keyname><forenames>Amor</forenames></author><author><keyname>Fredj</keyname><forenames>Ouissem Ben</forenames></author><author><keyname>Slimani</keyname><forenames>Thabet</forenames></author></authors><title>IAX-Based Peer-to-Peer VoIP Architecture</title><categories>cs.NI</categories><comments>11 pages, 2 figures</comments><journal-ref>Published in IJCSI Journal, Volume 10, Issue 3, No 2, May 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, Voice over IP (VoIP) constitutes a privileged field of service
innovation. One benefit of the VoIP technology is that it may be deployed using
a centralized or a distributed architecture. One of the most efficient
approaches used in the deployment of centralized VoIP systems is based on the
use of IAX (Inter-Asterisk Exchange), an open-source signaling/data exchange
protocol. Even though they are currently widely used, client-server VoIP
systems suffer from many weaknesses such as the presence of single points of
failure, an inefficient resources management, and system non-scalability. In
order to help the development of scalable and reliable VoIP systems, the
development community starts tending towards the deployment of the VoIP service
using a peer-to-peer distributed architecture. The aim of this paper is to
develop an IAX-based peer-to-peer VoIP architecture, an optimized VoIP
architecture that takes advantage of the benefits of the IAX protocol and those
of the peer-to-peer distribution model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5806</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5806</id><created>2013-10-22</created><authors><author><keyname>Yuan</keyname><forenames>Zhengzhong</forenames></author><author><keyname>Zhao</keyname><forenames>Chen</forenames></author><author><keyname>Di</keyname><forenames>Zengru</forenames></author><author><keyname>Wang</keyname><forenames>Wen-Xu</forenames></author><author><keyname>Lai</keyname><forenames>Ying-Cheng</forenames></author></authors><title>Exact Controllability of Complex Networks</title><categories>physics.soc-ph cond-mat.dis-nn cs.SI</categories><comments>19 pages, 3 figures, 3 tables</comments><journal-ref>Nat. Commun. 4:2447 (2013)</journal-ref><doi>10.1038/ncomms3447</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Controlling complex networks is of paramount importance in science and
engineering. Despite the recent development of structural-controllability
theory, we continue to lack a framework to control undirected complex networks,
especially given link weights. Here we introduce an exact-controllability
paradigm based on the maximum multiplicity to identify the minimum set of
driver nodes required to achieve full control of networks with arbitrary
structures and link-weight distributions. The framework reproduces the
structural controllability of directed networks characterized by structural
matrices. We explore the controllability of a large number of real and model
networks, finding that dense networks with identical weights are difficult to
be controlled. An efficient and accurate tool is offered to assess the
controllability of large sparse and dense networks. The exact-controllability
framework enables a comprehensive understanding of the impact of network
properties on controllability, a fundamental problem towards our ultimate
control of complex systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5812</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5812</id><created>2013-10-22</created><authors><author><keyname>Ordu&#xf1;a-Malea</keyname><forenames>Enrique</forenames></author><author><keyname>Regazzi</keyname><forenames>John J.</forenames></author></authors><title>U.S. academic libraries: understanding their web presence and their
  relationship with economic indicators</title><categories>cs.DL</categories><doi>10.1007/s11192-013-1001-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main goal of this research is to analyze the web structure and
performance of units and services belonging to U.S. academic libraries in order
to check their suitability for webometric studies. Our objectives include
studying their possible correlation with economic data and assessing their use
for complementary evaluation purposes. We conducted a survey of library
homepages, institutional repositories, digital collections, and online catalogs
(a total of 374 URLs) belonging to the 100 U.S. universities with the highest
total expenditures in academic libraries according to data provided by the
National Center for Education Statistics (NCES). Several data points were taken
and analyzed, including web variables (page count, external links, and visits)
and economic variables (total expenditures, expenditures on printed and
electronic books, and physical visits). The results indicate that the variety
of URL syntaxes is wide, diverse and complex, which produces a
misrepresentation of academic library web resources and reduces the accuracy of
web analysis. On the other hand, institutional and web data indicators are not
highly correlated. Better results are obtained by correlating total library
expenditures with URL mentions measured by Google (r= 0.546) and visits
measured by Compete (r= 0.573), respectively. Because correlation values
obtained are not highly significant, we estimate such correlations will
increase if users can avoid linkage problems (due to the complexity of URLs)
and gain direct access to log files (for more accurate data about visits).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5814</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5814</id><created>2013-10-22</created><authors><author><keyname>Ordu&#xf1;a-Malea</keyname><forenames>Enrique</forenames></author></authors><title>Aggregation of the web performance of internal university units as a
  method of quantitative analysis of a university system: the case of Spain</title><categories>cs.DL</categories><journal-ref>JASIST, 64(10), 2100-2114</journal-ref><doi>10.1002/asi.22912</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aggregation of web performance (page count and visibility) of internal
university units could constitute a more precise indicator than the overall web
performance of the universities and, therefore, be of use in the design of
university web rankings. In order to test this hypothesis, a longitudinal
analysis of the internal units of the Spanish university system was conducted
over the course of 2010. For the 13800 URLs identified, page count and
visibility was calculated using the Yahoo API. The internal values obtained
were aggregated by university and compared with the values obtained from the
analysis of the university general URLs. The results indicate that, although
the correlations between general and internal values are high, internal
performance is low in comparison to general performance, and that they give
rise to different performance rankings. The conclusion is that the aggregation
of unit performance is of limited use due to the low levels of internal
development of the websites, and so its use is not recommended for the design
of rankings. Despite this, the internal analysis enabled the detection of,
amongst other things, a low correlation between page count and visibility due
to the widespread use of subdirectories and problems accessing certain content.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5815</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5815</id><created>2013-10-22</created><authors><author><keyname>Orduna-Malea</keyname><forenames>Enrique</forenames></author><author><keyname>Ontalba-Ruiperez</keyname><forenames>Jose-Antonio</forenames></author></authors><title>Selective linking from social platforms to university websites: a case
  study of the Spanish academic system</title><categories>cs.DL cs.SI physics.soc-ph</categories><journal-ref>Scientometrics Vol. 95, Issue 2, pp. 594-614 (2013)</journal-ref><doi>10.1007/s11192-012-0851-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mention indicators have frequently been used in Webometric studies because
they provide a powerful tool for determining the degree of visibility and
impact of web resources. Among mention indicators, hypertextual links were a
central part of many studies until Yahoo discontinued the linkdomain command in
2011. Selective links constitute a variant of external links where both the
source and target of the link can be selected. This paper intends to study the
influence of social platforms (measured through the number of selective
external links) on academic environments, in order to ascertain both the
percentage that they constitute and whether some of them can be used as
substitutes of total external links. For this purpose, 141 URLs belonging to 76
Spanish universities were compiled in 2010 (before Yahoo! stopped their link
services), and the number of links from 13 selected social platforms to these
universities were calculated. Results confirm a good correlation between total
external links and links that come from social platforms, with the exception of
some applications (such as Digg and Technorati). For those universities with a
higher number of total external links, the high correlation is only maintained
on Delicious and Wikipedia, which can be utilized as substitutes of total
external links in the context analyzed. Notwithstanding, the global percentage
of links from social platforms constitute only a small fraction of total links,
although a positive trend is detected, especially in services such as Twitter,
Youtube, and Facebook.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5816</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5816</id><created>2013-10-22</created><authors><author><keyname>Ordu&#xf1;a-Malea</keyname><forenames>Enrique</forenames></author><author><keyname>Ontalba-Ruip&#xe9;rez</keyname><forenames>Jos&#xe9;-Antonio</forenames></author></authors><title>Proposal for a multilevel university cybermetric analysis model</title><categories>cs.DL</categories><journal-ref>Scientometrics, v. 95(3), pp.863-884</journal-ref><doi>10.1007/s11192-012-0868-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  University online seats have gradually become complex systems of dynamic
information where all their institutions and services are linked and
potentially accessible. These online seats now constitute a central node around
which universities construct and document their main activities and services.
This information can be quantitative measured by cybermetric techniques in
order to design university web rankings, taking the university as a global
reference unit. However, previous research into web subunits shows that it is
possible to carry out systemic web analyses, which open up the possibility of
carrying out studies which address university diversity, necessary for both
describing the university in greater detail and for establishing comparable
ranking units. To address this issue, a multilevel university cybermetric
analysis model is proposed, based on parts (core and satellite), levels
(institutional and external) and sublevels (contour and internal), providing a
deeper analysis of institutions. Finally the model is integrated into another
which is independent of the technique used, and applied by analysing Harvard
University as an example of use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5828</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5828</id><created>2013-10-22</created><updated>2014-05-05</updated><authors><author><keyname>Gregoire</keyname><forenames>Jean</forenames></author><author><keyname>Bonnabel</keyname><forenames>Silv&#xe8;re</forenames></author><author><keyname>de La Fortelle</keyname><forenames>Arnaud</forenames></author></authors><title>Priority-based intersection management with kinodynamic constraints</title><categories>cs.RO</categories><comments>to be presented at ECC2014; 6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of coordinating a collection of robots at an
intersection area taking into account dynamical constraints due to actuator
limitations. We adopt the coordination space approach, which is standard in
multiple robot motion planning. Assuming the priorities between robots are
assigned in advance and the existence of a collision-free trajectory respecting
those priorities, we propose a provably safe trajectory planner satisfying
kinodynamic constraints. The algorithm is shown to run in real time and to
return safe (collision-free) trajectories. Simulation results on synthetic data
illustrate the benefits of the approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5839</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5839</id><created>2013-10-22</created><authors><author><keyname>Brayford</keyname><forenames>David</forenames></author><author><keyname>Allalen</keyname><forenames>Momme</forenames></author><author><keyname>Weinberg</keyname><forenames>Volker</forenames></author></authors><title>Extreme Scaling of Lattice Quantum Chromodynamics</title><categories>cs.DC cs.PF hep-lat physics.comp-ph</categories><comments>5 pages, 2 figures, talk given at the &quot;Extreme Scaling on SuperMUC&quot;
  Minisymposium during ParCo 2013, International Conference on Parallel
  Computing, 10-13 September 2013, Munich</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the complexity and size of challenges in science and engineering are
continually increasing, it is highly important that applications are able to
scale strongly to very large numbers of cores (&gt;100,000 cores) to enable HPC
systems to be utilised efficiently. This paper presents results of strong
scaling tests performed with an MPI only and a hybrid MPI + OpenMP version of
the Lattice QCD application BQCD on the European Tier-0 system SuperMUC at LRZ.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5841</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5841</id><created>2013-10-22</created><authors><author><keyname>Mouhni</keyname><forenames>Naoual</forenames></author><author><keyname>Elkalay</keyname><forenames>Abderrafiaa</forenames></author></authors><title>Ontology based data warehouses federation management system</title><categories>cs.DB</categories><comments>6 pages</comments><journal-ref>IJCSI Intenational Journal of Computer Science Issues, Vol. 10,
  Issue 4, No 1, July 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data warehouses are nowadays an important component in every competitive
system, it's one of the main components on which business intelligence is
based. We can even say that many companies are climbing to the next level and
use a set of Data warehouses to provide the complete information or it's
generally due to fusion of two or many companies. these Data warehouses can be
heterogeneous and geographically separated, this structure is what we call
federation, and even if the components are physically separated, they are
logically seen as a single component. generally, these items are heterogeneous
which make it difficult to create the logical federation schema,and the
execution of user queries a complicated mission. In this paper, we will fill
this gap by proposing an extension of an existent algorithm in order to treat
different schema types (star, snow flack) including the treatment of
hierarchies dimension using ontology
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5842</identifier>
 <datestamp>2013-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5842</id><created>2013-10-22</created><updated>2013-12-20</updated><authors><author><keyname>Fang</keyname><forenames>Jianbin</forenames></author><author><keyname>Varbanescu</keyname><forenames>Ana Lucia</forenames></author><author><keyname>Sips</keyname><forenames>Henk</forenames></author><author><keyname>Zhang</keyname><forenames>Lilun</forenames></author><author><keyname>Che</keyname><forenames>Yonggang</forenames></author><author><keyname>Xu</keyname><forenames>Chuanfu</forenames></author></authors><title>An Empirical Study of Intel Xeon Phi</title><categories>cs.DC cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With at least 50 cores, Intel Xeon Phi is a true many-core architecture.
Featuring fairly powerful cores, two cache levels, and very fast
interconnections, the Xeon Phi can get a theoretical peak of 1000 GFLOPs and
over 240 GB/s. These numbers, as well as its flexibility - it can be used both
as a coprocessor or as a stand-alone processor - are very tempting for parallel
applications looking for new performance records.
  In this paper, we present an empirical study of Xeon Phi, stressing its
performance limits and relevant performance factors, ultimately aiming to
present a simplified view of the machine for regular programmers in search for
performance.
  To do so, we have micro-benchmarked the main hardware components of the
processor - the cores, the memory hierarchies, the ring interconnect, and the
PCIe connection. We show that, in ideal microbenchmarking conditions, the
performance that can be achieved is very close to the theoretical peak, as
given in the official programmer's guide. We have also identified and
quantified several causes for significant performance penalties. Our findings
have been captured in four optimization guidelines, and used to build a
simplified programmer's view of Xeon Phi, eventually enable the design and
prototyping of applications on a functionality-based model of the architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5848</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5848</id><created>2013-10-22</created><authors><author><keyname>Shafi</keyname><forenames>Naveed Anjum Imran</forenames></author><author><keyname>Abidi</keyname><forenames>Sohail</forenames></author></authors><title>Evaluation and Performance of Reactive Protocols Using Mobility Model</title><categories>cs.NI</categories><comments>10 pages, 14 figures, http://www.IJCSI.org</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 10,
  Issue 3, No 1, May 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Mobile Ad-hoc Network (MANET) is a self-motivated wireless network which
has no centralized point. It is an independent network that is connected by
wireless link so, in which every point or device work as a router. In this
network every node forward the packets to the destination as a router and it's
not operating as an ending point. In this network every node adjusts them self
by on his way in any direction because they are independent and change their
position regularly. There are exist three main types of routing protocols which
are reactive, proactive and final is hybrid protocols. This whole work compares
the performance of some reactive protocols which also known as on - demand
protocols, which are DSR, AODV and the final is AOMDV. DSR and AODV are
reactive protocols which connected the devices on the network when needed by a
doorway. The AOMDV protocol was designed for ad hoc networks whenever any route
or link fail and also maintain routes with sequence numbers to avoid looping.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5850</identifier>
 <datestamp>2013-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5850</id><created>2013-10-22</created><authors><author><keyname>Villan</keyname><forenames>Angel Gonzalez</forenames></author><author><keyname>Jorba</keyname><forenames>Josep</forenames></author></authors><title>Remote Control of Mobile Devices in Android Platform</title><categories>cs.HC cs.NI</categories><acm-class>I.3.2; K.6.5; C.2.1; C.1.4; C.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Remote control systems are a very useful element to control and monitor
devices quickly and easily. This paper proposes a new architecture for remote
control of Android mobile devices, analyzing the different alternatives and
seeking the optimal solution in each case. Although the area of remote control,
in case of mobile devices, is little explored, it may provide important
advantages for testing software and hardware developments in several real
devices. It can also allow an efficient management of various devices of
different types for performing different tasks, related for example to security
or forensic tasks.
  The main idea behind the proposed architecture was the design of a system to
use it as a platform which provides the services needed to perform remote
control of mobile devices. As a result of this research, a proof of concept was
implemented. An Android application running a group of server programs on the
device, connected to the network or USB interface, depending on availability.
This servers can be controlled through a small client written in Java and
runnable both on desktop and web systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5884</identifier>
 <datestamp>2013-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5884</id><created>2013-10-22</created><updated>2013-12-05</updated><authors><author><keyname>Ferrer-i-Cancho</keyname><forenames>Ramon</forenames></author></authors><title>The optimality of attaching unlinked labels to unlinked meanings</title><categories>cs.CL physics.data-an physics.soc-ph</categories><comments>New mathematical appendix, discussion improved, minor corrections,
  new references added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vocabulary learning by children can be characterized by many biases. When
encountering a new word, children as well as adults, are biased towards
assuming that it means something totally different from the words that they
already know. To the best of our knowledge, the 1st mathematical proof of the
optimality of this bias is presented here. First, it is shown that this bias is
a particular case of the maximization of mutual information between words and
meanings. Second, the optimality is proven within a more general information
theoretic framework where mutual information maximization competes with other
information theoretic principles. The bias is a prediction from modern
information theory. The relationship between information theoretic principles
and the principles of contrast and mutual exclusivity is also shown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5892</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5892</id><created>2013-10-22</created><authors><author><keyname>Robinson-Garcia</keyname><forenames>Nicolas</forenames></author><author><keyname>Calero-Medina</keyname><forenames>Clara</forenames></author></authors><title>What do university rankings by fields rank? Exploring discrepancies
  between the organizational structure of universities and bibliometric
  classifications</title><categories>cs.DL</categories><comments>Paper published in Scientometrics</comments><doi>10.1007/s11192-013-1157-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  University rankings by fields are usually based on the research output of
universities. However, research managers and rankings consumers expect to see
in such fields a reflection of the structure of their own organizational
institution. In this study we address such misinterpretation by developing the
research profile of the organizational units of two Spanish universities:
University of Granada and Pompeu Fabra University. We use two classification
systems, the subject categories offered by Thomson Scientific which are
commonly used on bibliometric studies, and the 37 disciplines displayed by the
Spanish I-UGR Rankings which are constructed from an aggregation of the former.
We also describe in detail problems encountered when working with address data
from a top down approach and we show differences between universities
structures derived from the interdisciplinary organizational forms of new
managerialism at universities. We conclude by highlighting that rankings by
fields should clearly state the methodology for the construction of such
fields. We indicate that the construction of research profiles may be a good
solution for universities for finding out levels of discrepancy between
organizational units and subject fields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5895</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5895</id><created>2013-10-22</created><authors><author><keyname>Walk</keyname><forenames>Philipp</forenames></author><author><keyname>Jung</keyname><forenames>Peter</forenames></author></authors><title>Stable Recovery from the Magnitude of Symmetrized Fourier Measurements</title><categories>cs.IT math.IT</categories><comments>4 pages, will be submitted to ICASSP14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note we show that stable recovery of complex-valued signals
$x\in\mathbb{C}^n$ up to global sign can be achieved from the magnitudes of
$4n-1$ Fourier measurements when a certain &quot;symmetrization and zero-padding&quot; is
performed before measurement ($4n-3$ is possible in certain cases). For real
signals, symmetrization itself is linear and therefore our result is in this
case a statement on uniform phase retrieval. Since complex conjugation is
involved, such measurement procedure is not complex-linear but recovery is
still possible from magnitudes of linear measurements on, for example,
$(\Re(x),\Im(x))$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5896</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5896</id><created>2013-10-22</created><authors><author><keyname>MIshra</keyname><forenames>Dheerendra</forenames></author></authors><title>The Cryptanalysis of Lee's Chaotic Maps-Based Authentication and Key
  Agreement Scheme using Smart card for Telecare Medicine Information Systems</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Telecare medicine information system (TMIS) is developed to provide
Telecare services to the remote user. A user can access remote medical servers
using internet without moving from his place. Although remote user and server
exchange their messages/data via public networks. An adversary is considered to
be enough powerful that he may have full control over the public network. This
makes these Telecare services vulnerable to attacks. To ensure secure
communication between the user and server many password based authentication
schemes have been proposed. In 2013, Hao et al. presented chaotic maps-based
password authentication scheme for TMIS. Recently, Lee identified that Hao et
al.'s scheme fails to satisfy key agreement property, such that a malicious
server can predetermine the session key. Lee also presented an efficient
chaotic map-based password authentication and key agreement scheme using Smart
cards for TMIS. In this article, we briefly review Lee's scheme and
demonstrates the weakness of Lee's scheme. The study shows that the Lee's
scheme inefficiency of password change phase causes denial of service attack
and login phase results extra computation and communication overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5902</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5902</id><created>2013-10-22</created><authors><author><keyname>Rafat</keyname><forenames>Khan Farhan</forenames></author><author><keyname>Sher</keyname><forenames>M.</forenames></author></authors><title>On The Limits Of Perfect Security For Steganographic System</title><categories>cs.CR</categories><msc-class>68Q25</msc-class><journal-ref>International Journal of Computer Science Issues (IJCSI), Vol. 10,
  Issue 4, No 1, July 2013 ISSN (Print): 1694-0814 | ISSN (Online): 1694-0784</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Until now the discussion on perfect security for steganographic systems has
remained confined within the realm of mathematicians and information theory
experts whose concise and symbolic representation of their philosophies,
postulates, and inference thereafter has made it hard for the na\&quot;ive academics
to have an insight of the concepts. This paper is an endeavor not only to
appraise on the limitations of one of such pioneer comprehensions but also to
illustrate a pitfall in another scheme that asserts on having perfect security
without the use of public or secret key. Goals set are accomplished through
contrasting test results of a steganographic scheme that exploits English words
with corresponding acronyms for hiding bits of secret information in chat - a
preferred way to exchange messages these days. The misapprehension about
perfect security and reign in characteristic of stego key in bit embedding
process are unfolded respectively by launching elementary chosen-message and
chosen-cover attack, and through proposed enhancement of target scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5916</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5916</id><created>2013-10-22</created><authors><author><keyname>Bianchi</keyname><forenames>Matteo</forenames></author></authors><title>A temporal semantics for Nilpotent Minimum logic</title><categories>math.LO cs.LO</categories><comments>19 pages, 2 tables</comments><msc-class>03B50</msc-class><doi>10.1016/j.ijar.2013.10.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In [Ban97] a connection among rough sets (in particular, pre-rough algebras)
and three-valued {\L}ukasiewicz logic {\L}3 is pointed out. In this paper we
present a temporal like semantics for Nilpotent Minimum logic NM ([Fod95,
EG01]), in which the logic of every instant is given by {\L}3: a completeness
theorem will be shown. This is the prosecution of the work initiated in [AGM08]
and [ABM09], in which the authors construct a temporal semantics for the
many-valued logics of G\&quot;odel ([G\&quot;od32], [Dum59]) and Basic Logic ([H\'aj98]).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5930</identifier>
 <datestamp>2015-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5930</id><created>2013-10-22</created><updated>2014-07-07</updated><authors><author><keyname>Noel</keyname><forenames>Adam</forenames></author><author><keyname>Cheung</keyname><forenames>Karen C.</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>A Unifying Model for External Noise Sources and ISI in Diffusive
  Molecular Communication</title><categories>cs.IT math.IT</categories><comments>14 pages, 7 figures, 4 tables, 1 appendix. To appear in IEEE Journal
  on Selected Areas in Communications (JSAC). Submitted October 21, 2013,
  revised April 21, 2014, accepted June 3, 2014</comments><doi>10.1109/JSAC.2014.2367693</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the impact of external noise sources, including
interfering transmitters, on a diffusive molecular communication system, where
the impact is measured as the number of noise molecules expected to be observed
at a passive receiver. A unifying model for noise, multiuser interference, and
intersymbol interference is presented, where, under certain circumstances,
interference can be approximated as a noise source that is emitting
continuously. The model includes the presence of advection and molecule
degradation. The time-varying and asymptotic impact is derived for a series of
special cases, some of which facilitate closed-form solutions. Simulation
results show the accuracy of the expressions derived for the impact of a
continuously-emitting noise source, and show how approximating intersymbol
interference as a noise source can simplify the calculation of the expected bit
error probability of a weighted sum detector.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5949</identifier>
 <datestamp>2013-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5949</id><created>2013-10-22</created><updated>2013-11-04</updated><authors><author><keyname>Najaflou</keyname><forenames>Yashar</forenames></author><author><keyname>Jedari</keyname><forenames>Behrouz</forenames></author><author><keyname>Xia</keyname><forenames>Feng</forenames></author><author><keyname>Yang</keyname><forenames>Laurence T.</forenames></author><author><keyname>Obaidat</keyname><forenames>Mohammad S.</forenames></author></authors><title>Safety Challenges and Solutions in Mobile Social Networks</title><categories>cs.NI cs.CR</categories><comments>accepted, 21 pages, 13 figures, 3 tables. IEEE System Journal, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile social networks (MSNs) are specific types of social media which
consolidate the ability of omnipresent connection for mobile users/devices to
share user-centric data objects among interested users. Taking advantage of the
characteristics of both social networks and opportunistic networks, MSNs are
capable of providing an efficient and effective mobile environment for users to
access, share, and distribute data. However, lack of a protective
infrastructure in these networks has turned them in to convenient targets for
various perils. This is the main impulse why MSNs carry disparate and intricate
safety concerns and embrace divergent safety challenging problems. In this
paper, we aim to provide a clear categorization on safety challenges and a deep
exploration over some recent solutions in MSNs. This work narrows the safety
challenges and solution techniques down from opportunistic networks (OppNets)
and delay tolerant networks (DTNs) to MSNs with the hope of covering all the
work proposed around security, privacy and trust in MSNs. To conclude, several
major open research issues are discussed and future research directions are
outlined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5957</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5957</id><created>2013-10-22</created><authors><author><keyname>Mat&#xfa;&#x161;</keyname><forenames>Franti&#x161;ek</forenames></author><author><keyname>Csirmaz</keyname><forenames>L&#xe1;szlo</forenames></author></authors><title>Entropy region and convolution</title><categories>cs.IT math.IT math.PR</categories><msc-class>94A17, 68Q30, 94A15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The entropy region is constructed from vectors of random variables by
collecting Shannon entropies of all subvectors. Its shape is studied here by
means of polymatroidal constructions, notably by convolution. The closure of
the region is decomposed into the direct sum of tight and modular parts,
reducing the study to the tight part. The relative interior of the reduction
belongs to the entropy region. Behavior of the decomposition under
selfadhesivity is clarified. Results are specialized to and completed for the
region of four random variables. This and computer experiments help to
visualize approximations of a symmetrized part of the entropy region. Four-atom
conjecture on the minimization of Ingleton score is refuted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5960</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5960</id><created>2013-10-22</created><authors><author><keyname>Hu</keyname><forenames>Yuan</forenames></author></authors><title>Optimizing Device-to-Device Communications in Cellular Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this report, we consider how to dynamically select transmission bands and
multi-hop routes for device-to-device (D2D) communications in co-existence with
a cellular network overlay. Firstly, we consider different wireless routing
algorithms, i.e. broadcasting-routing (BR) method, and shortest-path-routing
(SPR) method. The results show that depending on the co-existence cellular
users' outage constraint, different routing strategies have different merits.
BR is acceptable at the low D2D user density but is terrible at high density.
We also consider the channel band performance (Uplink band and Downlink band).
The results show that the multi-hop D2D can achieve a low outage probability
using the uplink band (approximately 5%), and D2D in the downlink band performs
a little poorly (approximately 12%) outage with SPR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5962</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5962</id><created>2013-10-22</created><authors><author><keyname>Aftab</keyname><forenames>Muhammad Umar</forenames></author><author><keyname>Nisar</keyname><forenames>Amna</forenames></author><author><keyname>Asif</keyname><forenames>Dr.</forenames></author><author><keyname>Ashraf</keyname><forenames>Adeel</forenames></author><author><keyname>Gill</keyname><forenames>Burhan</forenames></author></authors><title>RBAC Architecture Design Issues in Institutions Collaborative
  Environment</title><categories>cs.SE</categories><comments>8 pages, 3 figures, 11 References</comments><journal-ref>International Journal of Computer Science Issues, Volume 10, Issue
  4, July 2013</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Institutional collaborative systems focus on providing the fast, and secure
connections to students, teaching and non-teaching staff members. Access
control is more important in these types of systems because different kind of
users access the system on different levels. So a proper architecture must be
there for these kinds of systems, for providing an efficient and secure system.
As lot of work was done in RBAC like for grouping, securing the system, ease of
use, and for enterprise etc but no one apply all these concepts as a whole on
institution level. So, this paper will be a step towards administrative load
sharing, securing the system, and ease of use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5963</identifier>
 <datestamp>2013-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5963</id><created>2013-10-22</created><authors><author><keyname>Kiamarzpour</keyname><forenames>Foruzan</forenames></author><author><keyname>Dianat</keyname><forenames>Rouhollah</forenames></author><author><keyname>bahrani</keyname><forenames>Mohammad</forenames></author><author><keyname>Sadeghzadeh</keyname><forenames>Mehdi</forenames></author></authors><title>Improving the methods of email classification based on words ontology</title><categories>cs.IR cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Internet has dramatically changed the relationship among people and their
relationships with others people and made the valuable information available
for the users. Email is the service, which the Internet provides today for its
own users; this service has attracted most of the users' attention due to the
low cost. Along with the numerous benefits of Email, one of the weaknesses of
this service is that the number of received emails is continually being
enhanced, thus the ways are needed to automatically filter these disturbing
letters. Most of these filters utilize a combination of several techniques such
as the Black or white List, using the keywords and so on in order to identify
the spam more accurately In this paper, we introduce a new method to classify
the spam. We are seeking to increase the accuracy of Email classification by
combining the output of several decision trees and the concept of ontology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5965</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5965</id><created>2013-10-22</created><authors><author><keyname>Rajabi</keyname><forenames>Roozbeh</forenames></author><author><keyname>Ghassemian</keyname><forenames>Hassan</forenames></author></authors><title>Fusion of Hyperspectral and Panchromatic Images using Spectral Uumixing
  Results</title><categories>cs.CV</categories><comments>4 pages, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hyperspectral imaging, due to providing high spectral resolution images, is
one of the most important tools in the remote sensing field. Because of
technological restrictions hyperspectral sensors has a limited spatial
resolution. On the other hand panchromatic image has a better spatial
resolution. Combining this information together can provide a better
understanding of the target scene. Spectral unmixing of mixed pixels in
hyperspectral images results in spectral signature and abundance fractions of
endmembers but gives no information about their location in a mixed pixel. In
this paper we have used spectral unmixing results of hyperspectral images and
segmentation results of panchromatic image for data fusion. The proposed method
has been applied on simulated data using AVRIS Indian Pines datasets. Results
show that this method can effectively combine information in hyperspectral and
panchromatic images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5980</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5980</id><created>2013-10-22</created><authors><author><keyname>Grassi</keyname><forenames>Giulio</forenames></author><author><keyname>Pesavento</keyname><forenames>Davide</forenames></author><author><keyname>Wang</keyname><forenames>Lucas</forenames></author><author><keyname>Pau</keyname><forenames>Giovanni</forenames></author><author><keyname>Vuyyuru</keyname><forenames>Rama</forenames></author><author><keyname>Wakikawa</keyname><forenames>Ryuji</forenames></author><author><keyname>Zhang</keyname><forenames>Lixia</forenames></author></authors><title>Vehicular Inter-Networking via Named Data</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we apply the Named Data Networking, a newly proposed Internet
architecture, to networking vehicles on the run. Our initial design, dubbed
V-NDN, illustrates NDN's promising potential in providing a unifying
architecture that enables networking among all computing devices independent
from whether they are connected through wired infrastructure, ad hoc, or
intermittent DTN. This paper describes the prototype implementation of V-NDN
and its preliminary performance assessment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5984</identifier>
 <datestamp>2014-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5984</id><created>2013-10-22</created><updated>2014-10-22</updated><authors><author><keyname>Kozik</keyname><forenames>Jakub</forenames></author></authors><title>Multipass greedy coloring of simple uniform hypergraphs</title><categories>math.CO cs.DM</categories><msc-class>05C15, 05C65, 05D40</msc-class><acm-class>G.2.1; G.2.2; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $m^*(n)$ be the minimum number of edges in an $n$-uniform simple
hypergraph that is not two colorable. We prove that
$m^*(n)=\Omega(4^n/\ln^2(n))$. Our result generalizes to $r$-coloring of
$b$-simple uniform hypergraphs. For fixed $r$ and $b$ we prove that a maximum
vertex degree in $b$-simple $n$-uniform hypergraph that is not $r$-colorable
must be $\Omega(r^n /\ln(n))$. By trimming arguments it implies that every such
graph has $\Omega((r^n /\ln(n))^{b+1/b})$ edges. For any fixed $r \geq 2$ our
techniques yield also a lower bound $\Omega(r^n/\ln(n))$ for van der Waerden
numbers $W(n,r)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5985</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5985</id><created>2013-10-22</created><authors><author><keyname>Gupta</keyname><forenames>Ruchir</forenames></author><author><keyname>Maali</keyname><forenames>Abhijeet C.</forenames></author><author><keyname>Singh</keyname><forenames>Yatindra Nath</forenames></author></authors><title>Adaptive Push-Then-Pull Gossip Algorithm for Scale-free Networks</title><categories>cs.NI cs.DC cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real life networks are generally modelled as scale free networks. Information
diffusion in such networks in decentralised environment is a difficult and
resource consuming affair. Gossip algorithms have come up as a good solution to
this problem. In this paper, we have proposed Adaptive First Push Then Pull
gossip algorithm. We show that algorithm works with minimum cost when the
transition round to switch from Adaptive Push to Adaptive Pull is close to
Round(log(N)). Furthermore, we compare our algorithm with Push, Pull and First
Push Then Pull and show that the proposed algorithm is the most cost efficient
in Scale Free networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5988</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5988</id><created>2013-10-22</created><updated>2013-11-11</updated><authors><author><keyname>Kelemen</keyname><forenames>Z&#xe1;dor D&#xe1;niel</forenames></author><author><keyname>Kusters</keyname><forenames>Rob</forenames></author><author><keyname>Trienekens</keyname><forenames>Jos</forenames></author><author><keyname>Balla</keyname><forenames>Katalin</forenames></author></authors><title>Towards Applying Text Mining Techniques on Software Quality Standards
  and Models</title><categories>cs.SE</categories><comments>This paper has been withdrawn by the author due to a crucial
  misspelling</comments><report-no>TR_201302</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many of quality approaches are described in hundreds of textual pages. Manual
processing of information consumes plenty of resources. In this report we
present a text mining approach applied on CMMI, one well known and widely known
quality approach. The text mining analysis can provide a quick overview on the
scope of a quality approaches. The result of the analysis could accelerate the
understanding and the selection of quality approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.5999</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.5999</id><created>2013-10-22</created><authors><author><keyname>Abbadi</keyname><forenames>Nidhal Khdhair El</forenames></author><author><keyname>Saadi</keyname><forenames>Enas Hamood Al</forenames></author></authors><title>Improvement of Automatic Hemorrhages Detection Methods Using Shapes
  Recognition</title><categories>cs.CV</categories><comments>6-pages,3 figures</comments><journal-ref>Journal of Computer Science 9 (9): 1205-1210, 2013</journal-ref><doi>10.3844/jcssp.2013.1205.1210</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Diabetic Retinopathy is a medical condition where the retina is damaged
because fluid leaks from blood vessels into the retina. The presence of
hemorrhages in the retina is the earliest symptom of diabetic retinopathy. The
number and shape of hemorrhages is used to indicate the severity of the
disease. Early automated hemorrhage detection can help reduce the incidence of
blindness. This paper introduced new method depending on the hemorrhage shape
to detect the dot hemorrhage (DH), its number, and size at early stage, this
can be achieved by reducing the retinal image details. Detection and recognize
the DH by following three sequential steps, removing the fovea, removing the
vasculature and recognize DH by determining the circularity for all the objects
in the image, finally determine the shape factor which is related to DH
recognition, this stage strengthens the recognition process. The proposed
method recognizes and separates all the DH.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6007</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6007</id><created>2013-10-22</created><updated>2013-11-11</updated><authors><author><keyname>Cao</keyname><forenames>Yanshuai</forenames></author><author><keyname>Brubaker</keyname><forenames>Marcus A.</forenames></author><author><keyname>Fleet</keyname><forenames>David J.</forenames></author><author><keyname>Hertzmann</keyname><forenames>Aaron</forenames></author></authors><title>Efficient Optimization for Sparse Gaussian Process Regression</title><categories>cs.LG</categories><comments>To appear in NIPS 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an efficient optimization algorithm for selecting a subset of
training data to induce sparsity for Gaussian process regression. The algorithm
estimates an inducing set and the hyperparameters using a single objective,
either the marginal likelihood or a variational free energy. The space and time
complexity are linear in training set size, and the algorithm can be applied to
large regression problems on discrete or continuous domains. Empirical
evaluation shows state-of-art performance in discrete cases and competitive
results in the continuous case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6008</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6008</id><created>2013-10-22</created><authors><author><keyname>Cameron</keyname><forenames>Peter J.</forenames></author><author><keyname>Fairbairn</keyname><forenames>Ben</forenames></author><author><keyname>Gadouleau</keyname><forenames>Maximilien</forenames></author></authors><title>Computing in permutation groups without memory</title><categories>cs.CC cs.DM math.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Memoryless computation is a new technique to compute any function of a set of
registers by updating one register at a time while using no memory. Its aim is
to emulate how computations are performed in modern cores, since they typically
involve updates of single registers. The memoryless computation model can be
fully expressed in terms of transformation semigroups, or in the case of
bijective functions, permutation groups. In this paper, we consider how
efficiently permutations can be computed without memory. We determine the
minimum number of basic updates required to compute any permutation, or any
even permutation. The small number of required instructions shows that very
small instruction sets could be encoded on cores to perform memoryless
computation. We then start looking at a possible compromise between the size of
the instruction set and the length of the resulting programs. We consider
updates only involving a limited number of registers. In particular, we show
that binary instructions are not enough to compute all permutations without
memory when the alphabet size is even. These results, though expressed as
properties of special generating sets of the symmetric or alternating groups,
provide guidelines on the implementation of memoryless computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6009</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6009</id><created>2013-10-22</created><authors><author><keyname>Cameron</keyname><forenames>Peter J.</forenames></author><author><keyname>Fairbairn</keyname><forenames>Ben</forenames></author><author><keyname>Gadouleau</keyname><forenames>Maximilien</forenames></author></authors><title>Computing in matrix groups without memory</title><categories>cs.CC cs.DM math.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Memoryless computation is a novel means of computing any function of a set of
registers by updating one register at a time while using no memory. We aim to
emulate how computations are performed on modern cores, since they typically
involve updates of single registers. The computation model of memoryless
computation can be fully expressed in terms of transformation semigroups, or in
the case of bijective functions, permutation groups. In this paper, we view
registers as elements of a finite field and we compute linear permutations
without memory. We first determine the maximum complexity of a linear function
when only linear instructions are allowed. We also determine which linear
functions are hardest to compute when the field in question is the binary field
and the number of registers is even. Secondly, we investigate some matrix
groups, thus showing that the special linear group is internally computable but
not fast. Thirdly, we determine the smallest set of instructions required to
generate the special and general linear groups. These results are important for
memoryless computation, for they show that linear functions can be computed
very fast or that very few instructions are needed to compute any linear
function. They thus indicate new advantages of using memoryless computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6011</identifier>
 <datestamp>2014-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6011</id><created>2013-10-22</created><updated>2014-05-30</updated><authors><author><keyname>Dragotti</keyname><forenames>Pier Luigi</forenames></author><author><keyname>Lu</keyname><forenames>Yue M.</forenames></author></authors><title>On Sparse Representation in Fourier and Local Bases</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the classical problem of finding the sparse representation of a
signal in a pair of bases. When both bases are orthogonal, it is known that the
sparse representation is unique when the sparsity $K$ of the signal satisfies
$K&lt;1/\mu(D)$, where $\mu(D)$ is the mutual coherence of the dictionary.
Furthermore, the sparse representation can be obtained in polynomial time by
Basis Pursuit (BP), when $K&lt;0.91/\mu(D)$. Therefore, there is a gap between the
unicity condition and the one required to use the polynomial-complexity BP
formulation. For the case of general dictionaries, it is also well known that
finding the sparse representation under the only constraint of unicity is
NP-hard.
  In this paper, we introduce, for the case of Fourier and canonical bases, a
polynomial complexity algorithm that finds all the possible $K$-sparse
representations of a signal under the weaker condition that $K&lt;\sqrt{2}
/\mu(D)$. Consequently, when $K&lt;1/\mu(D)$, the proposed algorithm solves the
unique sparse representation problem for this structured dictionary in
polynomial time. We further show that the same method can be extended to many
other pairs of bases, one of which must have local atoms. Examples include the
union of Fourier and local Fourier bases, the union of discrete cosine
transform and canonical bases, and the union of random Gaussian and canonical
bases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6012</identifier>
 <datestamp>2015-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6012</id><created>2013-10-22</created><updated>2015-11-24</updated><authors><author><keyname>Olson</keyname><forenames>Randal S.</forenames></author><author><keyname>Knoester</keyname><forenames>David B.</forenames></author><author><keyname>Adami</keyname><forenames>Christoph</forenames></author></authors><title>Evolution of swarming behavior is shaped by how predators attack</title><categories>q-bio.PE cs.NE</categories><comments>25 pages, 11 figures, 5 tables, including 2 Supplementary Figures.
  Version to appear in &quot;Artificial Life&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Animal grouping behaviors have been widely studied due to their implications
for understanding social intelligence, collective cognition, and potential
applications in engineering, artificial intelligence, and robotics. An
important biological aspect of these studies is discerning which selection
pressures favor the evolution of grouping behavior. In the past decade,
researchers have begun using evolutionary computation to study the evolutionary
effects of these selection pressures in predator-prey models. The selfish herd
hypothesis states that concentrated groups arise because prey selfishly attempt
to place their conspecifics between themselves and the predator, thus causing
an endless cycle of movement toward the center of the group. Using an
evolutionary model of a predator-prey system, we show that how predators attack
is critical to the evolution of the selfish herd. Following this discovery, we
show that density-dependent predation provides an abstraction of Hamilton's
original formulation of ``domains of danger.'' Finally, we verify that
density-dependent predation provides a sufficient selective advantage for prey
to evolve the selfish herd in response to predation by coevolving predators.
Thus, our work corroborates Hamilton's selfish herd hypothesis in a digital
evolutionary model, refines the assumptions of the selfish herd hypothesis, and
generalizes the domain of danger concept to density-dependent predation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6019</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6019</id><created>2013-10-21</created><authors><author><keyname>Fleck</keyname><forenames>Tobias</forenames></author><author><keyname>Kappes</keyname><forenames>Andrea</forenames></author><author><keyname>Wagner</keyname><forenames>Dorothea</forenames></author></authors><title>Graph Clustering with Surprise: Complexity and Exact Solutions</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clustering graphs based on a comparison of the number of links within
clusters and the expected value of this quantity in a random graph has gained a
lot of attention and popularity in the last decade. Recently, Aldecoa and Marin
proposed a related, but slightly different approach leading to the quality
measure surprise, and reported good behavior in the context of synthetic and
real world benchmarks. We show that the problem of finding a clustering with
optimum surprise is NP-hard. Moreover, a bicriterial view on the problem
permits to compute optimum solutions for small instances by solving a small
number of integer linear programs, and leads to a polynomial time algorithm on
trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6055</identifier>
 <datestamp>2013-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6055</id><created>2013-10-22</created><authors><author><keyname>Guenther</keyname><forenames>Michael</forenames></author><author><keyname>Sandu</keyname><forenames>Adrian</forenames></author></authors><title>Multirate generalized additive Runge Kutta methods</title><categories>cs.NA math.NA</categories><report-no>Computational Science Laboratory Technical Report CSL-TR-6/2013</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work constructs a new class of multirate schemes based on the recently
developed generalized additive Runge-Kutta (GARK) methods (Sandu and Guenther,
2013). Multirate schemes use different step sizes for different components and
for different partitions of the right-hand side based on the local activity
levels. We show that the new multirate GARK family includes many well-known
multirate schemes as special cases. The order conditions theory follows
directly from the GARK accuracy theory. Nonlinear stability and monotonicity
investigations show that these properties are inherited from the base schemes
provided that additional coupling conditions hold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6063</identifier>
 <datestamp>2013-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6063</id><created>2013-10-22</created><authors><author><keyname>Sarkar</keyname><forenames>Sayantan</forenames></author></authors><title>Word Spotting in Cursive Handwritten Documents using Modified Character
  Shape Codes</title><categories>cs.CV</categories><comments>10 Pages Advances In Computing And Information Technology:
  Proceedings Of The Second International Conference On Advances In Computing
  And Information Technology, July, 2012. ISBN13: 9783642315992</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  There is a large collection of Handwritten English paper documents of
Historical and Scientific importance. But paper documents are not recognized
directly by computer. Hence the closest way of indexing these documents is by
storing their document digital image. Hence a large database of document images
can replace the paper documents. But the document and data corresponding to
each image cannot be directly recognized by the computer.
  This paper applies the technique of word spotting using Modified Character
Shape Code to Handwritten English document images for quick and efficient query
search of words on a database of document images. It is different from other
Word Spotting techniques as it implements two level of selection for word
segments to match search query. First based on word size and then based on
character shape code of query. It makes the process faster and more efficient
and reduces the need of multiple pre-processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6066</identifier>
 <datestamp>2013-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6066</id><created>2013-10-22</created><authors><author><keyname>Sarkar</keyname><forenames>Sayantan</forenames></author></authors><title>Skin Segmentation based Elastic Bunch Graph Matching for efficient
  multiple Face Recognition</title><categories>cs.CV</categories><comments>10 Pages Advances in Computer Science, Engineering Applications, May,
  2012</comments><doi>10.1007/978-3-642-30157-5_4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is aimed at developing and combining different algorithms for face
detection and face recognition to generate an efficient mechanism that can
detect and recognize the facial regions of input image. For the detection of
face from complex region, skin segmentation isolates the face-like regions in a
complex image and following operations of morphology and template matching
rejects false matches to extract facial region. For the recognition of the
face, the image database is now converted into a database of facial segments.
Hence, implementing the technique of Elastic Bunch Graph matching (EBGM) after
skin segmentation generates Face Bunch Graphs that acutely represents the
features of an individual face enhances the quality of the training set. This
increases the matching probability significantly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6078</identifier>
 <datestamp>2013-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6078</id><created>2013-10-22</created><authors><author><keyname>Mayne</keyname><forenames>Richard</forenames></author><author><keyname>Patton</keyname><forenames>David</forenames></author><author><keyname>Costello</keyname><forenames>Ben de Lacy</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author><author><keyname>Patton</keyname><forenames>Rosemary Camilla</forenames></author></authors><title>On the internalisation, intraplasmodial carriage and excretion of
  metallic nanoparticles in the slime mould Physarum polycephalum</title><categories>cs.ET physics.bio-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The plasmodium of Physarum polycephalum is a large single cell visible with
the naked eye. When inoculated on a substrate with attractants and repellents
the plasmodium develops optimal networks of protoplasmic tubes which span sites
of attractants (i.e. nutrients) yet avoid domains with a high nutrient
concentration. It should therefore be possible to program the plasmodium
towards deterministic adaptive transformation of internalised nano- and
micro-scale materials. In laboratory experiments with magnetite nanoparticles
and glass micro-spheres coated with silver metal we demonstrate that the
plasmodium of P. polycephalum can propagate the nano-scale objects using a
number of distinct mechanisms including endocytosis, transcytosis and dragging.
The results of our experiments could be used in the development of novel
techniques targeted towards the growth of metallised biological wires and
hybrid nano- and micro-circuits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6084</identifier>
 <datestamp>2013-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6084</id><created>2013-10-22</created><authors><author><keyname>Hossain</keyname><forenames>Md. Iqbal</forenames></author><author><keyname>Rahman</keyname><forenames>Md. Saidur</forenames></author></authors><title>Monotone Grid Drawings of Planar Graphs</title><categories>cs.CG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A monotone drawing of a planar graph $G$ is a planar straight-line drawing of
$G$ where a monotone path exists between every pair of vertices of $G$ in some
direction. Recently monotone drawings of planar graphs have been proposed as a
new standard for visualizing graphs. A monotone drawing of a planar graph is a
monotone grid drawing if every vertex in the drawing is drawn on a grid point.
In this paper we study monotone grid drawings of planar graphs in a variable
embedding setting. We show that every connected planar graph of $n$ vertices
has a monotone grid drawing on a grid of size $O(n)\times O(n^2)$, and such a
drawing can be found in O(n) time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6092</identifier>
 <datestamp>2013-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6092</id><created>2013-10-22</created><authors><author><keyname>Bauer</keyname><forenames>Miriam H. A.</forenames></author><author><keyname>Barbieri</keyname><forenames>Sebastiano</forenames></author><author><keyname>Klein</keyname><forenames>Jan</forenames></author><author><keyname>Egger</keyname><forenames>Jan</forenames></author><author><keyname>Kuhnt</keyname><forenames>Daniela</forenames></author><author><keyname>Freisleben</keyname><forenames>Bernd</forenames></author><author><keyname>Hahn</keyname><forenames>Horst K.</forenames></author><author><keyname>Nimsky</keyname><forenames>Christopher</forenames></author></authors><title>A Ray-based Approach for Boundary Estimation of Fiber Bundles Derived
  from Diffusion Tensor Imaging</title><categories>cs.CV</categories><comments>5 pages, 2 figures, 7 references</comments><journal-ref>Int J CARS, Vol. 5, Suppl. 1, pp. 47-48, Springer Press, June 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Diffusion Tensor Imaging (DTI) is a non-invasive imaging technique that
allows estimation of the location of white matter tracts in-vivo, based on the
measurement of water diffusion properties. For each voxel, a second-order
tensor can be calculated by using diffusion-weighted sequences (DWI) that are
sensitive to the random motion of water molecules. Given at least 6
diffusion-weighted images with different gradients and one unweighted image,
the coefficients of the symmetric diffusion tensor matrix can be calculated.
Deriving the eigensystem of the tensor, the eigenvectors and eigenvalues can be
calculated to describe the three main directions of diffusion and its
magnitude. Using DTI data, fiber bundles can be determined, to gain information
about eloquent brain structures. Especially in neurosurgery, information about
location and dimension of eloquent structures like the corticospinal tract or
the visual pathways is of major interest. Therefore, the fiber bundle boundary
has to be determined. In this paper, a novel ray-based approach for boundary
estimation of tubular structures is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6110</identifier>
 <datestamp>2013-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6110</id><created>2013-10-23</created><authors><author><keyname>Hara</keyname><forenames>Keisuke</forenames></author><author><keyname>Kamada</keyname><forenames>Tomihisa</forenames></author></authors><title>A two-step model and the algorithm for recalling in recommender systems</title><categories>cs.IR</categories><comments>6 pages, No figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When a user finds an interesting recommendation in a recommender system, the
user may want to recall related items recommended in the past to reconsider or
to enjoy them again. If the system can pick up such &quot;recalled&quot; items at each
user's request, it must deepen the user experience.
  We propose a model and the algorithm for such personalized &quot;recalling&quot; in
conventional recommender systems, which is an application of neural networks
for associative memory. In our model, the &quot;recalled&quot; items can reflect each
user's personality beyond naive similarities between items.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6119</identifier>
 <datestamp>2015-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6119</id><created>2013-10-23</created><updated>2015-01-15</updated><authors><author><keyname>Patsonakis</keyname><forenames>Christos</forenames></author><author><keyname>Roussopoulos</keyname><forenames>Mema</forenames></author></authors><title>Asynchronous Rumour Spreading in Social and Signed Topologies</title><categories>cs.SI physics.soc-ph</categories><comments>10 pages, 4 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an experimental analysis of the asynchronous push &amp;
pull rumour spreading protocol. This protocol is, to date, the best-performing
rumour spreading protocol for simple, scalable, and robust information
dissemination in distributed systems. We analyse the effect that multiple
parameters have on the protocol's performance, such as using memory to avoid
contacting the same neighbor twice in a row, varying the stopping criteria used
by nodes to decide when to stop spreading the rumour, employing more
sophisticated neighbor selection policies instead of the standard uniform
random choice, and others. Prior work has focused on either providing
theoretical upper bounds regarding the number of rounds needed to spread the
rumour to all nodes, or, proposes improvements by adjusting isolated
parameters. To our knowledge, our work is the first to study how multiple
parameters affect system behaviour both in isolation and combination and under
a wide range of values. Our analysis is based on experimental simulations using
real-world social network datasets, thus complementing prior theoretical work
to shed light on how the protocol behaves in practical, real-world systems. We
also study the behaviour of the protocol on a special type of social graph,
called signed networks (e.g., Slashdot and Epinions), whose links indicate
stronger trust relationships. Finally, through our detailed analysis, we
demonstrate how a few simple additions to the protocol can improve the total
time required to inform 100% of the nodes by a maximum of 99.69% and an average
of 82.37%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6129</identifier>
 <datestamp>2013-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6129</id><created>2013-10-23</created><authors><author><keyname>Pei</keyname><forenames>Tao</forenames></author><author><keyname>Sobolevsky</keyname><forenames>Stanislav</forenames></author><author><keyname>Ratti</keyname><forenames>Carlo</forenames></author><author><keyname>Shaw</keyname><forenames>Shih-Lung</forenames></author><author><keyname>Zhou</keyname><forenames>Chenghu</forenames></author></authors><title>A New Insight into Land Use Classification Based on Aggregated Mobile
  Phone Data</title><categories>cs.CY</categories><comments>35 pages, 7 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Land use classification is essential for urban planning. Urban land use types
can be differentiated either by their physical characteristics (such as
reflectivity and texture) or social functions. Remote sensing techniques have
been recognized as a vital method for urban land use classification because of
their ability to capture the physical characteristics of land use. Although
significant progress has been achieved in remote sensing methods designed for
urban land use classification, most techniques focus on physical
characteristics, whereas knowledge of social functions is not adequately used.
Owing to the wide usage of mobile phones, the activities of residents, which
can be retrieved from the mobile phone data, can be determined in order to
indicate the social function of land use. This could bring about the
opportunity to derive land use information from mobile phone data. To verify
the application of this new data source to urban land use classification, we
first construct a time series of aggregated mobile phone data to characterize
land use types. This time series is composed of two aspects: the hourly
relative pattern, and the total call volume. A semi-supervised fuzzy c-means
clustering approach is then applied to infer the land use types. The method is
validated using mobile phone data collected in Singapore. Land use is
determined with a detection rate of 58.03%. An analysis of the land use
classification results shows that the accuracy decreases as the heterogeneity
of land use increases, and increases as the density of cell phone towers
increases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6132</identifier>
 <datestamp>2013-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6132</id><created>2013-10-23</created><authors><author><keyname>Dar</keyname><forenames>Ronen</forenames></author><author><keyname>Feder</keyname><forenames>Meir</forenames></author><author><keyname>Mecozzi</keyname><forenames>Antonio</forenames></author><author><keyname>Shtaif</keyname><forenames>Mark</forenames></author></authors><title>Time varying ISI model for nonlinear interference noise</title><categories>physics.optics cs.IT math.IT</categories><comments>Submitted to the optical fiber communication conference (OFC), 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the effect of nonlinear interference in WDM systems is
equivalent to slowly varying inter-symbol-interference (ISI), and hence its
cancellation can be carried out by means of adaptive linear filtering. We
characterize the ISI coefficients and discuss the potential gain following from
their cancellation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6139</identifier>
 <datestamp>2013-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6139</id><created>2013-10-23</created><authors><author><keyname>Tedik</keyname><forenames>Semiha</forenames></author><author><keyname>Kurt</keyname><forenames>G&#xfc;ne&#x15f; Karabulut</forenames></author></authors><title>Practical Full Duplex Physical Layer Network Coding</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a practical network code for the wireless two-way relay channel
where all nodes communicate in full duplex (FD) mode. The physical layer
network coding (PNC) operation is applied with the FD operating nodes, reducing
the transmission time to a single time slot, hence doubling the spectral
efficiency when compared to classical PNC systems. In our system model, binary
phase shift keying modulated signals are transmitted over Rayleigh fading
channels. We derive the theoretical error rates at relay and end nodes
according to the maximum likelihood detection rule, in case of non-ideal
self-interference cancellation. Theoretical results are also verified via
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6162</identifier>
 <datestamp>2013-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6162</id><created>2013-10-23</created><authors><author><keyname>Orduna-Malea</keyname><forenames>Enrique</forenames></author><author><keyname>Lopez-Cozar</keyname><forenames>Emilio Delgado</forenames></author></authors><title>Google Scholar Metrics evolution: an analysis according to languages</title><categories>cs.DL</categories><comments>14 pages, 4 figures, 7 tables, This paper has been accepted for
  publication in the Scientometrics</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In November 2012 the Google Scholar Metrics (GSM) journal rankings were
updated, making it possible to compare bibliometric indicators in the 10
languages indexed and their stability with the April 2012 version. The h-index
and h 5 median of 1000 journals were analysed, comparing their averages,
maximum and minimum values and the correlation coefficient within rankings. The
bibliometric figures grew significantly. In just seven and a half months the h
index of the journals increased by 15% and the median h-index by 17%. This
growth was observed for all the bibliometric indicators analysed and for
practically every journal. However, we found significant differences in growth
rates depending on the language in which the journal is published. Moreover,
the journal rankings seem to be stable between April and November, reinforcing
the credibility of the data held by Google Scholar and the reliability of the
GSM journal rankings, despite the uncontrolled growth of Google Scholar. Based
on the findings of this study we suggest, firstly, that Google should upgrade
its rankings at least semiannually and, secondly, that the results should be
displayed in each ranking proportionally to the number of journals indexed by
language
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6171</identifier>
 <datestamp>2013-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6171</id><created>2013-10-23</created><authors><author><keyname>Siris</keyname><forenames>Vasilios A.</forenames></author><author><keyname>Anagnostopoulou</keyname><forenames>Maria</forenames></author><author><keyname>Dimopoulos</keyname><forenames>Dimitris</forenames></author></authors><title>Improving Mobile Video Streaming with Mobility Prediction and
  Prefetching in Integrated Cellular-WiFi Networks</title><categories>cs.NI cs.MM</categories><comments>7 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present and evaluate a procedure that utilizes mobility and throughput
prediction to prefetch video streaming data in integrated cellular and WiFi
networks. The effective integration of such heterogeneous wireless technologies
will be significant for supporting high performance and energy efficient video
streaming in ubiquitous networking environments. Our evaluation is based on
trace-driven simulation considering empirical measurements and shows how
various system parameters influence the performance, in terms of the number of
paused video frames and the energy consumption; these parameters include the
number of video streams, the mobile, WiFi, and ADSL backhaul throughput, and
the number of WiFi hotspots. Also, we assess the procedure's robustness to time
and throughput variability. Finally, we present our initial prototype that
implements the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6173</identifier>
 <datestamp>2013-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6173</id><created>2013-10-23</created><authors><author><keyname>Weaver</keyname><forenames>Carl</forenames></author><author><keyname>Monogioudis</keyname><forenames>Pantelis</forenames></author></authors><title>Self-Organizing Mobility Robustness Optimization in LTE Networks with
  eICIC</title><categories>cs.NI cs.PF cs.SY</categories><comments>18 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of Mobility Robustness Optimization (MRO) and describe
centralized Self Organizing Network (SON) solutions that can optimize
connected-mode mobility Key Performance Indicators (KPIs). Our solution extends
the earlier work of eICIC parameter optimization [7], to heterogeneous networks
with mobility, and outline methods of progressive complexity that optimize the
Retaining/Offloading Bias which are macro/pico views of Cell Individual Offset
parameters. Simulation results under real LTE network deployment assumptions of
a US metropolitan area demonstrate the effects of such solutions on the
mobility KPIs. To our knowledge, this solution is the first that demonstrates
the joint optimization of eICIC and MRO.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6176</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6176</id><created>2013-10-23</created><updated>2015-05-23</updated><authors><author><keyname>Bernardi</keyname><forenames>Giovanni</forenames></author><author><keyname>Hennessy</keyname><forenames>Matthew</forenames></author></authors><title>Using higher-order contracts to model session types</title><categories>cs.LO</categories><comments>Added definitions of m-closed terms, of 'dual', and a discussion to
  show the problems of the complement function</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Session types are used to describe and structure interactions between
independent processes in distributed systems. Higher-order types are needed in
order to properly structure delegation of responsibility between processes.
  In this paper we show that higher-order web-service contracts can be used to
provide a fully-abstract model of recursive higher-order session types. The
model is set-theoretic, in the sense that the meaning of a contract is given in
terms of the set of contracts with which it complies.
  The proof of full-abstraction depends on a novel notion of the complement of
a contract. This in turn gives rise to an alternative to the type duality
commonly used in systems for type-checking session types. We believe that the
notion of complement captures more faithfully the behavioural intuition
underlying type duality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6205</identifier>
 <datestamp>2015-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6205</id><created>2013-10-23</created><updated>2014-11-21</updated><authors><author><keyname>Thomass&#xe9;</keyname><forenames>St&#xe9;phan</forenames></author><author><keyname>Trotignon</keyname><forenames>Nicolas</forenames></author><author><keyname>Vuskovi&#x107;</keyname><forenames>Kristina</forenames></author></authors><title>Parameterized algorithm for weighted independent set problem in
  bull-free graphs</title><categories>cs.DM</categories><msc-class>05C85</msc-class><journal-ref>Parameterized algorithm for weighted independent set problem in
  bull-free graphs. Algorithmica, November 2015, 1--23</journal-ref><doi>10.1007/s00453-015-0083-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The maximum stable set problem is NP-hard, even when restricted to
triangle-free graphs. In particular, one cannot expect a polynomial time
algorithm deciding if a bull-free graph has a stable set of size $k$, when $k$
is part of the instance. Our main result in this paper is to show the existence
of an FPT algorithm when we parameterize the problem by the solution size $k$.
A polynomial kernel is unlikely to exist for this problem. We show however that
our problem has a polynomial size Turing-kernel. More precisely, the hard cases
are instances of size $O(k^5)$. As a byproduct, if we forbid odd holes in
addition to the bull, we show the existence of a polynomial time algorithm for
the stable set problem. We also prove that the chromatic number of a bull-free
graph is bounded by a function of its clique number and the maximum chromatic
number of its triangle-free induced subgraphs. All our results rely on a
decomposition theorem of bull-free graphs due to Chudnovsky which is modified
here, allowing us to provide extreme decompositions, adapted to our
computational purpose.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6238</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6238</id><created>2013-10-23</created><updated>2013-10-31</updated><authors><author><keyname>Childs</keyname><forenames>Andrew M.</forenames></author><author><keyname>Ivanyos</keyname><forenames>G&#xe1;bor</forenames></author></authors><title>Quantum computation of discrete logarithms in semigroups</title><categories>quant-ph cs.CR</categories><comments>8 pages, 1 figure</comments><journal-ref>Journal of Mathematical Cryptology 8, no. 4, 405-416 (2014)</journal-ref><doi>10.1515/jmc-2013-0038</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe an efficient quantum algorithm for computing discrete logarithms
in semigroups using Shor's algorithms for period finding and discrete log as
subroutines. Thus proposed cryptosystems based on the presumed hardness of
discrete logarithms in semigroups are insecure against quantum attacks. In
contrast, we show that some generalizations of the discrete log problem are
hard in semigroups despite being easy in groups. We relate a shifted version of
the discrete log problem in semigroups to the dihedral hidden subgroup problem,
and we show that the constructive membership problem with respect to $k \ge 2$
generators in a black-box abelian semigroup of order $N$ requires $\tilde
\Theta(N^{\frac{1}{2}-\frac{1}{2k}})$ quantum queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6257</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6257</id><created>2013-10-23</created><updated>2016-01-02</updated><authors><author><keyname>Gatterbauer</keyname><forenames>Wolfgang</forenames></author><author><keyname>Suciu</keyname><forenames>Dan</forenames></author></authors><title>Dissociation and Propagation for Efficient Query Evaluation over
  Probabilistic Databases</title><categories>cs.DB cs.AI</categories><comments>32 pages, 25 figures, full version of arXiv:1412.1069 [ PVLDB
  8(5):629-640, 2015: &quot;Approximate lifted inference with probabilistic
  databases&quot;, http://www.vldb.org/pvldb/vol8/p629-gatterbauer.pdf ]</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic inference over large data sets is a challenging data management
problem as exact inference is generally #P-hard and requires sampling-based
methods. This paper proposes an alternative approach for approximate evaluation
of queries with probabilistic databases: In our approach, every query is
evaluated entirely in the database engine by evaluating a fixed number of query
plans, each providing an upper bound on the true probability, then taking their
minimum. We provide an algorithm that takes into account important schema
information to enumerate only the minimal necessary plans among all possible
plans. Importantly, this algorithm is a strict generalization of all known
PTIME self-join-free conjunctive queries: A query is in PTIME if and only if
our algorithm returns one single plan. Furthermore, our approach is a
generalization of a family of efficient ranking functions from graphs to
hypergraphs. We also apply three relational query optimization techniques to
evaluate all minimal plans very fast. We give a detailed experimental
evaluation of our approach and, in the process, provide a new way of thinking
about the value of probabilistic methods over non-probabilistic methods for
ranking query answers. We also note that the techniques developed in this paper
apply immediately to lifted inference from statistical relational models since
lifted inference corresponds to PTIME plans in probabilistic databases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6259</identifier>
 <datestamp>2013-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6259</id><created>2013-10-16</created><authors><author><keyname>Raghava</keyname><forenames>N. S</forenames></author><author><keyname>De</keyname><forenames>Asok</forenames></author><author><keyname>Kataria</keyname><forenames>Nitish</forenames></author><author><keyname>Chatterjee</keyname><forenames>Sarthak</forenames></author></authors><title>Stacked Patch Antenna With Cross Slot Electronic Band Gap Structure</title><categories>cs.OH</categories><comments>4 pages, 4 figures, 1 table. Presented Research Paper in
  International Congress on Innovative Trends in Information Technology and
  Computing Sciences for Competitive World Order (ITITCSCWO - 2013) on 2nd and
  3rd of March 2013, at J.N.U, New Delhi. Research Paper published in
  International Journal of Information and Computation Technology(IJICT),
  Volume 3, Number 5, 2013. ISSN 0974-2239</comments><msc-class>94-06</msc-class><journal-ref>International Journal of Information and Computation Technology,
  Volume 3, Number 5,2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A cross slotted electronic band gap (EBG) with stacked rectangular patches
shorted with a shorting pin is proposed in this paper. The study is being done
on how the various parameters are varied by changing the probe feed location.
The design is constructed by using stacking of patches, shorting pin and cross
slotted EBG to form an optimized antenna design with antenna efficiency of
approximately 99.06%. The radiation patterns are given at 2.586 GHz which can
be used for wireless communications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6265</identifier>
 <datestamp>2013-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6265</id><created>2013-10-23</created><authors><author><keyname>Modenini</keyname><forenames>Andrea</forenames></author><author><keyname>Rusek</keyname><forenames>Fredrik</forenames></author><author><keyname>Colavolpe</keyname><forenames>Giulio</forenames></author></authors><title>Optimal Transmit Filters for ISI Channels under Channel Shortening
  Detection</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider channels affected by intersymbol interference with
reduced-complexity, mutual information optimized, channel-shortening detection.
For such settings, we optimize the transmit filter, taking into consideration
the reduced receiver complexity constraint. As figure of merit, we consider the
achievable information rate of the entire system and with functional analysis,
we establish a general form of the optimal transmit filter, which can then be
optimized by standard numerical methods. As a corollary to our main result, we
obtain some insight of the behavior of the standard waterfilling algorithm for
intersymbol interference channels. With only some minor changes, the general
form we derive can be applied to multiple-input multiple-output channels with
intersymbol interference. To illuminate the practical use of our results, we
provide applications of our theoretical results by deriving the optimal shaping
pulse of a linear modulation transmitted over a bandlimited additive white
Gaussian noise channel which has possible applications in the
faster-than-Nyquist/time packing technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6271</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6271</id><created>2013-10-23</created><updated>2013-12-22</updated><authors><author><keyname>Bundala</keyname><forenames>Daniel</forenames></author><author><keyname>Z&#xe1;vodn&#xfd;</keyname><forenames>Jakub</forenames></author></authors><title>Optimal Sorting Networks</title><categories>cs.DM cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper settles the optimality of sorting networks given in The Art of
Computer Programming vol. 3 more than 40 years ago. The book lists efficient
sorting networks with n &lt;= 16 inputs. In this paper we give general
combinatorial arguments showing that if a sorting network with a given depth
exists then there exists one with a special form. We then construct
propositional formulas whose satisfiability is necessary for the existence of
such a network. Using a SAT solver we conclude that the listed networks have
optimal depth. For n &lt;= 10 inputs where optimality was known previously, our
algorithm is four orders of magnitude faster than those in prior work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6283</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6283</id><created>2013-10-23</created><updated>2014-10-27</updated><authors><author><keyname>Henry</keyname><forenames>Christopher S.</forenames></author></authors><title>The (Nested) Word Problem</title><categories>cs.FL math.GR</categories><comments>1 figure</comments><msc-class>20F10, 20E05, 68Q45, 03D40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we provide a new perspective on the word problem of a group
by using languages of nested words. These were introduced by Alur and
Madhusudan as a way to model programming languages such as HTML. We demonstrate
how a class of nested word languages called visibly pushdown can be used to
study the word problem of virtually free groups in a natural way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6288</identifier>
 <datestamp>2014-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6288</id><created>2013-10-23</created><authors><author><keyname>Zhang</keyname><forenames>Hao</forenames></author><author><keyname>Zhang</keyname><forenames>Liqing</forenames></author></authors><title>Spatial-Spectral Boosting Analysis for Stroke Patients' Motor Imagery
  EEG in Rehabilitation Training</title><categories>stat.ML cs.AI cs.LG</categories><comments>10 pages,3 figures</comments><doi>10.3233/978-1-61499-419-0-537</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current studies about motor imagery based rehabilitation training systems for
stroke subjects lack an appropriate analytic method, which can achieve a
considerable classification accuracy, at the same time detects gradual changes
of imagery patterns during rehabilitation process and disinters potential
mechanisms about motor function recovery. In this study, we propose an adaptive
boosting algorithm based on the cortex plasticity and spectral band shifts.
This approach models the usually predetermined spatial-spectral configurations
in EEG study into variable preconditions, and introduces a new heuristic of
stochastic gradient boost for training base learners under these preconditions.
We compare our proposed algorithm with commonly used methods on datasets
collected from 2 months' clinical experiments. The simulation results
demonstrate the effectiveness of the method in detecting the variations of
stroke patients' EEG patterns. By chronologically reorganizing the weight
parameters of the learned additive model, we verify the spatial compensatory
mechanism on impaired cortex and detect the changes of accentuation bands in
spectral domain, which may contribute important prior knowledge for
rehabilitation practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6298</identifier>
 <datestamp>2013-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6298</id><created>2013-10-23</created><authors><author><keyname>Li</keyname><forenames>Ye</forenames></author><author><keyname>West</keyname><forenames>Richard</forenames></author><author><keyname>Missimer</keyname><forenames>Eric</forenames></author></authors><title>The Quest-V Separation Kernel for Mixed Criticality Systems</title><categories>cs.OS</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi- and many-core processors are becoming increasingly popular in embedded
systems. Many of these processors now feature hardware virtualization
capabilities, such as the ARM Cortex A15, and x86 processors with Intel VT-x or
AMD-V support. Hardware virtualization offers opportunities to partition
physical resources, including processor cores, memory and I/O devices amongst
guest virtual machines. Mixed criticality systems and services can then
co-exist on the same platform in separate virtual machines. However,
traditional virtual machine systems are too expensive because of the costs of
trapping into hypervisors to multiplex and manage machine physical resources on
behalf of separate guests. For example, hypervisors are needed to schedule
separate VMs on physical processor cores. In this paper, we discuss the design
of the Quest-V separation kernel, that partitions services of different
criticalities in separate virtual machines, or sandboxes. Each sandbox
encapsulates a subset of machine physical resources that it manages without
requiring intervention of a hypervisor. Moreover, a hypervisor is not needed
for normal operation, except to bootstrap the system and establish
communication channels between sandboxes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6299</identifier>
 <datestamp>2014-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6299</id><created>2013-10-23</created><updated>2014-01-03</updated><authors><author><keyname>Acar</keyname><forenames>Umut A.</forenames></author><author><keyname>Ahmed</keyname><forenames>Amal</forenames></author><author><keyname>Cheney</keyname><forenames>James</forenames></author><author><keyname>Perera</keyname><forenames>Roly</forenames></author></authors><title>A Core Calculus for Provenance</title><categories>cs.PL</categories><journal-ref>Journal of Computer Security 21 (2013) 919-969</journal-ref><doi>10.3233/JCS-130487</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Provenance is an increasing concern due to the ongoing revolution in sharing
and processing scientific data on the Web and in other computer systems. It is
proposed that many computer systems will need to become provenance-aware in
order to provide satisfactory accountability, reproducibility, and trust for
scientific or other high-value data. To date, there is not a consensus
concerning appropriate formal models or security properties for provenance. In
previous work, we introduced a formal framework for provenance security and
proposed formal definitions of properties called disclosure and obfuscation.
  In this article, we study refined notions of positive and negative disclosure
and obfuscation in a concrete setting, that of a general-purpose programing
language. Previous models of provenance have focused on special-purpose
languages such as workflows and database queries. We consider a higher-order,
functional language with sums, products, and recursive types and functions, and
equip it with a tracing semantics in which traces themselves can be replayed as
computations. We present an annotation-propagation framework that supports many
provenance views over traces, including standard forms of provenance studied
previously. We investigate some relationships among provenance views and
develop some partial solutions to the disclosure and obfuscation problems,
including correct algorithms for disclosure and positive obfuscation based on
trace slicing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6301</identifier>
 <datestamp>2013-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6301</id><created>2013-10-23</created><authors><author><keyname>Li</keyname><forenames>Ye</forenames></author><author><keyname>Missimer</keyname><forenames>Eric</forenames></author><author><keyname>West</keyname><forenames>Richard</forenames></author></authors><title>Predictable Migration and Communication in the Quest-V Multikernel</title><categories>cs.OS</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quest-V is a system we have been developing from the ground up, with
objectives focusing on safety, predictability and efficiency. It is designed to
work on emerging multicore processors with hardware virtualization support.
Quest-V is implemented as a &quot;distributed system on a chip&quot; and comprises
multiple sandbox kernels. Sandbox kernels are isolated from one another in
separate regions of physical memory, having access to a subset of processing
cores and I/O devices. This partitioning prevents system failures in one
sandbox affecting the operation of other sandboxes. Shared memory channels
managed by system monitors enable inter-sandbox communication.
  The distributed nature of Quest-V means each sandbox has a separate physical
clock, with all event timings being managed by per-core local timers. Each
sandbox is responsible for its own scheduling and I/O management, without
requiring intervention of a hypervisor.
  In this paper, we formulate bounds on inter-sandbox communication in the
absence of a global scheduler or global system clock. We also describe how
address space migration between sandboxes can be guaranteed without violating
service constraints. Experimental results on a working system show the
conditions under which Quest-V performs real-time communication and migration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6303</identifier>
 <datestamp>2013-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6303</id><created>2013-10-23</created><authors><author><keyname>Hofman</keyname><forenames>Piotr</forenames></author><author><keyname>Lasota</keyname><forenames>Slawomir</forenames></author><author><keyname>Mayr</keyname><forenames>Richard</forenames></author><author><keyname>Totzke</keyname><forenames>Patrick</forenames></author></authors><title>Simulation Over One-counter Nets is PSPACE-Complete</title><categories>cs.FL</categories><comments>Extended version of material presented at the FST&amp;TCS 2013
  conference. 22 pages</comments><report-no>EDI-INF-RR-1418</report-no><msc-class>68Q45</msc-class><acm-class>F.1.1; D.2.4</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  One-counter nets (OCN) are Petri nets with exactly one unbounded place. They
are equivalent to a subclass of one-counter automata with just a weak test for
zero. Unlike many other semantic equivalences, strong and weak simulation
preorder are decidable for OCN, but the computational complexity was an open
problem. We show that both strong and weak simulation preorder on OCN are
PSPACE-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6304</identifier>
 <datestamp>2013-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6304</id><created>2013-10-23</created><updated>2013-10-24</updated><authors><author><keyname>Karampatziakis</keyname><forenames>Nikos</forenames></author><author><keyname>Mineiro</keyname><forenames>Paul</forenames></author></authors><title>Combining Structured and Unstructured Randomness in Large Scale PCA</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Principal Component Analysis (PCA) is a ubiquitous tool with many
applications in machine learning including feature construction, subspace
embedding, and outlier detection. In this paper, we present an algorithm for
computing the top principal components of a dataset with a large number of rows
(examples) and columns (features). Our algorithm leverages both structured and
unstructured random projections to retain good accuracy while being
computationally efficient. We demonstrate the technique on the winning
submission the KDD 2010 Cup.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6311</identifier>
 <datestamp>2013-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6311</id><created>2013-10-20</created><authors><author><keyname>Carsenat</keyname><forenames>Elian</forenames></author></authors><title>Onomastics and Big Data Mining</title><categories>cs.CY</categories><comments>ParisTech Review, October 2013</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  As of today, the main business application of onomastics is naming, or
branding: finding the proper name for your company or your product to stand out
in the world. Meaningfully, Onoma, the Greek root for name, is also a
registered trademark of Nomen, the naming agency founded by Marcel Botton in
1981. Nomen initially licensed one of Roland Moreno's inventions, the Radoteur
name generator, and created many distinctive and global brand names such as:
Vinci, Clio or Amundi. But once your business has a name, should you forget
about onomastics? Not anymore. Globalization, digitalization and the Big Data
open new fields to experiment disruptive applications in Sales and Marketing,
Communication, HR and Risk Management. Though discriminating names carries a
high risk of abuse, it can also drive new, unexpected ways for developing poor
areas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6323</identifier>
 <datestamp>2013-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6323</id><created>2013-10-23</created><authors><author><keyname>Verbrugge</keyname><forenames>Rineke</forenames></author></authors><title>Logic in the Lab</title><categories>cs.AI cs.GT cs.LO</categories><comments>2 pages, Plenary talk at TARK 2013 (arXiv:1310.6382)
  http://www.tark.org</comments><proxy>Burkhard Schipper</proxy><report-no>TARK/2013/p4</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This file summarizes the plenary talk on laboratory experiments on logic at
the TARK 2013 - 14th Conference on Theoretical Aspects of Rationality and
Knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6324</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6324</id><created>2013-10-23</created><updated>2013-11-04</updated><authors><author><keyname>Khuri-Makdisi</keyname><forenames>Kamal</forenames></author></authors><title>On Jacobian group arithmetic for typical divisors on curves</title><categories>math.NT cs.SC math.AG</categories><comments>26 pages, tightened up the abstract from version 1 and some minor
  edits</comments><msc-class>14Q05 (primary), 11Y16, 14H40, 11G20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a previous joint article with F. Abu Salem, we gave efficient algorithms
for Jacobian group arithmetic of &quot;typical&quot; divisor classes on C_{3,4} curves,
improving on similar results by other authors. At that time, we could only
state that a generic divisor was typical, and hence unlikely to be encountered
if one implemented these algorithms over a very large finite field. This
article pins down an explicit characterization of these typical divisors, for
an arbitrary smooth projective curve of genus g &gt;= 1 having at least one
rational point. We give general algorithms for Jacobian group arithmetic with
these typical divisors, and prove that if our algorithms can be carried out,
then this provides a guarantee that the resulting output is correct and that
the resulting divisor is also typical. These results apply in particular to our
earlier algorithms for C_{3,4} curves. As a byproduct, we obtain a further
speedup of approximately 15% on our previous algorithms for C_{3,4} curves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6333</identifier>
 <datestamp>2013-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6333</id><created>2013-10-23</created><authors><author><keyname>Kak</keyname><forenames>Subhash</forenames></author></authors><title>Threshold Quantum Cryptography</title><categories>quant-ph cs.CR</categories><comments>10 pages, 4 figures. arXiv admin note: text overlap with
  arXiv:1206.6778</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most current research on quantum cryptography requires transmission and
reception of single photons that creates severe implementation challenges and
limits range. This paper argues for the development of threshold quantum
cryptography protocols in which the system is secure so long as the number of
photons being exchanged between Alice and Bob is below a specified threshold.
We speak of a (p-k-n) threshold system where if the number of photons exchanged
is less than p, the system is completely secure, when it is between p and k,
the system is partially secure, and when it exceeds k, the system is insecure.
The BB84 protocol is (1-1-1) whereas the three-stage protocol appears to be
(p-4p-n), where p is the least number of photons necessary to determine the
polarization state of identically prepared photons. New quantum cryptography
systems should be sought that provide greater flexibility in the choice of p
and k.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6338</identifier>
 <datestamp>2015-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6338</id><created>2013-10-23</created><authors><author><keyname>Hintze</keyname><forenames>Arend</forenames></author><author><keyname>Olson</keyname><forenames>Randal S.</forenames></author><author><keyname>Adami</keyname><forenames>Christoph</forenames></author><author><keyname>Hertwig</keyname><forenames>Ralph</forenames></author></authors><title>Risk aversion as an evolutionary adaptation</title><categories>q-bio.PE cs.GT cs.NE</categories><comments>18 pages, 7 figures</comments><journal-ref>Scientific Reports 5 (2015) 8242</journal-ref><doi>10.1038/srep08242</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Risk aversion is a common behavior universal to humans and animals alike.
Economists have traditionally defined risk preferences by the curvature of the
utility function. Psychologists and behavioral economists also make use of
concepts such as loss aversion and probability weighting to model risk
aversion. Neurophysiological evidence suggests that loss aversion has its
origins in relatively ancient neural circuitries (e.g., ventral striatum).
Could there thus be an evolutionary origin to risk avoidance? We study this
question by evolving strategies that adapt to play the equivalent mean payoff
gamble. We hypothesize that risk aversion in the equivalent mean payoff gamble
is beneficial as an adaptation to living in small groups, and find that a
preference for risk averse strategies only evolves in small populations of less
than 1,000 individuals, while agents exhibit no such strategy preference in
larger populations. Further, we discover that risk aversion can also evolve in
larger populations, but only when the population is segmented into small groups
of around 150 individuals. Finally, we observe that risk aversion only evolves
when the gamble is a rare event that has a large impact on the individual's
fitness. These findings align with earlier reports that humans lived in small
groups for a large portion of their evolutionary history. As such, we suggest
that rare, high-risk, high-payoff events such as mating and mate competition
could have driven the evolution of risk averse behavior in humans living in
small groups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6342</identifier>
 <datestamp>2013-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6342</id><created>2013-10-23</created><authors><author><keyname>Gabora</keyname><forenames>Liane</forenames></author></authors><title>Cultural Evolution as Distributed Computation</title><categories>cs.MA nlin.AO</categories><comments>13 pages Gabora, L. (2013). Cultural evolution as distributed human
  computation. In P. Michelucci (Ed.) Handbook of Human Computation. Berlin:
  Springer</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The speed and transformative power of human cultural evolution is evident
from the change it has wrought on our planet. This chapter proposes a human
computation program aimed at (1) distinguishing algorithmic from
non-algorithmic components of cultural evolution, (2) computationally modeling
the algorithmic components, and amassing human solutions to the non-algorithmic
(generally, creative) components, and (3) combining them to develop
human-machine hybrids with previously unforeseen computational power that can
be used to solve real problems. Drawing on recent insights into the origins of
evolutionary processes from biology and complexity theory, human minds are
modeled as self-organizing, interacting, autopoietic networks that evolve
through a Lamarckian (non-Darwinian) process of communal exchange. Existing
computational models as well as directions for future research are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6343</identifier>
 <datestamp>2013-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6343</id><created>2013-10-23</created><authors><author><keyname>Arora</keyname><forenames>Sanjeev</forenames></author><author><keyname>Bhaskara</keyname><forenames>Aditya</forenames></author><author><keyname>Ge</keyname><forenames>Rong</forenames></author><author><keyname>Ma</keyname><forenames>Tengyu</forenames></author></authors><title>Provable Bounds for Learning Some Deep Representations</title><categories>cs.LG cs.AI stat.ML</categories><comments>The first 18 pages serve as an extended abstract and a 36 pages long
  technical appendix follows</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give algorithms with provable guarantees that learn a class of deep nets
in the generative model view popularized by Hinton and others. Our generative
model is an $n$ node multilayer neural net that has degree at most $n^{\gamma}$
for some $\gamma &lt;1$ and each edge has a random edge weight in $[-1,1]$. Our
algorithm learns {\em almost all} networks in this class with polynomial
running time. The sample complexity is quadratic or cubic depending upon the
details of the model.
  The algorithm uses layerwise learning. It is based upon a novel idea of
observing correlations among features and using these to infer the underlying
edge structure via a global graph recovery procedure. The analysis of the
algorithm reveals interesting structure of neural networks with random edge
weights.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6349</identifier>
 <datestamp>2013-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6349</id><created>2013-10-23</created><authors><author><keyname>West</keyname><forenames>Richard</forenames></author><author><keyname>Li</keyname><forenames>Ye</forenames></author><author><keyname>Missimer</keyname><forenames>Eric</forenames></author></authors><title>Quest-V: A Virtualized Multikernel for Safety-Critical Real-Time Systems</title><categories>cs.OS</categories><comments>12 pages. arXiv admin note: text overlap with arXiv:1112.5136,
  arXiv:1310.6301</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern processors are increasingly featuring multiple cores, as well as
support for hardware virtualization. While these processors are common in
desktop and server-class computing, they are less prevalent in embedded and
real-time systems. However, smartphones and tablet PCs are starting to feature
multicore processors with hardware virtualization. If the trend continues, it
is possible that future real-time systems will feature more sophisticated
processor architectures. Future automotive or avionics systems, for example,
could replace complex networks of uniprocessors with consolidated services on a
smaller number of multicore processors. Likewise, virtualization could be used
to isolate services and increase the availability of a system even when
failures occur.
  This paper investigates whether advances in modern processor technologies
offer new opportunities to rethink the design of real-time operating systems.
We describe some of the design principles behind Quest-V, which is being used
as an exploratory vehicle for real-time system design on multicore processors
with hardware virtualization capabilities. While not all embedded systems
should assume such features, a case can be made that more robust,
safety-critical systems can be built to use hardware virtualization without
incurring significant overheads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6376</identifier>
 <datestamp>2013-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6376</id><created>2013-10-23</created><authors><author><keyname>Dutta</keyname><forenames>Abhishek</forenames></author><author><keyname>Veldhuis</keyname><forenames>Raymond</forenames></author><author><keyname>Spreeuwers</keyname><forenames>Luuk</forenames></author></authors><title>Can Facial Uniqueness be Inferred from Impostor Scores?</title><categories>cs.CV</categories><comments>A 6 page paper presented in the Biometric Technologies in Forensic
  Science (BTFS) 2013 Conference, Oct 14-15 2013, Nijmegen, Netherlands. Full
  proceeding is available at http://www.ru.nl/clst/btfs/btfs-2013/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Biometrics, facial uniqueness is commonly inferred from impostor
similarity scores. In this paper, we show that such uniqueness measures are
highly unstable in the presence of image quality variations like pose, noise
and blur. We also experimentally demonstrate the instability of a recently
introduced impostor-based uniqueness measure of [Klare and Jain 2013] when
subject to poor quality facial images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6377</identifier>
 <datestamp>2013-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6377</id><created>2013-10-22</created><authors><author><keyname>Takyar</keyname><forenames>Uday</forenames></author><author><keyname>Maugey</keyname><forenames>Thomas</forenames></author><author><keyname>Frossard</keyname><forenames>Pascal</forenames></author></authors><title>Multiview Navigation based on Extended Layered Depth Image
  Representation</title><categories>cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Emerging applications in multiview streaming look for providing interactive
navigation services to video players. The user can ask for information from any
viewpoint with a minimum transmission delay. The purpose is to provide user
with as much information as possible with least number of redundancies. The
recent concept of navigation segment representation consists of regrouping a
given number of viewpoints in one signal and transmitting them to the users
according to their navigation path. The question of the best description
strategy of these navigation segments is however still open. In this paper, we
propose to represent and code navigation segments by a method that extends the
recent layered depth image (LDI) format. It consists of describing the scene
from a viewpoint with multiple images organized in layers corresponding to the
different levels of occluded objects. The notion of extended LDI comes from the
fact that the size of this image is adapted to take into account the sides of
the scene also, in contrary to classical LDI. The obtained results show a
significant rate-distortion gain compared to classical multiview compression
approaches in navigation scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6382</identifier>
 <datestamp>2013-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6382</id><created>2013-10-23</created><authors><author><keyname>Schipper</keyname><forenames>Burkhard C.</forenames></author></authors><title>TARK 2013 - Proceedings of the 14. Conference on Theoretical Aspects of
  Rationality and Knowledge</title><categories>cs.GT cs.LO</categories><comments>Chennai, India, January 7 - 9, 2013, 27 articles, ISBN:
  978-0-615-74716-3, http://www.tark.org/</comments><proxy>Burkhard Schipper</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The biannual TARK conferences bring together researchers from a wide variety
of fields sharing a common interest in reasoning about rationality and
knowledge. The impact of this tradition, going back to 1986, is apparent in
many of today's research trends and in the growth of an intellectual community
beyond traditional disciplinary boundaries. This volume documents the 14th TARK
conference, held at the Institute of Mathematical Sciences, Chennai, India on
January 7 to 9, 2013. It includes 18 contributed talks, 8 poster presentations,
and 3 invited talks given at the conference. Like earlier volumes in this
series, it gives a sense of the state of the art in studies of knowledge and
rationality in areas such as game theory, decision theory, belief revision,
language analysis, and computation. It should be of value to researchers,
teachers, and students.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6383</identifier>
 <datestamp>2014-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6383</id><created>2013-10-23</created><updated>2014-05-06</updated><authors><author><keyname>Lutz</keyname><forenames>Jack H.</forenames></author></authors><title>The Frequent Paucity of Trivial Strings</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A 1976 theorem of Chaitin can be used to show that arbitrarily dense sets of
lengths n have a paucity of trivial strings (only a bounded number of strings
of length n having trivially low plain Kolmogorov complexities). We use the
probabilistic method to give a new proof of this fact. This proof is much
simpler than previously published proofs, and it gives a tighter paucity bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6397</identifier>
 <datestamp>2013-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6397</id><created>2013-10-23</created><authors><author><keyname>Basly</keyname><forenames>Maryam</forenames></author><author><keyname>Kochkar</keyname><forenames>Hedia</forenames></author><author><keyname>Bouallegue</keyname><forenames>Ammar</forenames></author></authors><title>Variance Based Scheduling Algorithm with Relay Selection and Resource
  Allocation in Cooperative OFDMA Networks</title><categories>cs.NI</categories><comments>6 pages 4 figures</comments><journal-ref>IJCSI Volume 10, Issue 3, May 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays the radio interface of several standards is enhanced with advanced
technologies such as OFDMA and extension technology such as relay. By using
those promising transmission technology for the next generation wireless
communications, scheduling problem becomes more crucial and challenging. In our
work, we aim to maximize the overall system capacity while selecting the most
suitable relay station under fairness constraint among both users and relay
station by proposing a Gap- based scheduling. This one considers the channel
state information and the unbalanced rate capacity of the two hops links.
Simulations results show the effectiveness of our approach in terms of fairness
and the overall system performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6398</identifier>
 <datestamp>2014-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6398</id><created>2013-10-23</created><updated>2014-04-04</updated><authors><author><keyname>Petersen</keyname><forenames>Holger</forenames></author></authors><title>Some Remarks on Lower Bounds for Queue Machines (Preliminary Report)</title><categories>cs.CC cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We first give an improved lower bound for the deterministic online-simulation
of tapes or pushdown stores by queues. Then we inspect some proofs in a
classical work on queue machines in the area of Formal Languages and outline
why a main argument in the proofs is incomplete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6405</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6405</id><created>2013-10-23</created><authors><author><keyname>Anderson</keyname><forenames>Gabrielle</forenames></author><author><keyname>Collinson</keyname><forenames>Matthew</forenames></author><author><keyname>Pym</keyname><forenames>David</forenames></author></authors><title>Utility-based Decision-making in Distributed Systems Modelling</title><categories>cs.LO cs.MA</categories><comments>11 pages, Contributed talk at TARK 2013 (arXiv:1310.6382)
  http://www.tark.org</comments><proxy>Burkhard Schipper</proxy><report-no>TARK/2013/p8</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a calculus of resources and processes as a basis for modelling
decision-making in multi-agent systems. The calculus represents the regulation
of agents' choices using utility functions that take account of context.
Associated with the calculus is a (Hennessy Milner-style) context sensitive
modal logic of state. As an application, we show how a notion of `trust domain'
can be defined for multi-agent systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6406</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6406</id><created>2013-10-23</created><authors><author><keyname>Aucher</keyname><forenames>Guillaume</forenames></author><author><keyname>Schwarzentruber</keyname><forenames>Francois</forenames></author></authors><title>On the Complexity of Dynamic Epistemic Logic</title><categories>cs.LO</categories><comments>10 pages, Contributed talk at TARK 2013 (arXiv:1310.6382)
  http://www.tark.org</comments><proxy>Burkhard Schipper</proxy><report-no>TARK/2013/p19</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although Dynamic Epistemic Logic (DEL) is an influential logical framework
for representing and reasoning about information change, little is known about
the computational complexity of its associated decision problems. In fact, we
only know that for public announcement logic, a fragment of DEL, the
satisfiability problem and the model-checking problem are respectively
PSPACE-complete and in P. We contribute to fill this gap by proving that for
the DEL language with event models, the model-checking problem is,
surprisingly, PSPACE-complete. Also, we prove that the satisfiability problem
is NEXPTIME-complete. In doing so, we provide a sound and complete tableau
method deciding the satisfiability problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6407</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6407</id><created>2013-10-23</created><authors><author><keyname>Ben-Zvi</keyname><forenames>Ido</forenames></author><author><keyname>Moses</keyname><forenames>Yoram</forenames></author></authors><title>The Shape of Reactive Coordination Tasks</title><categories>cs.LO cs.DC</categories><comments>10 pages, Contributed talk presented at TARK 2013 (arXiv:1310.6382)
  http://www.tark.org</comments><proxy>Burkhard Schipper</proxy><report-no>TARK/2013/p29</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the interaction between knowledge, time and coordination
in systems in which timing information is available. Necessary conditions are
given for the causal structure in coordination problems consisting of
orchestrating a set of actions in a manner that satisfies a variety of temporal
ordering assumptions. Results are obtained in two main steps: A specification
of coordination is shown to require epistemic properties, and the causal
structure required to obtain these properties is characterised via &quot;knowledge
gain&quot; theorems. A new causal structure called a centibroom structure is
presented, generalising previous causal structures for this model. It is shown
to capture coordination tasks in which a sequence of clusters of events is
performed in linear order, while within each cluster all actions must take
place simultaneously. This form of coordination is shown to require the agents
to gain a nested common knowledge of particular facts, which in turn requires a
centibroom. Altogether, the results presented provide a broad view of the
causal shape underlying partially ordered coordinated actions. This, in turn,
provides insight into and can enable the design of efficient solutions to the
coordination tasks in question.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6408</identifier>
 <datestamp>2013-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6408</id><created>2013-10-23</created><authors><author><keyname>Bjorndahl</keyname><forenames>Adam</forenames></author><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>Pass</keyname><forenames>Rafael</forenames></author></authors><title>Language-based Games</title><categories>cs.GT cs.LO</categories><comments>10 pages, Contributed talk presented at TARK 2013 (arXiv:1310.6382)
  http://www.tark.org</comments><proxy>Burkhard Schipper</proxy><report-no>TARK/2013/p39</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce language-based games, a generalization of psychological games
[6] that can also capture reference-dependent preferences [7]. The idea is to
extend the domain of the utility function to situations, maximal consistent
sets in some language. The role of the underlying language in this framework is
thus particularly critical. Of special interest are languages that can express
only coarse beliefs [9]. Despite the expressive power of the approach, we show
that it can describe games in a simple, natural way. Nash equilibrium and
rationalizability are generalized to this setting; Nash equilibrium is shown
not to exist in general, while the existence of rationalizable strategies is
proved under mild conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6409</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6409</id><created>2013-10-23</created><authors><author><keyname>Britz</keyname><forenames>Katarina</forenames></author><author><keyname>Varzinczak</keyname><forenames>Ivan</forenames></author></authors><title>Defeasible Modalities</title><categories>cs.LO</categories><comments>12 pages, Contributed talk at TARK 2013 (arXiv:1310.6382)
  http://www.tark.org</comments><proxy>Burkhard Schipper</proxy><report-no>TARK/2013/p49</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nonmonotonic logics are usually characterized by the presence of some notion
of 'conditional' that fails monotonicity. Research on nonmonotonic logics is
therefore largely concerned with the defeasibility of argument forms and the
associated normality (or abnormality) of its constituents. In contrast,
defeasible modes of inference aim to formalize the defeasible aspects of modal
notions such as actions, obligations and knowledge. In this work we enrich the
standard possible worlds semantics with a preference ordering on worlds in
Kripke models. The resulting family of modal logics allow for the elegant
expression of defeasible modalities. We also propose a tableau calculus which
is sound and complete with respect to our preferential semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6410</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6410</id><created>2013-10-23</created><authors><author><keyname>van Ditmarsch</keyname><forenames>Hans</forenames></author><author><keyname>French</keyname><forenames>Tim</forenames></author><author><keyname>Velazquez-Quesada</keyname><forenames>Fernando R.</forenames></author><author><keyname>Wang</keyname><forenames>Yi N.</forenames></author></authors><title>Knowledge, Awareness, and Bisimulation</title><categories>cs.LO</categories><comments>10 pages, Contributed talk at TARK 2013 (arXiv:1310.6382)
  http://www.tark.org</comments><proxy>Burkhard Schipper</proxy><report-no>TARK/2013/p61</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We compare different epistemic notions in the presence of awareness of
propositional variables: the logics of implicit knowledge (in which explicit
knowledge is definable), explicit knowledge, and speculative knowledge.
Different notions of bisimulation are suitable for these logics. We provide
correspondence between bisimulation and modal equivalence on image-finite
models for these logics. The logic of speculative knowledge is equally
expressive as the logic of explicit knowledge, and the logic of implicit
knowledge is more expressive than both. We also provide axiomatizations for the
three logics -- only the one for speculative knowledge is novel. Then we move
to the study of dynamics by recalling action models incorporating awareness. We
show that any conceivable change of knowledge or awareness can be modelled in
this setting, we give a complete axiomatization for the dynamic logic of
implicit knowledge. The dynamic versions of all three logics are, surprising,
equally expressive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6411</identifier>
 <datestamp>2013-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6411</id><created>2013-10-23</created><authors><author><keyname>Espinosa-Avila</keyname><forenames>Eduardo</forenames></author><author><keyname>Hernandez-Quiroz</keyname><forenames>Francisco</forenames></author></authors><title>Bounded rationality in a dynamic alternate game</title><categories>cs.GT cs.LO</categories><comments>7 pages, Contributed talk presented at TARK 2013 (arXiv:1310.6382)
  http://www.tark.org</comments><proxy>Burkhard Schipper</proxy><report-no>TARK/2013/p71</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  From the standpoint of game theory, dominoes is a game that has not received
much attention (specially the variety known as draw). It is usually thought
that this game is already solved, given general results in game theory.
However, the determination of equilibria is not feasible for the general case
because of the well known problem of node explosion in the tree expressing the
game. We propose a new model based in limited forecast as a kind of bounded
rationality for dynamic alternate games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6413</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6413</id><created>2013-10-23</created><updated>2014-01-05</updated><authors><author><keyname>Aronov</keyname><forenames>Boris</forenames></author><author><keyname>Yagnatinsky</keyname><forenames>Mark V.</forenames></author></authors><title>How To Place a Point to Maximize Angles</title><categories>cs.CG</categories><comments>fixed typo, added some clarifications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a randomized algorithm that, given a set $P$ of points in the
plane, computes the best location to insert a new point $p$, such that the
Delaunay triangulation of $P\cup\{p\}$ has the largest possible minimum angle.
The expected running time of our algorithm is at most cubic, improving the
roughly quartic time of the best previously known algorithm. It slows down to
slightly super-cubic if we also specify a set of non-crossing segments with
endpoints in $P$ and insist that the triangulation respect these segments,
i.e., is the constrained Delaunay triangulation of the points and segments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6414</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6414</id><created>2013-10-23</created><authors><author><keyname>Gonczarowski</keyname><forenames>Yannai A.</forenames></author><author><keyname>Moses</keyname><forenames>Yoram</forenames></author></authors><title>Timely Common Knowledge</title><categories>cs.LO</categories><comments>15 pages, Contributed talk at TARK 2013 (arXiv:1310.6382)
  http://www.tark.org</comments><proxy>Burkhard Schipper</proxy><report-no>TARK/2013/p79</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coordinating activities at different sites of a multi-agent system typically
imposes epistemic constraints on the participants. Specifying explicit bounds
on the relative times at which actions are performed induces combined temporal
and epistemic constraints on when agents can perform their actions. This paper
characterises the interactive epistemic state that arises when actions must
meet particular temporal constraints. The new state, called timely common
knowledge, generalizes common knowledge, as well as other variants of common
knowledge. While known variants of common knowledge are defined in terms of a
fixed point of an epistemic formula, timely common knowledge is defined in
terms of a vectorial fixed point of temporal-epistemic formulae. A general
class of coordination tasks with timing constraints is defined, and timely
common knowledge is used to characterise both solvability and optimal solutions
of such tasks. Moreover, it is shown that under natural conditions, timely
common knowledge is equivalent to an infinite conjunction of temporal-epistemic
formulae, in analogy to the popular definition of common knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6416</identifier>
 <datestamp>2013-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6416</id><created>2013-10-23</created><authors><author><keyname>Grossi</keyname><forenames>Davide</forenames></author><author><keyname>Lorini</keyname><forenames>Emiliano</forenames></author><author><keyname>Schwarzentruber</keyname><forenames>Francois</forenames></author></authors><title>Ceteris Paribus Structure in Logics of Game Forms</title><categories>cs.GT cs.LO</categories><comments>11 pages, Contributed talk at TARK 2013 (arXiv:1310.6382)
  http://www.tark.org</comments><proxy>Burkhard Schipper</proxy><report-no>TARK/2013/p94</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article introduces a ceteris paribus modal logic interpreted on the
equivalence classes induced by sets of propositional atoms. This logic is used
to embed two logics of agency and games, namely atemporal STIT and the
coalition logic of propositional control (CL-PC). The embeddings highlight a
common ceteris paribus structure underpinning the key modal operators of both
logics, they clarify the relationship between STIT and CL-PC, and enable the
transfer of complexity results to the ceteris paribus logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6418</identifier>
 <datestamp>2013-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6418</id><created>2013-10-23</created><authors><author><keyname>Hellman</keyname><forenames>Ziv</forenames></author></authors><title>Deludedly Agreeing to Agree</title><categories>cs.GT cs.LO</categories><comments>6 pages, Contributed paper at TARK 2013 (arXiv:1310.6382)
  http://www.tark.org</comments><proxy>Burkhard Schipper</proxy><report-no>TARK/2013/p105</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study conditions relating to the impossibility of agreeing to disagree in
models of interactive KD45 belief (in contrast to models of S5 knowledge, which
are used in nearly all the agreements literature). We show that even when the
truth axiom is not assumed it turns out that players will find it impossible to
agree to disagree under fairly broad conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6422</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6422</id><created>2013-10-23</created><updated>2014-04-13</updated><authors><author><keyname>Mishra</keyname><forenames>Dheerendra</forenames></author></authors><title>Cryptanalysis of Sun and Cao's Remote Authentication Scheme with User
  Anonymity</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic ID-based remote user authentication schemes ensure efficient and
anonymous mutual authentication between entities. In 2013, Khan et al. proposed
an improved dynamic ID-based authentication scheme to overcome the security
flaws of Wang et al.'s authentication scheme. Recently, Sun and Cao showed that
Khan et al. does not satisfies the claim of the user's privacy and proposed an
efficient authentication scheme with user anonymity. The Sun and Cao's scheme
achieve improvement over Khan et al.'s scheme in both privacy and performance
point of view. Unfortunately, we identify that Sun and Cao's scheme does not
resist password guessing attack. Additionally, Sun and Cao's scheme does not
achieve forward secrecy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6423</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6423</id><created>2013-10-23</created><authors><author><keyname>Huang</keyname><forenames>X.</forenames></author><author><keyname>van der Meyden</keyname><forenames>R.</forenames></author></authors><title>Symbolic Synthesis of Knowledge-based Program Implementations with
  Synchronous Semantics</title><categories>cs.LO</categories><comments>10 pages, Contributed talk at TARK 2013 (arXiv:1310.6382)
  http://www.tark.org</comments><proxy>Burkhard Schipper</proxy><report-no>TARK/2013/p121</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the automated synthesis of implementations of
knowledge-based programs with respect to two synchronous semantics (clock and
synchronous perfect recall). An approach to the synthesis problem based on the
use of symbolic representations is described. The method has been implemented
as an extension to the model checker MCK. Two applications of the implemented
synthesis system are presented: the muddy children puzzle (where performance is
compared to an explicit state method for a related problem implemented in the
model checker DEMO), and a knowledge-based program for a dynamic leader
election problem in a ring of processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6424</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6424</id><created>2013-10-23</created><authors><author><keyname>Kane</keyname><forenames>Jeffrey</forenames></author><author><keyname>Naumov</keyname><forenames>Pavel</forenames></author></authors><title>Epistemic Logic for Communication Chains</title><categories>cs.LO</categories><comments>7 pages, Contributed talk at TARK 2013 (arXiv:1310.6382)
  http://www.tark.org</comments><proxy>Burkhard Schipper</proxy><report-no>TARK/2013/p131</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper considers epistemic properties of linear communication chains. It
describes a sound and complete logical system that, in addition to the standard
axioms of S5 in a multi-modal language, contains two non-trivial axioms that
capture the linear structure of communication chains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6427</identifier>
 <datestamp>2013-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6427</id><created>2013-10-23</created><authors><author><keyname>Lechner</keyname><forenames>Gottfried</forenames></author><author><keyname>Pacher</keyname><forenames>Christoph</forenames></author></authors><title>Estimating Channel Parameters from the Syndrome of a Linear Code</title><categories>cs.IT math.IT</categories><comments>5 pages, accepted for IEEE Communications Letters</comments><journal-ref>IEEE Communications Letters, Vol. 17, p. 2148 - 2151 (2013)</journal-ref><doi>10.1109/LCOMM.2013.091113.131646</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we analyse the properties of a maximum likelihood channel
estimator based on the syndrome of a linear code. For the two examples of a
binary symmetric channel and a binary input additive white Gaussian noise
channel, we derive expressions for the bias and the mean squared error and
compare them to the Cram\'er-Rao bound. The analytical expressions show the
relationship between the estimator properties and the parameters of the linear
code, i.e., the number of check nodes and the check node degree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6429</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6429</id><created>2013-10-23</created><authors><author><keyname>Lang</keyname><forenames>Jerome</forenames></author><author><keyname>Zanuttini</keyname><forenames>Bruno</forenames></author></authors><title>Knowledge-Based Programs as Plans: Succinctness and the Complexity of
  Plan Existence</title><categories>cs.AI cs.LO</categories><comments>10 pages, Contributed talk at TARK 2013 (arXiv:1310.6382)
  http://www.tark.org</comments><proxy>Burkhard Schipper</proxy><report-no>TARK/2013/p138</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowledge-based programs (KBPs) are high-level protocols describing the
course of action an agent should perform as a function of its knowledge. The
use of KBPs for expressing action policies in AI planning has been surprisingly
overlooked. Given that to each KBP corresponds an equivalent plan and vice
versa, KBPs are typically more succinct than standard plans, but imply more
on-line computation time. Here we make this argument formal, and prove that
there exists an exponential succinctness gap between knowledge-based programs
and standard plans. Then we address the complexity of plan existence. Some
results trivially follow from results already known from the literature on
planning under incomplete knowledge, but many were unknown so far.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6430</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6430</id><created>2013-10-23</created><authors><author><keyname>Naumov</keyname><forenames>Pavel</forenames></author><author><keyname>Nicholls</keyname><forenames>Brittany</forenames></author></authors><title>R.E. Axiomatization of Conditional Independence</title><categories>cs.LO</categories><comments>8 pages, Contributed talk at TARK 2013 (arXiv:1310.6382)
  http://www.tark.org</comments><proxy>Burkhard Schipper</proxy><report-no>TARK/2013/p148</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper investigates properties of the conditional independence relation
between pieces of information. This relation is also known in the database
theory as embedded multivalued dependency. In 1980, Parker and Parsaye-Ghomi
established that the properties of this relation can not be described by a
finite system of inference rules. In 1995, Herrmann proved that the
propositional theory of this relation is undecidable. The main result of this
paper is a complete recursively enumerable axiomatization of this theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6432</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6432</id><created>2013-10-23</created><authors><author><keyname>Pacuit</keyname><forenames>Eric</forenames></author><author><keyname>Pedersen</keyname><forenames>Arthur Paul</forenames></author><author><keyname>Romeijn</keyname><forenames>Jan-Willem</forenames></author></authors><title>When is an Example a Counterexample?</title><categories>cs.AI</categories><comments>10 pages, Contributed talk at TARK 2013 (arXiv:1310.6382)
  http://www.tark.org</comments><proxy>Burkhard Schipper</proxy><report-no>TARK/2013/p156</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this extended abstract, we carefully examine a purported counterexample to
a postulate of iterated belief revision. We suggest that the example is better
seen as a failure to apply the theory of belief revision in sufficient detail.
The main contribution is conceptual aiming at the literature on the
philosophical foundations of the AGM theory of belief revision [1]. Our
discussion is centered around the observation that it is often unclear whether
a specific example is a &quot;genuine&quot; counterexample to an abstract theory or a
misapplication of that theory to a concrete case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6433</identifier>
 <datestamp>2013-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6433</id><created>2013-10-23</created><authors><author><keyname>Tarbush</keyname><forenames>Bassel</forenames></author></authors><title>Agreeing on Decisions: An Analysis with Counterfactuals</title><categories>cs.GT cs.LO</categories><comments>8 pages, Contributed talk at TARK 2013 (arXiv:1310.6382)
  http://www.tark.org</comments><proxy>Burkhard Schipper</proxy><report-no>TARK/2013/p166</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Moses &amp; Nachum ([7]) identify conceptual flaws in Bacharach's generalization
([3]) of Aumann's seminal &quot;agreeing to disagree&quot; result ([1]). Essentially,
Bacharach's framework requires agents' decision functions to be defined over
events that are informationally meaningless for the agents. In this paper, we
argue that the analysis of the agreement theorem should be carried out in
information structures that can accommodate for counterfactual states. We
therefore develop a method for constructing such &quot;counterfactual structures&quot;
(starting from partitional structures), and prove a new agreement theorem
within such structures. Furthermore, we show that our approach also resolves
the conceptual flaws in the sense that, within our framework, decision
functions are always only defined over events that are informationally
meaningful for the agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6434</identifier>
 <datestamp>2013-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6434</id><created>2013-10-23</created><authors><author><keyname>Bozianu</keyname><forenames>Rodica</forenames></author><author><keyname>Dima</keyname><forenames>Catalin</forenames></author><author><keyname>Enea</keyname><forenames>Constantin</forenames></author></authors><title>Model Checking an Epistemic mu-calculus with Synchronous and Perfect
  Recall Semantics</title><categories>cs.GT cs.LO</categories><comments>10 pages, Poster presentation at TARK 2013 (arXiv:1310.6382)
  http://www.tark.org</comments><proxy>Burkhard Schipper</proxy><report-no>TARK/2013/p175</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We identify a subproblem of the model-checking problem for the epistemic
\mu-calculus which is decidable. Formulas in the instances of this subproblem
allow free variables within the scope of epistemic modalities in a restricted
form that avoids embodying any form of common knowledge. Our subproblem
subsumes known decidable fragments of epistemic CTL/LTL, may express winning
strategies in two-player games with one player having imperfect information and
non-observable objectives, and, with a suitable encoding, decidable instances
of the model-checking problem for ATLiR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6435</identifier>
 <datestamp>2013-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6435</id><created>2013-10-23</created><authors><author><keyname>Brauner</keyname><forenames>Torben</forenames></author></authors><title>Hybrid-Logical Reasoning in False-Belief Tasks</title><categories>cs.GT cs.LO</categories><comments>10 pages, Poster presentation at TARK 2013 (arXiv:1310.6382)
  http://www.tark.org</comments><proxy>Burkhard Schipper</proxy><report-no>TARK/2013/p186</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main aim of the present paper is to use a proof system for hybrid modal
logic to formalize what are called falsebelief tasks in cognitive psychology,
thereby investigating the interplay between cognition and logical reasoning
about belief. We consider two different versions of the Smarties task,
involving respectively a shift of perspective to another person and to another
time. Our formalizations disclose that despite this difference, the two
versions of the Smarties task have exactly the same underlying logical
structure. We also consider the Sally-Anne task, having a somewhat more
complicated logical structure, presupposing a &quot;principle of inertia&quot; saying
that a belief is preserved over time, unless there is belief to the contrary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6436</identifier>
 <datestamp>2013-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6436</id><created>2013-10-23</created><authors><author><keyname>van Ditmarsch</keyname><forenames>Hans</forenames></author><author><keyname>Lang</keyname><forenames>Jerome</forenames></author><author><keyname>Saffidine</keyname><forenames>Abdallah</forenames></author></authors><title>Strategic Voting and the Logic of Knowledge</title><categories>cs.GT cs.LO</categories><comments>10 pages, Poster presentation at TARK 2013 (arXiv:1310.6382)
  http://www.tark.org</comments><proxy>Burkhard Schipper</proxy><report-no>TARK/2013/p196</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a general framework for strategic voting when a voter may lack
knowledge about other votes or about other voters' knowledge about her own
vote. In this setting we define notions of manipulation and equilibrium. We
also model action changing knowledge about votes, such as a voter revealing its
preference or as a central authority performing a voting poll. Some forms of
manipulation are preserved under such updates and others not. Another form of
knowledge dynamics is the effect of a voter declaring its vote. We envisage
Stackelberg games for uncertain profiles. The purpose of this investigation is
to provide the epistemic background for the analysis and design of voting rules
that incorporate uncertainty.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6437</identifier>
 <datestamp>2013-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6437</id><created>2013-10-23</created><authors><author><keyname>van Eijck</keyname><forenames>Jan</forenames></author></authors><title>PDL as a Multi-Agent Strategy Logic</title><categories>cs.GT cs.LO</categories><comments>10 pages, Poster presentation at TARK 2013 (arXiv:1310.6382)
  http://www.tark.org</comments><proxy>Burkhard Schipper</proxy><report-no>TARK/2013/p206</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Propositional Dynamic Logic or PDL was invented as a logic for reasoning
about regular programming constructs. We propose a new perspective on PDL as a
multi-agent strategic logic (MASL). This logic for strategic reasoning has
group strategies as first class citizens, and brings game logic closer to
standard modal logic. We demonstrate that MASL can express key notions of game
theory, social choice theory and voting theory in a natural way, we give a
sound and complete proof system for MASL, and we show that MASL encodes
coalition logic. Next, we extend the language to epistemic multi-agent
strategic logic (EMASL), we give examples of what it can express, we propose to
use it for posing new questions in epistemic social choice theory, and we give
a calculus for reasoning about a natural class of epistemic game models. We end
by listing avenues for future research and by tracing connections to a number
of other logics for reasoning about strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6438</identifier>
 <datestamp>2013-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6438</id><created>2013-10-23</created><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>Pass</keyname><forenames>Rafael</forenames></author></authors><title>Game Theory with Translucent Players</title><categories>cs.GT cs.LO</categories><comments>6 pages, Poster presentation at TARK 2013 (arXiv:1310.6382)
  http://www.tark.org</comments><proxy>Burkhard Schipper</proxy><report-no>TARK/2013/p216</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A traditional assumption in game theory is that players are opaque to one
another -- if a player changes strategies, then this change in strategies does
not affect the choice of other players' strategies. In many situations this is
an unrealistic assumption. We develop a framework for reasoning about games
where the players may be translucent to one another; in particular, a player
may believe that if she were to change strategies, then the other player would
also change strategies. Translucent players may achieve significantly more
efficient outcomes than opaque ones.
  Our main result is a characterization of strategies consistent with
appropriate analogues of common belief of rationality. Common Counterfactual
Belief of Rationality (CCBR) holds if (1) everyone is rational, (2) everyone
counterfactually believes that everyone else is rational (i.e., all players i
believe that everyone else would still be rational even if i were to switch
strategies), (3) everyone counterfactually believes that everyone else is
rational, and counterfactually believes that everyone else is rational, and so
on. CCBR characterizes the set of strategies surviving iterated removal of
minimax dominated strategies: a strategy $\sigma_i$ is minimax dominated for i
if there exists a strategy $\sigma'_i$ for i such that $\min_{\mu'_{-i}}
u_i(\sigma_i, \mu_{-i}') &gt; \max_{\mu_{-i}} u_i(\sigma_i, \mu_{-i})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6439</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6439</id><created>2013-10-23</created><authors><author><keyname>Papai</keyname><forenames>Tivadar</forenames></author><author><keyname>Kautz</keyname><forenames>Henry</forenames></author><author><keyname>Stefankovic</keyname><forenames>Daniel</forenames></author></authors><title>Reasoning Under the Principle of Maximum Entropy for Modal Logics K45,
  KD45, and S5</title><categories>cs.LO</categories><comments>7 pages, Poster presentation at TARK 2013 (arXiv:1310.6382)
  http://www.tark.org</comments><proxy>Burkhard Schipper</proxy><report-no>TARK/2013/p222</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose modal Markov logic as an extension of propositional Markov logic
to reason under the principle of maximum entropy for modal logics K45, KD45,
and S5. Analogous to propositional Markov logic, the knowledge base consists of
weighted formulas, whose weights are learned from data. However, in contrast to
Markov logic, in our framework we use the knowledge base to define a
probability distribution over non-equivalent epistemic situations (pointed
Kripke structures) rather than over atoms, and use this distribution to assign
probabilities to modal formulas. As in all probabilistic representations, the
central task in our framework is inference. Although the size of the state
space grows doubly exponentially in the number of propositions in the domain,
we provide an algorithm that scales only exponentially in the size of the
knowledge base. Finally, we briefly discuss the case of languages with an
infinite number of propositions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6440</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6440</id><created>2013-10-23</created><authors><author><keyname>Seligman</keyname><forenames>Jeremy</forenames></author><author><keyname>Liu</keyname><forenames>Fenrong</forenames></author><author><keyname>Girard</keyname><forenames>Patrick</forenames></author></authors><title>Facebook and the Epistemic Logic of Friendship</title><categories>cs.LO cs.SI</categories><comments>10 pages, Poster presentation at TARK 2013 (arXiv:1310.6382)
  http://www.tark.org</comments><proxy>Burkhard Schipper</proxy><report-no>TARK/2013/p229</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a two-dimensional modal logic for reasoning about the
changing patterns of knowledge and social relationships in networks organised
on the basis of a symmetric 'friendship' relation, providing a precise language
for exploring 'logic in the community' [11]. Agents are placed in the model,
allowing us to express such indexical facts as 'I am your friend' and 'You, my
friends, are in danger'.
  The technical framework for this work is general dynamic dynamic logic (GDDL)
[4], which provides a general method for extending modal logics with dynamic
operators for reasoning about a wide range of model-transformations, starting
with those definable in propositional dynamic logic (PDL) and extended to allow
for the more subtle operators involved in, for example, private communication,
as represented in dynamic epistemic logic (DEL) and related systems. We provide
a hands-on introduction to GDDL, introducing elements of the formalism as we
go, but leave the reader to consult [4] for technical details.
  Instead, the purpose of this paper is to investigate a number of conceptual
issues that arise when considering communication between agents in such
networks, both from one agent to another, and broadcasts to socially-defined
groups of agents, such as the group of my friends.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6441</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6441</id><created>2013-10-23</created><authors><author><keyname>Tsukada</keyname><forenames>Yasuyuki</forenames></author><author><keyname>Sakurada</keyname><forenames>Hideki</forenames></author><author><keyname>Mano</keyname><forenames>Ken</forenames></author><author><keyname>Manabe</keyname><forenames>Yoshifumi</forenames></author></authors><title>An Epistemic Approach to Compositional Reasoning about Anonymity and
  Privacy</title><categories>cs.CR cs.LO</categories><comments>10 pages, Poster presentation at TARK 2013 (arXiv:1310.6382)
  http://www.tark.org</comments><proxy>Burkhard Schipper</proxy><report-no>TARK/2013/p239</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an epistemic logic approach to the compositionality
of several privacy-related informationhiding/ disclosure properties. The
properties considered here are anonymity, privacy, onymity, and identity. Our
initial observation reveals that anonymity and privacy are not necessarily
sequentially compositional; this means that even though a system comprising
several sequential phases satisfies a certain unlinkability property in each
phase, the entire system does not always enjoy a desired unlinkability
property. We show that the compositionality can be guaranteed provided that the
phases of the system satisfy what we call the independence assumptions. More
specifically, we develop a series of theoretical case studies of what
assumptions are sufficient to guarantee the sequential compositionality of
various degrees of anonymity, privacy, onymity, and/or identity properties.
Similar results for parallel composition are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6443</identifier>
 <datestamp>2013-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6443</id><created>2013-10-23</created><authors><author><keyname>Santacruz</keyname><forenames>Pedro E.</forenames></author><author><keyname>Aggarwal</keyname><forenames>Vaneet</forenames></author><author><keyname>Sabharwal</keyname><forenames>Ashutosh</forenames></author></authors><title>Leveraging Physical Layer Capabilites: Distributed Scheduling in
  Interference Networks with Local Views</title><categories>cs.NI cs.IT math.IT</categories><comments>14 pages, Submitted to IEEE/ACM Transactions on Networking, October
  2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In most wireless networks, nodes have only limited local information about
the state of the network, which includes connectivity and channel state
information. With limited local information about the network, each node's
knowledge is mismatched; therefore, they must make distributed decisions. In
this paper, we pose the following question - if every node has network state
information only about a small neighborhood, how and when should nodes choose
to transmit? While link scheduling answers the above question for
point-to-point physical layers which are designed for an interference-avoidance
paradigm, we look for answers in cases when interference can be embraced by
advanced PHY layer design, as suggested by results in network information
theory.
  To make progress on this challenging problem, we propose a constructive
distributed algorithm that achieves rates higher than link scheduling based on
interference avoidance, especially if each node knows more than one hop of
network state information. We compare our new aggressive algorithm to a
conservative algorithm we have presented in [1]. Both algorithms schedule
sub-networks such that each sub-network can employ advanced
interference-embracing coding schemes to achieve higher rates. Our innovation
is in the identification, selection and scheduling of sub-networks, especially
when sub-networks are larger than a single link.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6481</identifier>
 <datestamp>2013-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6481</id><created>2013-10-24</created><authors><author><keyname>Dai</keyname><forenames>Liyun</forenames></author><author><keyname>Gan</keyname><forenames>Ting</forenames></author><author><keyname>Xia</keyname><forenames>Bican</forenames></author><author><keyname>Zhan</keyname><forenames>Naijun</forenames></author></authors><title>Barrier Certificates Revisited</title><categories>cs.SY</categories><comments>11 pages 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A barrier certificate can separate the state space of a con- sidered hybrid
system (HS) into safe and unsafe parts ac- cording to the safety property to be
verified. Therefore this notion has been widely used in the verification of
HSs. A stronger condition on barrier certificates means that less expressive
barrier certificates can be synthesized. On the other hand, synthesizing more
expressive barrier certificates often means high complexity. In [9], Kong et al
consid- ered how to relax the condition of barrier certificates while still
keeping their convexity so that one can synthesize more expressive barrier
certificates efficiently using semi-definite programming (SDP). In this paper,
we first discuss how to relax the condition of barrier certificates in a
general way, while still keeping their convexity. Particularly, one can then
utilize different weaker conditions flexibly to synthesize dif- ferent kinds of
barrier certificates with more expressiveness efficiently using SDP. These
barriers give more opportuni- ties to verify the considered system. We also
show how to combine two functions together to form a combined barrier
certificate in order to prove a safety property under consid- eration, whereas
neither of them can be used as a barrier certificate separately, even according
to any relaxed condi- tion. Another contribution of this paper is that we
discuss how to discover certificates from the general relaxed condi- tion by
SDP. In particular, we focus on how to avoid the unsoundness because of numeric
error caused by SDP with symbolic checking
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6485</identifier>
 <datestamp>2014-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6485</id><created>2013-10-24</created><updated>2014-01-06</updated><authors><author><keyname>Hooshmand</keyname><forenames>Reza</forenames></author></authors><title>Secret Key Cryptosystem based on Non-Systematic Polar Codes</title><categories>cs.CR cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polar codes are a new class of error correcting linear block codes, whose
generator matrix is specified by the knowledge of transmission channel
parameters, code length and code dimension. Moreover, regarding computational
security, it is assumed that an attacker with a restricted processing power has
unlimited access to the transmission media. Therefore, the attacker can
construct the generator matrix of polar codes, especially in the case of Binary
Erasure Channels, on which this matrix can be easily constructed. In this
paper, we introduce a novel method to keep the generator matrix of polar codes
in secret in a way that the attacker cannot access the required information to
decode the intended polar code. With the help of this method, a secret key
cryptosystem is proposed based on non-systematic polar codes. In fact, the main
objective of this study is to achieve an acceptable level of security and
reliability through taking advantage of the special properties of polar codes.
The analyses revealed that our scheme resists the typical attacks on the secret
key cryptosystems based on linear block codes. In addition, by employing some
efficient methods, the key length of the proposed scheme is decreased compared
to that of the previous cryptosystems. Moreover, this scheme enjoys other
advantages including high code rate, and proper error performance as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6486</identifier>
 <datestamp>2013-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6486</id><created>2013-10-24</created><authors><author><keyname>Sergueiva</keyname><forenames>Antoaneta</forenames></author></authors><title>Systemic Risk Identification, Modelling, Analysis, and Monitoring: An
  Integrated Approach</title><categories>cs.CE q-fin.GN</categories><comments>The author is grateful to J Doyne Farmer and Yaneer Bar-Yam for their
  constructive comments and time, and to Kevin James for the opportunity to
  attend and present at the Bank of England seminars on systemic risk and
  financial stability. Would like to thank Jeffrey Johnson for kindly providing
  a copy of his forthcoming book in advance, and to Marzena Rostek for sending
  a recent unpublished</comments><acm-class>G.2.2; I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research capacity is critical in understanding systemic risk and informing
new regulation. Banking regulation has not kept pace with all the complexities
of financial innovation. The academic literature on systemic risk is rapidly
expanding. The majority of papers analyse a single source or a consolidated
source of risk and its effect. A fraction of publications quantify systemic
risk measures or formulate penalties for systemically important financial
institutions that are of practical regulatory relevance. The challenges facing
systemic risk evaluation and regulation still persist, as the definition of
systemic risk is somewhat unsettled and that affects attempts to provide
solutions. Our understanding of systemic risk is evolving and the awareness of
data relevance is rising gradually; this challenge is reflected in the focus of
major international research initiatives. There is a consensus that the direct
and indirect costs of a systemic crisis are enormous as opposed to preventing
it, and that without regulation the externalities will not be prevented; but
there is no consensus yet on the extent and detail of regulation, and research
expectations are to facilitate the regulatory process. This report outlines an
integrated approach for systemic risk evaluation based on multiple types of
interbank exposures through innovative modelling approaches as tensorial
multilayer networks, suggests how to relate underlying economic data and how to
extend the network to cover financial market information. We reason about data
requirements and time scale effects, and outline a multi-model hypernetwork of
systemic risk knowledge as a scenario analysis and policy support tool. The
argument is that logical steps forward would incorporate the range of risk
sources and their interrelated effects as contributions towards an overall
systemic risk indicator, would perform an integral analysis of ...
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6502</identifier>
 <datestamp>2013-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6502</id><created>2013-10-24</created><authors><author><keyname>Zhou</keyname><forenames>Runlin</forenames></author><author><keyname>Shi</keyname><forenames>Yingjie</forenames></author><author><keyname>Zhu</keyname><forenames>Chunge</forenames></author><author><keyname>Liu</keyname><forenames>Fan</forenames></author></authors><title>AxPUE: Application Level Metrics for Power Usage Effectiveness in Data
  Centers</title><categories>cs.DC</categories><comments>8 pages, 4 figures, The First Workshop on Big Data Benchmarks,
  Performance Optimization, and Emerging hardware (BPOE 2013) In conjunction
  with 2013 IEEE International Conference on Big Data (IEEE Big Data 2013)</comments><msc-class>68N99</msc-class><acm-class>D.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rapid growth of data volume brings big challenges to the data center
computing, and energy efficiency is one of the most concerned problems.
Researchers from various fields are now proposing solutions to green the data
center operations. Power usage effectiveness metric plays an important role in
the energy saving research. However, the exising usage effectiveness metrics
focus on measuring the relationship between the total facility energy consumed
and the IT equipment energy consumed, without reflecting the energy efficiency
of applications. In this paper, we analyze the requirements of
application-level metrics for power usage efficiency of the data centers, and
propose two novel energy efficiency metrics to provide strong guidance and
useful insight to data center design and optimization. We conduct comprehensive
experiments in the practical data centers using BigDataBench, a big data
benchmark suite, and the results demonstrate the rationality and efficiency of
AxPUE in measuring the actual computation energy consumption in data centers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6511</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6511</id><created>2013-10-24</created><updated>2014-02-11</updated><authors><author><keyname>Krikidis</keyname><forenames>Ioannis</forenames></author></authors><title>Simultaneous Information and Energy Transfer in Large-Scale Networks
  with/without Relaying</title><categories>cs.IT math.IT</categories><comments>IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy harvesting (EH) from ambient radio-frequency (RF) electromagnetic
waves is an efficient solution for fully autonomous and sustainable
communication networks. Most of the related works presented in the literature
are based on specific (and small-scale) network structures, which although give
useful insights on the potential benefits of the RF-EH technology, cannot
characterize the performance of general networks. In this paper, we adopt a
large-scale approach of the RF-EH technology and we characterize the
performance of a network with random number of transmitter-receiver pairs by
using stochastic-geometry tools. Specifically, we analyze the outage
probability performance and the average harvested energy, when receivers employ
power splitting (PS) technique for &quot;simultaneous&quot; information and energy
transfer. A non-cooperative scheme, where information/energy are conveyed only
via direct links, is firstly considered and the outage performance of the
system as well as the average harvested energy are derived in closed form in
function of the power splitting. For this protocol, an interesting optimization
problem which minimizes the transmitted power under outage probability and
harvesting constraints, is formulated and solved in closed form. In addition,
we study a cooperative protocol where sources' transmissions are supported by a
random number of potential relays that are randomly distributed into the
network. In this case, information/energy can be received at each destination
via two independent and orthogonal paths (in case of relaying). We characterize
both performance metrics, when a selection combining scheme is applied at the
receivers and a single relay is randomly selected for cooperative diversity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6516</identifier>
 <datestamp>2013-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6516</id><created>2013-10-24</created><authors><author><keyname>Picard</keyname><forenames>Willy</forenames></author></authors><title>Simulating the Influence of Collaborative Networks on the Structure of
  Networks of Organizations, Employment Structure, and Organization Value</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>10 pages, 1 figure, conference paper at the 14th IFIP Working
  Conference on Virtual Enterprises, PRO-VE'13, http://www.pro-ve.org/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  From the perspective of reindustrialization, it is important to understand
the evolution of the structure of the network of organizations employment
structure, and organization value. Understanding the potential influence of
collaborative networks (CNs) on these aspects may lead to the development of
appropriate economic policies. In this paper, we propose a theoretical approach
to analysis this potential influence, based on a model of dynamic networked
ecosystem of organizations encompassing collaboration relations among
organization, employment mobility, and organization value. A large number of
simulations has been performed to identify factors influencing the structure of
the network of organizations employment structure, and organization value. The
main findings are that 1) the higher the number of members of CNs, the better
the clustering and the shorter the average path length among organizations; 2)
the constitution of CNs does not affect neither the structure of the network of
organizations, nor the employment structure and the organization value.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6524</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6524</id><created>2013-10-24</created><updated>2014-09-25</updated><authors><author><keyname>Jerrum</keyname><forenames>Mark</forenames></author><author><keyname>Meeks</keyname><forenames>Kitty</forenames></author></authors><title>Some hard families of parameterised counting problems</title><categories>cs.CC cs.DM math.CO</categories><comments>A few more minor changes. This version to appear in the ACM
  Transactions on Computation Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider parameterised subgraph-counting problems of the following form:
given a graph G, how many k-tuples of its vertices have a given property? A
number of such problems are known to be #W[1]-complete; here we substantially
generalise some of these existing results by proving hardness for two large
families of such problems. We demonstrate that it is #W[1]-hard to count the
number of k-vertex subgraphs having any property where the number of distinct
edge-densities of labelled subgraphs that satisfy the property is o(k^2). In
the special case that the property in question depends only on the number of
edges in the subgraph, we give a strengthening of this result which leads to
our second family of hard problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6536</identifier>
 <datestamp>2013-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6536</id><created>2013-10-24</created><authors><author><keyname>Balduzzi</keyname><forenames>David</forenames></author></authors><title>Randomized co-training: from cortical neurons to machine learning and
  back again</title><categories>cs.LG q-bio.NC stat.ML</categories><comments>NIPS workshop: Randomized methods for machine learning</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite its size and complexity, the human cortex exhibits striking
anatomical regularities, suggesting there may simple meta-algorithms underlying
cortical learning and computation. We expect such meta-algorithms to be of
interest since they need to operate quickly, scalably and effectively with
little-to-no specialized assumptions.
  This note focuses on a specific question: How can neurons use vast quantities
of unlabeled data to speed up learning from the comparatively rare labels
provided by reward systems? As a partial answer, we propose randomized
co-training as a biologically plausible meta-algorithm satisfying the above
requirements. As evidence, we describe a biologically-inspired algorithm,
Correlated Nystrom Views (XNV) that achieves state-of-the-art performance in
semi-supervised learning, and sketch work in progress on a neuronal
implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6542</identifier>
 <datestamp>2013-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6542</id><created>2013-10-24</created><updated>2013-10-25</updated><authors><author><keyname>Eggert</keyname><forenames>Michael</forenames></author><author><keyname>H&#xe4;u&#xdf;ling</keyname><forenames>Roger</forenames></author><author><keyname>Henze</keyname><forenames>Martin</forenames></author><author><keyname>Hermerschmidt</keyname><forenames>Lars</forenames></author><author><keyname>Hummen</keyname><forenames>Ren&#xe9;</forenames></author><author><keyname>Kerpen</keyname><forenames>Daniel</forenames></author><author><keyname>P&#xe9;rez</keyname><forenames>Antonio Navarro</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Thi&#xdf;en</keyname><forenames>Dirk</forenames></author><author><keyname>Wehrle</keyname><forenames>Klaus</forenames></author></authors><title>SensorCloud: Towards the Interdisciplinary Development of a Trustworthy
  Platform for Globally Interconnected Sensors and Actuators</title><categories>cs.DC cs.CR cs.CY cs.SE</categories><comments>14 pages, 3 figures, published as technical report of the Department
  of Computer Science of RWTH Aachen University</comments><report-no>AIB-2013-13</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although Cloud Computing promises to lower IT costs and increase users'
productivity in everyday life, the unattractive aspect of this new technology
is that the user no longer owns all the devices which process personal data. To
lower scepticism, the project SensorCloud investigates techniques to understand
and compensate these adoption barriers in a scenario consisting of cloud
applications that utilize sensors and actuators placed in private places. This
work provides an interdisciplinary overview of the social and technical core
research challenges for the trustworthy integration of sensor and actuator
devices with the Cloud Computing paradigm. Most importantly, these challenges
include i) ease of development, ii) security and privacy, and iii) social
dimensions of a cloud-based system which integrates into private life. When
these challenges are tackled in the development of future cloud systems, the
attractiveness of new use cases in a sensor-enabled world will considerably be
increased for users who currently do not trust the Cloud.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6546</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6546</id><created>2013-10-24</created><authors><author><keyname>Quan</keyname><forenames>Jing</forenames></author><author><keyname>Shi</keyname><forenames>Yingjie</forenames></author><author><keyname>Zhao</keyname><forenames>Ming</forenames></author><author><keyname>Yang</keyname><forenames>Wei</forenames></author></authors><title>The Implications from Benchmarking Three Big Data Systems</title><categories>cs.DC cs.PF</categories><comments>8 pages, 10 figures, The First Workshop on Big Data Benchmarks,
  Performance Optimization, and Emerging hardware (BPOE 2013) In conjunction
  with 2013 IEEE International Conference on Big Data (IEEE Big Data 2013)</comments><msc-class>68N99</msc-class><acm-class>B.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Along with today's data explosion and application diversification, a variety
of hardware platforms for big data are emerging, attracting interests from both
industry and academia. The existing hardware platforms represent a wide range
of implementation approaches, and different hardware platforms have different
strengths. In this paper, we conduct comprehensive evaluations on three
representative big data systems: Intel Xeon, Atom (low power processors), and
many-core Tilera using BigDataBench - a big data benchmark suite. Then we
explore the relative performance of the three implementation approaches by
running BigDataBench, and provide strong guidance for the big data systems
construction. Through our experiments, we have inferred that a big data system
based on specific hardware has different performance in the context of
different applications and data volumes. When we construct a system, we should
take into account not only the performance or energy consumption of the pure
hardware, but also the characteristics of applications running on them. Data
scale, application type and complexity should be considered comprehensively
when researchers or architects plan to choose fundamental components for their
big data systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6555</identifier>
 <datestamp>2014-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6555</id><created>2013-10-24</created><updated>2014-01-08</updated><authors><author><keyname>Ciccarese</keyname><forenames>Paolo</forenames></author><author><keyname>Soiland-Reyes</keyname><forenames>Stian</forenames></author><author><keyname>Clark</keyname><forenames>Tim</forenames></author></authors><title>Web Annotation as a First Class Object</title><categories>cs.DL cs.IR</categories><comments>This is authors' Accepted version as of 2013-08-09. For the final,
  published version, see IEEE Explore:
  http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6682930</comments><report-no>uk-ac-man-scw:211608</report-no><acm-class>H.3.5; H.3.7; H.5.4</acm-class><journal-ref>IEEE Internet Computing 2013; 17(6) 71-75</journal-ref><doi>10.1109/MIC.2013.123</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scholars have made handwritten notes and comments in books and manuscripts
for centuries. Today's blogs and news sites typically invite users to express
their opinions on the published content; URLs allow web resources to be shared
with accompanying annotations and comments using third-party services like
Twitter or Facebook. These contributions have until recently been constrained
within specific services, making them second-class citizens of the Web.
  Web Annotations are now emerging as fully independent Linked Data in their
own right, no longer restricted to plain textual comments in application silos.
Annotations can now range from bookmarks and comments, to fine-grained
annotations of a selection of, for example, a section of a frame within a video
stream. Technologies and standards now exist to create, publish, syndicate,
mash-up and consume, finely targeted, semantically rich digital annotations on
practically any content, as first-class Web citizens. This development is being
driven by the need for collaboration and annotation reuse amongst domain
researchers, computer scientists, scientific publishers, and scholarly content
databases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6564</identifier>
 <datestamp>2013-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6564</id><created>2013-10-24</created><authors><author><keyname>Oza</keyname><forenames>Nilay</forenames><affiliation>University of Helsinki, Finland</affiliation></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames><affiliation>University of Helsinki, Finland</affiliation></author><author><keyname>Garbajosa</keyname><forenames>Juan</forenames><affiliation>Universidad Politecnica Madrid, Spain</affiliation></author><author><keyname>Yague</keyname><forenames>Agustin</forenames><affiliation>Indra Software Labs, Spain</affiliation></author><author><keyname>Ortega</keyname><forenames>Eloy Gonzalez</forenames><affiliation>Indra Software Labs, Spain</affiliation></author></authors><title>Identifying Potential Risks and Benefits of Using Cloud in Distributed
  Software Development</title><categories>cs.SE cs.DC</categories><comments>11 pages, 2 figures. The final publication is available at
  link.springer.com. Link:
  http://link.springer.com/chapter/10.1007%2F978-3-642-39259-7_19
  DOI:10.1007/978-3-642-39259-7_19</comments><acm-class>D.2; D.2.9; K.6; K.6.1; K.6.3</acm-class><journal-ref>Proceedings of the 14th International Conference on
  Product-Focused Software Development and Process Improvement (PROFES 2013),
  LNCS 7983, Springer-Verlag, pp. 229-239, 2013</journal-ref><doi>10.1007/978-3-642-39259-7_19</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud-based infrastructure has been increasingly adopted by the industry in
distributed software development (DSD) environments. Its proponents claim that
its several benefits include reduced cost, increased speed and greater
productivity in software development. Empirical evaluations, however, are in
the nascent stage of examining both the benefits and the risks of cloud-based
infrastructure. The objective of this paper is to identify potential benefits
and risks of using cloud in a DSD project conducted by teams based in Helsinki
and Madrid. A cross-case qualitative analysis is performed based on focus
groups conducted at the Helsinki and Madrid sites. Participants' observations
are used to supplement the analysis. The results of the analysis indicated that
the main benefits of using cloud are rapid development, continuous integration,
cost savings, code sharing, and faster ramp-up. The key risks determined by the
project are dependencies, unavailability of access to the cloud, code
commitment and integration, technical debt, and additional support costs. The
results revealed that if such environments are not planned and set up
carefully, the benefits of using cloud in DSD projects might be overshadowed by
the risks associated with it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6572</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6572</id><created>2013-10-24</created><updated>2015-01-08</updated><authors><author><keyname>Cain</keyname><forenames>Alan J.</forenames></author><author><keyname>Gray</keyname><forenames>Robert D.</forenames></author><author><keyname>Malheiro</keyname><forenames>Ant&#xf3;nio</forenames></author></authors><title>Rewriting systems and biautomatic structures for Chinese, hypoplactic,
  and sylvester monoids</title><categories>math.GR cs.FL</categories><comments>27 pages; 3 figures. Minor revision to fix typos and update
  references</comments><msc-class>20M05 (Primary) 68Q45, 16S36 (Secondary)</msc-class><doi>10.1142/S0218196715400044</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies complete rewriting systems and biautomaticity for three
interesting classes of finite-rank homogeneous monoids: Chinese monoids,
hypoplactic monoids, and sylvester monoids. For Chinese monoids, we first give
new presentations via finite complete rewriting systems, using more lucid
constructions and proofs than those given independently by Chen &amp; Qui and
G\&quot;uzel Karpuz; we then construct biautomatic structures. For hypoplactic
monoids, we construct finite complete rewriting systems and biautomatic
structures. For sylvester monoids, which are not finitely presented, we prove
that the standard presentation is an infinite complete rewriting system, and
construct biautomatic structures. Consequently, the monoid algebras
corresponding to monoids of these classes are automaton algebras in the sense
of Ufnarovskij.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6592</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6592</id><created>2013-10-24</created><updated>2016-01-28</updated><authors><author><keyname>Liu</keyname><forenames>Xi</forenames></author><author><keyname>Gong</keyname><forenames>Li</forenames></author><author><keyname>Gong</keyname><forenames>Yongxi</forenames></author><author><keyname>Liu</keyname><forenames>Yu</forenames></author></authors><title>Revealing travel patterns and city structure with taxi trip data</title><categories>physics.soc-ph cs.SI</categories><comments>24 pages, 15 figures</comments><journal-ref>Journal of Transport Geography, 2015, 43, 78-90</journal-ref><doi>10.1016/j.jcrysgro.2015.01.022</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detecting regional spatial structures based on spatial interactions is
crucial in applications ranging from urban planning to traffic control. In the
big data era, various movement trajectories are available for studying spatial
structures. This research uses large scale Shanghai taxi trip data extracted
from GPS-enabled taxi trajectories to reveal traffic flow patterns and urban
structure of the city. Using the network science methods, 15 temporally stable
regions reflecting the scope of people's daily travels are found using
community detection method on the network built from short trips, which
represent residents' daily intra-urban travels and exhibit a clear pattern. In
each region, taxi traffic flows are dominated by a few 'hubs' and 'hubs' in
suburbs impact more trips than 'hubs' in urban areas. Land use conditions in
urban regions are different from those in suburban areas. Additionally, 'hubs'
in urban area associate with office buildings and commercial areas more,
whereas residential land use is more common in suburban hubs. The taxi flow
structures and land uses reveal the polycentric and layered concentric
structure of Shanghai. Finally, according to the temporal variations of taxi
flows and the diversity levels of taxi trip lengths, we explore the total taxi
traffic properties of each region and proved the city structure we find.
External trips across regions also take large proportion of the total traffic
in each region, especially in suburbs. The results could help transportation
policy making and shed light on the way to reveal urban structures with big
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6635</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6635</id><created>2013-10-24</created><updated>2013-12-22</updated><authors><author><keyname>Cloud</keyname><forenames>Jason</forenames></author><author><keyname>Leith</keyname><forenames>Douglas</forenames></author><author><keyname>Medard</keyname><forenames>Muriel</forenames></author></authors><title>Network Coded TCP (CTCP) Performance over Satellite Networks</title><categories>cs.NI</categories><comments>4 pages, 4 figures, Accepted at SPACOMM 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show preliminary results for the performance of Network Coded TCP (CTCP)
over large latency networks. While CTCP performs very well in networks with
relatively short RTT, the slow-start mechanism currently employed does not
adequately fill the available bandwidth when the RTT is large. Regardless, we
show that CTCP still outperforms current TCP variants (i.e., Cubic TCP and
Hybla TCP) for high packet loss rates (e.g., &gt;2.5%). We then explore the
possibility of a modified congestion control mechanism based off of H-TCP that
opens the congestion window quickly to overcome the challenges of large latency
networks. Preliminary results are provided that show the combination of network
coding with an appropriate congestion control algorithm can provide gains on
the order of 20 times that of existing TCP variants. Finally, we provide a
discussion of the future work needed to increase CTCP's performance in these
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6637</identifier>
 <datestamp>2013-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6637</id><created>2013-10-24</created><authors><author><keyname>YesuRaju</keyname><forenames>P</forenames></author><author><keyname>KiranSree</keyname><forenames>P</forenames></author></authors><title>A language independent web data extraction using vision based page
  segmentation algorithm</title><categories>cs.IR</categories><comments>arXiv admin note: text overlap with arXiv:1201.0385 by other authors
  without attribution</comments><journal-ref>IJRET APR 2013,Volume: 2 Issue: 4,635 - 639,ISSN: 2319 - 1163</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web usage mining is a process of extracting useful information from server
logs i.e. users history. Web usage mining is a process of finding out what
users are looking for on the internet. Some users might be looking at only
textual data, where as some others might be interested in multimedia data. One
would retrieve the data by copying it and pasting it to the relevant document.
But this is tedious and time consuming as well as difficult when the data to be
retrieved is plenty. Extracting structured data from a web page is challenging
problem due to complicated structured pages. Earlier they were used web page
programming language dependent; the main problem is to analyze the html source
code. In earlier they were considered the scripts such as java scripts and
cascade styles in the html files. When it makes different for existing
solutions to infer the regularity of the structure of the Web Pages only by
analyzing the tag structures. To overcome this problem we are using a new
algorithm called VIPS algorithm i.e. independent language. This approach
primary utilizes the visual features on the webpage to implement web data
extraction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6650</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6650</id><created>2013-10-24</created><updated>2014-01-10</updated><authors><author><keyname>Chen</keyname><forenames>Kai</forenames></author><author><keyname>Niu</keyname><forenames>Kai</forenames></author><author><keyname>He</keyname><forenames>Zhiqiang</forenames></author><author><keyname>Lin</keyname><forenames>Jiaru</forenames></author></authors><title>Polar Coded HARQ Scheme with Chase Combining</title><categories>cs.IT math.IT</categories><comments>Accepted by WCNC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A hybrid automatic repeat request scheme with Chase combing (HARQ-CC) of
polar codes is proposed. The existing analysis tools of the underlying
rate-compatible punctured polar (RCPP) codes for additive white Gaussian noise
(AWGN) channels are extended to Rayleigh fading channels. Then, an
approximation bound of the throughput efficiency for the polar coded HARQ-CC
scheme is derived. Utilizing this bound, the parameter configurations of the
proposed scheme can be optimized. Simulation results show that, the proposed
HARQ-CC scheme under a low-complexity SC decoding is only about $1.0$dB away
from the existing schemes with incremental redundancy (\mbox{HARQ-IR}).
Compared with the polar coded \mbox{HARQ-IR} scheme, the proposed HARQ-CC
scheme requires less retransmissions and has the advantage of good
compatibility to other communication techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6654</identifier>
 <datestamp>2013-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6654</id><created>2013-10-24</created><authors><author><keyname>Sikka</keyname><forenames>Sahil</forenames></author><author><keyname>Sikka</keyname><forenames>Karan</forenames></author><author><keyname>Bhuyan</keyname><forenames>M. K.</forenames></author><author><keyname>Iwahori</keyname><forenames>Yuji</forenames></author></authors><title>Pseudo vs. True Defect Classification in Printed Circuits Boards using
  Wavelet Features</title><categories>cs.CV</categories><comments>6 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, Printed Circuit Boards (PCB) have become the backbone of a
large number of consumer electronic devices leading to a surge in their
production. This has made it imperative to employ automatic inspection systems
to identify manufacturing defects in PCB before they are installed in the
respective systems. An important task in this regard is the classification of
defects as either true or pseudo defects, which decides if the PCB is to be
re-manufactured or not. This work proposes a novel approach to detect most
common defects in the PCBs. The problem has been approached by employing highly
discriminative features based on multi-scale wavelet transform, which are
further boosted by using a kernalized version of the support vector machines
(SVM). A real world printed circuit board dataset has been used for
quantitative analysis. Experimental results demonstrated the efficacy of the
proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6657</identifier>
 <datestamp>2013-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6657</id><created>2013-10-24</created><authors><author><keyname>Hao</keyname><forenames>Chenxi</forenames></author><author><keyname>Clerckx</keyname><forenames>Bruno</forenames></author></authors><title>MISO Broadcast Channel with Imperfect and (Un)matched CSIT in the
  Frequency Domain: DoF Region and Transmission Strategies</title><categories>cs.IT math.IT</categories><comments>published in IEEE International Symposium on Personal, Indoor and
  Mobile Radio Communications (PIMRC) 2013, London, United Kingdom, Sept. 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this contribution, we focus on a frequency domain two-user
Multiple-Input-Single-Output Broadcast Channel (MISO BC) where the transmitter
has imperfect and (un)matched Channel State Information (CSI) of the two users
in two subbands. We provide an upper-bound to the Degrees-of-Freedom (DoF)
region, which is tight compared to the state of the art. By decomposing the
subbands into subchannels according to the CSI feedback qualities, we interpret
the DoF region as the weighted-sum of that in each subchannel. Moreover, we
study the sum \emph{DoF} loss when employing sub-optimal schemes, namely
Frequency Division Multiple Access (FDMA), Zero-Forcing Beamforming (ZFBF) and
the $S_3^{3/2}$ scheme proposed by Tandon et al. The results show that by
switching among the sub-optimal strategies, we can obtain at least 80% and
66.7% of the optimal sum \emph{DoF} performance for the unmatched and matched
CSIT scenario respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6669</identifier>
 <datestamp>2013-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6669</id><created>2013-10-24</created><authors><author><keyname>Hao</keyname><forenames>Chenxi</forenames></author><author><keyname>Rassouli</keyname><forenames>Borzoo</forenames></author><author><keyname>Clerckx</keyname><forenames>Bruno</forenames></author></authors><title>Degrees-of-Freedom Region of MISO-OFDMA Broadcast Channel with Imperfect
  CSIT</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This contribution investigates the Degrees-of-Freedom region of a two-user
frequency correlated Multiple-Input-Single-Output (MISO) Broadcast Channel (BC)
with imperfect Channel State Information at the transmitter (CSIT). We assume
that the system consists of an arbitrary number of subbands, denoted as $L$.
Besides, the CSIT state varies across users and subbands. A tight outer-bound
is found as a function of the minimum average CSIT quality between the two
users. Based on the CSIT states across the subbands, the DoF region is
interpreted as a weighted sum of the optimal DoF regions in the scenarios where
the CSIT of both users are perfect, alternatively perfect and not known.
Inspired by the weighted-sum interpretation and identifying the benefit of the
optimal scheme for the unmatched CSIT proposed by Chen et al., we also design a
scheme achieving the upper-bound for the general $L$-subband scenario in
frequency domain BC, thus showing the optimality of the DoF region.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6670</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6670</id><created>2013-10-24</created><authors><author><keyname>Bellettini</keyname><forenames>Carlo</forenames></author><author><keyname>Camilli</keyname><forenames>Matteo</forenames></author><author><keyname>Capra</keyname><forenames>Lorenzo</forenames></author><author><keyname>Monga</keyname><forenames>Mattia</forenames></author></authors><title>Distributed CTL Model Checking in the Cloud</title><categories>cs.SE cs.DC cs.LO</categories><comments>8 pages</comments><doi>10.1109/SYNASC.2014.52</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent extensive availability of &quot;big data&quot; platforms calls for a more
widespread adoption by the formal verification community. In fact, formal
verification requires high performance data processing software for extracting
knowledge from the unprecedented amount of data which come from analyzed
systems. Since cloud based computing resources have became easily accessible,
there is an opportunity for verification techniques and tools to undergo a deep
technological transition to exploit the new available architectures. This has
created an increasing interest in parallelizing and distributing verification
techniques. In this paper we introduce a distributed approach which exploits
techniques typically used by the &quot;big data&quot; community to enable verification of
Computation Tree Logic (CTL) formulas on very large state spaces using
distributed systems and cloud computing facilities. The outcome of several
tests performed on benchmark specifications are presented, thus showing the
convenience of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6672</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6672</id><created>2013-10-24</created><updated>2013-10-25</updated><authors><author><keyname>Cooper</keyname><forenames>Jeff</forenames></author><author><keyname>Dutta</keyname><forenames>Kunal</forenames></author><author><keyname>Mubayi</keyname><forenames>Dhruv</forenames></author></authors><title>Counting independent sets in hypergraphs</title><categories>math.CO cs.DM</categories><doi>10.1017/S0963548314000182</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G$ be a triangle-free graph with $n$ vertices and average degree $t$. We
show that $G$ contains at least \[ e^{(1-n^{-1/12})\frac{1}{2}\frac{n}{t}\ln t
(\frac{1}{2}\ln t-1)} \] independent sets. This improves a recent result of the
first and third authors \cite{countingind}. In particular, it implies that as
$n \to \infty$, every triangle-free graph on $n$ vertices has at least
$e^{(c_1-o(1)) \sqrt{n} \ln n}$ independent sets, where $c_1 = \sqrt{\ln 2}/4 =
0.208138..$. Further, we show that for all $n$, there exists a triangle-free
graph with $n$ vertices which has at most $e^{(c_2+o(1))\sqrt{n}\ln n}$
independent sets, where $c_2 = 1+\ln 2 = 1.693147..$. This disproves a
conjecture from \cite{countingind}.
  Let $H$ be a $(k+1)$-uniform linear hypergraph with $n$ vertices and average
degree $t$. We also show that there exists a constant $c_k$ such that the
number of independent sets in $H$ is at least \[ e^{c_{k}
\frac{n}{t^{1/k}}\ln^{1+1/k}{t}}. \] This is tight apart from the constant
$c_k$ and generalizes a result of Duke, Lefmann, and R\&quot;odl
\cite{uncrowdedrodl}, which guarantees the existence of an independent set of
size $\Omega(\frac{n}{t^{1/k}} \ln^{1/k}t)$. Both of our lower bounds follow
from a more general statement, which applies to hereditary properties of
hypergraphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6674</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6674</id><created>2013-10-24</created><updated>2014-05-01</updated><authors><author><keyname>Yin</keyname><forenames>Haifan</forenames></author><author><keyname>Gesbert</keyname><forenames>David</forenames></author><author><keyname>Cottatellucci</keyname><forenames>Laura</forenames></author></authors><title>Dealing with Interference in Distributed Large-scale MIMO Systems: A
  Statistical Approach</title><categories>cs.IT math.IT</categories><comments>12 pages, 11 figures, to appear in IEEE Journal of Selected Topics in
  Signal Processing</comments><doi>10.1109/JSTSP.2014.2322583</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of interference control through the use of
second-order statistics in massive MIMO multi-cell networks. We consider both
the cases of co-located massive arrays and large-scale distributed antenna
settings. We are interested in characterizing the low-rankness of users'
channel covariance matrices, as such a property can be exploited towards
improved channel estimation (so-called pilot decontamination) as well as
interference rejection via spatial filtering. In previous work, it was shown
that massive MIMO channel covariance matrices exhibit a useful finite rank
property that can be modeled via the angular spread of multipath at a MIMO
uniform linear array. This paper extends this result to more general settings
including certain non-uniform arrays, and more surprisingly, to two dimensional
distributed large scale arrays. In particular our model exhibits the dependence
of the signal subspace's richness on the scattering radius around the user
terminal, through a closed form expression. The applications of the
low-rankness covariance property to channel estimation's denoising and
low-complexity interference filtering are highlighted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6675</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6675</id><created>2013-10-24</created><updated>2013-10-25</updated><authors><author><keyname>Trodden</keyname><forenames>P. A.</forenames></author><author><keyname>Bukhsh</keyname><forenames>W. A.</forenames></author><author><keyname>Grothey</keyname><forenames>A.</forenames></author><author><keyname>McKinnon</keyname><forenames>K. I. M.</forenames></author></authors><title>Optimization-based Islanding of Power Networks using Piecewise Linear AC
  Power Flow</title><categories>math.OC cs.SY</categories><comments>19 pages, only change is reformatting</comments><journal-ref>IEEE Transactions on Power Systems 29(3) (2014) 1212-1220</journal-ref><doi>10.1109/TPWRS.2013.2291660</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a flexible optimization-based framework for intentional
islanding is presented. The decision is made of which transmission lines to
switch in order to split the network while minimizing disruption, the amount of
load shed, or grouping coherent generators. The approach uses a piecewise
linear model of AC power flow, which allows the voltage and reactive power to
be considered directly when designing the islands. Demonstrations on standard
test networks show that solution of the problem provides islands that are
balanced in real and reactive power, satisfy AC power flow laws, and have a
healthy voltage profile.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6686</identifier>
 <datestamp>2013-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6686</id><created>2013-10-24</created><authors><author><keyname>Ju&#xe1;rez-Ram&#xed;rez</keyname><forenames>Reyes</forenames></author><author><keyname>Verd&#xed;n</keyname><forenames>Karen Cort&#xe9;s</forenames></author><author><keyname>de la Torre</keyname><forenames>Beatriz Ang&#xe9;lica Toscano</forenames></author><author><keyname>Oktaba</keyname><forenames>Hanna</forenames></author><author><keyname>Fern&#xe1;ndez-y-Fern&#xe1;ndez</keyname><forenames>Carlos Alberto</forenames></author><author><keyname>R&#xed;os</keyname><forenames>Brenda Leticia Flores</forenames></author><author><keyname>Molina</keyname><forenames>Fabiola Angulo</forenames></author></authors><title>Estado Actual de la Pr\'actica de la Ingenier\'ia de Software en
  M\'exico</title><categories>cs.SE</categories><journal-ref>Congreso Internacional de Investigaci\'on e Innovaci\'on en
  Ingenier\'ia de Software (Conisoft 2013), pp. 3-14, Xalapa, Veracruz,
  M\'exico, 2013. ISBN: 978-0-615-89523-9</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The software engineering is a relatively new discipline compared to other
sciences, since the origins of the term itself dates back to the years 1968 and
1969. At present, the market and the software industry have a significant
relevance in several countries of the world; however, although Mexico is
immersed in this race, has not even reached the level of success achieved in
other countries in this sector. This paper presents an overview of the
situation that keeps the practice of software engineering in Mexico, with
emphasis on the academic realm. It shows a compilation of scientific research
activity carried out in universities, as well as a brief analysis of
undergraduate educational programs including the software engineering
discipline . At the end, future work to be done is proposed in order to find a
point of convergence between academia and industry, and also to support the
flourishing of this business which somehow will have a positive impact on the
economy of our country.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6704</identifier>
 <datestamp>2013-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6704</id><created>2013-10-24</created><authors><author><keyname>Vinyals</keyname><forenames>Meritxell</forenames></author><author><keyname>Voice</keyname><forenames>Thomas</forenames></author><author><keyname>Ramchurn</keyname><forenames>Sarvapali</forenames></author><author><keyname>Jennings</keyname><forenames>Nicholas R.</forenames></author></authors><title>A Hierarchical Dynamic Programming Algorithm for Optimal Coalition
  Structure Generation</title><categories>cs.MA</categories><acm-class>I.2; F.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new Dynamic Programming (DP) formulation of the Coalition
Structure Generation (CSG) problem based on imposing a hierarchical
organizational structure over the agents. We show the efficiency of this
formulation by deriving DyPE, a new optimal DP algorithm which significantly
outperforms current DP approaches in speed and memory usage. In the classic
case, in which all coalitions are feasible, DyPE has half the memory
requirements of other DP approaches. On graph-restricted CSG, in which
feasibility is restricted by a (synergy) graph, DyPE has either the same or
lower computational complexity depending on the underlying graph structure of
the problem. Our empirical evaluation shows that DyPE outperforms the
state-of-the-art DP approaches by several orders of magnitude in a large range
of graph structures (e.g. for certain scalefree graphs DyPE reduces the memory
requirements by $10^6$ and solves problems that previously needed hours in
minutes).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6718</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6718</id><created>2013-10-24</created><updated>2014-06-13</updated><authors><author><keyname>Bouland</keyname><forenames>Adam</forenames></author><author><keyname>Aaronson</keyname><forenames>Scott</forenames></author></authors><title>Generation of Universal Linear Optics by Any Beamsplitter</title><categories>quant-ph cs.CC</categories><comments>14 pages; edited Lemma 3.3 and updated references. Results are
  unchanged</comments><journal-ref>Phys. Rev. A 89, 062316 (2014)</journal-ref><doi>10.1103/PhysRevA.89.062316</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 1994, Reck et al. showed how to realize any unitary transformation on a
single photon using a product of beamsplitters and phaseshifters. Here we show
that any single beamsplitter that nontrivially mixes two modes, also densely
generates the set of unitary transformations (or orthogonal transformations, in
the real case) on the single-photon subspace with m&gt;=3 modes. (We prove the
same result for any two-mode real optical gate, and for any two-mode optical
gate combined with a generic phaseshifter.) Experimentally, this means that one
does not need tunable beamsplitters or phaseshifters for universality: any
nontrivial beamsplitter is universal for linear optics. Theoretically, it means
that one cannot produce &quot;intermediate&quot; models of linear optical computation
(analogous to the Clifford group for qubits) by restricting the allowed
beamsplitters and phaseshifters: there is a dichotomy; one either gets a
trivial set or else a universal set. No similar classification theorem for
gates acting on qubits is currently known. We leave open the problem of
classifying optical gates that act on three or more modes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6719</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6719</id><created>2013-10-24</created><updated>2014-04-17</updated><authors><author><keyname>Patole</keyname><forenames>Sujeet</forenames></author><author><keyname>Torlak</keyname><forenames>Murat</forenames></author></authors><title>Two Dimensional Array Imaging with Beam Steered Data</title><categories>cs.CV cs.IT math.IT stat.AP</categories><report-no>ISSN: 1057-7149</report-no><journal-ref>IEEE Transactions on Image Processing Dec. 2013 (Volume:22, Issue:
  12, Page(s):5181 - 5189 )</journal-ref><doi>10.1109/TIP.2013.2282115</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses different approaches used for millimeter wave imaging of
two-dimensional objects. Imaging of a two dimensional object requires reflected
wave data to be collected across two distinct dimensions. In this paper, we
propose a reconstruction method that uses narrowband waveforms along with two
dimensional beam steering. The beam is steered in azimuthal and elevation
direction, which forms the two distinct dimensions required for the
reconstruction. The Reconstruction technique uses inverse Fourier transform
along with amplitude and phase correction factors. In addition, this
reconstruction technique does not require interpolation of the data in either
wavenumber or spatial domain. Use of the two dimensional beam steering offers
better performance in the presence of noise compared with the existing methods,
such as switched array imaging system. Effects of RF impairments such as
quantization of the phase of beam steering weights and timing jitter which add
to phase noise, are analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6731</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6731</id><created>2013-10-24</created><updated>2014-06-13</updated><authors><author><keyname>Russell</keyname><forenames>Benjamin</forenames></author><author><keyname>Stepney</keyname><forenames>Susan</forenames></author></authors><title>Zermelo Navigation and a Speed Limit to Quantum Information Processing</title><categories>quant-ph cs.ET</categories><comments>7 pages, accepted by Phys.Rev.A</comments><journal-ref>Phys. Rev. A 90, 012303 (2014)</journal-ref><doi>10.1103/PhysRevA.90.012303</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use a specific geometric method to determine speed limits to the
implementation of quantum gates in controlled quantum systems that have a
specific class of constrained control functions. We achieve this by applying a
recent theorem of Shen, which provides a connection between time optimal
navigation on Riemannian manifolds and the geodesics of a certain Finsler
metric of Randers type. We use the lengths of these geodesics to derive the
optimal implementation times (under the assumption of constant control fields)
for an arbitrary quantum operation (on a finite dimensional Hilbert space), and
explicitly calculate the result for the case of a controlled single spin system
in a magnetic field, and a swap gate in a Heisenberg spin chain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6736</identifier>
 <datestamp>2013-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6736</id><created>2013-10-23</created><authors><author><keyname>Thota</keyname><forenames>Rahul</forenames></author><author><keyname>Vaswani</keyname><forenames>Sharan</forenames></author><author><keyname>Kale</keyname><forenames>Amit</forenames></author><author><keyname>Vydyanathan</keyname><forenames>Nagavijayalakshmi</forenames></author></authors><title>Fast 3D Salient Region Detection in Medical Images using GPUs</title><categories>cs.CV</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automated detection of visually salient regions is an active area of research
in computer vision. Salient regions can serve as inputs for object detectors as
well as inputs for region based registration algorithms. In this paper we
consider the problem of speeding up computationally intensive bottom-up salient
region detection in 3D medical volumes.The method uses the Kadir Brady
formulation of saliency. We show that in the vicinity of a salient region,
entropy is a monotonically increasing function of the degree of overlap of a
candidate window with the salient region. This allows us to initialize a sparse
seed-point grid as the set of tentative salient region centers and iteratively
converge to the local entropy maxima, thereby reducing the computation
complexity compared to the Kadir Brady approach of performing this computation
at every point in the image. We propose two different approaches for achieving
this. The first approach involves evaluating entropy in the four quadrants
around the seed point and iteratively moving in the direction that increases
entropy. The second approach we propose makes use of mean shift tracking
framework to affect entropy maximizing moves. Specifically, we propose the use
of uniform pmf as the target distribution to seek high entropy regions. We
demonstrate the use of our algorithm on medical volumes for left ventricle
detection in PET images and tumor localization in brain MR sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6740</identifier>
 <datestamp>2013-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6740</id><created>2013-10-24</created><authors><author><keyname>Garnett</keyname><forenames>Roman</forenames></author><author><keyname>Osborne</keyname><forenames>Michael A.</forenames></author><author><keyname>Hennig</keyname><forenames>Philipp</forenames></author></authors><title>Active Learning of Linear Embeddings for Gaussian Processes</title><categories>stat.ML cs.LG</categories><msc-class>68T05</msc-class><acm-class>I.2.6; I.5.2; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an active learning method for discovering low-dimensional
structure in high-dimensional Gaussian process (GP) tasks. Such problems are
increasingly frequent and important, but have hitherto presented severe
practical difficulties. We further introduce a novel technique for
approximately marginalizing GP hyperparameters, yielding marginal predictions
robust to hyperparameter mis-specification. Our method offers an efficient
means of performing GP regression, quadrature, or Bayesian optimization in
high-dimensional spaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6749</identifier>
 <datestamp>2013-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6749</id><created>2013-10-24</created><authors><author><keyname>Schwarz</keyname><forenames>Martin</forenames></author><author><keyname>Nest</keyname><forenames>Maarten Van den</forenames></author></authors><title>Simulating Quantum Circuits with Sparse Output Distributions</title><categories>quant-ph cs.CC</categories><comments>21 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that several quantum circuit families can be simulated efficiently
classically if it is promised that their output distribution is approximately
sparse i.e. the distribution is close to one where only a polynomially small, a
priori unknown subset of the measurement probabilities are nonzero. Classical
simulations are thereby obtained for quantum circuits which---without the
additional sparsity promise---are considered hard to simulate. Our results
apply in particular to a family of Fourier sampling circuits (which have
structural similarities to Shor's factoring algorithm) but also to several
other circuit families, such as IQP circuits. Our results provide examples of
quantum circuits that cannot achieve exponential speed-ups due to the presence
of too much destructive interference i.e. too many cancelations of amplitudes.
The crux of our classical simulation is an efficient algorithm for
approximating the significant Fourier coefficients of a class of states called
computationally tractable states. The latter result may have applications
beyond the scope of this work. In the proof we employ and extend sparse
approximation techniques, in particular the Kushilevitz-Mansour algorithm, in
combination with probabilistic simulation methods for quantum circuits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6753</identifier>
 <datestamp>2013-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6753</id><created>2013-10-24</created><authors><author><keyname>Backstrom</keyname><forenames>Lars</forenames></author><author><keyname>Kleinberg</keyname><forenames>Jon</forenames></author></authors><title>Romantic Partnerships and the Dispersion of Social Ties: A Network
  Analysis of Relationship Status on Facebook</title><categories>cs.SI physics.soc-ph</categories><comments>Proc. 17th ACM Conference on Computer Supported Cooperative Work and
  Social Computing (CSCW), 2014</comments><acm-class>H.2.8</acm-class><doi>10.1145/2531602.2531642</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A crucial task in the analysis of on-line social-networking systems is to
identify important people --- those linked by strong social ties --- within an
individual's network neighborhood. Here we investigate this question for a
particular category of strong ties, those involving spouses or romantic
partners. We organize our analysis around a basic question: given all the
connections among a person's friends, can you recognize his or her romantic
partner from the network structure alone? Using data from a large sample of
Facebook users, we find that this task can be accomplished with high accuracy,
but doing so requires the development of a new measure of tie strength that we
term `dispersion' --- the extent to which two people's mutual friends are not
themselves well-connected. The results offer methods for identifying types of
structurally significant people in on-line applications, and suggest a
potential expansion of existing theories of tie strength.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6767</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6767</id><created>2013-10-24</created><authors><author><keyname>Girdhar</keyname><forenames>Yogesh</forenames></author><author><keyname>Whitney</keyname><forenames>David</forenames></author><author><keyname>Dudek</keyname><forenames>Gregory</forenames></author></authors><title>Curiosity Based Exploration for Learning Terrain Models</title><categories>cs.RO</categories><comments>7 pages, 5 figures, submitted to ICRA 2014</comments><doi>10.1109/ICRA.2014.6906913</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a robotic exploration technique in which the goal is to learn to a
visual model and be able to distinguish between different terrains and other
visual components in an unknown environment. We use ROST, a realtime online
spatiotemporal topic modeling framework to model these terrains using the
observations made by the robot, and then use an information theoretic path
planning technique to define the exploration path. We conduct experiments with
aerial view and underwater datasets with millions of observations and varying
path lengths, and find that paths that are biased towards locations with high
topic perplexity produce better terrain models with high discriminative power,
especially with paths of length close to the diameter of the world.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6772</identifier>
 <datestamp>2013-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6772</id><created>2013-10-24</created><authors><author><keyname>Solorio</keyname><forenames>Thamar</forenames></author><author><keyname>Hasan</keyname><forenames>Ragib</forenames></author><author><keyname>Mizan</keyname><forenames>Mainul</forenames></author></authors><title>Sockpuppet Detection in Wikipedia: A Corpus of Real-World Deceptive
  Writing for Linking Identities</title><categories>cs.CL cs.CR cs.CY</categories><comments>4 pages, under submission at LREC 2014</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper describes the corpus of sockpuppet cases we gathered from
Wikipedia. A sockpuppet is an online user account created with a fake identity
for the purpose of covering abusive behavior and/or subverting the editing
regulation process. We used a semi-automated method for crawling and curating a
dataset of real sockpuppet investigation cases. To the best of our knowledge,
this is the first corpus available on real-world deceptive writing. We describe
the process for crawling the data and some preliminary results that can be used
as baseline for benchmarking research. The dataset will be released under a
Creative Commons license from our project website: http://docsig.cis.uab.edu.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6775</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6775</id><created>2013-10-24</created><authors><author><keyname>Vepstas</keyname><forenames>Linas</forenames></author></authors><title>Durkheim Project Data Analysis Report</title><categories>cs.AI cs.CL cs.LG</categories><comments>43 pages, to appear as appendix of primary science publication
  Poulin, et al &quot;Predicting the risk of suicide by analyzing the text of
  clinical notes&quot;</comments><acm-class>I.2.1; I.2.2; I.2.7; J.4</acm-class><doi>10.1371/journal.pone.0085733.s001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report describes the suicidality prediction models created under the
DARPA DCAPS program in association with the Durkheim Project
[http://durkheimproject.org/]. The models were built primarily from
unstructured text (free-format clinician notes) for several hundred patient
records obtained from the Veterans Health Administration (VHA). The models were
constructed using a genetic programming algorithm applied to bag-of-words and
bag-of-phrases datasets. The influence of additional structured data was
explored but was found to be minor. Given the small dataset size,
classification between cohorts was high fidelity (98%). Cross-validation
suggests these models are reasonably predictive, with an accuracy of 50% to 69%
on five rotating folds, with ensemble averages of 58% to 67%. One particularly
noteworthy result is that word-pairs can dramatically improve classification
accuracy; but this is the case only when one of the words in the pair is
already known to have a high predictive value. By contrast, the set of all
possible word-pairs does not improve on a simple bag-of-words model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6780</identifier>
 <datestamp>2014-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6780</id><created>2013-10-24</created><updated>2014-10-22</updated><authors><author><keyname>Mukherjee</keyname><forenames>Arko Provo</forenames></author><author><keyname>Xu</keyname><forenames>Pan</forenames></author><author><keyname>Tirthapura</keyname><forenames>Srikanta</forenames></author></authors><title>Mining Maximal Cliques from an Uncertain Graph</title><categories>cs.DS cs.DB</categories><comments>ICDE 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider mining dense substructures (maximal cliques) from an uncertain
graph, which is a probability distribution on a set of deterministic graphs.
For parameter 0 &lt; {\alpha} &lt; 1, we present a precise definition of an
{\alpha}-maximal clique in an uncertain graph. We present matching upper and
lower bounds on the number of {\alpha}-maximal cliques possible within an
uncertain graph. We present an algorithm to enumerate {\alpha}-maximal cliques
in an uncertain graph whose worst-case runtime is near-optimal, and an
experimental evaluation showing the practical utility of the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6795</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6795</id><created>2013-10-24</created><updated>2014-11-02</updated><authors><author><keyname>Gupta</keyname><forenames>Abhishek K.</forenames></author><author><keyname>Dhillon</keyname><forenames>Harpreet S.</forenames></author><author><keyname>Vishwanath</keyname><forenames>Sriram</forenames></author><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author></authors><title>Downlink Multi-Antenna Heterogeneous Cellular Network with Load
  Balancing</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We model and analyze heterogeneous cellular networks with multiple antenna
BSs (multi-antenna HetNets) with K classes or tiers of base stations (BSs),
which may differ in terms of transmit power, deployment density, number of
transmit antennas, number of users served, transmission scheme, and path loss
exponent. We show that the cell selection rules in multi-antenna HetNets may
differ significantly from the single-antenna HetNets due to the possible
differences in multi-antenna transmission schemes across tiers. While it is
challenging to derive exact cell selection rules even for maximizing
signal-to-interferenceplus-noise-ratio (SINR) at the receiver, we show that
adding an appropriately chosen tier-dependent cell selection bias in the
received power yields a close approximation. Assuming arbitrary selection bias
for each tier, simple expressions for downlink coverage and rate are derived.
For coverage maximization, the required selection bias for each tier is given
in closed form. Due to this connection with biasing, multi-antenna HetNets may
balance load more naturally across tiers in certain regimes compared to
single-antenna HetNets, where a large cell selection bias is often needed to
offload traffic to small cells.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6808</identifier>
 <datestamp>2013-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6808</id><created>2013-10-24</created><authors><author><keyname>Islam</keyname><forenames>Mohammad shahidul</forenames></author></authors><title>Gender Classification Using Gradient Direction Pattern</title><categories>cs.CV</categories><comments>3 pages, 5 figures, 3 tables, SCI journal</comments><acm-class>I.5.4</acm-class><journal-ref>Sci.Int(Lahore),25(4),797-799,2013 ISSN 1013-5316; CODEN: SINTE 8</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel methodology for gender classification is presented in this paper. It
extracts feature from local region of a face using gray color intensity
difference. The facial area is divided into sub-regions and GDP histogram
extracted from those regions are concatenated into a single vector to represent
the face. The classification accuracy obtained by using support vector machine
has outperformed all traditional feature descriptors for gender classification.
It is evaluated on the images collected from FERET database and obtained very
high accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6813</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6813</id><created>2013-10-24</created><updated>2015-06-18</updated><authors><author><keyname>Selinger</keyname><forenames>Peter</forenames><affiliation>Dalhousie University, Canada</affiliation></author></authors><title>Generators and relations for n-qubit Clifford operators</title><categories>quant-ph cs.ET cs.LO</categories><proxy>LMCS</proxy><journal-ref>LMCS 11 (2:10) 2015</journal-ref><doi>10.2168/LMCS-11(2:10)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a normal form for Clifford circuits, and we prove that every
Clifford operator has a unique normal form. Moreover, we present a rewrite
system by which any Clifford circuit can be reduced to normal form. This yields
a presentation of Clifford operators in terms of generators and relations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6817</identifier>
 <datestamp>2013-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6817</id><created>2013-10-25</created><authors><author><keyname>Zhou</keyname><forenames>Hongchao</forenames></author><author><keyname>Schwartz</keyname><forenames>Moshe</forenames></author><author><keyname>Jiang</keyname><forenames>Anxiao</forenames></author><author><keyname>Bruck</keyname><forenames>Jehoshua</forenames></author></authors><title>Systematic Error-Correcting Codes for Rank Modulation</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rank-modulation scheme has been recently proposed for efficiently storing
data in nonvolatile memories. Error-correcting codes are essential for rank
modulation, however, existing results have been limited. In this work we
explore a new approach, \emph{systematic error-correcting codes for rank
modulation}. Systematic codes have the benefits of enabling efficient
information retrieval and potentially supporting more efficient encoding and
decoding procedures. We study systematic codes for rank modulation under
Kendall's $\tau$-metric as well as under the $\ell_\infty$-metric.
  In Kendall's $\tau$-metric we present $[k+2,k,3]$-systematic codes for
correcting one error, which have optimal rates, unless systematic perfect codes
exist. We also study the design of multi-error-correcting codes, and provide
two explicit constructions, one resulting in $[n+1,k+1,2t+2]$ systematic codes
with redundancy at most $2t+1$. We use non-constructive arguments to show the
existence of $[n,k,n-k]$-systematic codes for general parameters. Furthermore,
we prove that for rank modulation, systematic codes achieve the same capacity
as general error-correcting codes.
  Finally, in the $\ell_\infty$-metric we construct two $[n,k,d]$ systematic
multi-error-correcting codes, the first for the case of $d=O(1)$, and the
second for $d=\Theta(n)$. In the latter case, the codes have the same
asymptotic rate as the best codes currently known in this metric.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6833</identifier>
 <datestamp>2013-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6833</id><created>2013-10-25</created><authors><author><keyname>Sowjanya</keyname><forenames>A. M.</forenames></author><author><keyname>Shashi</keyname><forenames>M.</forenames></author></authors><title>New Proximity Estimate for Incremental Update of Non-uniformly
  Distributed Clusters</title><categories>cs.DB</categories><comments>19 pages</comments><acm-class>H.2.8</acm-class><journal-ref>International Journal of Data Mining &amp; Knowledge Management
  Process (IJDKP) Vol.3, No.5, September 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The conventional clustering algorithms mine static databases and generate a
set of patterns in the form of clusters. Many real life databases keep growing
incrementally. For such dynamic databases, the patterns extracted from the
original database become obsolete. Thus the conventional clustering algorithms
are not suitable for incremental databases due to lack of capability to modify
the clustering results in accordance with recent updates. In this paper, the
author proposes a new incremental clustering algorithm called CFICA(Cluster
Feature-Based Incremental Clustering Approach for numerical data) to handle
numerical data and suggests a new proximity metric called Inverse Proximity
Estimate (IPE) which considers the proximity of a data point to a cluster
representative as well as its proximity to a farthest point in its vicinity.
CFICA makes use of the proposed proximity metric to determine the membership of
a data point into a cluster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6847</identifier>
 <datestamp>2013-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6847</id><created>2013-10-25</created><authors><author><keyname>Cimatti</keyname><forenames>Alessandro</forenames></author><author><keyname>Griggio</keyname><forenames>Alberto</forenames></author><author><keyname>Mover</keyname><forenames>Sergio</forenames></author><author><keyname>Tonetta</keyname><forenames>Stefano</forenames></author></authors><title>IC3 Modulo Theories via Implicit Predicate Abstraction</title><categories>cs.LO cs.SE</categories><acm-class>D.2.4; B.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel approach for generalizing the IC3 algorithm for invariant
checking from finite-state to infinite-state transition systems, expressed over
some background theories. The procedure is based on a tight integration of IC3
with Implicit (predicate) Abstraction, a technique that expresses abstract
tran- sitions without computing explicitly the abstract system and is
incremental with respect to the addition of predicates. In this scenario, IC3
operates only at the Boolean level of the abstract state space, discovering
inductive clauses over the abstraction predicates. Theory reasoning is confined
within the underlying SMT solver, and applied transparently when performing
satisfiability checks. When the current abstraction allows for a spurious
counterexample, it is refined by discov- ering and adding a sufficient set of
new predicates. Importantly, this can be done in a completely incremental
manner, without discarding the clauses found in the previous search. The
proposed approach has two key advantages. First, unlike current SMT gener-
alizations of IC3, it allows to handle a wide range of background theories
without relying on ad-hoc extensions, such as quantifier elimination or
theory-specific clause generalization procedures, which might not always be
available, and can moreover be inefficient. Second, compared to a direct
exploration of the concrete transition system, the use of abstraction gives a
significant performance improve- ment, as our experiments demonstrate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6870</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6870</id><created>2013-10-25</created><updated>2014-02-24</updated><authors><author><keyname>Park</keyname><forenames>Jaehyun</forenames></author><author><keyname>Clerckx</keyname><forenames>Bruno</forenames></author></authors><title>Joint Wireless Information and Energy Transfer in a K-User MIMO
  Interference Channel</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1303.1693</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, joint wireless information and energy transfer (JWIET) methods have
been proposed to relieve the battery limitation of wireless devices. However,
the JWIET in a general K-user MIMO interference channel (IFC) has been
unexplored so far. In this paper, we investigate for the first time the JWIET
in K-user MIMO IFC, in which receivers either decode the incoming information
data (information decoding, ID) or harvest the RF energy (energy harvesting,
EH). In the K-user IFC, we consider three different scenarios according to the
receiver mode -- i) multiple EH receivers and a single ID receiver, ii)
multiple IDs and a single EH, and iii) multiple IDs and multiple EHs. For all
scenarios, we have found a common necessary condition of the optimal
transmission strategy and, accordingly, developed the transmission strategy
that satisfies the common necessary condition, in which all the transmitters
transferring energy exploit a rank-one energy beamforming. Furthermore, we have
also proposed an iterative algorithm to optimize the covariance matrices of the
transmitters that transfer information and the powers of the energy beamforming
transmitters simultaneously, and identified the corresponding achievable
rate-energy tradeoff region. Finally, we have shown that by selecting EH
receivers according to their signal-to-leakage-and-harvested energy-ratio
(SLER), we can improve the achievable rate-energy region further.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6876</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6876</id><created>2013-10-25</created><updated>2013-12-10</updated><authors><author><keyname>Mukhopadhyay</keyname><forenames>Sabyasachi</forenames></author><author><keyname>Dash</keyname><forenames>Debadatta</forenames></author><author><keyname>Mitra</keyname><forenames>Asish</forenames></author><author><keyname>Panigrahi</keyname><forenames>Prasanta K.</forenames></author></authors><title>Application of Fourier and Wavelet Transform for analysing 300 years
  Sunspot numbers to Explain the Solar Cycles</title><categories>cs.CE</categories><comments>This paper has been withdrawn by the author due to some modifications
  are required for current paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper Fourier Transform and Wavelet Transform are applied in case of
recent 300 years of sunspot numbers to explain the solar cycles. Here basically
parallel study of Fourier and Wavelet analysis are done and we have observed
that the better result can be obtained from Wavelet analysis during sunspot
number analysis. We are able to show various minima and maxima in the recent
ages of solar cycles with this tool. The exact periodicity and other possible
periodicities in the cyclic phenomenon of sunspot activity are determined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6880</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6880</id><created>2013-10-25</created><updated>2013-12-28</updated><authors><author><keyname>Adame</keyname><forenames>Toni</forenames></author><author><keyname>Bel</keyname><forenames>Albert</forenames></author><author><keyname>Bellalta</keyname><forenames>Boris</forenames></author><author><keyname>Barcelo</keyname><forenames>Jaume</forenames></author><author><keyname>Gonzalez</keyname><forenames>Javier</forenames></author><author><keyname>Oliver</keyname><forenames>Miquel</forenames></author></authors><title>Capacity Analysis of IEEE 802.11ah WLANs for M2M Communications</title><categories>cs.NI</categories><comments>Multiple Access Communications 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Focusing on the increasing market of the sensors and actuators networks, the
IEEE 802.11ah Task Group is currently working on the standardization of a new
amendment. This new amendment will operate at the sub-1GHz band, ensure
transmission ranges up to 1 Km, data rates above 100 kbps and very low power
operation. With IEEE 802.11ah, the WLANs will offer a solution for applications
such as smart metering, plan automation, eHealth or surveillance. Moreover,
thanks to a hierarchical signalling, the IEEE 802.11ah will be able to manage a
higher number of stations (STAs) and improve the 802.11 Power Saving
Mechanisms. In order to support a high number of STAs, two different signalling
modes are proposed, TIM and Non-TIM Offset. In this paper we present a
theoretical model to predict the maximum number of STAs supported by both modes
depending on the traffic load and the data rate used. Moreover, the IEEE
802.11ah performance and energy consumption for both signalling modes and for
different traffic patterns and data rates is evaluated. Results show that both
modes achieve similar Packet Delivery Ratio values but the energy consumed with
the TIM Offset is, in average, a 11.7% lower.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6887</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6887</id><created>2013-10-25</created><authors><author><keyname>Brand&#xe3;o</keyname><forenames>Filipe</forenames></author><author><keyname>Pedroso</keyname><forenames>Jo&#xe3;o Pedro</forenames></author></authors><title>Bin Packing and Related Problems: General Arc-flow Formulation with
  Graph Compression</title><categories>math.OC cs.DS</categories><report-no>DCC-2013-08</report-no><msc-class>80M50</msc-class><acm-class>G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an exact method, based on an arc-flow formulation with side
constraints, for solving bin packing and cutting stock problems --- including
multi-constraint variants --- by simply representing all the patterns in a very
compact graph. Our method includes a graph compression algorithm that usually
reduces the size of the underlying graph substantially without weakening the
model. As opposed to our method, which provides strong models, conventional
models are usually highly symmetric and provide very weak lower bounds.
  Our formulation is equivalent to Gilmore and Gomory's, thus providing a very
strong linear relaxation. However, instead of using column-generation in an
iterative process, the method constructs a graph, where paths from the source
to the target node represent every valid packing pattern.
  The same method, without any problem-specific parameterization, was used to
solve a large variety of instances from several different cutting and packing
problems. In this paper, we deal with vector packing, graph coloring, bin
packing, cutting stock, cardinality constrained bin packing, cutting stock with
cutting knife limitation, cutting stock with binary patterns, bin packing with
conflicts, and cutting stock with binary patterns and forbidden pairs. We
report computational results obtained with many benchmark test data sets, all
of them showing a large advantage of this formulation with respect to the
traditional ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6894</identifier>
 <datestamp>2013-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6894</id><created>2013-10-25</created><authors><author><keyname>Gupta</keyname><forenames>Vikash</forenames></author><author><keyname>Voruganti</keyname><forenames>Hari K.</forenames></author><author><keyname>Dasgupta</keyname><forenames>Bhaskar</forenames></author></authors><title>Domain Mapping for Volumetric Parameterization using Harmonic Functions</title><categories>cs.CG</categories><comments>www.cadanda.com</comments><journal-ref>Computer-Aided Design &amp; Applications, Vol. 10(a), 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Volumetric parameterization problem refers to parameterization of both the
interior and boundary of a 3D model. It is a much harder problem compared to
surface parameterization where a parametric representation is worked out only
for the boundary of a 3D model (which is a surface). Volumetric
parameterization is typically helpful in solving complicated geometric problems
pertaining to shape matching, morphing, path planning of robots, and
isogeometric analysis etc. A novel method is proposed in which a volume
parameterization is developed by mapping a general non-convex (genus-0) domain
to its topologically equivalent convex domain. In order to achieve a continuous
and bijective mapping of a domain, first we use the harmonic function to
establish a potential field over the domain. The gradients of the potential
values are used to track the streamlines which originate from the boundary and
converge to a single point, referred to as the shape center. Each streamline
approaches the shape center at a unique polar angle and an azimuthal angle .
Once all the three parameters (potential value, polar angle, azimuthal angle)
necessary to represent any point in the given domain are available, the domain
is said to be parameterized. Using our method, given a 3D non-convex domain, we
can parameterize the surface as well as the interior of the domain. The
proposed method is implemented and the algorithm is tested on many standard
cases to demonstrate the effectiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6900</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6900</id><created>2013-10-25</created><updated>2015-05-26</updated><authors><author><keyname>Pach</keyname><forenames>J&#xe1;nos</forenames></author><author><keyname>P&#xe1;lv&#xf6;lgyi</keyname><forenames>D&#xf6;m&#xf6;t&#xf6;r</forenames></author></authors><title>Unsplittable coverings in the plane</title><categories>math.MG cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A system of sets forms an {\em $m$-fold covering} of a set $X$ if every point
of $X$ belongs to at least $m$ of its members. A $1$-fold covering is called a
{\em covering}. The problem of splitting multiple coverings into several
coverings was motivated by classical density estimates for {\em sphere
packings} as well as by the {\em planar sensor cover problem}. It has been the
prevailing conjecture for 35 years (settled in many special cases) that for
every plane convex body $C$, there exists a constant $m=m(C)$ such that every
$m$-fold covering of the plane with translates of $C$ splits into $2$
coverings. In the present paper, it is proved that this conjecture is false for
the unit disk. The proof can be generalized to construct, for every $m$, an
unsplittable $m$-fold covering of the plane with translates of any open convex
body $C$ which has a smooth boundary with everywhere {\em positive curvature}.
Somewhat surprisingly, {\em unbounded} open convex sets $C$ do not misbehave,
they satisfy the conjecture: every $3$-fold covering of any region of the plane
by translates of such a set $C$ splits into two coverings. To establish this
result, we prove a general coloring theorem for hypergraphs of a special type:
{\em shift-chains}. We also show that there is a constant $c&gt;0$ such that, for
any positive integer $m$, every $m$-fold covering of a region with unit disks
splits into two coverings, provided that every point is covered by {\em at
most} $c2^{m/2}$ sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6901</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6901</id><created>2013-10-25</created><updated>2013-10-28</updated><authors><author><keyname>Minh</keyname><forenames>Trang Cao</forenames></author><author><keyname>Bellalta</keyname><forenames>Boris</forenames></author><author><keyname>Oechsner</keyname><forenames>Simon</forenames></author><author><keyname>Liao</keyname><forenames>Ruizhi</forenames></author><author><keyname>Oliver</keyname><forenames>Miquel</forenames></author></authors><title>Managing Heterogeneous WSNs in Smart Cities: Challenges and Requirements</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dramatic advances in wireless communications and electronics have enabled
the development of Wireless Sensor Networks (WSNs). WSNs consist of many
affordable and portable sensor nodes for collecting data from the environment.
In this article, we address management requirements of WSNs through presenting
some key management scenarios in the Smart Cities context, such as intelligent
transportation systems, smart grids and smart buildings. The limited resources
and heterogeneous characteristics of WSNs pose new challenges in network
management, which include the presence of various faults, the difficulty in
replacing and repairing a large number of sensor nodes, the existence of an
uncertain topology, and the resource allocation. To cope with these challenges,
we first discuss advantages and disadvantages of centralized and distributed
management approaches and then discuss the benefit of the multilevel management
schema. Next, we present in detail the specific features for a WSN management
system such as lightweight, self-detection, self-configuration, sharing
infrastructure, service monitoring, plug and play, context awareness and
interoperability. Finally, we present the required mechanisms for some basic
management functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6921</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6921</id><created>2013-10-25</created><updated>2014-03-25</updated><authors><author><keyname>Criado</keyname><forenames>Regino</forenames></author><author><keyname>Garcia</keyname><forenames>Esther</forenames></author><author><keyname>Pedroche</keyname><forenames>Francisco</forenames></author><author><keyname>Romance</keyname><forenames>Miguel</forenames></author></authors><title>Comparing rankings by means of competitivity graphs: structural
  properties and computation</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce a new technique to analyze families of rankings
focused on the study of structural properties of a new type of graphs. Given a
finite number of elements and a family of rankings of those elements, we say
that two elements compete when they exchange their relative positions in at
least two rankings. This allows us to define an undirected graph by connecting
elements that compete. We call this graph a competitivity graph. We study the
relationship of competitivity graphs with other well-known families of graphs,
such as permutation graphs, comparability graphs and chordal graphs. In
addition to this, we also introduce certain important sets of nodes in a
competitivity graph. For example, nodes that compete among them form a
competitivity set and nodes connected by chains of competitors form a set of
eventual competitors. We analyze hese sets and we show a method to obtain sets
of eventual competitors directly from a family of rankings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6924</identifier>
 <datestamp>2013-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6924</id><created>2013-10-24</created><authors><author><keyname>Kotagiri</keyname><forenames>Vamsi Sashank</forenames></author></authors><title>New Results on the Number Theoretic Hilbert Transform</title><categories>cs.CR</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents new results in the theory of number theoretic Hilbert
(NHT) transforms. New polymorphic solutions have been found for the 14-point
and 16-point transforms. Several transform pairs are computed and solutions
found for which the sequence and the transform have the same shape. The
multiplicity of solutions for the same moduli increases their applicability to
cryptography.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6925</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6925</id><created>2013-10-24</created><updated>2014-06-09</updated><authors><author><keyname>Lam</keyname><forenames>Albert Y. S.</forenames></author><author><keyname>Leung</keyname><forenames>Yiu-Wing</forenames></author><author><keyname>Chu</keyname><forenames>Xiaowen</forenames></author></authors><title>Electric Vehicle Charging Station Placement: Formulation, Complexity,
  and Solutions</title><categories>cs.SY math.OC</categories><comments>Submitted to IEEE Transactions on Smart Grid, revised</comments><doi>10.1109/TSG.2014.2344684</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To enhance environmental sustainability, many countries will electrify their
transportation systems in their future smart city plans. So the number of
electric vehicles (EVs) running in a city will grow significantly. There are
many ways to re-charge EVs' batteries and charging stations will be considered
as the main source of energy. The locations of charging stations are critical;
they should not only be pervasive enough such that an EV anywhere can easily
access a charging station within its driving range, but also widely spread so
that EVs can cruise around the whole city upon being re-charged. Based on these
new perspectives, we formulate the Electric Vehicle Charging Station Placement
Problem (EVCSPP) in this paper. We prove that the problem is non-deterministic
polynomial-time hard. We also propose four solution methods to tackle EVCSPP
and evaluate their performance on various artificial and practical cases. As
verified by the simulation results, the methods have their own characteristics
and they are suitable for different situations depending on the requirements
for solution quality, algorithmic efficiency, problem size, nature of the
algorithm, and existence of system prerequisite.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6938</identifier>
 <datestamp>2013-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6938</id><created>2013-10-25</created><authors><author><keyname>Farias</keyname><forenames>Rodrigo Cabral</forenames></author><author><keyname>Moisan</keyname><forenames>Eric</forenames></author><author><keyname>Brossier</keyname><forenames>Jean-Marc</forenames></author></authors><title>Optimal Asymmetric Binary Quantization for Estimation Under
  Symmetrically Distributed Noise</title><categories>cs.IT math.IT</categories><comments>4 pages, 5 figures</comments><msc-class>94A29, 62F10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Estimation of a location parameter based on noisy and binary quantized
measurements is considered in this letter. We study the behavior of the
Cramer-Rao bound as a function of the quantizer threshold for different
symmetric unimodal noise distributions. We show that, in some cases, the
intuitive choice of threshold position given by the symmetry of the problem,
placing the threshold on the true parameter value, can lead to locally worst
estimation performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6945</identifier>
 <datestamp>2013-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6945</id><created>2013-10-25</created><authors><author><keyname>Farias</keyname><forenames>Rodrigo Cabral</forenames></author><author><keyname>Brossier</keyname><forenames>Jean-Marc</forenames></author></authors><title>Optimal Scalar Quantization for Parameter Estimation</title><categories>cs.IT math.IT</categories><comments>10 pages, 3 figures, 2 tables</comments><msc-class>94A29, 62F10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study an asymptotic approximation of the Fisher information
for the estimation of a scalar parameter using quantized measurements. We show
that, as the number of quantization intervals tends to infinity, the loss of
Fisher information induced by quantization decreases exponentially as a
function of the number of quantization bits. A characterization of the optimal
quantizer through its interval density and an analytical expression for the
Fisher information are obtained. A comparison between optimal uniform and
non-uniform quantization for the location and scale estimation problems shows
that non-uniform quantization is only slightly better. As the optimal
quantization intervals are shown to depend on the unknown parameters, by
applying adaptive algorithms that jointly estimate the parameter and set the
thresholds in the location and scale estimation problems, we show that the
asymptotic results can be approximately obtained in practice using only 4 or 5
quantization bits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6955</identifier>
 <datestamp>2014-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6955</id><created>2013-10-25</created><updated>2014-02-28</updated><authors><author><keyname>Aichholzer</keyname><forenames>Oswin</forenames></author><author><keyname>Hackl</keyname><forenames>Thomas</forenames></author><author><keyname>Lutteropp</keyname><forenames>Sarah</forenames></author><author><keyname>Mchedlidze</keyname><forenames>Tamara</forenames></author><author><keyname>Pilz</keyname><forenames>Alexander</forenames></author><author><keyname>Vogtenhuber</keyname><forenames>Birgit</forenames></author></authors><title>Monotone Simultaneous Embedding of Directed Paths</title><categories>cs.CG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study monotone simultaneous embeddings of upward planar digraphs, which
are simultaneous embeddings where the drawing of each digraph is upward planar,
and the directions of the upwardness of different graphs can differ. We first
consider the special case where each digraph is a directed path. In contrast to
the known result that any two directed paths admit a monotone simultaneous
embedding, there exist examples of three paths that do not admit such an
embedding for any possible choice of directions of monotonicity. We prove that
if a monotone simultaneous embedding of three paths exists then it also exists
for any possible choice of directions of monotonicity. We provide a
polynomial-time algorithm that, given three paths, decides whether a monotone
simultaneous embedding exists and, in the case of existence, also constructs
such an embedding. On the other hand, we show that already for three paths, any
monotone simultaneous embedding might need a grid of exponential (w.r.t. the
number of vertices) size. For more than three paths, we present a
polynomial-time algorithm that, given any number of paths and predefined
directions of monotonicity, decides whether the paths admit a monotone
simultaneous embedding with respect to the given directions, including the
construction of a solution if it exists. Further, we show several implications
of our results on monotone simultaneous embeddings of general upward planar
digraphs. Finally, we discuss complexity issues related to our problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6976</identifier>
 <datestamp>2013-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6976</id><created>2013-10-25</created><authors><author><keyname>Antunes</keyname><forenames>L.</forenames><affiliation>University of Porto</affiliation></author><author><keyname>Souto</keyname><forenames>A.</forenames><affiliation>Techical University of Lissabon</affiliation></author><author><keyname>Vitanyi</keyname><forenames>P. M. B.</forenames><affiliation>CWI and the University of Amsterdam</affiliation></author></authors><title>On Logical Depth and the Running Time of Shortest Programs</title><categories>cs.CC</categories><comments>12 pages LaTex (this supercedes arXiv:1301.4451)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The logical depth with significance $b$ of a finite binary string $x$ is the
shortest running time of a binary program for $x$ that can be compressed by at
most $b$ bits. There is another definition of logical depth. We give two
theorems about the quantitative relation between these versions: the first
theorem concerns a variation of a known fact with a new proof, the second
theorem and its proof are new. We select the above version of logical depth and
show the following. There is an infinite sequence of strings of increasing
length such that for each $j$ there is a $b$ such that the logical depth of the
$j$th string as a function of $j$ is incomputable (it rises faster than any
computable function) but with $b$ replaced by $b+1$ the resuling function is
computable. Hence the maximal gap between the logical depths resulting from
incrementing appropriate $b$'s by 1 rises faster than any computable function.
All functions mentioned are upper bounded by the Busy Beaver function. Since
for every string its logical depth is nonincreasing in $b$, the minimal
computation time of the shortest programs for the sequence of strings as a
function of $j$ rises faster than any computable function but not so fast as
the Busy Beaver function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6978</identifier>
 <datestamp>2013-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6978</id><created>2013-10-23</created><authors><author><keyname>Mijajlovic</keyname><forenames>Zarko</forenames></author><author><keyname>Pejovic</keyname><forenames>Aleksandar</forenames></author></authors><title>Computing finite models using free Boolean generators</title><categories>cs.LO math.LO</categories><comments>20 pages, 12 references</comments><msc-class>03C98</msc-class><acm-class>B.2.1; C.1.2; G.2.1; I.1.2; I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A parallel method for computing Boolean expressions based on the properties
of finite free Boolean algebras is presented. We also show how various finite
combinatorial objects can be codded in the formalism of Boolean algebras and
counted by this procedure. Particularly, using a translation of first order
predicate formulas to propositional formulas, we give a method for constructing
and counting finite models of the first order theories. An implementation of
the method that can be run on multi-core CPUs as well as on highly parallel
GPUs is outlined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6992</identifier>
 <datestamp>2014-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6992</id><created>2013-10-25</created><updated>2014-05-16</updated><authors><author><keyname>Shah</keyname><forenames>Shalin</forenames></author><author><keyname>Limbachiya</keyname><forenames>Dixita</forenames></author><author><keyname>Gupta</keyname><forenames>Manish K.</forenames></author></authors><title>DNACloud: A Potential Tool for storing Big Data on DNA</title><categories>cs.ET</categories><comments>revised version, 6 pages, 2 tables, 1 figure, appeared as a poster in
  FNANO 2014 conference, Software available at http://www.guptalab.org/dnacloud</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The term Big Data is usually used to describe huge amount of data that is
generated by humans from digital media such as cameras, internet, phones,
sensors etc. By building advanced analytics on the top of big data, one can
predict many things about the user such as behavior, interest etc. However
before one can use the data, one has to address many issues for big data
storage. Two main issues are the need of large storage devices and the cost
associated with it. Synthetic DNA storage seems to be an appropriate solution
to address these issues of the big data. Recently in 2013, Goldman and his
collegues from European Bioinformatics Institute demonstrated the use of the
DNA as storage medium with capacity of storing 2.2 peta bytes of information on
one gram of DNA and retrived the data successfully with low error rate. This
significant step shows a promise for synthetic DNA storage as a useful
technology for the future data storage. Motivated by this, we have developed a
software called DNACloud which makes it easy to store the data on the DNA. In
this work, we present detailed description of the software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6997</identifier>
 <datestamp>2013-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6997</id><created>2013-10-23</created><authors><author><keyname>Hemaspaandra</keyname><forenames>Edith</forenames></author><author><keyname>Hemaspaandra</keyname><forenames>Lane A.</forenames></author><author><keyname>Rothe</keyname><forenames>Joerg</forenames></author></authors><title>The Complexity of Online Manipulation of Sequential Elections</title><categories>cs.GT</categories><comments>10 pages, Contributed talk at TARK 2013 (arXiv:1310.6382)
  http://www.tark.org , this is the conference version of the technical report
  arXiv:1202.6655</comments><proxy>Burkhard Schipper</proxy><report-no>TARK/2013/p111</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most work on manipulation assumes that all preferences are known to the
manipulators. However, in many settings elections are open and sequential, and
manipulators may know the already cast votes but may not know the future votes.
We introduce a framework, in which manipulators can see the past votes but not
the future ones, to model online coalitional manipulation of sequential
elections, and we show that in this setting manipulation can be extremely
complex even for election systems with simple winner problems. Yet we also show
that for some of the most important election systems such manipulation is
simple in certain settings. This suggests that when using sequential voting,
one should pay great attention to the details of the setting in choosing one's
voting rule.
  Among the highlights of our classifications are: We show that, depending on
the size of the manipulative coalition, the online manipulation problem can be
complete for each level of the polynomial hierarchy or even for PSPACE. We
obtain the most dramatic contrast to date between the nonunique-winner and
unique-winner models: Online weighted manipulation for plurality is in P in the
nonunique-winner model, yet is coNP-hard (constructive case) and NP-hard
(destructive case) in the unique-winner model. And we obtain what to the best
of our knowledge are the first PNP[1]-completeness and PNP-completeness results
in the field of computational social choice, in particular proving such
completeness for, respectively, the complexity of 3-candidate and 4-candidate
(and unlimited-candidate) online weighted coalition manipulation of veto
elections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.6998</identifier>
 <datestamp>2013-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.6998</id><created>2013-10-25</created><authors><author><keyname>Sinha</keyname><forenames>Shiladitya</forenames></author><author><keyname>Dyer</keyname><forenames>Chris</forenames></author><author><keyname>Gimpel</keyname><forenames>Kevin</forenames></author><author><keyname>Smith</keyname><forenames>Noah A.</forenames></author></authors><title>Predicting the NFL using Twitter</title><categories>cs.SI cs.LG physics.soc-ph stat.ML</categories><comments>Presented at ECML/PKDD 2013 Workshop on Machine Learning and Data
  Mining for Sports Analytics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the relationship between social media output and National Football
League (NFL) games, using a dataset containing messages from Twitter and NFL
game statistics. Specifically, we consider tweets pertaining to specific teams
and games in the NFL season and use them alongside statistical game data to
build predictive models for future game outcomes (which team will win?) and
sports betting outcomes (which team will win with the point spread? will the
total points be over/under the line?). We experiment with several feature sets
and find that simple features using large volumes of tweets can match or exceed
the performance of more traditional features that use game statistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7001</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7001</id><created>2013-10-25</created><updated>2015-03-31</updated><authors><author><keyname>Rogalin</keyname><forenames>Ryan</forenames></author><author><keyname>Bursalioglu</keyname><forenames>Ozgun</forenames></author><author><keyname>Papadopoulos</keyname><forenames>Haralabos</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author><author><keyname>Molisch</keyname><forenames>Andreas</forenames></author><author><keyname>Michaloliakos</keyname><forenames>Antonios</forenames></author><author><keyname>Balan</keyname><forenames>Vlad</forenames></author><author><keyname>Psounis</keyname><forenames>Konstantinos</forenames></author></authors><title>Scalable Synchronization and Reciprocity Calibration for Distributed
  Multiuser MIMO</title><categories>cs.NI cs.IT math.IT</categories><comments>Replaced Figure 5 with correct version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large-scale distributed Multiuser MIMO (MU-MIMO) is a promising wireless
network architecture that combines the advantages of &quot;massive MIMO&quot; and &quot;small
cells.&quot; It consists of several Access Points (APs) connected to a central
server via a wired backhaul network and acting as a large distributed antenna
system. We focus on the downlink, which is both more demanding in terms of
traffic and more challenging in terms of implementation than the uplink. In
order to enable multiuser joint precoding of the downlink signals, channel
state information at the transmitter side is required. We consider Time
Division Duplex (TDD), where the {\em downlink} channels can be learned from
the user uplink pilot signals, thanks to channel reciprocity. Furthermore,
coherent multiuser joint precoding is possible only if the APs maintain a
sufficiently accurate relative timing and phase synchronization. AP
synchronization and TDD reciprocity calibration are two key problems to be
solved in order to enable distributed MU-MIMO downlink. In this paper, we
propose novel over-the-air synchronization and calibration protocols that scale
well with the network size. The proposed schemes can be applied to networks
formed by a large number of APs, each of which is driven by an inexpensive
802.11-grade clock and has a standard RF front-end, not explicitly designed to
be reciprocal. Our protocols can incorporate, as a building block, any suitable
timing and frequency estimator. Here we revisit the problem of joint ML timing
and frequency estimation and use the corresponding Cramer-Rao bound to evaluate
the performance of the synchronization protocol. Overall, the proposed
synchronization and calibration schemes are shown to achieve sufficient
accuracy for satisfactory distributed MU-MIMO performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7004</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7004</id><created>2013-10-25</created><updated>2013-11-13</updated><authors><author><keyname>Cibulka</keyname><forenames>Josef</forenames></author><author><keyname>Gao</keyname><forenames>Pu</forenames></author><author><keyname>Kr&#x10d;&#xe1;l</keyname><forenames>Marek</forenames></author><author><keyname>Valla</keyname><forenames>Tom&#xe1;&#x161;</forenames></author><author><keyname>Valtr</keyname><forenames>Pavel</forenames></author></authors><title>On the Geometric Ramsey Number of Outerplanar Graphs</title><categories>math.CO cs.DM</categories><comments>15 pages, 7 figures</comments><msc-class>52C35 (Primary) 05C55, 05C10 (Secondary)</msc-class><journal-ref>Discrete and Computational Geometry 53 (1): 64-79 (2015)</journal-ref><doi>10.1007/s00454-014-9646-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove polynomial upper bounds of geometric Ramsey numbers of pathwidth-2
outerplanar triangulations in both convex and general cases. We also prove that
the geometric Ramsey numbers of the ladder graph on $2n$ vertices are bounded
by $O(n^{3})$ and $O(n^{10})$, in the convex and general case, respectively. We
then apply similar methods to prove an $n^{O(\log(n))}$ upper bound on the
Ramsey number of a path with $n$ ordered vertices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7007</identifier>
 <datestamp>2013-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7007</id><created>2013-10-25</created><authors><author><keyname>Kuipers</keyname><forenames>J.</forenames></author><author><keyname>Ueda</keyname><forenames>T.</forenames></author><author><keyname>Vermaseren</keyname><forenames>J. A. M.</forenames></author></authors><title>Code Optimization in FORM</title><categories>cs.SC hep-ph</categories><comments>31 pages</comments><report-no>Nikhef 2013-036, TTP13-031, SFB/CPP-13-80</report-no><msc-class>97Rxx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe the implementation of output code optimization in the open source
computer algebra system FORM. This implementation is based on recently
discovered techniques of Monte Carlo tree search to find efficient multivariate
Horner schemes, in combination with other optimization algorithms, such as
common subexpression elimination. For systems for which no specific knowledge
is provided it performs significantly better than other methods we could
compare with. Because the method has a number of free parameters, we also show
some methods by which to tune them to different types of problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7028</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7028</id><created>2013-10-25</created><updated>2014-08-11</updated><authors><author><keyname>Gupta</keyname><forenames>Manish K.</forenames></author><author><keyname>Wilde</keyname><forenames>Mark M.</forenames></author></authors><title>Multiplicativity of completely bounded $p$-norms implies a strong
  converse for entanglement-assisted capacity</title><categories>quant-ph cs.IT math.IT</categories><comments>21 pages, final version accepted for publication in Communications in
  Mathematical Physics</comments><journal-ref>Communications in Mathematical Physics, vol. 334, no. 2, pages
  867-887 (March 2015)</journal-ref><doi>10.1007/s00220-014-2212-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fully quantum reverse Shannon theorem establishes the optimal rate of
noiseless classical communication required for simulating the action of many
instances of a noisy quantum channel on an arbitrary input state, while also
allowing for an arbitrary amount of shared entanglement of an arbitrary form.
Turning this theorem around establishes a strong converse for the
entanglement-assisted classical capacity of any quantum channel. This paper
proves the strong converse for entanglement-assisted capacity by a completely
different approach and identifies a bound on the strong converse exponent for
this task. Namely, we exploit the recent entanglement-assisted &quot;meta-converse&quot;
theorem of Matthews and Wehner, several properties of the recently established
sandwiched Renyi relative entropy (also referred to as the quantum Renyi
divergence), and the multiplicativity of completely bounded $p$-norms due to
Devetak et al. The proof here demonstrates the extent to which the Arimoto
approach can be helpful in proving strong converse theorems, it provides an
operational relevance for the multiplicativity result of Devetak et al., and it
adds to the growing body of evidence that the sandwiched Renyi relative entropy
is the correct quantum generalization of the classical concept for all
$\alpha&gt;1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7048</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7048</id><created>2013-10-25</created><authors><author><keyname>Wang</keyname><forenames>Jie</forenames></author><author><keyname>Wonka</keyname><forenames>Peter</forenames></author><author><keyname>Ye</keyname><forenames>Jieping</forenames></author></authors><title>Scaling SVM and Least Absolute Deviations via Exact Data Reduction</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The support vector machine (SVM) is a widely used method for classification.
Although many efforts have been devoted to develop efficient solvers, it
remains challenging to apply SVM to large-scale problems. A nice property of
SVM is that the non-support vectors have no effect on the resulting classifier.
Motivated by this observation, we present fast and efficient screening rules to
discard non-support vectors by analyzing the dual problem of SVM via
variational inequalities (DVI). As a result, the number of data instances to be
entered into the optimization can be substantially reduced. Some appealing
features of our screening method are: (1) DVI is safe in the sense that the
vectors discarded by DVI are guaranteed to be non-support vectors; (2) the data
set needs to be scanned only once to run the screening, whose computational
cost is negligible compared to that of solving the SVM problem; (3) DVI is
independent of the solvers and can be integrated with any existing efficient
solvers. We also show that the DVI technique can be extended to detect
non-support vectors in the least absolute deviations regression (LAD). To the
best of our knowledge, there are currently no screening methods for LAD. We
have evaluated DVI on both synthetic and real data sets. Experiments indicate
that DVI significantly outperforms the existing state-of-the-art screening
rules for SVM, and is very effective in discarding non-support vectors for LAD.
The speedup gained by DVI rules can be up to two orders of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7062</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7062</id><created>2013-10-25</created><authors><author><keyname>Manchester</keyname><forenames>Ian R.</forenames></author><author><keyname>Umenberger</keyname><forenames>Jack</forenames></author></authors><title>Real-Time Planning with Primitives for Dynamic Walking over Uneven
  Terrain</title><categories>cs.SY cs.RO</categories><comments>Conference submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm for receding-horizon motion planning using a finite
family of motion primitives for underactuated dynamic walking over uneven
terrain. The motion primitives are defined as virtual holonomic constraints,
and the special structure of underactuated mechanical systems operating subject
to virtual constraints is used to construct closed-form solutions and a special
binary search tree that dramatically speed up motion planning. We propose a
greedy depth-first search and discuss improvement using energy-based
heuristics. The resulting algorithm can plan several footsteps ahead in a
fraction of a second for both the compass-gait walker and a planar
7-Degree-of-freedom/five-link walker.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7093</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7093</id><created>2013-10-26</created><authors><author><keyname>Kurz</keyname><forenames>Alexander</forenames></author><author><keyname>Suzuki</keyname><forenames>Tomoyuki</forenames></author><author><keyname>Tuosto</keyname><forenames>Emilio</forenames></author></authors><title>Nominal Regular Expressions for Languages over Infinite Alphabets.
  Extended Abstract</title><categories>cs.FL</categories><acm-class>F.4.3; F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose regular expressions to abstractly model and study properties of
resource-aware computations. Inspired by nominal techniques -- as those popular
in process calculi -- we extend classical regular expressions with names (to
model computational resources) and suitable operators (for allocation,
deallocation, scoping of, and freshness conditions on resources). We discuss
classes of such nominal regular expressions, show how such expressions have
natural interpretations in terms of languages over infinite alphabets, and give
Kleene theorems to characterise their formal languages in terms of nominal
automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7112</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7112</id><created>2013-10-26</created><authors><author><keyname>Jeon</keyname><forenames>Sang-Woon</forenames></author><author><keyname>Wang</keyname><forenames>Chien-Yi</forenames></author><author><keyname>Gastpar</keyname><forenames>Michael</forenames></author></authors><title>Computation Over Gaussian Networks With Orthogonal Components</title><categories>cs.IT math.IT</categories><comments>30 pages, 12 figures, submitted to IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Function computation of arbitrarily correlated discrete sources over Gaussian
networks with orthogonal components is studied. Two classes of functions are
considered: the arithmetic sum function and the type function. The arithmetic
sum function in this paper is defined as a set of multiple weighted arithmetic
sums, which includes averaging of the sources and estimating each of the
sources as special cases. The type or frequency histogram function counts the
number of occurrences of each argument, which yields many important statistics
such as mean, variance, maximum, minimum, median, and so on. The proposed
computation coding first abstracts Gaussian networks into the corresponding
modulo sum multiple-access channels via nested lattice codes and linear network
coding and then computes the desired function by using linear Slepian-Wolf
source coding. For orthogonal Gaussian networks (with no broadcast and
multiple-access components), the computation capacity is characterized for a
class of networks. For Gaussian networks with multiple-access components (but
no broadcast), an approximate computation capacity is characterized for a class
of networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7114</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7114</id><created>2013-10-26</created><authors><author><keyname>Bauckhage</keyname><forenames>Christian</forenames></author><author><keyname>Kersting</keyname><forenames>Kristian</forenames></author></authors><title>Efficient Information Theoretic Clustering on Discrete Lattices</title><categories>cs.CV</categories><comments>This paper has been presented at the workshop LWA 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of clustering data that reside on discrete, low
dimensional lattices. Canonical examples for this setting are found in image
segmentation and key point extraction. Our solution is based on a recent
approach to information theoretic clustering where clusters result from an
iterative procedure that minimizes a divergence measure. We replace costly
processing steps in the original algorithm by means of convolutions. These
allow for highly efficient implementations and thus significantly reduce
runtime. This paper therefore bridges a gap between machine learning and signal
processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7115</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7115</id><created>2013-10-26</created><authors><author><keyname>Alhawarat</keyname><forenames>Mohammad</forenames></author><author><keyname>Nazih</keyname><forenames>Waleed</forenames></author><author><keyname>Eldesouki</keyname><forenames>Mohammad</forenames></author></authors><title>Studying a Chaotic Spiking Neural Model</title><categories>cs.AI cs.NE</categories><comments>Journal, 13 pages</comments><journal-ref>International Journal of Artificial Intelligence &amp; Applications
  (IJAIA), Vol. 4, No. 5, September 2013</journal-ref><doi>10.5121/ijaia.2013.4508</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamics of a chaotic spiking neuron model are being studied mathematically
and experimentally. The Nonlinear Dynamic State neuron (NDS) is analysed to
further understand the model and improve it. Chaos has many interesting
properties such as sensitivity to initial conditions, space filling, control
and synchronization. As suggested by biologists, these properties may be
exploited and play vital role in carrying out computational tasks in human
brain. The NDS model has some limitations; in thus paper the model is
investigated to overcome some of these limitations in order to enhance the
model. Therefore, the models parameters are tuned and the resulted dynamics are
studied. Also, the discretization method of the model is considered. Moreover,
a mathematical analysis is carried out to reveal the underlying dynamics of the
model after tuning of its parameters. The results of the aforementioned methods
revealed some facts regarding the NDS attractor and suggest the stabilization
of a large number of unstable periodic orbits (UPOs) which might correspond to
memories in phase space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7117</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7117</id><created>2013-10-26</created><authors><author><keyname>Sanderson</keyname><forenames>Yasmine B.</forenames></author></authors><title>On infinite words avoiding a finite set of squares</title><categories>math.CO cs.DM</categories><comments>18 pages</comments><msc-class>68R15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Building an infinite square-free word by appending one letter at a time while
simultaneously avoiding the creation of squares is most likely to fail. When
the alphabet has two letters this approach is impossible. When the alphabet has
three or more letters, one will most probably create a word in which the
addition of any letter invariably creates a square. When one restricts the set
of undesired squares to a finite one, this can be possible. We study the
constraints on the alphabet and the set of squares which permit this approach
to work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7123</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7123</id><created>2013-10-26</created><updated>2014-12-10</updated><authors><author><keyname>Goldenbaum</keyname><forenames>Mario</forenames></author><author><keyname>Boche</keyname><forenames>Holger</forenames></author><author><keyname>Sta&#x144;czak</keyname><forenames>S&#x142;awomir</forenames></author></authors><title>Nomographic Functions: Efficient Computation in Clustered Gaussian
  Sensor Networks</title><categories>cs.IT math.IT</categories><comments>to appear in IEEE Transactions on Wireless Communications</comments><doi>10.1109/TWC.2014.2380317</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a clustered wireless sensor network is considered that is
modeled as a set of coupled Gaussian multiple-access channels. The objective of
the network is not to reconstruct individual sensor readings at designated
fusion centers but rather to reliably compute some functions thereof. Our
particular attention is on real-valued functions that can be represented as a
post-processed sum of pre-processed sensor readings. Such functions are called
nomographic functions and their special structure permits the utilization of
the interference property of the Gaussian multiple-access channel to reliably
compute many linear and nonlinear functions at significantly higher rates than
those achievable with standard schemes that combat interference. Motivated by
this observation, a computation scheme is proposed that combines a suitable
data pre- and post-processing strategy with a nested lattice code designed to
protect the sum of pre-processed sensor readings against the channel noise.
After analyzing its computation rate performance, it is shown that at the cost
of a reduced rate, the scheme can be extended to compute every continuous
function of the sensor readings in a finite succession of steps, where in each
step a different nomographic function is computed. This demonstrates the
fundamental role of nomographic representations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7134</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7134</id><created>2013-10-26</created><updated>2014-12-18</updated><authors><author><keyname>Wright</keyname><forenames>Mason</forenames></author><author><keyname>Sengupta</keyname><forenames>Pratim</forenames></author></authors><title>Modeling Oligarchs' Campaign Donations and Ideological Preferences with
  Simulated Agent-Based Spatial Elections</title><categories>cs.MA physics.soc-ph</categories><comments>appears in Journal of Artificial Societies and Social Simulations,
  2014</comments><acm-class>I.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the interactions among oligarchs, political
parties, and voters using an agent-based modeling approach. We introduce the
OLIGO model, which is based on the spatial model of democracy, where voters
have positions in a policy space and vote for the party that appears closest to
them, and parties move in policy space to seek more votes. We extend the
existing literature on agent-based models of political economy in the following
manner: (1) by introducing a new class of agents- oligarchs - that represent
leaders of firms in a common industry who lobby for beneficial subsidies
through campaign donations; and (2) by investigating the effects of ideological
preferences of the oligarchs on legislative action. We test hypotheses from the
literature in political economics on the behavior of oligarchs and political
parties as they interact, under conditions of imperfect information and bounded
rationality. Our key results indicate that (1) oligarchs tend to donate less to
political campaigns when the parties are more resistant to changing their
policies, or when voters are more informed; and (2) if Oligarchs donate to
parties based on a combination of ideological and profit motivations, Oligarchs
will tend to donate at a lower equilibrium level, due to the influence of lost
profits. We validate these outcomes via comparisons to real world polling data
on changes in party support over time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7137</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7137</id><created>2013-10-26</created><authors><author><keyname>Klimann</keyname><forenames>Ines</forenames><affiliation>LIAFA</affiliation></author><author><keyname>Picantin</keyname><forenames>Matthieu</forenames><affiliation>LIAFA</affiliation></author></authors><title>A characterization of those automata that structurally generate finite
  groups</title><categories>cs.FL math.GR</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Antonenko and Russyev independently have shown that any Mealy automaton with
no cycles with exit--that is, where every cycle in the underlying directed
graph is a sink component--generates a fi- nite (semi)group, regardless of the
choice of the production functions. Antonenko has proved that this constitutes
a characterization in the non-invertible case and asked for the invertible
case, which is proved in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7158</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7158</id><created>2013-10-27</created><authors><author><keyname>Ma</keyname><forenames>Shuai</forenames></author><author><keyname>Hong</keyname><forenames>Mingyi</forenames></author><author><keyname>Song</keyname><forenames>Enbin</forenames></author><author><keyname>Wang</keyname><forenames>Xiangfeng</forenames></author><author><keyname>Sun</keyname><forenames>Dechun</forenames></author></authors><title>Outage Constrained Robust Secure Transmission for MISO Wiretap Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the robust secure beamformer design for MISO
wiretap channels. Assume that the eavesdroppers' channels are only partially
available at the transmitter, we seek to maximize the secrecy rate under the
transmit power and secrecy rate outage probability constraint. The outage
probability constraint requires that the secrecy rate exceeds certain threshold
with high probability. Therefore including such constraint in the design
naturally ensures the desired robustness. Unfortunately, the presence of the
probabilistic constraints makes the problem non-convex and hence difficult to
solve. In this paper, we investigate the outage probability constrained secrecy
rate maximization problem using a novel two-step approach. Under a wide range
of uncertainty models, our developed algorithms can obtain high-quality
solutions, sometimes even exact global solutions, for the robust secure
beamformer design problem. Simulation results are presented to verify the
effectiveness and robustness of the proposed algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7159</identifier>
 <datestamp>2013-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7159</id><created>2013-10-27</created><authors><author><keyname>Eid</keyname><forenames>Abdulla</forenames></author><author><keyname>Duursma</keyname><forenames>Iwan</forenames></author></authors><title>Using concatenated algebraic geometry codes in channel polarization</title><categories>cs.IT math.AG math.IT</categories><comments>8 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polar codes were introduced by Arikan in 2008 and are the first family of
error-correcting codes achieving the symmetric capacity of an arbitrary
binary-input discrete memoryless channel under low complexity encoding and
using an efficient successive cancellation decoding strategy. Recently,
non-binary polar codes have been studied, in which one can use different
algebraic geometry codes to achieve better error decoding probability. In this
paper, we study the performance of binary polar codes that are obtained from
non-binary algebraic geometry codes using concatenation. For binary polar codes
(i.e. binary kernels) of a given length $n$, we compare numerically the use of
short algebraic geometry codes over large fields versus long algebraic geometry
codes over small fields. We find that for each $n$ there is an optimal choice.
For binary kernels of size up to $n \leq 1,800$ a concatenated Reed-Solomon
code outperforms other choices. For larger kernel sizes concatenated Hermitian
codes or Suzuki codes will do better.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7163</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7163</id><created>2013-10-27</created><authors><author><keyname>Li</keyname><forenames>Lihong</forenames></author></authors><title>Generalized Thompson Sampling for Contextual Bandits</title><categories>cs.LG cs.AI stat.ML stat.OT</categories><msc-class>62L05</msc-class><acm-class>I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Thompson Sampling, one of the oldest heuristics for solving multi-armed
bandits, has recently been shown to demonstrate state-of-the-art performance.
The empirical success has led to great interests in theoretical understanding
of this heuristic. In this paper, we approach this problem in a way very
different from existing efforts. In particular, motivated by the connection
between Thompson Sampling and exponentiated updates, we propose a new family of
algorithms called Generalized Thompson Sampling in the expert-learning
framework, which includes Thompson Sampling as a special case. Similar to most
expert-learning algorithms, Generalized Thompson Sampling uses a loss function
to adjust the experts' weights. General regret bounds are derived, which are
also instantiated to two important loss functions: square loss and logarithmic
loss. In contrast to existing bounds, our results apply to quite general
contextual bandits. More importantly, they quantify the effect of the &quot;prior&quot;
distribution on the regret bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7170</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7170</id><created>2013-10-27</created><updated>2013-12-09</updated><authors><author><keyname>Gleibman</keyname><forenames>Andrew</forenames></author></authors><title>Object Recognition System Design in Computer Vision: a Universal
  Approach</title><categories>cs.CV</categories><comments>18 pages, 11 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The first contribution of this paper is architecture of a multipurpose
system, which delegates a range of object detection tasks to a classifier,
applied in special grid positions of the tested image. The second contribution
is Gray Level-Radius Co-occurrence Matrix, which describes local image texture
and topology and, unlike common second order statistics methods, is robust to
image resolution. The third contribution is a parametrically controlled
automatic synthesis of unlimited number of numerical features for
classification. The fourth contribution is a method of optimizing parameters C
and gamma in LibSVM-based classifier, which is 20-100 times faster than the
commonly applied method. The work is essentially experimental, with
demonstration of various methods for definition of objects of interest in
images and video sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7197</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7197</id><created>2013-10-27</created><authors><author><keyname>Bygi</keyname><forenames>Mojtaba Nouri</forenames></author><author><keyname>Ghodsi</keyname><forenames>Mohammad</forenames></author></authors><title>Weak Visibility Queries of Line Segments in Simple Polygons and
  Polygonal Domains</title><categories>cs.CG</categories><comments>10 pages. arXiv admin note: substantial text overlap with
  arXiv:1309.7803</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the problem of computing the weak visibility
polygon of any query line segment $pq$ (or $WVP(pq)$) inside a given polygon
$P$. Our first non-trivial algorithm runs in simple polygons and needs $O(n^3
\log n)$ time and $O(n^3)$ space in the preprocessing phase to report $WVP(pq)$
of any query line segment $pq$ in time $O(\log n + |WVP(pq)|)$. We also give an
algorithm to compute the weak visibility polygon of a query line segment in a
non-simple polygon with $h$ pairwise-disjoint polygonal obstacles with a total
of $n$ vertices. Our algorithm needs $O(n^2 \log n)$ time and $O(n^2)$ space in
the preprocessing phase and computes $WVP(pq)$ in query time of $O(n\hbar \log
n + k)$, in which $\hbar$ is an output sensitive parameter of at most
$\min(h,k)$, and $k = O(n^2h^2)$ is the output size. This is the best
query-time result on this problem so far.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7198</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7198</id><created>2013-10-27</created><updated>2014-06-27</updated><authors><author><keyname>Ji</keyname><forenames>Kaihua</forenames></author><author><keyname>Liu</keyname><forenames>Jiawei</forenames></author><author><keyname>Xiang</keyname><forenames>Gang</forenames></author></authors><title>Anti-rumor dynamics and emergence of the timing threshold on complex
  network</title><categories>physics.soc-ph cs.SI</categories><comments>11 pages, 5 figures, Physica A: Statistical Mechanics and its
  Applications (2014)</comments><doi>10.1016/j.physa.2014.06.013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Anti-rumor dynamics is proposed on the basis of rumor dynamics and the
characteristics of anti-rumor dynamics are explored by both mean-field
equations and numerical simulations on complex network. The main metrics we
study are the timing effect of combating rumor and the identification of
influential nodes, which are what an efficient strategy against rumor may
concern about. The results indicate that, there exists robust time dependence
of anti-rumor dynamics and the timing threshold emerges as a consequence of
launching the anti-rumor at different delay time after the beginning of rumor
spreading. The timing threshold as a critical feature is further verified on a
series of Barabasi-Albert scale-free networks (BA networks), where anti-rumor
dynamics arises explicitly. The timing threshold is a network-dependent
quantity and its value decreases as the average degree of the BA network
increases until close to zero. Meanwhile, coreness also constitutes a better
topological descriptor to identify hubs. Our results will hopefully be useful
for the understanding of spreading behaviors of rumor and anti-rumor and
suggest a possible avenue for further study of interplays of multiple pieces of
information on complex network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7202</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7202</id><created>2013-10-27</created><updated>2016-01-30</updated><authors><author><keyname>Shabat</keyname><forenames>Gil</forenames></author><author><keyname>Shmueli</keyname><forenames>Yaniv</forenames></author><author><keyname>Aizenbud</keyname><forenames>Yariv</forenames></author><author><keyname>Averbuch</keyname><forenames>Amir</forenames></author></authors><title>Randomized LU Decomposition</title><categories>math.NA cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a fast randomized algorithm that computes a low rank LU
decomposition. Our algorithm uses random projections type techniques to
efficiently compute a low rank approximation of large matrices. The randomized
LU algorithm can be parallelized and further accelerated by using sparse random
matrices in its projection step. Several different error bounds are proven for
the algorithm approximations. To prove these bounds, recent results from random
matrix theory related to subgaussian matrices are used. As an application, we
also show how the algorithm can be utilized to solve problems such as the
rank-deficient least squares problem. Numerical examples, which illustrate the
performance of the algorithm and compare it to other decomposition methods, are
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7205</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7205</id><created>2013-10-27</created><authors><author><keyname>Schattka</keyname><forenames>Moritz</forenames></author></authors><title>Algorithms for Timed Consistency Models</title><categories>cs.DC cs.DB cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the major challenges in distributed systems is establishing
consistency among replicated data in a timely fashion. While the consistent
ordering of events has been extensively researched, the time span to reach a
consistent state is mostly considered an effect of the chosen consistency
model, rather than being considered a parameter itself. This paper argues that
it is possible to give guarantees on the timely consistency of an operation.
Subsequent to an update the cloud and all connected clients will either be
consistent with the update within the defined upper bound of time or the update
will be returned. This paper suggests the respective algorithms and protocols
capable of producing such comprehensive Timed Consistency, as conceptually
proposed by Torres-Rojas et al. The solution offers business customers an
increasing level of predictability and adjustability. The temporal certainty
concerning the execution makes the cloud a more attractive tool for
time-critical or mission-critical applications fearing the poor availability of
Strong Consistency in cloud environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7215</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7215</id><created>2013-10-27</created><updated>2014-05-25</updated><authors><author><keyname>Iatsenko</keyname><forenames>Dmytro</forenames></author><author><keyname>McClintock</keyname><forenames>Peter V. E.</forenames></author><author><keyname>Stefanovska</keyname><forenames>Aneta</forenames></author></authors><title>Linear and synchrosqueezed time-frequency representations revisited.
  Part I: Overview, standards of use, related issues and algorithms</title><categories>math.NA cs.NA physics.data-an</categories><comments>45 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time-frequency representations (TFRs) of signals, such as the windowed
Fourier transform (WFT), wavelet transform (WT) and their synchrosqueezed
variants (SWFT, SWT), provide powerful analysis tools. However, there are many
important issues related to the practical use of TFRs that need to be
clarified. Here we present a thorough review of these TFRs, summarizing all
theoretical, practical and numerical aspects of their use, reconsidering some
conventions and introducing new concepts and procedures. The purposes of this
work are: (i) to provide a consistent overview of the computation, properties,
and use of the (S)WFT/(S)WT methods; (ii) to establish general standards
related to their use, both theoretical and practical; and (iii) to provide
clean and optimized algorithms and MatLab codes, appropriate for any window or
wavelet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7217</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7217</id><created>2013-10-27</created><authors><author><keyname>Fang</keyname><forenames>Jian</forenames></author><author><keyname>Xu</keyname><forenames>Zongben</forenames></author><author><keyname>Zhang</keyname><forenames>Bingchen</forenames></author><author><keyname>Hong</keyname><forenames>Wen</forenames></author><author><keyname>Wu</keyname><forenames>Yirong</forenames></author></authors><title>Compressed Sensing SAR Imaging with Multilook Processing</title><categories>cs.IT cs.CV math.IT</categories><comments>Will be submitted to GRS letter</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multilook processing is a widely used speckle reduction approach in synthetic
aperture radar (SAR) imaging. Conventionally, it is achieved by incoherently
summing of some independent low-resolution images formulated from overlapping
subbands of the SAR signal. However, in the context of compressive sensing (CS)
SAR imaging, where the samples are collected at sub-Nyquist rate, the data
spectrum is highly aliased that hinders the direct application of the existing
multilook techniques. In this letter, we propose a new CS-SAR imaging method
that can realize multilook processing simultaneously during image
reconstruction. The main idea is to replace the SAR observation matrix by the
inverse of multilook procedures, which is then combined with random sampling
matrix to yield a multilook CS-SAR observation model. Then a joint sparse
regularization model, considering pixel dependency of subimages, is derived to
form multilook images. The suggested SAR imaging method can not only
reconstruct sparse scene efficiently below Nyquist rate, but is also able to
achieve a comparable reduction of speckles during reconstruction. Simulation
results are finally provided to demonstrate the effectiveness of the proposed
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7232</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7232</id><created>2013-10-27</created><authors><author><keyname>Christman</keyname><forenames>Ananya</forenames></author><author><keyname>Forcier</keyname><forenames>William</forenames></author></authors><title>Maximizing Revenues for Online-Dial-a-Ride</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the classic Dial-a-Ride Problem, a server travels in some metric space to
serve requests for rides. Each request has a source, destination, and release
time. We study a variation of this problem where each request also has a
revenue that is earned if the request is satisfied. The goal is to serve
requests within a time limit such that the total revenue is maximized. We first
prove that the version of this problem where edges in the input graph have
varying weights is NP-complete. We also prove that no algorithm can be
competitive for this problem. We therefore consider the version where edges in
the graph have unit weight and develop a 2-competitive algorithm for this
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7247</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7247</id><created>2013-10-27</created><authors><author><keyname>Garnaev</keyname><forenames>Andrey</forenames></author><author><keyname>Trappe</keyname><forenames>Wade</forenames></author><author><keyname>Kung</keyname><forenames>Chun-Ta</forenames></author></authors><title>Optimizing scanning strategies: Selecting scanning bandwidth in
  adversarial RF environments</title><categories>cs.GT</categories><journal-ref>2013 8th International Conference on Cognitive Radio Oriented
  Wireless Networks (CROWNCOM), 8-10 July 2013, Washington, DC, USA, pp.
  148-153</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate the problem of designing a spectrum scanning
strategy to detect an intelligent Invader who wants to utilize spectrum
undetected for his/her unapproved purposes. To deal with this problem we apply
game-theoretical tools. We model the situation as a game between a Scanner and
an Invader where the Invader faces a dilemma: the more bandwidth the Invader
attempts to use leads to a larger payoff if he is not detected, but at the same
time also increases the probability of being detected and thus fined.
Similarly, the Scanner faces a dilemma: the wider the bandwidth scanned, the
higher the probability of detecting the Invader, but at the expense of
increasing the cost of building the scanning system. The equilibrium strategies
are found explicitly and reveal interesting properties. In particular, we have
found a discontinuous dependence of the equilibrium strategies on the network
parameters, fine and the type of the Invader's award. This discontinuity on
fine means that the network provider has to take into account a human factor
since some threshold values of fine could be very sensible for the Invader,
while in other situations simply increasing the fine has minimal deterrence
impact. Also we show how different reward types for the Invader (e.g. motivated
by using different type of application, say, video-streaming or downloading
files) can be incorporated into scanning strategy to increase its efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7262</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7262</id><created>2013-10-27</created><authors><author><keyname>Cheong</keyname><forenames>Seunggyun</forenames></author><author><keyname>Manchester</keyname><forenames>Ian R.</forenames></author></authors><title>Input Design for Model Discrimination and Fault Detection via Convex
  Relaxation</title><categories>cs.SY math.OC</categories><comments>Extended version of conference submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the design of input signals for the purpose of
discriminating among a finite set of models dynamic systems within a given
finite time interval. A motivating application is fault detection and
isolation. We propose several specific optimization problems, with objectives
or constraints based on signal power, signal amplitude, and probability of
successful model discrimination. Since these optimization problems are
nonconvex, we suggest a suboptimal solution via a random search algorithm
guided by the semidefinite relaxation (SDR) and analyze the accuracy of the
suboptimal solution. We conclude with a simple example taken from a benchmark
problem on fault detection for wind turbines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7274</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7274</id><created>2013-10-27</created><updated>2014-05-25</updated><authors><author><keyname>Iatsenko</keyname><forenames>Dmytro</forenames></author><author><keyname>McClintock</keyname><forenames>Peter V. E.</forenames></author><author><keyname>Stefanovska</keyname><forenames>Aneta</forenames></author></authors><title>Linear and synchrosqueezed time-frequency representations revisited.
  Part II: Resolution, reconstruction and concentration</title><categories>math.NA cs.NA physics.data-an</categories><comments>39 pages, 28 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Having reviewed the aspects of the linear and synchrosqueezed time-frequency
representations (TFRs) needed for their understanding and correct use in Part I
of this review, we now consider three more subtle issues that are nonetheless
of crucial importance for effective application of these methods. (i) What
effect do the window/wavelet parameters have on the resultant TFR, and how can
they most appropriately be chosen? (ii) What are the errors inherent in the two
reconstruction methods (direct and ridge) and which of them is the better?
(iii) What are the advantages and drawbacks associated with synchrosqueezing?
To answer these questions, we perform a detailed numerical and theoretical
study of the TFRs under consideration. We consider the relevant estimates in
the presence of the complications that arise in practical applications
including interference between components, amplitude modulation, frequency
modulation, and noise. Taken together, the results provide an in-depth
understanding of the issues in question.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7276</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7276</id><created>2013-10-27</created><updated>2015-09-27</updated><authors><author><keyname>Iatsenko</keyname><forenames>Dmytro</forenames></author><author><keyname>McClintock</keyname><forenames>Peter V. E.</forenames></author><author><keyname>Stefanovska</keyname><forenames>Aneta</forenames></author></authors><title>On the extraction of instantaneous frequencies from ridges in
  time-frequency representations of signals</title><categories>cs.CE math.NA physics.data-an</categories><comments>13 pages, 7 figures, plus 4 supplementary figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The extraction of oscillatory components and their properties from different
time-frequency representations, such as windowed Fourier transform and wavelet
transform, is an important topic in signal processing. The first step in this
procedure is to find an appropriate ridge curve: a sequence of amplitude peak
positions (ridge points), corresponding to the component of interest. This is
not a trivial issue, and the optimal method for extraction is still not settled
or agreed. We discuss and develop procedures that can be used for this task and
compare their performance on both simulated and real data. In particular, we
propose a method which, in contrast to many other approaches, is highly
adaptive so that it does not need any parameter adjustment for the signal to be
analysed. Being based on dynamic path optimization and fixed point iteration,
the method is very fast, and its superior accuracy is also demonstrated. In
addition, we investigate the advantages and drawbacks that synchrosqueezing
offers in relation to curve extraction. The codes used in this work are freely
available for download.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7282</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7282</id><created>2013-10-27</created><authors><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author></authors><title>Massive MIMO Systems: Signal Processing Challenges and Research Trends</title><categories>cs.IT math.IT</categories><comments>10 pages, 5 figures. URSI Radio Science Bulletin, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents a tutorial on multiuser multiple-antenna wireless
systems with a very large number of antennas, known as massive multi-input
multi-output (MIMO) systems. Signal processing challenges and future trends in
the area of massive MIMO systems are presented and key application scenarios
are detailed. A linear algebra approach is considered for the description of
the system and data models of massive MIMO architectures. The operational
requirements of massive MIMO systems are discussed along with their operation
in time-division duplexing mode, resource allocation and calibration
requirements. In particular, transmit and receiver processing algorithms are
examined in light of the specific needs of massive MIMO systems. Simulation
results illustrate the performance of transmit and receive processing
algorithms under scenarios of interest. Key problems are discussed and future
trends in the area of massive MIMO systems are pointed out.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7297</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7297</id><created>2013-10-27</created><authors><author><keyname>Choudhury</keyname><forenames>Farhana Murtaza</forenames></author><author><keyname>Ali</keyname><forenames>Mohammed Eunus</forenames></author><author><keyname>Masud</keyname><forenames>Sarah</forenames></author><author><keyname>Nath</keyname><forenames>Suman</forenames></author><author><keyname>Rabban</keyname><forenames>Ishat E</forenames></author></authors><title>Scalable Visibility Color Map Construction in Spatial Databases</title><categories>cs.DB</categories><comments>12 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in 3D modeling provide us with real 3D datasets to answer
queries, such as &quot;What is the best position for a new billboard?&quot; and &quot;Which
hotel room has the best view?&quot; in the presence of obstacles. These applications
require measuring and differentiating the visibility of an object (target) from
different viewpoints in a dataspace, e.g., a billboard may be seen from two
viewpoints but is readable only from the viewpoint closer to the target. In
this paper, we formulate the above problem of quantifying the visibility of
(from) a target object from (of) the surrounding area with a visibility color
map (VCM). A VCM is essentially defined as a surface color map of the space,
where each viewpoint of the space is assigned a color value that denotes the
visibility measure of the target from that viewpoint. Measuring the visibility
of a target even from a single viewpoint is an expensive operation, as we need
to consider factors such as distance, angle, and obstacles between the
viewpoint and the target. Hence, a straightforward approach to construct the
VCM that requires visibility computation for every viewpoint of the surrounding
space of the target, is prohibitively expensive in terms of both I/Os and
computation, especially for a real dataset comprising of thousands of
obstacles. We propose an efficient approach to compute the VCM based on a key
property of the human vision that eliminates the necessity of computing the
visibility for a large number of viewpoints of the space. To further reduce the
computational overhead, we propose two approximations; namely, minimum bounding
rectangle and tangential approaches with guaranteed error bounds. Our extensive
experiments demonstrate the effectiveness and efficiency of our solutions to
construct the VCM for real 2D and 3D datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7300</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7300</id><created>2013-10-27</created><updated>2015-08-31</updated><authors><author><keyname>Guan</keyname><forenames>Peng</forenames></author><author><keyname>Raginsky</keyname><forenames>Maxim</forenames></author><author><keyname>Willett</keyname><forenames>Rebecca</forenames></author></authors><title>Relax but stay in control: from value to algorithms for online Markov
  decision processes</title><categories>cs.LG math.OC stat.ML</categories><comments>40 pages; additional results in the convex-analytic framework</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online learning algorithms are designed to perform in non-stationary
environments, but generally there is no notion of a dynamic state to model
constraints on current and future actions as a function of past actions.
State-based models are common in stochastic control settings, but commonly used
frameworks such as Markov Decision Processes (MDPs) assume a known stationary
environment. In recent years, there has been a growing interest in combining
the above two frameworks and considering an MDP setting in which the cost
function is allowed to change arbitrarily after each time step. However, most
of the work in this area has been algorithmic: given a problem, one would
develop an algorithm almost from scratch. Moreover, the presence of the state
and the assumption of an arbitrarily varying environment complicate both the
theoretical analysis and the development of computationally efficient methods.
This paper describes a broad extension of the ideas proposed by Rakhlin et al.
to give a general framework for deriving algorithms in an MDP setting with
arbitrarily changing costs. This framework leads to a unifying view of existing
methods and provides a general procedure for constructing new ones. Several new
methods are presented, and one of them is shown to have important advantages
over a similar method developed from scratch via an online version of
approximate dynamic programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7305</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7305</id><created>2013-10-27</created><authors><author><keyname>Hassibi</keyname><forenames>Babak</forenames></author><author><keyname>Hansen</keyname><forenames>Morten</forenames></author><author><keyname>Dimakis</keyname><forenames>Alexandros Georgios</forenames></author><author><keyname>Alshamary</keyname><forenames>Haider Ali Jasim</forenames></author><author><keyname>Xu</keyname><forenames>Weiyu</forenames></author></authors><title>Optimized Markov Chain Monte Carlo for Signal Detection in MIMO Systems:
  an Analysis of Stationary Distribution and Mixing Time</title><categories>cs.IT math.IT</categories><comments>14 pages. arXiv admin note: substantial text overlap with
  arXiv:1203.2213</comments><doi>10.1109/TSP.2014.2334558</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce an optimized Markov Chain Monte Carlo (MCMC)
technique for solving the integer least-squares (ILS) problems, which include
Maximum Likelihood (ML) detection in Multiple-Input Multiple-Output (MIMO)
systems. Two factors contribute to the speed of finding the optimal solution by
the MCMC detector: the probability of the optimal solution in the stationary
distribution, and the mixing time of the MCMC detector. Firstly, we compute the
optimal value of the &quot;temperature&quot; parameter, in the sense that the temperature
has the desirable property that once the Markov chain has mixed to its
stationary distribution, there is polynomially small probability
($1/\mbox{poly}(N)$, instead of exponentially small) of encountering the
optimal solution. This temperature is shown to be at most
$O(\sqrt{SNR}/\ln(N))$, where $SNR$ is the signal-to-noise ratio, and $N$ is
the problem dimension. Secondly, we study the mixing time of the underlying
Markov chain of the proposed MCMC detector. We find that, the mixing time of
MCMC is closely related to whether there is a local minimum in the lattice
structures of ILS problems. For some lattices without local minima, the mixing
time of the Markov chain is independent of $SNR$, and grows polynomially in the
problem dimension; for lattices with local minima, the mixing time grows
unboundedly as $SNR$ grows, when the temperature is set, as in conventional
wisdom, to be the standard deviation of noises. Our results suggest that, to
ensure fast mixing for a fixed dimension $N$, the temperature for MCMC should
instead be set as $\Omega(\sqrt{SNR})$ in general. Simulation results show that
the optimized MCMC detector efficiently achieves approximately ML detection in
MIMO systems having a huge number of transmit and receive dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7311</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7311</id><created>2013-10-28</created><authors><author><keyname>Liu</keyname><forenames>Tingting</forenames></author><author><keyname>Yang</keyname><forenames>Chenyang</forenames></author></authors><title>On the Degrees of Freedom of Asymmetric MIMO Interference Broadcast
  Channels</title><categories>cs.IT math.IT</categories><comments>6 pages, 3 figures, submitted to ICC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the degrees of freedom (DoF) of the asymmetric
multi-input-multi-output interference broadcast channel (MIMO-IBC). By
introducing a notion of connection pattern chain, we generalize the genie chain
proposed in [11] to derive and prove the necessary condition of IA feasibility
for asymmetric MIMO-IBC, which is denoted as irreducible condition. It is
necessary for both linear interference alignment (IA) and asymptotic IA
feasibility in MIMO-IBC with arbitrary configurations. In a special class of
asymmetric two-cell MIMOIBC, the irreducible condition is proved to be the
sufficient and necessary condition for asymptotic IA feasibility, while the
combination of proper condition and irreducible condition is proved to the
sufficient and necessary condition for linear IA feasibility. From these
conditions, we derive the information theoretic maximal DoF per user and the
maximal DoF per user achieved by linear IA, and these DoFs are also the DoF per
user upper-bounds of asymmetric G-cell MIMO-IBC with asymptotic IA and linear
IA, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7320</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7320</id><created>2013-10-28</created><updated>2013-11-15</updated><authors><author><keyname>Donoho</keyname><forenames>David</forenames></author><author><keyname>Montanari</keyname><forenames>Andrea</forenames></author></authors><title>High Dimensional Robust M-Estimation: Asymptotic Variance via
  Approximate Message Passing</title><categories>math.ST cs.IT math.IT stat.TH</categories><comments>32 pages, 5 figures (v2 contains numerical simulations)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a recent article (Proc. Natl. Acad. Sci., 110(36), 14557-14562), El Karoui
et al. study the distribution of robust regression estimators in the regime in
which the number of parameters p is of the same order as the number of samples
n. Using numerical simulations and `highly plausible' heuristic arguments, they
unveil a striking new phenomenon. Namely, the regression coefficients contain
an extra Gaussian noise component that is not explained by classical concepts
such as the Fisher information matrix. We show here that that this phenomenon
can be characterized rigorously techniques that were developed by the authors
to analyze the Lasso estimator under high-dimensional asymptotics. We introduce
an approximate message passing (AMP) algorithm to compute M-estimators and
deploy state evolution to evaluate the operating characteristics of AMP and so
also M-estimates. Our analysis clarifies that the `extra Gaussian noise'
encountered in this problem is fundamentally similar to phenomena already
studied for regularized least squares in the setting n&lt;p.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7321</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7321</id><created>2013-10-28</created><authors><author><keyname>Hamed</keyname><forenames>Aya</forenames></author><author><keyname>Lee</keyname><forenames>Troy</forenames></author></authors><title>Rank and fooling set size</title><categories>cs.CC math.CO</categories><comments>8 pages</comments><msc-class>15B36</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Say that A is a Hadamard factorization of the identity I_n of size n if the
entrywise product of A and the transpose of A is I_n. It can be easily seen
that the rank of any Hadamard factorization of the identity must be at least
sqrt{n}. Dietzfelbinger et al. raised the question if this bound can be
achieved, and showed a boolean Hadamard factorization of the identity of rank
n^{0.792}. More recently, Klauck and Wolf gave a construction of Hadamard
factorizations of the identity of rank n^{0.613}. Over finite fields, Friesen
and Theis resolved the question, showing for a prime p and r=p^t+1 a Hadamard
factorization of the identity A of size r(r-1)+1 and rank r over F_p.
  Here we resolve the question for fields of zero characteristic, up to a
constant factor, giving a construction of Hadamard factorizations of the
identity of rank r and size (r+1)r/2. The matrices in our construction are
blockwise Toeplitz, and have entries whose magnitudes are binomial
coefficients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7324</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7324</id><created>2013-10-28</created><updated>2014-10-22</updated><authors><author><keyname>Talarico</keyname><forenames>Salvatore</forenames></author><author><keyname>Schmid</keyname><forenames>Natalia A.</forenames></author><author><keyname>Alkhweldi</keyname><forenames>Marwan</forenames></author><author><keyname>Valenti</keyname><forenames>Matthew C.</forenames></author></authors><title>Distributed Estimation of a Parametric Field: Algorithms and Performance
  Analysis</title><categories>cs.IT math.IT</categories><comments>12 pages, 12 figures, IEEE Transactions on Signal Processing,
  accepted for publication</comments><doi>10.1109/TSP.2013.2288684</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a distributed estimator for a deterministic parametric
physical field sensed by a homogeneous sensor network and develops a new
transformed expression for the Cramer-Rao lower bound (CRLB) on the variance of
distributed estimates. The proposed transformation reduces a multidimensional
integral representation of the CRLB to an expression involving an infinite sum.
Stochastic models used in this paper assume additive noise in both the
observation and transmission channels. Two cases of data transmission are
considered. The first case assumes a linear analog modulation of raw
observations prior to their transmission to a fusion center. In the second
case, each sensor quantizes its observation to $M$ levels, and the quantized
data are communicated to a fusion center. In both cases, parallel additive
white Gaussian channels are assumed. The paper develops an iterative
expectation-maximization (EM) algorithm to estimate unknown parameters of a
parametric field, and its linearized version is adopted for numerical analysis.
The performance of the developed numerical solution is compared to the
performance of a simple iterative approach based on Newton's approximation.
While the developed solution has a higher complexity than Newton's solution, it
is more robust with respect to the choice of initial parameters and has a
better estimation accuracy. Numerical examples are provided for the case of a
field modeled as a Gaussian bell, and illustrate the advantages of using the
transformed expression for the CRLB. However, the distributed estimator and the
derived CRLB are general and can be applied to any parametric field. The
dependence of the mean-square error (MSE) on the number of quantization levels,
the number of sensors in the network and the SNR of the observation and
transmission channels are analyzed. The variance of the estimates is compared
to the derived CRLB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7346</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7346</id><created>2013-10-28</created><authors><author><keyname>Mathiesen</keyname><forenames>Joachim</forenames></author><author><keyname>Angheluta</keyname><forenames>Luiza</forenames></author><author><keyname>Ahlgren</keyname><forenames>Peter T. H.</forenames></author><author><keyname>Jensen</keyname><forenames>Mogens H.</forenames></author></authors><title>Excitable human dynamics driven by extrinsic events in massive
  communities</title><categories>physics.soc-ph cs.CE cs.SI</categories><comments>9 pages, 3 figures</comments><journal-ref>Proceedings of the National Academy of Sciences 110, no. 43
  (2013): 17259-17262</journal-ref><doi>10.1073/pnas.1304179110</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using empirical data from a social media site (Twitter) and on trading
volumes of financial securities, we analyze the correlated human activity in
massive social organizations. The activity, typically excited by real-world
events and measured by the occurrence rate of international brand names and
trading volumes, is characterized by intermittent fluctuations with bursts of
high activity separated by quiescent periods. These fluctuations are broadly
distributed with an inverse cubic tail and have long-range temporal
correlations with a $1/f$ power spectrum. We describe the activity by a
stochastic point process and derive the distribution of activity levels from
the corresponding stochastic differential equation. The distribution and the
corresponding power spectrum are fully consistent with the empirical
observations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7353</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7353</id><created>2013-10-28</created><authors><author><keyname>Erfan</keyname><forenames>Azadeh</forenames></author></authors><title>Determinants of a Successful Migration to Cloud Computing in Iranian
  Telecommunication Industry</title><categories>cs.CY</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although the research support for Cloud Computing (CC) is still developing,
the concept of this paper has provided comprehensive frameworks for a
successful migration to cloud computing (SMCC) in telecommunication industry in
Iran. Using an academic orientation, the conceptual research is focusing on the
determinants of a successful migration from legacy to cloud computing. The
study attempts to reveal the constructive effects and deconstructive defects of
migration to cloud computing with a close regard into prior literature and
practical practices all around the world. Conceptual frameworks are deducted
from the literature and Telco's revolutionary movements toward cloud computing.
The confirmatory quantitative approach tries to verify or reject the validity
of determinants of successful migration. The study reports that there are some
success and failure factors which are influencing a successful migration of
data centres and servers of Iranian Telecommunication to the cloud. Considering
these approved factors before any migration decision would be valuable for
engaged project members and finally for Telecommunication organization. This
paper can be used as reliable model for any migration beforehand taking any
action. Enforcing proper determinants in accordance with the prior success
stories and academia viewpoints in Telecommunication industry in Iran as a
first mover research in this field provides a precious insight for policy and
decision makers to change their mindset and grant a proper space for cloud
computing to grow in this industry due to its advantages. Obviously, like any
other big Telco in the world, Iranian Telco might start cloud projects to
sustain its presence in the global market.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7367</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7367</id><created>2013-10-28</created><authors><author><keyname>Slimani</keyname><forenames>Thabet</forenames></author></authors><title>Semantic Description of Web Services</title><categories>cs.AI</categories><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 10,
  Issue 1, No 3, January 2013, ISSN (Print): 1694-0784 | ISSN (Online):
  1694-0814</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The tasks of semantic web service (discovery, selection, composition, and
execution) are supposed to enable seamless interoperation between systems,
whereby human intervention is kept at a minimum. In the field of Web service
description research, the exploitation of descriptions of services through
semantics is a better support for the life-cycle of Web services. The large
number of developed ontologies, languages of representations, and integrated
frameworks supporting the discovery, composition and invocation of services is
a good indicator that research in the field of Semantic Web Services (SWS) has
been considerably active. We provide in this paper a detailed classification of
the approaches and solutions, indicating their core characteristics and
objectives required and provide indicators for the interested reader to follow
up further insights and details about these solutions and related software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7368</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7368</id><created>2013-10-28</created><authors><author><keyname>Ghazanfari-Rad</keyname><forenames>Saeed</forenames></author><author><keyname>Labeau</keyname><forenames>Fabrice</forenames></author></authors><title>Formulation and Steady-state Analysis of LMS Adaptive Networks for
  Distributed Estimation in the Presence of Transmission Errors</title><categories>cs.SY cs.NI</categories><comments>28 pages, 10 figures, submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents the formulation and steady-state analysis of the
distributed estimation algorithms based on the diffusion cooperation scheme in
the presence of errors due to the unreliable data transfer among nodes. In
particular, we highlight the impact of transmission errors on the least-mean
squares (LMS) adaptive networks. We develop the closed-form expressions of the
steady-state mean-square deviation (MSD) which is helpful to assess the effects
of the imperfect information flow on on the behavior of the diffusion LMS
algorithm in terms of the steady-state error. The model is then validated by
performing Monte Carlo simulations. It is shown that local and global MSD
curves are not necessarily monotonic increasing functions of the error
probability. We also assess sufficient conditions that ensure mean and
mean-square stability of diffusion LMS strategies in the presence of
transmission errors. Moreover, issues such as scalability in the sense of
network size and regressor size, spatially correlated observations, as well as
the effect of the distribution of the noise variance are studied.
  While the proposed theoretical framework is general in the sense that it is
not confined to a particular source of error during information diffusion, for
practical reasons we additionally study a specific scenario where errors occur
at the medium access control (MAC) level. We develop a model to quantify the
MAC-level transmission errors according to the network topology and system
parameters for a set of nodes employing a backoff procedure to access the
channel. To overcome the problem of unreliable data exchange, we propose an
enhanced combining rule that can be deployed in order to improve the
performance of diffusion estimation algorithms by using the knowledge of the
properties of the transmission errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7376</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7376</id><created>2013-10-28</created><authors><author><keyname>Das</keyname><forenames>Rajib K</forenames></author></authors><title>Eccentricity of the nodes of OTIS-cube and Enhanced OTIS-cube</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we have classified the nodes of OTIS-cube based on their
eccentricities. OTIS (optical transpose interconnection system) is a large
scale optoelectronic computer architecture, proposed in \cite{KMKE92}, that
benefit from both optical and electronic technologies. We show that radius and
diameter of OTIS-$Q_n$ is $n+1$ and $2n+1$ respectively. We also show that
average eccentricity of OTIS-cube is $(3n/2+1)$.
  In \cite{D05}, a variant of OTIS-cube, called Enhanced OTIS-cube
(E-OTIS-$Q_n$) was proposed.
  E-OTIS-$Q_n$ is regular of degree $n+1$ and maximally fault-tolerant.
  In this paper we have given a classification of the nodes of E-OTIS cube and
derived expressions for the eccentricities of the nodes in each class. Based on
these results we show that radius and diameter of E-OTIS-$Q_n$ is $n+1$ and
$\lfloor {4n+4/3} \rfloor$ respectively. We have also computed the average
eccentricity of E-OTIS-$Q_n$ for values of $n$ upto 20.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7378</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7378</id><created>2013-10-28</created><authors><author><keyname>Repiso</keyname><forenames>Rafael</forenames></author><author><keyname>Lopez-Cozar</keyname><forenames>Emilio Delgado</forenames></author></authors><title>H Index Communication Journals according to Google Scholar Metrics
  (2008-2012)</title><categories>cs.DL</categories><comments>13 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The aim of this report is to present a ranking of Communication journals
covered in Google Scholar Metrics for the period 2008-2012. It corresponds to
the H Index update made last year for the period 2007-2011 (Delgado
L\'opez-C\'ozar and Repiso 2013). Google Scholar Metrics doesnt currently allow
to group and sort all journals belonging to a scientific discipline. In the
case of Communication, in the ten listings displayed by GSM we can only locate
46 journals. Therefore, in an attempt to overcome this limitation, we have used
the diversity of search procedures allowed by GSM to identify the greatest
number of scientific journals of Communication with H Index calculated by this
bibliometric tool. The result is a ranking of 354 communication journals sorted
by the same H Index, and mean as discriminating value. Journals are also
grouped by quartiles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7397</identifier>
 <datestamp>2013-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7397</id><created>2013-10-28</created><updated>2013-10-29</updated><authors><author><keyname>Golab</keyname><forenames>Wojciech</forenames></author></authors><title>Deconstructing Queue-Based Mutual Exclusion</title><categories>cs.DC</categories><report-no>HPL-2012-100</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We formulate a modular approach to the design and analysis of a particular
class of mutual exclusion algorithms for shared memory multiprocessor systems.
Specifically, we consider algorithms that organize waiting processes into a
queue. Such algorithms can achieve O(1) remote memory reference (RMR)
complexity, which minimizes (asymptotically) the amount of traffic through the
processor-memory interconnect. We first describe a generic mutual exclusion
algorithm that relies on a linearizable implementation of a particular
queue-like data structure that we call MutexQueue. Next, we show two
implementations of MutexQueue using O(1) RMRs per operation based on
synchronization primitives commonly available in multiprocessors. These
implementations follow closely the queuing code embedded in previously
published mutual exclusion algorithms. We provide rigorous correctness proofs
and RMR complexity analyses of the algorithms we present.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7409</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7409</id><created>2013-10-28</created><authors><author><keyname>Wild</keyname><forenames>Sebastian</forenames></author><author><keyname>Nebel</keyname><forenames>Markus E.</forenames></author></authors><title>Average Case Analysis of Java 7's Dual Pivot Quicksort</title><categories>cs.DS math.PR</categories><comments>Best paper award at ESA 2012, recorded talk:
  http://www.slideshare.net/sebawild/average-case-analysis-of-java-7s-dual-pivot-quicksort</comments><journal-ref>In L. Epstein &amp; P. Ferragina (Eds.), ESA 2012 (LNCS 7501, pp.
  825-836). Springer Berlin/Heidelberg</journal-ref><doi>10.1007/978-3-642-33090-2_71</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, a new Quicksort variant due to Yaroslavskiy was chosen as standard
sorting method for Oracle's Java 7 runtime library. The decision for the change
was based on empirical studies showing that on average, the new algorithm is
faster than the formerly used classic Quicksort. Surprisingly, the improvement
was achieved by using a dual pivot approach, an idea that was considered not
promising by several theoretical studies in the past. In this paper, we
identify the reason for this unexpected success. Moreover, we present the first
precise average case analysis of the new algorithm showing e.g. that a random
permutation of length $n$ is sorted using $1.9n\ln n-2.46n+\mathcal{O}(\ln n)$
key comparisons and $0.6n\ln n+0.08n+\mathcal{O}(\ln n)$ swaps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7417</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7417</id><created>2013-10-28</created><updated>2013-12-03</updated><authors><author><keyname>Kerstan</keyname><forenames>Henning</forenames><affiliation>Universit&#xe4;t Duisburg-Essen</affiliation></author><author><keyname>K&#xf6;nig</keyname><forenames>Barbara</forenames><affiliation>Universit&#xe4;t Duisburg-Essen</affiliation></author></authors><title>Coalgebraic Trace Semantics for Continuous Probabilistic Transition
  Systems</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 4 (December
  4, 2013) lmcs:859</journal-ref><doi>10.2168/LMCS-9(4:16)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coalgebras in a Kleisli category yield a generic definition of trace
semantics for various types of labelled transition systems. In this paper we
apply this generic theory to generative probabilistic transition systems, short
PTS, with arbitrary (possibly uncountable) state spaces. We consider the
sub-probability monad and the probability monad (Giry monad) on the category of
measurable spaces and measurable functions. Our main contribution is that the
existence of a final coalgebra in the Kleisli category of these monads is
closely connected to the measure-theoretic extension theorem for sigma-finite
pre-measures. In fact, we obtain a practical definition of the trace measure
for both finite and infinite traces of PTS that subsumes a well-known result
for discrete probabilistic transition systems. Finally we consider two example
systems with uncountable state spaces and apply our theory to calculate their
trace measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7418</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7418</id><created>2013-10-28</created><authors><author><keyname>Dibert</keyname><forenames>Alexander</forenames></author><author><keyname>Csirmaz</keyname><forenames>L&#xe1;szl&#xf3;</forenames></author></authors><title>Infinite Secret Sharing -- Examples</title><categories>cs.CR cs.IT math.IT math.PR</categories><msc-class>60A99, 60B05, 60G15, 62F10, 94A62, 46C99, 54D10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The motivation for extending secret sharing schemes to cases when either the
set of players is infinite or the domain from which the secret and/or the
shares are drawn is infinite or both, is similar to the case when switching to
abstract probability spaces from classical combinatorial probability. It might
shed new light on old problems, could connect seemingly unrelated problems, and
unify diverse phenomena.
  Definitions equivalent in the finitary case could be very much different when
switching to infinity, signifying their difference. The standard requirement
that qualified subsets should be able to determine the secret has different
interpretations in spite of the fact that, by assumption, all participants have
infinite computing power. The requirement that unqualified subsets should have
no, or limited information on the secret suggests that we also need some
probability distribution. In the infinite case events with zero probability are
not necessarily impossible, and we should decide whether bad events with zero
probability are allowed or not.
  In this paper, rather than giving precise definitions, we enlist an abundance
of hopefully interesting infinite secret sharing schemes. These schemes touch
quite diverse areas of mathematics such as projective geometry, stochastic
processes and Hilbert spaces. Nevertheless our main tools are from probability
theory. The examples discussed here serve as foundation and illustration to the
more theory oriented companion paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7419</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7419</id><created>2013-10-28</created><updated>2014-02-12</updated><authors><author><keyname>Fearnley</keyname><forenames>John</forenames></author><author><keyname>Savani</keyname><forenames>Rahul</forenames></author></authors><title>Finding Approximate Nash Equilibria of Bimatrix Games via Payoff Queries</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the deterministic and randomized query complexity of finding
approximate equilibria in bimatrix games. We show that the deterministic query
complexity of finding an $\epsilon$-Nash equilibrium when $\epsilon &lt;
\frac{1}{2}$ is $\Omega(k^2)$, even in zero-one constant-sum games. In
combination with previous results \cite{FGGS13}, this provides a complete
characterization of the deterministic query complexity of approximate Nash
equilibria. We also study randomized querying algorithms. We give a randomized
algorithm for finding a $(\frac{3 - \sqrt{5}}{2} + \epsilon)$-Nash equilibrium
using $O(\frac{k \cdot \log k}{\epsilon^2})$ payoff queries, which shows that
the $\frac{1}{2}$ barrier for deterministic algorithms can be broken by
randomization. For well-supported Nash equilibria (WSNE), we first give a
randomized algorithm for finding an $\epsilon$-WSNE of a zero-sum bimatrix game
using $O(\frac{k \cdot \log k}{\epsilon^4})$ payoff queries, and we then use
this to obtain a randomized algorithm for finding a $(\frac{2}{3} +
\epsilon)$-WSNE in a general bimatrix game using $O(\frac{k \cdot \log
k}{\epsilon^4})$ payoff queries. Finally, we initiate the study of lower bounds
against randomized algorithms in the context of bimatrix games, by showing that
randomized algorithms require $\Omega(k^2)$ payoff queries in order to find a
$\frac{1}{6k}$-Nash equilibrium, even in zero-one constant-sum games. In
particular, this rules out query-efficient randomized algorithms for finding
exact Nash equilibria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7423</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7423</id><created>2013-10-28</created><authors><author><keyname>Csirmaz</keyname><forenames>L&#xe1;szl&#xf3;</forenames></author></authors><title>Probabilistic Infinite Secret Sharing</title><categories>cs.CR cs.IT math.IT math.PR</categories><msc-class>60B05, 94A62, 46C99, 54D10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study of probabilistic secret sharing schemes using arbitrary probability
spaces and possibly infinite number of participants lets us investigate
abstract properties of such schemes. It highlights important properties,
explains why certain definitions work better than others, connects this topic
to other branches of mathematics, and might yield new design paradigms.
  A probabilistic secret sharing scheme is a joint probability distribution of
the shares and the secret together with a collection of secret recovery
functions for qualified subsets. The scheme is measurable if the recovery
functions are measurable. Depending on how much information an unqualified
subset might have, we define four scheme types: perfect, almost perfect, ramp,
and almost ramp. Our main results characterize the access structures which can
be realized by schemes of these types.
  We show that every access structure can be realized by a non-measurable
perfect probabilistic scheme. The construction is based on a paradoxical pair
of independent random variables which determine each other.
  For measurable schemes we have the following complete characterization. An
access structure can be realized by a (measurable) perfect, or almost perfect
scheme if and only if the access structure, as a subset of the Sierpi\'nski
space $\{0,1\}^P$, is open, if and only if it can be realized by a span
program. The access structure can be realized by a (measurable) ramp or almost
ramp scheme if and only if the access structure is a $G_\delta$ set
(intersection of countably many open sets) in the Sierpi\'nski topology, if and
only if it can be realized by a Hilbert-space program.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7425</identifier>
 <datestamp>2014-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7425</id><created>2013-10-28</created><updated>2014-06-19</updated><authors><author><keyname>Gupta</keyname><forenames>Gaurav</forenames></author><author><keyname>Chaturvedi</keyname><forenames>Ajit K</forenames></author></authors><title>User Selection in MIMO Interfering Broadcast Channels</title><categories>cs.IT math.IT</categories><comments>9 pages, 5 figures</comments><journal-ref>IEEE Trans. Commun. 62 5 (2014) 1568-1576</journal-ref><doi>10.1109/TCOMM.2014.031614.130618</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interference alignment aims to achieve maximum degrees of freedom in an
interference system. For achieving Interference alignment in interfering
broadcast systems a closed-form solution is proposed in [1] which is an
extension of the grouping scheme in [2]. In a downlink scenario where there are
a large number of users, the base station is required to select a subset of
users such that the sum rate is maximized. To search for the optimal user
subset using brute-force approach is computationally exhaustive because of the
large number of possible user subset combinations. We propose a user selection
algorithm achieving sum rate close to that of optimal solution. The algorithm
employs coordinate ascent approach and exploits orthogonality between the
desired signal space and the interference channel space in the reciprocal
system to select the user at each step. For the sake of completeness, we have
also extended the sum rate approach based algorithm to Interfering broadcast
channel. The complexity of both these algorithms is shown to be linear with
respect to the total number of users as compared to exponential in brute-force
search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7428</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7428</id><created>2013-10-28</created><authors><author><keyname>Bugaychenko</keyname><forenames>Dmitry</forenames></author><author><keyname>Dzuba</keyname><forenames>Alexandr</forenames></author></authors><title>Musical recommendations and personalization in a social network</title><categories>cs.IR</categories><comments>This is a full version of a 4 pages article published at ACM RecSys
  2013</comments><doi>10.1145/2507157.2507192</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a set of algorithms used for music recommendations and
personalization in a general purpose social network www.ok.ru, the second
largest social network in the CIS visited by more then 40 millions users per
day. In addition to classical recommendation features like &quot;recommend a
sequence&quot; and &quot;find similar items&quot; the paper describes novel algorithms for
construction of context aware recommendations, personalization of the service,
handling of the cold-start problem, and more. All algorithms described in the
paper are working on-line and are able to detect and address changes in the
user's behavior and needs in the real time.
  The core component of the algorithms is a taste graph containing information
about different entities (users, tracks, artists, etc.) and relations between
them (for example, user A likes song B with certainty X, track B created by
artist C, artist C is similar to artist D with certainty Y and so on). Using
the graph it is possible to select tracks a user would most probably like, to
arrange them in a way that they match each other well, to estimate which items
from a fixed list are most relevant for the user, and more.
  In addition, the paper describes the approach used to estimate algorithms
efficiency and analyze the impact of different recommendation related features
on the users' behavior and overall activity at the service.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7433</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7433</id><created>2013-10-28</created><updated>2014-01-26</updated><authors><author><keyname>Fang</keyname><forenames>Chung-Chieh</forenames></author></authors><title>Unified Subharmonic Oscillation Conditions for Peak or Average Current
  Mode Control</title><categories>cs.SY math.DS nlin.CD</categories><comments>Subharmonic oscillation conditions of a class of nonlinear systems
  (triangular wave generator with linear feedback) are obtained. Any
  current-mode-control DC-DC converter can be modeled by such such a nonlinear
  system and its subharmonic oscillation condition is obtained. This second
  version adds Nomenclature, parameter lables in figures, and Fig. 24</comments><journal-ref>International Journal of Circuit Theory and Applications, 43(8),
  pp. 995-1014, Aug. 2015</journal-ref><doi>10.1002/cta.1989</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is an extension of the author's recent research in which only buck
converters were analyzed. Similar analysis can be equally applied to other
types of converters. In this paper, a unified model is proposed for buck,
boost, and buck-boost converters under peak or average current mode control to
predict the occurrence of subharmonic oscillation. Based on the unified model,
the associated stability conditions are derived in closed forms. The same
stability condition can be applied to buck, boost, and buck-boost converters.
Based on the closed-form conditions, the effects of various converter
parameters including the compensator poles and zeros on the stability can be
clearly seen, and these parameters can be consolidated into a few ones.
High-order compensators such as type-II and PI compensators are considered.
Some new plots are also proposed for design purpose to avoid the instability.
The instability is found to be associated with large crossover frequency. A
conservative stability condition, agreed with the past research, is derived.
The effect of the voltage loop ripple on the instability is also analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7440</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7440</id><created>2013-10-09</created><authors><author><keyname>Ammar</keyname><forenames>Boulbaba Ben</forenames></author></authors><title>Neural perceptual model to global-local vision for recognition of the
  logical structure of administrative documents</title><categories>cs.CV</categories><comments>17 pages, International Journal of Artificial Intelligence &amp;
  Applications (IJAIA), Vol. 4, No. 5, September 2013</comments><doi>10.5121/ijaia</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper gives the definition of Transparent Neural Network &quot;TNN&quot; for the
simulation of the globallocal vision and its application to the segmentation of
administrative document image. We have developed and have adapted a recognition
method which models the contextual effects reported from studies in
experimental psychology. Then, we evaluated and tested the TNN and the
multi-layer perceptron &quot;MLP&quot;, which showed its effectiveness in the field of
the recognition, in order to show that the TNN is clearer for the user and more
powerful on the level of the recognition. Indeed, the TNN is the only system
which makes it possible to recognize the document and its structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7441</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7441</id><created>2013-09-14</created><updated>2014-08-19</updated><authors><author><keyname>Gillis</keyname><forenames>Nicolas</forenames></author><author><keyname>Kuang</keyname><forenames>Da</forenames></author><author><keyname>Park</keyname><forenames>Haesun</forenames></author></authors><title>Hierarchical Clustering of Hyperspectral Images using Rank-Two
  Nonnegative Matrix Factorization</title><categories>cs.CV cs.IR math.OC</categories><comments>29 pages, 19 figures. New experiment on Terrain data set. Accepted in
  IEEE Trans. Geosci. Remote Sens</comments><journal-ref>IEEE Trans. on Geoscience and Remote Sensing 53 (4), pp.
  2066-2078, 2015</journal-ref><doi>10.1109/TGRS.2014.2352857</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we design a hierarchical clustering algorithm for
high-resolution hyperspectral images. At the core of the algorithm, a new
rank-two nonnegative matrix factorizations (NMF) algorithm is used to split the
clusters, which is motivated by convex geometry concepts. The method starts
with a single cluster containing all pixels, and, at each step, (i) selects a
cluster in such a way that the error at the next step is minimized, and (ii)
splits the selected cluster into two disjoint clusters using rank-two NMF in
such a way that the clusters are well balanced and stable. The proposed method
can also be used as an endmember extraction algorithm in the presence of pure
pixels. The effectiveness of this approach is illustrated on several synthetic
and real-world hyperspectral images, and shown to outperform standard
clustering techniques such as k-means, spherical k-means and standard NMF.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7442</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7442</id><created>2013-10-28</created><authors><author><keyname>Du</keyname><forenames>Yuxian</forenames></author><author><keyname>Chen</keyname><forenames>Shiyu</forenames></author><author><keyname>Hu</keyname><forenames>Yong</forenames></author><author><keyname>Chan</keyname><forenames>Felix T. S.</forenames></author><author><keyname>Mahadevan</keyname><forenames>Sankaran</forenames></author><author><keyname>Deng</keyname><forenames>Yong</forenames></author></authors><title>Ranking basic belief assignments in decision making under uncertain
  environment</title><categories>cs.AI</categories><comments>16 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dempster-Shafer theory (D-S theory) is widely used in decision making under
the uncertain environment. Ranking basic belief assignments (BBAs) now is an
open issue. Existing evidence distance measures cannot rank the BBAs in the
situations when the propositions have their own ranking order or their inherent
measure of closeness. To address this issue, a new ranking evidence distance
(RED) measure is proposed. Compared with the existing evidence distance
measures including the Jousselme's distance and the distance between betting
commitments, the proposed RED measure is much more general due to the fact that
the order of the propositions in the systems is taken into consideration. If
there is no order or no inherent measure of closeness in the propositions, our
proposed RED measure is reduced to the existing evidence distance. Numerical
examples show that the proposed RED measure is an efficient alternative to rank
BBAs in decision making under uncertain environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7443</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7443</id><created>2013-09-16</created><authors><author><keyname>Prasath</keyname><forenames>V. B. S.</forenames></author><author><keyname>Moreno</keyname><forenames>Juan C.</forenames></author></authors><title>On Convergent Finite Difference Schemes for Variational - PDE Based
  Image Processing</title><categories>cs.CV math.NA</categories><comments>23 pages, 12 figures, 2 tables</comments><msc-class>65N06, 65N22, 68U10</msc-class><acm-class>I.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study an adaptive anisotropic Huber functional based image restoration
scheme. By using a combination of L2-L1 regularization functions, an adaptive
Huber functional based energy minimization model provides denoising with edge
preservation in noisy digital images. We study a convergent finite difference
scheme based on continuous piecewise linear functions and use a variable
splitting scheme, namely the Split Bregman, to obtain the discrete minimizer.
Experimental results are given in image denoising and comparison with additive
operator splitting, dual fixed point, and projected gradient schemes illustrate
that the best convergence rates are obtained for our algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7444</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7444</id><created>2013-10-28</created><authors><author><keyname>Gao</keyname><forenames>Juntao</forenames></author><author><keyname>Shen</keyname><forenames>Yulong</forenames></author><author><keyname>Jiang</keyname><forenames>Xiaohong</forenames></author><author><keyname>Li</keyname><forenames>Jie</forenames></author></authors><title>Source Delay in Mobile Ad Hoc Networks</title><categories>cs.NI</categories><comments>11pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Source delay, the time a packet experiences in its source node, serves as a
fundamental quantity for delay performance analysis in networks. However, the
source delay performance in highly dynamic mobile ad hoc networks (MANETs) is
still largely unknown by now. This paper studies the source delay in MANETs
based on a general packet dispatching scheme with dispatch limit $f$ (PD-$f$
for short), where a same packet will be dispatched out up to $f$ times by its
source node such that packet dispatching process can be flexibly controlled
through a proper setting of $f$. We first apply the Quasi-Birth-and-Death (QBD)
theory to develop a theoretical framework to capture the complex packet
dispatching process in PD-$f$ MANETs. With the help of the theoretical
framework, we then derive the cumulative distribution function as well as mean
and variance of the source delay in such networks. Finally, extensive
simulation and theoretical results are provided to validate our source delay
analysis and illustrate how source delay in MANETs are related to network
parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7447</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7447</id><created>2013-08-14</created><authors><author><keyname>Nongpiur</keyname><forenames>R. C.</forenames></author></authors><title>Impulse Noise Removal In Speech Using Wavelets</title><categories>cs.CV</categories><comments>Icassp 2008</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new method for removing impulse noise from speech in the wavelet transform
domain is proposed. The method utilizes the multiresolution property of the
wavelet transform, which provides finer time resolution at the higher
frequencies than the short-time Fourier transform (STFT), to effectively
identify and remove impulse noise. It uses two features of speech to
discriminate speech from impulse noise: one is the slow time-varying nature of
speech and the other is the Lipschitz regularity of the speech components. On
the basis of these features, an algorithm has been developed to identify and
suppress wavelet coefficients that correspond to impulse noise. Experiment
results show that the new method is able to significantly reduce impulse noise
without degrading the quality of the speech signal or introducing any audible
artifacts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7448</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7448</id><created>2013-09-16</created><authors><author><keyname>Sun</keyname><forenames>Yuli</forenames></author><author><keyname>Tao</keyname><forenames>Jinxu</forenames></author><author><keyname>Liu</keyname><forenames>Conggui</forenames></author></authors><title>An iterative algorithm for computed tomography image reconstruction from
  limited-angle projections</title><categories>cs.CV</categories><comments>14 pages, 1 figure, 1 table</comments><acm-class>G.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In application of tomography imaging, limited-angle problem is a quite
practical and important issue. In this paper, an iterative
reprojection-reconstruction (IRR) algorithm using a modified Papoulis-Gerchberg
(PG) iterative scheme is developed for reconstruction from limited-angle
projections which contain noise. The proposed algorithm has two iterative
update processes, one is the extrapolation of unknown data, and the other is
the modification of the known noisy observation data. And the algorithm
introduces scaling factors to control the two processes, respectively. The
convergence of the algorithm is guaranteed, and the method of choosing the
scaling factors is given with energy constraints. The simulation result
demonstrates our conclusions and indicates that the algorithm proposed in this
paper can obviously improve the reconstruction quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7449</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7449</id><created>2013-10-28</created><authors><author><keyname>Giachino</keyname><forenames>Elena</forenames></author><author><keyname>Laneve</keyname><forenames>Cosimo</forenames></author></authors><title>Deadlock detection in linear recursive programs</title><categories>cs.PL cs.LO</categories><msc-class>68Q85</msc-class><acm-class>F.3.1; F.3.2; F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deadlock detection in recursive programs that admit dynamic resource creation
is extremely complex and solutions either give imprecise answers or do not
scale.
  We define an algorithm for detecting deadlocks of &quot;linear recursive programs&quot;
of a basic model. The theory that underpins the algorithm is a generalization
of the theory of permutations of names to so-called &quot;mutations&quot;, which
transform tuples by introducing duplicates and fresh names.
  Our algorithm realizes the back-end of deadlock analyzers for object-oriented
programming languages, once the association programs/basic-model-programs has
been defined as front-end.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7452</identifier>
 <datestamp>2014-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7452</id><created>2013-10-28</created><updated>2014-03-20</updated><authors><author><keyname>Sun</keyname><forenames>Jiajun</forenames></author></authors><title>How Much Should I Pay for Privacy Concerns in Truthful Online Crowd
  Sensing?</title><categories>cs.GT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1306.5677,
  arXiv:1308.4013 by other authors This paper is withdrawn by the author due to
  a crucial issue that is not solved</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crowd sensing is a new paradigm which leverages the pervasive smartphones to
efficiently collect sensing data, enabling numerous novel applications. To
achieve good service quality for a crowd sensing application, incentive
mechanisms are indispensable to attract more user participation. Most of
existing mechanisms only apply for the offline scenario, where the system has
full information about the users' sensing profiles, i.e., a set of locations or
mobility as well as the type of smartphones used, and their true costs. On the
contrary, we focus on a more real scenario where users with their own privacy
concerns arrive one by one online in a random order. We model the problem as a
privacy-respecting online auction in which users are willing to negotiate
access to certain private information and submit their sensing profiles
satisfying privacy concerns to the platform (the provider of crowd sensing
applications) over time, and the platform aims to the total total value of the
services provided by selected users under a budget constraint. We then design
two online mechanisms for a budgeted crowd sensing application, satisfying the
computational efficiency, individual rationality, budget feasibility,
truthfulness, consumer sovereignty, constant competitiveness and privacy
concerns. Through extensive simulations, we evaluate the performance and
validate the theoretical properties of our online mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7453</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7453</id><created>2013-10-28</created><authors><author><keyname>Versaci</keyname><forenames>Francesco</forenames></author></authors><title>OutFlank Routing: Increasing Throughput in Toroidal Interconnection
  Networks</title><categories>cs.DC cs.NI</categories><comments>9 pages, 5 figures, to be presented at ICPADS 2013</comments><doi>10.1109/ICPADS.2013.40</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new, deadlock-free, routing scheme for toroidal interconnection
networks, called OutFlank Routing (OFR). OFR is an adaptive strategy which
exploits non-minimal links, both in the source and in the destination nodes.
When minimal links are congested, OFR deroutes packets to carefully chosen
intermediate destinations, in order to obtain travel paths which are only an
additive constant longer than the shortest ones. Since routing performance is
very sensitive to changes in the traffic model or in the router parameters, an
accurate discrete-event simulator of the toroidal network has been developed to
empirically validate OFR, by comparing it against other relevant routing
strategies, over a range of typical real-world traffic patterns. On the
16x16x16 (4096 nodes) simulated network OFR exhibits improvements of the
maximum sustained throughput between 14% and 114%, with respect to Adaptive
Bubble Routing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7467</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7467</id><created>2013-10-28</created><updated>2016-02-21</updated><authors><author><keyname>Robinson</keyname><forenames>Andrew</forenames></author><author><keyname>Turner</keyname><forenames>Katharine</forenames></author></authors><title>Hypothesis Testing for Topological Data Analysis</title><categories>stat.AP cs.CG math.AT</categories><comments>14 pages, 5 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Persistent homology is a vital tool for topological data analysis. Previous
work has developed some statistical estimators for characteristics of
collections of persistence diagrams. However, tools that provide statistical
inference for observations that are persistence diagrams are limited.
Specifically, there is a need for tests that can assess the strength of
evidence against a claim that two samples arise from the same population or
process. We propose the use of randomization-style null hypothesis significance
tests (NHST) for these situations. The test is based on a loss function that
comprises pairwise distances between the elements of each sample and all the
elements in the other sample. We use this method to analyze a range of
simulated and experimental data. Through these examples we experimentally
explore the power of the p-values. Our results show that the
randomization-style NHST based on pairwise distances can distinguish between
samples from different processes, which suggests that its use for hypothesis
tests upon persistence diagrams is reasonable. We demonstrate its application
on a real dataset of fMRI data of patients with ADHD.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7469</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7469</id><created>2013-10-28</created><authors><author><keyname>Jiang</keyname><forenames>Feng</forenames></author><author><keyname>Wang</keyname><forenames>Jiemin</forenames></author><author><keyname>Hindle</keyname><forenames>Abram</forenames></author><author><keyname>Nascimento</keyname><forenames>Mario A.</forenames></author></authors><title>Mining the Temporal Evolution of the Android Bug Reporting Community via
  Sliding Windows</title><categories>cs.SE</categories><report-no>TR13-07</report-no><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The open source development community consists of both paid and volunteer
developers as well as new and experienced users. Previous work has applied
social network analysis (SNA) to open source communities and has demonstrated
value in expertise discovery and triaging. One problem with applying SNA
directly to the data of the entire project lifetime is that the impact of local
activities will be drowned out. In this paper we provide a method for
aggregating, analyzing, and visualizing local (small time periods) interactions
of bug reporting participants by using the SNA to measure the betweeness
centrality of these participants. In particular we mined the Android bug
repository by producing social networks from overlapping 30-day windows of bug
reports, each sliding over by day. In this paper we define three patterns of
participant behaviour based on their local centrality. We propose a method of
analyzing the centrality of bug report participants both locally and globally,
then we conduct a thorough case study of the bug reporter's activity within the
Android bug repository. Furthermore, we validate the conclusions of our method
by mining the Android version control system and inspecting the Android release
history. We found that windowed SNA analysis elicited local behaviour that were
invisible during global analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7473</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7473</id><created>2013-10-28</created><authors><author><keyname>Georgiou</keyname><forenames>Orestis</forenames></author><author><keyname>Dettmann</keyname><forenames>Carl P.</forenames></author><author><keyname>Coon</keyname><forenames>Justin P.</forenames></author></authors><title>Connectivity of confined 3D Networks with Anisotropically Radiating
  Nodes</title><categories>cs.IT cond-mat.dis-nn cond-mat.stat-mech cs.NI math.IT</categories><comments>12 pages, 10 figures</comments><journal-ref>IEEE Transactions Wireless Communications 13, 4534-4546 (2014)</journal-ref><doi>10.1109/TWC.2014.2314109</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nodes in ad hoc networks with randomly oriented directional antenna patterns
typically have fewer short links and more long links which can bridge together
otherwise isolated subnetworks. This network feature is known to improve
overall connectivity in 2D random networks operating at low channel path loss.
To this end, we advance recently established results to obtain analytic
expressions for the mean degree of 3D networks for simple but practical
anisotropic gain profiles, including those of patch, dipole and end-fire array
antennas. Our analysis reveals that for homogeneous systems (i.e. neglecting
boundary effects) directional radiation patterns are superior to the isotropic
case only when the path loss exponent is less than the spatial dimension.
Moreover, we establish that ad hoc networks utilizing directional transmit and
isotropic receive antennas (or vice versa) are always sub-optimally connected
regardless of the environment path loss. We extend our analysis to investigate
boundary effects in inhomogeneous systems, and study the geometrical reasons
why directional radiating nodes are at a disadvantage to isotropic ones.
Finally, we discuss multi-directional gain patterns consisting of many equally
spaced lobes which could be used to mitigate boundary effects and improve
overall network connectivity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7525</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7525</id><created>2013-10-28</created><updated>2014-04-20</updated><authors><author><keyname>Mosonyi</keyname><forenames>Milan</forenames></author></authors><title>Inequalities for the quantum Renyi divergences with applications to
  compound coding problems</title><categories>quant-ph cs.IT math-ph math.IT math.MP</categories><comments>29 pages. v3: Title changed. Results extended to infinite compound
  channels and Stein's lemma with infinite composite null-hypothesis. Added
  section on universal state compression</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show two-sided bounds between the conventional quantum R\'enyi divergences
and the new notion of R\'enyi divergences introduced recently in
M\&quot;uller-Lennert, Dupuis, Szehr, Fehr and Tomamichel, J. Math. Phys. 54,
122203, (2013), and Wilde, Winter, Yang, arXiv:1306.1586. The bounds imply that
the two versions can be used interchangeably near alpha=1, and hence one can
benefit from the best properties of both when proving coding theorems in the
case of asymptotically vanishing error. We illustrate this by giving short and
simple proofs of the quantum Stein's lemma with composite null-hypothesis,
universal source compression, and the achievability part of the classical
capacity of compound quantum channels. Apart from the above interchangeability,
we benefit from a weak quasi-concavity property of the new Renyi divergences
that we also establish here.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7529</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7529</id><created>2013-10-28</created><updated>2014-04-07</updated><authors><author><keyname>Gillis</keyname><forenames>Nicolas</forenames></author></authors><title>Successive Nonnegative Projection Algorithm for Robust Nonnegative Blind
  Source Separation</title><categories>stat.ML cs.LG math.NA math.OC</categories><comments>31 pages, 7 figures, 4 tables. Main changes: new numerical
  experiments on column-rank-deficient matrices, typos corrected, discussion on
  the comparison with XRAY</comments><journal-ref>SIAM J. on Imaging Sciences 7 (2), pp. 1420-1450, 2014</journal-ref><doi>10.1137/130946782</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new fast and robust recursive algorithm for
near-separable nonnegative matrix factorization, a particular nonnegative blind
source separation problem. This algorithm, which we refer to as the successive
nonnegative projection algorithm (SNPA), is closely related to the popular
successive projection algorithm (SPA), but takes advantage of the nonnegativity
constraint in the decomposition. We prove that SNPA is more robust than SPA and
can be applied to a broader class of nonnegative matrices. This is illustrated
on some synthetic data sets, and on a real-world hyperspectral image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7532</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7532</id><created>2013-10-28</created><updated>2014-10-16</updated><authors><author><keyname>Lee</keyname><forenames>Sang Hoon</forenames></author><author><keyname>Ffrancon</keyname><forenames>Robyn</forenames></author><author><keyname>Abrams</keyname><forenames>Daniel M.</forenames></author><author><keyname>Kim</keyname><forenames>Beom Jun</forenames></author><author><keyname>Porter</keyname><forenames>Mason A.</forenames></author></authors><title>Matchmaker, Matchmaker, Make Me a Match: Migration of Populations via
  Marriages in the Past</title><categories>physics.soc-ph cond-mat.dis-nn cs.CE nlin.AO q-bio.PE</categories><comments>24 pages, 23 figures, 5 tables</comments><journal-ref>Phys. Rev. X 4, 041009 (2014)</journal-ref><doi>10.1103/PhysRevX.4.041009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study of human mobility is both of fundamental importance and of great
potential value. For example, it can be leveraged to facilitate efficient city
planning and improve prevention strategies when faced with epidemics. The
newfound wealth of rich sources of data---including banknote flows, mobile
phone records, and transportation data---has led to an explosion of attempts to
characterize modern human mobility. Unfortunately, the dearth of comparable
historical data makes it much more difficult to study human mobility patterns
from the past. In this paper, we present an analysis of long-term human
migration, which is important for processes such as urbanization and the spread
of ideas. We demonstrate that the data record from Korean family books (called
&quot;jokbo&quot;) can be used to estimate migration patterns via marriages from the past
750 years. We apply two generative models of long-term human mobility to
quantify the relevance of geographical information to human marriage records in
the data, and we find that the wide variety in the geographical distributions
of the clans poses interesting challenges for the direct application of these
models. Using the different geographical distributions of clans, we quantify
the &quot;ergodicity&quot; of clans in terms of how widely and uniformly they have spread
across Korea, and we compare these results to those obtained using surname data
from the Czech Republic. To examine population flow in more detail, we also
construct and examine a population-flow network between regions. Based on the
correlation between ergodicity and migration in Korea, we identify two
different types of migration patterns: diffusive and convective. We expect the
analysis of diffusive versus convective effects in population flows to be
widely applicable to the study of mobility and migration patterns across
different cultures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7536</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7536</id><created>2013-10-28</created><authors><author><keyname>Grassl</keyname><forenames>Markus</forenames></author><author><keyname>Shor</keyname><forenames>Peter</forenames></author><author><keyname>Smith</keyname><forenames>Graeme</forenames></author><author><keyname>Smolin</keyname><forenames>John</forenames></author><author><keyname>Zeng</keyname><forenames>Bei</forenames></author></authors><title>New Constructions of Codes for Asymmetric Channels via Concatenation</title><categories>cs.IT math.IT</categories><comments>9 pages, 3 figures, 4 tables</comments><journal-ref>IEEE Transactions on Information Theory, vol. 61, no. 4, pp.
  1879-1886, 2015</journal-ref><doi>10.1109/TIT.2015.2401567</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present new constructions of codes for asymmetric channels for both binary
and nonbinary alphabets, based on methods of generalized code concatenation.
For the binary asymmetric channel, our methods construct nonlinear
single-error-correcting codes from ternary outer codes. We show that some of
the Varshamov-Tenengol'ts-Constantin-Rao codes, a class of binary nonlinear
codes for this channel, have a nice structure when viewed as ternary codes. In
many cases, our ternary construction yields even better codes. For the
nonbinary asymmetric channel, our methods construct linear codes for many
lengths and distances which are superior to the linear codes of the same length
capable of correcting the same number of symmetric errors.
  In the binary case, Varshamov has shown that almost all good linear codes for
the asymmetric channel are also good for the symmetric channel. Our results
indicate that Varshamov's argument does not extend to the nonbinary case, i.e.,
one can find better linear codes for asymmetric channels than for symmetric
ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7552</identifier>
 <datestamp>2014-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7552</id><created>2013-10-28</created><updated>2014-03-07</updated><authors><author><keyname>Chen</keyname><forenames>Yuxin</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author><author><keyname>Goldsmith</keyname><forenames>Andrea J.</forenames></author></authors><title>An Algorithm for Exact Super-resolution and Phase Retrieval</title><categories>cs.IT cs.NA math.IT</categories><comments>accepted to IEEE ICASSP 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore a fundamental problem of super-resolving a signal of interest from
a few measurements of its low-pass magnitudes. We propose a 2-stage tractable
algorithm that, in the absence of noise, admits perfect super-resolution of an
$r$-sparse signal from $2r^2-2r+2$ low-pass magnitude measurements. The spike
locations of the signal can assume any value over a continuous disk, without
increasing the required sample size. The proposed algorithm first employs a
conventional super-resolution algorithm (e.g. the matrix pencil approach) to
recover unlabeled sets of signal correlation coefficients, and then applies a
simple sorting algorithm to disentangle and retrieve the true parameters in a
deterministic manner. Our approach can be adapted to multi-dimensional spike
models and random Fourier sampling by replacing its first step with other
harmonic retrieval algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7556</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7556</id><created>2013-10-28</created><updated>2014-02-03</updated><authors><author><keyname>Halyo</keyname><forenames>V.</forenames></author><author><keyname>LeGresley</keyname><forenames>P.</forenames></author><author><keyname>Lujan</keyname><forenames>P.</forenames></author><author><keyname>Karpusenko</keyname><forenames>V.</forenames></author><author><keyname>Vladimirov</keyname><forenames>A.</forenames></author></authors><title>First Evaluation of the CPU, GPGPU and MIC Architectures for Real Time
  Particle Tracking based on Hough Transform at the LHC</title><categories>physics.comp-ph cs.DC hep-ex</categories><comments>13 pages, 4 figures, Accepted to JINST</comments><doi>10.1088/1748-0221/9/04/P04005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent innovations focused around {\em parallel} processing, either through
systems containing multiple processors or processors containing multiple cores,
hold great promise for enhancing the performance of the trigger at the LHC and
extending its physics program. The flexibility of the CMS/ATLAS trigger system
allows for easy integration of computational accelerators, such as NVIDIA's
Tesla Graphics Processing Unit (GPU) or Intel's \xphi, in the High Level
Trigger. These accelerators have the potential to provide faster or more energy
efficient event selection, thus opening up possibilities for new complex
triggers that were not previously feasible. At the same time, it is crucial to
explore the performance limits achievable on the latest generation multicore
CPUs with the use of the best software optimization methods. In this article, a
new tracking algorithm based on the Hough transform will be evaluated for the
first time on a multi-core Intel Xeon E5-2697v2 CPU, an NVIDIA Tesla K20c GPU,
and an Intel \xphi\ 7120 coprocessor. Preliminary time performance will be
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7558</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7558</id><created>2013-10-28</created><updated>2014-08-25</updated><authors><author><keyname>Laso&#x144;</keyname><forenames>Micha&#x142;</forenames></author><author><keyname>Micek</keyname><forenames>Piotr</forenames></author><author><keyname>Pawlik</keyname><forenames>Arkadiusz</forenames></author><author><keyname>Walczak</keyname><forenames>Bartosz</forenames></author></authors><title>Coloring intersection graphs of arc-connected sets in the plane</title><categories>math.CO cs.CG cs.DM</categories><comments>Minor changes + some additional references not included in the
  journal version</comments><msc-class>05C62, 05C15</msc-class><journal-ref>Discrete Comput.Geom. 52 (2014) 399-415</journal-ref><doi>10.1007/s00454-014-9614-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A family of sets in the plane is simple if the intersection of its any
subfamily is arc-connected, and it is pierced by a line $L$ if the intersection
of its any member with $L$ is a nonempty segment. It is proved that the
intersection graphs of simple families of compact arc-connected sets in the
plane pierced by a common line have chromatic number bounded by a function of
their clique number.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7568</identifier>
 <datestamp>2013-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7568</id><created>2013-10-28</created><authors><author><keyname>Tero</keyname><forenames>Atsushi</forenames></author><author><keyname>Akiyama</keyname><forenames>Masakazu</forenames></author><author><keyname>Owaki</keyname><forenames>Dai</forenames></author><author><keyname>Kano</keyname><forenames>Takeshi</forenames></author><author><keyname>Ishiguro</keyname><forenames>Akio</forenames></author><author><keyname>Kobayashi</keyname><forenames>Ryo</forenames></author></authors><title>Interlimb neural connection is not required for gait transition in
  quadruped locomotion</title><categories>q-bio.QM cs.RO cs.SY</categories><comments>6 pages, 2figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quadrupeds transition spontaneously to various gait patterns (e.g., walk,
trot, pace, gallop) in response to the locomotion speed. The generation of
these gait patterns has been the subject of debate for a long time. We propose
a coupled oscillator model that is coupled with the physical interactions of
the body. The results of this study showed that the gait pattern transitions
spontaneously to walking/trotting/pacing/bounding in manner similar to that of
real quadruped animals when the resonating portion of the body is changed
according to the speed of leg movement. We also observed that pacing is
expressed exclusively instead of trotting by changing the physical
characteristics. In addition to leading to understanding of the principles of
locomotion in living things, the coupled oscillator model proposed in this
study is expected to lead to the creation of a legged robot that can select an
energy-efficient gait and transition to it spontaneously.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7610</identifier>
 <datestamp>2013-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7610</id><created>2013-10-28</created><authors><author><keyname>Mathkar</keyname><forenames>Adwaitvedant S.</forenames></author><author><keyname>Borkar</keyname><forenames>Vivek S.</forenames></author></authors><title>Distributed Reinforcement Learning via Gossip</title><categories>cs.DC cs.AI math.OC</categories><comments>18 pages, 3 figures, Submitted to Discrete Event Dynamic Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the classical TD(0) algorithm implemented on a network of agents
wherein the agents also incorporate the updates received from neighboring
agents using a gossip-like mechanism. The combined scheme is shown to converge
for both discounted and average cost problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7616</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7616</id><created>2013-10-28</created><updated>2014-04-21</updated><authors><author><keyname>Kim</keyname><forenames>Jinsub</forenames></author><author><keyname>Tong</keyname><forenames>Lang</forenames></author><author><keyname>Thomas</keyname><forenames>Robert J.</forenames></author></authors><title>Data Framing Attack on State Estimation</title><categories>cs.CR</categories><comments>To appear in IEEE Journal on Selected Areas in Communications:
  Special series on Smart Grid Communications, 2014</comments><journal-ref>IEEE Journal on Selected Areas in Communications, vol. 32, no. 7,
  pp 1460-1470, 2014</journal-ref><doi>10.1109/JSAC.2014.2332032</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new mechanism aimed at misleading a power system control center about the
source of a data attack is proposed. As a man-in-the-middle state attack, a
data framing attack is proposed to exploit the bad data detection and
identification mechanisms currently in use at most control centers. In
particular, the proposed attack frames meters that are providing correct data
as sources of bad data such that the control center will remove useful
measurements that would otherwise be used by the state estimator.
  The optimal design of a data framing attack is formulated as a quadratically
constrained quadratic program (QCQP). It is shown that the proposed attack is
capable of perturbing the power system state estimate by an arbitrary degree
controlling only half of a critical set of measurements that are needed to make
a system unobservable. Implications of this attack on power system operations
are discussed, and the attack performance is evaluated using benchmark systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7627</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7627</id><created>2013-10-28</created><updated>2014-02-17</updated><authors><author><keyname>Beyersdorff</keyname><forenames>Olaf</forenames></author><author><keyname>Kullmann</keyname><forenames>Oliver</forenames></author></authors><title>Hardness measures and resolution lower bounds</title><categories>cs.CC cs.LO</categories><comments>43 pages, preliminary version (yet the application part is only
  sketched, with proofs missing)</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Various &quot;hardness&quot; measures have been studied for resolution, providing
theoretical insight into the proof complexity of resolution and its fragments,
as well as explanations for the hardness of instances in SAT solving. In this
report we aim at a unified view of a number of hardness measures, including
different measures of width, space and size of resolution proofs. We also
extend these measures to all clause-sets (possibly satisfiable).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7648</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7648</id><created>2013-10-28</created><updated>2015-03-17</updated><authors><author><keyname>Nasir</keyname><forenames>Ali Arshad</forenames></author><author><keyname>Zhou</keyname><forenames>Xiangyun</forenames></author><author><keyname>Durrani</keyname><forenames>Salman</forenames></author><author><keyname>Kennedy</keyname><forenames>Rodney A.</forenames></author></authors><title>Wireless-Powered Relays in Cooperative Communications: Time-Switching
  Relaying Protocols and Throughput Analysis</title><categories>cs.IT math.IT</categories><comments>accepted for publication in IEEE Transactions on Communications</comments><journal-ref>IEEE Transactions on Communications, vol. 63, no. 5, pp.
  1607-1622, May 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider wireless-powered amplify-and-forward and decode-and-forward
relaying in cooperative communications, where an energy constrained relay node
first harvests energy through the received radio-frequency signal from the
source and then uses the harvested energy to forward the source information to
the destination node. We propose time-switching based energy harvesting (EH)
and information transmission (IT) protocols with two modes of EH at the relay.
For continuous time EH, the EH time can be any percentage of the total
transmission block time. For discrete time EH, the whole transmission block is
either used for EH or IT. The proposed protocols are attractive because they do
not require channel state information at the transmitter side and enable relay
transmission with preset fixed transmission power. We derive analytical
expressions of the achievable throughput for the proposed protocols. The
derived expressions are verified by comparison with simulations and allow the
system performance to be determined as a function of the system parameters.
Finally, we show that the proposed protocols outperform the existing fixed time
duration EH protocols in the literature, since they intelligently track the
level of the harvested energy to switch between EH and IT in an online fashion,
allowing efficient use of resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7652</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7652</id><created>2013-10-28</created><updated>2015-04-01</updated><authors><author><keyname>Radcliffe</keyname><forenames>Mary</forenames></author><author><keyname>Young</keyname><forenames>Stephen J.</forenames></author></authors><title>Connectivity and Giant Component of Stochastic Kronecker Graphs</title><categories>math.CO cs.DM cs.SI</categories><comments>14 pages</comments><msc-class>05C80, 05C82</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic Kronecker graphs are a model for complex networks where each edge
is present independently according the Kronecker (tensor) product of a fixed
matrix k-by-k matrix P with entries in [0,1]. We develop a novel correspondence
between the adjacencies in a general stochastic Kronecker graph and the action
of a fixed Markov chain. Using this correspondence we are able to generalize
the arguments of Horn and Radcliffe on the emergence of the giant component
from the case where k = 2 to arbitrary k. We are also able to use this
correspondence to completely analyze the connectivity of a general stochastic
Kronecker graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7654</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7654</id><created>2013-10-28</created><updated>2014-10-19</updated><authors><author><keyname>Babichenko</keyname><forenames>Yakov</forenames></author><author><keyname>Barman</keyname><forenames>Siddharth</forenames></author><author><keyname>Peretz</keyname><forenames>Ron</forenames></author></authors><title>Empirical Distribution of Equilibrium Play and Its Testing Application</title><categories>cs.GT</categories><comments>Updated writeup, 29 pages</comments><acm-class>F.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that in any $n$-player $m$-action normal-form game, we can obtain an
approximate equilibrium by sampling any mixed-action equilibrium a small number
of times. We study three types of equilibria: Nash, correlated and coarse
correlated. For each one of them we obtain upper and lower bounds on the number
of samples required for the empirical distribution over the sampled action
profiles to form an approximate equilibrium with probability close to one.
  These bounds imply that using a small number of samples we can test whether
or not players are playing according to an approximate equilibrium, even in
games where $n$ and $m$ are large. In addition, our results substantially
improve previously known upper bounds on the support size of approximate
equilibria in games with many players. In particular, for all the three types
of equilibria we show the existence of approximate equilibrium with support
size polylogarithmic in $n$ and $m$, whereas the previously best-known upper
bounds were polynomial in $n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7665</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7665</id><created>2013-10-28</created><updated>2014-10-14</updated><authors><author><keyname>Jha</keyname><forenames>Madhav</forenames></author><author><keyname>Seshadhri</keyname><forenames>C.</forenames></author><author><keyname>Pinar</keyname><forenames>Ali</forenames></author></authors><title>Counting Triangles in Real-World Graph Streams: Dealing with Repeated
  Edges and Time Windows</title><categories>cs.DS cs.DM cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real-world graphs often manifest as a massive temporal stream of edges. The
need for real-time analysis of such large graph streams has led to progress on
low memory, one-pass streaming graph algorithms. These algorithms were designed
for simple graphs, assuming an edge is not repeated in the stream. Real graph
streams however, are almost always multigraphs i.e., they contain many
duplicate edges. The assumption of no repeated edges requires an extra pass
*storing all the edges* just for deduplication, which defeats the purpose of
small memory algorithms.
  We describe an algorithm for estimating the triangle count of a multigraph
stream of edges. We show that all previous streaming algorithms for triangle
counting fail for multigraph streams, despite their impressive accuracies for
simple graphs. The bias created by duplicate edges is a major problem, and
leads these algorithms astray. Our algorithm avoids these biases through
careful debiasing strategies and has provable theoretical guarantees and
excellent empirical performance. Our algorithm builds on the previously
introduced wedge sampling methodology.
  Another challenge in analyzing temporal graphs is finding the right temporal
window size. Our algorithm seamlessly handles multiple time windows, and does
not require committing to any window size(s) a priori. We apply our algorithm
to discover fascinating transitivity and triangle trends in real-world graph
streams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7679</identifier>
 <datestamp>2013-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7679</id><created>2013-10-29</created><authors><author><keyname>Ding</keyname><forenames>Ni</forenames></author><author><keyname>Sadeghi</keyname><forenames>Parastoo</forenames></author><author><keyname>Kennedy</keyname><forenames>Rodney A.</forenames></author></authors><title>Structured Optimal Transmission Control in Network-coded Two-way Relay
  Channels</title><categories>cs.SY stat.ML</categories><comments>32 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a transmission control problem in network-coded two-way
relay channels (NC-TWRC), where the relay buffers random symbol arrivals from
two users, and the channels are assumed to be fading. The problem is modeled by
a discounted infinite horizon Markov decision process (MDP). The objective is
to find a transmission control policy that minimizes the symbol delay, buffer
overflow and transmission power consumption and error rate simultaneously and
in the long run. By using the concepts of submodularity, multimodularity and
L-natural convexity, we study the structure of the optimal policy searched by
dynamic programming (DP) algorithm. We show that the optimal transmission
policy is nondecreasing in queue occupancies or/and channel states under
certain conditions such as the chosen values of parameters in the MDP model,
channel modeling method, modulation scheme and the preservation of stochastic
dominance in the transitions of system states. The results derived in this
paper can be used to relieve the high complexity of DP and facilitate real-time
control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7682</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7682</id><created>2013-10-29</created><updated>2013-11-01</updated><authors><author><keyname>Gabora</keyname><forenames>Liane</forenames></author><author><keyname>Aerts</keyname><forenames>Diederik</forenames></author></authors><title>Contextualizing concepts using a mathematical generalization of the
  quantum formalism</title><categories>q-bio.NC cs.AI quant-ph</categories><comments>31 pages. arXiv admin note: substantial text overlap with
  arXiv:quant-ph/0205161</comments><journal-ref>Journal of Experimental and Theoretical Artificial Intelligence,
  14(4), 327-358</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We outline the rationale and preliminary results of using the state context
property (SCOP) formalism, originally developed as a generalization of quantum
mechanics, to describe the contextual manner in which concepts are evoked, used
and combined to generate meaning. The quantum formalism was developed to cope
with problems arising in the description of (i) the measurement process, and
(ii) the generation of new states with new properties when particles become
entangled. Similar problems arising with concepts motivated the formal
treatment introduced here. Concepts are viewed not as fixed representations,
but entities existing in states of potentiality that require interaction with a
context-a stimulus or another concept-to 'collapse' to an instantiated form
(e.g. exemplar, prototype, or other possibly imaginary instance). The stimulus
situation plays the role of the measurement in physics, acting as context that
induces a change of the cognitive state from superposition state to collapsed
state. The collapsed state is more likely to consist of a conjunction of
concepts for associative than analytic thought because more stimulus or concept
properties take part in the collapse. We provide two contextual measures of
conceptual distance-one using collapse probabilities and the other weighted
properties-and show how they can be applied to conjunctions using the pet fish
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7696</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7696</id><created>2013-10-29</created><updated>2015-05-06</updated><authors><author><keyname>Boissonnat</keyname><forenames>Jean-Daniel</forenames><affiliation>INRIA Sophia Antipolis / INRIA Saclay - Ile de France</affiliation></author><author><keyname>Dyer</keyname><forenames>Ramsay</forenames><affiliation>INRIA Sophia Antipolis / INRIA Saclay - Ile de France</affiliation></author><author><keyname>Ghosh</keyname><forenames>Arijit</forenames><affiliation>ACMU</affiliation></author></authors><title>Delaunay stability via perturbations</title><categories>cs.CG</categories><proxy>ccsd</proxy><journal-ref>Int. J. Comput. Geom. Appl. 24, 125 (2014)</journal-ref><doi>10.1142/S021819591450006X</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm that takes as input a finite point set in Euclidean
space, and performs a perturbation that guarantees that the Delaunay
triangulation of the resulting perturbed point set has quantifiable stability
with respect to the metric and the point positions. There is also a guarantee
on the quality of the simplices: they cannot be too flat. The algorithm
provides an alternative tool to the weighting or refinement methods to remove
poorly shaped simplices in Delaunay triangulations of arbitrary dimension, but
in addition it provides a guarantee of stability for the resulting
triangulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7697</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7697</id><created>2013-10-29</created><updated>2016-01-04</updated><authors><author><keyname>Auger</keyname><forenames>Anne</forenames><affiliation>TAO</affiliation></author><author><keyname>Hansen</keyname><forenames>Nikolaus</forenames><affiliation>TAO</affiliation></author></authors><title>Linear Convergence of Comparison-based Step-size Adaptive Randomized
  Search via Stability of Markov Chains</title><categories>cs.NA</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider \emph{comparison-based} adaptive stochastic
algorithms for solving numerical optimisation problems. We consider a specific
subclass of algorithms called \cprs (CB-SARS), where the state variables at a
given iteration are a vector of the search space and a positive parameter, the
step-size, typically controlling the overall standard deviation of the
underlying search distribution.We investigate the \emph{linear} convergence of
CB-SARS on \emph{scaling-invariant} objective functions. Scaling-invariant
functions preserve the ordering of points with respect to their function value
when the points are scaled with the same positive parameter (the scaling is
done w.r.t.\ a fixed reference point). This class of functions includes norms
composed with strictly increasing functions as well as many \emph{non
quasi-convex} and \emph{non-continuous} functions. On scaling-invariant
functions, we show the existence of a homogeneous Markov chain, as a
consequence of natural invariance properties of CB-SARS (essentially
scale-invariance and invariance to strictly increasing transformation of the
objective function).We then derive sufficient conditions for \emph{global
linear convergence} of CB-SARS, expressed in terms of different stability
conditions of the normalised homogeneous Markov chain (irreducibility,
positivity, Harris recurrence, geometric ergodicity) and thus define a general
methodology for proving global linear convergence of CB-SARS algorithms on
scaling-invariant functions. As a by-product we provide a connexion between
\emph{comparison-based} adaptive stochastic algorithms and Markov chain Monte
Carlo algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7708</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7708</id><created>2013-10-29</created><updated>2014-05-21</updated><authors><author><keyname>Okayama</keyname><forenames>Tomoaki</forenames></author></authors><title>Theoretical analysis of a Sinc-Nystr\&quot;om method for Volterra
  integro-differential equations and its improvement</title><categories>math.NA cs.NA</categories><comments>Keywords: Sinc numerical method, initial value problem, convergence
  analysis, tanh transformation, double-exponential transformation</comments><msc-class>65L03, 65L05, 65R20, 45J05, 34A12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Sinc-Nystr\&quot;om method for Volterra integro-differential equations was
developed by Zarebnia in 2010. This paper reinforces the method by presenting
two theoretical results: 1) the regularity of the solution, which is required
to implement the method, is analyzed, and 2) its convergence rate is rigorously
analyzed. Moreover, this paper improves the method so that a much higher
convergence rate can be attained, and theoretical results similar to those
listed above are provided. Numerical comparisons are also provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7717</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7717</id><created>2013-10-29</created><updated>2014-10-20</updated><authors><author><keyname>Bui</keyname><forenames>Nicola</forenames></author><author><keyname>Rossi</keyname><forenames>Michele</forenames></author></authors><title>Staying Alive: System Design for Self-Sufficient Sensor Networks</title><categories>cs.NI cs.PF</categories><comments>42 pages, 17 figures, accepted in Transactions on Sensor Networks</comments><acm-class>G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Self-sustainability is a crucial step for modern sensor networks. Here, we
offer an original and comprehensive framework for autonomous sensor networks
powered by renewable energy sources. We decompose our design into two nested
optimization steps: the inner step characterizes the optimal network operating
point subject to an average energy consumption constraint, while the outer step
provides online energy management policies making the system energetically
self-sufficient in the presence of unpredictable and intermittent energy
sources. Our framework sheds new light into the design of pragmatic schemes for
the control of energy harvesting sensor networks} and permits to gauge the
impact of key sensor network parameters, such as the battery capacity, the
harvester size, the information transmission rate and the radio duty cycle. We
analyze the robustness of the obtained energy management policies in the cases
where the nodes have differing energy inflow statistics and where topology
changes may occur, devising effective heuristics. Our energy management
policies are finally evaluated considering real solar radiation traces,
validating them against state of the art solutions and describing the impact of
relevant design choices in terms of achievable network throughput and battery
level dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7729</identifier>
 <datestamp>2013-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7729</id><created>2013-10-29</created><authors><author><keyname>Gregoire</keyname><forenames>Jean</forenames></author><author><keyname>Bonnabel</keyname><forenames>Silv&#xe8;re</forenames></author><author><keyname>de La Fortelle</keyname><forenames>Arnaud</forenames></author></authors><title>Optimal cooperative motion planning for vehicles at intersections</title><categories>cs.SY</categories><comments>presented to IEEE IV 2012 Workshop on Navigation, Accurate
  Positioning and Mapping for Intelligent Vehicles; 6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of cooperative intersection management. It arises in
automated transportation systems for people or goods but also in multi-robots
environment. Therefore many solutions have been proposed to avoid collisions.
The main problem is to determine collision-free but also deadlock-free and
optimal algorithms. Even with a simple definition of optimality, finding a
global optimum is a problem of high complexity, especially for open systems
involving a large and varying number of vehicles. This paper advocates the use
of a mathematical framework based on a decomposition of the problem into a
continuous optimization part and a scheduling problem. The paper emphasizes
connections between the usual notion of vehicle priority and an abstract
formulation of the scheduling problem in the coordination space. A constructive
locally optimal algorithm is proposed. More generally, this work opens up for
new computationally efficient cooperative motion planning algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7741</identifier>
 <datestamp>2013-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7741</id><created>2013-10-29</created><authors><author><keyname>McCreesh</keyname><forenames>Ciaran</forenames></author><author><keyname>Prosser</keyname><forenames>Patrick</forenames></author></authors><title>Greedy Graph Colouring is a Misleading Heuristic</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  State of the art maximum clique algorithms use a greedy graph colouring as a
bound. We show that greedy graph colouring can be misleading, which has
implications for parallel branch and bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7769</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7769</id><created>2013-10-29</created><updated>2015-11-02</updated><authors><author><keyname>Fabbri</keyname><forenames>Renato</forenames></author><author><keyname>Fabbri</keyname><forenames>Ricardo</forenames></author><author><keyname>Antunes</keyname><forenames>Deborah C.</forenames></author><author><keyname>Pisani</keyname><forenames>Marilia M.</forenames></author><author><keyname>Maia</keyname><forenames>Leonardo P.</forenames></author><author><keyname>Oliveira</keyname><forenames>Osvaldo N.</forenames><suffix>Jr</suffix></author></authors><title>Temporal stability in human interaction networks</title><categories>cs.SI physics.soc-ph</categories><comments>See ancillary Supporting Information PDF file for further tables and
  figures. See Gmane python package. This version is an update with less files
  to be submitted to a peer review journal</comments><acm-class>I.5.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reports on stable properties of human interaction networks, with
benchmarks derived from public email lists. Activity along time and topology
evolution were observed by snapshots in a timeline and at different scales. Our
analysis show that the activity across timescales, ranging from seconds to
months, is practically the same for all networks. The most important metrics to
the dispersion of participants in the topological measures space are shown to
be are centrality detrics (degree, strength and betweenness), followed by
symmetry-related metrics and then clustering coefficient. The observed activity
of participants follows the expected scale-free trace, thus yielding the hub,
intermediary and periphery classes of vertices by comparison against the
Erd\&quot;os-R\'enyi model. The relative sizes of these three sectors are shown to
be essentially the same for all email lists and the same along time. Typically,
3-12\% of the vertices are hubs, 15-45\% are intermediary and 44-81\% are
peripheral vertices. Similar results for the distribution of participants in
the three sectors and for the relative importance of the topological metrics
were obtained for 12 additional networks from Facebook, Twitter and
Participabr. These properties are consistent with expectations derived from the
literature and may be general for human interaction networks, which has
important implications for establishing a typology of participants based on
quantitative criteria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7774</identifier>
 <datestamp>2013-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7774</id><created>2013-10-29</created><authors><author><keyname>Peck</keyname><forenames>Mariano Martinez</forenames><affiliation>INRIA Lille - Nord Europe, URIA</affiliation></author><author><keyname>Bouraqadi</keyname><forenames>Noury</forenames><affiliation>URIA</affiliation></author><author><keyname>Ducasse</keyname><forenames>St&#xe9;phane</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Fabresse</keyname><forenames>Luc</forenames><affiliation>URIA</affiliation></author><author><keyname>Denker</keyname><forenames>Marcus</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author></authors><title>Ghost: A Uniform and General-Purpose Proxy Implementation</title><categories>cs.PL</categories><comments>in submission</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A proxy object is a surrogate or placeholder that controls access to another
target object. Proxy objects are a widely used solution for different scenarios
such as remote method invocation, future objects, behavioral reflection, object
databases, inter-languages communications and bindings, access control, lazy or
parallel evaluation, security, among others. Most proxy implementations support
proxies for regular objects but are unable to create proxies for objects with
an important role in the runtime infrastructure such as classes or methods.
Proxies can be complex to install, they can have a significant overhead, they
can be limited to certain kind of classes, etc. Moreover, proxy implementations
are often not stratified and they do not have a clear separation between
proxies (the objects intercepting messages) and handlers (the objects handling
interceptions). In this paper, we present Ghost: a uniform and general-purpose
proxy implementation for the Pharo programming language. Ghost provides low
memory consuming proxies for regular objects as well as for classes and
methods. When a proxy takes the place of a class, it intercepts both the
messages received by the class and the lookup of methods for messages received
by its instances. Similarly, if a proxy takes the place of a method, then the
method execution is intercepted too.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7780</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7780</id><created>2013-10-29</created><updated>2014-04-29</updated><authors><author><keyname>Raskutti</keyname><forenames>Garvesh</forenames></author><author><keyname>Mukherjee</keyname><forenames>Sayan</forenames></author></authors><title>The Information Geometry of Mirror Descent</title><categories>stat.ML cs.LG</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information geometry applies concepts in differential geometry to probability
and statistics and is especially useful for parameter estimation in exponential
families where parameters are known to lie on a Riemannian manifold.
Connections between the geometric properties of the induced manifold and
statistical properties of the estimation problem are well-established. However
developing first-order methods that scale to larger problems has been less of a
focus in the information geometry community. The best known algorithm that
incorporates manifold structure is the second-order natural gradient descent
algorithm introduced by Amari. On the other hand, stochastic approximation
methods have led to the development of first-order methods for optimizing noisy
objective functions. A recent generalization of the Robbins-Monro algorithm
known as mirror descent, developed by Nemirovski and Yudin is a first order
method that induces non-Euclidean geometries. However current analysis of
mirror descent does not precisely characterize the induced non-Euclidean
geometry nor does it consider performance in terms of statistical relative
efficiency. In this paper, we prove that mirror descent induced by Bregman
divergences is equivalent to the natural gradient descent algorithm on the dual
Riemannian manifold. Using this equivalence, it follows that (1) mirror descent
is the steepest descent direction along the Riemannian manifold of the
exponential family; (2) mirror descent with log-likelihood loss applied to
parameter estimation in exponential families asymptotically achieves the
classical Cram\'er-Rao lower bound and (3) natural gradient descent for
manifolds corresponding to exponential families can be implemented as a
first-order method through mirror descent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1310.7782</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1310.7782</id><created>2013-10-29</created><updated>2015-06-23</updated><authors><author><keyname>Baronchelli</keyname><forenames>Andrea</forenames></author><author><keyname>Loreto</keyname><forenames>Vittorio</forenames></author><author><keyname>Puglisi</keyname><forenames>Andrea</forenames></author></authors><title>Individual Biases, Cultural Evolution, and the Statistical Nature of
  Language Universals: The Case of Colour Naming Systems</title><categories>physics.soc-ph cs.CL cs.MA q-bio.PE</categories><journal-ref>PLoS ONE 10(5): e0125019 (2015)</journal-ref><doi>10.1371/journal.pone.0125019</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Language universals have long been attributed to an innate Universal Grammar.
An alternative explanation states that linguistic universals emerged
independently in every language in response to shared cognitive or perceptual
biases. A computational model has recently shown how this could be the case,
focusing on the paradigmatic example of the universal properties of colour
naming patterns, and producing results in quantitative agreement with the
experimental data. Here we investigate the role of an individual perceptual
bias in the framework of the model. We study how, and to what extent, the
structure of the bias influences the corresponding linguistic universal
patterns. We show that the cultural history of a group of speakers introduces
population-specific constraints that act against the pressure for uniformity
arising from the individual bias, and we clarify the interplay between these
two forces.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="51000" completeListSize="102538">1122234|52001</resumptionToken>
</ListRecords>
</OAI-PMH>
